{"id": "LmyjuhNgML", "number": 4136, "cdate": 1757609565528, "mdate": 1759898051354, "content": {"title": "LLM-informed Object Search in Partially-Known Environments via Model-based Planning and Prompt Selection", "abstract": "We present a novel LLM-informed model-based planning framework for object search in partially-known environments. Our approach uses an LLM to estimate statistics about the likelihood of finding the target object when searching various locations throughout the scene that, combined with travel costs extracted from the environment map, are used to instantiate a model, thus using the LLM to inform, rather than replace, planning and achieve effective search performance. Moreover, the abstraction upon which our approach relies is amenable to deployment-time model selection via the recent offline replay approach, an insight we leverage to enable fast prompt and LLM selection during deployment. Simulation experiments demonstrate that our LLM-informed model-based planning approach outperforms the baseline planning strategy that fully relies on LLM and optimistic strategy with as much as 11.8% and 39.2% improvements respectively, and our bandit-like selection approach enables quick selection of best prompts and LLMs resulting in 6.5% lower average cost and 33.8% lower average cumulative regret over baseline UCB bandit selection. Real-robot experiments in household settings demonstrate similar improvements and so further validate our approach.", "tldr": "We present a novel LLM-informed model-based planning framework for object search in partially-known environments, and propose deployment-time prompt selection in this domain.", "keywords": ["llm-informed object search", "model-based planning", "planning under uncertainty", "prompt selection"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/585a5eaebe4deaae2772804a73b842112d221bbb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an approach to integrate LLMs into planning, in particular for object search in partially observable environments. The key idea is to query LLMs for the probability of an object being in a certain location, and integrate this probability into the Q-Value equation. An LLM/prompt selection strategy is also presented, based on the offline replay approach of Paudel & Stein (2023). Experiments are performed in simulation using procedurally generated homes, and with a real robot in one apartment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Combining LLM with planning is interesting and relevant.\n\n- Prompt and LLM selection is also studied, adding more technical contributions to the paper.\n\n- The application studied (object search in household environments) is interesting and relevant.\n\n- 150 distinct household environments are used in simulation, and one real apartment is also explored.\n\n- The paper is in general well-written and well-presented."}, "weaknesses": {"value": "- The idea of querying a ML model to bias planning is quite straightforwards. For instance, it reminds me of the AlphaZero approach for on-line planning. The context and technique here is obviously different, but the point is just that the key idea is not very surprising.\n\n- There is no statistical study of the results. Not even the variance is shown.\n\n- Although the paper is well-written, the problem formulation still seems confusing/inconsistent. Sometimes the cost of searching a location seems to be considered, but sometimes only the travelling cost seems to be taken into account.\n\n- It is unclear how this approach would scale, e.g., as the number of containers and apartment size grows. It is also unclear how the approach would scale in terms of potential prompts/LLMS.\n\n- I am quite confused by the prompt selection approach. I would expect it to be used to select prompts and/or LLMs to integrate into the planning approach, but in the end approaches without planning and/or prompts also seem to be considered.\n\n- Baselines could be stronger:\n\n-- The P-DIRECT prompt seems to be ignoring the container search cost R_{search}.\n\n-- OPTIMISTIC+GREEDY seems to also ignore the cost of searching a container. \n\n-- What about directly sampling an action from the probabilities outputted by the LLM?\n\n-- Isn't it possible to compare against existing approaches in the literature? (E.g., PUCT strategy, or some state-of-the-art LLM+planning approach?)\n\n= Detailed Comments =\n\n- \"these approaches focus selecting prompts\" -> \"focus on\"\n\n- \"Our robot is tasked find a target\" -> \"to find\"\n\n- \"without the robot having deploy the plans informed by LLMs\" -> \"having to\""}, "questions": {"value": "1 - Are the results statistically significant? What is the confidence interval of the results?\n\n2 - Could you clarify the problem formulation? Are you considering only the navigation cost, or do you also consider the cost of searching inside a specific container? If the cost of searching a container is considered, why this is later ignored in the paper?\n\n3 - How scalable is the approach to problem size, and prompt/LLM options size?\n\n4 - Are you also considering LLM-DIRECT and OPTIMISTIC+GREEDY as potential options for your prompt selection approach (replay selection), and also for UCB selection? If so, why is that reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G5EMgWi2k1", "forum": "LmyjuhNgML", "replyto": "LmyjuhNgML", "signatures": ["ICLR.cc/2026/Conference/Submission4136/Reviewer_6AAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4136/Reviewer_6AAV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797530100, "cdate": 1761797530100, "tmdate": 1762917193622, "mdate": 1762917193622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for LLM-informed object search in partially-known household environments, tackling the challenge of long-horizon planning under uncertainty. The core contribution is the LLM+MODEL planning approach, which uses a Large Language Model (LLM) not to directly dictate actions, but to provide an estimate of the object finding likelihood, $P_S$. This likelihood is integrated into a Bellman-like equation that minimizes the expected navigation cost. The work further introduces a method for fast deployment-time selection of the best prompt/LLM combination, leveraging an offline replay mechanism compatible with the high-level action abstraction. Experiments in simulation (ProcTHOR) and on a real robot (LoCoBot) show that the LLM+MODEL planner significantly outperforms LLM-DIRECT and purely optimistic baselines, and the replay selection outperforms standard UCB-bandit selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting approach by integrating Large Language Models (LLMs) with formal model-based planning for long-horizon object search, a strategy that intelligently mitigates the LLM's known shortcomings in quantitative planning. \n\nInstead of using the LLM as a brittle planner, the work uses it as a robust knowledge source to estimate the object finding likelihood, $P_S$, and incorporates this into a planning equation. This clever formulation enables reasoning about the future consequences of actions, leading to significantly better performance than both LLM-direct and myopic greedy baselines. \n\nI think that this approach can be highly practical due to the novel Replay Selection mechanism, which leverages the planning framework to quickly identify the best-performing LLM/prompt combination during deployment. The quality of the validation is commendable, with experiments conducted across two distinct state-of-the-art LLMs (GPT-5 and Gemini 2.5) in large-scale simulation and successfully verified on a real robot (LoCoBot)."}, "weaknesses": {"value": "A few weaknesses:\n\n1. The probability $P_S$ is generated by prompting the LLM to output a numerical percentage value (e.g., \"95%\"). It is unclear how this raw, subjective LLM output is interpreted or normalized to be a valid probability mass, $\\sum_{i} P_S(a_i) \\leq 1$, across all available containers $a_i$. Simply asking for a probability does not guarantee a statistically or mathematically rigorous distribution across all containers.\n2. While the concept of offline replay is compelling, the description (Section 5.3) states that for an alternative policy $\\pi_{\\theta^{\\prime}}$, the cost is computed by \"pessimistically assuming that all other containers would not have contained the target object\" This seems like a strong assumption that might bias the replayed cost. It is not entirely clear if the replayed policy $\\pi_{\\theta^{\\prime}}$ re-plans its actions at every step based on its \"simulated\" belief state, $b_t'$, or if it simulates a full, fixed sequence of actions generated at $t=0$. It would be great if the authors could provide further clarification.\n3. The LLM’s role is restricted to outputting a single number ($P_S$). This is a very narrow use of the LLM's reasoning capability. The action space is also restricted to searching pre-defined containers. This limits the robot’s ability to plan novel exploration strategies or decide if a room itself is worth exploring, which is critical in a fully unknown environment, a stated goal for future work. But I am confused with the motivation for using the LLMs in this manner. Could the authors provide more clarification on this? \n4. The experiments does not include comparisons with open-source LLMs (only Gemini and GPT-5 are included). Also, the authors should provide approximate tokens used for their experiments and the costs associated with querying close sourced models.\n\nFew missing reference that are relevant to this topic and would be a good addition:\n1. Zhang, X., Qin, H., Wang, F., Dong, Y., & Li, J. (2025, May). Lamma-p: Generalizable multi-agent long-horizon task allocation and planning with lm-driven pddl planner. In 2025 IEEE International Conference on Robotics and Automation (ICRA) (pp. 10221-10221). IEEE.\n2. Nayak, S., Morrison Orozco, A., Have, M., Zhang, J., Thirumalai, V., Chen, D., ... & Balakrishnan, H. (2024). Long-horizon planning for multi-agent robots in partially observable environments. Advances in Neural Information Processing Systems, 37, 67929-67967.\n3. Ling, S., Wang, Y., Fan, C., Lam, T. L., & Hu, J. (2025). ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration https://www.arxiv.org/abs/2509.24230"}, "questions": {"value": "1. Please clarify how the individual likelihood values $P_S(a_t)$ provided by the LLM are used in the Bellman equation (Eq. 3). If the LLM returns $90$% for container $A$ and $80$% for container $B$ (both possible from the prompt design), how do you ensure that the search actions $a_A$ and $a_B$ respect the laws of probability in the model? Is there a normalization step applied to the raw LLM outputs $\\hat{P}_S$ such that $\\sum_{a_i \\in \\mathcal{A}(b_t)} P_S(a_i) \\leq 1$?\n2. The OPTIMISTIC+GREEDY baseline is uninformed. A more informative baseline would be a model-based planner that uses a naive, uniform probability distribution for $P_S$ (i.e., $P_S = 1/|\\mathcal{A}(b_t)|$ for all containers) or perhaps one based on environment size, rather than LLM commonsense. Did the authors compare against a non-LLM, purely Information-Theoretic Planning baseline? This would quantify the *true* value of the LLM's commonsense beyond simply outperforming a greedy approach. Another baseline to consider would be LLaMAR [1] where they use a heuristic-based exploration to find objects relevant to the search using semantic matching through a sentence transformer.\n3. Table 1 shows that for Gemini, P-CONTEXT-B performs best, while for GPT-5, P-CONTEXT-A performs best. Since the semantic difference between P-CONTEXT-A and P-CONTEXT-B is minimal (”differ in terms of the language is used in the prompt text” ), could the authors elaborate on *why* this subtle linguistic difference causes such a significant performance divergence between the two LLMs? This insight would be valuable for the LLM community. I believe that this leads us to the ongoing research question of how much does the prompt affect the performance of LLMs for different tasks.\n4. Currently, the action space $\\mathcal{A}$ consists of pre-identified containers. Could the high-level action abstraction be redefined to also include an action, $a_{\\text{explore}}$, where the robot navigates to a new room/area? This would be key to extending the work to the “fully unknown environments” mentioned in the conclusion.\n\n[1]: Nayak, S., Morrison Orozco, A., Have, M., Zhang, J., Thirumalai, V., Chen, D., ... & Balakrishnan, H. (2024). Long-horizon planning for multi-agent robots in partially observable environments. Advances in Neural Information Processing Systems, 37, 67929-67967."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aOF2ota4ez", "forum": "LmyjuhNgML", "replyto": "LmyjuhNgML", "signatures": ["ICLR.cc/2026/Conference/Submission4136/Reviewer_WQ43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4136/Reviewer_WQ43"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849030707, "cdate": 1761849030707, "tmdate": 1762917193270, "mdate": 1762917193270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two key challenges in object search within partially known environments using LLM-based planning: (1) how to leverage an LLM’s commonsense world knowledge to reduce the cost of finding a target object, and (2) how to select the optimal prompt–LLM combination in an online setting. To tackle the first challenge, the paper introduces a Bellman-style function that computes the expected action cost while incorporating the LLM’s knowledge about how likely a target object is to appear in a given location, based on the agent’s current belief. For the second challenge, the authors employ a replay buffer that stores completed search runs—including ground-truth object locations—to retrospectively evaluate the performance of different prompt–LLM combinations (“what would have happened” if a given combination had been used). Using these estimated performances, a UCB-style algorithm dynamically selects the most promising combination online. Through both simulation and real-robot experiments, the paper demonstrates consistent improvements over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Quality: this paper set up the problem clearly and easy to follow. one plus is the paper not only provides simulation experimental results but also show demonstrated improvements in real robot experiments."}, "weaknesses": {"value": "- When introducing the Bellman equation, the authors could provide a more detailed explanation of how the Q-function is computed through dynamic programming. A step-by-step illustration of the recursive computation process would clarify how expected costs are propagated across belief states.\n- My main concern of this paper is the formulation of the second problem—selecting the best prompt and LLM combination—is arguably limited in its general applicability to LLM-based object search. For prompt optimization, it would be more natural to use language feedback to iteratively refine prompts, rather than relying on a fixed, predefined set of prompts and applying a bandit-style selection algorithm. As implemented, the best achievable prompt is constrained by the initial candidate set, which may restrict the system’s adaptability and overall performance. Maybe the author can explain why you select this setting."}, "questions": {"value": "1. How is the belief state updated when the robot not found the target location in the containers it currently is searching? \n2. In the prompt design, LLM doesn't have access to previous searching location of the robots, which potentially could speed up the searching. Without this information given to LLM, how LLM differs its output from different time step or different known information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Zxo7Rg9Cl", "forum": "LmyjuhNgML", "replyto": "LmyjuhNgML", "signatures": ["ICLR.cc/2026/Conference/Submission4136/Reviewer_yYRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4136/Reviewer_yYRt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946800894, "cdate": 1761946800894, "tmdate": 1762917192735, "mdate": 1762917192735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using a zero-shot probability generated from an LLM to contribute to calculating the cost of finding a target object in a candidate container. They also incorporate distance and search costs to the overall cost. This approach allows for increased efficiency and performance in finding target objects. They also contribute an offline replay approach to tune prompt and LLM selection. They provide multiple variations of their methods and provide real-robotic results to show improvement over UCB."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The aspect of automatically finding the best LLM for the task is an important area of research that is under explored, and I appreciate it being tackled in this work\n\n-I like how the authors build upon previous zero-shot LLM approaches for object navigation and also use distance costs to inform decisions\n\n-Authors provide multiple variations of their approach\n\n-Strong results with real-world robot demonstrations"}, "weaknesses": {"value": "-L47, some citations are necessary to backup the claim of poor performance on quantitative reasoning tasks, especially since L46 mentions “it is well-established”.\n\n-In Figure 1, why is P_S “informed by LLM” as annotated underneath the equation, when it is the probability of finding the object at the given action/container?  The probability that a book is in a container is independent of what the LLM thinks, so why is the probability based on the LLM, or maybe I’m misunderstanding what the authors mean by “informed by LLM”. Maybe there should a \\hat{P}_S that is “informed by LLM” and then during checking P_S is the true probability."}, "questions": {"value": "Minor comments\n-In L39, “long term goodness” is a bit vague, perhaps “long term feasibility” or “long term optimality”. Or “long term value” to align more with sequential planning/RL?\n\n-In L50, could the authors give specific ambiguities or design questions that need to considered when integrate LLMs with model-based planners? That will help will clarifying the motivation. When I first read it, I assumed the authors meant prompting strategies to encode information of the environment model to the LLM, but then the next paragraph talks about prompting strategies “In addition”, so I am unclear what the authors refer to in L50."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KeWySKEoLD", "forum": "LmyjuhNgML", "replyto": "LmyjuhNgML", "signatures": ["ICLR.cc/2026/Conference/Submission4136/Reviewer_drFT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4136/Reviewer_drFT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994652672, "cdate": 1761994652672, "tmdate": 1762917192391, "mdate": 1762917192391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}