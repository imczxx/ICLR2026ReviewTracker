{"id": "vndn1Wrult", "number": 1631, "cdate": 1756899090271, "mdate": 1763561498155, "content": {"title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning", "abstract": "Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic thinking tokens and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain---and often ignore---the \\emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \\emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \\emph{Echo Likelihood Gap} $\\Delta\\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP.  Consequently, we develop \\emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \\emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \\emph{attention refocusing} mechanism. We evaluate under identical decoding settings and compute budgets on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines.", "tldr": "LRMs often repeat the question before thinking. We formalize this Echo of Prompt via a probabilistic cost, the Echo Likelihood Gap, and show it refocuses attention. ED-SFT and Echoic Prompting exploit it for consistent gains on math reasoning.", "keywords": ["LRM", "reasoning", "finetuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f53cde26188b97699cb504836160b00f23a9c4e0.pdf", "supplementary_material": "/attachment/e366cf3bf68d7845c84815ef42dac62d0298fafc.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the \"Echo of Prompt\", the tendency of large reasoning models to repeat a user's query before providing an answer. The authors challenge the view of this behavior as a mere flaw, hypothesizing instead that it functions as an intrinsic \"attention-refocusing mechanism\" that grounds the model's subsequent reasoning process.\nTo analyze this, they introduce a probabilistic framework to measure the cost and effect of EOP, finding that it correlates with higher accuracy by increasing attention to intermediate reasoning representations within the model's middle layers.\nThe paper introduces two methods: Echo-Distilled SFT, a fine-tuning approach that instills an \"echo-then-reason\" pattern, and Echoic Prompting, a training-free technique to re-ground the model during inference. Both methods demonstrate performance gains over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to understand."}, "weaknesses": {"value": "1. The paper claims to provide a \"mechanistic explanation\" for EOP's effectiveness. However, showing that attention patterns differ between correct and incorrect answers is more of a detailed observation or characterization of a correlation but not causation.\n2. The analysis is almost entirely based on aggregated attention scores—the average attention from all subsequent \"answer\" tokens to the initial \"prefix\" tokens. This is a very high-level metric. The analysis does not explore: (1) Which specific tokens in the prompt are being attended to; (2) How information from the prompt/prefix is being transformed and utilized across layers."}, "questions": {"value": "1. What is the definition of \"suffix-only gap\" in line 196?\n2. Which language and dataset did you use for the analysis presented in Table 2 and Figure 3?\n3. In Section 3.3, the authors group samples into Correct and Wrong outcomes and analyze their attention patterns. What about comparing groups based on the presence or absence of EOP itself (i.e., EOP-present vs. EOP-absent traces)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BBMbsBjrs9", "forum": "vndn1Wrult", "replyto": "vndn1Wrult", "signatures": ["ICLR.cc/2026/Conference/Submission1631/Reviewer_LFkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1631/Reviewer_LFkJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749484140, "cdate": 1761749484140, "tmdate": 1762915837204, "mdate": 1762915837204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies LLM's tendency to repeat or echo the question in the reasoning trace, and what role does such behavior play. The authors argue that such echo of prompt (EOP) serves a cognitive role by helping the model refocus attention on key details of the problem. The formalization includes a notion of probabilistic cost: the amount of likelihood the model “spends” on such echo, and how such cost differs across both correct and incorrect traces. The findings show that such likelihood gap correlates with answer correctness. Also, authors find that correct traces show higher answer-to answer-prefix attention in the middle layers. This supports the idea that echoes serve as anchors for reasoning, helping the model stay aligned with its internal problem framing. Lastly, the authors use these findings to improve reasoning by either finetuning or prompting the model to generate such echoes and the results show improved accuracy compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on an understudied and not well-understood phenomenon in LLM reasoning. It asks how redundancy in the reasoning traces could actually be helpful to the model reasoning. \n\n2. The analysis framework is reasonable, and the results suggest some correlation between EOP and reasoning correctness. The analysis is deep and insightful. Careful ablations such as on prefix length, attention-layer grouping support the results.\n\n3. I find how the authors took their findings and used them to design a prompting/finetuning strategy as opposed to purely focusing on analysis. \n\n3. The paper is well written and fun to read."}, "weaknesses": {"value": "1. Causality remains speculative: The correlation between echoes and accuracy is solid, but the paper doesn’t prove causality. It’s perfectly possible that correct traces happen to include EOPs because the model is already more deliberate. \n\n2. Some of the conclusions are not fully justified: I am not super convinced that the answer-to-answer-attention gap shown in Fig. 3 left is purely a product of EOPs. The authors should show the same analysis on traces without EOPs. \n\n3. The finetuning setup may be problematic. The authors collect CoTs for training by prompting a teacher to generate the EOPs and compare to finetuning on traces prompted without this. This may conflate the benefit of better teacher data (caused by generating the EOP) and the existence of the EOP. A fair comparison is to train on the same CoTs but with the EOP part removed. \n\n4. Narrow evaluation: most analysis focuses on simple GSM8K and a single 8B model. It remains unclear whether the results will generalize to different model families/sizes."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FhKauTKnxD", "forum": "vndn1Wrult", "replyto": "vndn1Wrult", "signatures": ["ICLR.cc/2026/Conference/Submission1631/Reviewer_gXeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1631/Reviewer_gXeE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886264833, "cdate": 1761886264833, "tmdate": 1762915837067, "mdate": 1762915837067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Echo of Prompt (EOP), a mechanism that leverages language models’ natural tendency to restate questions. By formalizing its likelihood cost and developing Echo-Distilled SFT and Echoic Prompting, the authors enhance reasoning efficiency via attention refocusing, achieving consistent gains on GSM8K, MathQA, and MATH benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and well-motivated. It starts from the phenomenon that “restate the question would help answer” and introduces their study methods and experiments solidly.\n- The idea of using Likelihood Gap is inspiring and interesting.\n- The attention-based analysis of the Echo Prompt’s effects is well-motivated and insightful.\n- Two types of experiments to demonstrate the effects of EOP are promising and comprehensive."}, "weaknesses": {"value": "- As the author said in Lines 193-197, it seems a contradictory result. The “suffix-only gap” is actually larger for the wrong group (1.29 > 1.14), which contradicts the authors’ claim that EOP improves the correct group. They describe it as “the same pattern,” but the data show the opposite trend. Additionally, the authors should add the definition of “uffix-only gap” in the main paper.\n- Could you use experiments to prove that there is no “absolute weight value fluctuation” issue across different layers, which would lower the weight of Table 2 and its conclusion?\n- The “answer-prefix” tokens are located near the beginning of the sequence, while middle-layer attention naturally tends to focus on nearby tokens. This means that the observed +2% difference may arise from positional bias rather than the EOP mechanism itself. \n- It would be better to write the implementation details in Section 4 about the training details of norm-SFT and ED-SFT. If no such description, it is hard to say the performance gained from the ED-SFT rather than the fluctuation or hyperparameter modulation. \n- Regarding Line 409, it is difficult to claim “out-of-domain generalization,” since GSM-8K, MathQA, and Hendrycks-MATH all belong to the same task domain."}, "questions": {"value": "In Section 4.2 (Echoic Prompting) and Figure 4, how can we be sure that the observed performance gains truly stem from the Echo of Prompt (EOP) mechanism, rather than from confounding factors such as increased context length or the model’s inherent tendency to rephrase or restate the question?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bkR7fSEwWU", "forum": "vndn1Wrult", "replyto": "vndn1Wrult", "signatures": ["ICLR.cc/2026/Conference/Submission1631/Reviewer_aJgj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1631/Reviewer_aJgj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762453121160, "cdate": 1762453121160, "tmdate": 1762915836933, "mdate": 1762915836933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate a phenomenon they dub “echo of prompt” (EOP) where reasoning models spend early tokens in their CoTs effectively just restating the problem. They analyze the probabilistic cost of it by rejection sampling away CoTs which include the echo to demonstrate the gains in performance and accuracy from this repetition. They then use SFT to reinforce this echoing behavior, as well as a mid-trace “echo prompting.”\n\nThe echos are naturally very common across open reasoning models from Qwen, Deepseek, and Openai. They use a trained MLP to predict whether a sequence contains an EOP or not to reject samples containing it, allowing them to compute the relative length-normalized token likelihood of sequences which do or don’t contain them (echo likelihood gap).\n\nThe authors claim that the echo likelihood gap is more pronounced in sequences that are correctly answered. I had trouble making sense of the rationale here (see weaknesses). They examine attention patterns to offer a “refocusing” explanation of how EOP helps. They find that the attention importance between the answer tokens and prefix (echo) tokens are higher than those to the question itself on average, and that a *difference*  between the attention importance in the correct and wrong states is only present for the prefix-answer condition, suggesting that a higher correlation between those parts correlates w/ better performance. I’m not sure what to make of some of these results such as the “middle-layer dominance”.\n\nThe layerwise discriminability results (Table 3) are the most compelling. They find that based on AUC and Cohen’s d the difference in Ans->pref attention is more predictive of correct/incorrect than Ans->Q.\n\nFinally, they perform SFT on distilled reasoning traces from gpt-oss which contain the EOP in order to instill this behavior on Qwen and Deepseek models. They do find that consistently, fine-tuning the models on EOP data improves performance considerably more than those without the echo."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Simple, original, and well-motivated idea\n\nThe latter half of the paper contains reasonably strong evidence suggesting their claims are true. By showing a strong improvement on SFT with EOP vs weak improvement without, I was convinced that EOP is mechanistically important to higher performance in RMs."}, "weaknesses": {"value": "I am having trouble making sense of the claims within p3-4. Table 1 contains a lot of information that isn’t really explained. What is the N for each “group”? Are the “correct” and “wrong” the number of samples where the answer is correct in both cases, and in some it contains the EOP and in others it doesn’t? Do the same questions have samples in both classes? What does it mean for a specific raw trace to have a single echo-trimmed counterpart? Are they the same question? Further, how significant is a difference of 0.08 nats/token? \n\nFigure 2 doesn’t seem to show anything that supports the text. I don’t see a “mode of 200” here (l240). You need way more bins to support any of the claims as they are all about >21 tokens\n\nI’m not sure how the attention analysis really shows that the prefix tokens are “used” for refocusing. After all, even in the wrong answers these tokens are still being generated. While there is a modestly lower attention weight on average for the wrong answers, I’m sure the distributions overlap pretty considerably. I think a statistical significance test between these conditions would be more compelling than a delta between the means\n\nThere are lots of results in here, but some of them don’t really seem to matter for the overall message of the paper and feel more like padding. For example the key insights in lines 301-320."}, "questions": {"value": "See weaknesses.\n\nI am aware of interp work claiming that analysis of attention patterns is weak evidence at best for explaining observed behavior in transformers (see Attention is not Explanation, Jain & Wallace 2019 https://arxiv.org/abs/1902.10186), but I do not have a strong opinion one way or the other. Can you defend this method of analysis?\n\nRe: not having a statistical significance test for the attention weight refocusing analysis, could you provide one? For example, kolmogorov-smirnov or even just a t-test.\n\nSome details in sec4.1 are underspecified. How do you produce the baseline normal-SFT traces? Do those also come from gpt-oss but with rejection sampling using your MLP? I want to know that you’re providing SFT data that is otherwise “as good” or else it could just be a difference in overall CoT quality that doesn’t have to do with the EOP."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PMbqcVJnaI", "forum": "vndn1Wrult", "replyto": "vndn1Wrult", "signatures": ["ICLR.cc/2026/Conference/Submission1631/Reviewer_VPGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1631/Reviewer_VPGP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762599440415, "cdate": 1762599440415, "tmdate": 1762915836770, "mdate": 1762915836770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are encouraged that they find our study of the Echo of Prompt (EOP) to be \"simple, original, and well-motivated\" (Reviewer `VPGP`), \"inspiring and interesting\" (Reviewer `aJgj`), and a valuable investigation into an \"understudied phenomenon\" (Reviewer `gXeE`). We are also glad that the reviewers appreciate our \"reasonable analysis framework\" (Reviewer `gXeE`) and the \"reasonably strong evidence\" (Reviewer `VPGP`) supporting the effectiveness of our proposed methods.\n\nIn this revision, we have worked to fully address the reviewers' concerns, particularly regarding the causal link between EOP and reasoning performance, and the granularity of our attention analysis. We have expanded the paper by **6 pages to include detailed experimental setups, additional visualizations, and robust statistical tests**. Key updates include:\n\n1.  **Causal Evidence (Response to `LFkJ`, `gXeE`):** We introduced a new semi-online intervention experiment (echo-insertion ablation) which demonstrates that explicitly injecting an echo into a failing trace significantly improves the probability of recovering the correct answer, providing stronger causal evidence beyond correlation.\n2.  **Fine-grained Attention Analysis (Response to `LFkJ`, `VPGP`, `aJgj`):** We expanded our attention analysis to include token-wise significance tests, word-level case studies, and information flow visualizations. These results confirm that the \"refocusing\" effect is statistically significant and targets specific, task-critical tokens rather than being a broad positional artifact.\n3.  **Clarified Metrics and Baselines (Response to `aJgj`, `VPGP`, `gXeE`):** We have clarified the definitions of the Echo Likelihood Gap and SFT baselines, ensuring that our comparisons are fair and our metrics are rigorously defined.\n\nWe believe these additional experiments and clarifications substantially strengthen the paper's core claims. Below, we address each reviewer's comments in detail."}}, "id": "21yvbm920o", "forum": "vndn1Wrult", "replyto": "vndn1Wrult", "signatures": ["ICLR.cc/2026/Conference/Submission1631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1631/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1631/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763553953594, "cdate": 1763553953594, "tmdate": 1763553953594, "mdate": 1763553953594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}