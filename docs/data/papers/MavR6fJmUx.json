{"id": "MavR6fJmUx", "number": 22346, "cdate": 1758329889075, "mdate": 1763657746804, "content": {"title": "Hyperbolic Associative Memory Networks", "abstract": "Associative memory models encode a set of candidate patterns as “memories” and, upon receiving a partial or noisy query, retrieve the patterns most relevant to the query via similarity interactions/energy minimization, thereby recovering or recalling target patterns from incomplete inputs; they have achieved widespread success across many perception and representation learning tasks. However, when the retrieval process is constrained to Euclidean geometry, hierarchical structure in the data is difficult to capture accurately: in many tasks that require handling hierarchical data, Hopfield networks based on Euclidean representations tend to introduce bias and distortion into semantic relations.\nTo this end, we extend modern Hopfield retrieval to hyperbolic space. Specifically, we map query and memory vectors from Euclidean space to hyperbolic space via exponential maps, and define an energy function with clear theoretical grounding based on the Minkowski inner product; the retrieval procedure adopts Riemannian manifold optimization, combining curvature-aware gradients with exponential maps to ensure that the optimization trajectory remains on the manifold and yields stable updates.\nOur central view can be stated as a hierarchy-sensitivity hypothesis: when the data exhibit clear and deeper hierarchical structure, hyperbolic geometry brings statistically significant improvements; when the hierarchy is weak or only shallow, performance shows no significant difference from Euclidean modern Hopfield networks. We validate this through depth-controlled comparisons and cross-level consistency metrics, and the empirical results are consistent with the hypothesis.\nAccordingly, the proposed hyperbolic associative memory can serve as a plug-and-play general memory module embedded into task architectures that require hierarchical understanding, for storing and retrieving raw inputs, intermediate representations, or learned prototypes, and explicitly exploiting hierarchical information.\nMoreover, our method is formulated in a model-agnostic manner and applies to any hyperbolic model with constant negative curvature. In this paper, we instantiate it with the Poincaré ball for experiments.", "tldr": "We extend modern Hopfield networks to hyperbolic space with a model-agnostic, Riemannian energy formulation, yielding a plug-and-play memory module that excels on deeply hierarchical data while matching Euclidean baselines on shallow cases.", "keywords": ["Hyperbolic Geometry", "Modern Hopfield Networks", "Associative Memory", "Riemannian Optimization", "Hierarchical Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eaa864e27bfc7e588f91291a2dbfad46894939a8.pdf", "supplementary_material": "/attachment/ad72a70e3d78179854b1ef805fe2d912652626ed.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Hyperbolic Associative Memory Networks (HAMNs), a novel extension of modern Hopfield networks (MHNs) to hyperbolic space. The core motivation is that Euclidean-based MHNs struggle to capture hierarchical data structures, leading to distortion, while hyperbolic geometry is naturally suited for such data.\n\nThe authors propose a principled formulation that:\n\nMaps query and memory vectors from Euclidean to hyperbolic space using exponential maps.\n\nDefines an LSE-based energy function using a hyperbolic similarity metric (based on $-cosh(d_{\\mathcal{M}})$).\n\nEmploys a Riemannian optimization (CCCP) to perform the memory retrieval process on the manifold.\n\nThe central claim is a \"hierarchy-sensitivity hypothesis\": HAMNs should significantly outperform Euclidean MHNs on data with deep hierarchical structure, while performing on par with them on flat or shallow-hierarchy data.\n\nThe authors test this by creating artificial 2, 3, and 4-level hierarchies for CIFAR-100 and comparing performance on level-specific accuracy and a cross-level consistency metric. They also test on MIL and MoleculeNet datasets as \"weak hierarchy\" benchmarks.\n\nWhile the theoretical formulation is sound and the research direction is well-motivated, the empirical evidence is not strong enough to fully support the claims. The results are mixed, the computational overhead is significant, and the primary experiment relies on an artificial data hierarchy, which fails to provide a conclusive test of the model's ability to capture innate hierarchical data structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear conceptual idea and principled derivation.**  \n   The paper provides a clean extension of modern Hopfield energy-based retrieval to hyperbolic geometry, with a principled energy based on geodesic distance / −cosh(d) (Minkowski-like similarity) and a CCCP-based Riemannian update rule. The math for gradients, CCCP surrogate and closed-form update is sound and carefully explained.\n\n2. **Model-agnostic design & practical modules.**  \n   The framework is model-agnostic (Poincaré, Lorentz, etc.) and the authors provide concrete module designs (HypHopfield / HypPooling / HypLayer) that can be dropped into networks—valuable for broad adoption. Implementation hints and pseudocode are provided.\n\n3. **Thoughtful theory + capacity discussion.**  \n   The paper analyzes energy-well separation and sphere-packing style capacity bounds in hyperbolic space, which grounds the method theoretically for hierarchical data.\n\n4. **Relevant experiments and ablations.**  \n   The authors run a spectrum of experiments: controlled CIFAR-100 hierarchical label trees (2/3/4 levels), weak-hierarchy tasks (MIL, MoleculeNet), and ablations on curvature and stored-pattern count. This shows attention to the core hypothesis."}, "weaknesses": {"value": "1. **Empirical evidence for “captures hierarchical structure” is limited / not fully convincing.**  \n   The core hierarchy-sensitivity hypothesis is only evaluated by (i) synthetic label hierarchies on CIFAR-100 and (ii) coarse–fine consistency metrics (cophenetic correlation). While sensible, these do **not** directly demonstrate that learned embeddings or retrieval dynamics actually encode hierarchy (e.g., radial depth separation or LCA distances). The claim that HAMNs exploit hierarchical geometry needs stronger direct evidence.\n\n2. **Improvements are often modest and inconsistent across metrics / levels.**  \n   Table 1 shows HAMNs sometimes yield only small gains (or are tied) versus baselines; e.g., on the 3-layer setup the gains are marginal and on 2-layer results are mixed. The paper asserts “statistically significant improvements as depth increases,” but effect sizes and significance tests are not fully laid out. Gains appear incremental rather than transformative.\n\n3. **Tests rely heavily on one synthetic-controlled dataset (CIFAR-100 label restructurings).**  \n   Restructuring labels is useful but it does not guarantee that **input representations** or the memory module truly organize patterns in a hierarchical metric manner. The other evaluations (MIL, MoleculeNet) belong to the “weak / shallow” regime and show only small margins. The test-suite lacks **datasets with known hierarchical latent structure** (trees, taxonomy graphs, synthetic trees with noise, ontologies).\n\n4. **Runtime & memory trade-offs are underplayed.**  \n   While HAMNs have theoretically fewer FLOPs/parameters, actual runtime and GPU-memory are worse in the reported implementation (Table 2). This is an important practical drawback that should be discussed in more depth."}, "questions": {"value": "1. **Direct hierarchy evidence:**  \n   Can you show that embeddings (memories + queries) arrange by tree depth and branch—e.g., radial coordinate correlates with level, pairwise geodesic distance correlates with LCA depth?\n\n2. **Choice of CIFAR label restructurings:**  \n   How sensitive are results to different hierarchical clusterings? Would the observed gains persist under alternative splits?\n\n3. **Temperature and curvature interactions:**  \n   How was θ chosen relative to curvature c? Is there a principled relation ensuring CCCP concavity conditions hold?\n\n4. **Effect of stored-pattern count:**  \n   Why does top-level accuracy sometimes degrade with more stored patterns? Does this interact with curvature or temperature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uXzflJsNkm", "forum": "MavR6fJmUx", "replyto": "MavR6fJmUx", "signatures": ["ICLR.cc/2026/Conference/Submission22346/Reviewer_kjQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22346/Reviewer_kjQH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503604289, "cdate": 1761503604289, "tmdate": 1762942178644, "mdate": 1762942178644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of Euclidean Hopfield networks on hierarchical data by extending associative memory retrieval to hyperbolic geometry.\nThe authors propose a Hyperbolic Associative Memory Network (HAMN) that maps inputs to a hyperbolic manifold and defines a new energy function using the Minkowski inner product in hyperbolic space.\nA Riemannian optimization (concave–convex procedure) is employed for memory retrieval, ensuring updates stay on the manifold. \nExperiments on hierarchical classification (CIFAR-100 with label trees), multi-instance learning, and molecular property prediction confirm the hypothesis: HAMN yields significant gains on deeply hierarchical tasks while matching Euclidean baselines on shallow structures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. modeling hierarchical data/task is very important (and not easy) given their combinatorial nature.\n\n2. I skeem through the math. seems ok but not sure about correctness. (didn't check them line-by-line. might be more willing to do that after other issues are addressed.)\n\n3. HAMNs are presented as a model-agnostic module that can “plug into” various architectures. This is very good for applicability."}, "weaknesses": {"value": "Replacing the Euclidean inner product in MHNs is not new. The paper’s novelty narrows to a Riemannian instantiation in hyperbolic space with curvature‑aware optimization and empirical focus on hierarchical data.\n\n1. this paper omits many key developments on modern Hopfield networks that are directly relevant to its main claims.. \n    - Especially, **replacing the euclidean dot product is not new.** https://arxiv.org/abs/2404.03827 has kernelized the inner product as a learnable operator conditioned on the memory set with rich geometric intuition. https://arxiv.org/abs/2410.23126 showed the optimality of such method. Yet neither work is mentioned at all. \n    - Moreover, **there are fundamental design principles for MHNs reported in literature already**, yet not mentioned at all. \n    - Krotov & Hopfield https://arxiv.org/abs/2008.06996 gives a dynamical system derivation of a series of energy function and the update rule of MHNs through Legendre transformation in the classical mechanics sense.\n    - Han Liu's group https://arxiv.org/abs/2309.12673 and https://openreview.net/forum?id=6iwg437CZs gives a unified framework for constructing the energy functions of modern Hopfield networks via entropy regularizer. This \"entropy regularizer\" framework extends and covers many MHN-attention correspondence (cf [Ramsauer et al. (2020)]): softmax, sparsemax, $\\alpha$-entropymax. \n    - Similarly, the nonparametric framework for MHNs https://arxiv.org/abs/2404.03900 is also omitted, even though it covers many MHN-attention correspondence: linear, sparse, top-K, random feature...etc. \n    - similar efforts from Andre Martins' group are also omitted, e.g. https://arxiv.org/abs/2402.13725 (ICML 2024)\n\n    Many of these works covered similar CCCP derivations, memory/energy bound, retrieval errors, noise robustness and associative retrieval task validations. I just checked, all of the above-mentioned works have been published in NeurIPS, ICML, or ICLR. Given that this paper builds upon the MHN framework, disregarding these prior works raises serious concerns about scholarly completeness and citation practice. It’s understandable to miss a few. It happens to all of us. But missing all of them is very strange.\n  \n    Also, for empirical validations, many of these works should serve as baselines. Yet this paper did none of them.\n\n\n\n\n2. **novelty:** \n    - At a high level, the method could be seen as a relatively straightforward combination of known components: taking the existing modern Hopfield network (which is essentially an attention module) and operating it in a hyperbolic embedding space. The novelty is more in execution than in conceptual breakthrough. One might argue that given hyperbolic attention networks existed https://openreview.net/forum?id=rJxHsjRqFQ and this work establishes a new Hopfield = attention correspondence. Otherwise (and probably even so), the step to hyperbolic Hopfield is not huge. The authors’ contributions lie in working out the math and demonstrating the effects, but the both the idea itself and the math may not feel surprising. This could be perceived as a minor conceptual advance (“apply known method A in context B”). However, the quality of execution somewhat mitigates this. It’s incremental but well done.\n\n    - [minor, more like a personal opinion] using hyperbolic space to embed a large tree (or combinatorial problems or the \"hierarchical problem\") while keeping the dimension small is not new. it gives a continuous relaxation of a combinatorial problem (e.g tree) where distance correlates with depth. Yet it only makes the embedding space geometrically larger, not structurally smarter. all these are sort of well-known, why not, instead, embed precise discrete or symbolic structures into your memory model to achieve the same goals (modeling hierarchical data) strictly more compact and precise. It's methodologically more efficient and economic and more align with standard CS practice.\n\n3. **limited scope and clarity of motivation:** \n\n    - By design, HAMNs shine only in hierarchical settings. Yet it never gives a clear definition of what do they mean by \"hierarchical data\". Without such clarity, it's hard to parse why the method works better on these data/tasks. \n    - this paper does not provide a precise explanation of the benefits of using the Minkowski inner product. It only offers a heuristic motivation. I can follow the reasoning because of my background in differential geometry and this area of research, but I doubt that general ML readers will find it equally accessible or convincing.\n    - In tasks without a clear hierarchy, they yield little to no improvement (and in a few cases marginal drops). This is not a flaw per se. It’s exactly what the paper predicts, but it means the method’s impact is confined to certain problem types. For an audience seeking a generally superior memory network, the answer is: use hyperbolic only if you expect hierarchical structure. The paper could emphasize this scope more. As is, a reader might wonder: if my data isn’t tree-like, do I incur overhead for nothing? The answer from results is yes, overhead with no gain. So the practical utility is conditional. Remember, the overhead to compute \"geometric\" gradients/optimization is very large (see below)\n\n4. **computational overhead:** The current implementation is significantly slower and more memory-hungry than the Euclidean counterpart. This is a concern for scaling up. The experiments used moderate-scale data (CIFAR-100, 50k instances & MoleculeNet tasks with up to ~10k molecules). If one were to apply HAMNs to very large memory sets or high-dimensional data, the runtime could be problematic. So, while not a conceptual weakness, the practicality in large-scale or real-time scenarios is a concern.\n\n5. **implementation complexity:** Using HAMNs requires familiarity with Riemannian optimization. The method introduces significant complexity: one must handle exponential and logarithmic maps, maintain numerical stability near the boundary, and tune an extra hyperparameter (curvature $c$). This is more involved than a standard attention or memory layer. This could be a barrier for general users/readers. The paper and appendix provide guidance, but integrating this module might still be hard compared to a Euclidean one.\n\n6. **failure cases anylsis:** The paper does not deeply analyze scenarios where HAMN underperforms or equals baseline. For instance, why did HAMN slightly lag on some tasks? Is it purely because no hierarchy = noise, or could it be that the hyperbolic optimization sometimes finds a suboptimal attractor (since Hopfield can get stuck in a local minimum that isn’t the correct memory)? In Hopfield networks, spurious attractors can occur. With hyperbolic, could there be more risk of weird local minima? They didn’t discuss these. This makes the validity of HAMN questionable.\n\n7. **minor:**\n    - there are duplicate reference entries for the same paper\n    - besides above mentioned literature omissions, the authors should also clarify how Hyperbolic Attention Networks differ from and relate to HAMN. Currently, they include HypAttn as a baseline, but a direct discussion in the text would help readers understand the conceptual advancement. Emphasize why energy-based retrieval with manifold optimization offers an advantage over a direct hyperbolic attention mechanism.\n    - The paper primarily tests classification tasks (or retrieval framed as classification). It doesn’t explore standard associative memory scenarios, e.g., pattern completion or denoising.\n    - To further demonstrate HAMN’s utility, it would be great to test on other domains with deep hierarchies, such as: Knowledge Graph completion or taxonomy reasoning, Hierarchical clustering or few-shot classification with class hierarchies, and most interestingly Language tasks with latent hierarchy. These would broaden the impact and show the method in action where tree structure is intrinsic. While CIFAR-100 with WordNet-like label trees is a good start, tasks with naturally deep hierarchies (like WordNet itself) would solidify the claims."}, "questions": {"value": "please also see above weakness\n\nLLM disclaimer: I used LLMs to check (a few of) my claims, to understand some refs mentioned in the submission, and to polish my language."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No new concerns beyond standard MHN misuse risks."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSHyE2lCju", "forum": "MavR6fJmUx", "replyto": "MavR6fJmUx", "signatures": ["ICLR.cc/2026/Conference/Submission22346/Reviewer_8WQQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22346/Reviewer_8WQQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767275036, "cdate": 1761767275036, "tmdate": 1762942178415, "mdate": 1762942178415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This theoretical paper presents a new way to model Hopfield networks in the hyperbolic embedding spaces. The rationale to move from Euclidean to hyperbolic spaces is to allow hierarchical concepts to be modeled such as those represented in graphs and trees.  While this benefit was also shown in an earlier NeurIPS 2017 paper, this paper applies those ideas in the context of Hopfield networks. The paper is largely theoretical and will benefit from illustrations indicating the benefit of doing this for the Hopfield networks. Experiments are also a bit sketchy and it appears that the implementation still needs some work."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The strength of the paper lies in the formulation itself for modeling hierarchical object storage and retrieval through Hopfield network formulations. The claim made is that associative memory mechanisms purely within Euclidean geometry may distort the underlying structural information during memory retrieval. This is backed up by experimental data.  The design of the energy function is novel and stable update is being assured."}, "weaknesses": {"value": "Despite the promised of Hopfield networks, getting them to work on large scale datasets has been a problem both due to computational complexity and the metastability problems projecting the raw data. How much the use of a hyperbolic space alters that is not clear. Showing the value proposition though a visual example would be helpful in this case. In particular, the ability to discriminate between the patterns was a function of the shape of the energy landscape and how sharp the basins of attraction were. Will it be similar in the hyperbolic space or the manifold is smooth per level while still being distinct across hierarchy levels.\n\nAlso, newer ontology embeddings are capturing the depths of hierarchy better through contrastive formulations in Euclidean spaces. Would that change the observation and need to do this modeling?"}, "questions": {"value": "Is there any biological motivation for the hyperbolic formulation for Hopfield networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0QirP8koN1", "forum": "MavR6fJmUx", "replyto": "MavR6fJmUx", "signatures": ["ICLR.cc/2026/Conference/Submission22346/Reviewer_phhi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22346/Reviewer_phhi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849311155, "cdate": 1761849311155, "tmdate": 1762942178200, "mdate": 1762942178200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hyperbolic version of modern Hopfield networks. It is formulated under the hyperbolic metric with a focus on the Lorentz model. The paper first show the guarantees of energy convergence, next they give the definition of new hyperbolic Hopfield layers for deep learning. Their empirical results show their proposed models outperforms other baselines on several tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The empirical results show the effectiveness of their proposed model."}, "weaknesses": {"value": "1. A major contribution to computational models of associative memory is their biological plausibility, which I believe the paper is lacking. \n2. Without the closed form of neuron updates, it is very difficult to provide any biological intuitions.\n3. The math in this paper seems to be incorrect. In general, Hadamard DC programming cannot guarantee to converge to a stationary point. Existing literatures only guarantees to critical points at best.\n4. Another main feature studying Hopfield networks is their recall error. To my understanding, as the paper uses polynomial kernels, the recall error will likely scale with separation. In other words, with larger $R$, the larger we have the recall error, which seems not intuitive of a memory model.\n5. What would the size of the attractor basin be? This part should be justified as this concerns the retrievability of patterns.\n6. The reference format for equations is problematic; many are presented as \"eq Equation x\". \n7. The use of the \\paragraph is not consistent, some with period some without.\n8. I did not find the limitation section, which is essential for a research paper.\n\nOverall, I think this paper requires major revision for both presentations and results."}, "questions": {"value": "1. Why would Riemannian DC problems apply Euclidean CCCP? Can the authors justify this part?\n2. How did the authors setup their experiments? Did you assume input data points are hyperbolic points? Or did you use an Exp map to first project it into hyperbolic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ycyhUrkMWw", "forum": "MavR6fJmUx", "replyto": "MavR6fJmUx", "signatures": ["ICLR.cc/2026/Conference/Submission22346/Reviewer_KiWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22346/Reviewer_KiWD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963256318, "cdate": 1761963256318, "tmdate": 1762942177972, "mdate": 1762942177972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}