{"id": "DgJqQk6y19", "number": 9636, "cdate": 1758131679813, "mdate": 1759897707651, "content": {"title": "The Softmax Bottleneck Does Not Limit the Probabilities of the Most Likely Tokens", "abstract": "In many popular transformer architectures, an output projection matrix linearly maps lower-dimensional embeddings into a higher-dimensional space of logits.\nIt has been shown that this leads to a {\\em softmax bottleneck} that prevents the production of arbitrary probability distributions.  It has been argued that this limits large language models (LLMs) in their ability to express next token probabilities that perfectly align with the statistics of natural language.  We focus on the ability of such models to produce accurate probabilities for just the top-$m$ tokens.  We provide theoretical bounds that show that even a randomly initialized projection matrix can successfully do this for rather large values of $m$, supported by empirical results on random and trained matrices. This suggests that the softmax bottleneck does not significantly limit the capabilities of LLMs. We also derive bounds on the maximal value of $m$ for which this is possible, given an embedding dimension, bounding the possible performance of any trained matrix.", "tldr": "We show that randomly initialized or trained output projection matrices can successfully produce exact probabilities for the top m tokens for rather large values of m.", "keywords": ["Softmax Bottleneck+", "Transformer+", "Output Projection Matrix+", "Large Language Models+"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c0497ec507f90707ac40bbfff68208f92369b24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper offers a new look at the so-called \"Softmax Bottleneck,\" which is said to limit the ability of a model as it can struggle to go from inner embeddings to logits over the whole vocabulary: the authors claim that what matters in practice is to predict accurately the probabilities for top tokens, especially given that sampling is usually done from a truncated distribution. The paper first gives theoretical lower bounds showing that indeed any chosen set of m tokens can be the most likely ones, with high probability, and even reach, collectively, a specific probability. It then validates this on GPTâ€‘2 and TinyLlama: in practice this works for somewhat surprisingly larges m."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- this is a really inspiring take on the softmax bottleneck, as indeed given the prevalence of truncated sampling, what matters is often the ability to produce valid probabilities for the top tokens of a vocabulary ;\n- the authors show the theoretical possibility to produce the embeddings that would result in a given m-subset getting the most probabilities, for  a randomly initialized projection matrix, with some bounds, and even to reach a given probabilities for this subset ;\n- the experiments validate their theory for a trained GPT-2 and TinyLlama, far exceeding the lower bounds ;\n- the paper is convincingly written, with a well formulated problem, sound proofs and related experiments."}, "weaknesses": {"value": "- the proofs seem sound but this is really to the best of my understanding and I had to trust the authors for this theoretical work. Similarly, it's not clear what to make of such a sentence \"since this derivation involves some approximation, we have empirically confirmed that simulations match our theoretical predictions (not shown)\";\n- although this work proves the existence of \"embedding that the OPM will map into the appropriate probability distribution\", there is no guarantee that the model will produce it ;\n- furthermore, although this is interesting, it does not say anything about the ability of a given network to produce the embeddings for \"desired\" m-subsets in many contexts. This is not the point of this work, but it does take a practical look at the softmax bottleneck, so my _practical_ concern might be warranted..."}, "questions": {"value": "repeating what I see as a weakness:\n- what do you think of the reachability question? Your results do establish existence of an embedding $x$ for a given $A$ but I wonder whether a network can be trained to produce that $x$...\n- are we sure that same (trained) network would produce the valid embeddings, whatever the context? This seems important to fully grasp the extent of your work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ufFE7CS9Di", "forum": "DgJqQk6y19", "replyto": "DgJqQk6y19", "signatures": ["ICLR.cc/2026/Conference/Submission9636/Reviewer_3zJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9636/Reviewer_3zJk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570996348, "cdate": 1761570996348, "tmdate": 1762921168403, "mdate": 1762921168403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work addresses the question of the softmax bottleneck in transformers -- that is, the output projection limiting the production of arbitrary probability distributions. The authors ask whether the softmax bottleneck restricts the LLM from representing the probabilities of the top-m most probable tokens, arguing that the exact probabilities of unlikely tokens are less important. The authors provide theoretical results showing that there exist OPMs that can represent any specific probabilities over top-m tokens for large m and empirical results on GPT-2 and TinyLlama."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extends on prior work on softmax bottleneck, reframing the problem around top-m probabilities\n- The research question is clear as well as the writing\n- The derivations give nice lower bounds for random matrices and the theoretical insights are supported by experiments on GPT-2 and TinyLlama."}, "weaknesses": {"value": "- The results show the existence of embeddings that can realize given top-m probabilities, but does not address whether these are learned by real transformers\n- Assumes low-probability tokens are irrelevant, but there may be domains (e.g., RL fine-tuning or exploration) where coverage over rare tokens is important"}, "questions": {"value": "- What is the role of weight tying here? \n- Are there settings (e.g., RL, exploration, or calibration tasks) where representing low-probability tokens accurately would matter, and how would your framework apply there? Do the values of m you found seem sufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7iTqkvmNUV", "forum": "DgJqQk6y19", "replyto": "DgJqQk6y19", "signatures": ["ICLR.cc/2026/Conference/Submission9636/Reviewer_B1qY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9636/Reviewer_B1qY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842587825, "cdate": 1761842587825, "tmdate": 1762921167953, "mdate": 1762921167953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the softmax bottleneck problem, i.e. the study of the expressivity of usual neural language models that use a hidden dimension that is smaller than the vocabulary size $N$. In that setup, it has been shown that there exist probability distributions in $\\Delta^N$ that cannot be predicted. In this paper, the authors state that language model outputs can match probability distributions with a relatively large support, either by predicting the token set that belong in that support, or even by predicting exactly the probabilities for the tokens in such supports. As a result, they argue that the softmax bottleneck is not a dramatic issue for language models, which they assess through a short experimental analysis."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper studies a very interesting topic and conducts a novel and relevant theoretical analysis.\n- **Theoretical results**: The propositions presented in this paper are novel and provide a more profound understanding of this issue. They shed light on the complexity of reachable distributions, and probabilize (a part of) the set of reachable distributions, which is both a challenging and exciting outcome. Even though the proof in section 4.1 is not perfectly rigorous, it makes very reasonable assumptions and uses the Inverse Wishart distribution in this context, which is a new and very insightful contribution in the context of random matrices for the softmax bottleneck problem. I appreciate that the authors extended their work to fixed top-m probability. Proposition 4 is very elegant. The use of results on the signrank is also very novel and could open new venues for this topic. It leads to a clean result on the minimal dimensions needed to accurately match the supports of probability distributions."}, "weaknesses": {"value": "Although I deeply appreciate the core theoretical results of this work, I quite disagree with some of the claims and conclusions made in the abstract, introduction, and empirical sections. I also believe that this paper could be enhanced by deeper empirical experiments.\n- **Claims about broader implications**: It is mentioned in the abstract and introduction that \"the softmax bottleneck does not significantly limit the capabilities of LLMs\", or that \"limitations to the expressiveness of transformers are not really that significant\". My understanding of what is proven in the paper is 1- for a single given target probability vector and a random OPM matrix, there exists an $x$ for which the predicted probability matches the top-$m$ target probabilities; 2- if one only cares about the support but wants to match any set of targets on the top-$m$ ranking of tokens, then in most setups $d \\approx m/2$ is sufficient. Hence, it is unclear whether matching any set of targets for the correct probabilities (let alone the order of such probabilities) is possible. This is a crucial distinction, as the complexity of matching any permutation of token order is much higher than matching the support as a set. Moreover, even when there would exist output representations that would give the desired probabilities, they might be configured in ways that are very difficult to reach during training, which would limit the applicability of these results to usual training setups. What can thus be concluded from this paper is that the softmax bottleneck phenomenon does not strongly limit the prediction of the next-token probability supports, and that individual \"low-entropy\" probability vectors should be truthfully matched with non-negligible probability.\n- **Lack of clarity in some proofs**: The proof of Proposition 2 is a bit hard to follow and could be made clearer. The first paragraph is a bit confusing and it seems like it could be summarized to convey the idea more directly. Moreover, the statistical independence of the entries of $(A_mA_m^T)^{-1}$ is never verified empirically in the paper.\n- **Experimental design**: The experimental section is less appealing than the theoretical section. Figure 1 verifies the lower bound given in Propositions 1 and 2 (and incidentally shows that it is not particularly tight). Figure 2 computes the bound curve for several setups. Figure 3 explores a question that seems loosely related to the topic at hand, and that was covered many times by the anisotropy literature (see works of Ethayarajh et al., among others). A lot of questions remain unanswered, from the necessary experiments to extensions that would have been relevant: given an actual token distribution taken from natural language, how much of the probability supports can a random\\trained matrix cover? What are the types of x and s that are observed? In a synthetic controlled setup, e.g. data generated from a bigram with known supports, can a model be trained to properly recover the supports up to $m$ tokens? In cases where the supports are of different sizes, ie where m should be different across target probabilities, what types of solutions are found and with what performance? The experimental section also ignores the second theoretical section of the paper.\n\nIn the current state of the paper, the claims and conclusions that are made about the relevance of the softmax bottleneck are not strongly supported by the theoretical section and the experiments. It is particularly important as this article is quite technical, which implies that readers may take these claims for granted without thoroughly reading the paper and understanding its potential limitations. Hence, it is crucial that the claims accurately reflect the results that are presented to avoid any misinterpretation.\n\nOverall, I am excited by the topic and theoretical part (which I would rate 8/10), but I am underwhelmed by the conclusions and experimental sections (which I would rate 2/10)."}, "questions": {"value": "- Could you report the results that show independence for the rows of $(A_mA_m^T)^{-1}$?\n- By my understanding, Proposition 3 could be proven more easily by setting all a_j at distinct positions on the unit sphere in 2d and setting x_j = a_j?\n- Do OPMs actually train to account for larger m values? Is it possible that the learned A_m do not have the used invertibility properties?\n- To what extent does token rank and specified ratios would affect the results in section 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cgRmflaXU6", "forum": "DgJqQk6y19", "replyto": "DgJqQk6y19", "signatures": ["ICLR.cc/2026/Conference/Submission9636/Reviewer_SQX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9636/Reviewer_SQX5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856427558, "cdate": 1761856427558, "tmdate": 1762921167442, "mdate": 1762921167442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates both theoretically and empirically the impact of the softmax bottleneck on the ability of large language models to correctly represent the probabilities of the m most probable tokens. Their conclusion is that the softmax bottleneck does not seem to \"provide any limitations to LLMs in most realistic settings\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- An interesting contribution to the \"softmax bottleneck\" literature that goes beyond previous work (notably Demeter et al. 2020 and Grivas et al. 2022).\n- The paper combines theoretical analyses and empirical results.\n- The paper is generally clear and well written."}, "weaknesses": {"value": "- The motivation for the research question is not convincing enough. Why is it so important that models can assign the correct probabilities, summing to (almost) 1, to the m best tokens? If it is possible (and it is for fairly large m's), what does it tell us about language models? If it hadn't been possible, why would that have been an issue, beyond very niche scenarios such as choosing a number or a US state at random?\n- The discussion in section 6 could be more detailed. In particular, how do your results complement previously published papers that found that the softmax bottleneck *is* an issue for language models (e.g. Parthiban et al. 2021, Godey et al. 2024)?"}, "questions": {"value": "Suggestions rather than questions:\n- Please double-check that the papers you cite as arXiv papers have not been published (by that I mean \"really\" published, in the proceedings of a conference or in a journal). Whenever it is the case, the proper publication should be cited, not the arXiv pre-print. There are multiple cases of this issue in your bibliography.\n- Although the paper is generally well written, please refrain from using abbreviations such as \"WLOG\" (used twice)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fQGbjfiRTY", "forum": "DgJqQk6y19", "replyto": "DgJqQk6y19", "signatures": ["ICLR.cc/2026/Conference/Submission9636/Reviewer_kpFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9636/Reviewer_kpFi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000049747, "cdate": 1762000049747, "tmdate": 1762921167007, "mdate": 1762921167007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}