{"id": "AeIGeRFDAy", "number": 19778, "cdate": 1758299249298, "mdate": 1759897020084, "content": {"title": "SHED Light on Segmentation for Monocular Depth Estimation", "abstract": "Monocular depth estimation is a dense prediction task that infers per-pixel depth from a single image, fundamental to 3D perception and robotics. There are extensively strong depth foundation models, supported by a backbone pre-trained with a massive scale of data. However, do these depth foundation models really understand the structure? Although real-world scenes exhibit strong structure, these methods treat it as an independent pixel-wise regression problem, often resulting in structural inconsistencies in depth maps, such as ambiguous object shapes. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly from spatio-layout by incorporating segmentation into depth estimation. Inspired by the bidirectional hierarchical reasoning in human perception, SHED redesigns the vision transformer by replacing fixed patch tokens with segment tokens, which are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, and the intermediate segment hierarchy emerges naturally without explicit supervision. SHED offers three key advantages. First, it improves depth boundaries and segment coherence, and demonstrates robust cross-domain generalization. Second, it enables features and segments to better capture global scene layout. Third, it enhances 3D reconstruction and reveals part structures that conventional pixel-wise methods fail to capture.", "tldr": "", "keywords": ["depth estimation", "segment hierarchy", "vision transformer"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6123d96e12992cc17636961898dff67a31621c2c.pdf", "supplementary_material": "/attachment/b3a1435183d3d40e27960e7a43ad71be32bb4805.pdf"}, "replies": [{"content": {"summary": {"value": "This paper ensembles the CAST into the DPT architecture with a proposed decoder (Reverse hierarchy). SHED forms superpixel tokens that are hierarchically pooled in the encoder and unpooled in the decoder. The hierarchy is learned without segmentation labels, supervised only by the final depth loss. Experiments show the advantages of this SHED. The authors provided a quite comprehensive analysis based on the resutls."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated. Ensembling segmentation process in monocular depth estimation sounds reasonable to me. The idea of introducing CAST is also natural from my point of view.\n\n- Layout-aware retrieval evaluation is interesting. This metric provides a reasonable way to evaluate the gain from adding CAST.\n\n- Large gains in boundary/part structure can somehow demonstrate the effectiveness of this method."}, "weaknesses": {"value": "- The technical contribution can be incremental. The major point is to adopt CAST for depth estimation, with merely one decoder proposed.\n\n- Comparative breadth and fairness. Outdoor / diverse domains: Experiments focus on NYUv2 (indoor). It remains unclear if the segment-hierarchy inductive bias retains benefits on outdoor benchmarks\n\n- It would be better to add DPT results in Table 2 (zero-shot). Is it possible to align the encoder of DAV2 when comparing? The zero-shot performance difference can be derived by the encoder. \n\n- Runtime, memory, and scalability. The method adds CAST into DPT. There are qualitative claims about efficiency, but no measured throughput, VRAM, or FLOPs relative to DPT/DAV2\n\n- Positioning vs. multi-task & post-processing baselines. While SHED is not for a classical multi-task learning, the most direct competitors, depth+semantics or edge-guided methods, are missed in the paper.\n\n- When leaking the segmentation GT, it would be good to apply such a weak supervision. But the paper majorly focuses on NYU, a small dataset with segmentation labels. I was wondering if the method can be scaled up, using more training data."}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdAiwpoPmR", "forum": "AeIGeRFDAy", "replyto": "AeIGeRFDAy", "signatures": ["ICLR.cc/2026/Conference/Submission19778/Reviewer_o4eZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19778/Reviewer_o4eZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760889241351, "cdate": 1760889241351, "tmdate": 1762931625236, "mdate": 1762931625236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SHED, a novel encoder–decoder architecture that explicitly incorporates geometric priors from spatial layouts by integrating semantic segmentation into the depth estimation process.\nSpecifically, SHED transforms the input image into superpixel tokens and applies graph pooling to construct coarser segments during encoding, enhancing depth boundary sharpness, segment coherence, and cross-domain robustness.\nBuilt upon the DPT backbone, SHED achieves superior performance on both in-domain and cross-domain depth estimation benchmarks compared to the original DPT model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Incorporation of Superpixels into Depth Estimation\nSHED integrates superpixel information into the ViT token representation within the depth estimation network, enabling the model to jointly exploit raw RGB features and segmentation-aware cues.\nBy embedding discrete object boundary information through superpixels, the network gains enhanced structural awareness, leading to improved scene layout understanding and depth prediction accuracy.\n\n2. Performance on Depth Estimation and 3D Layout\nThe authors validate the effectiveness of SHED through both quantitative depth estimation metrics and qualitative 3D layout results.\nTheir experiments demonstrate that SHED not only achieves superior pixel-wise depth accuracy but also produces more coherent and geometrically consistent scene layouts."}, "weaknesses": {"value": "1. Limited Novelty\nThe main contribution of this paper lies in the hierarchical clustering component, while the majority of the pipeline is directly adapted from existing frameworks such as DPT and CAST. As a result, much of the observed performance gain may stem from these strong baselines rather than the proposed SHED module itself. This concern is further supported by the narrow performance gap between DPT and SHED reported in the results table, which raises questions about the true contribution of the newly introduced components.\n\n2. Absence of Ablation Study on Model Architecture\nAlthough the proposed method claims architectural innovation, the paper lacks an ablation study to validate the contribution of each architectural component. Without such analysis, it is difficult to determine whether the performance improvement originates from the encoder, decoder, or hierarchical clustering mechanism, and to what extent each contributes to the overall gain.\n\n3. Missing Evaluation on Zero-Shot Depth Estimation\nAs far as I understand, the cross-domain evaluation in this paper still involves shared datasets for fine-tuning and testing, which limits its generalization claims. Given that recent trends in depth estimation research emphasize zero-shot performance across unseen domains, the absence of such evaluation weakens the paper’s claim of robust cross-domain generalization.\n\n4. Insufficient Comparison with State-of-the-Art Methods\nThe experimental comparisons are primarily conducted against DPT, which is no longer representative of the current state-of-the-art. To strengthen the validity and competitiveness of SHED, the authors should include comparisons with more recent high-performing models such as Marigold and DepthPro. This would provide a fairer and more persuasive assessment of the proposed method’s effectiveness."}, "questions": {"value": "1. What happened if we drop hierarchical clustering with the same SHED decoder or keep hierarchical clustering with DPT decoder? The paper needs some ablation study.\n\n2. What is the performance of SHED with zero-shot depth estimation task (evaluation on NYUv2, KITTI, ETH3D, ScanNet, DIODE)?\n\n3. Please refer to the upper weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uOfkwJPzgL", "forum": "AeIGeRFDAy", "replyto": "AeIGeRFDAy", "signatures": ["ICLR.cc/2026/Conference/Submission19778/Reviewer_yMXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19778/Reviewer_yMXn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875786950, "cdate": 1761875786950, "tmdate": 1762931624455, "mdate": 1762931624455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SHED, a ViT-based model for monocular depth estimation. The method introduces a bidirectional segment hierarchy into the DPT framework. On the encoder side, the model follows the CAST strategy by replacing traditional square patch tokens with superpixel tokens, which are progressively clustered by feature similarity to form a fine-to-coarse hierarchical representation. On the decoder side, a reverse hierarchy is introduced, performing soft unpooling and segment-to-pixel projection to remap hierarchical semantic features back to spatial feature maps, thereby generating structured depth outputs. The design treats segmentation as a geometric structural prior, enhancing boundary sharpness and intra-segment consistency in depth prediction. Compared with DPT-based baselines, SHED achieves improvements in object boundary quality, local depth consistency, and layout-aware retrieval performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear presentation: concepts (superpixel/segment tokens, soft unpooling) and implementation details are well explained and reproducible.\n- Strong results: sharper boundaries, better intra-segment consistency, and improved layout-aware retrieval over DPT-based baselines."}, "weaknesses": {"value": "1. Limited in novelty.  \n   While the bidirectional segment hierarchy is somewhat new, the broader idea of leveraging segmentation structure to assist depth estimation has been explored [1]. The paper should more clearly delineate how SHED differs from prior joint semantic–segmentation–depth approaches and what new contributions it adds.\n2. Insufficient experiments.  \n   The paper lacks a quantitative evaluation of the learned segmentation results and only presents visualizations. It should add comparisons of segmentation metrics such as F1 score and IoU, similar to CAST. While the paper shows better results than DepthAnything in the constrained, fine-tuned setting, it's crucial to show DepthAnything's zero-shot performance on NYU as well. The inferior performance of DA may be caused by the fine-tuning process instead of the model itself.\n3. Lack of efficiency analysis.  \n   Although the method emphasizes structural advantages, it provides no quantitative comparison against DPT baselines in terms of FLOPs, parameter count, inference speed, or GPU memory.\n4. Lack of comparison with segmentation-guided depth estimation methods.  \n   The paper’s main claim is that segmentation structures help with depth estimation, but the comparisons do not include some state-of-the-art methods that also utilize segmentation to improve depth estimation.\n\n--\n[1] Object-aware Monocular Depth Prediction with Instance Convolutions."}, "questions": {"value": "1. The authors present boundary results for SHED and DPT in Figure 4. Although SHED has more complete boundaries than DPT, it appears that SHED introduces more erroneous boundaries. Could the authors explain this?\n2. The experiments mention only the NYUv2 dataset. Could you provide results on an additional dataset such as KITTI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IlYyOZEjdD", "forum": "AeIGeRFDAy", "replyto": "AeIGeRFDAy", "signatures": ["ICLR.cc/2026/Conference/Submission19778/Reviewer_o4PH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19778/Reviewer_o4PH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989100513, "cdate": 1761989100513, "tmdate": 1762931623651, "mdate": 1762931623651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a superpixel-based method for dense prediction via hierarchical grouping. The method, named SHED, operates on the internal feature space and creates a hierarchy of superpixel-level segments across which both a fine-to-coarse aggregation and a coarse-to-fine refinement are performed. The primary task of interest on which the method is validated is monocular depth estimation / 3D estimation, which is shown to benefit from the segment-level grouping when working with a modern DPT-based network. The authors also demonstrate quantitative improvements in other related tasks over the baseline network, such as boundary detection and image retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow. The method section is well-structured and the mathematical notation is defined rigorously.\n\n2. The idea of applying a reverse segmentation hierarchy for coarse-to-fine refinement in a dense prediction task such as depth estimation is novel, interesting, well-motivated, and shown to yield promising results."}, "weaknesses": {"value": "1. The method is built on a modern yet not state-of-the-art network for depth estimation, i.e. DPT, which dates from 2021. Apart from the comparison to Depth Anything v2 in Table 2, the authors have not considered any other recent and high-performing ViT-based networks to show their improvement holds in a more competitive setting, e.g. [A], [B], [C].\n\n2. For the depth estimation training and evaluation, the authors have trained SHED only on two datasets (separately on each), i.e. NYUv2 and HyperSim, and used the test set of only one of these two datasets (NYUv2) to evaluate their method. This practice has two issues. First, the training data are not diverse, while recent methods [A], [B], [C] have shown that leveraging large-scale, diverse training data is equally important to a sophisticated model design for well-generalizing depth estimation. Second, the limited-domain evaluation which is performed by the authors is less indicative of the true ability of the trained model to generalize, which is why most recent works primarily focus on the zero-shot evaluation setting, testing a single universally trained model on *several different datasets* from those included in training. It would have been beneficial to perform such diverse zero-shot evaluation for SHED on depth estimation too.\n\n[A] UniDepth: universal monocular metric depth estimation. In CVPR, 2024.\n\n[B] Metric3D: towards zero-shot metric 3D prediction from a single image. In ICCV, 2023.\n\n[C] UniK3D: universal camera monocular 3D estimation. In CVPR, 2025."}, "questions": {"value": "1. Can the authors include more methods in a fair, in-domain comparison for depth estimation with training and testing on NYUv2, beyond just DPT and Depth Anything v2?\n\n2. Can the authors extend their evaluation to a more complete and diverse set of evaluation datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NTDiVbnyON", "forum": "AeIGeRFDAy", "replyto": "AeIGeRFDAy", "signatures": ["ICLR.cc/2026/Conference/Submission19778/Reviewer_UhUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19778/Reviewer_UhUV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996400676, "cdate": 1761996400676, "tmdate": 1762931622970, "mdate": 1762931622970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}