{"id": "hESsatbRee", "number": 9371, "cdate": 1758120521106, "mdate": 1763641450210, "content": {"title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates", "abstract": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce **S**ource-**S**hielded **U**pdates (**SSU**), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.2% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.", "tldr": "SSU prevents catastrophic forgetting when adapting instruct LLMs to a target language by proactively freezing core parameters before training on unlabeled data, while achieving strong target performance.", "keywords": ["Cross-lingual Transfer", "Language Adaptation", "Continual Pre-training", "Multilinguality", "Catastrophic Forgetting", "Instruct LLM", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e941af722e4b8b199571ee5ebdde58a468d2e81.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on adapting instruct-tuned LLMs to target languages using only unlabled target language text. Standard continual pretraining on target language data often results in catastophic forgetting where the new training data erases source knowledge and significnatly affects the core chat and instruction-following capabilities. To address this issue, this paper proposes Source-Shielded Updates, a simple and effective source-focused appraoch that shields source knowledge by freezing specific columns of the weight matrices while training on target language data. The columns to be frozen are selected by using a small set of source data and a parameter importance scoring method.\n\nThe proposed approach is verified by adapting 7B and 13B OLMo2 instruct models (trained on English-dominated Common Crawl corpus) to five different target languages. Experimental results demonstrate that SSU consistently outperforms relevant baselines in terms of target-language proficiency while preserving general source-language performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The proposed approach is simple and easy to use, which I consider as strengths given its effectiveness.\n\nDifferent from existing approaches that use target data-driven signals to identify which parameters are to be trained, the proposed approach is source-focused and uses source data to select parameters that are kept frozen during training. This makes sense and also works better compared to target data-driven GMT method.\n\nSeveral downstream evaluation datasets have been used to demonstrate source knowledge preservation.\n\nAblation studies were conducted demonstrating the effect of freezing ratio on the trade-off between source knowledge retention and target knowledge acquisition.\n\nEffectiveness of proposed SSU strategy is demonstrated using multiple importance scoring methods."}, "weaknesses": {"value": "The proposed approach is compared with alternative selection straggles at 50% freezing ratio. While 50% provides a good operating point for the proposed approach (in terms of trade-off between source and target metrics), it may not be the best operating point for the alternative approaches. So, the current results do not provide a full comparison between different selective update strategies."}, "questions": {"value": "Getting plots like Fig.2 for the other selective update strategies will make the comparison between different approaches more thorough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TM86SuMlyt", "forum": "hESsatbRee", "replyto": "hESsatbRee", "signatures": ["ICLR.cc/2026/Conference/Submission9371/Reviewer_A7vK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9371/Reviewer_A7vK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698340431, "cdate": 1761698340431, "tmdate": 1762920987251, "mdate": 1762920987251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Source-Shielded Updates (SSU), a parameter update strategy to mitigate catastrophic forgetting during target language adaptation of instruction-tuned LLMs. SSU first identify critical parameters with Wanda technique, then freeze the corresponding parameters to prevent gradient update. Experiments conducted on five low-resource languages and two model scales (7B, 13B) demonstrate that SSU reduces catastrophic forgetting by a substantial margin compared to full fine-tuning and several strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The question of adapting an instruction tuned LLM to support new languages without forgetting is critical. \nThe proposed method is well motivated and mathematically sound.\nThe empirical results are strong. The proposed method successfully reduced forgetting."}, "weaknesses": {"value": "1.\tThe novelty is largely overclaimed and many related works and baselines are not included as they should be. First, the idea of freezing critically parameters for learned knowledge has been widely adopted. A classical CL approaches, HAT, shares this idea. More recent works such as CAT and SPG. Therefore, the claim that `However, existing paradigms are ill-suited for the specific challenge of adapting instruct models with unlabeled target language text. They either rely on random selection, offering no principled way to preserve knowledge, or on signals from the new data to guide updates (target-focused).` is not true. Additionally, many CL baselines are missing. \n\n2.\tThe evaluation mostly considers ICL ability of LLMs. It would be better to include more generation/language modeling tasks to eval the new language ability.\n\n3.\tNew language typically faces tokenizer issue. For example, new language may be OOV or be over tokenized. This requires updating the dictionary. The current approach do not support this."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8QlApOS7Sl", "forum": "hESsatbRee", "replyto": "hESsatbRee", "signatures": ["ICLR.cc/2026/Conference/Submission9371/Reviewer_7cj2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9371/Reviewer_7cj2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908552506, "cdate": 1761908552506, "tmdate": 1762920986860, "mdate": 1762920986860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Source-Shielded Updates (SSU), a source-driven selective-parameter update framework for adapting instruct-tuned LLMs to underrepresented languages using only unlabeled target-language text. SSU aims to mitigate catastrophic forgetting while retaining source-language instruction-following abilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear, well-structured, and easy to follow, making the paper accessible and logically organized.\n\n\n2. The column-wise masking design is an elegant and technically sound insight, offering a simple yet effective structural approach to preserve model representations.\n\n\n3. The source-driven importance scoring provides a principled and data-grounded alternative to random or target-data-based freezing strategies."}, "weaknesses": {"value": "1. The paper lacks a comprehensive hyperparameter search for baseline methods, which may make the reported comparisons less fair or less reproducible.\n\n\n2. The proposed method benefits from additional source-language data for parameter importance estimation, while baseline methods do not, introducing a potential source of unfair advantage.\n\n\n3. The novelty is limited, as similar importance-based freezing or selective update methods have been explored in prior works [1,2]. \n\n\n4. The paper does not include baselines that also leverage source data, which would provide a more balanced evaluation of the benefits of source-informed adaptation.\n\n\n5. The comparison omits recent state-of-the-art methods addressing catastrophic forgetting in multilingual CPT, making it difficult to precisely quantify how much SSU advances the field. \n\n[1] Jung, S., Ahn, H., Cha, S., & Moon, T. (2020). Continual Learning with Node-Importance based Adaptive Group Sparse Regularization.\n\n[2] Yao, K., Gao, P., Li, L., Zhao, Y., Wang, X., Wang, W., & Zhu, J. (2024). Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models."}, "questions": {"value": "Would it be possible to modify existing baselines or introduce new ones that also incorporate source data to ensure a more fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lkG2EHpaLg", "forum": "hESsatbRee", "replyto": "hESsatbRee", "signatures": ["ICLR.cc/2026/Conference/Submission9371/Reviewer_t7Au"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9371/Reviewer_t7Au"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985578716, "cdate": 1761985578716, "tmdate": 1762920986359, "mdate": 1762920986359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to mitigate catastrophic forgetting in LLM adaptation by identifying and freezing a subset of the parameters during adaptation. Empirically, the proposed approach has better performance than some existing adaptation methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Catastrophic forgetting is an important problem. The proposed approach is tested on models up to 13B and works well empirically. However the comparisons are limited to a small subset of relevant baselines and miss many important and relevant methods in the literature.\n\n- The paper is well written."}, "weaknesses": {"value": "- The baselines used in empirical comparisons are not comprehensive. The paper misses many key baselines, both in related work summary and in empirical comparisons, such as: [1, 2, 3]. It's important to see how the proposed method's performance compares with these relevant baselines. \n\n[1] [Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs](https://arxiv.org/abs/2406.16797)\n\n[2] [LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation](https://arxiv.org/html/2504.07448v1)\n\n[3] [S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity](https://arxiv.org/abs/2412.06289)"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R9j9TkC9FO", "forum": "hESsatbRee", "replyto": "hESsatbRee", "signatures": ["ICLR.cc/2026/Conference/Submission9371/Reviewer_gbcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9371/Reviewer_gbcc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160315415, "cdate": 1762160315415, "tmdate": 1762920986068, "mdate": 1762920986068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision Plan"}, "comment": {"value": "Dear Reviewers and Area Chair,\n\nThank you all for your constructive feedback. We have a clear plan to revise the paper based on your valuable comments. For your convenience, here is a summary of our revision plan:\n\nOur main actions are:\n\n1.  **Expanded Comparisons**: We are running new experiments to add two key baselines, LoTA and S2FT (Reviewers gbcc, 7cj2).\n2.  **Hyperparameter Analysis**: We are conducting a freezing-ratio analysis for the HFT and GMT baselines, similar to Figure 2 in the paper, to ensure a fair comparison (Reviewers t7Au, A7vK).\n3.  **New Analysis:** We will add an analysis showing SSU is not sensitive to the small calibration data size, addressing fairness concerns (Reviewer t7Au).\n4.  **Text Revisions:** We will revise the paper to: (i) enhance the “Related Work” section to better situate SSU against continual learning and other methods, (ii) refine our claims in L56-59, and (iii) clarify that tokenizer adaptation is out of scope but an important topic for future work (Reviewers 7cj2, t7Au).\n\nDue to computational constraints, these additional experiments are being conducted on the OLMo-2 7B / Igbo setting, consistent with our existing analysis. We are already running the new experiments and will incorporate all results, analyses, and textual revisions into the final version of the paper. We will post an update with the new results as soon as they are available within the author response period.\n\nThank you again for your guidance."}}, "id": "lszWkqhWbT", "forum": "hESsatbRee", "replyto": "hESsatbRee", "signatures": ["ICLR.cc/2026/Conference/Submission9371/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9371/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9371/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763100644510, "cdate": 1763100644510, "tmdate": 1763100644510, "mdate": 1763100644510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}