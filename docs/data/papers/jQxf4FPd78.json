{"id": "jQxf4FPd78", "number": 17096, "cdate": 1758272074869, "mdate": 1759897198551, "content": {"title": "Re$^{3}$: Retrieval, Reranking, and Reasoning for Agentic Knowledge Graph Question Answering", "abstract": "Recently, the integration of Large Language Models (LLMs) and knowledge graphs has emerged as a promising approach for knowledge graph question answering by enhancing the reasoning capability in knowledge-intensive applications. However, existing methods face a key trade-off: they either introduce high computational costs when LLMs reason directly on graphs, or suffer from poor reasoning quality due to over-reliance on retrieval methods. To mitigate these issues, we introduce a computationally efficient framework based on Retrieval, Reranking, and Reasoning (Re$^3$). Specifically, we first develop \"cognitively-informed retrieval\" that improves subgraph retrieval quality via Question-Entity (Q-E) discrepancy scoring and hierarchical information aggregation. Second, we propose path-aware reranking, which employs lightweight cross-encoders to evaluate and prune reasoning paths efficiently. Third, we apply \"agentic reasoning\" to perform autonomous reasoning on high-quality subgraphs while balancing reasoning quality and computational overhead. Extensive experimental results on WebQSP and CWQ demonstrate that Re$^3$ significantly outperforms existing methods.", "tldr": "Retrieval, Reranking, and Reasoning for Agentic Knowledge Graph Question Answering", "keywords": ["Knowledge Graph", "Question Answering", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/691c2ba4179da149d04227bcc02c0e7dcec46e2e.pdf", "supplementary_material": "/attachment/008fa5fb839d98d053a30337d3dee5cfc53160ea.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a knowledge graph question answering (KGQA) method based on LLMs. The approach works in three main steps. First, LLMs are used to generate multiple reasoning paths. Next, relevant information is retrieved from the knowledge graph according to these paths. A path-aware reranking model then evaluates and prunes the reasoning paths using a fine-tuned cross-encoder. Finally, LLMs perform reasoning on the retrieved subgraph as an agent, optionally using external tools to support the process. Experimental results demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance. The proposed first RAG then reasoning method is interesting, however, the novelty of the approach is limited."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an innovative combination of RAG-based methods with agent-based reasoning. In this approach, an agent performs reasoning on the retrieved subgraph, which is a somewhat interesting idea that leverages the strengths of both retrieval and agent reasoning.\n\nExperimental results demonstrate the effectiveness of the proposed method. It outperforms baseline methods in both Hit@k and F1 scores, indicating that the combination of retrieval and agent reasoning can lead to meaningful improvements in knowledge graph question answering."}, "weaknesses": {"value": "The experimental comparison seems somewhat unfair. In the proposed method, the baseline LLM is GPT-4o-mini, while other baseline methods, such as RoG, use Llama2-7B. Because of this difference, it is hard to tell whether the observed performance improvement is truly due to the proposed algorithm or simply because a more powerful LLM was used. This makes it difficult to fairly evaluate the contribution of the method.\n\nIn addition, the overall idea seems fairly incremental. The approach follows a common pipeline: first retrieve relevant information, then use an LLM as an agent to perform reasoning, with in-context learning applied to further boost performance. While the method works, the components and their combination are widely used in existing work, so the novelty of the approach is limited."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xDNZxK0JUR", "forum": "jQxf4FPd78", "replyto": "jQxf4FPd78", "signatures": ["ICLR.cc/2026/Conference/Submission17096/Reviewer_REw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17096/Reviewer_REw7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949257720, "cdate": 1761949257720, "tmdate": 1762927099449, "mdate": 1762927099449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Re^3 (Retrieval, Reranking, and Reasoning), a modular framework for knowledge graph question answering that integrates large language models with structured graph reasoning. It combines cognitively-informed retrieval, a fine-tuned reranking module, and agentic reasoning with tool use to balance accuracy and efficiency. The paper perform experiments on WebQSP and CWQ, though improvements over strong baselines are modest and inconsistent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a well-structured, modular design. Each stage (retrieval, reranking, reasoning) is clearly motivated and independently evaluable.\n2. The paper provides detailed prompts, pseudocode, and dataset construction provided in appendices for reproducibility."}, "weaknesses": {"value": "1. While the paper repeatedly claims that Re^3 “outperforms existing methods,” the reported results (Table 1) show mixed outcomes. On WebQSP, Re^3 matches or slightly improves Hit but underperforms in F1 compared with SubgraphRAG and GCR. On CWQ, **both Hit and F1 are notably lower than GCR**. These discrepancies raise questions about the strength of the claimed improvement and suggest that the benefits may not generalize across datasets or metrics. Interestingly, in the experiment results section, the author only indicates that the Re^3 can “attains 66.4% Hit and 57.5% F1” without comparing to these baseline results.\n2. Although the paper claims “extensive experimental results,” the evaluation is limited to only **two** Freebase-based benchmarks (WebQSP and CWQ), which are both well-studied and structurally similar. This narrow scope weakens the claim of generality and leaves open questions about how well the proposed framework performs on other knowledge graphs.\n3. The described aggregation equations (Eq. 2–3) closely resemble attention-based message-passing used in existing GNNs such as GAT or RotatE-enhanced KG encoders. While the cognitive framing is novel, the underlying mechanism appears technically conventional.\n4. The experiment was only performed once, without multi-run results or standard deviation. But the performance discrepancies shown in the Table and the experiment discussion is a bigger issue.\n\nPresentation weakness:\n1. The proposed hierarchical aggregation mechanism, described as a multi-layer information propagation framework, is a key technical component but is not illustrated in Figure 2 or visually explained. Readers cannot easily connect the text equations to the overall pipeline.\n2. Certain parts of the paper appear to contain unpolished text, such as the standalone line “Directionality and multi-relations:” in Section 3.1 (line #212), which reads like a leftover draft comment rather than a polished sentence. These minor issues detract from an otherwise well-written paper."}, "questions": {"value": "Please address the inconsistencies mentioned in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TuNypAvVlV", "forum": "jQxf4FPd78", "replyto": "jQxf4FPd78", "signatures": ["ICLR.cc/2026/Conference/Submission17096/Reviewer_ipAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17096/Reviewer_ipAY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961814392, "cdate": 1761961814392, "tmdate": 1762927099048, "mdate": 1762927099048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets KGQA with LLMs and points out that current LLM-enhanced methods fall into two imperfect extremes: (1) Agent-on-Graph methods that let the LLM explore the KG step by step but become too expensive because they need many powerful LLM calls, and (2) Retrieve-on-Graph methods that are cheaper but rely too much on retrieval and can miss key nodes/edges. Inspired by dual-process cognition (fast retrieval + slow reasoning) and information-theoretic tradeoffs, the authors propose Re³, a three-stage KGQA framework: (i) a fast, cognitively inspired retrieval to get a broad but noisy subgraph, (ii) a path-aware reranking step using a lightweight cross-encoder to prune and organize the retrieved paths into coherent reasoning chains, and (iii) an agentic reasoning stage where an LLM reasons over this high-quality subgraph and calls KG tools only when needed. On WebQSP and CWQ, Re³ is reported to outperform prior methods on complex multi-hop questions while keeping LLM calls under control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper doesn’t just say “retrieve then reason,” it ties the design to cognitive science (fast vs slow thinking) and information theory (coverage vs capacity), which gives a clearer rationale than many ad-hoc KGQA pipelines.\n\n2.Splitting into retrieval → path-aware rerank → agentic reasoning is practical: each stage can be swapped or scaled depending on resources (smaller reranker, cheaper LLM, different retriever).\n\n3.Empirical validation on multi-hop benchmarks show gains on WebQSP and CWQ—datasets where incomplete retrieval really hurts—supports the claim that better path selection actually helps LLM reasoning, not just retrieval metrics."}, "weaknesses": {"value": "1.the proposed method incorporates a more complicated pipeline for KGQA tasks, with three stages and additional models like the ranker. As a system, this work is good and also exhibits better performance. But it is hard to say whether the technical contributions of this paper are also insightful and useful for other domains. Actually, it is more promising to build the complex system for more complicated tasks like complex agent reasoning or real-world planning tasks. More comparison with other system-level designs in more complex tasks should be added in this paper.\n\n2.Important baselines are not discussed or compared in this paper, e.g., KG-Agent, Chain-of-query"}, "questions": {"value": "Please refer to the weakness part. I agree this is a good system paper, but the technical novelty and contribution are not very clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Please refer to the weakness part."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DGVjzNyrIA", "forum": "jQxf4FPd78", "replyto": "jQxf4FPd78", "signatures": ["ICLR.cc/2026/Conference/Submission17096/Reviewer_qZWG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17096/Reviewer_qZWG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135088473, "cdate": 1762135088473, "tmdate": 1762927098551, "mdate": 1762927098551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}