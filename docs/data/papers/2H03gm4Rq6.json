{"id": "2H03gm4Rq6", "number": 8420, "cdate": 1758082669438, "mdate": 1759897785041, "content": {"title": "Towards Self-Evolving Agent Benchmarks : Validatable Agent Trajectory via Test-Time Exploration", "abstract": "Recent advances in large language models (LLMs) and agent system designs have empowered agents with unprecedented levels of capability. However, existing agent benchmarks are showing a trend of rapid ceiling-hitting by newly developed agents, making it difficult to meet the demands for evaluating agent abilities. To address this problem, we propose the Trajectory-based Reproducible Agent-\nbenchmark Complexity Evolution (TRACE) framework. This framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording traceable agent trajectories. The framework proceeds in three stages: (1) evolutionary proposal mining, which provides task evolution proposals through preliminary exploration and divergent thinking; (2) problem formation and free exploration, where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories; and (3) multi-level validation, which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories. This work marks a paradigm shift from static, manually curated benchmarks to dynamic, self-evolving evaluation systems, providing a sustainable and challenging runway for agent development", "tldr": "", "keywords": ["Benchmark Evolution", "Agent Evaluation", "Test-Time Exploration", "Multi-Agent Systems", "Large Language Models", "Dynamic Task Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df04011892bda313906950eb0e6369e642288a97.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **TRACE**, an automated pipeline for generating new and harder tasks from existing benchmarks.  The system leverages a combination of LLM-based reasoning and validation to transform existing agent tasks into more complex variants, with the stated goal of alleviating benchmark saturation and stimulating progress in reasoning-heavy domains.  TRACE operates by (1) analyzing existing benchmarks such as GAIA, (2) proposing modifications that make the tasks more challenging, and (3) validating the modified tasks through an auxiliary solver before including them in the new dataset.  Empirically, the authors demonstrate that models perform worse on these generated tasks, suggesting that TRACE successfully creates more difficult evaluations.  The idea of using LLMs to automatically expand and stress-test benchmarks is promising and timely.\n\nThis is an exciting and well-motivated step toward automated benchmark evolution, but the methodology currently conflates task difficulty with model failure and lacks grounding in construct validity.   With better controls, clearer interpretability, and a modular release for community use, this work could become a genuinely valuable contribution to the field."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Originality and relevance.**  \n   The notion of an *automatic benchmark generator* is highly relevant. Existing benchmarks saturate quickly, and manually curating new tasks is costly. TRACE offers a compelling automation framework that can rejuvenate established benchmarks by introducing more challenging variants.  \n\n2. **Methodological soundness (conceptually).**  \n   The pipeline’s modular design—comprising generation, validation, and filtering—provides a flexible structure that could be extended to multiple domains. In principle, this could evolve into a general-purpose benchmark-expansion tool.  \n\n3. **Potential for community impact.**  \n   If released as an open-source package, TRACE could become a widely adopted toolkit. It could serve as a plug-and-play module attached to popular agent benchmarks (e.g., GAIA, SWE-Bench, Terminal-Bench, WebArena).  I encourage the authors to release TRACE as a standalone, modular Python package or API so that the community can easily apply it to existing datasets.  Since some teams (e.g., Terminal Bench) have been building adapters to existing agent benchmarks, this is probably not too much work.  Doing so would make TRACE a genuine contribution of infrastructure and reproducibility.\n\n4. **Empirical value.**  \n   Demonstrating that LLMs perform significantly worse on TRACE-generated tasks suggests that the system indeed increases difficulty and task complexity.  The paper shows that the generated tasks are longer, require more steps, and involve deeper reasoning than the originals.\n\n\n\nLastly, the command of English is excellent.  Good job."}, "weaknesses": {"value": "Despite its creativity and potential, the methodology has a few central problems.  Problems 1 is the most severe since it undermines all data reported in the paper.  Problem 2 should also be fixed.  I don't mind if problem 3 is left unsolved.\n\n---\n\n### 1. Selection bias — selecting on what you are reporting\n\nThe main methodological flaw is **selection bias** in the task-filtering procedure.   The pipeline uses an “auxiliary validator” (the specific model is unclear to me—possibly Qwen3-Coder-480B?) and retains only tasks that this validator cannot solve.   Subsequently, the paper reports that LLMs perform worse on these filtered “new tasks” than on the original dataset.  This comparison is inherently **unfair** because the filtering criterion (unsolved by validator) directly ensures lower model performance.  \n\nTo test whether the decreased performance is meaningful rather than tautological, I encourage the authors to perform a **control experiment**:  run the same filtering procedure on the original GAIA dataset, selecting only the tasks the validator cannot solve, and then compare model accuracy on this filtered subset.   I predict the same “performance drop” would appear even without TRACE-generated tasks, simply because of selection bias.\n\nBy the way, this problem mirrors the issue in *Humanity’s Last Exam (HLE)*: if one only includes tasks that all frontier models initially fail, one will trivially obtain a benchmark where all frontier models score poorly.  Such difficulty is not an intrinsic property of the tasks—it merely reflects transient weaknesses of current models.  As newer models emerge, the scores rise sharply, revealing that the selection mechanism was biased toward the temporary state of frontier capabilities.  This is what happened with HLE.  A few months later, when the newer models are released, there was an instead jump in scores.\n\nTo make the “decreased performance” claim meaningful, the authors should either:\n- provide an **independent definition of difficulty**, not one derived from model failure; or  \n- report **orthogonal measures of benchmark quality**, such as separability with confidence (see the Agent-Hard paper) or correlation with human difficulty ratings.\n\n---\n\n### 2. Construct validity — realism and meaningfulness of new tasks\n\nConstruct validity is the core problem here. See the literature on measurement in general.  Goes back to Cronbach, L. J., & Meehl, P. E. (1955).  While the new tasks may indeed require more steps and longer trajectories, it is unclear whether they measure anything meaningful or realistic.  In benchmark design, *construct validity*—the degree to which a test measures what it purports to measure—is paramount.  \nA benchmark should not only resist saturation but also preserve *semantic relevance* and *economic relevant*.\n\nEven thouhg SWE-Bench Verified has a lot of bugs, it has a clear external meaning: solving GitHub issues corresponds to resolving real software bugs.  If all models achieve $100%$ on GSM8K, we can still interpret that success as mastery of grade-school math reasoning.  If all models do well in telling dogs and cats apart, we can still understand the state of progress in Computer Vision.  But what exactly does it mean, conceptually or practically, for deepseek-v3 to achieve $24.7%$ on GAIA-TRACE versus $41.8%$ on GAIA?  If these scores cannot be linked to a real-world applicable capabilities, then the numbers risk becoming meaningless.  \n\nThis issue parallels the design philosophy behind overly artificial exams (such as the Chinese Gao Kao): one can always construct harder and more complicated problems, but hardness alone does not confer value.  TRACE currently optimizes only for *difficulty* and *saturation resistance*, rather than for *meaningfulness* or *construct validity*.  Without grounding in a real-world construct, the benchmark may end up measuring the ability to chain API calls or manipulate evidence density—interesting but not inherently valuable.  How should you explain to my grandma, an outsider to the field, what this 24.7% mean here, if she wants to make an investment decision in AI agents?\n\nTo address this, I recommend that the authors:\n- provide human evaluations of *task realism* and *relevance*; \n- discuss construct validity of the benchmark in light of the measurement-theory literature;\n- help readers understand why the new tasks are not just messier and more complicated to solve but also meaningful and realistic to expect an agent to solve.\n\nThis will encourage community adoption. \n\n---\n\n### 3. Missing comparisons and broader context\n\nThe paper would benefit from situating TRACE in relation to similar ideas.  For example, Jason Weston’s *Self-Challenge Agents* also propose generating new tasks automatically to train and evaluate agents.  How do TRACE’s generated tasks differ in structure, diversity, or usefulness?  Do these novel tasks help for training?  Benchmarks nowadays are not just used for measurements but also serve as RL training environments.  It'd be wonderful if TRACE can make the dataset of RL environments for agents larger, since we are all running out of high quality data nowadays.  Clarifying this comparison would strengthen the conceptual framing."}, "questions": {"value": "1. **Auxiliary validator model.**  \n   What exact model serves as the validator? How sensitive are the results to its capacity (e.g., Qwen3-32B vs Qwen3-480B)?  \n\n2. **Independent validation of difficulty.**  \n   Could human annotators or external scoring metrics (e.g., model confidence, entropy, or step length) be used to verify that TRACE-generated tasks are indeed harder for legitimate reasons?  \n\n3. **Construct validity assessment.**  \n   How do the authors define the “meaningfulness” of a task?  Have they considered measuring correlations between TRACE scores and other established benchmarks of reasoning or problem-solving ability?  \n\n4. **Integration and reproducibility.**  \n   Will the authors release TRACE as an open-source toolkit that can plug into existing agent benchmarks (e.g., via a simple adaptor API)?  This would significantly enhance its impact and adoption.  I also hope that we can see if the new tasks can be turned into an RL environment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mmcgaEgl7E", "forum": "2H03gm4Rq6", "replyto": "2H03gm4Rq6", "signatures": ["ICLR.cc/2026/Conference/Submission8420/Reviewer_s49g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8420/Reviewer_s49g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760907536505, "cdate": 1760907536505, "tmdate": 1762920317496, "mdate": 1762920317496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TRACE, a self-evolving benchmark framework generating harder tasks via 3 stages (proposal mining, exploration, validation) with validatable trajectories. It enhances complexity/diversity, outperforms static benchmarks on GAIA. Paradigm shift to dynamic, self-evolving benchmarks enabling sustainable, adaptive, and continual agent evaluation progress."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is carefully engineered and empirically validated, featuring a well-defined three-stage pipeline supported by detailed algorithmic design.\n\n2. The paper is clear, written making it easy to follow the motivation, methodology, and outcomes."}, "weaknesses": {"value": "1. While the paper presents an interesting multi-agent system for benchmark evolution, its distinct contribution relative to existing “agentic” methods is not sharply delineated. \n\n2. Experiments are mostly centered on GAIA, with limited exploration across other benchmarks or domains. \n\n3. The multi-agent pipeline (Proposer–Executor–Validator) introduces substantial systemic and computational overhead, which may limit practical deployment. The paper does not discuss resource consumption, time efficiency."}, "questions": {"value": "none."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n7XUpNbIFQ", "forum": "2H03gm4Rq6", "replyto": "2H03gm4Rq6", "signatures": ["ICLR.cc/2026/Conference/Submission8420/Reviewer_dJpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8420/Reviewer_dJpA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396894844, "cdate": 1761396894844, "tmdate": 1762920317097, "mdate": 1762920317097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for the self-evolution of benchmarks, aiming to address the issue of benchmark performance saturation. Through multi-agent collaboration, the method further extends the difficulty and diversity of questions in existing benchmarks.\n\nThe approach consists of three stages, each carried out by a different agent:\n* The first agent proposes potential directions for question evolution.\n* The second agent generates evolved questions according to the proposed directions.\n* The third agent verifies the feasibility and validity of the evolved questions.\n\nThis framework provides an automated mechanism for question expansion within benchmarks, and experimental results show that the evolved questions are significantly more challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This framework provides an automated mechanism for question expansion within benchmarks. The expansion process is fully automated, requiring no additional human annotation.  \n* Evaluation results show that the evolved tasks are significantly more challenging for models to complete, while the validation stage ensures that all evolved tasks remain solvable and valid."}, "weaknesses": {"value": "A benchmark is not designed merely to make models score lower; rather, its purpose is to ensure that the scores reflect meaningful aspects of model capability. Therefore, this work appears to lack some necessary analyses to demonstrate that the benchmark evolution preserves this interpretability of scores.\n\n* This work lacks a detailed statistical analysis of the evolved tasks — for example, in the style of GAIA, which provides statistics on the types of skills or capabilities required to complete each task.  \n\n* In addition, it does not provide a systematic analysis of the observed score degradation after benchmark expansion, which would help to clarify which aspects of task difficulty contribute most to the model's performance drop."}, "questions": {"value": "* Could you include quantitative analyses of model capabilities after benchmark evolution? For instance, comparing the distribution of capability dimensions between the original and evolved benchmarks, or analyzing differences in token lengths of correct responses, could help clarify what specific aspects of model performance have degraded. Such statistics would make it clearer what the observed performance drop actually indicates about model weaknesses.\n\n* Moreover, how to interpret changes in the ranking order of models across the original and evolved benchmarks? If the relative ranking of models shifts significantly, does it suggest that the evolved benchmark measures distinct or more nuanced abilities that were underrepresented before?\n\n* Finally, the authenticity and validity of the evolved questions should be further examined. It remains unclear how the framework ensures that increased difficulty does not lead to artificial or ill-posed questions that merely make tasks harder without providing meaningful evaluation signals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z3Q8k3JVYz", "forum": "2H03gm4Rq6", "replyto": "2H03gm4Rq6", "signatures": ["ICLR.cc/2026/Conference/Submission8420/Reviewer_aYop"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8420/Reviewer_aYop"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574818113, "cdate": 1761574818113, "tmdate": 1762920316673, "mdate": 1762920316673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores building self-evolving AI agents that can continuously improve their capabilities through experience accumulation. The proposed framework includes iterative cycles of experience collection, reflective learning, and capability updating. Agents learn not only from successes but also extract lessons from failures. Through meta-learning mechanisms, the system gradually optimizes decision strategies and behavioral patterns. Experiments demonstrate that self-evolved agents show sustained performance improvement on long-horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Long-term perspective**: Focuses on continuous agent evolution rather than single-task performance, aligning with real-world needs\n2. **Learning from failure**: Values not just successes but also reflection on failures, more closely resembling human learning\n3. **Well-designed experiments**: Long-term tracking studies observe capability improvements over sufficient evaluation periods\n4. **Good interpretability**: Experience repository and reflection logs enable tracing the agent's learning process\n5. **General framework**: Method isn't limited to specific task types, showing reasonable generalizability"}, "weaknesses": {"value": "1. **Catastrophic forgetting**: The paper insufficiently addresses this classic continual learning problem - how to ensure learning new capabilities doesn't erase old ones?\n2. **Evolution direction control**: Self-evolution may proceed in undesired directions, like over-optimizing certain metrics. How is evolutionary direction guided and constrained?\n3. **Convergence concerns**: No theoretical guarantee that evolution converges to better strategies - may get stuck in local optima. Experiments also show some performance fluctuations\n4. **Experience quality dependency**: Relies on automatically labeled experience quality, but labels may be inaccurate. Erroneous experiences can mislead subsequent learning\n5. **Computational resources**: Continuous self-optimization requires sustained computation, potentially infeasible in resource-constrained environments"}, "questions": {"value": "1. How do you distinguish valuable experiences from noise? Is there an experience filtering or quality assessment mechanism?\n2. When agents evolve undesirable behaviors, how can you rollback or correct them?\n3. In multi-task scenarios, how do you balance learning across different tasks? How to avoid specializing in one task while regressing on others?\n4. How can self-evolution integrate with human feedback? Can users guide evolutionary direction?\n5. What accounts for the performance fluctuations observed in long-term evolution experiments? How stable is the process?\n6. Do different agent types (rule-based vs. learning-based) differ in evolvability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vm74CXrx13", "forum": "2H03gm4Rq6", "replyto": "2H03gm4Rq6", "signatures": ["ICLR.cc/2026/Conference/Submission8420/Reviewer_H9KX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8420/Reviewer_H9KX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998380180, "cdate": 1761998380180, "tmdate": 1762920316353, "mdate": 1762920316353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}