{"id": "LnKLT5apxB", "number": 1706, "cdate": 1756909680655, "mdate": 1763112673519, "content": {"title": "SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation", "abstract": "Sign language generation faces the challenge of producing natural and expressive results due to the complexity of sign language, which involves hand gestures, facial expressions, and body movements. In this work, we propose a novel method called SignAligner for realistic sign language generation. The framework consists of three stages: text-driven multimodal co-generation, online collaborative correction, and realistic video synthesis. First, a joint generator incorporating a Transformer-based text encoder and cross-modal attention simultaneously produces posture, gesture, and body movements from text. Next, an online correction module refines the generated modalities using dynamic loss weighting and cross-modal attention to resolve spatiotemporal conflicts and enhance semantic consistency. Finally, the corrected poses are input into a pre-trained video generation network to synthesize high-fidelity sign language videos. Additionally, we introduce a dataset extension scheme that derives three new landmark representations (i.e., Pose, Hamer, and Smplerx) via pre-trained models, validated on PHOENIX14T and CSL-daily. Extensive experiments show that SignAligner significantly improves the accuracy and expressiveness of generated sign videos.", "tldr": "", "keywords": ["sign language generation", "sign language production"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/781074cc60c5f0bfc07072001d43876a7c9e8ed0.pdf", "supplementary_material": "/attachment/5e958560d5c64e071bb12927ffd2c6a5f55a7ea9.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SignAligner, a novel framework for sign language generation that integrates multiple pose modalities to improve the accuracy, expressiveness, and realism of generated sign language videos. The method is structured in three stages: text-driven pose modalities co-generation, online collaborative correction, and realistic video synthesis. The model utilizes a Transformer-based encoder and a cross-modal attention mechanism to generate sign language poses, hand shapes, and body movements, with a focus on ensuring temporal consistency and semantic alignment. Experimental results show that SignAligner outperforms existing methods like PTSLP and LVMCN across various metrics, demonstrating its effectiveness in improving both language accuracy and visual fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel approach: The approach of harmonizing multiple pose modalities (Pose, Hamer, and Smplerx) for sign language generation is innovative and addresses key challenges in producing coherent and natural sign language videos. \n\n2.\tValuable dataset extension: Enriches two benchmarks, PHOENIX14T and CSL-daily with high-fidelity modalities including pose, hamer and smplerx , filling gaps in existing SLG data which only include videos and basic skeletons before .\n\n3.\tComprehensive experiments: This paper provides comprehensive experiments and user study. The results show significant improvements in BLEU, ROUGE, SSIM, PSNR, and FID scores, validating the effectiveness of the proposed method."}, "weaknesses": {"value": "1.\tNo hyperparameter sensitivity analysis: Key parameters (OCC’s α/β/γ, Transformer hidden size/attention heads) lack impact analysis, harming reproducibility .\n\n2.\tInsufficient framework ablation: Fails to isolate contributions of single stages (e.g., co-gen + synthesis without OCC) to confirm three-stage necessity .\n\n3.\tRelated work: While the related work section provides a solid overview of previous methods,  it is recommended to conduct a more detailed comparison between the contributions of SignAligner and recent advancements (such as concerned multimodal models and cross-modal fusion techniques)."}, "questions": {"value": "Please refer to weaknesses ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wu6r7VCBNi", "forum": "LnKLT5apxB", "replyto": "LnKLT5apxB", "signatures": ["ICLR.cc/2026/Conference/Submission1706/Reviewer_PQAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1706/Reviewer_PQAu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476809966, "cdate": 1761476809966, "tmdate": 1762915862306, "mdate": 1762915862306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "h44Xdw67Hp", "forum": "LnKLT5apxB", "replyto": "LnKLT5apxB", "signatures": ["ICLR.cc/2026/Conference/Submission1706/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1706/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763112672750, "cdate": 1763112672750, "tmdate": 1763112672750, "mdate": 1763112672750, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for generating sign language videos from text.\nTo do so, they co-generate three modalities: pose, HaMeR, and SMPLer-X from text with a transformer-based model. \nThen, they align the generated poses and meshes with an online collaborative correction they introduce, and finally convert the generated modalities into photo-realistic videos using a RealisDance model finetuned over sign language datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors extend the PHOENIX-14T and CSL-daily datasets by extracting and providing DWPose poses and HaMeR, SMPLer-X meshes, which can contribute to future SLP work.\n2. Each modality alone is imperfect, hence their combination helps in achieving better results.\n3. The paper proposes a new alignment strategy between modalities, where they use a different modality for each of the queries, keys, and values."}, "weaknesses": {"value": "1. Novelty is limited. Most of the components were proposed in prior work, and the only new component is the collaborative correction, a cross-attention with different Q/K/V, which is neither explained, motivated, nor validated as better than other approaches.\n2. The paper has many typos and problematic citations, which make it hard to follow. See 1. below for examples.\n3. Many irrelevant details and not enough relevant details, see 2. below.\n4. Extraction quality discussion is unclear, see 4. below.\n5. No limitations discussion. For example, the abstract mentioned the importance of facial expressions in sign languages, however the facial expressions in the supplemented video do not match those of the GT, some hand shapes and touches are still incorrect, etc.\n6. Very few visual examples and comparisons with competing methods. Specifically, I would like to see comparisons with LVMCN, which has the closest metric scores compared to SignAligner."}, "questions": {"value": "1. The paper has many typos and problematic citations, for example:\n- Duplicates in citations as in Huang et al. Huang et al. (2021) (line 42), Saunders et al. Saunders et al. (2022) (line 51), etc.\n- Missing space (and preferably parentheses) to make citations clearer, e.g., “LVMCNWang et al.” (line 44)\n- G2P is mentioned in line 44 before explaining what it means \n- Line 228 - “positional coding” instead of positional encoding\n\n2. On the one hand, the paper presents too many irrelevant details that are not part of the newly proposed method. On the other hand, details that are relevant and new, such as those related to the alignment strategy with triple cross attention, where each of the Q/K/V comes from a different modality, are missing. What is the motivation for it? Have the authors tried different combinations, such as using the other 2 modalities as both keys and values, or using one at a time as both keys and values?\n\n3. Although LVMCN is mentioned and compared to in several tables, it is weirdly missing from Table 6, where it achieved higher results than signAligner based on the LVMCN paper, e.g. BLEU-4 9.36.\n\n4. Extraction quality discussion - “our extracted modalities consistently achieved high subjective scores exceeding 4.0, demonstrating their superior visual presentation and dynamic coherence” - superior over..? \nThe extraction analysis is long, mostly irrelevant, and unclear. If anything, figure 3 tells me each modality (or at least pose and SMPL) have different strengths that the other modalities don’t possess."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WaRYShi3K", "forum": "LnKLT5apxB", "replyto": "LnKLT5apxB", "signatures": ["ICLR.cc/2026/Conference/Submission1706/Reviewer_1qYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1706/Reviewer_1qYk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570253498, "cdate": 1761570253498, "tmdate": 1762915862131, "mdate": 1762915862131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SignAligner, a three-stage sign language video generation framework: text-driven multi-pose joint generation, online collaborative correction (OCC), and photorealistic video synthesis. The core idea is to generate and align three complementary representations (skeleton Pose with facial keypoints, fine-grained hand Hamer, and 3D full-body Smplerx) and then produce videos using a pretrained video generator. The authors also propose a dataset expansion scheme based on pretrained estimators to automatically add these three types of supervision to common corpora. Compared with baselines such as PTSLP, GEN-OBT, LVMCN, and fine-tuned CogVideoX, the method improves both semantic metrics (BLEU, ROUGE, WER) and visual metrics (SSIM, PSNR, FID) on PHOENIX14T and CSL-daily, and ablations show the effectiveness of joint generation and OCC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The motivation is clear: single-modality or multi-stage pipelines lead to semantic and spatiotemporal consistency issues, while joint modeling with online correction can mitigate them. The framework is well structured; combining three-modality joint generation with OCC is a reasonable technical path. Experiments cover two common datasets, report both semantic and visual metrics, and include ablations with stable and sizable gains. The dataset expansion scheme may provide reusable supervision for later work."}, "weaknesses": {"value": "(1) Lack of quantified error propagation and robustness: all three representations introduce errors during acquisition and generation. The paper does not provide systematic noise injection tests or small-scale human-calibrated comparisons, so it is unclear how errors are amplified through the pipeline or which representation is most sensitive.\n(2) Limited datasets and benchmarks: results are mainly on PHOENIX14T and CSL-daily; larger datasets with native keypoint/hand annotations such as How2Sign are not used for validation or external generalization.\n(3) Indirect comparison to strong baselines: the gap to vs GFSLT on SLT/SLR is not analyzed in depth."}, "questions": {"value": "Can the authors run robustness tests during training or inference by injecting controlled noise into Pose, Hamer, and Smplerx (e.g., Gaussian coordinate noise, temporal jitter, frame drop under occlusion) and report sensitivity curves for SSIM/FID and semantic metrics? This would directly address whether pseudo-label errors are amplified.\n\nCan the authors report results on How2Sign, zero-shot or few-shot generalization tests, and whether the method can generate new sign sentences/videos?\n\nRegarding the gap to GFSLT: can the method effectively improve existing GFSLT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BDGCXHb8RO", "forum": "LnKLT5apxB", "replyto": "LnKLT5apxB", "signatures": ["ICLR.cc/2026/Conference/Submission1706/Reviewer_h4Zv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1706/Reviewer_h4Zv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758281337, "cdate": 1761758281337, "tmdate": 1762915861946, "mdate": 1762915861946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SignAligner, a novel three-stage framework for realistic sign language generation. It is designed to solve the problem of \"modal fragmentation,\" where processing hand gestures, facial expressions, and body movements separately can lead to poor semantic fidelity and a lack of spatiotemporal continuity. A key contribution is a dataset expansion scheme that augments the PHOENIX14T and CSL-daily datasets with three new, high-quality landmark representations derived from pre-trained models: Pose (high-precision skeleton), Hamer (detailed 3D hand shape), and Smplerx (3D full-body posture). The SignAligner framework first uses a Transformer-based model for text-driven co-generation, simultaneously producing all three pose modalities. Next, an Online Collaborative Correction (OCC) module refines these modalities using cross-modal attention and dynamic loss weighting to resolve spatiotemporal conflicts. Finally, the corrected poses are fed into a pre-trained video synthesis network to generate high-fidelity sign language videos."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. SignAligner significantly outperforms existing state-of-the-art approaches on both the PHOENIX14T and CSL-daily datasets. On the PHOENIX14T test set, it achieves superior scores in semantic accuracy (e.g., 20.56 BLEU-1, 8.17 BLEU-4) and visual quality (e.g., 0.731 SSIM, 26.257 FID). \n2. The paper construct a dataset with three modalities, whose quality is validated by a robust user study involving 100 volunteers, which found SignAligner's videos to be markedly better in naturalness, temporal consistency, and gesture transitions, including a 23% improvement in visual clarity over competitors. \n3. The paper's claims are well-supported by detailed ablation studies, which confirm the essential contribution of both the co-generation and the OCC modules; removing either component leads to a significant drop in performance, validating their synergistic effect."}, "weaknesses": {"value": "1. Paper details need clarification. For example, the sentences from line 168 to 173 are hard to understand. Variables such as n should be in math form in latex. In line 266, the verb should be \"contrain\".\n2. The proposed method lacks novelty. The dataset is just contructed by leveraging existing techniques to extract pose, Hamer,and Smplerx for two sign language datasets. The proposed method leverages the extracted three modalities with simple feature reconstruction and cross-atttention-based feature interaction, which lacks novelty from a whole view.\n3. While this paper adopt three modalities for sign language production, it's not fair to directly compare it with prior works which just use one modality. As shown in Tab.8, lacking any modality leads to severe performance drop for the proposed method.\n4. While using three modalities, the proposed method lacks analysis for model efficiency."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QRFiVEEqet", "forum": "LnKLT5apxB", "replyto": "LnKLT5apxB", "signatures": ["ICLR.cc/2026/Conference/Submission1706/Reviewer_P6S9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1706/Reviewer_P6S9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919213263, "cdate": 1761919213263, "tmdate": 1762915861835, "mdate": 1762915861835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}