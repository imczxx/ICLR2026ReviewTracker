{"id": "XAnQOoFfBw", "number": 8126, "cdate": 1758066697177, "mdate": 1759897805389, "content": {"title": "EVALUATING LLM SAFETY ACROSS CHILD DEVELOPMENT STAGES: A SIMULATED AGENT APPROACH", "abstract": "Large Language Models (LLMs) are rapidly becoming part of tools used by children; however, existing benchmarks fail to capture how these models manage language, reasoning, and safety needs\nthat are specific to various ages. We present ChildSafe, a benchmark that evaluates LLM safety\nthrough simulated child agents that embody four developmental stages. These agents, grounded in\ndevelopmental psychology, enable a systematic study of child safety without the ethical implications\nof involving real children. ChildSafe assesses responses across nine safety dimensions (including\nprivacy, misinformation, and emotional support) using age-weighted scoring in both sensitive and\nneutral contexts. Multi-turn experiments with multiple LLMs uncover consistent vulnerabilities that\nvary by simulated age, exposing shortcomings in existing alignment practices. By releasing agent\ntemplates, evaluation protocols, and an experimental corpus, we provide a reproducible framework\nfor age-aware safety research. We encourage the community to expand this work with real childcentered data and studies, advancing the development of LLMs that are genuinely safe and developmentally aligned.", "tldr": "", "keywords": ["LLM Safety", "Child Development", "Simulated Agents", "AI alignment", "Child Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9033aaf70cf184fbc3da802a857fa4a24bc84e5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ChildSafe, a novel benchmark designed to evaluate LLM safety using simulated child agents across four developmental stages. The authors leverage instruction-tuned models for a synthetic data generation pipeline (i.e., for generating multi-turn conversations), which is followed by linguistic and behavioral analysis of the generated data. Finally, the authors propose a 9-dimensional safety evaluation framework and a composite safety score base on different weightings of the 9 safety dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Important and Under-Investigated Topic:** Contextual and age-specific safety is an important, under-investigated topic\n- **Principled and Grounded Dataset Generation Pipeline:** The data generation pipeline appears to be grounded in cognitive psychology (influencing the prompt template) and leveraging multiple control variables. **Caveat:** No further details on the exact pipeline implementation (we need to trust the high-level description, no system prompts or examples are given.\n- **Extensive Verification of Authenticity**: A major strength is the explicit and effort to verify the authenticity of the generated child interactions. Many evaluation studies using synthetic data omit this step. By employing both automated linguistic checks (against CHILDES) and human expert validation (Section 3.2), the authors build a strong case for the validity of their simulated agents, which is crucial for the paper's central claims. **Caveat:** However, no further details or results are given on this point."}, "weaknesses": {"value": "- **Ambiguity of the Composite Score:** I am still unsure how the composite score needs to be interpreted. How do different weighting affect the score and influence the model rankings? In addition, some standard deviations seem to be quite high in Table 1. The composite scores might be quite sensitive to different weightings -- could you provide some ablation on this point (e.g. contrast it with a uniform weighting)?\n- **Omitting Many Important Details:** Many important deadline on the data construction and verification are missing\n- **Age-Specific Weighting are Not Provided:** Please provide a full table of the selected age-dependent weightings\n- **Missing Confidence Intervalls:** Could you please add confidence intervals to Figure 2 and 3?\n- **Example Conversations & Failure Cases:** Could you please provide some example conversations and model behaviors. Right now, one needs to fully trust the numbers given in the paper and cannot verify the quality of the conversations."}, "questions": {"value": "- Could the authors please provide the full weighting matrix (9 dimensions x 4 age groups) used to calculate the composite scores?\n- Why were confidence intervals or error bars omitted from Figures 2 and 3? Given the high standard deviations in Table 1, visualizing this uncertainty is essential for interpreting the results.\n- Given the high variance in some dimensions, how sensitive is the final model ranking to small changes in the (undisclosed) dimensional weights?\n- Could the authors release the system prompts used to create the developmental agents and the detailed rubrics given to the human expert validators, as well as the results, to allow for true reproducibility?\n\nI am happy to increase my score if the authors can address my concerns and questions. In particular, if the authors could make the paper more \"complete\" -- so far it would be an educated guess as much relevant points are omitted in the paper that are needed to make a fair assessment of the quality of the work."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "Apologies -- I marked the ethics review as many details on the data construction & data labelling were omitted. But not completely sure, if this should really trigger an ethics review."}}, "id": "m52fLrdEeH", "forum": "XAnQOoFfBw", "replyto": "XAnQOoFfBw", "signatures": ["ICLR.cc/2026/Conference/Submission8126/Reviewer_5Gbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8126/Reviewer_5Gbo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613808399, "cdate": 1761613808399, "tmdate": 1762920102573, "mdate": 1762920102573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ChildSafe, a benchmark that simulates child agents across four age stages to evaluate LLM safety. Testing nine safety dimensions on four major models, it finds GPT-5 safest overall, but all models perform worse with younger children and struggle with boundary respect and long-term impact. The work highlights the need for age-adaptive safety evaluation and real-world validation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Study a safety issue that has not been studied before"}, "weaknesses": {"value": "Concerns\n1. The writing requires substantial improvement. The current draft lacks a clear structure, and the experiment results and dataset construction details are interleaved, making the paper difficult to follow.\n2. The dataset construction process is insufficiently described. The paper should include a clear and logical explanation of how the datasets are built.\n3. What does the dataset look like? Showing a few representative samples or conversations would make the benchmark more understandable.\n4. The evaluation procedure is unclear. The authors should specify how the scores for each evaluation dimension are computed.\n\nOverall, the paper omits several key details expected from a benchmark paper, which makes it difficult for readers to interpret or reproduce the results."}, "questions": {"value": "Llama 3.1-8B-Instruct does not possess a pretty strong ability to function as an agent. Why don't authors use more powerful models here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FRQIJGs6c7", "forum": "XAnQOoFfBw", "replyto": "XAnQOoFfBw", "signatures": ["ICLR.cc/2026/Conference/Submission8126/Reviewer_kVna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8126/Reviewer_kVna"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709347354, "cdate": 1761709347354, "tmdate": 1762920102110, "mdate": 1762920102110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ChildSafe, a new benchmark for evaluating the child safety of large-scale language models (LLMs). The authors point out that existing safety benchmarks (e.g., HarmBench, JailbreakBench) are adult-centric and fail to account for the unique developmental characteristics of children. Specifically, children have limited critical thinking skills, high trust in authority figures, and differing language comprehension and emotional regulation abilities across developmental stages, making it difficult to directly apply adult assessment criteria. To address these issues, the paper presents an assessment methodology utilizing simulated child agents. Based on Piaget's theory of cognitive development and Vygotsky's theory of the zone of proximal development, agents representing four age groups (6-8, 9-11, 12-14, and 15-17) were implemented using the Llama 3.1-8B-Instruct model. Each agent is designed to mimic the cognitive characteristics, language patterns, social awareness, and emotional expression of the corresponding age group. The evaluation framework consists of nine safety dimensions (Content Appropriateness, Boundary Respect, Educational Impact, Social Influence, Emotional Safety, Privacy Protection, Manipulation Resistance, Developmental Sensitivity, and Long-term Impact), each with different weightings for different age groups. The scenarios are categorized into five categories: Educational Support, Social Interaction, Entertainment and Creativity, Information Exploration, and Boundary Testing, each of which is a multi-turn conversation (five turns). As a result, GPT-5 achieved the highest overall safety score of 0.777, while all models performed worst in interactions with younger elementary school students (ages 6-8), demonstrating overall weaknesses in the Boundary Respect dimension."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper's greatest contribution lies in its systematic integration of developmental psychology theory into the LLM safety assessment. Implementing Piaget and Vygotsky's theoretical framework into a practical assessment system is a prime example of interdisciplinary research. The agent design, which explicitly reflects age-specific differences in concrete vs. formal operational thinking, vocabulary levels (5,000-20,000+), and developmental stages of social cognition, demonstrates a level of sophistication that goes beyond the simple \"child-friendly AI\" discussion.\n\n2. The nine-dimensional safety assessment framework is comprehensive. While existing benchmarks primarily focus on detecting explicit harmful content, this paper incorporates nuanced yet important dimensions such as educational impact, long-term impact, and developmental sensitivity. In particular, dimensions such as \"Boundary Respect\" and \"Long-term Impact\" capture the unique characteristics of child-AI interactions.\n\n3. This study has ethical value in attempting to systematically assess safety without exposing actual children to risk. Research involving children requires complex ethical procedures, including IRB approval, parental consent, and child assent. Studies evaluating potentially unsafe interactions with LLMs are particularly ethically sensitive. A simulation approach circumvents these ethical dilemmas while enabling large-scale evaluations."}, "weaknesses": {"value": "1. A major concern with the paper is the lack of fundamental validation of the child agent simulation's validity. While the paper claims that \"the simulation agent closely mimics real children,\" there is no \"data of actual child-LLM conversations\" to validate this. While linguistic comparisons with the CHILDES database, expert evaluations (by two developmental psychologists and child philosophers), and consistency testing were conducted, the CHILDES database is a collection of typical, everyday conversations between children and their parents. While it captures the linguistic patterns of natural conversation, it does not capture the specific behavioral patterns that children might exhibit when interacting with LLMs. \n\n2. The paper claims to assess each safety dimension using \"keyword libraries, linguistic patterns, and contextual indicators,\" but the specifics of these are not disclosed. For example, it is unclear how \"Emotional Safety\" is assessed, or which keywords and patterns are considered risk signals. The age-based weighting is only stated as \"based on child development literature,\" and no specific justification is provided for why Emotional Safety is 0.20 and Privacy Protection is 0.15 in lower elementary school grades. The system prompts for generating the agent are not included in the paper or appendix.\n\n3. The limitations of the experimental design are also problematic. 300 conversations may not be sufficient to adequately cover four models, four age groups, and five scenarios. It is questionable whether statistically reliable conclusions can be drawn with only three or four conversations per condition.\n\n4. This paper is an inappropriate venue for submission to ICLR. ICLR is a conference that covers machine learning algorithms, optimization techniques, novel architectures, and representation learning theory. However, this paper does not propose a new ML methodology, nor does it propose a benchmark. It does not specify the evaluation prompts and protocol used. Instead, it applies prompt engineering to an existing LLM (Llama 3.1) to create an agent and proposes a benchmark for evaluating other LLMs. The core contribution of this paper lies in designing an evaluation framework based on developmental psychology theory, which is closer to AI ethics or social science contributions than to machine learning."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XY5rKsJQtU", "forum": "XAnQOoFfBw", "replyto": "XAnQOoFfBw", "signatures": ["ICLR.cc/2026/Conference/Submission8126/Reviewer_NeDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8126/Reviewer_NeDw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716519120, "cdate": 1761716519120, "tmdate": 1762920101628, "mdate": 1762920101628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to evaluate how well dominant large language models (LLMs) ensure safety when interacting with children of different ages. To achieve this, the authors use LLMs to simulate child users at various developmental stages (ages 6–17) and engage them in age-appropriate multi-turn conversations with the target models. The resulting benchmark, ChildSafe, measures model performance across nine safety dimensions and shows that most LLMs exhibit significant safety degradation with younger simulated children, particularly in emotional and privacy-related interactions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a socially important and under-explored research question, assessing how safe current LLMs are when interacting with children of different ages.\n\n2. The authors build a novel dataset and benchmark (ChildSafe) that could serve as a useful community resource for studying age-dependent LLM safety.\n\n3. The study provides systematic cross-model evaluation results, revealing consistent safety degradation for younger simulated users, which highlights an important real-world vulnerability."}, "weaknesses": {"value": "1. Lack of technical innovation.\nThe paper’s contribution lies mainly in problem framing and dataset construction rather than any methodological or algorithmic advance. For example, the authors reference developmental psychology to design age-specific personas, but this aspect is humanistic and does not introduce any new modeling techniques or technical depth. The proposed multi-turn dialogue evaluation setup is largely a direct application of existing “LLM-as-simulator” and “LLM-as-evaluator” paradigms, with minimal adaptation or new mechanisms introduced. \n\n2. Limited interpretability.\nThe evaluation results are mainly aggregate scores without qualitative case analysis or human verification. It remains unclear whether the measured “safety degradation” corresponds to genuine unsafe behaviors or simply differences in lexical or stylistic matching across age groups. \n\n3. Weak validation.\nAlthough the benchmark is presented as standardized, there is no validation of its reliability or calibration. All modules depend on LLM outputs without human cross-verification, making it uncertain whether the benchmark meaningfully measures child safety."}, "questions": {"value": "1. How are the nine safety dimensions operationalized and scored in practice?\nThe paper lists dimensions such as privacy, emotional safety, and manipulation resistance, but it remains unclear what exact prompts, heuristics, or rubrics are used for scoring. Are these criteria manually designed or entirely LLM-judged? Without clear implementation details, it is difficult to reproduce or interpret the reported results.\n\n2. How is the child-agent simulation controlled or verified for age fidelity?\nSince all “child” dialogues are LLM-generated, how do the authors ensure that linguistic complexity, emotional tone, and reasoning style actually reflect the intended developmental stages? Is there any quantitative measurement (e.g., readability index, sentence complexity) or expert validation confirming the fidelity of the simulated children?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W9tRPmi1QE", "forum": "XAnQOoFfBw", "replyto": "XAnQOoFfBw", "signatures": ["ICLR.cc/2026/Conference/Submission8126/Reviewer_m86o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8126/Reviewer_m86o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902086299, "cdate": 1761902086299, "tmdate": 1762920101113, "mdate": 1762920101113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}