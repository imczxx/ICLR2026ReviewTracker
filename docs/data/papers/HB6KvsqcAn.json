{"id": "HB6KvsqcAn", "number": 8941, "cdate": 1758103279186, "mdate": 1759897752614, "content": {"title": "Towards Physically Executable 3D Gaussian for Embodied Navigation", "abstract": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose **SAGE-3D** (**S**emantically and Physically **A**ligned **G**aussian **E**nvironments for **3D** Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: **(1) Object-Centric Semantic Grounding**, which adds object-level fine-grained annotations to 3DGS; and **(2) Physics-Aware Execution Jointing**, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release **InteriorGS**, containing 1K object-annotated 3DGS indoor scene data, and introduce **SAGE-Bench**, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task.", "tldr": "", "keywords": ["3D Gaussian Splatting; Vision-and-Language Navigation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83b27e5c0b213e310a6b1ad8424d13760b2c25e9.pdf", "supplementary_material": "/attachment/f185e61c14883b4bd8ee7da66d0425cabb338361.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new paradigm named SAGE-3D (i.e., Semantically and Physically Aligned Gaussian Environments for 3D Navigation) that upgrades 3DGS into an executable, semantically and physically aligned environment. It contains two components, including Object-Centric Semantic Grounding and Physics-Aware Execution Jointing. It achieves faster scene rendering and better generalizability than scanned mesh data. Besides, this paper releases InteriorGS dataset containing 1000 manually object-annotated 3DGS scenes, and constructs SAGE-Bench including 2M new trajectory-instruction pairs and 554k detailed collision bodies. Extensive experiments demonstrate the effectiveness of the proposed paradigm."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n1) The idea of SAGE-3D that upgrades 3DGS from a purely perceptual scene representation to an executable, semantically and physically aligned environment foundation is novel and valuable.\n2) The constructed InteriorGS containing 1000 manually object-annotated 3DGS scenes, is beneficial for the field developement. The SAGE-Bench, the first fully 3DGS-based VLN benchmark with 2M new trajectory-instruction pairs and 554K detailed collision bodies, is beneficial for the downstream applications.\n3) Extensive experiments are conducted on the proposed paradigm and validate the superiority of the newly introduced data."}, "weaknesses": {"value": "Weaknesses:\n1) In the second paragraph of Introduction section, the authors claims 3DGS offers three key advantages, but only two items are listed as advantages.\n2) The organization of the scene data is based on GSplat, which may introduce more gaussian parameters than the mesh representation. Besides, the physics simulation introduces 3DGS-Mesh Hybrid representation, the collision bodies is computed by manually-created triangle mesh, does it introduce extra memory cost and rendering complexibility?\n3) The data generation of SAGE-Bench is developed based on the IneriorGS, but is similar with the construction of existing VLN datasets. What is the core contribution of SAGE-Bench? It is not clear.\n4) The definition of Nogoal-Nav is similar with visual exploration tasks, is there any difference? If there is no difference, there is no need to rename an existing task, visual exploration is more suitable."}, "questions": {"value": "Please try to address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FEPuSvpeLj", "forum": "HB6KvsqcAn", "replyto": "HB6KvsqcAn", "signatures": ["ICLR.cc/2026/Conference/Submission8941/Reviewer_zosw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8941/Reviewer_zosw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965654042, "cdate": 1760965654042, "tmdate": 1762920684185, "mdate": 1762920684185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new dataset and benchmark for visual-language navigation. The paper endows 3D Gaussian splatting with semantic meanings and physical properties to construct a comprehensive 3D environment for VLN. However, experiments do not show a clear improvement over the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of integrating semantic and physical information into 3D Gaussian splatting is reasonable.\n2. The proposed dataset is large in scale and distinguish itself from others with diverse instructions and accurate geometry.\n3. The proposed evaluation metrics align well with the real application scenarios."}, "weaknesses": {"value": "1. The performance on SAGE-Bench is worse than baselines in terms of CR, ICP and PS."}, "questions": {"value": "1. Why is the performance on SAGE-Bench worse than baselines in terms of CR, ICP and PS?\n2. How does the additional semantic and physical information of 3D GS benefit the performance? The paper whould conduct ablation study on that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vO23VM8QRE", "forum": "HB6KvsqcAn", "replyto": "HB6KvsqcAn", "signatures": ["ICLR.cc/2026/Conference/Submission8941/Reviewer_mZXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8941/Reviewer_mZXf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761397575668, "cdate": 1761397575668, "tmdate": 1762920683768, "mdate": 1762920683768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAGE-3D, a novel paradigm that extends 3D Gaussian Splatting (3DGS) from a purely photorealistic rendering technique into a semantically and physically executable environment for embodied navigation. The authors propose two key modules—Object-Level Semantic Grounding, which augments 3DGS with dense, object-centric annotations, and Physics-Aware Execution Jointing, which integrates collision bodies to enable realistic physics simulation. Building on these components, the work releases InteriorGS, a large-scale dataset of 1,000 richly annotated indoor 3DGS scenes, and SAGE-Bench, the first 3DGS-based benchmark for vision-language navigation (VLN) with 2M instruction–trajectory pairs and new metrics for continuous navigation evaluation (Continuous Success Ratio, Integrated Collision Penalty, Path Smoothness). Experiments demonstrate that 3DGS environments render faster yet challenge model convergence, and that training on SAGE-Bench significantly improves generalization to unseen VLN settings. Overall, this work contributes an integrated data–simulation–benchmark framework that advances the use of 3DGS for physically grounded embodied AI."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: This work pioneers the first-of-its-kind benchmark built on the 3D Gaussian Splatting (3DGS) representation for vision-language navigation (VLN). By repurposing and re-contextualizing 3DGS—traditionally a rendering/novel-view synthesis technique—into the embodied navigation domain, the authors introduce a novel problem formulation and open a promising new research direction.\n\nQuality: The paper clearly presents its ideas with structured exposition and strong narrative flow. The methodology is articulated in accessible language, the dataset/benchmark design is sufficiently detailed, and the experiments convincingly demonstrate value. The clarity of presentation makes the core contributions easy to follow and the evaluation pipeline reproducible.\n\nClarity: The authors succeed in explaining the motivation, technical approach, dataset construction, task definitions and evaluation metrics in a straightforward manner. Complex concepts (e.g., converting 3DGS scenes into a navigation-capable environment) are broken down cleanly and the presentation avoids unnecessary jargon. As a result, the reader is able to readily understand both “what was done” and “why it matters”.\n\nSignificance: The proposed dataset and benchmark’s potential impact on the embodied AI community is substantial. By enabling faster rendered observations and high-quality 3DGS-based environments, this work could significantly accelerate research in navigation, simulation, and multimodal grounding. The resource thus has the capacity to become a standard tool or reference point for related tasks, facilitating broader progress in the field."}, "weaknesses": {"value": "Limited reproducibility and generalisation of the paradigm: While the paper presents an interesting direction of adapting 3D Gaussian Splatting (3DGS) for embodied navigation, much of the work depends on manual annotation of objects and artist-created meshes/scene enrichment. The reliance on handcrafted assets makes it difficult for other researchers to easily replicate or scale the setup, and raises questions about how well the approach will generalise to entirely new scenes or domains without heavy annotation effort. The authors should consider describing a less labour-intensive pipeline for scene creation (e.g., automated annotation, mesh generation, domain adaptation) and discuss how one might extend their benchmarking assets beyond the current curated set.\n\nInsufficient discussion of experimental anomalies: In Table 2 the results show that several metrics for the “*-SAGE” variants (i.e., trained on the new 3DGS-based benchmark) are actually worse than baseline in some important metric(CR,ICP,PS)— yet the manuscript offers minimal explanation for these. Given these surprising outcomes, a deeper analysis is required. Without this discussion, the reader is left uncertain about the limitations and trade-offs of using the proposed dataset/benchmark."}, "questions": {"value": "1. In the paper you mention that the high-level instructions are generated by feeding an MLLM (multilingual/multimodal large language model). Did you evaluate or validate the resulting instructions (e.g., accuracy, relevance, diversity)? Consider including a small user study or error analysis showing how instruction quality affects downstream navigation performance.\n2. For the component titled “2D Semantic Top-Down Map Generation”, will this design choice limit your dataset to planar/2.5D navigation tasks (e.g., ground-level walking) and restrict its applicability to fully 6DoF navigation scenarios (such as flying drones or free-roam agents)?\n3. You define the metric CSR (Continuous Success Ratio) as the proportion of time the agent stays within a permissible corridor around the reference path while satisfying task conditions. However, in many navigation tasks there may be multiple valid paths to the goal. In that case, is the “reference path” prior too restrictive — and is the metric truly fair? Could you discuss how you handle alternative optimal paths, whether you allow multiple corridors, or how you adjust for path diversity? Consider providing sensitivity analysis of CSR with respect to path choices.\n4. In Tables 2, 4, 5, and 6 many of the “best” results are not highlighted or visually distinguished. For improved readability and clearer takeaway for the reader, would you consider bolding or colouring the top‐performing numbers, adding a “best” row/column summary, or clarifying via annotation? This is a minor editorial issue, but it strongly affects accessibility of the results.\n5. You mention that models trained on your dataset are “harder to converge”, which you suggest might indicate higher task difficulty or richer learning. However — does harder to converge necessarily correlate with better training quality or better generalisation? It might also be a sign of optimization instability, noisy labels, or an ill-posed task. I suggest adding a deeper analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a7c62xrJUp", "forum": "HB6KvsqcAn", "replyto": "HB6KvsqcAn", "signatures": ["ICLR.cc/2026/Conference/Submission8941/Reviewer_Wreg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8941/Reviewer_Wreg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725657363, "cdate": 1761725657363, "tmdate": 1762920683468, "mdate": 1762920683468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SAGE-3D**, a framework that enhances 3D Gaussian Splatting (3DGS) with object-level semantics and physical interfaces to create an executable environment for embodied navigation. The paper presents **InteriorGS**, a dataset of 1K annotated 3DGS indoor scenes, and **SAGE-Bench**, the first 3DGS-based Vision-Language Navigation benchmark. Based on the semantically and physically aligned 3DGS, the framework enables photorealistic rendering, semantic labeling, and physical collision modeling. Experiments show its effectiveness in improving model generalization and advancing embodied AI research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### **Strengths**\n1. The paper is well-organized and easy to understand, with clear explanations and intuitive figures illustrating the methodology.  \n2. The idea makes sense. 3DGS with object-level semantics and physical validity enables photorealistic rendering, semantic instance labeling, and physical interaction modeling. Based on this representation, a realistic and executable simulation environment can be created for embodied AI research, which is highly important. \n3. Experiments demonstrate that the dataset improves the generalization of current VLN models and, as a benchmark, can support future research in embodied navigation models."}, "weaknesses": {"value": "### **Weaknesses and Questions**\n1. Creating the dataset appears to rely on detailed mesh scenes and extensive manual annotations, which may incur high costs and limit further scalability. It would be worth exploring more cost-efficient approaches for dataset construction, such as leveraging current vision-based semantic/geometry foundation models for scene reconstruction and semantic annotation.  \n2. The dataset seems to consist solely of static scenes. Introducing dynamic objects into the scenes could enhance its applicability and better simulate real-world scenarios for embodied AI research."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uaImfTF8ce", "forum": "HB6KvsqcAn", "replyto": "HB6KvsqcAn", "signatures": ["ICLR.cc/2026/Conference/Submission8941/Reviewer_hytT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8941/Reviewer_hytT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839444534, "cdate": 1761839444534, "tmdate": 1762920683039, "mdate": 1762920683039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}