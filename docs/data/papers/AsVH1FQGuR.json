{"id": "AsVH1FQGuR", "number": 17132, "cdate": 1758272551090, "mdate": 1759897194724, "content": {"title": "Augmentations in Offline Reinforcement Learning for Active Positioning", "abstract": "We propose a method for data augmentation in offline reinforcement learning applied to active positioning problems. \nThe approach enables the training of off-policy models from a limited number of trajectories generated by a suboptimal logging policy.  \nOur method is a trajectory-based augmentation technique that exploits task structure and quantify the effect of admissible perturbations on the data using the geometric\ninterplay of properties of the reward, the value function, and the logging policy.\nMoreover, we show that by training an off-policy model with our augmentation while collecting data, the suboptimal logging policy can be supported \nduring collection, leading to higher data quality and improved offline reinforcement learning performance.\nWe provide theoretical justification for these strategies and validate them empirically across positioning tasks of varying dimensionality and under partial observability.", "tldr": "We introduce a trajectory-based data augmentation method that improves offline reinforcement learning in active positioning tasks by leveraging task structure and geometric properties of rewards, values, and logging policies.", "keywords": ["Offline Reinforcement Learning", "Reinforcement Learning", "Active Position", "Off-Policy Learning", "Value Function Geometry"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a443b395f953e6218693509a0d5a3dc009c2db8d.pdf", "supplementary_material": "/attachment/bc599ad161c10f4d3c9c0b2faba2a5e8b9f311e9.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes LIFT, a data augmentation method for offline reinforcement learning specifically designed for active positioning problems. The authors provide theoretical justification for LIFT and demonstrate empirically that they improve data quality and the performance of the final offline RL policy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Practical impact on active positioning\n\nLIFT makes offline RL practically useful for active positioning, delivering consistent improvements over baselines.\n\n\n2. Theoretical Support\n\nRather than relying solely on empirical results, the paper mathematically establishes the effectiveness of the LIFT framework."}, "weaknesses": {"value": "1. Missing related works of data augmentation for offline RL\n\nA significant limitation of this paper's empirical evaluation is the omission of relevant baselines for data augmentation in offline RL. The community has extensively explored this area, with established lines of work including noise injection techniques [1, 2] and, more recently, generative models [3, 4, 5]. Without comparisons against these crucial and relevant methods, it is difficult to assess the relative effectiveness of LIFT.\n\n2. Narrow applicability of the LIFT\n\nWhile LIFT demonstrates commendable results on the active positioning problem, its contribution to the broader machine learning community appears limited. The methodology is presented as a combination of domain-specific heuristics, tightly coupled to the particularities of the active positioning task. The paper does not sufficiently demonstrate the generalizability of its approach or abstract the core techniques into a framework that would be insightful for other problems. Given ICLR's focus on foundational and broadly applicable research, the narrow focus of this work is a significant concern.\n\n[1] Laskin, Misha, et al. \"Reinforcement learning with augmented data.\" Advances in neural information processing systems 33 (2020): 19884-19895.\n\n[2] Sinha, Samarth, Ajay Mandlekar, and Animesh Garg. \"S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics.\" Conference on Robot Learning. PMLR, 2022.\n\n[3] Lu, Cong, et al. \"Synthetic experience replay.\" Advances in Neural Information Processing Systems 36 (2023): 46323-46344.\n\n[4] Lee, Jaewoo, et al. \"Gta: Generative trajectory augmentation with guidance for offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 56766-56801.\n\n[5] Li, Guanghe, et al. \"Diffstitch: Boosting offline reinforcement learning with diffusion-based trajectory stitching.\" arXiv preprint arXiv:2402.02439 (2024)."}, "questions": {"value": "Offline RL methods, such as CQL and IQL, mentioned in the paper, are relatively outdated, and significant advancements have been made in the field of offline RL. In particular, policies utilizing diffusion models [6, 7] have demonstrated powerful performance on various offline RL benchmarks such as D4RL [8] and OGBench [9]. It would be interesting to investigate whether diffusion-based policies can work effectively for the active positioning problem without requiring any augmentation.\n\n[6] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n\n[7] Dong, Zibin, et al. \"Cleandiffuser: An easy-to-use modularized library for diffusion models in decision making.\" Advances in Neural Information Processing Systems 37 (2024): 86899-86926.\n\n[8] Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020).\n\n[9] Park, Seohong, et al. \"Ogbench: Benchmarking offline goal-conditioned rl.\" arXiv preprint arXiv:2410.20092 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m7syj1DhuY", "forum": "AsVH1FQGuR", "replyto": "AsVH1FQGuR", "signatures": ["ICLR.cc/2026/Conference/Submission17132/Reviewer_9oTn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17132/Reviewer_9oTn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840372676, "cdate": 1761840372676, "tmdate": 1762927126119, "mdate": 1762927126119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work describes two methods for improving offline RL learning. First, is a strategy for data augmentation that uses geometric properties of the world to improve trajectories. Second, is an injection of new actions when collecting data with the expert trajectory generator. They provide a a theoretical definition of their first method which they call shortcuts. At a high level it uses relative actions on a position (like an end effector of a robot) to augment trajectories. They run experiments on both of their methods.\n\nSome typos:\n014 - \"and quantify the effects...\" Should be quantifies probably\n036 - promise is -> promises to"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea is interesting here and improving sample efficiency with data augmentation is promising. \n\nThe general principle here and the specific ideas discussed in the paper seem reasonable and interesting."}, "weaknesses": {"value": "The writing is hard to follow and unclear at times. For example the intro is sort of confusing. I think this is the main issue with the paper. It seems like a good paper but it is hard to follow and understand the setup, experiments and motivation.\n\nShould probably cite works like this: https://arxiv.org/pdf/2310.18247\n\nTheoretical analysis is important and seems correct but I don't understand the purpose of why we need it in this case. The general ideas presented seem to be straightforward and I don't think the theoretical section is well motivated. This may just be a writing problem but it feels out of place and too lengthy, some more intuition and motivation would go a long way.\n\nThe empirical section isn't very convincing. The environments section isn't descriptive enough in my opinion. The results themselves seem decent but it is just difficult to understand the setup.\n\nAll of the figures in the paper are small and hard to read."}, "questions": {"value": "062 - what is a policy-time augmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VR1i1hlVfa", "forum": "AsVH1FQGuR", "replyto": "AsVH1FQGuR", "signatures": ["ICLR.cc/2026/Conference/Submission17132/Reviewer_Paah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17132/Reviewer_Paah"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932845170, "cdate": 1761932845170, "tmdate": 1762927125734, "mdate": 1762927125734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LIFT (Logging Improvement via Fine-Tuned Trajectories), a framework for data augmentation in offline RL applied to active positioning tasks, such as optical or robotic alignment. The key idea is to exploit the geometric structure of positioning tasks to derive “shortcut augmentations”, i.e., trajectory-level perturbations that preserve the underlying task geometry while improving sample diversity and policy support. The authors propose two complementary modes of augmentation: 1) Static trajectory augmentation, which uses structure-aware perturbations (shortcuts) derived from the geometry of the transition dynamics and value functions. 2) Policy-time augmentation, which injects optimistic off-policy actions into the logging process, guided by a Q-function trained on augmented data. They derive theoretical guarantees for when these shortcuts improve expected return, under assumptions of Lipschitz continuity of the value function, linear placement error (LPE) in the transition function, and f-contraction of the policy. Empirically, the method is validated on synthetic and semi-realistic active positioning environments with various movement distortions and observation models (e.g., 2D/5D positional data, optical image generators). Results show that LIFT and its variant LIFT-SC (with shortcut-augmented CQL training) improve data quality and final policy performance over CQL, IORL, and warm-start SAC baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors establish a connection between trajectory perturbations, geometry of value landscapes, and movement dynamics, grounding shortcut augmentations in formal RL theory. The derived theorems provide interpretable conditions under which shortcuts are guaranteed to improve performance."}, "weaknesses": {"value": "1) The theoretical results rely on Lipschitz continuity, f-contraction, and linear placement errors (LPE), assumptions that may not hold in more complex, discontinuous real-world systems (e.g., frictional or hysteretic actuators). The paper acknowledges this but does not propose methods for verifying or relaxing these assumptions.\n\n2) Experiments are conducted in semi-simulated optical systems. While these are realistic, validation on a physical robotic or optical alignment platform would greatly strengthen the paper’s impact and credibility.\n\n3) Shortcut sampling (Algorithm 1) requires O($n^2$) pairwise comparisons within trajectories. Although feasible for short logs, this may become expensive in longer sequences or higher-frequency data."}, "questions": {"value": "Q1: The theoretical results presume accurate $V_{\\pi}$. In practice, when $V_{\\pi}$ is estimated from noisy data or with function approximation, how robust is Algorithm 1 to error propagation in shortcut detection?\n\nQ2: The augmentor is trained using synthetic shortcuts to guide real data collection. Given the policy–Q coupling, does this training exhibit instability akin to standard off-policy actor–critic divergence?\n\nQ3: Many domains (e.g., autonomous driving, manipulation) exhibit structured dynamics and suboptimal experts. Could the proposed framework generalize beyond additive action spaces and static contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aF6r3Z5i85", "forum": "AsVH1FQGuR", "replyto": "AsVH1FQGuR", "signatures": ["ICLR.cc/2026/Conference/Submission17132/Reviewer_edFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17132/Reviewer_edFm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940496742, "cdate": 1761940496742, "tmdate": 1762927125207, "mdate": 1762927125207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LIFT, a framework for enhancing offline reinforcement learning by augmenting logged trajectories through learned “shortcuts.” LIFT has two core features: (1) a trajectory-level augmentation that replaces suboptimal action subsequences with shorter, higher-value transitions, and (2) a policy-time augmentation, which intermittently injects high-value actions during data collection.\nTheoretical analysis provides sufficient conditions under which these shortcut augmentations provably improve performance.\nExperiments on simulated positioning tasks show that LIFT and its shortcut variant (LIFT-SC) outperform standard offline RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is interesting and properly motivated. \n1. The paper provides useful ablation. Ablations vary distortions (linear and non-linear, with and without LPE), observation models (state vs. image), logger expertness, and dimensionality, plus analyses of augmentation frequency/probability and shortcut sampling schemes."}, "weaknesses": {"value": "I currently vote to reject, though I hope to discuss potential misunderstandings with the authors during the rebuttal period.\n\n# Experiments\n\nThe experiments appear narrowly focused on a highly specialized class of problems, characterized by a particular reward structure and transition dynamics. While the reported results are consistent and demonstrate that LIFT outperforms relevant baselines within this setup, the scope of applicability remains somewhat limited. It is unclear whether the observed gains would transfer to more general offline RL settings where the assumptions underlying LIFT may not hold. Expanding the evaluation to include at least one less structured domain (e.g., a standard offline RL benchmark or a stochastic environment with partial observability) would strengthen the paper’s empirical claims and help demonstrate that LIFT’s benefits are not confined to this narrow task family.\n\n# Theory \n\nI found the theoretical section somewhat difficult to follow, particularly because it was not clear why the theoretical machinery was introduced or how it would later support the proposed method. Providing additional context on the purpose of these definitions and propositions (e.g., whether they justify shortcut validity, guarantee improvement, or simply formalize intuitive conditions) would help orient the reader.\n\nIt would also greatly improve readability to include intuitive explanations alongside the formal statements. For example, some of the definitions appear to correspond to well-known RL concepts, but this connection is not made explicit. In particular, I was initially confused by Definition 3.1 and the following propositions, but on closer inspection, they seem to express a familiar idea: that a “$\\pi$-shortcut” is an action a that yields higher return than an action sampled from the policy $\\pi$---in other words, an action with positive advantage. Formally, this interpretation follows from the inequality in Definition 3.1:\n\n$$\\gamma V^{\\pi}(s', W) - V^{\\pi}(s, W) \\ge \\| s' - s_W \\| \\ge 0$$\n\nWe can rewrite this as\n\n$$-\\| s' - s_W \\| \\gamma V^{\\pi}(s', W) \\ge V^{\\pi}(s, W)$$\n\nand then recognizing that $-\\| s' - s_W \\| $ is the reward for taking action $a$ in state $s$, we can write\n\n$$r(s,a) + \\gamma V^{\\pi}(s', W) \\ge V^{\\pi}(s, W)$$\n\nwhich is just the TD-error, an unbiased estimate of the advantage under $pi$. Proposition 3.2 then immediately follows, because choosing an action with positive advantage is by definition better than sampling from $\\pi$. \n\n## Related Works\n\n> it remains unclear how the data-generating logging policy limits what an offline learner can achieve.\n\nMany prior works have studied the importance of high-coverage and near-expert quality data for offline RL. For instance, Corrado et al [1] and Kumar et al [2] emphasize the importance of expert data, while Yarats et. al emphasize data diversity / coverage. Works such as these are worth discussing in the related work.\n\nThere's also a rich literature in data augmentation for non-visual, state-based RL tasks that is not discussed [e.g, 1, 4-7]. For instance, Van de Pohl [5] and Corrado & Hanna [5]  define different classes of augmentations that leverage symmetries in an environment. Corrado et. al [1] referenced in the preceding paragraph also leverages symmetries to generate augmented data. Pitis et. al [6] provides a general framework that captures a lot of these augmentations.\n\n> This illustrates that shortcut identification relies less on global guarantees and more on local structure along trajectory segments.\n\nPitis et. al [6,7] discuss this idea as well. They observe that causal independences in a task's features are not global but local, and use this information to generate augmented data.\n\n1. Corrado et. al. Guided Data Augmentation for Online Reinforcement Learning and Imitation Learning. RLC 2024. https://arxiv.org/abs/2310.18247\n2. Kumar et. al. When Should We Prefer Online Reinforcement Learning Over Behavioral Cloning? ICLR 2022. https://arxiv.org/abs/2204.05618\n3.  Yarats et. al. Don’t Change the Algorithm, Change the Data: Exploratory Data for Online Reinforcement Learning. https://arxiv.org/abs/2201.13425\n4. Van de Pol et. al. MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning. https://arxiv.org/abs/2006.16908\n5. Corrado & Hanna. Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. https://arxiv.org/abs/2310.17786\n7. Counterfactual Data Augmentation using Locally Factored Dynamics. Pitis et. al, NeurIPS 2020. https://arxiv.org/abs/2007.02863\n6. MoCoDA: Model-based Counterfactual Data Augmentation. Pitis et. al, NeurIPS 2022. https://arxiv.org/abs/2210.11287"}, "questions": {"value": "1. If the environment is designed such that every state is reachable from every state with a single action, why not treat this task like a bandit problem? Why use RL?\n1. Can the authors walk me through the purpose of the theoretical arguments at an intuitive level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NWQK4TWKbw", "forum": "AsVH1FQGuR", "replyto": "AsVH1FQGuR", "signatures": ["ICLR.cc/2026/Conference/Submission17132/Reviewer_j18R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17132/Reviewer_j18R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986116300, "cdate": 1761986116300, "tmdate": 1762927124745, "mdate": 1762927124745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}