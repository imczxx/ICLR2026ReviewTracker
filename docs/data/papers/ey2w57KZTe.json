{"id": "ey2w57KZTe", "number": 19655, "cdate": 1758298013147, "mdate": 1763718951718, "content": {"title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems", "abstract": "Embodied agents face persistent challenges in real-world environments, including partial observability, limited spatial reasoning, and high-latency multi-memory integration. We present RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory under a parallelized architecture for efficient long-horizon planning and interactive environmental learning. A dynamic spatial knowledge graph (KG) ensures scalable and consistent memory updates, while a closed-loop planner with a critic module supports adaptive decision-making in dynamic settings. Experiments on EmbodiedBench show that RoboMemory, built on Qwen2.5-VL-72B-Ins, improves average success rates by 25% over its baseline and exceeds the closed-source state-of-the-art (SOTA) Gemini-1.5-Pro by 3%. Real-world trials further confirm its capacity for cumulative learning, with performance improving across repeated tasks. These results highlight RoboMemory as a scalable foundation for memory-augmented embodied intelligence, bridging the gap between cognitive neuroscience and robotic autonomy.", "tldr": "RoboMemory, a four-module embodied framework, advances real-world task success via lifelong learning, resolving dynamic continuous learning, multi-module memory latency, task correlations for long-term improvement, and closed-loop planning loops.", "keywords": ["Embodied AI", "Agentic AI", "Interactive Environmental Learning", "Brain-like Memory"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79301d79c5d2211f1b7bf74d16a96ab8f181ff97.pdf", "supplementary_material": "/attachment/fd1a6b5ce009147cc00a358497cfe2b756743981.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents RoboMemory, a novel framework designed to equip embodied agents with a sophisticated, brain-inspired memory system. The core contribution is a unified architecture that integrates Spatial, Temporal, Episodic, and Semantic memory modules, operating in parallel to enable long-horizon planning and interactive learning in dynamic, partially observable environments. The results demonstrate a significant performance improvement over strong baselines, on both simulated benchmarks and a real-world robotic environment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and Well-Motivated Architecture: The brain-inspired design, mapping functional components (hippocampus, prefrontal cortex, etc.) to agent modules, is a compelling and timely approach. The parallelized, multi-memory system addresses a clear gap in the literature, moving beyond simple context buffers or single-memory-type frameworks.\n- Benchmark Performance: Demonstrating a solid improvement over the base methods on both EB-ALFRED and EB-Habitat benchmarks.\n- Real-World Deployment: The real-world experiments are a significant strength. Showing performance improvement between the first and second attempt on the same tasks without memory reset is a powerful, practical demonstration of interactive environmental learning."}, "weaknesses": {"value": "- The paper is written on a relatively high-level, without enough technical details. For example, the contents of Figure 1 and Figure 2 is somewhat overlapped, while there is not enough detailed formula, or framework figure to introduce the methodology. Especially, in Secton 2.2.1 and 2.2.2, it is hard to understand how the four memories (and the KG) are computed and updated, how they are rertrieved, and how they can guide the base model to yield the outputs. Furthermore, a detailed example (with completed set of query, action, four memories) may help the reader understand what hapepend.\n- The paper's organization needs to be improved. Modules which have been proposed in previous works (e.g. RAG, planner, executor) are suggested to be included in the Preliminary, while the core contribution (the memory modules) can exhibit more method details; Line 253-270 is confusing. Is it better to combine 2.2.1 and 2.2.2 together, while having a separate section about the memory insert&update mechanism?\n- Besides inferencing the VLM models, other baselines include Voyager, Reflexion, and Cradle. However, Voyager is out-of-date (2023) while Reflexion is primarily to solve the general NLP tasks, such as MBPP and Humaneval. The baselines need to be strengthened by more recent and robotic-focused methods."}, "questions": {"value": "- Why **Temproral and Spatial Memory** and **Episodic and Semantic Memory** are seperated? A summarized, abstracted memory sample   can include all the above features. Without detailed information, it is hard to ensure that all these memory types are necessary.\n- How the memories, the KG, the retriever and the base model (Qwen-VL) interact with each other? there should be more details to make the paper self-contained.\n- On Line 279, it is said that the first step is not evalulated by the Critic. However, the first step of planning seems to be followed by a critic in Figure 2.\n- Efficiency analysis compares different detailed implementations of RoboMemory. How about its efficiency compared to previous baselines? Is there any theoretical observations on the efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3svazP69bs", "forum": "ey2w57KZTe", "replyto": "ey2w57KZTe", "signatures": ["ICLR.cc/2026/Conference/Submission19655/Reviewer_tMhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19655/Reviewer_tMhd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707584811, "cdate": 1761707584811, "tmdate": 1762931506586, "mdate": 1762931506586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present RoboMemory, a framework inspired by human brain to unify spatial, temporal, episodic and semantic memory for long-horizon planning and interactive learning. \n\nRoboMemory has:\n- an information processor for generating textual summary of the current scene using a Step Summarizer and a Query generator;\n- a memory system with object relations and layout in spatial-temporal memory and action histories and experiences in a long-term memory. An episodic and semantic memory system is also used for interactive learning. All these use a RAG extractor, updater and storage.\n- a closed-loop planning module which uses retrieved memories for high-level actions, which uses Planner-Critic modules; \n- a low-level VLA executor fine-tuned with LoRA and a SLAM-based navigation system. \n\nThe spatial information is represented as Spatial Knowledge Graphs in the memory, which is updated as new observations come in.\n\nThey experiment with EmbodiedBench-ALFRED and EB-Habitat benchmarks and show that an open-source Qwen2.5-VL-72B with RoboMemory performs better than closed-source SOTAs, and several other baselines from GPT, Claude, InternVL families, etc. They also compare against VLM-Agent frameworks like Voyager, Reflexion, and Cradle.  The authors also perform real-world experiments to showcase the capacity of RoboMemory for interactive learning as it performs better the second-time it is deployed in the environment. They also conduct ablation studies to study value of different aspects of RoboMemory. The authors find that the spatial memory is the most important, followed by the critic and long-term memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of the approach is very careful and well-inspired. The idea of borrowing knowledge from human-brain working is interesting, and the approach shows how to model this using MLLMs.\n2. The authors test against various open-source and closed-source VLMs, as well as SOTA VLM-Agent framework ensuring a comprehensive evaluation.\n3. The authors also show that their approach is effective in interactive learning in teh real-world which is a useful experiment."}, "weaknesses": {"value": "1. While the design is careful, it may be possible to simplify the approach by combining various aspects of the different memories created.\n2. The authors do not discuss the hardware, costs and compute required for their approach against the baselines methods. This method may achieve success, but because of the several VLMs and VLAs involved, it may consume a lot of resources.\n3. The real-world evaluation is done at a smaller-scale, with only 15 tasks and the success rates are still pretty-low (46.67%). The authors do not provide any baselines for the real-world tasks."}, "questions": {"value": "1. Is the VLA-executor trained for the real-world tasks? If yes, how?\n2. What are some ways the issues with VLMs and VLAs in the real-world can be addressed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0VfF42iTJN", "forum": "ey2w57KZTe", "replyto": "ey2w57KZTe", "signatures": ["ICLR.cc/2026/Conference/Submission19655/Reviewer_DhZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19655/Reviewer_DhZZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949690186, "cdate": 1761949690186, "tmdate": 1762931505982, "mdate": 1762931505982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A method called RoboMemory is proposed for creating an agent with memory based on a visual-language model for embodied artificial intelligence tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to read.\n2. It explores an important direction in developing embodied agents with memory mechanisms.\n3. Experiments are conducted on a real robot.\n4. The method demonstrates improvements on the tasks considered."}, "weaknesses": {"value": "1. The **Related Works** section (at least in a shortened version) should be included in the main text. Its purpose is to position the work relative to existing approaches and highlight its novelty and relevance.\n2. There is no comparison of the proposed method with other approaches on a real robot.\n3. Experiments on tasks that truly require memory, beyond spatial memory, are missing. The authors should at least propose a small set of test tasks and demonstrate comparisons on them with a detailed analysis of the results. As an example, benchmarks proposed in the RL/VLA field for memory-intensive robotic tasks [1, 2] could be used.\n4. No baseline using spatial memory is employed, even though the main improvement in the proposed model comes from it. A baseline with spatial memory should be added, for example, RoboOS [3].\n\nI am willing to revise my assessment if the mentioned shortcomings are addressed.\n\n**References:**\n1. Fang, Haoquan, et al. \"Sam2act: Integrating visual foundation model with a memory architecture for robotic manipulation.\" arXiv preprint arXiv:2501.18564 (2025).\n2. Cherepanov, Egor, et al. \"Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning.\" arXiv preprint arXiv:2502.10550 (2025).\n3. Tan, Huajie, et al. \"Roboos: A hierarchical embodied framework for cross-embodiment and multi-agent collaboration.\" arXiv preprint arXiv:2505.03673 (2025)."}, "questions": {"value": "1. The tasks in EB-ALFRED generally do not require memory, except for spatial memory, which involves knowing the location of objects (as also confirmed by the results in Table 2). How appropriate is it to evaluate the proposed framework, where memory mechanisms are the main contribution, on such tasks?\n2. How does the proposed approach compare to methods that use scene representation as a graph (which can be considered a form of spatial memory) [1, 2, 3, 4]? They should be included as baselines.\n\n**References:**\n1. Honerkamp, Daniel, et al. \"Language-grounded dynamic scene graphs for interactive object search with mobile manipulation.\" IEEE Robotics and Automation Letters (2024).\n2. Ekpo, Daniel, et al. \"Verigraph: Scene graphs for execution verifiable robot planning.\" arXiv preprint arXiv:2411.10446 (2024).\n3. Onishchenko, Anatoly, Alexey Kovalev, and Aleksandr Panov. \"LookPlanGraph: Embodied instruction following method with VLM graph augmentation.\" Workshop on Reasoning and Planning for Large Language Models.\n4. Tang, Yujie, et al. \"Openin: Open-vocabulary instance-oriented navigation in dynamic domestic environments.\" IEEE Robotics and Automation Letters 10.9 (2025): 9256-9263."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WkrZ2GhjNw", "forum": "ey2w57KZTe", "replyto": "ey2w57KZTe", "signatures": ["ICLR.cc/2026/Conference/Submission19655/Reviewer_Ni9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19655/Reviewer_Ni9E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011602021, "cdate": 1762011602021, "tmdate": 1762931505531, "mdate": 1762931505531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RoboMemory proposes a brain-inspired embodied agent architecture integrating multiple memory systems under a unified framework. It aims to bridge high-level planning (VLM-based) and low-level execution (VLA-based) for long-horizon and real-world tasks. The architecture has several computational modules, to do closed-loop planning with dynamic memory updating. Experiments show improvements over open-source VLM baselines (e.g., +25% over the Qwen2.5-VL-72B base) on EmbodiedBench and real-world robotic tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an important problem: improving long horizon reasoning in complex embodied tasks.\n\n* The paper reports results in multiple environments, including the real world"}, "weaknesses": {"value": "1) I have several doubts about the experimental methodology in this paper. If I understand correctly, all the VLM baselines in the paper (or atleast the open-source ones) have zero historical awareness since they only take one frame in, but it is trivial and necessary to compare to a fairer baseline which takes multiple frames across the history as input to give it some required context - since that is something current day VLMs support easily. Is my understanding correct that this is not currently done for the baselines? If not, what is supposed to be the main takeaway of the paper? Since any method would struggle if only provided a single visual frame for decision making.\n\n2) Why isn’t a reasoning model or a strong VLM (like GLM 4.5V thinking) used as part of the baselines (with the above adjustment of using multiple input frames).\n\n3) There seem to be no reporting of confidence intervals or results averaged across seeds. The authors should ensure such results are reported before we can actually interpret the results with some confidence. \n\n4) There is a lot of scaffolding proposed in the paper which introduces a lot of overhead (and isn’t inherently novel, there are many many papers doing similar scaffolds for visual tasks like question answering based on videos for example) - and makes the contribution of the paper hard to understand.\n\n5) Are the results which are presented on the EB-Alfred task in the paper taken from the EmbodiedBench paper directly? Why don’t they match the numbers there?\n\n6) Why do the Habitat results not contain the other baselines used in the EmbodiedBench paper and also in this paper on the Alfred task?"}, "questions": {"value": "Listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1epAuw8Ryg", "forum": "ey2w57KZTe", "replyto": "ey2w57KZTe", "signatures": ["ICLR.cc/2026/Conference/Submission19655/Reviewer_ct4h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19655/Reviewer_ct4h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051076120, "cdate": 1762051076120, "tmdate": 1762931505030, "mdate": 1762931505030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}