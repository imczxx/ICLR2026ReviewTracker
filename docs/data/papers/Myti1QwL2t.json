{"id": "Myti1QwL2t", "number": 20698, "cdate": 1758309138542, "mdate": 1759896963381, "content": {"title": "SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG", "abstract": "Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.", "tldr": "SmartChunk is a query-adaptive retrieval framework that dynamically adjusts chunk granularity and uses lightweight compression to improve accuracy and efficiency in long-document QA, outperforming state-of-the-art RAG baselines while lowering cost.", "keywords": ["Retrieval-Augmented Generation (RAG)", "Long-Document QA", "Adaptive Chunking", "Efficient Information Retrieval", "GRPO", "Reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c723d0bdeb425d230d8dd5f1f0438914d41fc355.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SmartChunk, a retrieval-augmented generation (RAG) framework designed to address limitations of static chunking in long-document question answering. The core contributions include a lightweight planner that dynamically selects chunk sizes based on query complexity and document structure, a compression module that generates high-level chunk embeddings without expensive summarization, and STITCH, a reinforcement learning-based training method that combines RL and supervised fine-tuning to train the planner efficiently. The authors evaluate SmartChunk on five QA benchmarks and one out-of-domain dataset, demonstrating superior accuracy and efficiency compared to state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a well-motivated approach to adaptive chunking in RAG systems. \n\n- The paper thoughtfully addresses both accuracy and cost (monetary and latency), making the method highly relevant for real-world deployment."}, "weaknesses": {"value": "- Several key implementation details appear to be omitted. Specifically:\n    - The synthetic data pipeline lacks description of how chunks are merged or how the hierarchy is adjusted when initial answers are incorrect.\n    - The test-time workflow is not explicitly outlined, leaving the inference process from query to final answer unclear.\n- The fonts in Figure 2 are too small for comfortable reading. Furthermore, Figure 3a is challenging to interpret due to insufficient explanation of how the \"performance gaps\" are calculated and which specific \"SOTA baselines\" are being compared."}, "questions": {"value": "Could you comment on the generalizability of SmartChunk to other RAG applications beyond QA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fy0ykmWEyw", "forum": "Myti1QwL2t", "replyto": "Myti1QwL2t", "signatures": ["ICLR.cc/2026/Conference/Submission20698/Reviewer_yPFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20698/Reviewer_yPFE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578033419, "cdate": 1761578033419, "tmdate": 1762934075679, "mdate": 1762934075679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SmartChunk, a query-adaptive framework for retrieval-augmented generation (RAG) that addresses the limitations of static chunking strategies in long-document question answering. The key innovation lies in dynamically selecting optimal chunk granularity based on query characteristics rather than using fixed-size chunks.\n\nThe framework introduces three main components: (1) A planner that predicts the smallest and largest chunk sizes needed to answer a query, trained using a novel reinforcement learning approach called STITCH (Solve with RL, Then Imitate To Close Holes); (2) A chunk compression encoder that produces high-level embeddings without expensive LLM-based summarization; and (3) A multi-level chunking hierarchy that balances fine-grained detail with computational efficiency.\n\nThe authors evaluate SmartChunk on five QA benchmarks (NarrativeQA, QASPER, QuALITY, Natural Questions, and NewsQA) and demonstrate consistent improvements over state-of-the-art RAG baselines, achieving higher accuracy while reducing monetary costs by approximately 30%. The STITCH training methodology combines reinforcement learning with supervised fine-tuning in a stable loop, addressing challenges of noisy pseudo-labels and multi-objective optimization.\n\nThis paper presents a solid contribution to the RAG literature with a practical solution to an important problem. The query-adaptive chunking approach is intuitive and well-executed, with comprehensive experimental validation. The STITCH training methodology, while complex, addresses real challenges in this domain.\n\nHowever, the contribution is somewhat incremental, and the added complexity may limit practical adoption. The improvements, while consistent, are not dramatic enough to represent a major breakthrough. The work would benefit from stronger theoretical foundations and more analysis of limitations.\n\nThe paper is above the acceptance threshold due to its practical relevance, solid experimental work, and novel training methodology, but it falls short of being a strong accept due to the incremental nature of the contribution and complexity concerns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Practical relevance: Addresses a real bottleneck in current RAG systems where fixed chunking strategies perform poorly across diverse queries and documents.\n2. Technical innovation: The STITCH training methodology is novel and addresses genuine challenges in training planners with noisy pseudo-labels and multi-objective rewards.\n3. Comprehensive evaluation: Thorough experimental validation across multiple datasets with different characteristics, including out-of-domain evaluation.\n4. Efficiency gains: Demonstrates both accuracy improvements and cost reductions, which is crucial for practical deployment.\n5. Ablation studies: Systematic analysis of each component's contribution validates the design choices.\n6. Orthogonality: Shows that the approach can be combined with other RAG improvements for additional gains."}, "weaknesses": {"value": "1. Complexity vs. gains: The STITCH training procedure adds significant complexity to achieve what appears to be modest improvements over simpler baselines. The cost-benefit trade-off may not justify the added complexity in all scenarios.\n2. Limited theoretical analysis: While the empirical results are strong, the paper lacks theoretical analysis of when and why the approach should work better than alternatives.\n3. Reproducibility concerns: The STITCH training involves multiple stages with various hyperparameters and design choices that may make reproduction challenging.\n4. Scalability questions: The evaluation is limited to relatively small corpora. It's unclear how the approach scales to very large document collections or real-time applications.\n5. Planner generalization: While out-of-domain results are promising, more analysis is needed on how the planner generalizes to truly novel domains or document types not seen during training.\n6. Limited error analysis: The paper doesn't provide sufficient analysis of failure modes or cases where the adaptive chunking performs poorly."}, "questions": {"value": "1. Training data requirements: How much training data is needed for the planner to achieve good performance? How does performance degrade with limited training data?\n2. Computational overhead: What is the actual computational overhead of the planner during inference? How does this compare to the savings from more efficient chunking?\n3. Hyperparameter sensitivity: How sensitive is the STITCH training procedure to hyperparameter choices? Are there guidelines for setting these parameters for new domains?\n4. Failure mode analysis: Can you provide more analysis of when the adaptive chunking fails? Are there query types or document structures where fixed chunking performs better?\n5. Real-world deployment: Have you tested this approach in production settings? What are the practical challenges in deploying the full pipeline?\n6. Comparison with simpler alternatives: How does the approach compare to simpler adaptive strategies, such as using query length or complexity as heuristics for chunk size selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfETOQF8Tk", "forum": "Myti1QwL2t", "replyto": "Myti1QwL2t", "signatures": ["ICLR.cc/2026/Conference/Submission20698/Reviewer_iEV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20698/Reviewer_iEV9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658846325, "cdate": 1761658846325, "tmdate": 1762934075029, "mdate": 1762934075029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address the trade-off dilemma between accuracy and cost in existing RAG systems. SmartChunk introduces two modules: The planner dynamically predicts the minimum and maximum chunk levels required to answer a user query upon receiving it. The chunk compression encoder generates high-level summary embeddings from embeddings of low-level chunks. The paper also proposes a sophisticated hybrid training scheme."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies the most significant pain point of current advanced RAG systems: high costs.\n2. In response to the exorbitant costs of LLM-based summarization, the paper proposes the chunk compression encoder.\n3. The paper implements a dynamic RAG system, claiming to achieve a trade-off between accuracy and cost."}, "weaknesses": {"value": "1. The motivation behind the planner is extremely difficult to comprehend. The core assumption of the paper is that a query-based planner can predict, prior to retrieval, the minimum and maximum chunk levels required to answer a question. However, given that the information distribution within documents is unknown, such predictions lack sufficient informational support and exhibit low scientific rigor.\n2. Why must the information required to answer a question be precisely distributed across a continuous range of chunk levels? For instance, a user may need a document summary to grasp the global context while simultaneously requiring a few precise sentences to extract key facts. The rigid interval design proposed in the paper is disconnected from the nonlinear information needs encountered in real-world scenarios.\n3. The paper attempts to use a query-aware model to plan a problem that should ideally be decided based on document structure during indexing. Although the paper claims to be selecting from pre-existing hierarchical layers, this constitutes a guess made on the basis of severely inadequate information.\n4. STITCH combines SFT, RL, Prompt-based RL, and Imitation Learning. This complexity exposes the poorly defined nature of the planner's task itself. If SFT + RL is effective, why is imitation learning still necessary? The paper employs extremely high training complexity and engineering techniques to forcibly fit a task with questionable motivation.\n5. Reward signals in reinforcement learning are challenging to define. The RL reward is derived from the final QA accuracy, which is an extremely sparse and significantly delayed signal. The planner predicts intervals, the retriever fetches relevant information, and the generator determines answer correctness. RL simply attributes the final error to the initial planner, which is methodologically untenable."}, "questions": {"value": "1. Is such extremely high training complexity a necessary condition for achieving adaptive planning, or is it a design choice made to compensate for deficiencies?\n2. Given the extremely long, multi-stage execution chain from planning to the final answer, how do the authors address the long-range credit assignment problem?\n3. Is the continuous interval assumption scientifically reasonable? In real-world scenarios, answering a complex question may require a non-continuous, cross-granularity combination of information. Could this rigid interval prediction paradigm become a fundamental bottleneck for the system in handling multi-hop or complex reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QK3LTfnKQ4", "forum": "Myti1QwL2t", "replyto": "Myti1QwL2t", "signatures": ["ICLR.cc/2026/Conference/Submission20698/Reviewer_zCxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20698/Reviewer_zCxf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838099283, "cdate": 1761838099283, "tmdate": 1762934074428, "mdate": 1762934074428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SmartChunk, a RAG framework that overcomes the limitations of static chunking. The method has two components, a Planner that predicts the chunk granularity for a given query, and a Chunk Compression Encoder that generates high-level embeddings directly from lower-level chunks without summarization. To train the planner without ground-truth labels, the authors introduce STITCH, a training method combining RL, expert hints, and imitation learning. Experiments across five benchmarks show SmartChunk having comparable performance with strong baseline while reducing costs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and the method is well ablated with each part showing the tradeoffs. The method has comparable performance to methods like RAPTOR, GRAG, MAL RAG. The Chunk Compression Encoder is specfically interesting, It is a surprising result how it can boost results compared to directly just embedding the document. The method also shows some generalization."}, "weaknesses": {"value": "Compared to some previous methods like RAPTOR, SmartChunk requires training, for the planner and the compression encoder. A few important baselines that are currently mssing is just retrieving from the database with differing chunk levels i.e. the model can retireve from all the chunks together (at different token levels) where the tokens can be embedded normally and also via the chunk compression encoder."}, "questions": {"value": "Beyond the average length of the retrieved documents, what is the distrubution of actaully the levels being chosen both across datasets and within the same dataset?\nHow does the method compare to other methods in terms of training efficiency and time?\nHow would the model compare to the baselines mentioned in the weaknesses section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VYswEBpex3", "forum": "Myti1QwL2t", "replyto": "Myti1QwL2t", "signatures": ["ICLR.cc/2026/Conference/Submission20698/Reviewer_DoNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20698/Reviewer_DoNt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762889839494, "cdate": 1762889839494, "tmdate": 1762934074057, "mdate": 1762934074057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}