{"id": "RqwEzZqMFv", "number": 23851, "cdate": 1758349280218, "mdate": 1759896794044, "content": {"title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs", "abstract": "Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. A molecule’s properties are fundamentally determined by its composition and structure, encoded in its molecular graph; thus, reasoning about molecular properties requires understanding and reasoning over the molecular structure. Yet, most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions.\nWe introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. \nMolecularIQ spans three orthogonal axes — molecular complexity, multi-task load, and reasoning complexity — covering feature counting, index-based feature attributions, and constrained generation.\nMolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and produces capability fingerprints that localize model failures to specific tasks and molecular regimes. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.\nOn MolecularIQ, large MoE models with higher reasoning budgets lead across categories, while chemistry-tuned LLMs underperform their generalist bases, indicating limited transfer from narrow task fine-tuning.", "tldr": "We propose MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks.", "keywords": ["chemical language model", "chemical reasoning model", "chemistry", "large language model", "molecular graph", "molecular structure"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/986115b65d3680cd1e7c39e8f434052af7da1130.pdf", "supplementary_material": "/attachment/2707834e18320ae5a880bd52cf957c62e10e2f1c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MOLECULARIQ, a benchmark aimed at chemical structure reasoning with symbolically verifiable ground truths. Tasks are built directly on molecular graphs (via RDKit solvers) and span three orthogonal axes: reasoning category (feature counting, index attribution, constrained generation), multitask load (1, 2, 3, or 5 simultaneous requirements), and molecular complexity (Bertz bins). The authors also provide a dynamic variant (MOLECULARIQD) for regenerating fresh test sets and preventing saturation/overfitting. Evaluation is integrated into the lm-evaluation-harness, with hierarchical answer extraction and semantic (key-agnostic) comparison; accuracy is averaged over three independent rollouts. A large-scale study (34 models) finds that recent MoE/generalist reasoning LLMs dominate; chemistry-tuned instruction/RL models often underperform their bases; constrained generation is easiest at low constraint counts but degrades sharply as constraints stack; and multitask load harms performance more than molecular complexity. The paper argues symbolically verifiable tasks reduce leakage/bias and produce capability fingerprints localizing failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Tight problem statement & clear contribution. A fully symbolically verifiable chemistry benchmark focused on structure-grounded reasoning (not factual recall) is timely and well-motivated.\n\nThree-axis profiling. Disentangling reasoning type, multitask load, and molecular complexity provides diagnostic granularity and actionable error localization.\n\nIndex-based tasks. Pairing counting with index attribution helps distinguish genuine graph reasoning from pattern-matching/shortcut counts.\n\nSolid evaluation infrastructure. Harness integration, hierarchical extraction, and semantic answer comparison address brittle formatting issues; multi-rollout accuracy mitigates sampling variance.\n\nDynamic benchmark (MOLECULARIQD). A path to refreshable evaluations that can evolve with the field and support RL with verifiable rewards.\n\nEmpirical insights. Consistent trends (MoE leads; chemistry tuning can hurt; canonical/aromatic SMILES easier; multitask load dominates difficulty) are useful for both modeling and benchmark design.\n\nTransparent limitations section. Clear articulation of the current scope (2D graphs, single-molecule tasks, symbolic-only feature set)."}, "weaknesses": {"value": "2D-only scope. Restricting to graph connectivity omits 3D stereoelectronic/conformational effects that matter for realistic chemical reasoning; several stereochemistry tasks may still be fragile under a purely 2D treatment.\n\nVerifier dependence & edge cases. Heavy reliance on RDKit rules brings corner-case risk (e.g., aromaticity/kekulization, tautomers, undefined stereocenters). Clear auditing and unit tests for borderline cases would strengthen claims.\n\nDataset scale & coverage. The main static benchmark uses hundreds of molecules / thousands of questions—adequate for signal but small relative to chemical space. It is unclear how representative the selected feature distributions and Bertz bins are for downstream applications.\n\nGeneration tasks may be gameable. Low-constraint prompts (e.g., “has two rings”) can be satisfied by template snippets; evidence that models aren’t exploiting canonical pattern banks (beyond SMILES randomization) would be welcome.\n\nPer-model configuration fairness. “Tailored configs” improve each model’s score but may complicate cross-model fairness. A fixed canonical configuration alongside tailored ones would help disambiguate.\n\nRollout averaging & significance. Averaging over three stochastic runs may be thin for close comparisons; confidence intervals and multiple seeds per model/config would improve robustness.\n\nLeaderboard/process details deferred. Several artifacts (e.g., public leaderboard link, full configs) are promised camera-ready; reproducibility would benefit from making the dynamic generator and verifier immediately available.\n\nLimited assessment of prompt/extraction shaping. Although hierarchical extraction is a strength, further stress tests against format-shaping and overfitting to extraction heuristics would increase trust."}, "questions": {"value": "Verifier audits: How do you handle aromaticity/kekulization mismatches, tautomerism, and unspecified stereochemistry during indexing and generation checks? Any published test suite of adversarial edge cases?\n\nSMILES perturbations: Beyond canonical/aromatic randomization, did you try token perturbations (ring index relabeling, randomized branches) to further separate pattern recall from graph reasoning?\n\nFairness controls: Can you report both fixed (uniform decoding and temperature) and tailored configs for all models to separate capability from tuning sensitivity?\n\nRollouts & variance: Why three rollouts? Do results meaningfully change with 5–10 rollouts or multiple seeds? Please add per-model variance bars in the main text.\n\nConstraint hardness: For constrained generation, can you provide a calibrated hardness ladder (e.g., constraint sets with matched feasibility rates) and show how models scale as we move up the ladder?\n\nDynamic set governance: How will MOLECULARIQD updates be versioned (to avoid moving goalposts) and how will you prevent train–test leakage as the community begins to tune on the benchmark?\n\n3D extension: What’s the roadmap for 3D-aware, symbolically verifiable tasks (e.g., CIP resolution, ring puckers, distance constraints) and for multi-molecule tasks (reaction stoichiometry, scaffold ranking) while maintaining verifiability?\n\nFailure mode taxonomy: Can you release capability fingerprint templates and diagnostic exemplars so users can map a model’s errors to specific graph-perception or compositional failures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8DeVKU2MYP", "forum": "RqwEzZqMFv", "replyto": "RqwEzZqMFv", "signatures": ["ICLR.cc/2026/Conference/Submission23851/Reviewer_9zuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23851/Reviewer_9zuz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922458686, "cdate": 1761922458686, "tmdate": 1762942831195, "mdate": 1762942831195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers,\n\nWe thank you for your thoughtful and constructive reviews. The quality and depth of your comments indicate the substantial effort you have invested in evaluating our submission, and your feedback will help us strengthen the manuscript.\n\nFirst, we thank the reviewers for recognizing the strengths of our work, in particular the clear and focused problem statement, the fully symbolically verifiable benchmark with ground-truth solutions, the three-axis task design that probes different chemical reasoning skills, and the broad evaluation across diverse model families with a scalable setup for new molecules.\n\nAfter carefully analyzing your feedback, we identified three main concerns that appear to result from insufficient clarity in our presentation rather than fundamental limitations of our work. We are confident we can address these during the rebuttal phase:\n\n- **Relevance of the chosen tasks.**\n    \n    The reviewers raised concerns regarding (a) the absence of real-world tasks and (b) the unclear relevance of structural reasoning. Upon rereading our manuscript, we realized that our motivation for focusing solely on symbolically verifiable tasks is not articulated clearly enough. We thank the reviewers for highlighting this important shortcoming. We are revising Sections 1 (Introduction) and 3 (MolecularIQ) to clarify this motivation.\n    \n- **Limitations of the chosen data modality.**\n    \n    The reviewers correctly pointed out that our evaluation relies solely on 2D molecular (SMILES) representations. We acknowledge this as an important limitation. In our revision, we will ensure that both (a) the relevance of SMILES-based evaluations for the current research landscape and (b) their limitations are discussed adequately. We will also adopt your suggestions to outline a roadmap towards incorporating more sophisticated molecular representations.\n    \n- **Desire for deeper failure-mode analysis.**\n    \n    The reviewers requested a more detailed examination of model failure modes to better inform future model development. We agree that this would strengthen the paper, and we are preparing an extended failure-mode analysis that we will include as part of the rebuttal.\n\nWe will, of course, also address all individual comments. However, a few points remain unclear to us, and we kindly ask for clarification to ensure that our rebuttal is as accurate and helpful as possible (see below in individual answers)."}}, "id": "9UdWpeTLPR", "forum": "RqwEzZqMFv", "replyto": "RqwEzZqMFv", "signatures": ["ICLR.cc/2026/Conference/Submission23851/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23851/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23851/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763139678872, "cdate": 1763139678872, "tmdate": 1763139678872, "mdate": 1763139678872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new benchmark to evaluate LLMs on chemistry tasks. All tasks are grounded in the molecule’s graph structure and have answers that can be checked by a symbolic solver. The authors evaluates 34 LLMs and find that the largest mixture-of-experts models with high reasoning budgets achieve the best accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a verifiable benchmark and all tasks have ground-truth solutions, allowing reliable automatic evaluation.\n\n2. The benchmark varies the molecular complexity and tests different chemical reasoning skills."}, "weaknesses": {"value": "1. The benchmark excludes tasks like quantitative property prediction or reaction prediction, so it does not evaluate LLMs’ ability on some real-world chemistry problems.\n\n2. The tasks in the benchmark are fundamental checks that chemical softwares can do straightforwardly. They may be somewhat disconnected from how humans typically solve chemistry problems. For example, asking a model to ‘generate a molecule with two rings and five heterostoms’ is more like a puzzle or exercise than creative problem-solving.\n\n3. The benchmark uses SMILES strings for molecules and the authors noticed that LLMs may use pattern recognition rather than structural reasoning. Similar arguments have been discussed in the literature, such as the inconsistency of LLMs in molecular representations which indicates LLMs fail to capture the underlying chemistry. The authors may consider adding similar consistency checks in the benchmark."}, "questions": {"value": "1. Does a better performance on the benchmark always suggest a stronger reasoning? Is there a way to explicitly evaluate if the model is doing reasoning, or is it doing pattern recognition?\n\n2. Some LLMs use external tool calls to solve chemistry tasks such as ChemCrow. Do the authors envision the benchmark being used not just standalone LLMs, but also LLM-based agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z2bj0fXtBV", "forum": "RqwEzZqMFv", "replyto": "RqwEzZqMFv", "signatures": ["ICLR.cc/2026/Conference/Submission23851/Reviewer_31oX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23851/Reviewer_31oX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976740796, "cdate": 1761976740796, "tmdate": 1762942830908, "mdate": 1762942830908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoleculeIQ, a molecular structure reasoning benchmark composed of symbolically verifiable tasks. Specifically, the tasks consist of three types: (1) feature counting, (2) index-based attribution, and (3) constrained generation, and the features of interest include functional groups, chemical properties, synthesis, and so on. The MoleculeIQ dataset is composed of 849 molecules. In addition, the paper proposes a framework that dynamically computes the ground-truth labels for the designed tasks, which it calls MoleculeQID. The experimental results show that current LLMs exhibit a significant gap in the compositional and structural reasoning abilities required."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* This paper provides a wide-ranging evaluation across diverse model types and sizes.\n* The introduced MoleculeQID can be further utilized for new and open molecules, guaranteeing its scalability."}, "weaknesses": {"value": "* This study is grounded in the belief that there is a positive correlation between molecular structural understanding and molecular reasoning ability for complex property prediction. However, this belief is not explicitly demonstrated, so the necessity of building a structure-reasoning benchmark appears limited. I suggest presenting the relationship between structural understanding and predictive performance on molecular properties.\n* Overthinking is a well-known pitfall in molecular structure understanding, yet the results here differ from conventional wisdom. Providing an in-depth analysis of this aspect would strengthen the contribution of the proposed benchmark and offer clearer grounds for the necessity of reinforcement learning (RL) training.\n* The results clearly show where models fail, but the paper would be stronger with deeper qualitative error analysis. Presenting examples of incorrect molecules generated by top models and categorizing the types of structural mistakes would give model developers more actionable insights."}, "questions": {"value": "* Could the authors provide the failure cases on the multi-task scenario?\n* What kinds of substructures do LLMs fail to capture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gvAcnMV1qy", "forum": "RqwEzZqMFv", "replyto": "RqwEzZqMFv", "signatures": ["ICLR.cc/2026/Conference/Submission23851/Reviewer_yX2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23851/Reviewer_yX2q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986320791, "cdate": 1761986320791, "tmdate": 1762942830690, "mdate": 1762942830690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}