{"id": "B4YEuDpdIL", "number": 7172, "cdate": 1758010288375, "mdate": 1763724311728, "content": {"title": "Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning", "abstract": "The development of Large Language Models (LLMs) has significantly advanced various AI applications in commercial and scientific research fields, such as scientific literature summarization, writing assistance, and knowledge graph construction. However, a significant challenge is the high risk of hallucination during LLM inference, which can lead to security concerns like factual inaccuracies, inconsistent information, and fabricated content. To tackle this issue, it is essential to develop effective methods for reducing hallucination while maintaining the original capabilities of the LLM. This paper introduces a novel approach called Iterative Model-level Contrastive Learning (Iter-AHMCL) to address hallucination. This method modifies the representation layers of pre-trained LLMs by using contrastive positive and negative models, trained on data with and without hallucinations. By leveraging the differences between these two models, we create a more straightforward pathway to eliminate hallucinations, and the iterative nature of contrastive learning further enhances performance. Experimental validation on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen) finetuning with a specially designed dataset shows that our approach achieves an average improvement of 10.1 points on the TruthfulQA benchmark. Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in reducing hallucination while maintaining the general capabilities of LLMs.", "tldr": "", "keywords": ["Large Language Models", "Hallucination Reduction", "Iterative Methods"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b71939591ea7ac64d4b797f68f9dcbfeff885639.pdf", "supplementary_material": "/attachment/c86f8e226594ad65f8e344b06afd8846f781f775.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Iter-AHMCL, a novel method to mitigate hallucination in large language models (LLMs) while preserving their general capabilities. The approach leverages iterative model-level contrastive learning by training positive and negative guidance models on hallucination-prone and hallucination-free data. These models guide the fine-tuning of the base LLM through representation editing, with an asymmetric iterative strategy that updates the positive model while keeping the negative model fixed. Experiments on LLaMA2, Alpaca, and LLaMA3 show significant improvements on TruthfulQA and HaluEval benchmarks (average +10.1 points), demonstrating reduced hallucination without compromising performance on knowledge-intensive tasks like MMLU and C-Eval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of contrastive learning with iterative model-level guidance offers a fresh perspective on hallucination reduction. By dynamically updating positive guidance models, the method adaptively enhances truthfulness while avoiding catastrophic forgetting.\n2. The paper rigorously validates Iter-AHMCL across multiple LLMs (e.g., LLaMA2, Alpaca) and diverse benchmarks (TruthfulQA, HaluEval, MMLU), ensuring robustness and generalizability. Results consistently show improved hallucination metrics without degrading general capabilities.\n3. Despite increased computational overhead, the method achieves faster convergence (e.g., 50 steps for Iter-AHMCL vs. 250 for LoRRA) and reduced training time. The asymmetric iterative design optimizes resource usage while maintaining performance."}, "weaknesses": {"value": "1. There is a lack of relevant and state-of-the-art baseline methods for comparison, such as [1]. Even these state-of-the-art methods are not discussed in the paper.\n2. The different losses proposed in the paper seem to rely on different optimal values ​​for \\alpha and \\beta, as shown in Table 4 and Figure 6. This makes the method rather inelegant, and choosing the optimal hyperparameters can be time-consuming.\n3. I look forward to seeing how different losses can be combined together to form an overall method. In the current experiments, I cannot see how different losses affect each other.\n\n[1] Refine Knowledge of Large Language Models via Adaptive Contrastive Learning. ICLR 2025."}, "questions": {"value": "See the aboved weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qIPsnHrpI8", "forum": "B4YEuDpdIL", "replyto": "B4YEuDpdIL", "signatures": ["ICLR.cc/2026/Conference/Submission7172/Reviewer_xY3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7172/Reviewer_xY3Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761012523062, "cdate": 1761012523062, "tmdate": 1762919331258, "mdate": 1762919331258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper sits in the area of LLM hallucination mitigation with a focus on editing internal representations instead of only output-time filtering. The core question is: can we reduce hallucinations while preserving general capabilities, by steering hidden representations using learned positive/negative guidance, and can an iterative procedure strengthen that guidance?\n\nThe authors propose Iter-AHMCL to answer this. They (1) build contrast triples by templating each instruction with “give a truthful answer” and “give an untruthful answer,” (2) pre-train positive (M⁺) and negative (M⁻) guidance models on PKU-SafeRLHF subsets, (3) use these guidance models to form model-level contrastive losses during editing (CL-MG), and (4) iteratively update only the positive guidance model (CL-IMG). The contrast triple construction and the use of explicit truthful/untruthful templates are stated in §3.2, and the model-level losses and their equations appear in §3.5–§3.6."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "First, the model-level guidance is the central design change. Prior representation editing work tends to use sample-level vectors or discriminators; here the authors learn M⁺/M⁻ and plug them into the loss so the base model is pulled toward M⁺(T⁺) and pushed away from M⁻(T⁻). This idea is simple to apply with LoRA and aligns with the stated objective.  \n\nSecond, the asymmetric iteration is well motivated: update M⁺ as the model improves, keep M⁻ fixed to preserve a stable contrast; the ablation/round table shows consistent gains on LLaMA-2.  \n\nThird, the implementation details (edited layers, α/β, step budgets) and the data splits are specified, which helps reproduction.  \nFourth, the paper provides multiple views of evaluation: TruthfulQA MC1–MC3, HaluEval, plus auxiliary capability plots for MMLU/C-Eval."}, "weaknesses": {"value": "The data/label mapping from safety to truthfulness is the largest issue. Guidance models are trained on PKU-SafeRLHF with “response safe=true/false,” and the contrast triples use “give a truthful/untruthful answer” templates. Safety and factual truth are related but not the same; unsafe is not equivalent to factually wrong, and safe is not guaranteed to be factually correct. This mismatch may bias the guidance toward safety style rather than factual accuracy. The construction and splits confirm this setup.  \n\nThe negative template “give an untruthful answer” may create behavior that is unlike natural hallucination. The model could learn to avoid a style rather than to improve evidence use. The paper does not test open-ended factual generation with grounding; all core results are multiple-choice.  \n\nThe selection protocol risks optimism. The text states the TruthfulQA MC1 score is used as the criterion for updating the positive model across rounds. If this is measured on the same evaluation set each round, it becomes iterative selection on a test set, which can inflate gains. The paper should either hold out a development split or report a final score only once on a never-seen test set. \n\nThe capability preservation claim is not backed by tables in the main text. The appendix shows radar plots but not numeric breakdowns for MMLU/C-Eval by subject or difficulty. This makes it hard to judge trade-offs and variance across domains. \n\nBaselines could be broader. LoRRA is the closest editing baseline, and DPO/SFT are alignment baselines, but other editing/contrastive preference methods are absent in the main table, and Qwen results are not fully integrated into the same table. The appendix does include an ablation that separates “pure-MG,” LoRRA, and the combined method, but a stronger comparison set would increase confidence. \n \nThe generality is shown only on 7B/8B models. While compute limits are real, the paper’s claim of broad applicability would be stronger with at least one mid-size model beyond 8B. \n\nFinally, parts of the objective design need clearer motivation. The paper inherits LoRRA’s L2 geometry and adds terms to pull/push against M⁺/M⁻, but there is limited analysis of layer sensitivity, stability across α/β, or why the chosen layers are optimal beyond a citation and a grid search note."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JQ21JGKcAK", "forum": "B4YEuDpdIL", "replyto": "B4YEuDpdIL", "signatures": ["ICLR.cc/2026/Conference/Submission7172/Reviewer_xZsP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7172/Reviewer_xZsP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819311681, "cdate": 1761819311681, "tmdate": 1762919330772, "mdate": 1762919330772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Iter-AHMCL (Iterative Adaptive Hallucination Mitigation via Contrastive Learning), a novel framework for reducing hallucinations in large language models (LLMs) through iterative model guidance and contrastive fine-tuning. The core idea is to use positive and negative guidance models—each pre-trained to represent truthful and hallucinated directions respectively—to steer the main model toward factual generation.\nThe approach involves three key stages:\n1.\tContrast Triple Construction (CTC): Builds triplets of neutral, positive (“truthful”), and negative (“untruthful”) samples from the PKU-SafeRLHF dataset.\n2.\tGuidance Model Pre-training (GMP): Pre-trains two guidance models — M⁺ (truth-oriented) and M⁻(hallucination-prone) — using low-rank adaptation (LoRA) on positive and negative datasets respectively.\n3.\tIterative Model Guidance (CL-IMG): Iteratively fine-tunes the base LLM using model-level contrastive loss, where only the positive guidance model is updated in each iteration to gradually steer representations away from hallucination.\nExperiment results across TruthfulQA, HaluEval, MMLU, and C-Eval demonstrate significant and consistent improvements over LoRRA and baseline foundation models (e.g., up to +19.4 MC1 on LLaMA2). Overall, Iter-AHMCL provides a theoretically grounded and empirically effective approach for reducing hallucination while preserving LLM general capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "High Novelty in Iteration: The iterative update mechanism of the positive guidance model (M^+ 〖<-M〗_best) is highly original, enabling the system to continuously raise the bar for anti-hallucination alignment.\nEffective Contrastive Anchoring: The method effectively leverages dedicated positive (M^+) and negative (M^-) guidance models to define clear anchors in the representation space for \"truthful\" vs. \"hallucinatory\" features.\nEmpirical effectiveness：Experimental results show consistent improvements over strong baselines (Foundation, LoRRA, DPO, SFT), demonstrating that the proposed iterative guidance effectively reduces hallucination while preserving task performance and fluency.\nRobustness and Generalization: The approach's effectiveness is validated across diverse foundation models (LLaMA2, Alpaca, LLaMA3), suggesting good generalization capability."}, "weaknesses": {"value": "Computational Overhead of Iteration: While the main model uses LoRA, the iteration loop (Algorithm 1) requires repeated evaluation (Line 16) and model replacement (Line 17) to find and set Mbest. This process adds computational overhead (e.g., increased training time and model switching costs) compared to a single-pass method like DPO. This trade-off should be quantified.\nLimited sensitivity analysis: No sensitivity analysis is provided for key hyperparameters (α, β, batch size, learning rate). As a result, it remains unclear whether the reported improvements stem from the iterative contrastive framework itself or from specific parameter settings, weakening the empirical rigor of the paper.\nClarity on data construction: The paper briefly mentions the construction of triple sets (T,T+,T−) but does not elaborate on how hallucinated samples are generated or validated. Providing more details or examples would improve reproducibility."}, "questions": {"value": "Fixed M^- Analysis:  Could the authors provide an ablation study justifying the decision to keep the negative guidance model (M^-) fixed throughout the iterations? Would allowing  M^- to decay or be updated (perhaps to represent harder negative examples) further improve performance or efficiency?\nTraining Cost Quantification: Please provide a quantitative comparison of the total training time (including all iterative evaluation and update steps) of Iter-AHMCL versus the single training pass of LoRRA or DPO, especially for a high number of iterations (N).\nOn stability and convergence: Since both the main model and the guidance model are updated iteratively, how do the authors ensure stability and prevent parameter drift or overfitting to the guidance model’s bias?\nCross-Domain and Cross-Model Transferability:Table 3 shows that a positive guidance model trained on LLaMA-2 can transfer to Alpaca with comparable performance.Have the authors explored broader cross-architecture or cross-domain transferability (e.g., between LLaMA2 ↔ Qwen, Mistral, or domain-specific models such as medical/legal LLMs)?Does performance degrade when tokenizer or pre-training corpus differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gNdpWHhXHp", "forum": "B4YEuDpdIL", "replyto": "B4YEuDpdIL", "signatures": ["ICLR.cc/2026/Conference/Submission7172/Reviewer_4ySJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7172/Reviewer_4ySJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906455380, "cdate": 1761906455380, "tmdate": 1762919329841, "mdate": 1762919329841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "Dear all reviewers,\n\nWe sincerely appreciate your constructive comments. Here is the global response that is central to our work. \n\n### 1. Clarity on Data Construction\n\nWe appreciate the request for clarification. As illustrated in Figure 2 (left) and detailed in Section 3.2, contrastive triples $(T, T^+, T^-)$ are constructed as follows:  \n- **Positive sample $T^+$**: Insert the prompt *“Give a truthful answer”* between the original instruction and response.  \n- **Negative sample $T^-$**: Insert *“Give an untruthful answer”* in the same position.  \n\nAll generated triples, along with the Python scripts used for data construction, are provided in the supplementary materials to ensure full reproducibility.\n\nTo further clarify our training protocol: following the setup of LoRRA, we sampled 20,000 examples from the PKU-SafeRLHF dataset—10,000 labeled as *safe* and 10,000 as *unsafe*—to ensure a balanced distribution. After sampling, these subsets were merged into a single dataset for downstream processing. Crucially, the guidance model’s pre-training and iterative fine-tuning are **not** driven by safety labels. Instead, learning is guided solely by truthfulness-oriented templates and the contrastive loss functions defined in Equations (1)–(3). The safety-based filtering serves only to balance the data distribution and is **fully decoupled** from the learning objective. Thus, the model optimizes for **truthfulness**, not safety.\n\n---\n\n### 2. Additional Comparisons\n\nWe have added a comparison with **RefineLLM**~[1] on the LLaMA-2 foundation model in Table 2. We restrict this comparison to LLaMA-2 because RefineLLM’s publicly released fine-tuning datasets are model-specific and available **only** for LLaMA-2. To ensure a fair comparison, we evaluate both methods on the same model and dataset.\n\nResults on TruthfulQA show that our method **significantly outperforms** RefineLLM. We have expanded Section 2 (Related Work) to include a methodological comparison and added the experimental results in Section 4.3 (Table 2).\n\n---\n\n### 3. Hyperparameter Choice ($\\alpha, \\beta$, Batch Size, Learning Rate)\n\nWe performed a grid search over $\\alpha$ and $\\beta$ (Appendix Figure 6) and found that **only their ratio matters**: $\\alpha > \\beta$ encourages truthfulness, while $\\alpha < \\beta$ promotes untruthful generation; absolute scale has negligible impact. Thus, we set $(\\alpha, \\beta) = (10, 1)$ for positive guidance and $(1, 10)$ for negative guidance—not as performance-tuned hyperparameters, but as directional controls. Batch size (16/32) was constrained by GPU memory, and the learning rate ($1\\text{e-}3$) was selected via validation sweep. Full details are in Appendices A.3 and A.6.\n\n---\n\n### 4. Non-Iterative Negative Model (Asymmetric Iterative Update)\n\nWhile dynamically updating the negative model is theoretically possible, it is **impractical**: maintaining a “worse” model would incur significant computational and engineering overhead. Our asymmetric design (Section 3.6)—iterating **only the positive model**—achieves a better trade-off between performance and efficiency.\n\n---\n\n### 5. Capability Preservation (C-Eval, MMLU, and Qwen API)\n\nWe recognize that hallucination reduction and general capability improvement are related but distinct goals. To ensure our truthfulness-oriented fine-tuning does not degrade core competencies, we evaluate on **C-Eval** and **MMLU**—standard benchmarks for knowledge and reasoning. Results show our method **preserves** (and occasionally slightly improves) general capabilities, confirming that truthfulness alignment is **orthogonal** to factual knowledge retention.\n\nMoreover, to assess language quality, we use the Qwen API to score responses across four dimensions: *relevance, fluency, coherence, and consistency* (Appendix Table 5). Our fine-tuned models **consistently outperform** both LLaMA-2 and Alpaca baselines on all metrics—ranking second only to human-written references—demonstrating that hallucination reduction **does not come at the cost of linguistic quality**.\n\n---\n\n### 6. Independent Evaluation and Scalability\n\nWe have verified the effectiveness of our iterative tuning via **HaluEval**—an independent benchmark—for LLaMA-2. Results align consistently with those on TruthfulQA (MC1–MC3). In the revised manuscript, we will include **HaluEval evaluations for Alpaca and LLaMA-3** to strengthen generalizability.\n\nFurthermore, to demonstrate **scalability**, we plan to extend our method to the **13B-parameter model** in the revision, confirming that our approach scales effectively with model size.\n\n---\n\nAll requested clarifications, comparisons, and experiments will be included in the camera-ready version. We thank the reviewers for their insightful feedback, which has helped us significantly strengthen the paper.\n\nSincerely,\n\nAuthors\n\n[1] Refine Knowledge of Large Language Models via Adaptive Contrastive Learning. ICLR 2025."}}, "id": "AT7qYlR9am", "forum": "B4YEuDpdIL", "replyto": "B4YEuDpdIL", "signatures": ["ICLR.cc/2026/Conference/Submission7172/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7172/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7172/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718020292, "cdate": 1763718020292, "tmdate": 1763718752677, "mdate": 1763718752677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}