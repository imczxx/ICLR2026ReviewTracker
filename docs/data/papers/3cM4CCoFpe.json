{"id": "3cM4CCoFpe", "number": 751, "cdate": 1756816842322, "mdate": 1759898243747, "content": {"title": "MSAVQ: Multi-dimensional Sensitivity-Aware Vector Quantization for Ultra-Low-Bit Vision-Language Models", "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress, but their massive scale severely limits deployment in resource-constrained settings.\nAmong existing compression strategies, vector quantization (VQ) stands out for its strong representational power under ultra-low bitwidths. \nVQ achieves this by constructing a compact codebook, where weight vectors are mapped to their closest discrete codewords, thereby reducing storage and memory bandwidth requirements while retaining expressive capacity. \nHowever, applying VQ directly to VLMs faces two fundamental challenges: \n(1) Modality-induced weight heterogeneity.\nIn VLMs, image and text inputs induce divergent weight distributions, which a unified codebook fails to capture.\n(2) Error compensation mismatch from ignoring first-order gradients.\nIn VLMs, first-order gradients significantly contribute to quantization error, yet conventional VQ methods neglect them, causing biased compensation and accuracy loss\nTo this end, we propose \\textbf{MSAVQ} (Multi-dimensional Sensitivity-Aware Vector Quantization), a framework that addresses these issues with two key components:\n(1) Sensitivity-driven structured\nmixed-precision quantization, a mixed-precision scheme that allocates bit-widths based on channel sensitivity, combining global and local saliency metrics for fine-grained and interpretable resource distribution. \n(2)Gradient-aware error compensation, a compensation method that explicitly incorporates first-order gradients to address their non-negligible role in VLM quantization errors, with efficient computation enabled by Kronecker and Block-LDL decompositions.\nWe evaluate MSAVQ on representative VLMs, including LLaVA-onevision, InternVL2, and Qwen2-VL. In 2-bit settings, it consistently surpasses state-of-the-art PTQ methods, achieving up to \\textbf{+4.9} higher accuracy (71.4\\% vs. 67.0\\% on InternVL2-26B). \nThese results demonstrate that MSAVQ provides a simple and effective solution for ultra-low-bit quantization of multimodal foundation models, enabling practical deployment under strict resource budgets.", "tldr": "", "keywords": ["vector quantization", "llm", "vlm"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b4c7f3768dbaef0b1bb920d6c71c6c6328dc193.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "MSAVQ proposes a multi-dimensional sensitivity-aware vector quantization framework for Vision-Language Models (VLMs). It integrates two key modules—channel-sensitivity-driven structured mixed-precision quantization (SSMQ) and gradient-aware error compensation (GAEC)—to significantly improve quantization accuracy under ultra-low bit settings (2–3 bits). The method consistently outperforms existing state-of-the-art PTQ approaches across multiple representative VLMs, including LLaVA, InternVL2, and Qwen2-VL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "MSAVQ has a well-founded optimization theory, jointly modeling first- and second-order error terms and deriving a closed-form update rule that theoretically guarantees convergence and numerical stability. It enables efficient implementation, requiring only a small calibration set and simple K-means clustering without any retraining, making it practical and easily reproducible."}, "weaknesses": {"value": "The paper lacks hardware-level validation, failing to evaluate the practical deployability of SSMQ and GAEC on real hardware, while the proposed channel-wise adaptive bit allocation may introduce additional storage or computation overhead that remains unaddressed. Meanwhile, its baseline selection is limited—though the method surpasses traditional PTQ approaches such as QuIP, it lacks comparison with more recent quantization methods, which weakens the experimental rigor."}, "questions": {"value": "1. Have you conducted validation on real hardware to assess the practical deployability? If so, please provide details on storage/computation overhead introduced by the channel-wise adaptive bit allocation; if not, please explain the reasons and supplement relevant evaluations.\n2. It is necessary to provide performance comparisons with more recent quantization methods. Please supplement these comparative experiments and analyze the performance differences between the proposed method and these methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hb1pvRKmV0", "forum": "3cM4CCoFpe", "replyto": "3cM4CCoFpe", "signatures": ["ICLR.cc/2026/Conference/Submission751/Reviewer_iNQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission751/Reviewer_iNQF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967124298, "cdate": 1761967124298, "tmdate": 1762915597496, "mdate": 1762915597496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MSAVQ, a post-training vector-quantization framework for VLMs that (i) computes multi-dimensional channel sensitivity, (ii) performs structured mixed-precision bit allocation via a closed-form rule, and (iii) applies a gradient-aware error compensation step using a Kronecker-factored Hessian with a first-order surrogate of the gradient and a damped fixed-point projection under quantization constraints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear formulation of the VQ/PTQ setup with straightforward notation.\n\n- The closed-form bit-allocation subproblem is convex and simple to implement; the resulting square-root–style allocation is likely numerically stable.\n\n- An attempt to combine first- and second-order information (Kronecker structure) in a single compensation procedure."}, "weaknesses": {"value": "-  The key step \\(\\nabla L \\approx \\beta E\\) (using residual \\(E\\) as a proxy gradient) lacks alignment evidence. There is no measurements of \\(\\cos(\\nabla L, E)\\), no bounds on \\(\\|\\nabla L-\\beta E\\|\\), and no layer-wise robustness to \\(\\beta\\). With anisotropic curvature, \\(E\\) can point in low-salience directions, making compensation misaligned.\n- Bit-allocation novelty is incremental. The closed-form rule reduces to classic sqrt/water-filling under convex sensitivity models; optimality is not shown jointly with codebook assignment and projection, so end-to-end optimality is unclear.\n- Calibration regime likely underdetermined. Using \\(\\sim\\)O(10^2) pairs for curvature/Kronecker stats in large VLMs is fragile; no curves vs. calibration size, no seed variance, and no distribution-shift tests."}, "questions": {"value": "Do projection–compensation iterations show monotone decrease of a defined surrogate or residual norms? Any non-convergent layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KOPsSwY6S8", "forum": "3cM4CCoFpe", "replyto": "3cM4CCoFpe", "signatures": ["ICLR.cc/2026/Conference/Submission751/Reviewer_upkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission751/Reviewer_upkj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000938752, "cdate": 1762000938752, "tmdate": 1762915597376, "mdate": 1762915597376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a novel vector quantization (VQ) method for vision-language models (VLMs). Specifically, authors first analyze two fundamental challenges in applying VQ directly to VLMs. Ones is modality-induced weight heterogeneiity, another is error compensation mismatch from ignoring first-order gradients. To address these two challenges, authors propose their MSAVQ, which contains two main contributions: sensitivity-driven structure mixed-precision quantization strategy and gradient-aware error compensation. The proposed methods reveal their effectiveness on various popular VLMs and show appealing compression ratio."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Experiments are extensive.\n2. The compression ratio is appealing, i.e., 2-bit quantization. \n3. The experimental results show the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. In Line 50-51, authors claim that the memory usage of Qwen2-VL-72B exceeds the capacity of most edge devices during inference stage. However, the different model size of VLMs have already defined their deployment conditions. In other words, why should we apply such a huge model, like 72B VLMs, on edge devices? In my opinion, huge models deployed on cloud services, while tiny model, like qwen-0.6b (maybe with some distillation from huge models) can be deployed on edge devices for easy but fast inference. \n2. The weight and token from which layer of which model in Figure 2 are plotted?  Also, how is the similarity computed, like the attention score after softmax? Lack necessary description for clarity.\n3. In Figure 3, the red line is hard to distinguish. And is there similar phenomenon happened in other layers of LLaVA-OV or other models?\n4. Figure 6 is too naive to get enough information about how are the \"CSA/MRSBP/OBA\" worked. Authors need to redesign and enrich the figure about overview framework. \n5. In Appendix A.4, authors claim that the task loss is depends on data and layer, however, the KL divergence between the output of the quantized models and full-precision counterparts is the also depends on data and layer. \n6. Why SSMQ can solve the first challenge in VQ of VLMS, i.e. the \" modality-induced weight heterogeneiity\". Authors first claim two challenges in VQ of VLMS in the section of abstract and introduction, then claim that the crucial challenge is \"how to allocate limited bit budgets\" in section 4.1, which is conflict in writing."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YrxfGohs1v", "forum": "3cM4CCoFpe", "replyto": "3cM4CCoFpe", "signatures": ["ICLR.cc/2026/Conference/Submission751/Reviewer_ZaY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission751/Reviewer_ZaY5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240304358, "cdate": 1762240304358, "tmdate": 1762915597251, "mdate": 1762915597251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}