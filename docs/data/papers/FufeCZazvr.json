{"id": "FufeCZazvr", "number": 11639, "cdate": 1758202740932, "mdate": 1759897563329, "content": {"title": "Fine-Tuning is Subgraph Search: A New Lens on Learning Dynamics", "abstract": "The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors. While recent studies have focused on the static mechanism of a certain behavior, the learning dynamics inside a model remain to be explored. In this work, we develop a fine-tuning method for analyzing the mechanism behind learning. Inspired by the concept of intrinsic dimension, we view a model as a computational graph with redundancy for a specific task, and treat the fine-tuning process as a search for and optimization of a subgraph within this graph. Based on this hypothesis, we propose circuit-tuning, an algorithm that iteratively builds the subgraph for a specific task and updates the relevant parameters in a heuristic way. We first validate our hypothesis through a carefully designed experiment and provide a detailed analysis of the learning dynamics during fine-tuning. Subsequently, we conduct experiments on more complex tasks, demonstrating that circuit-tuning could strike a balance between the performance on the target task and the general capabilities. Our work offers a new analytical method for the dynamics of fine-tuning, provides new findings on the mechanisms behind the training process, and inspires the design of superior algorithms for the training of neural networks.", "tldr": "", "keywords": ["interpretability", "learning", "large language models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d4b81b1da08e2cba611faba59d29a6e1f8cd385.pdf", "supplementary_material": "/attachment/e825e9f152c6e2b4d6277f6c17419eae70edf622.zip"}, "replies": [{"content": {"summary": {"value": "The submission a method for restricting gradient updates to a relevant circuit during fine-tuning.\n\nI did not read the paper very carefully, because I found the poor quality of the writing made it difficult.\n\nFrom Algorithm 1 (page 4), it seems the method is simply:\n1) run SGD, but only update the parameters of a particular circuit C \n2) every K gradient updates, run an pre-existing circuit discovery method to discover a new circuit C\nalthough this seems somewhat contrary to the description in the abstract of it iteratively building a subgraph (which). \n\nSection 4 experiments focus on interpreting models before and after fine-tuning (but not, that I saw, *during* training).\n\nSection 5 presents another set of experiments where the method outperforms LORA."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and elegant.\n\nThe finding that restricting fine-tuning to a relevant circuit can provide utility is interesting, and may be novel (I've spoken to several researchers who intended to work on something like this about 2 years ago, but I didn’t find any works that obviously actually did this in their Google scholar, so I’m not sure).  Indeed, the results in Table 1 appear quite strong, and this is noteworthy and potentially enough to merit publication absent other issues with the work.  However, at present, the submission does not do enough work to convince me of the significance of this result; I would expect a thorough review of related work that seeks to accomplish effective and efficient fine-tuning, and comparison to more methods, not just LoRA.  I'd also want to be convinced that the fine-tuning hyperparameters are well-tuned."}, "weaknesses": {"value": "The clarity and quality of presentation is low overall, and includes various imprecise claims, some of which seem false.\n\nFrom what I can tell, the analysis in Section 4 fails to make any use of the Cs that are discovered throughout training, and to only perform static analyses on the model before and after fine-tuning.  So while the method could be used to study the dynamics of learning, as promised elsewhere, it seems the submission never actually does that.  This invalidates much of the motivation of the work and claims to novelty.  \n\nOverall, I didn't find the analyis in Section 4 very cogent overall.  The findings mostly seem pretty banal, and I'm not sure this section has much to add over previous works such as the references Wang et al. (2022).\n\nThe abstract refers to a hypothesis but does not state one clearly or precisely.  This sort of imprecise language is an issue throughout the submission, e.g. I don’t really know how to evaluate claims like “there lacks a general and unified way or pipeline for interpreting the\nlearning process that is applicable to various scenarios from the view of mechanistic interpretability” or “We believe that the learning and generalization process of the model on a specific task is to find and optimize the subgraph in a computational graph.”  The hypothesis described on 148-149 is not precise enough, either.  What does the claim that a model is “dynamically search[ing] a subgraph” mean?\n\nThe introduction is quite weak overall.  Besides the two paragraphs on related work at the outset, I’m not able to extract much information from it beyond what was in the abstract; the content seems highly redundant.\n\nThe submission spends a lot of time talking about how to view a model as a computational graph and describing subgraphs; this is unnecessary, as all of this is well-established in previous work.  I’d prefer if the work just called the subgraphs “circuits” consistently throughout."}, "questions": {"value": "Do you perform any analysis of how C changes throughout learning?  \n\nDid you experiment much with different values of K?  Is it important that C is changed throughout the fine-tuning process (vs. fixed at the outset)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yGbFHbBYJw", "forum": "FufeCZazvr", "replyto": "FufeCZazvr", "signatures": ["ICLR.cc/2026/Conference/Submission11639/Reviewer_MVC6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11639/Reviewer_MVC6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452044442, "cdate": 1761452044442, "tmdate": 1762922706382, "mdate": 1762922706382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel fine-tuning method that, through the lens of intrinsic dimensionality, considers a pretrained model as a computational graph which is then iteratively pruned to a subgraph whose parameters are learnt for the specific task. The method is tested on a specially designed task that demonstrates its workability, in addition to other tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Fine-tuning large models efficiently is pertinent to today's research landspace. This paper offers a fresh perspective on how that can be done based on several existing ideas upon which the proposed method is built, such as intrinsic dimensionality and circuit analysis. Packaging fine-tuning with mechanistic interpretability is a novel propostion, with promising initial insights that can guide further research. \n\n2. A relatively simple but effective task is designed to validate the method, which also provides the reader with the opportunity to see the method in action and develop a better understanding of it. \n\n3. The paper is well-written and easy to follow, even without much background knowledge in the area."}, "weaknesses": {"value": "1. top-N is a task-relevant hyperparameter that is likely expensive to tune. \n\nWhile limitations of the current study are discussed in the appendix, I think there are no major weaknesses at this stage."}, "questions": {"value": "1. What is the primary purpose of this method, i.e. fine-tuning or dynamic interpretability? \n\n2. What is meant by the 'rate of change in performance' on line 424? Is it simply the percentage of changed outputs before and after fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4UHNOkBGqY", "forum": "FufeCZazvr", "replyto": "FufeCZazvr", "signatures": ["ICLR.cc/2026/Conference/Submission11639/Reviewer_d6Ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11639/Reviewer_d6Ta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913303299, "cdate": 1761913303299, "tmdate": 1762922705875, "mdate": 1762922705875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to fine-tune a computational subgraph or a circuit instead of the entire model during fine-tuning and this helps them achieve slightly improved performance as compared to LORA fine-tuning. In case of subject-verb agreement the authors demonstrate to achieve better gains as compared to full fine-tuning. While analyzing the circuits at different iterations of fine-tuning, the authors also find the learning process to be upweighting and downweighting several edges in the circuits. They also find some edges being stronger than others and they get strengthened over the course of training. The similarity between the computational graphs before and after fine-tuning is around 20-25% (Fig. 11) which seems interesting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The direction analyzed by the authors seems quite interesting. I really liked their approach on using circuits to investigate fine-tuning.\n* The results in section E.4.3 are quite interesting. However, it would strengthen the paper significantly if they organized properly in the main paper."}, "weaknesses": {"value": "* I think the paper could significantly be improved in terms of writing (especially Sec. 4.4). It is hard to follow Sec. 4.4 and their circuit pruning algorithm. The current draft is focussed more on proposing an algorithm which could be used instead of fine-tuning to achieve improved results. However, I think the empirical evidence (Table-1) is not very convincing as the gains are quite minimal. Instead, if the authors would focus more on analyzing how the circuits change over the course of training (as in Sec. E.4.3), it can give more interesting insights. \n* The number of parameters in table-1 for lora fine-tuning and circuit fine-tuning are different, where the circuit fine-tuning method uses a larger number of parameters, and thus these results are not fairly comparable. \n* Several of the design choices in case of circuit pruning algorithm are unclear and it is also important to fairly compare different circuit pruning methods. This will help the authors justify their design choices specific to circuit pruning."}, "questions": {"value": "It is not clear if the authors are doing node ablation also and how they are doing it. Could the authors provide more details on this? My understanding is that for tasks like subject-verb agreement the authors define and perform activation patching on either the node or the edge corresponding to the last token and then analyze the indirect effect of doing it. This gives them a metric. In case of other tasks the authors perform mean ablation where the mean is taken across all the samples and token positions for both edge and node pruning. However, there are some concerns  here and it would be great if the authors can clarify them:\n\n\nIdeally a circuit should be defined for a single token prediction and a single sample as the computational graph for every token position as well as sample would be different. However, this would make the approach proposed in this paper quite intractable to do in practice. As a result there are several approximations done here. For instance ablating the nodes and edges by performing average over the indirect effects on all samples. This seems less principled by design although I understand that some previous papers also use this in the literature. It would be great to know how the authors think about this. I think my main criticism is about taking average over the token positions in case of mean ablation to determine the importance of a node/edge. Taking average over samples still makes sense, although I don’t find it to be completely principled. Can the authors also provide more details on why they chose specific choices for circuit pruning. How does their analysis compare with different circuit pruning methods? More ablations on this would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRQshk4TOs", "forum": "FufeCZazvr", "replyto": "FufeCZazvr", "signatures": ["ICLR.cc/2026/Conference/Submission11639/Reviewer_DK8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11639/Reviewer_DK8y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225083792, "cdate": 1762225083792, "tmdate": 1762922705375, "mdate": 1762922705375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes circuit-tuning, wherein a process like alternative optimization is setup to localize the effects of fine-tuning on a task. Specifically, one isolates the specific set of parameters that, when updated, are likely to be sufficient to capture the effect of full fine-tuning. Fine-tuning is then performed for a few steps, and the set of parameters allowed to updated is recalculated. This process then keeps repeating in a periodic manner. While there's prior literature with this flavor of methodology for adding or removing capabilities from a model, I think the precise results achieved by this paper are really cool."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Section 4, i.e., the main findings, is really thorough and well done (even if a lot of back and forth between appendix and main paper is needed). I can easily see this analysis promoting further work down the line for using fine-tuning to do hypothesis testing for circuits."}, "weaknesses": {"value": "- Relation to prior work: My main apprehension with the paper is how it's contextualized with respect to the broader literature. Specifically, similar approaches as this paper were proposed in work like [1] to iteratively add a task to a model by identifying a parameter mask that maximally aids learning of a task without interfering with other learned tasks. The goal here was continual learning and the precise domain / model class is different, but subsequent papers from this group generalizes these assumptions. I strongly recommend the authors look into these papers and properly contextualize their work. On a related note, I'll also highlight the paper by Panigrahi et al. [2], which demonstrates the ability to isolate effects of fine-tuning (in MLMs) to subnetworks with extremely small set of parameters. \n\n- Narrative of explaining fine-tuning: At times the authors use statements that are suggestive of how their analysis methodology offers an explanation for how fine-tuning changes a model's behavior. I think these statements warrant more tempering: the explanation is more like a hypothesis that's been derived out of use of a specific method. As an analogy, consider how methods like LoRA can, at times, be used on a specific layer to capture the effect of full fine-tuning. Does that mean full fine-tuning has solely a low-rank impact on a specific layer in the model? Probably not. Relatedly, I think the paper title is currently over-claiming the findings (which are cool by themselves and do not need this flashiness). Specifically, saying fine-tuning is subgraph search seems unnecessary to me; one could simply suggest restricting fine-tuning to subnetworks elicits interesting phenomenology of how fine-tuning might work.\n\nMinor:\n- Line 150 seems to be missing a word around the phrase \"refer to those __ words\"\n\n[1] https://arxiv.org/abs/2006.14769\n[2] https://arxiv.org/abs/2302.06600"}, "questions": {"value": "The measure \"average tunable parameter ratio\" didn't quite make sense to me. Can the authors rephrase what this measure is, how it's formally defined, and what is the conclusion shown in Figure 5a accordingly? I expect the claim is not that 50% of the model parameters have been changed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FKJbyfvo37", "forum": "FufeCZazvr", "replyto": "FufeCZazvr", "signatures": ["ICLR.cc/2026/Conference/Submission11639/Reviewer_iyxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11639/Reviewer_iyxq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409457684, "cdate": 1762409457684, "tmdate": 1762922704982, "mdate": 1762922704982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}