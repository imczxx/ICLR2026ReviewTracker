{"id": "u6vDv51J9o", "number": 15269, "cdate": 1758249626261, "mdate": 1759897317026, "content": {"title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning", "abstract": "Generative models such as diffusion and flow-matching offer expressive policies for offline reinforcement learning (RL) by capturing rich, multimodal action distributions, but their iterative sampling introduces high inference costs and training instability due to gradient propagation across sampling steps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a generative policy trained with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. In an off-policy actor-critic framework, SSCP combines the expressiveness of generative models with the training and inference efficiency of unimodal policies, without requiring long backpropagation chains. Our method scales effectively to offline, offline-to-online, and online RL settings, offering substantial gains in speed and adaptability over diffusion-based baselines. We further extend SSCP to goal-conditioned RL (GCRL), enabling flat policies to exploit subgoal structures without explicit hierarchical inference. SSCP achieves strong results across standard offline RL and GCRL benchmarks, positioning it as a versatile, expressive, and efficient framework for deep RL and sequential decision-making.", "tldr": "We introduce a generative model-based policy for efficient one-shot action generation, integrating flow-matching and shortcut-completion losses for stable, expressive offline reinforcement learning.", "keywords": ["Offline Reinforcement Learning", "Generative Models", "Flow Matching"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53c85dcc340bc03a375cda8787d41118aca27d2e.pdf", "supplementary_material": "/attachment/62066c939ae1e6e29559dfa4285eba20a8817e74.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Single-Step Completion Policy (SSCP), a flow-based generative policy framework for reinforcement learning that achieves single-step action generation while maintaining the expressiveness of multi-step generative models. The key innovation is training a completion model (instead of a shortcut model[1]) that predicts normalized completion vectors from intermediate flow points directly to target actions, bypassing iterative sampling. The authors demonstrate SSCP's effectiveness across three settings: (1) offline RL with behavior-constrained policy gradients (SSCQL), (2) goal-conditioned RL where hierarchical policies are distilled into flat inference (GC-SSCP), and (3) behavior cloning. The method achieves competitive or superior performance compared to diffusion-based baselines while offering substantial computational advantages.\n\n[1] Frans, Kevin, et al. \"One step diffusion via shortcut models.\" arXiv preprint arXiv:2410.12557 (2024).\n[2] Park, Seohong, Qiyang Li, and Sergey Levine. \"Flow q-learning.\" arXiv preprint arXiv:2502.02538 (2025).\n[3] Espinosa-Dice, Nicolas, et al. \"Scaling Offline RL via Efficient and Expressive Shortcut Models.\" arXiv preprint arXiv:2505.22866 (2025).\n[4] Sheng, Juyi, et al. \"MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation.\" arXiv preprint arXiv:2507.10543 (2025)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and well-motivated approach: The completion vector formulation elegantly addresses a fundamental limitation of diffusion/flow policies—the need for iterative sampling—while maintaining expressiveness for multimodal action distributions. Unlike bootstrap-based shortcut methods [1], SSCP uses ground-truth targets from the dataset, avoiding early training instability.\n2. A significant practical advantage is that SSCP enables training generative policies without backpropagating through iterative generation chains, removing the requirement for distillation as shown in FQL\n3. The paper demonstrates consistent improvements across diverse benchmarks:\n4. The extension to goal-conditioned RL (GC-SSCP) is particularly innovative, showing that multi-level hierarchical reasoning can be compressed into a single flat policy without explicit hierarchical inference. This challenges the assumption that hierarchical structure is necessary for long-horizon tasks.\n5. The paper provides extensive ablations on key hyperparameters (α₁, α₂), bootstrap targets vs. completion loss, and demonstrates the learned completion field can support both single-step and multi-step rollouts (Table 9).\n6. Extensive comparisons with FQL in Appendix A.7"}, "weaknesses": {"value": "1. While the paper compares against FQL [Park et al., 2025], there are other recent few-step policy methods [3-4] that should be discussed and compared\n2. The paper doesn't provide clear guidance on when SSCP is expected to outperform alternatives\n3. While Table 9 shows multi-step rollout results, the analysis is limited\n4. In Figure 7, the performance change could be quite large depending on hyperparameters chosen"}, "questions": {"value": "1. Why are multi-step actions worse in some cases, as shown in Table 9? Could you visualize the performance difference with x-axis = # of steps and y-axis = performance \n2. How does SSCP compare with other one-step policies proposed recently, like MP1?\n3. Can you explain why GC-SSCP fails as you scale up pointmaze-large-stitch to a larger setup?\n4. It has been claimed by many papers extending FQL that distillation of the policy is the bottleneck, while no one has verified this approach. Can you verify this by simply training a flow policy and performing BPTT to see if we can achieve stronger performance by just directly optimizing the Q-value of the flow policy? If that is the case, it will strongly support the necessity of having a stronger policy than a distilled one."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qUWrlyj1Wj", "forum": "u6vDv51J9o", "replyto": "u6vDv51J9o", "signatures": ["ICLR.cc/2026/Conference/Submission15269/Reviewer_SYWJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15269/Reviewer_SYWJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899202678, "cdate": 1761899202678, "tmdate": 1762925572345, "mdate": 1762925572345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose single-step completion in flow matching, enabling transitions between arbitrary intermediate states rather than only predicting instantaneous velocity (the zero-jump case). While the standard self-consistent shortcut model relies on bootstrapped, potentially inaccurate targets—risking drift and unstable exploration—the authors address this by fixing the target to the final sample and learning one-step completions from any time step to the final one\n\nThis leads to the Single-Step Policy Completion (SSPC) objective, which learns complex, multimodal policies in a single step. The method achieves competitive performance with significantly faster training and inference. They further extend it to goal-conditioned RL, where a shortcut-based flat policy replaces hierarchical structures, improving efficiency while maintaining strong results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Big efficiency gains while maintaining or exceeding baseline performance.\n- The paper is well-written and the method is well documented.\n- Figure 8 in the appendix shows the strength of SSCP against shortcut models and makes the case that relying on bootstrap targets induces instability in the training. **I think that this figure is important since it clearly motivates the use of SSCP and thus would like to see it in the main text.**"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "6fddAWB3qJ", "forum": "u6vDv51J9o", "replyto": "u6vDv51J9o", "signatures": ["ICLR.cc/2026/Conference/Submission15269/Reviewer_o22J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15269/Reviewer_o22J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925268407, "cdate": 1761925268407, "tmdate": 1762925570243, "mdate": 1762925570243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers generative modeling in RL, like diffusion models and flow-matching. Although these models have made great progress, they always rely on high inference costs and training instability caused by iterative sampling. To address this, this work proposes the Single-Step Completion Policy (SSCP), with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. This method can be extended into offline RL, offline-to-online RL, and online RL. This method is verified across various settings and different environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well written and easy to follow.\n\n- Stable training and efficient sampling are core concerns in diffusion policies in RL.\n\n- The proposed method is flexible to different settings, like offline RL, online RL, and offline-to-online RL."}, "weaknesses": {"value": "- The Q update function (4) utilizes the standard TD error. However, various works in offline RL propose that there is an overestimation error of the Q function caused by the distribution shift. Thus, several works will choose conservative Q learning techniques like IQL. What about the performance of using IQL in the offline setting?\n\n- What is the difference between online RL and offline RL when applying SSCP?\n\n- In offline-to-online experiments like Fig.4 and Fig.5, it seems that online fine-tuning in various environments can not improve the performance. Is there any explanation?\n\n- It is better to add offline-to-online and online RL experiments to the main text.\n\n- There are still some diffusion policies for RL that need to be discussed, including online fine-tuning [1-3] and offline diffusion planners [4-6].\n\nRef:\n\n[1] Policy agnostic RL: Offline RL and online RL fine-tuning of any class and backbone\n\n[2] Exploratory Diffusion Model for Unsupervised Reinforcement Learning\n\n[3] Efficient Online Reinforcement Learning for Diffusion Policy\n\n[4] What makes a good diffusion planner for decision making?\n\n[5] Simple hierarchical planning with diffusion\n\n[6] Latent diffusion planning for imitation learning"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hoE2tqYanL", "forum": "u6vDv51J9o", "replyto": "u6vDv51J9o", "signatures": ["ICLR.cc/2026/Conference/Submission15269/Reviewer_a6ST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15269/Reviewer_a6ST"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927510155, "cdate": 1761927510155, "tmdate": 1762925569305, "mdate": 1762925569305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}