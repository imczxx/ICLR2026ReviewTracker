{"id": "Nt0cnag6n8", "number": 7641, "cdate": 1758030248841, "mdate": 1759897841722, "content": {"title": "Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models", "abstract": "Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.", "tldr": "", "keywords": ["zero-shot", "video restoration", "video enhancement", "video diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31c1d5a530391a341a60f6d323a7d58f46c93eda.pdf", "supplementary_material": "/attachment/50d45ca80335b5233f46dd7474806949b077fa49.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ZVRV, a training-free framework that leverages video diffusion models to improve temporal consistency in zero-shot video restoration. Key contributions include homologous and heterogeneous latent fusion methods (enabling the use of any T2V model regardless of VAE compatibility), a COT-based strategy for adaptive fusion-weight selection, and temporal post-processing with I2V models. Experiments show 73-84% reduction in temporal flickering across super-resolution and low-light enhancement tasks, achieving state-of-the-art results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper identifies and addresses a real limitation: temporal flickering when applying image restoration methods to videos by leveraging temporal priors from pre-trained video diffusion models.\n- The key technical insight enabling cross-VAE latent fusion through encode-decode operations is elegant and practical. This allows the framework to utilize state-of-the-art T2V models (CogVideoX, HunyuanVideo) regardless of VAE compatibility.\n- Demonstrates 73-84% reduction in temporal flickering (Warping Error) while maintaining/improving visual quality metrics. The method outperforms both supervised and zero-shot baselines across three different tasks.\n- The framework requires no training and works with any LDM-based restoration method, making it immediately practical and deployable.\n- Applying chain-of-thought reasoning with test-time verification to select fusion ratios is an interesting adaptation of LLM techniques to diffusion models."}, "weaknesses": {"value": "- The method runs multiple models simultaneously (image restoration + homologous T2V + heterogeneous T2V + I2V post-processing), and the COT strategy requires M+1 forward passes at each timestep for three different fusion ratios (\\lambda^F1, \\lambda^F2, \\lambda^F). This represents massive computational overhead, yet the paper provides no runtime analysis, memory requirements, or comparison with baselines.\n- The paper's core contributions have limited novelty. Homologous fusion is acknowledged as similar to FVDM (Lu et al. 2024), adapted from video editing to restoration, and heterogeneous fusion via VAE encode-decode is relatively straightforward. The COT application to fusion ratio selection, while interesting, feels more like an engineering trick than a fundamental contribution. The central insight is essentially \"use video diffusion models to reduce temporal flickering,\" which is intuitive but not deeply novel.\n- The evaluation uses very small test sets (18 videos for super-resolution, 10 for enhancement), raising concerns about generalization, and resolution is severely restricted to 576x320 due to computational constraints, which doesn't reflect real-world high-resolution video restoration needs. The paper lacks failure case analysis or discussion of when the method doesn't work, and critical ablations are missing for key hyperparameters such as M and r in the COT strategy, and the choice of fusion schedules."}, "questions": {"value": "- Can you provide detailed runtime comparisons (wall-clock time, memory usage, FLOPs) against baseline methods? How many total forward passes does your method require per video with typical M values? What is the practical maximum resolution your method can handle on standard GPUs, and how does this scale?\n- How sensitive is performance to the choice of M (sample number) and r (range)? Can you provide ablation studies showing the performance vs. computational cost trade-offs for different M values? Why use COT sampling at every timestep rather than learning or predicting good fusion ratios?\n- The test sets are very small (18 and 10 videos). Have you evaluated on larger benchmarks? Can you provide results on standard video restoration datasets at full resolution? How does the method perform on longer videos (>50 frames)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SRFyg1OnD0", "forum": "Nt0cnag6n8", "replyto": "Nt0cnag6n8", "signatures": ["ICLR.cc/2026/Conference/Submission7641/Reviewer_jDUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7641/Reviewer_jDUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616327223, "cdate": 1761616327223, "tmdate": 1762919716232, "mdate": 1762919716232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a zero-shot video restoration framework that leverages diffusion generative priors for multiple degradation tasks without training. The method performs diffusion inversion on each frame and introduces a temporal alignment module for consistency across frames. The approach is applied to denoising, deblurring, and super-resolution tasks, showing competitive zero-shot performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed zero-shot setting is novel and practically motivated.  \n- Integrating diffusion priors with temporal alignment is conceptually elegant.  \n- The method demonstrates versatility across multiple restoration tasks."}, "weaknesses": {"value": "- Lacks comparison with recent diffusion-based video restoration models such as **Upscale-A-Video (CVPR 2024)** and **SeedVR (CVPR 2025)**, making it hard to gauge true competitiveness.  \n- No runtime, peak memory, or parameter analysis is provided, which limits understanding of efficiency and scalability.  \n• Temporal consistency evaluation is weak, reporting only **Warping Error (WE)** without metrics like **DOVER** or **tLPIPS**, which better reflect human-perceived temporal coherence and detail stability."}, "questions": {"value": "1. How does the method perform compared with diffusion-based video restoration baselines such as Upscale-A-Video or SeedVR  \n2. Can the authors report runtime, peak memory, and parameter counts for a fair efficiency analysis  \n3. Please include more temporal consistency metrics (e.g., DOVER, tLPIPS) to strengthen the evaluation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6tSxSivFmi", "forum": "Nt0cnag6n8", "replyto": "Nt0cnag6n8", "signatures": ["ICLR.cc/2026/Conference/Submission7641/Reviewer_FoHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7641/Reviewer_FoHs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926680877, "cdate": 1761926680877, "tmdate": 1762919715796, "mdate": 1762919715796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I think the paper grafts pre-trained T2V/I2V diffusion models onto image-based zero-shot IR to reduce flicker:  (1) homologous latent fusion when the 2D VAE matches.  (2) heterogeneous latent fusion via 2D↔3D VAE round-trips. (3) a “CoT-based” per-timestep fusion-ratio search with CLIP-IQA + warping-error as the verifier.  (4) A Stable Video Diffusion post-processing pass for extra temporal smoothing. On REDS/Vid4/UDM10/DAVIS and a small low-light set, the method beats PSLD and some zero-shot/editing baselines on PSNR/SSIM/LPIPS/CLIP-IQA/WE/FVD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) I think the goal is practical: using actual video priors to fix temporal instability in zero-shot diffusion IR.\n\n(2) The heterogeneous latent bridge (2D↔3D VAE encode/decode to align latents) is a straightforward engineering workaround that makes modern T2V usable.\n\n(3) The pipeline is training-free w.r.t. new networks and slots into several zero-shot IR backbones.\n\n(4) Results show lower WE/FVD and perceptual gains over PSLD-only variants; ablations indicate each block helps."}, "weaknesses": {"value": "(1) “First framework” is oversold. I think the novelty is thinner than claimed. Homologous fusion is basically FVDM-style latent mixing applied to restoration rather than editing; heterogeneous fusion is a vanilla encode–decode bridge between VAEs; and the “CoT-based” strategy is just a best-of-N hyperparameter search per timestep with two off-the-shelf metrics. Slapping “CoT” on a verifier-guided grid search doesn’t make it reasoning-based. The pitch feels buzzwordy rather than conceptually new.    \n\n(2) The “training-free” recipe quietly burns a lot of test-time compute. I think sampling multiple fusion ratios at every diffusion step for every clip is a huge multiplier on runtime. Then you post-process with SVD (inversion + EDM sampling) on top. There’s no honest wall-clock accounting or energy/latency comparison vs. stronger baselines at their own optimal step counts. This makes the method look practical on paper but painful in reality.   \n\n(3) Metric gaming and small-scale evaluation. The main verifier metric at test-time includes CLIP-IQA, which your own method optimizes against during ratio selection; unsurprisingly, you win it. Datasets are tiny (e.g., 18 videos for SR; 10 for low-light). Frames are even down-cropped to 576×320 “due to slow sampling, which conveniently hides high-res temporal artifacts. I think this undercuts the generality of the claims.  \n\n(4) Fairness & clarity of baselines. I think comparisons are muddy: you mix supervised VSR models (trained for the task) with zero-shot methods, and for diffusion baselines, you lock them to specific step counts instead of reporting speed–quality curves. The DavIS blind SR table shows large gains, but I don’t see solid controls that exclude metric bias from the verifier loop."}, "questions": {"value": "- What’s the end-to-end wall-clock (and GPU budget) per 1-second 1080p clip for your full pipeline (fusion-ratio search + SVD)? Give a step-wise breakdown.   \n\n- Verifier leakage: Since CLIP-IQA is used to select fusion ratios, do you also report metrics not used in selection (e.g., t-LPIPS variants, VMAF-temporal) where you didn’t tune?  \n\n- Please provide speed–quality trade-off curves for PSLD/other diffusion baselines (10/25/50 steps) and your method with/without SVD, at matched wall-clock."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XaPLm9zSzt", "forum": "Nt0cnag6n8", "replyto": "Nt0cnag6n8", "signatures": ["ICLR.cc/2026/Conference/Submission7641/Reviewer_jDRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7641/Reviewer_jDRk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974307244, "cdate": 1761974307244, "tmdate": 1762919715291, "mdate": 1762919715291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new video restoration method that leverages an existing image restoration model, along with homologous and heterogeneous T2V models. The key idea is to fuse the latents from different models to perform per-frame restoration while maintaining temporal consistency. A post-processing step is then applied using an I2V model to further enforce consistency. The method generally shows improved results when combined with an existing image restoration (IR) method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is unsupervised and training-free, leveraging existing pre-trained models, which makes it practical.\n\n2. The method generally shows improvements compared to the baseline or other compared methods."}, "weaknesses": {"value": "1. I think the method section could be much better presented and many details should be introduced. I struggled to fully understand the method.  How do you get the noise $z_T$ in line 201? Do you invert the degraded video? How do you use the T2V model to generate a video similar to the input one?  Which text prompt are you using? In line 182, you mentioned that your input is only a video. \n\n2. Based on my understanding, I think the performance depends heavily on the hyperparameters used for latent fusion, which makes the method seem somehow ad hoc.\n\n3. The method applies multiple models, which makes it computationally expensive. A computational comparison is needed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P0nCis8jem", "forum": "Nt0cnag6n8", "replyto": "Nt0cnag6n8", "signatures": ["ICLR.cc/2026/Conference/Submission7641/Reviewer_vAKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7641/Reviewer_vAKE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989635611, "cdate": 1761989635611, "tmdate": 1762919714695, "mdate": 1762919714695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}