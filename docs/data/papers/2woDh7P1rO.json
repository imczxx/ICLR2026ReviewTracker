{"id": "2woDh7P1rO", "number": 3454, "cdate": 1757431216367, "mdate": 1759898089253, "content": {"title": "Adaptive Multi-Scale Attention-Based LSTM Coupling for Early Detection", "abstract": "This paper introduces a novel adaptive, attention-coupled Long Short-Term Memory (LSTM) architecture developed specifically for real-time scenario recognition and prediction in complex automotive electrical/electronic (E/E) systems. Modern vehicles generate rapidly growing data streams from signals such as current, voltage, and temperature. We address this by monitoring critical signal patterns via a fused LSTM. The proposed dual-path methodology comprises a trend path for long-term pattern modeling and a motif path for short-term pattern recognition, coupled via a bidirectional, attention-based gating mechanism that enables dynamic information exchange. The outputs provide a reliable basis for initiating high-resolution data capture or adaptive system responses once a scenario is identified with high confidence. Experimental results demonstrate significant reductions in mean squared error compared to the individual values and interpretable attention weights that reveal information-exchange patterns. The proposed approach enables robust, noise-resilient forecasts and allows for efficient, data-driven development for future EE architectures.", "tldr": "We propose an adaptive, dual-path attention-coupled LSTM that fuses short-term motif detection with long-term trend modeling to enable real-time scenario recognition and prediction in automotive E/E systems.", "keywords": ["LSTM", "Time Series Forecasting", "Early Detection", "Attention"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/362ead38e82ddbf78f284d7b0d81e2ffc52436ca.pdf", "supplementary_material": "/attachment/0dbf38ac77da1402effd548747e56f0d1400b3e6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an adaptive attention-coupled LSTM architecture. This architecture employs two separate LSTMs to process data with different window lengths, thereby capturing information across distinct temporal domains. It integrates information from the two pathways via cross-attention with adaptive weights and generates predictions for multiple time horizons. Specifically designed for real-time time-series forecasting and scenario detection in complex automotive electrical/electronic (E/E) systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.Clear Methodological Logic. The core idea of the adaptive attention-coupled LSTM is straightforward and well-structured. The technical route from problem definition to solution implementation is logical and easy to follow.  \n2.Tailored Weight Parameter Schemes for Different Scenarios. The paper designs distinct weight parameter strategies for various application scenarios. These scenario-specific weight designs enhance the method’s adaptability to diverse temporal patterns in automotive E/E system data."}, "weaknesses": {"value": "1.The proposed method has not been validated through experiments on other types of data.\n2.Lack of Justification for the Addressed Problem. The paper fails to sufficiently demonstrate the necessity and urgency of the problem it claims to solve—like the limitations of existing methods in real-time time-series forecasting for automotive E/E systems. \n3.Limited Innovation and Oversimplified Methodology. The adaptive attention-coupled LSTM primarily combines existing techniques (dual-pathway LSTMs + cross-attention) without introducing fundamental innovations. \n4.No Demonstration of Runtime Efficiency for Real-Time Scenarios. While the paper claims the method is designed for \"real-time scenarios,\" it provides no quantitative data on runtime efficiency. \n5.Insufficient Experiments and Lack of Baseline Comparisons. The experimental scope is narrow, and there is a lack of comparative data with mainstream baseline methods. For example:  The paper does not include benchmarks widely used in automotive time-series forecasting; It fails to validate performance on publicly available automotive E/E dataset, relying instead on potentially custom or synthetic data.  \n6.Low Paper Completion. No Clear Architectural Schematic. The paper lacks explicit architectural schematic diagrams and related works.\n7.Narrow Application Scope. The method is exclusively designed for automotive E/E systems and has not been validated on other time-series scenarios. This limits the method’s generalizability and academic impact.\n8.Lack of Ablation Studies and Parameter Sensitivity Analysis. There are no ablation experiments to validate the effectiveness of key components (e.g., adaptive weights, cross-attention) or analyze the impact of hyperparameters (e.g., time window length, number of hidden units). It is impossible to determine whether the improved performance stems from the proposed design or merely increased model parameters."}, "questions": {"value": "1.More experiments are needed. \n2. See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oP3nztXYhm", "forum": "2woDh7P1rO", "replyto": "2woDh7P1rO", "signatures": ["ICLR.cc/2026/Conference/Submission3454/Reviewer_x4J2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3454/Reviewer_x4J2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761474933345, "cdate": 1761474933345, "tmdate": 1762916731127, "mdate": 1762916731127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Adaptive Multi-Scale Attention network for time series forecasting, specifically motivated by early anomaly detection and scenario prediction in automotive systems. \nThe main idea is to separate short-term and long-term dependencies into 2 specialized LSTM branches (“dual-path”), which exchange information through bidirectional scaled dot-product attention.\nA 3-stage training schedule (specialization, coupling ramp-up, joint refinement) is introduced, along with an adaptive feature-weighting mechanism to emphasize dynamically relevant inputs.\nExperiments on a synthetic dataset show a significant reduction in Mean Squared Error compared to isolated LSTM baselines.\nWhile the architecture is conceptually sound and intuitively appealing, the novelty is not clearly demonstrated with respect to prior multi-scale or hierarchical LSTM approaches, and the evaluation is restricted to synthetic data. The paper would benefit from clearer theoretical justification and improved reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of decoupling short and long-term temporal patterns and coupling them via attention is intuitive and could have practical advantages.\n- The 3-phase schedule is well thought out and potentially generalizable.\n- The reported improvements in MSE demonstrate the model’s potential effectiveness in controlled scenarios.\n- The paper includes implementation information, which, if integrated into the main text, could help reproducibility and lead to a better understanding of the approach."}, "weaknesses": {"value": "- The paper could benefit from better motivation.\n- The authors do not clearly differentiate their proposal from prior work, particularly in the Related Work section.\n- All results are based on synthetic data with Gaussian noise, and there is no testing on real or public benchmarks, which limits the applicability and impact of the findings.\n- No code or data repository has been provided.\n- There are contradictions within the paper, especially regarding the notation and ranges for hyperparameters.\n- The description of the methodology is vague and contains excessive repetition of certain concepts (e.g., dual path or short vs. long). Some explanations are found only in the appendix. A more comprehensive description of the proposal should be included in the main body of the paper."}, "questions": {"value": "1) Please clarify the discrepancy regarding β_max: the main text limits it to [0.1, 0.8], while Appendix A.2.1 lists 1.5 as optimal and 2.0 as the maximum limit.\n2) The main text uses “d” for the dropout rate, whereas the appendix uses p_drop. Please confirm which notation is correct.\n3) Regarding adaptive windowing: Section 3.3 mentions averaging over 10 steps, while Appendix A.6.2 refers to 50 steps. What is the actual configuration used in the experiments?\n4) How exactly does LSTM A “dynamically adapt”? The methodology is not clearly explained.\n5) What optimization strategy was utilized for hyperparameter optimization?\n6) Considering that the dataset is synthetic, can this approach be tested on a publicly available time series dataset?\n7) Many important methodological choices (e.g., preprocessing steps, window sizes, data generation parameters, hyperparameter ranges) are only located in the appendices. Could the authors summarize or move this information to the main text to clarify the overall pipeline and justify these design choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lL1nDi87iZ", "forum": "2woDh7P1rO", "replyto": "2woDh7P1rO", "signatures": ["ICLR.cc/2026/Conference/Submission3454/Reviewer_thjK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3454/Reviewer_thjK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668025236, "cdate": 1761668025236, "tmdate": 1762916730969, "mdate": 1762916730969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core challenges of real-time scene recognition in complex automotive electronic and electrical systems and proposes a breakthrough solution - the adaptive attention Coupled LSTM architecture. This study has for the first time systematically constructed a dual-path time series modeling paradigm that combines specialization and collaboration, fundamentally resolving the inherent contradiction of a single time series model in capturing long and short time dependencies. The core theoretical innovation lies in the introduction of a bidirectional and adaptive attention coupling mechanism, which serves as an \"intelligent information hub\" connecting the two specialized paths. To achieve the optimal collaborative performance, the author designed a progressive multi-stage training protocol. Through independent expertise cultivation, progressive coupling introduction, and global joint optimization, it ensures that the model is both highly specialized and collaborative. There exist some issues to be addressed as follows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper addresses the core challenges of real-time scene recognition in complex automotive electronic/electrical (E/E) systems, proposing a novel and breakthrough solution—the adaptive attention-coupled LSTM architecture.\n\n2.It represents the first systematic construction of a dual-path time series modeling paradigm that effectively combines specialization and collaboration, fundamentally resolving the inherent contradiction in single models for capturing both long- and short-term dependencies.\n\n3.The core theoretical innovation lies in the introduction of a bidirectional and adaptive attention coupling mechanism, which acts as an \"intelligent information hub\" to dynamically connect the two specialized paths, enhancing information exchange.\n\n4.The authors designed a progressive multi-stage training protocol (including independent expertise cultivation, gradual coupling introduction, and global joint optimization) to ensure optimal collaborative performance, balancing specialization and integration."}, "weaknesses": {"value": "1.There is a lack of experimental data or justification for the design choices regarding the number of hidden units in LSTM A (24-32) and LSTM B (32-48). The paper does not provide empirical evidence or ablation studies to validate why these specific ranges were selected.\n\n2.The paper fails to compare the proposed model with recent state-of-the-art time series prediction architectures, such as Transformer-based models (e.g., Informer, FEDformer), which have demonstrated advantages in capturing multi-scale dependencies. This omission limits the comprehensiveness of the evaluation.\n\n3.The computational complexity and memory usage of the dual LSTM paths, combined with the attention mechanism, are not thoroughly analyzed. There is no deployment feasibility study or lightweight experiments for real-time inference on in-vehicle embedded devices, raising concerns about practical applicability.\n\n4.The experiments are entirely based on synthetic data and lack validation on real-world automotive E/E system data. This reduces the reliability and generalizability of the results for actual automotive applications."}, "questions": {"value": "1. Is there any experimental data to verify why the number of 24-32 and 32-48 hidden units, respectively, used by LSTM A and LSTM B was designed in this way?\n\n2. Why not compare it with the time series prediction architectures that have performed well in recent years, such as Transformer-based models (such as Informer, FEDformer)? These models also have their advantages in capturing multi-scale dependencies.\n\n3. How much does the computational complexity and memory usage of dual LSTM paths combined with the attention mechanism increase compared to a single LSTM baseline? Is there a deployment feasibility analysis or a lightweight experiment for real-time inference of in-vehicle embedded devices?\n\n4. The experiments are entirely based on synthetic data and have not been verified on real automotive E/E system data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "z45az9BpT3", "forum": "2woDh7P1rO", "replyto": "2woDh7P1rO", "signatures": ["ICLR.cc/2026/Conference/Submission3454/Reviewer_dXjN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3454/Reviewer_dXjN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793548640, "cdate": 1761793548640, "tmdate": 1762916730635, "mdate": 1762916730635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a deep learning based time-series prediction model comprising two data encoding towers, one based on short-term data (past 6 time-steps) and another based on long-term data (past 15 time-steps). The final prediction is based on an attention coupling of predictions from the short-term and long-term towers to obtain the `enhanced` final prediction. The authors evaluate the proposed method on synthetic data that they have generated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a method for an important problem of scenario recognition and prediction in the automotive system context. \n\n2. Overall, the de-coupled modeling approach to estimate the short-term and long-term patterns are intuitive and the attention based mechanism to couple representations from the two towers is somewhat novel."}, "weaknesses": {"value": "1. The paper is difficult to understand and lacks cohesion as many of the architecture choices seem somewhat arbitrary and scattered across sections 3 and 4. The paper (especially the methodology description) would benefit from a significant re-write. Several sections in the appendix can be eliminated. For example A.1.1 and A.1.2 are unnecessary as the machine learning audience is familiar with recurrent architectures like the LSTM and the attention mechanisms.\n\n\n2. Overall, the paper lacks rigorous evaluation on real-world data and has only been evaluated on synthetic data. Further, the dataset generation procedure has not been described in detail.\n\n3. There is no rigorous baseline comparison with other popular baselines e.g., state-space models like MAMBA."}, "questions": {"value": "1. How are $w_{i,base}$ and $\\gamma_i^{(t)}$ estimated? The text says $\\gamma_i^{(t)}$ reflects the \"average contribution\" of each feature but it is unclear how this is derived / what \"contribution\" means.\n\n2. How is $\\beta^t$ estimated?\n\n3. What are the various hyper-parameters that need to be tuned if the proposed method is to be adapted to a new dataset?\n\n4. Why has no comparison been conducted with state of the art baselines (e.g., Mamba [1], standard autoregressive based and single-layer linear [2] models) which have shown effective performance in time-series forecasting scenarios?\n\n5. Why is there no ablation study conducted to highlight the importance of various components (e.g., short-term, long-term and attention based components)? This is imperative to holistically understand how the various components contribute to the overall performance. \n\n6. Are there any real-world datasets on the automobile real-time scenario recognition and prediction use-case that can be employed to test the effectiveness of the proposed method? Synthetic data is useful but cannot serve as a comprehensive evaluation of the performance of the proposed method. \n\n# References\n\n1. Wang, Zihan, Fanheng Kong, Shi Feng, Ming Wang, Xiaocui Yang, Han Zhao, Daling Wang, and Yifei Zhang. \"Is mamba effective for time series forecasting?.\" Neurocomputing 619 (2025): 129178.\n\n2. Zeng, Ailing, Muxi Chen, Lei Zhang, and Qiang Xu. \"Are transformers effective for time series forecasting?.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 37, no. 9, pp. 11121-11128. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TVBfuCbv3A", "forum": "2woDh7P1rO", "replyto": "2woDh7P1rO", "signatures": ["ICLR.cc/2026/Conference/Submission3454/Reviewer_emjW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3454/Reviewer_emjW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929056478, "cdate": 1761929056478, "tmdate": 1762916730343, "mdate": 1762916730343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}