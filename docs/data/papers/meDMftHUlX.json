{"id": "meDMftHUlX", "number": 9103, "cdate": 1758111292435, "mdate": 1763711490573, "content": {"title": "Distribution-Aware Multi-Granularity Phase Coding: Towards Lower Conversion Error for Spike-Driven Large Language Models", "abstract": "Spiking large language models (LLMs) offer significant advantages on neuromorphic hardware, yet training them from scratch remains prohibitively expensive. A promising alternative is ANN-to-SNN conversion, which reuses pretrained ANN weights while minimizing conversion error. \nHowever, existing conversion frameworks neglect activation distributions, as reflected in SNN neurons with rate or temporal coding to map uniformly distributed rather than distribution-aligned discrete values, thus causing latent conversion error arising from distribution misalignment. \nTo tackle this problem, we propose a distribution-aware multi-granularity phase coding approach, which achieves reasonable discrete value allocation by minimizing conversion error relative to activation distributions. \nSpecifically, multi-granularity phase coding extends conventional phase coding with multiple learnable bases, incorporating representational capacity across different granularities. \nBuilding on this coding scheme, we further propose a novel ANN-to-SNN conversion paradigm designed towards lower conversion error. \nIn particular, our paradigm utilizes the activation distributions of hidden layers to sample data for cost-efficient neuron training, without requiring fine-tuning of model weights. \nTheoretically, we provide a convergence guarantee for the neuron training algorithm. \nExtensive experiments on the LLaMA model confirm the effectiveness of both our coding scheme and conversion paradigm.\nConcretely, our spiking LLM attains the lowest perplexity with ANN-level accuracy, accompanied by a 42\\% reduction in energy consumption of MAC and AC operations.", "tldr": "", "keywords": ["Spiking Neural Network", "Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b17b0d826ff2ced570a7de0a463adda7d289e24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the potential conversion error caused by activation distribution mismatch when converting pre-trained ANN LLMs to SNN LLMs. The authors propose a distribution-aware multi-granularity phase coding approach, which aims to achieve a reasonable discrete value allocation by minimizing conversion error relative to the activation distributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Distribution-aware multi-granularity phase coding is a novel and well-motivated method (derived from quantization distortion in information theory), providing a new approach to solve the distribution mismatch problem in ANN-SNN conversion.\n\n2. Experimental results (on LLaMA-2 and LLaMA-3 models) show that this method achieves SOTA conversion performance (close to the ANN's PPL and accuracy), and the time cost for conversion (i.e., training the neurons) is extremely low (within minutes)."}, "weaknesses": {"value": "I like the distribution-aware conversion ideas and appreciate the authors' effort in fine-tuning SNNs on LLMs within minutes, but my main concern lies in the energy claims：\n\n 1. End-to-end energy saving claim: In both the abstract and Section 6.3, the authors declare that \"On commodity GPUs,\" their SNNs achieve an \"end-to-end\" reduction compared with their ANN counterparts, which is a very strong claim. However, upon checking Table 3, I found that the energy saving calculation relies only on the count of MACs and ACs. The energy for memory access, data movement, and control logic is not considered. SNNs may suffer more from these overheads due to their long spike train representations and multiple weight accesses across the time window. \n2.  Furthermore, on GPUs, QNNs benefit from regular memory access patterns that enable efficient data reuse opportunities, whereas SNNs suffer from sparse and irregular memory access patterns. Can the authors explain how their SNNs running on GPUs solve these problems?Thus, if the authors want to claim the \"end-to-end\" \"42%\" energy consumption reduction, I recommend that they either: Consider these hardware overheads and discuss how to implement SNNs efficiently on GPUs; Measure the actual power when running the SNN and ANN on their GPU and multiply it by the running time to show the actual energy consumption on GPUs; Run their models on neuromorphic chips like Loihi and compare the energy.\n3. Also, the spiking Softmax and RMSNorm (Appendix B) seem to still have complex multiplication or exponentiation operations. Is there any way to do a simulation of these functions and also apply it to the ANN-to-SNN conversion paradigm? If not, on GPUs, was the energy consumption of these exp and sqrt operations, which run at every timestep T, included in the total energy calculation in Table 3?\n\nMinor Points:\n\n1. In Figure 1a, the meanings of the x and y axes should be labeled on the figure to be more intuitive.\n\n2. In Figure 2, the dimensions of Q, K, and V should include the time domain (e.g., n*d*T).\n\n3. For the energy comparison, the paper should also include a comparison with other SpikingLLM papers, like those mentioned in Table 1.\n\n4. If the authors tried weight quantization, does the distribution of each layer change? What are the results for, e.g., 4-bit or 8-bit weights for Spiking LLMs, since the baseline SpikeLLM can reduce the weight to 4-bits?\n\n5. Potential complexity of multi-granularity tuning: Although multi-granularity phase coding improves representational capacity, learning and optimizing multiple bases may be more complex than the traditional method with a single base. Can author discuss more about the energy overhead?"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qQortB61rw", "forum": "meDMftHUlX", "replyto": "meDMftHUlX", "signatures": ["ICLR.cc/2026/Conference/Submission9103/Reviewer_wbNY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9103/Reviewer_wbNY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470698564, "cdate": 1761470698564, "tmdate": 1762920803202, "mdate": 1762920803202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a ANN to SNN conversion method that accounts for the distribution of activation values."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is shown to improve accuracy at a reasonable cost in the experiments."}, "weaknesses": {"value": "The opening claim that activation distribution was not considered in previous work is clearly incorrect. The following are some examples that do just that - and not compared or even cited in the paper:\n\n. Li, C., Ma, L., & Furber, S. (2022).\nQuantization Framework for Fast Spiking Neural Networks (QFFS). Frontiers in Neuroscience.\nAvailable at: https://www.frontiersin.org/articles/10.3389/fnins.2022.918793/full\n\n2. Moser, B. A., & Lunglmayr, M. (2023).\nQuantization in Spiking Neural Networks. arXiv preprint arXiv:2305.08012.\nAvailable at: https://arxiv.org/abs/2305.08012\n\n3. Guo, Y., et al. (2023).\nRMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).\nAvailable at: https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_RMP-Loss_Regularizing_Membrane_Potential_Distribution_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf\n\nThe time steps of 8 is also relatively large compared to recent work. \n\nOverall, while I can see the approach seems intuitively on the right path, the evaluation needs to be improved."}, "questions": {"value": "1. How does the time step size affect the conclusions?\n\n2. Does it work for other models and data sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QqQep8taBU", "forum": "meDMftHUlX", "replyto": "meDMftHUlX", "signatures": ["ICLR.cc/2026/Conference/Submission9103/Reviewer_qqbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9103/Reviewer_qqbp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826572119, "cdate": 1761826572119, "tmdate": 1762920802853, "mdate": 1762920802853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Alternating Optimization Neuron Training algorithm for directly training Spiking Large Language Models (Spiking LLMs) based on multi-granularity phase coding. The core innovation lies in decoupling the spiking threshold θ(t) and reset mechanism h(t) from the traditional decoding factor d(t), treating them as independent, learnable parameters. By alternately optimizing these learnable parameters with the network weights, the method aims to minimize information loss during the ANN-to-SNN conversion, thereby achieving high accuracy while significantly reducing energy consumption. The paper claims state-of-the-art results in both performance and energy efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed \"multi-granularity phase coding\" framework and the idea of decoupling θ(t) and h(t) from d(t) are innovative.\n\n2.The paper provides a convergence proof for the proposed alternating optimization algorithm."}, "weaknesses": {"value": "1.The paper does not sufficiently demonstrate the necessity of the proposed decoupling mechanism. It lacks critical ablation studies to show how much performance would degrade if θ(t) and h(t) were not decouple.\n\n2.The paper mentions a 42.0% energy reduction but does not specify the baseline or the evaluation methodology. Is it an estimate based on spike count in a simulator? Or is it actual power consumption measured on specific hardware?"}, "questions": {"value": "1.In the alternating optimization process, how are the convergence speed and stability of the two steps—optimizing weights B with fixed thresholds o, and optimizing thresholds o with fixed weights B? Is one step dominant? Have you tried joint optimization instead of alternating optimization?\n\n2.How are the learnable bases B in \"multi-granularity phase coding\" initialized? What is their evolution trend during training?\n\n3.The paper assumes a fixed total number of timesteps T. In practical inference, can true \"event-driven\" or \"adaptive timestep\" computation be achieved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yV1MovcwgT", "forum": "meDMftHUlX", "replyto": "meDMftHUlX", "signatures": ["ICLR.cc/2026/Conference/Submission9103/Reviewer_xCuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9103/Reviewer_xCuD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985159573, "cdate": 1761985159573, "tmdate": 1762920802456, "mdate": 1762920802456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the issue of mismatched non-uniform activation value distribution when converting LLM to SNN, this paper introduced Distribution-Aware Multi-Granularity Phase Coding with multiple learnable bases and proposed the Alternating Optimization Neuron Training Algorithm to optimize the parameters of SNN neurons, thereby reducing the conversion time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The structure and writing of this paper are good and easy to understand. In terms of reducing conversion time, it improves efficiency and may be a promising method."}, "weaknesses": {"value": "1. This paper claims that spiking large language models (LLMs) have significant advantages on neuromorphic hardware, but the final conclusion is that the energy consumption is reduced by 42% on GPUs. Moreover, the way energy consumption is presented is extremely unscientific and rigorous, which seriously undermines the technical contribution and persuasiveness of the paper. First, the paper cites Horowitz's 2014 proposal that \"1 MAC ≈ 4.6 pJ and 1 AC ≈ 0.9 pJ under 45 nm CMOS technology\" as the basis for energy consumption conversion, but there is no specific model of \"commodity GPUs\". The energy consumption designs of different GPUs vary greatly. The mainstream process of current \"commercial GPUs\" has entered 3~8 nm, and there is a technical generation gap of more than several decades between the two. The deviation between the calculation results and the actual energy consumption of the hardware is at least an order of magnitude. The methodology of estimating the actual energy consumption of contemporary GPUs using the energy consumption coefficient of 45 nm is completely invalid [1][2]. Secondly, the energy consumption of LLM does not only come from \"computational operations (MAC/AC)\". In contemporary GPUs, the energy consumption of memory access (i.e., HBM, SRAM read and write), data movement (data transmission in PCIe), and control unit scheduling often exceeds the computational energy consumption. Moreover, LLM inference is memory bound, and memory energy consumption accounts for the vast majority of the total energy consumption. The paper only estimates the total energy consumption based on the number of MAC/AC, completely ignoring key energy consumption items such as memory access and data movement, which has no correlation with the \"end-to-end energy consumption\" of the actual GPU operation.\n\n2. The core problem pointed out by the authors of this paper is not new: conversion error arising from distribution misalignment. This problem does not only exist in the conversion of LLM to SNN, but is a long-standing inherent problem [3], and it still exists in the conversion of CNN to SNN.\n\n3. The Distribution-Aware Multi-Granularity Phase Coding in this paper actually adjusts the parameters such as h(t) and d(t) in FS neurons to multi-granularity learnable parameters, while the parameters of the original FS neurons are also learnable and adjustable, and there is no essential difference [4].\n\n4. The ablation study in Table 5 shows that when the granularity increases from 1 to 2 or 3, the performance gain is very limited, which means that the core contribution of the paper is actually the learnable B, rather than \"multi-granularity\". Compared with a carefully optimized \"single-granularity distribution-aware phase coding\", what is the real advantage of multi-granularity? It can be seen from Tables 1, 2, and 6 that when the time step is large (T=10), the performance gain by increasing the Grain is negligible, while when the time step is slightly smaller (T=8), the change in the result by changing the grain is slightly larger. Does this mean that the time step T is also an influencing factor? Therefore, an ablation experiment of time step T and grain should be added.\n\n5. The meaning of the formula is ambiguous or even ambiguous: In formula (3), the author writes \"Typically, $h(t)$, $d(t)$, and $\\theta(t)$ are specified as $B^{-t}$ as follows:$v(t + 1) = v(t) - B^{-t}s(t),\\ O(t) = B^{-t}s(t),\\ s(t) = \\Theta\\left(v(t)-B^{-t}\\right).$\" But in formula (8), it is claimed that \"we denote $\\{h(t)\\}_{t=1}^T$, $\\{\\theta(t)\\}_{t=1}^T$, $\\{d(t)\\}_{t=1}^T$, $\\{B_i\\}_{i=1}^n$ as $h$, $\\theta$, $d$, and $B$\" So what is the final expression?\n\n6. There is a lack of comparison with existing works. Only SpikeLLM, SpikedAttention, and LAS are compared, and more extensive control experiments are lacking.\n\n[1] https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf?utm_source=chatgpt.com\n\n[2] https://resources.nvidia.com/en-us-gpu-resources/h100-datasheet-24306?utm_source=chatgpt.com\n\n[3] Can Deep Neural Networks be Converted to Ultra  Low-Latency Spiking Neural Networks\n\n[4] Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes"}, "questions": {"value": "The problem is as described above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgJ9FO7WyP", "forum": "meDMftHUlX", "replyto": "meDMftHUlX", "signatures": ["ICLR.cc/2026/Conference/Submission9103/Reviewer_fm6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9103/Reviewer_fm6A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166668560, "cdate": 1762166668560, "tmdate": 1762920801927, "mdate": 1762920801927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}