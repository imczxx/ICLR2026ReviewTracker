{"id": "XNk56rmmiy", "number": 11551, "cdate": 1758201540233, "mdate": 1759897568338, "content": {"title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization", "abstract": "Recent advances in large language models (LLMs) have enabled the emergence of general-purpose agents for automating end-to-end machine learning (ML) workflows, including data analysis, feature engineering, model training, and competition solving. However, existing benchmarks remain limited in task coverage, domain diversity, difficulty modeling, and evaluation rigor, failing to capture the full capabilities of such agents in realistic settings.\nWe present TAM Bench, a diverse, realistic, and structured benchmark for evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three key innovations:\n(1) A browser automation and LLM-based task acquisition system that automatically collects and structures ML challenges from platforms such as Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities (e.g., tabular, text, image, graph, audio);\n(2) A leaderboard-driven difficulty modeling mechanism that estimates task complexity using participant counts and score dispersion, enabling scalable and objective task calibration;\n(3) A multi-dimensional evaluation framework incorporating performance, format compliance, constraint adherence, and task generalization. \nBased on 150 curated AutoML tasks, we construct three benchmark subsets of different sizes—Lite, Medium, and Full—designed for varying evaluation scenarios. The Lite version, with 18 tasks and balanced coverage across modalities and difficulty levels, serves as a practical testbed for daily benchmarking and comparative studies.", "tldr": "", "keywords": ["Benchmark", "Large Language Models", "Language Agents", "End-to-End Machine Learning", "Evaluation Framework", "Data Science Automation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc47f1ab1286144f95cf275cbc8432dbcb7a4ec9.pdf", "supplementary_material": "/attachment/289fb548ac967e3ae9c96aa381096d83e06850c4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TAM Bench, a diverse, realistic, and structured benchmark for evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three key innovations: (1) A browser automation and LLM-based task acquisition system that automatically collects and structures ML challenges; (2) A leaderboard-driven difficulty modeling mechanism that estimates task complexity using participant counts and score dispersion, enabling scalable and objective task calibration; (3) A multi-dimensional evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Automation and Scalability: The Web-Agent-driven task acquisition method improves task collection efficiency.\n﻿\n2. Objective Difficulty Modeling: The leaderboard-based difficulty assessment is more objective and scalable than previous manual time estimates.\n﻿\n3. Enhanced Benchmark Diversity: The Full version offers significantly broader coverage across data modalities and application domains.\n﻿\n4. Comprehensive Multi-Dimensional Evaluation: The inclusion of Constraint Adherence and Format Compliance metrics effectively addresses the limitations of single-metric evaluations in existing benchmarks."}, "weaknesses": {"value": "The evaluation relies on an LLM (e.g., GPT-4) as the judge. The paper, however, does not discuss whether LLM-based evaluation can faithfully and objectively reflect the true capabilities of the models. It is suggested that necessary experiments be added to demonstrate (1) the gap between LLM evaluation and human evaluation, (2) the reliability of different LLM judges, and (3) whether GPT-4 can be replaced by an open-source model, especially given the relatively high cost of calling the GPT-4 API."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "t75Ur1iLwN", "forum": "XNk56rmmiy", "replyto": "XNk56rmmiy", "signatures": ["ICLR.cc/2026/Conference/Submission11551/Reviewer_ghri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11551/Reviewer_ghri"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761072287598, "cdate": 1761072287598, "tmdate": 1762922641571, "mdate": 1762922641571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors aim to address several limitations of existing agent benchmarks—such as high manual annotation cost, imbalanced task distribution, and poorly calibrated task difficulty. To this end, they propose TAM-Bench, a diverse, realistic, and well-structured benchmark for evaluating LLM-based agents on end-to-end machine learning tasks. While the benchmark demonstrates clear advantages in terms of task diversity and scale, there remain notable shortcomings in the overall framework of its construction and evaluation methodology."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The task scale is 150, much larger than existing benchmarks such as MLEBench (75 tasks).\n2. The proposed benchmark contains more task fields like commerce, which is important for real-scenarios."}, "weaknesses": {"value": "1. In this work, the authors propose a difficulty modeling method via leaderboard structure, with many details unclear and questionable.\n(1) Since they use the score from the participants to determine the task difficulty, is there any filter mechanism on the participants? If no, how to avoid the distribution shift led by the difference of participants?\n(2) Current inclusion of number of participants seems not reasonable. Is there any scene that one task is too difficult / heavy to run such that its number of participants would be 1/100 or even 1/1000 of other simple-to-run tasks? In such case, will the difficulty be influenced in a wrong way?\n(3) Given all factors except the \"mean score\" fixed in eq (3), we might conclude that the higher the mean score is, the more difficult the task is, which is not reasonable.\n\n2. While the format validity metric is reasonable to evaluate the performance of agents, I think previous benchmarks might in-explicitly consider it, i.e., if it does not follow to the format, its answer might not even be parsed. Furthermore, I would appreciate it if the authors would provide more details of the generation of format requirements: test_labels.csv. If it is inherit from the construction of the task, I wonder its validness and diversity to evaluate agents' capability on this.\n\n3. The evaluation of this benchmark is not sufficient. Only GPT-4.1 & Deepseek-V3 are tested, and their performance seems different from the common sense knowledge on these two models. Further analyses are expected.\n\n4. Please adjust the usage of \\cite, \\citep, \\citet in the latex."}, "questions": {"value": "See Weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kn7s2rIYuo", "forum": "XNk56rmmiy", "replyto": "XNk56rmmiy", "signatures": ["ICLR.cc/2026/Conference/Submission11551/Reviewer_zzJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11551/Reviewer_zzJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212322783, "cdate": 1761212322783, "tmdate": 1762922641078, "mdate": 1762922641078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TAM-Bench, a new benchmark designed to test how well LLM-based agents can handle end-to-end machine learning tasks. Instead of relying on manual curation, it automatically gathers and standardizes real competition tasks from sites like Kaggle using a web-agent system. It also estimates task difficulty from leaderboard data and evaluates agents across several aspects, including performance, constraint following, and output format correctness. In experiments with AIDE and OpenHands using GPT-4.1 and DeepSeek-V3, GPT-4.1 was generally more stable and reliable, while DeepSeek-V3 showed strong results on certain tasks. Overall, TAM-Bench aims to provide a more practical and scalable way to evaluate AutoML agents in realistic settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper presents an automated and scalable benchmark pipeline that reduces manual effort and ensures diverse task coverage.\n2.\tThe leaderboard-based difficulty modeling offers a more objective and reproducible way to assess task complexity.\n3.\tThe evaluation framework is comprehensive, considering both performance and practical constraints."}, "weaknesses": {"value": "1.\tThe experimental design is shallow. TAM-Bench evaluates two open-source AutoML agent frameworks, but each framework’s base language model includes only one open-source model (DeepSeek-V3) and one closed-source model (GPT-4.1). Evaluating only two models is far from comprehensive and cannot reflect the capability boundaries of diverse AutoML agents, offering limited value to the community.\n2.\tThe selection of base models is arbitrary. Excluding the Qwen series models simply because they “encountered JSON parsing errors during execution” is unreasonable, as this issue could be resolved through function calling or post-processing the responses. Furthermore, it is unclear why the authors chose DeepSeek-V3 instead of Llama-3 or other comparable language models.\n3.\tThe authors propose an automatic pipeline for benchmark construction, but they do not systematically discuss the quality of the synthesized data, nor do they conduct any manual quality inspection of the benchmark samples. I am seriously concerned about the reliability of the automatically generated data.\n4.\tThe writing is poor. For example, Figure 1 is never mentioned in the main text, and its caption fails to provide any meaningful information, which leaves readers confused."}, "questions": {"value": "1.\tTAM-Bench focuses on language model-based agents, so how does it handle inputs such as audio and images?\n2.\tThe evaluation metrics in TAM-Bench are all based on final submissions, yet in long-sequence agent tasks, assessing the intermediate process is also meaningful. Why does TAM-Bench only consider result-based metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IoNKb2h43G", "forum": "XNk56rmmiy", "replyto": "XNk56rmmiy", "signatures": ["ICLR.cc/2026/Conference/Submission11551/Reviewer_HQoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11551/Reviewer_HQoU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822869186, "cdate": 1761822869186, "tmdate": 1762922640548, "mdate": 1762922640548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}