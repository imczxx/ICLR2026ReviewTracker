{"id": "8TgzLrWgrk", "number": 1309, "cdate": 1756869745664, "mdate": 1759898216095, "content": {"title": "3DPhysVideo: 3D Scene Reconstruction and Physical Animation Leveraging a Video Generation Model via Consistency-Guided Flow SDE", "abstract": "Video generative models have made remarkable progress, yet they often yield visual artifacts that violate grounding in real-world physical dynamics. Recent works such as PhysGen3D tackle single image-to-3D physics through mesh reconstruction and Physically-Based Rendering, but challenges remain in modeling fluid dynamics and photorealism. This work introduces 3DPhysVideo, a novel training-free pipeline that generates physically realistic videos from a single image. We repurpose an off-the-shelf video model for two stages. First, we use it as a novel view synthesizer to reconstruct complete 360-degree 3D scene geometry by guiding the image-to-video (I2V) flow model with rendered point clouds derived from an initial 3D estimation. Second, after applying Material Point Method (MPM) physics simulation to this geometry, the simulated point cloud is used to guide the same I2V flow model to synthesize final, high-quality videos. Consistency-Guided Flow SDE, which decomposes the predicted velocity of the I2V flow model into denoising and consistency bias, allows us to effectively repurpose the model for both 3D reconstruction and simulation-guided video generation. Our method successfully bridges the gap from single-images to physically plausible videos  while remaining efficient to run on a single consumer gpu. In the extensive experiments, our approach outperforms state-of-the-art baselines on both GPT-based evaluations and VideoPhy physics-consistency benchmark, across diverse scenarios including single-object, multi-object, and fluid interaction sequences.", "tldr": ".", "keywords": ["Video Generation", "3D Reconstruction", "Physically Plausible Video"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13d26d40c964cda42405f732e99da151493db22d.pdf", "supplementary_material": "/attachment/dfcc4a84191ef887cca8a31328cafe24cce39522.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free pipeline that generates physically realistic videos from a single image. It repurposes an off-the-shelf image-to-video flow model for two stages: reconstructing full 3D scene geometry using rendered point clouds, and synthesizing final videos guided by Material Point Method physics simulations. The authors also propose Consistency-Guided Flow SDE that decomposes predicted flow velocities to enable effective 3D reconstruction and simulation-guided video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Research on physically realistic video generation is both practical and meaningful.\n\n- The proposed pipeline is well-designed, feasible, and reasonable.\n\n- Experimental results demonstrate performance improvements across multiple scenarios."}, "weaknesses": {"value": "- From the appendix video examples, some cases appear worse than other methods. For instance, in the Apple sample, the back video shows no water splashing when the apple falls. What could be the possible reason for this?\n\n- What is the speed of generating a video sequence, and how does it compare to other methods?\n\n- The paper lacks a discussion of limitations and corresponding analysis."}, "questions": {"value": "Could the authors provide intermediate visual results showing MPM-simulated outputs under different types of interactions, such as solid–fluid collisions, fluid–fluid interactions, and so on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OPMoJQiYbO", "forum": "8TgzLrWgrk", "replyto": "8TgzLrWgrk", "signatures": ["ICLR.cc/2026/Conference/Submission1309/Reviewer_iowy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1309/Reviewer_iowy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349458179, "cdate": 1761349458179, "tmdate": 1762915731539, "mdate": 1762915731539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents 3DPhysVideo, a training-free pipeline designed to generate physically realistic videos from a single image input. It reuses a pre-trained Image-to-Video (I2V) flow model across two stages.\n\nIn Stage 1: Single Image to 3D, the I2V model functions as a view synthesizer to reconstruct 360-degree 3D scene geometry.\n\nIn Stage 2: Simulation to Video, Material Point Method (MPM) physics simulation is applied to the geometry. The resulting simulated point trajectories, which support complex dynamics like fluids and viscous substances, then guide the same I2V model to synthesize the final photorealistic video.\n\nThe core mechanism, Consistency-Guided Flow SDE, adapts the I2V model for both 3D reconstruction and simulation-guided rendering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The 3DPhysVideo pipeline generates physically realistic videos from a single image using a training-free approach. It repurposes an off-the-shelf Image-to-Video (I2V) model in two stages.\n1. 3D Reconstruction: The I2V model first acts as a novel view synthesizer to reconstruct 360-degree 3D scene geometry.\n2. Physics Generation: The geometry undergoes Material Point Method (MPM) physics simulation. The resulting simulated dynamics then guide the same I2V model to synthesize the final photorealistic video.\nThis dual functionality is enabled by the Consistency-Guided Flow SDE, which adapts the pre-trained model for both geometry and dynamics synthesis. The method achieves good physical realism compared to baselines, especially in multi-object and fluid interaction scenarios, while offering user control over physical properties."}, "weaknesses": {"value": "1. The proposed method appears incremental, with limited distinction from prior work.\n\n2.Experiments are limited in scope; key baselines and datasets are missing.\n\n3. Core assumptions lack rigorous justification or mathematical support.\n\n4. Result interpretation is shallow; no discussion of failure cases or parameter sensitivity.\n\n5. Figures and explanations are sometimes unclear, reducing readability and impact."}, "questions": {"value": "1.\tCould the authors elaborate on the empirical or theoretical rationale for entirely eliminating the denoising bias ?\n2.\tWhat is the measured reliability or accuracy of these automatically inferred physical parameters compared to manually specified inputs?\n3.\tSince the current SDE is heavily reliant on visual consistency, how would the core consistency metric and the model’s latent inputs need to be adapted or redefined to effectively enforce a non-visual inductive bias, such as alignment with a detailed text prompt, without requiring additional model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pGo74UbIoB", "forum": "8TgzLrWgrk", "replyto": "8TgzLrWgrk", "signatures": ["ICLR.cc/2026/Conference/Submission1309/Reviewer_DQZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1309/Reviewer_DQZT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933485028, "cdate": 1761933485028, "tmdate": 1762915731385, "mdate": 1762915731385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces 3DPhysVideo, a novel, training-free pipeline that generates physically realistic videos from a single input image. Instead of training a new model, it cleverly repurposes a single, pre-trained image-to-video (I2V) model for two distinct stages: 3D Scene Reconstruction and Physics-Guided Video Generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The entire pipeline requires no additional training. It runs on a single consumer GPU, making it highly accessible and efficient compared to methods that require training large, specialized models.\n2. By grounding the animation in an explicit physics engine (MPM), the final video exhibits a high degree of physical plausibility, especially in complex scenarios like fluid dynamics and multi-object interactions, where purely data-driven models (e.g., Sora, Gen-3) often fail.\n3. The paper is well written and organized."}, "weaknesses": {"value": "1. As a multi-stage pipeline, errors from any stage may make the result fail. In particular, the 3D reconstruction and physical property estimation (using LLM) parts are prone to errors. For example, the apple in the demo appears elastic (it should actually be similar to a rigid body). It would be better if the accuracy of these two parts could be assessed, and the potential limitations could be analyzed.\n2. While it can run on a consumer GPU, this method predictably significantly increases inference time due to the introduction of 3D reconstruction, physical property estimation, and MPM simulation. It would be better to report a comparison of inference time.\n3. Were the liquids in the scene also reconstructed in 3D? How is the physical realism of the fluid dynamics ensured?\n4. The article states that PhysGen3D cannot maintain the relative position of objects. However, PhysGen3D does perform pose estimation, so is this statement somewhat unreasonable?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t2CxOWrDMN", "forum": "8TgzLrWgrk", "replyto": "8TgzLrWgrk", "signatures": ["ICLR.cc/2026/Conference/Submission1309/Reviewer_evX1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1309/Reviewer_evX1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941643880, "cdate": 1761941643880, "tmdate": 1762915731202, "mdate": 1762915731202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces 3DPhys Video, a training-free pipeline for generating physically realistic and photorealistic videos from a single input image. It addresses the fundamental limitation of traditional video generative models, which often fail to adhere to real-world physical dynamics.\n\nThe pipeline operates in two main stages, Novel View Synthesis and Simulation to Video Generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core contribution is the training-free pipeline that repurposes an off-the-shelf image-to-video diffusion model for two entirely different tasks: 3D scene reconstruction and physics-guided video synthesis.\n\nThe authors conducted extensive experiments to validate the effectiveness of their proposed method. The results demonstrate that 3DPhysVideo outperforms state-of-the-art methods in terms of physical realism and semantic consistency while maintaining competitive photorealism."}, "weaknesses": {"value": "The Material Point Method (MPM) is computationally expensive, especially for high-resolution simulations and complex scenes with numerous interaction points. The overall pipeline's speed is likely bottlenecked by the MPM step. The authors should clearly address the runtime breakdown for the three main stages: 3D reconstruction, MPM simulation, and I2V synthesis, to highlight the practical efficiency of the \"training-free\" claim.\n\nThe demonstration mostly focuses on relatively contained scenes with specific, localized physical events (e.g., ball drops, liquid pouring). It is unclear how well the pipeline scales to large-scale, non-local physical phenomena like wind effects, cloth dynamics, or complex collisions involving many small particles."}, "questions": {"value": "Could you address the problems in the weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y9FQcHTVC3", "forum": "8TgzLrWgrk", "replyto": "8TgzLrWgrk", "signatures": ["ICLR.cc/2026/Conference/Submission1309/Reviewer_heD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1309/Reviewer_heD9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020388067, "cdate": 1762020388067, "tmdate": 1762915731010, "mdate": 1762915731010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}