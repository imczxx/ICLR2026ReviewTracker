{"id": "unVjUrm7Wm", "number": 6243, "cdate": 1757961586651, "mdate": 1759897927295, "content": {"title": "Analysis and Explainability of LLMs Via Evolutionary Methods", "abstract": "Evolutionary methods have proven to be useful for analysis and explainability in the areas of genetics, biology, ecology, and more. In this work, we expand upon and extend these methods for neural networks, specifically for Large Language Models (LLMs), to better analyze and explain the relationships between them. We demonstrate how relating weights to genotypes (genetic makeup) and output text to phenotypes (observable traits) can result in enhanced understanding of lineage of models, important datasets, purpose of different layers of the models and also improved visualizations. We demonstrate this with a controlled experiment, in which we show that our estimated evolutionary trees reliably recreate the topology of the ground-truth evolutionary tree. We further examine the most important weight layers according to the weight differences, and show through phenotypic experiments that a certain dataset for training seems to add more important information than the other datasets. Finally, we generate an unsupervised evolutionary tree of black-box foundation models. Throughout, we provide visualizations to provide a better understanding of evolutionary relationships.", "tldr": "This paper expands evolutionary methods for analyzing and drawing inferences about LLMs and relationships among the models for improved explainability and safety.", "keywords": ["LLMs", "Evolutionary Methods", "Trees"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed15d96e8071873640d499a4fe876b2c3220bde0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose methods to investigate the relationships between LLMs and identify crucial layers that make them different."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well inspired: using genetics to compute differences between models is an inspiring line of work.\n- It compares a lot of metrics and layers to show which ones seem to be the most relevant\n- the results seem to be coherent and to align with the theoretical expectations which make the findings very coherent\n- The paper uses relevant metrics and statistics to test their results"}, "weaknesses": {"value": "- **Missing related work comparison**: Prior work has already tried to apply genetics concepts to LLMs to build such trees (cf https://iclr.cc/virtual/2025/poster/28195) and the paper doesn't discuss the theoretical / practical differences between the two approaches. Similarly, other approaches used weight-based methods to reconstruct the phylogeny of LLMs (https://iclr.cc/virtual/2025/poster/29687). Discussing the related literature to show how this method converge / diverge from these previous methods would greatly enhance the paper.\n- **No General Scheme** : It would benefit from having a scheme recapping the principle of the paper. Section 3.3.1 is not very easy to understand but could be efficiently represented visually providing the reader with a 1 figure concept of the paper.\n- **Low number of models** : The paper only compares models trained for the purpose of the study in the weight-based part and doesn't test models in the wild. It would be very interesting to see how this method works on classical open access LLMs - would attention layers still be the most relevant ? Furthermore, it only compares a few LLMs from 2 families (LLama and GPT) for response based method. The results of the paper could benefit running more models to verify the strength of the results in practice.\n- **Only one dataset** : The paper focuses on a single task for finetuning and testing models in the response-based part (all datasets are about summarizing). Using more datasets could provide a better overview of which technique seems to work best."}, "questions": {"value": "-In the response-based experiment what is the variance of the completion embeddings ? Maybe the fact that tree reconstruction doesn't work so well on all prompts comes from the fact that asking the model to summarize the document only 5 times is too noisy ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pHUBhaPs7R", "forum": "unVjUrm7Wm", "replyto": "unVjUrm7Wm", "signatures": ["ICLR.cc/2026/Conference/Submission6243/Reviewer_2VdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6243/Reviewer_2VdW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521802876, "cdate": 1761521802876, "tmdate": 1762918568165, "mdate": 1762918568165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce an evolutionary framework for analyzing and explaining LLM. In the framework that is introduced, the internal weights are treated as genotypes, model outputs as phenotypes, and model fine-tuning sequences as evolutionary processes. The authors finetune a 60M parameter model (t5-small) on permutations of 10 summarization datasets. They reconstructed the phylogenetic trees of model relationships using weight distances and analyze layerwise contributions to change. They demonstrate that evolutionary analysis can effectively capture model lineage, identify the dynamic layers during fine-tuning, and reveal meaningful relationships between models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem discussed here is quite relevant for today, considering the increased use of LLMs and novel LLMs that are coming out.\n2. The application of phylogenetic concepts to LLM analysis is a creative approach. Treating model weights as genotypes and outputs as phenotypes is an interesting idea.\n3. The methodology is explained well: extracting model features, computing pairwise distances, constructing distance matrices, and generating phylogenetic trees.\n4. The paper discusses relationships from both LLMs trained using tree based fine-tuning steps and from models with different architectures, sizes and weights.\n5. The results demonstrate that evolutionary trees constructed from model weights can effectively recover the fine-tuning hierarchy."}, "weaknesses": {"value": "1. I found a lot of whitespaces and using one or two sentences in a paragraph multiple times when reading. Please format the paper well.\n2. The model is limited to only a 60M model, whereas now the models have even gone beyond 70B parameters. The authors should seriously consider using a novel model. DeepSeek might be an option to consider.\n3. Sometimes, phenotype-based tree accuracy can be very sensitive to how prompting is done. In terms of generalization, more justification is needed. For example, even small variations in prompts could alter the topology of the resulting trees.\n4. The authors do not formally justify why model weight changes should behave analogously to biological evolution. It would be better if this could be further justified either theoretically or empirically.\n5. Need more baselines to compare the evaluations in the results section.\n6. It is very difficult to either understand or read Figure 05."}, "questions": {"value": "1. How to scale to larger models? Have the authors considered applying their method to more recent architectures such as DeepSeek?\n2. Can you provide a stronger theoretical or empirical justification for treating model weight changes as analogs of genetic evolution?\n3. How stable are the phenotype-based evolutionary trees under different prompt formulations or datasets? Have you conducted any sensitivity analysis to evaluate how prompt variations affect the resulting tree topology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BX0CbA9HTV", "forum": "unVjUrm7Wm", "replyto": "unVjUrm7Wm", "signatures": ["ICLR.cc/2026/Conference/Submission6243/Reviewer_sWbr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6243/Reviewer_sWbr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984927991, "cdate": 1761984927991, "tmdate": 1762918567754, "mdate": 1762918567754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for analyzing and explaining Large Language Models (LLMs) by drawing an analogy from evolutionary biology. The authors map model weights to genotypes (internal representations) and model outputs (text/embeddings) to phenotypes (observable traits). Using this framework, they employ phylogenetic methods to construct evolutionary trees that represent the relationships between LLMs.\nThe authors conduct two main sets of experiments. First, in a controlled setting, they fine-tune a T5-small model on various sequences of summarization tasks and compare the model weights (genotype) to show they can reconstruct the ground-truth tree. Second, they apply the phenotypic approach to the Llama and OpenAI models to analyze the embeddings of generated summaries. And show the models from clusters by model family and construct an evolutionary tree."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of applying evolutionary methods to LLM analysis is interesting and provides a different conceptual angle on model relationships.\n- Examining both weight-based (genotype) and output-based (phenotype) methods provides breadth to the analysis.\n- The output-embedding approach that compares the closed-source models and shows intuitive family clustering (Llama vs OpenAI) is a nice takeaway."}, "weaknesses": {"value": "- The analogy between weights and genetics is unclear. Phylogenetic methods are designed for categorical data, but the weights are continuous values. In machine learning, when using gradient descent is directed optimization (guided with an explicit loss function), whereas evolution is undirected.\n- Section 4.1 shows that different distance metrics identify completely different conclusions about layer importance. L1 distance identifies decoder (Dense ReLU) layers, and cosine distance identifies encoder Q-matrices are most important. The paper acknowledges this contradiction but provides no resolution, ground truth validation, or guidance on which metric to choose or trust. This undermines the reliability of layer importance results.\n- Section 4.2 (Lines 378-403). The Response-based method (phenotype evaluation) appears circular, as it requires knowing the evaluation datasets a priori, since optimal trees are dataset-dependent. The method cannot discover which datasets matter without already knowing the answer.\n- Section 4.2. How sensitive are the blackbox models to other datasets or prompts? It is hard to conclude without more experimentation.\n- Figure 3 (left) illustrates the cluster formation of OpenAI models vs LLama models. How do we know if the clutter formed is due to the difference in training architecture or different training regime, or different data?\n- Unclear about the scope or end goal of the paper. The authors claim to help answer \"which models should I use?\" but provide no concrete guidance. No case studies, decision rules, or evidence that trees lead to better model selection than simply consulting leaderboards. It is unclear how this helps in practice.\n- The paper could benefit from comparisons to weight similarity metrics or model selection techniques. \n- The paper does not discuss how models react to different hyperparameters. Also, what's the reason behind using All-MiniLM-L6-v2 for embeddings? How sensitive are results to these choices?"}, "questions": {"value": "- In Section 3.3.1, it uses a T5-small model on a single task for summarization. How can we conclude that findings would be the same for more recent LLMs across different tasks like translation, generation, etc.. How can single-task evaluation be generalizable?\n- Figure 3 (right). Black-box models have no validation or ground truth. While it makes sense for the Llama models, how can we verify that o4-mini is more similar to GPT-5-mini or o3-mini?\n- Which distance metric is correct for identifying layer importance? How do you validate against ground truth?\n- Table 1 in the paper shows that there is a performance degradation with tree size. What is the maximum usable tree size it will work for?\n\n**Minor Comments:**\n- Use the most recent template for ICLR 2026 - The paper is actually in review for ICLR 2026 and not ICLR 2025 (see header)\n- The paper could benefit from better captions of tables and different columns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Co3W8Xrgy3", "forum": "unVjUrm7Wm", "replyto": "unVjUrm7Wm", "signatures": ["ICLR.cc/2026/Conference/Submission6243/Reviewer_QfAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6243/Reviewer_QfAX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762579280627, "cdate": 1762579280627, "tmdate": 1762918567435, "mdate": 1762918567435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel lens through which to analyze LLMs’ interrelationships and characteristics. The authors propose using evolutionary methods to characterize LLMs based on outputs and weight matrices. Experiments indicate that these methods, when applied to weights of trained models, can identify training datasets and task-important layers. Meanwhile, when applied to output sequences, the methods allow for building a sort of phylogenetic tree of model families.\nWhile this paper opens an interesting new perspective on LLM explainability, the experiments are limited and practical implications also seem moderate given the current results. This paper could benefit from broader experiments and a more grounded motivation for individual experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This is a unique way to analyze LLMs and allowing for both, a weight and output informed perspective offers flexibility when approaching black-box models. \n- The fact that the evolutionary models don’t require strong independence conditions is a good strength to point out as a motivation for using these in the context of LLMs and neural networks in general."}, "weaknesses": {"value": "- The first part of the introduction of the paper partially motivates this work by citing the number of task-specific, publicly available LLMs and the difficulty to make a choice for practitioners. However, it is unclear how results presented here address this problem. \n- The characterization of the work by Cloud et al. 2025 on subliminal learning should be reworded. For example “a teacher model that was known to prefer owls” is an odd description without at least a little additional context. \n- The phylogenetic tree showing how models from the GPT and Llama families are related is, in principle, a decent way of showing that this method works as intended. However, it would greatly benefit from the inclusion of more diverse models. For instance, why was no MoE model like Mixtral included, or a DeepSeek model? Or models from different families with similar numbers of parameters? As is, this is simply not enough to show that this can robustly interrelate LLMs in a meaningful way. Beyond that, the utility, aside from showing that this perspective has merit, is also unclear. \n- L. 168 points to the discussion for how the weight analysis can be done with larger models but this was not adequately mentioned in the discussion. \nIdentifying layers with large magnitudes of change, even if metric-dependent, could be interesting but is not adequately pursued here.  Presumably, the utility is that, in a limited finetuning setting, these layers could be trained while other layers are frozen for increased efficiency with little performance loss. It is not obvious that this would work however, so it should be added as an experiment. Otherwise, identifying these layers seems pointless.\n- The authors cite using their weight-based methods to identify training details for models with unknown origin. However, it is unclear how the methods would scale to many of these settings. Be it very large LLMs or LLMs trained on large, diverse corpora of datasets, since the experiments presented are relatively limited outside what is common in praxis. \n- Many of the points raised in the discussion as limitations should be addressed to make this a more well-rounded work. While not everything can be done in one paper, at least including more diverse model architectures and training datasets from more than one task domain would significantly bolster this paper’s credibility."}, "questions": {"value": "- Why was the text summarization tasks chosen specifically for this paper?\n- The authors repeatedly state that these methods could lead to improved visualizations. Could this point be elaborated upon? I understand that the evolutionary perspective adds a new kind of visualization with the phylogenetic trees but what specifically is their utility in the context of visualizations for LLM explainability that is needed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tEsDlXA7KQ", "forum": "unVjUrm7Wm", "replyto": "unVjUrm7Wm", "signatures": ["ICLR.cc/2026/Conference/Submission6243/Reviewer_4sat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6243/Reviewer_4sat"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762592483087, "cdate": 1762592483087, "tmdate": 1762918567102, "mdate": 1762918567102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}