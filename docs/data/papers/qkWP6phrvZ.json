{"id": "qkWP6phrvZ", "number": 4748, "cdate": 1757756726076, "mdate": 1759898016213, "content": {"title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents", "abstract": "Large language model (LLM)–based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy’s probability of producing the correct answer. These intrinsic rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines, achieving higher accuracy and improved sample efficiency.", "tldr": "We propose IGPO, a simple and effective reinforcement learning framework with turn-level reward for training multi-turn LLM-based agents.", "keywords": ["Turn-Level Reward", "Search Agent", "Agentic RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da6c548be56e2ad3ed7d0c2194b43ac5e699661c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. Specifically, they use log-likelihood of the ground truth at each turn to define the intrinsic rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The algorithm is clearly presented. The algorithm design is simple yet effective.\n- The empirical evaluation is solid with significance improvements observed over multiple baseline methods."}, "weaknesses": {"value": "- Limited Novelty: The paper introduces a new intrinsic reward design, but its novelty is somewhat constrained. Further exploration or differentiation from existing designs would enhance its contribution to the field.\n- Insufficient Theoretical Support: The paper lacks robust theoretical justification for the proposed intrinsic reward design.\n- Lack of Insight on Alternative Designs: The paper would benefit from a discussion on alternative intrinsic reward designs. Exploring how the proposed design could be integrated with other existing designs might provide valuable insights and broaden its applicability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VeLO5fQFZU", "forum": "qkWP6phrvZ", "replyto": "qkWP6phrvZ", "signatures": ["ICLR.cc/2026/Conference/Submission4748/Reviewer_miYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4748/Reviewer_miYY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448235172, "cdate": 1761448235172, "tmdate": 1762917552928, "mdate": 1762917552928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Information Gain-based Policy Optimization (IGPO), a reinforcement learning framework that enhances multi-turn LLM agents for tasks like web search and multi-hop question answering. IGPO employs dense, turn-level intrinsic rewards derived from the marginal probability increase of generating correct answers at each step. By integrating these per-turn rewards with conventional outcome-based rewards, IGPO addresses reward sparsity and advantage collapse in long-horizon RL, enabling superior credit assignment. Experiments across seven benchmarks demonstrate IGPO's superiority over prompt-based and RL baselines, supported by ablation studies, training analyses, and theoretical foundations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled and Well-Motivated Reward Design: IGPO addresses a pertinent weakness in agentic RL for LLMs—the reward sparsity problem—by introducing information-gain signals that provide stepwise, ground-truth-aware supervision. The approach is simple yet effective and is grounded in a clear theoretical motivation (see Appendix A).\n\n2. Thorough Mathematical Formulation: The paper provides clear derivations of the reward formulation (Equation 4–7), discounted advantage calculation, and the overall surrogate objective for IGPO. This mathematical clarity makes the method reproducible and portable to related settings.\n\n3. Comprehensive Empirical Evaluation: The authors conducted careful experiments across in-domain and out-of-domain benchmarks (NQ, TQ, HotpotQA, 2Wiki, MusiQue, Bamboogle, PopQA), and compared IGPO against strong baselines (both RL-based and prompt-based, see Table 1 and Table 2).\n\n4. Reproducibility: Public release of source code enhances transparency and facilitates replication."}, "weaknesses": {"value": "1. Novelty is Incremental vs. Contemporary Efforts: While IGPO’s design is sound, many core principles—such as dense, turn-level supervision, teacher-forced signals for policy confidence, and combining process and outcome rewards—are parallel to mechanisms introduced or explored in very recent works from 2025 that are not thoroughly contrasted or ablated against. The differences from GiGPO, ReasoningRAG, StepSearch, and especially the missing related work are not sharply drawn.\n  - Zeng, S., Wei, Q., Brown, W. (2025): \"Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment\"\n  - Wei, Q., Zeng, S., Brown, W. (2025): \"LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization\"\n  - Tang, X., Xu, W., Wang, Y. (2025): \"Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning\"\n\n2. Reliance on Ground-Truth for Intrinsic Reward: The information gain reward fundamentally requires access to the ground-truth answer for teacher-forcing in every trajectory step (see Section 3.2,). This is not always feasible for open-ended or real-world deployments, limiting applicability. The issue is acknowledged in the limitations, but its practical significance is not fully explored, nor are mitigations proposed.\n\n3. Absence of Fine-Grained Failure Analysis: While the aggregate numbers and training curves in Tables 1–3 and Figures 4–5 are generally positive, there remains a lack of granular breakdown where IGPO underperforms (if any), or qualitative investigation of error modes and tasks/environments where information gain may be less reliable (e.g., ambiguous or multi-answer questions). The failure cases in Figures 6 and 7 are insightful but could be complemented with quantitative measures of failure types."}, "questions": {"value": "1. Applicability Beyond Ground Truth Rich Environments. IGPO relies on ground-truth answers for teacher-forced reward estimation in every trajectory step. How would the method adapt to settings where ground truths are partial, noisy, or unavailable (e.g., open-ended queries, creative generation, real-world search)? Could unsupervised or self-consistency signals be integrated in lieu of explicit ground truth?\n\n2. Comparison With Omitted Recent Baselines. Please provide direct comparison (either empirical or qualitative) against the most recent 2025 turn-level/process-level RL methods listed above (e.g., Turn-Level Credit Assignment, LeTS, ToolRL). What fundamentally differentiates IGPO in real settings?\n  - Zeng, S., Wei, Q., Brown, W. (2025): \"Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment\"\n  - Wei, Q., Zeng, S., Brown, W. (2025): \"LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization\"\n  - Zeng, S., Wei, Q., Brown, W. (2025): \"ToolRL: Reward is All Tool Learning Needs\"\n\n3. Failure Modes and Robustness. What are the typical failure cases or degenerate behaviors for IGPO—especially when intermediate turns provide little actual information gain, or when there are multiple valid answer paths? Could the dense rewards inadvertently reinforce misleading intermediate confidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sTiHW94AZ0", "forum": "qkWP6phrvZ", "replyto": "qkWP6phrvZ", "signatures": ["ICLR.cc/2026/Conference/Submission4748/Reviewer_oHJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4748/Reviewer_oHJt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902800899, "cdate": 1761902800899, "tmdate": 1762917552571, "mdate": 1762917552571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IGPO, a reinforcement learning framework for training multi-turn LLM agents. IGPO addresses the reward sparsity and advantage collapse issues of outcome-only rewards by introducing a turn-level information gain reward, which quantifies how much each agent turn increases the policy’s probability of generating the correct answer. These dense, ground-truth-aware rewards are integrated with final outcome rewards to form a comprehensive signal for GRPO-style optimization. Experiments on seven search-based QA datasets (NQ, TQ, HotpotQA, 2Wiki, MusiQue, Bamboogle, PopQA) show that IGPO consistently outperforms prior outcome- and step-reward RL baselines, improving both sample efficiency and training stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. IGPO’s reward formulation is Simple yet effective, estimating turn-level information gain without requiring additional annotation or external evaluators.\n2. Strong empirical results: Extensive experiments on both in-domain and out-of-domain datasets show consistent improvements over strong baselines such as GiGPO and DeepResearcher.\n3. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. The evaluation focuses exclusively on search-based QA tasks. Such settings naturally align with the proposed information-gain reward, as acquiring relevant retrieved evidence directly increases the probability of producing the correct answer. However, in other domains, such as mathematical reasoning, embodied agents, or web agents, this reward may not lead to performance improvements. Consequently, the proposed method appears to be primarily applicable to information-retrieval tasks like search-based QA.\n\n2. Moreover, this reward could reinforce spurious correlations. LLMs are known to exploit shortcuts when solving problems, relying on superficial cues rather than learning the underlying reasoning process. Instead of acquiring new knowledge through comprehensive reasoning, LLMs may overfit to spurious correlations between final answers and intermediate components. In contrast, many recent works on mathematical reasoning explicitly aim to mitigate such spurious correlations. The design of the IGPO method could, in fact, encourage this undesired behavior, potentially having a negative impact on this research area.\n\n3. Finally, the paper computes normalized advantages using: A = r-mean(R)/std(R), where R aggregates all rewards across all steps and all rollouts within a group. This design is theoretically problematic. If the method assumes step-level rewards, the baseline should be computed per step. That is, by sampling multiple trajectories starting from that turn and averaging their returns. Otherwise, the estimator mixes rewards from different time steps with distinct state-action distributions, introducing bias. If, instead, the method assumes trajectory-level rewards, it should not be framed as step-level GRPO. The authors need to clarify or justify this design choice."}, "questions": {"value": "1. How does IGPO perform on non-search tasks such as mathematical reasoning, code generation, or web-agent? Would the information-gain reward still be meaningful there?\n2. Could you provide empirical evidence showing whether IGPO indeed mitigates spurious correlations, e.g., by testing on tasks requiring multi-step reasoning consistency rather than document retrieval?\n3. How sensitive is the training to the normalization scheme of turn-level rewards? Have you tried per-turn baselines or PPO?\n4. What is the computational cost compared to baseline algorithms, given the need to compute per-turn log probabilities of the ground truth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sc8OnKpPzl", "forum": "qkWP6phrvZ", "replyto": "qkWP6phrvZ", "signatures": ["ICLR.cc/2026/Conference/Submission4748/Reviewer_LxTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4748/Reviewer_LxTU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940278453, "cdate": 1761940278453, "tmdate": 1762917552303, "mdate": 1762917552303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}