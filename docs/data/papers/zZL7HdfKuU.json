{"id": "zZL7HdfKuU", "number": 3250, "cdate": 1757386975126, "mdate": 1759898099737, "content": {"title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "abstract": "Offline reinforcement learning (RL) learns policies from fixed datasets, thereby avoiding costly or unsafe environment interactions. However, its reliance on finite static datasets inherently restricts the ability to generalize beyond the training distribution.\nPrior solutions based on synthetic data augmentation often fail to generalize to unseen scenarios in the (augmented) dataset.\nTo address these challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for decision-making, which innovatively introduces a retrieval mechanism into offline RL. Specifically, RAD retrieves high-return and reachable states from the offline dataset as target states, and leverages a generative model to generate sub-trajectories conditioned on these targets for planning. Since the targets are high-return states, once the agent reaches such a target, it can continue to obtain high returns by following the associated high-return actions, thereby improving policy generalization. Extensive experiments confirm that RAD achieves competitive or superior performance compared to baselines across diverse benchmarks, validating its effectiveness. Our code is available at https://anonymous.4open.science/r/RAD_0925_1-690E.", "tldr": "", "keywords": ["Offline Reinforcement Learning", "Retrieval-Augmented Generation", "Goal-Conditioned Policy", "Trajectory Stitching"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32ac245bb424c85bdbc847925ce06e1cbf7b7138.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RAD, a retrieval-augmented framework for offline RL that enhances policy generalisation. RAD dynamically retrieves high-return and reachable states from the offline dataset as intermediate targets and employs a diffusion-based generative model to plan sub-trajectories towards them. This retrieval-guided planning allows agents to escape low-return or out-of-distribution regions and achieve higher rewards. Experiments show that RAD consistently matches or outperforms strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is well-structured and clearly written.\n\n2. The motivation is strong and effectively highlights the rationale of the method."}, "weaknesses": {"value": "The novelty of this method appears limited. To my understanding, many prior works in this area employ model-based approaches to augment offline datasets, such as various trajectory-stitching methods. The key contribution of this paper seems to lie in combining diffusion models with generalisation beyond the offline dataset. However, diffusion models have already been used for dataset augmentation, and other approaches have separately explored generalisation beyond offline data. As such, this paper mainly integrates these two existing directions, and its conceptual novelty may be insufficient for an ICLR paper."}, "questions": {"value": "1. The authors should cite the related work properly.\n\n2. There are several typos that should be corrected: line 159, line 201 (capital letter), Eq. 13 (subscript), line 294 (definition of $V(s_t)$), and line 406 (an extra line break).\n\n3. I believe that the TS, SE, and PL modules are executed during evaluation, functioning similarly to real-time planning. Therefore, I am concerned about the computational cost during evaluation. Could the authors provide details on the inference time or latency required to produce a single action?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IvejWaqnZh", "forum": "zZL7HdfKuU", "replyto": "zZL7HdfKuU", "signatures": ["ICLR.cc/2026/Conference/Submission3250/Reviewer_wzXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3250/Reviewer_wzXn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760545794278, "cdate": 1760545794278, "tmdate": 1762916628716, "mdate": 1762916628716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for improving offline RL. Their method (RAD) focuses on three components, (1) a target picker that from a state finds a reachable state with high return, (2) a step estimator to estimate how far the target position is from the current position and (3) a planner that creates a trajectory to get to the goal states. The high level idea is to train these components and use them to act in the environment. They perform experiments on offline RL tasks to compare to existing baselines and ablate the components of their method. They find that their method is able to perform as well as or better as baselines on their experiments and that their components all contribute to their final results. They also perform experiments on the generalization of their method and find it is able to reasonably generalize."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work has reasonable novelty. This work isn't directly proposing a diffusion model but instead builds upon other works models and adds in a method for guiding these models to create the training data.\n\nThe experimental evidence is reasonable. They compare to many baselines on a reasonable set of experiments. I would really like to see 95% confidence intervals here (like in table 1) as without them it is harder to distinguish great results from good ones.\n\nThe actual results on the tasks perform quite well. In general this method is doing much better than the baselines."}, "weaknesses": {"value": "In general I feel like the writing could be more clear and I think we are missing some information that I feel is critical. I will put my questions in the question section but I don't feel confident I understand how your method actually runs. Paragraph near 155 - This paragraph is pretty hard to read/understand, what does \"transit\" mean here? Do you mean a trajectory from s_t to s_t^g?.\n\nThe limitations of this method are not properly addressed. I'll put specific questions in the question part again but some limitations that I can think of. Speed - running diffusion models and querying a large dataset seems slow. Long horizon - there is a limit to your planner right so does increasing this limit cause issues with attempting to plan from your proposed state?\n\nIn general I come away from the paper with a lot of questions. I'll put them below but if you can answer them and update the paper then the weaknesses will be minimal and I will gladly raise my score to an accept since this is an interesting paper but I just don't fully understand the method and weaknesses."}, "questions": {"value": "How do you actually run your policy? Do you run one step of your method, then take the action or do you create a trajectory with your planner and run it open loop? How long does this take? Is it a feasible real-time method or not?\n\nYou say that you can just follow high return actions once you get to the in distribution states but how are these actions computed? Is it still from your method? Or are we following the exact actions in the dataset? Is the method used to simply get out of OOD situations and then we run a normal policy?\n\nDiffusion models are slow, how long does your method take with the extra search on top? If you have to query your entire state space when you run your method how long does that take? How would this deal with a large amount of offline data?\n\n212 - So the idea is you want to find the number of steps to a new state but doesn't the feature vector contain the position in each trajectory the state is at? Is there a problem with assuming we are in the \"same\" location as long as the difference between states is small enough?\n\nSmall thing but make sure to check for typos as well there are a handful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBIvcSOvpr", "forum": "zZL7HdfKuU", "replyto": "zZL7HdfKuU", "signatures": ["ICLR.cc/2026/Conference/Submission3250/Reviewer_woTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3250/Reviewer_woTf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836905829, "cdate": 1761836905829, "tmdate": 1762916628273, "mdate": 1762916628273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAD (Retrieval High-quality Demonstrations), a retrieval-augmented trajectory stitching method for offline RL, which is composed of three modules as Target Selection (TS), Step Estimation(ES), and Planning (PL). Given a current state, RAD (i) retrieves similar, high-return, and purportedly reachable target states from an offline dataset; (ii) estimates the temporal distance (step span) to the target; and (iii) conditions a diffusion planner (Diffuser or DiffuserLite) to generate a sub-trajectory that steers the agent toward the target, after which the policy continues along high-return actions in the demonstration. Experiments on D4RL tasks report broadly competitive results, plus ablations on each module and a generalization test on Random datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed RAD method combines target state retrieval with diffusion-based planning in offline RL. While trajectory stitching and diffusion planners exist, RAD’s idea of adaptive target retrieval at inference time (instead of static, offline augmentation) is a meaningful design point, appealing in sparse-reward or long-horizon domains (e.g., AntMaze), where “latching onto” good sub-goals helps escape low-value regions.\n\nThe “target-then-plan” decomposition (TS → ES → PL) is conceptually clear, and may be applied to many sequence-modeling or model-free offline baselines. The presentation of three modules and the illustrations make the paper easy to follow.\n\nThe experiment done in this paper is quite comprehensive, including a solid suite of baselines, visualization, ablations, and a small distribution-shift study."}, "weaknesses": {"value": "The idea is straightforward, but lacks theoretical support. Firstly, “reachability” is asserted, not guaranteed. TS currently filters by cosine similarity and high return, then picks the candidate with the longest remaining length; there is no principled guarantee that the target is reachable without collisions/obstacles under the learned dynamics—especially salient in mazes or any environment with an inconsistent transition model (e.g., a wall separating the two near states exists and has not been explored much in the dataset). The author trained an extra binary classifier to predict the connectability in Appendix D for D-RAD, but did not show that in the main text, nor did extra comparison on that. Besides, the pipeline seems deterministic once top-k are retrieved (tie-breaks aside). This can induce policy shift brittleness: if the top candidate is slightly wrong or not reachable, the planner may commit to a poor target with no alternative selection. For potential improvement, maybe introduce stochastic target sampling. Finally, the way to select high returns also seems problematic. The cumulative discounted reward from step t is defined as $v_t = \\sum_{i \\ge t } \\gamma^i r_i$ instead of $v_t = \\sum_{i \\ge t } \\gamma^{i-t}r_i$, which means that earlier states will always have a higher return in an environment with non-negative rewards.\n\nExperiment Issues: Only 3 seeds and no standard deviations/confidence intervals. The table highlights entries within 0.95×MAX, which is an unusual criterion and can obscure variance. Besides, it will be clearer if an average score is presented.\n\nThe proposed RAD seems closely related to trajectory stitching-based methods like DiffStitch, but there is no further comparison and discussion in the related works. \n\nMinor issues:\n\n- Typos (e.g., Line 159 “makinga”; Line 157 “TS then estimates the step” should be ES; Inconsistent capitalization for “Diffuserlite”; “Diverde” in Table 1).\n\n- Notation Inconsistency (‘G’ is used both as the goal state in Introduction and the return in 4.1. The latter could be replaced as $R(\\tau)$ denoted in equation (2); $f_e$ refers to different things in 4.2 and Appendix D)\n\n- 4.3 presents two sub-trajectory types (noisy state-action pair vs. noisy state), but no context for which one is used."}, "questions": {"value": "1. Please clarify or correct the definitions of v_t in 3.2 and anywhere else they propagate. \n\n2. Is ES trained with cross-entropy over H–1 classes as suggested in 4.2 or with the MSE in Eq. (14)? If the latter, how do you backprop through argmax?\n\n3. Can you show ablation results on how much the classifier mentioned in Appendix D helps to prevent TS from selecting an unreachable target state? \n\n4. Why does DL-RAD underperform in Maze2D compared with other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HqL2ccn9LN", "forum": "zZL7HdfKuU", "replyto": "zZL7HdfKuU", "signatures": ["ICLR.cc/2026/Conference/Submission3250/Reviewer_6yLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3250/Reviewer_6yLX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946401109, "cdate": 1761946401109, "tmdate": 1762916626411, "mdate": 1762916626411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAD (Retrieval High-quality Demonstrations), a retrieval-augmented offline reinforcement learning framework. Instead of relying on static data augmentation, RAD dynamically retrieves high-return and reachable states from an offline dataset as intermediate targets. It then employs a diffusion-based generative model (built upon Diffuser or DiffuserLite) to generate sub-trajectories toward these targets, improving generalization beyond the dataset distribution. Experiments on D4RL benchmarks show consistent or superior performance over baselines across locomotion, navigation, and manipulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Integrating state retrieval into offline RL is conceptually appealing, and the paper provides a clear motivation for using retrieved high-return states as adaptive guidance for policy improvement.\n2. The experiments cover a wide range of D4RL tasks (MuJoCo, AntMaze, Kitchen, Maze2D) with solid baselines including model-free, model-based, and diffusion-based methods. RAD demonstrates competitive or superior performance on most datasets."}, "weaknesses": {"value": "1. Several sections contain minor grammatical errors and redundant phrasing (e.g., “novelly integrates,” “makinga decision”). Figures could be improved for clarity and caption detail.\n2. The distribution-shift test (training on Medium-Replay, testing with Random starts) is limited to three environments; more systematic OOD tests would strengthen claims.\n3. Although the retrieval mechanism is new, the overall architecture largely reuses existing components from Diffuser/DiffuserLite, and the retrieval is applied at the state level without deep theoretical justification for its optimality or stability."}, "questions": {"value": "1. How sensitive is RAD to the accuracy of the value-based ranking in target retrieval? Would errors in return estimation significantly degrade performance?\n2. Can RAD handle multi-modal retrieval results (e.g., when several high-return trajectories exist but lead to different goals)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bRFiRBkuHV", "forum": "zZL7HdfKuU", "replyto": "zZL7HdfKuU", "signatures": ["ICLR.cc/2026/Conference/Submission3250/Reviewer_Wzph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3250/Reviewer_Wzph"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992366855, "cdate": 1761992366855, "tmdate": 1762916626202, "mdate": 1762916626202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}