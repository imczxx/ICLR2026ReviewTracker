{"id": "ypFvQSXFNC", "number": 6205, "cdate": 1757958516959, "mdate": 1759897929958, "content": {"title": "Generative Blocks World: Moving Things Around in Pictures", "abstract": "We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method, which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding the texture-consistency provided by existing techniques. These texture hints (a) allow accurate object and camera moves and (b) preserve the identity of objects. Our experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization. Code will be released.", "tldr": "We can use 3D primitives to control diffusion models", "keywords": ["3D primitives", "Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79c34eb5fb4231e3b7fd198f77319027f896f951.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new system named Generative Blocks World, for interactive image editing. The authors argue that primitive is a more straightforward, and intuitive concept for user to interact with rather than raw depth map. The detailed process of the model includes a primitive extraction model, hint image generation process and final image editing model. The authors first train the primitive extraction model on a subset of LAION dataset to predict primitives (with known number for each trained model). Then given an image to be edited, user can manipulate the extracted primitives to achieve both camera view editing and scene components editing. The edited primitives are used to generate hints: including depth map, and a \"warped\" hint image. These components are used to condition a depth-conditioned FLUX model for final output. The authors also conduct extensive analysis of the proposed system to show the validness of each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed Generative Blocks World has a good motivation. Primitive is a more straightforward concept comparing to raw depth map, and varying number of primitives also provide a flexible way to understand the image content. \n\nAuthors also conduct thorough analysis on the proposed method's components, which provides a very detailed understanding.\n\nThe results look promising with both camera control and image component control."}, "weaknesses": {"value": "The most important weakness of this paper is currently I think the authors are lack of analysis of the importance of the proposed \"primitives\". From my perspective, the actual image generation model is a finetuned FLUX model that conditions on a combination of \"hint image, depth, (mask?)\" The authors should:\n1. Validate the effectiveness of each components in the condition combination through some qualitative examples or analysis. But I also understand if this requires retrain the model and time is limited. But I think at least some intuitive discussion is needed.\n2. This pipeline, in my opinion, can actually be achieved via: use segmentation mask to achieve the same usage of primitives, and use SDEdit + a depth-conditioned FLUX model to accept both hint image and depth control. So I think authors should do some qualitative comparisons with this one or even three baselines to prove the validness of the proposed Generative Block World:\n- move segmentation mask to warp the depth and hint image, still use current (finetuned?) FLUX-Depth\n- primitive pipeline as current, but use original FLUX-Depth + SDEdit for hint image condition\n- move segmentation mask to warp the depth and hint image, use original FLUX-Depth + SDEdit for hint image condition\nI think these baselines are valid, but maybe time is limited so some comparison examples would be fine"}, "questions": {"value": "I feel like the paper is written in a rush and some details are not clear, so maybe my aforementioned weakness is with some misunderstandings.\nMy questions include:\n1. did the authors actually fine-tune the FLUX-Depth to take hint image as input, or actually not, then that step is just SDEdit? (Because it seems very much like SDEdit)\n2. What exactly are the conditions from primitive manipulation? Depth? Hint image? What about the confidence mask, I think this part requires an additional figure to explain for better understanding.\n3. What's the training time consumption for the primitive extraction model? And if there is, what's the training consumption for FLUX-Depth finetune? Is there any difference in training dynamics for primitive extraction model with different primitive numbers?\n4. See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OpOzPVS5uy", "forum": "ypFvQSXFNC", "replyto": "ypFvQSXFNC", "signatures": ["ICLR.cc/2026/Conference/Submission6205/Reviewer_Ka9k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6205/Reviewer_Ka9k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638032988, "cdate": 1761638032988, "tmdate": 1762918543817, "mdate": 1762918543817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Generative Blocks World,\" a novel and interesting framework for image editing that leverages explicit 3D representations. The core idea is to decompose an image into a set of 3D convex primitives, which a user can directly manipulate (e.g., move, scale, rotate) to edit the scene's geometry. A new image is then synthesized by a pretrained, depth-conditioned generative model (FLUX), guided by a depth map rendered from the edited primitives and a novel texture hint mechanism designed to preserve object identity. The motivation is clear, and the proposed method offers a training-free (at inference time) approach to solving challenging, geometry-aware image editing tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty and Motivation: The approach of revitalizing classic \"blocks world\" concepts for controlling modern generative models is highly innovative. It provides a clear and compelling solution for 3D-aware image manipulation, which is a significant problem in the field.\n\n2. Decoupling of Geometry and Texture: The framework effectively decouples geometric control (via primitives and depth maps) from appearance generation (via the generative model and texture hints). This modularity is a key strength, allowing for precise and predictable geometric edits."}, "weaknesses": {"value": "1. Limited Quantitative Evaluation: The quantitative comparison is confined to a single baseline (LooseControl) on a small, unstated set of test images. The paper's claims of superiority would be significantly strengthened by a more extensive evaluation on standard image editing benchmarks and against a wider array of recent methods, especially those with different interaction paradigms (e.g., drag-based).\n\n2. Lack of Critical Ablation Studies: The paper is missing important ablation studies that would provide valuable insight into the method's internal mechanics and sensitivity to hyperparameters. For instance, an analysis of how the texture hint's application window (i.e., the start and end timesteps, $t_{start}$ and $t_{end}$) affects generation quality is crucial. Without such analysis, the reader cannot fully appreciate the contribution of this specific component or understand the trade-offs involved in its tuning.\n\n3. Unaddressed User Interaction Challenges: The paper claims that manipulating primitives is an \"intuitive\" and \"natural\"  interaction paradigm. However, it does not address the significant practical challenges of this interface. For complex scenes or objects decomposed into many primitives, requiring the user to manually select, group, and manipulate these components could be tedious and imprecise. The paper would benefit from a more detailed discussion of the proposed interaction workflow and, ideally, a user study to validate its usability claims against other interfaces.\n\n4. Unintended Background Alterations: A notable weakness, stemming from the reliance on a holistic generative model like FLUX, is the tendency for static background regions to change during an edit. When a foreground object is manipulated via the depth map, the model re-synthesizes the entire image to ensure global coherence. This often results in undesirable and unpredictable alterations to the background, which should have remained unchanged. This lack of localized control undermines the precision that the primitive-based editing promises."}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKUyTOoIMp", "forum": "ypFvQSXFNC", "replyto": "ypFvQSXFNC", "signatures": ["ICLR.cc/2026/Conference/Submission6205/Reviewer_VvNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6205/Reviewer_VvNT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739397859, "cdate": 1761739397859, "tmdate": 1762918543501, "mdate": 1762918543501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Learn to do controlled spatial edits to a photo by learning to extract editable primitive 3D representations of the image contents which can then be modified by a human easily and conditioned on do generate a new view of the modified scene. Core idea is to find a way of doing 3D modifications to an image that is easier to do for a human and condition a diffusion model on."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Lots of qualitative demos for the paper and the quality of the model at preserving scene contents."}, "weaknesses": {"value": "Very minimal quantitative results to compare with other works, quantitative results feel like they're lacking overall. There are other works that do a similar style of task prompting from optical flow / correspondences (motion prompting and go-with-the-flow were the ones I knew, but you also referenced drag-diffusion). I might be wrong, but it feels like some more quantitative comparisons could be done against these kinds of models perhaps? This paper is an interesting way of approaching the problem of image edits but as someone that isn't working on this specific problem, I just don't have enough of an idea of how this sits in the landscape of existing works from the quantitative results provided."}, "questions": {"value": "Is there any reason why more quantitative baselines cannot be run to better inform the audience of how this work sits in comparison to other existing works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "miK7gtAAwn", "forum": "ypFvQSXFNC", "replyto": "ypFvQSXFNC", "signatures": ["ICLR.cc/2026/Conference/Submission6205/Reviewer_6yrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6205/Reviewer_6yrZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878663690, "cdate": 1761878663690, "tmdate": 1762918543130, "mdate": 1762918543130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for 3d aware image editing. This method directly edits 3D primitives, which is very accurate, and more intuitive to interact. The method can be divided into these stages: 1) decompose 2d image into 3d primitives, using pretrained convex decomposition models 2) edit the 3d primitives 3) render the modified primitives into depth map and texuture hint image 4) use a diffusion model Flux Depth to convert the depth map and texture hint image into the final edited image"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This method is training free, by leveraging existing models, it is able to provide a very good image editing result\n2. The speed is very fast, better than other 3d aware editing methods I known\n3. The texture Hint injection is pretty good, do not require training the diffusion model, just doing the injection during inference, but still get a very good result. It even keeps the text texture, which is amazing.\n4. Overall, I love this paper very much, it leverages foundamental graphics techniques, but achieving something big! The quality is very good, and the methods offer a great speed.\n5. The failure cases are fully discussed"}, "weaknesses": {"value": "Thanks a lot for the authors taking a time to discuss the failure cases, I feel the discussions are very valuable. All the weaknesses are acceptable, and I feels some can be solved by more advanced models within this framework. For example, the first row in Figure 8 is very likely to be a failure of Flux Depth, not the framework itself."}, "questions": {"value": "1. Is it possible to show some hint images for text textures, like the cases in row 3 of Figure 8, I feel this will helps us to better understand the texture mismatch."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4pznSkl8Jy", "forum": "ypFvQSXFNC", "replyto": "ypFvQSXFNC", "signatures": ["ICLR.cc/2026/Conference/Submission6205/Reviewer_UHGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6205/Reviewer_UHGg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156889790, "cdate": 1762156889790, "tmdate": 1762918542810, "mdate": 1762918542810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}