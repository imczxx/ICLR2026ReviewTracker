{"id": "Mr9a9XuLTI", "number": 8211, "cdate": 1758074425044, "mdate": 1763077814566, "content": {"title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection", "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial examples—subtle, imperceptible perturbations that can lead to incorrect predictions. While detection-based defenses offer a practical alternative to adversarial training, many existing methods depend on external models, complex architectures, or adversarial data, limiting their efficiency and generalizability. We introduce a lightweight, plug-in detection framework that leverages internal layer-wise inconsistencies within the target model itself, requiring only benign data for calibration. Our approach is grounded in the **A Few Large Shifts Assumption**, which posits that adversarial perturbations induce large, localized violations of *layer-wise Lipschitz continuity* in a small subset of layers. Building on this, we propose two complementary strategies—**Recovery Testing (RT)** and **Logit-layer Testing (LT)**—to empirically measure these violations and expose internal disruptions caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under both standard and adaptive threat models, our method achieves state-of-the-art detection performance with negligible computational overhead. Furthermore, our system-level analysis provides a practical method for selecting a detection threshold with a formal lower-bound guarantee on accuracy.", "tldr": "We propose a lightweight, plug-in adversarial detection method that leverages internal layer inconsistencies in a DNN to detect adversarial examples without requiring adversarial data, complex data structure, or external models.", "keywords": ["Adversarial detection", "deep learning", "layer inconsistency", "robust defense", "adaptive attacks."], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b4c48eafbebd4ad48dbbe551b727e01cc2aae1c7.pdf", "supplementary_material": "/attachment/c2bfa00df902d033afb59d4d74949db59fafcd22.zip"}, "replies": [{"content": {"summary": {"value": "This paper is on detection of adversarial inputs.\nEvaluations of the proposed method are given on\nCIFAR-10, CIFAR-100, and ImageNet datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Detecting adversarial inputs remains a challenging problem."}, "weaknesses": {"value": "The paper relegates to the appendix important issues such as the setting of the hyperparameters tau (line 156). Also, where does the \"Lipschitz\" parameters D come from in Assumption 1?  (For that matter, clearly define an adversarial input at this point.)\n\nThe fonts in Fig 1 and the ables are too small.  Within the\ntext, some of the mathematical expressions are hard to discern,\ne.g., softmax on line 164 (which I don't think is the standard\ndefinition).\n\nL_RT involves a \"benign\" dataset x_n of N samples. How big does N need to be?\nConsidering the authors proposed regression of a \"lightweight MLP\", how\nthe number of benign samples should be a point of comparison\nof different prior methods against the proposed ones, i.e.,\nN should be clearly indicated in the tables for the different methods.\nAre N samples employed per class in the proposed method?\n\nThe paper does not cite and compare against important related work, e.g., the early paper [1] (listed below) uses internal layers and compares the final layer activations against intermediate layers.  Other approaches build anomaly detectors during the training process, e.g., using (class conditional GANs [2] -- this method is easily applied to internal layers and the GANs could be learned post-training using enough benign data (N) as required train a lightweight MLP for regression.\n\n[1] D.J. Miller et al.  Anomaly Detection of Attacks (ADA) on DNN Classifiers at Test Time.  Neural Computation 31(8), Aug. 2019.\n\n[2] H. Wang et al.  Anomaly Detection of Test-Time Evasion Attacks Using Class-Conditional Generative Adversarial Networks.  Elsevier Computers & Security (COSE) 124, Jan 2023."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BXFmLA4b1o", "forum": "Mr9a9XuLTI", "replyto": "Mr9a9XuLTI", "signatures": ["ICLR.cc/2026/Conference/Submission8211/Reviewer_wnVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8211/Reviewer_wnVR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087662206, "cdate": 1761087662206, "tmdate": 1762920158334, "mdate": 1762920158334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the AC and Reviewers oikH and sue6 for their thoughtful and constructive reviews. We will incorporate their valuable suggestions in our revised manuscript."}}, "id": "fYc7o3BRzr", "forum": "Mr9a9XuLTI", "replyto": "Mr9a9XuLTI", "signatures": ["ICLR.cc/2026/Conference/Submission8211/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8211/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763077813652, "cdate": 1763077813652, "tmdate": 1763077813652, "mdate": 1763077813652, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors present a defense against adversarial examples, leveraging two complementary strategies to perform detection. The first, Recovery Testing (RT), is based on training regressors to reconstruct intermediate features of each defended network's layer from its final layer's output. An input sample is labeled as adversarial if its inner layers' representations diverge from the reconstructed ones. The second, Logit-layer Testing (LT), applies input perturbations and computes features and logit discrepancies. The two approaches can also be combined in a single RLT score that (as RT and LT) can be thresholded to perform the detection."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very clear and well-written.\n- The authors justify each choice behind the design of their defense.\n- The research problem is still open, and the provided contribution is relevant in this sense.\n- The experimental evaluation shows a clear improvement with respect to the considered competing approaches."}, "weaknesses": {"value": "- The provided robust accuracy under worst-case adaptive attackers reveals that the defense, in such a scenario (which is very relevant when considering security-related applications), is quite weak.\n\n- (minor) Additionally, I'm a bit skeptical about adversarial example detectors, as they often have been broken by well-crafted attacks that are able to overcome the defense mechanism. The authors"}, "questions": {"value": "- As maximum-confidence adversarial attacks might overemphasize inconsistencies across layers, thus making them more detectable by the proposed defense, it would be very interesting to evaluate the defense against state-of-the-art minimum-distance attacks, such as FMN [a] or DDN [b]. Could you please provide some results about that?\n\n[a] Pintor, M., Roli, F., Brendel, W., & Biggio, B. (2021). Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints. ArXiv, abs/2102.12827.\n\n[b] Rony, J., Hafemann, L.G., Oliveira, L., Ayed, I.B., Sabourin, R., & Granger, E. (2018). Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 4317-4325."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R3DGgKQ0eS", "forum": "Mr9a9XuLTI", "replyto": "Mr9a9XuLTI", "signatures": ["ICLR.cc/2026/Conference/Submission8211/Reviewer_oikH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8211/Reviewer_oikH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581634427, "cdate": 1761581634427, "tmdate": 1762920157919, "mdate": 1762920157919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Recovery and Logit Testing Combined framework, a novel solution for adversarial example detection. It is founded on the \"A Few Large Shifts Assumption.\" The method employs two complementary strategies to detect inconsistencies in the intermediate features and the final Logit layer, respectively. Operating as a lightweight, plug-in approach, this method enhances performance with less computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The RLT framework it proposes is based on the A Few Large Shifts Assumption and includes a clear comparison with existing methods.  \n2. The paper is clearly written, the figures and tables are easy to understand, and the overall flow of the text is good.  \n3. The paper demonstrates advantages in both accuracy and efficiency compared to existing methods, and its effectiveness is validated through experiments."}, "weaknesses": {"value": "1. The paper could benefit from further quantification of its core assumption. Although the phenomenon is empirically demonstrated through RT and LT, the work lacks a stronger theoretical or mathematical proof to explain why perturbations cause this local and disproportionate damage in \"a small subset of layers\" rather than being uniformly distributed across all layers.  \n2. The applicability of the proposed framework, particularly the Recovery Testing (RT) module, is limited by its training data being restricted exclusively to benign samples. This leaves the discussion incomplete regarding its robustness on more diverse datasets."}, "questions": {"value": "A Few Large Shifts Assumption is interesting. Are there any visualized experimental results available to validate this hypothesis? Furthermore, is there any specific analysis concerning these particular layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qFQGtgq6ZZ", "forum": "Mr9a9XuLTI", "replyto": "Mr9a9XuLTI", "signatures": ["ICLR.cc/2026/Conference/Submission8211/Reviewer_sue6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8211/Reviewer_sue6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655561687, "cdate": 1761655561687, "tmdate": 1762920157603, "mdate": 1762920157603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a plug-in detector that uses only the target model’s own internals and benign data to flag adversarial inputs. The core hypothesis, the A Few Large Shifts (FLS) Assumption, says adversarial perturbations create localized violations of layer-wise Lipschitz continuity at a small subset of layers. Two complementary tests are introduced: Recovery Testing (RT), which trains light regressors to reconstruct intermediate features from the last embedding and detects peaked reconstruction-error profiles; and Logit-layer Testing (LT), which learns a few mild, data-driven augmentations and detects cases where logits change disproportionately to internal feature drift. A fused score RLT combines both via quantile normalization. Experiments on CIFAR-10/100 and ImageNet report strong AUC under standard attacks and robust accuracy (RA) at fixed FPRs under adaptive attacks (Orthogonal-PGD; end-to-end PGD on the fused score). The paper also gives a system-level thresholding rule guaranteeing a lower bound on overall accuracy. (See Fig. 2 for the pipeline; Tables 2–3 for AUC; Tables 4–7 for adaptive/black-box; Fig. 3 for empirical support of FLS; Table 8 and App. G for overhead; App. H for system-level guarantees.)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- Clear, self-contained detection paradigm. The method uses only the target network’s layer traces and benign calibration—no adversarial data, no external SSL encoder, no kNN graphs. The diagram on p. 3 (Fig. 2) cleanly shows RT/LT and their fusion.\n\n2- Well-articulated mechanism + measurable proxies. The FLS assumption is made operational via layer-wise reconstruction errors (RT) and logit-vs-feature sensitivity ratios (LT), tied to local Lipschitz language and formalized in App. B with theorems motivating separability and the RLT fusion\n\n3- Strong empirical coverage (CNNs and ViT). On CIFAR-10/100 and ImageNet, RLT matches or beats baselines—including BEYOND—with far lower overhead (see Table 2 on p. 6 and Table 3 on p. 6; Tables 17–18 extend to multiple CNNs and ViT-B/16).\n\n4- Adaptive-attack evaluation beyond “paper rules.” The paper includes Orthogonal-PGD (Table 4), end-to-end PGD directly optimizing −Lcls + λ·RLT (Table 5), and a gradient-free SimBA stress test (Table 7) to address gradient obfuscation concerns.\n\n5- Overhead is explicitly quantified and tunable. Table 8 (main) and Tables 19–22 (App. G) report FLOPs/params/size; the method attains high AUC with small MLP regressors and few learned augmentations (G≤4; App. D.2)."}, "weaknesses": {"value": "1- Threat-model precision and attack breadth. While Orthogonal-PGD, end-to-end PGD on RLT, AutoAttack, and SimBA are included, the main tables emphasize ℓ∞ (and one ℓ2 ablation). Missing: EOT against any stochasticity in the learned augmentations, multi-restart ablations for PGD (step-size/steps/restarts grids), and transfer from surrogate models in a detection sense. \n\n2- Assumptions around RT \"invertibility.\" RT presumes the existence of approximate inverses from $z_L$ to earlier $z_k$ (Assumptions 2-3). This is plausible but not tested for distribution shift, different pre-act placements, or non-image domains; stability may be architecture- and dataset-dependent.\n\n3- Learned augmentations (LT) could be attackable. The augmentations $W^{(g)}$ are learned to preserve benign logits yet change features. An adaptive attacker can coopt this structure (differentiate through BPDA) to reduce the LT ratio (App. B. 5 notes conflict but gives no worst-case rate). A stronger evaluation with EOT or randomized augmentation families would shore this up.\n\n4- Quantile normalization as a potential leakage channel. RLT relies on benign-fit CDFs; an adaptive attacker with knowledge of this mapping (Perfect-Knowledge) could target score massaging near quantile knees. The paper implements BPDA through quantiles but does not include explicit attacks that optimize post-quantile loss landscapes.\n\n5- Calibration vs. cost tradeoffs not fully charted. App. G shows pointwise overheads and some Pareto comparisons, but there is no global Pareto frontier of (AUC/RA@FPR, FLOPs, Params, latency) across kRT/kLT, MLP width/depth, G—useful for deployment decisions\n\n6- Potential class-imbalance or per-class heterogeneity. Detection is reported as overall AUC/RA@FPR; per-class TPR/FPR (or per-image difficulty) aren’t shown. Since RT learns regressors over all benign data, hard classes might calibrate poorly.\n\n7- ImageNet evaluation is partial. Table 3 omits AutoAttack for baselines “due to resources,” complicating SOTA claims; only DenseNet-121 is used as the ImageNet backbone; no results for BN-heavy architectures or modern ViT-L/ConvNeXt-XL on ImageNet.\n\n8- Robustness to benign distribution shift. App. I tests random noise at fixed label (Table 27) which is helpful, but broader benign shifts (blur/lighting/cropping, CIFAR-C, ImageNet-C) and OOD (e.g., CIFAR-10.1/10.2) are not shown; false positives could rise on benign shifts."}, "questions": {"value": "1- PGD tuning & multi-restart. Please report step-count/step-size/restart sweeps for PGD (e.g., steps 10→200, step-size ε/10→ε, restarts 1→50), and show RA@FPR at the strongest discovered settings—especially for the end-to-end PGD on RLT in Table 5.\n\n2- EOT / stochasticity. Are the learned augmentations $W^{(g)}$ deterministic at test time? If not, add EOT-APGD and EOT-SimBA; if yes, test randomized ensembles of $W$ at inference and evaluate EOT against that randomized LT.\n\n3- Transfer-based detection attacks. Beyond SimBA, can you craft surrogate-modelguided inputs that both fool $f$ and minimize RLT (e.g., via a surrogate with similar RT/LT regressors/augmentations) and report transfer success?\n\n4- Invertibility stress tests. Show RT performance when the recovery MLPs are mis-specified (too shallow/too narrow), when you freeze them after partial training, and under benign shift (CIFAR-C/ImageNet-C). Does RT become over-sensitive (FPR↑) on blur/contrast shifts?\n\n5- Where is RT read from? For architectures with pre-act vs post-act conventions, batch-norm placements, and skip connections, how do choices of $z_k$ extraction points affect separability? Provide a small layer-selection study.\n\n6- Adapting to $W^{(g)}$. Evaluate a white-box attack that optimizes $-\\mathrm{Lcls}+\\lambda \\cdot \\mathrm{LT}$ with BPDA through quantiles and, crucially, adds regularization toward the benign quantiles of LT (post-CDF) to directly exploit the fusion step. Does RT rescue RLT in that setting?\n\n7- Randomized augmentation families. Replace fixed $W^{(g)}$ with a distribution over $W$ (e.g., low-rank affine or small spatial transforms) learned on benign data; report RA@FPR with and without EOT. \n\n8- Quantile-attack ablation. Craft attacks minimizing $R T_{\\text {norm }}$ or $L T_{\\text {norm }}$ post-quantile and compare with minimizing the raw RT/LT. Provide per-example scatter of (raw score, normalized score) to illustrate attackable knees in the empirical CDF.\n\n\n9- Pareto frontiers. For a fixed backbone, chart (AUC or RA@FPR, FLOPs, params, latency) across $k_{R T}, k_{L T}$, MLP sizes, and $G$. This helps practitioners select deployments under cost constraints.\n\n10- Broader datasets and tasks. Add ImageNet AA baselines (even if subset) and at least one non-vision or higher-resolution setting; include physical re-photography for a small subset to assess LT’s real-world stability.\n\n11- Benign distribution shift. Report FPR under CIFAR-C/ImageNet-C at thresholds tuned on clean validation (as in Table 27 but with semantic corruptions), and provide per-corruption results.\n\n## Suggestions\n\n1- Add EOT, PGD-grid sweeps, and a quantile-aware adaptive attack to harden the evaluation; include transfer-based detection attacks.\n\n2- Provide per-class TPR/FPR and distribution-shift FPR (CIFAR-C/ImageNet-C).\n\n3- Publish Pareto curves (accuracy vs cost) for RT/LT/RLT"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rapvE57M9k", "forum": "Mr9a9XuLTI", "replyto": "Mr9a9XuLTI", "signatures": ["ICLR.cc/2026/Conference/Submission8211/Reviewer_VLTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8211/Reviewer_VLTm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997790153, "cdate": 1761997790153, "tmdate": 1762920157250, "mdate": 1762920157250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}