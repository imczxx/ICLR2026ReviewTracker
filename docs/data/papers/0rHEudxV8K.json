{"id": "0rHEudxV8K", "number": 4050, "cdate": 1757592045883, "mdate": 1763061224044, "content": {"title": "FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing", "abstract": "Recent advances in pre-trained text-to-image flow models have enabled remarkable progress in text-based image editing. Mainstream approaches always adopt a corruption-then-restoration paradigm, where the source image is first corrupted into an ``intermediate state'' and then restored to the target image under the prompt guidance. However, current methods construct this intermediate state in a target-agnostic manner, i.e., they primarily focus on realizing source image reconstruction while neglecting the semantic gaps towards the specific editing target. This design inherently results in limited editability or inconsistency when the desired modifications substantially deviate from the source. In this paper, we argue that the intermediate state should be target-aware, i.e., selectively corrupting editing-relevant contents while preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel inversion-free and flow-based editing framework that parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process. By iteratively editing the source to the target and recovering back to the source with dual consistency constraints, FlowCycle learns to produce a target-aware intermediate state, enabling faithful modifications while preserving source consistency. Extensive ablations have demonstrated that FlowCycle achieves superior editing quality and consistency over state-of-the-art methods.", "tldr": "", "keywords": ["Cycle-Consistent", "Flow Matching", "Diffusion Models", "Image Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0fcbb31494500729914fb95073fe0ebd6edbfad6.pdf", "supplementary_material": "/attachment/e44750c8d12597f84b72fc381b93727bab842bca.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses text-based image editing with pre-trained rectified-flow models and argues that the usual corruption-then-restoration pipeline constructs a target-agnostic intermediate state, which limits editability and/or source consistency when the target edit departs from the source image. The authors propose FlowCycle, an inversion-free framework that learns a target-aware intermediate state by optimizing two learnable noises (ϵ_src, ϵ_tar) in a cycle between “source→target” and “target→source.” The optimization minimizes a recovery loss aligning the recovered image with the source and an alignment loss bringing the two corrupted intermediate states together, thereby encouraging corruption on editing-relevant regions while preserving editing-irrelevant content. Experiments on PIE-Bench using SD-3-medium report improved source-consistency metrics and competitive CLIP alignment versus recent baselines; ablations, qualitative results, and limited analysis on SD-1.5 are also provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Method quality and simplicity.** The mechanism—parameterizing corruption with learnable noises and optimizing with two MSE losses (recovery and alignment)—is simple to implement atop standard FM inference (Euler solver) and requires no architectural changes to the backbone. The losses and inference path are precisely specified. \n* **Clarity.** The paper clearly situates FlowCycle within the corruption-then-restoration paradigm and articulates the three-step cycle (source→target, target→source, and cycle optimization), aided by a succinct objective and inference description. \n* **Empirical significance.** On PIE-Bench, FlowCycle achieves the best or second-best results on most source-consistency metrics while maintaining solid CLIP alignment, indicating a favorable balance. Extensive qualitative comparisons and an optimization-steps ablation further support the claims."}, "weaknesses": {"value": "* **Compute/latency overhead.** The approach is optimization-based; the authors acknowledge it is relatively time-consuming, and tabled timings show noticeable overhead versus optimization-free methods—raising practical concerns for interactive editing.  \n* **Per-edit optimization and generalization.** The learnable noises are optimized per source–target pair, with limited evidence on reuse. A figure suggests some transfer across similar edit patterns, but systematic protocols for reusing noises are not established.  \n* **Evaluation scope.** Experiments rely primarily on PIE-Bench with automatic metrics; there is no human study, no multi-turn editing evaluation, and limited testing beyond SD-3-medium (one SD-1.5 table), limiting claims of generality.  \n* **Comparative fairness and coverage.** Some baselines (e.g., FlowAlign) are reported from the paper rather than reproduced (not open-sourced), and it is unclear whether stronger recent editing techniques outside the flow family were considered."}, "questions": {"value": "* **Sensitivity to hyperparameters.** How sensitive is performance to t, λ, and CFG scales? The appendix gives a table, but a more principled analysis (or defaults robust across datasets) would help practitioners. \n* **Failure modes.** Please characterize typical failures (e.g., strong geometric edits, cluttered scenes, small objects) and whether L_align in pixel space encourages undesirable content averaging; are feature-space variants of L_align beneficial? \n* **Beyond PIE-Bench.** Any plans to evaluate additional editing benchmarks or to include a small user study to corroborate automatic metrics, especially for perceived source consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G0W7wUzjoo", "forum": "0rHEudxV8K", "replyto": "0rHEudxV8K", "signatures": ["ICLR.cc/2026/Conference/Submission4050/Reviewer_dhTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4050/Reviewer_dhTm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760934515801, "cdate": 1760934515801, "tmdate": 1762917153640, "mdate": 1762917153640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "PZMDJ1NE9n", "forum": "0rHEudxV8K", "replyto": "0rHEudxV8K", "signatures": ["ICLR.cc/2026/Conference/Submission4050/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4050/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763061223303, "cdate": 1763061223303, "tmdate": 1763061223303, "mdate": 1763061223303, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FlowCycle is a flow-based, inversion-free framework that learns target-aware image editing through a cycle-consistent optimization process. It adds learnable noises to the source and target images, transforming them back and forth to enforce consistency between the two domains. By minimizing reconstruction and corruption losses, the model learns to focus corruption on editing-relevant regions while preserving other content. At inference time, the optimized noise produces a target-aware intermediate state, enabling accurate and consistent text-based image editing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Unlike conventional approaches, FlowCycle selectively corrupts only editing-relevant regions, preserving unrelated content and achieving faithful, precise modifications.\n\n2.By enforcing dual consistency between source and target images, it maintains structural and semantic integrity, ensuring strong source consistency even after major edits.\n\n3.FlowCycle demonstrated superior performance across most evaluation metrics."}, "weaknesses": {"value": "1.There is a lack of detailed comparison with baselines. As the authors explained in the Related Work section, existing text-based image editing methods encompass a variety of approaches. However, the authors only presented superior metric evaluations on PIEBench as evidence of performance.\n\n2.Although flow-based methods are still in their early stages — and thus could potentially demonstrate large performance improvements — FlowCycle does not show a notably significant improvement compared to existing approaches.\n\n3.While the method improves performance through a simple mechanism that incorporates MSE loss, there is some doubt as to whether this provides sufficient novelty or contribution."}, "questions": {"value": "1.How does the time consumption of FlowCycle compare to inversion-free corruption methods? For instance, I think FlowAlign, which does not compute CFG for both the source and target, would likely show better efficiency in terms of time consumption.\n\n2.FlowCycle shows very similar performance to FlowAlign across most metrics except for Distance. Please explain in more detail what advantages FlowCycle has over FlowAlign. The explanation provided in Section 4.1 of the paper seems somewhat insufficient.\n\n3.The attempt to optimize FlowCycle is interesting; however, the use of MSE and λ appears to be a rather simple approach. For example, in the ablation study on λ presented in Table 4, the results seem more sensitive to changes in source CFG and target CFG than to λ itself. Is adding λ as a new parameter to the existing source and target CFG hyperparameters truly effective? What would happen if λ were set to 1 to give equal weight to alignment and reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52SSdIxKWl", "forum": "0rHEudxV8K", "replyto": "0rHEudxV8K", "signatures": ["ICLR.cc/2026/Conference/Submission4050/Reviewer_1QJ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4050/Reviewer_1QJ7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570571522, "cdate": 1761570571522, "tmdate": 1762917153370, "mdate": 1762917153370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FlowCycle, a novel inversion-free, flow-based image editing framework that addresses a key limitation of existing text-based image editing methods: the target-agnostic nature of intermediate states. FlowCycle proposes a target-aware corruption strategy by parameterizing the corruption process with learnable noises optimized under cycle-consistency constraints. Extensive experiments on PIE-Bench show that FlowCycle achieves better source consistency and competitive semantic alignment compared to five state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel conceptual insight: Identifies and addresses the overlooked issue of target-agnostic corruption in flow-based editing.\n- Generalization evidence: Demonstrates that learned intermediate states generalize to similar editing patterns."}, "weaknesses": {"value": "- Optimization cost: The method requires iterative noise optimization (≈100 steps), making it slower than optimization-free baselines. The authors acknowledge this but do not propose clear mitigation strategies.\n- Limited theoretical depth: The paper motivates target awareness intuitively but lacks a rigorous theoretical link between the loss formulation and selective corruption behavior.\n- Marginal semantic gains: While source consistency improves significantly, the improvement in semantic alignment (CLIP Edited score) over FlowAlign is relatively small."}, "questions": {"value": "- How sensitive is the method to the λ parameter and timestep t — do small deviations significantly affect quality?\n- Would integrating attention-based masks for editing-relevant regions further enhance target awareness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yHjKMw3Xmh", "forum": "0rHEudxV8K", "replyto": "0rHEudxV8K", "signatures": ["ICLR.cc/2026/Conference/Submission4050/Reviewer_ZYcS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4050/Reviewer_ZYcS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898995984, "cdate": 1761898995984, "tmdate": 1762917153189, "mdate": 1762917153189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FlowCycle is a text-based image editing framework for rectified-flow models that rethinks how the noisy intermediate state is constructed during editing. Instead of using target-agnostic noise or ODE inversion optimized only for reconstruction, the method learns two prompt-dependent noise tensors so that the corrupted state becomes more “target-aware,” disrupting editing-relevant regions while preserving the rest. These noises are optimized via a bidirectional cycle: source→target→source, with pixel-wise alignment at the intermediate step and reconstruction at the image level, while keeping the pretrained flow model frozen. Experiments on PIE-Bench and additional DDPM-based tests show markedly better source consistency and also competitive target alignment compared with recent flow-based editors such as FlowEdit and FlowAlign."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a concrete limitation of existing flow-based editors—target-agnostic intermediate states tuned for reconstruction rather than editing, then builds the entire method around overcoming this bottleneck.\n\n- FlowCycle only optimizes noise tensors and does not fine-tune the underlying rectified-flow model, which makes it conceptually lightweight and easy to plug into a variety of backbones and even DDPMs.\n\n- On PIE-Bench, the method achieves the best/near-best scores on most consistency metrics (e.g. bg quality, structural distance) while maintaining CLIP-based alignment close to the strongest baselines.\n\n- The analysis of optimization steps, guidance scales, and the SD-1.5/DDPM variant shows that the core idea is not tied to a specific rectified-flow model.\n\n- The paper situates FlowCycle among inversion-based, inversion-free, injection-based, and flow-based editing methods, making it clear how this work complements trajectory-regularization approaches and optimization-free designs."}, "weaknesses": {"value": "- Each edit requires many optimization iterations and two denoising trajectories per iteration, which leads to latency and substantial GPU usage, which makes it hard to deploy in interactive editing tools.\n\n- The method is motivated by selectively corrupting editing-relevant regions, yet the losses act uniformly in pixel space and the experiments do not report region-wise analysis of the intermediate state.\n\n- Large structural changes, multi-object scenarios, or multi-attribute edits are not explored in depth, so the robustness of the approach in more demanding settings remains unclear.\n\n- While the SD-1.5 experiments include some DDIM/SDEdit baselines, there is no detailed comparison to SOTA inversion-based diffusion editors under comparable conditions, making it harder to assess whether flow-based editing + FlowCycle is truly preferable in practice."}, "questions": {"value": "- How sensitive is the method to the balance between intermediate alignment and source reconstruction losses, and have you explored schedules or adaptive weighting that might better handle large semantic changes?\n\n- Do you have quantitative evidence—using region masks from PIE-Bench—that the learned intermediate state indeed corrupts editing-relevant regions more strongly than background?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ydOlZZCJd3", "forum": "0rHEudxV8K", "replyto": "0rHEudxV8K", "signatures": ["ICLR.cc/2026/Conference/Submission4050/Reviewer_LEif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4050/Reviewer_LEif"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247193635, "cdate": 1762247193635, "tmdate": 1762917152590, "mdate": 1762917152590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}