{"id": "S7KyLgHqJf", "number": 1571, "cdate": 1756893013413, "mdate": 1759898200631, "content": {"title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding", "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. \nTo address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features  (1) a diverse, multi-level difficulty dataset covering 24 examination types, (2) 13 varying-difficulty tasks,  (3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning,  and (4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare.", "tldr": "", "keywords": ["Chain-of-Thought", "Multimodal Large Language Models", "M3CoTBench", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb9f55061d213c5326bb0aa48d8e920d41fc87a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces M3CoTBench, a benchmark for evaluating Chain-of-Thought reasoning in medical image understanding MLLMs. The dataset contains 1,079 image-QA pairs across 24 medical imaging modalities with expert-annotated reasoning steps and four evaluation dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Addresses Critical Gap: First comprehensive benchmark for CoT reasoning in medical imaging - important for clinical AI transparency and trust.\n- High-Quality Curation:\n  - Diverse coverage: 24 modalities from 55 public datasets\n  - Rigorous annotation: Multi-stage validation with medical experts\n  - Clinical alignment: 4-step reasoning framework mirrors diagnostic workflows\n- Novel Evaluation Framework: Four dimensions (correctness, efficiency, impact, consistency) provide comprehensive CoT assessment beyond accuracy.\n- Extensive Evaluation: Tests 13 MLLMs including general-purpose, reasoning-focused, and medical-specific models with interesting findings about CoT effectiveness."}, "weaknesses": {"value": "Methodological Concerns: \n- The dataset comprises only 1,079 images, relatively small compared to other medical reasoning benchmarks (e.g., OmniMedVQA with 118K+ images).\n- Potential Bias: Although reasoning steps undergo expert validation and revision, their initial generation by GPT-4o may introduce biases inherent to its reasoning style, which might persist despite subsequent human refinement.\n- Evaluation Circularity: The study uses GPT-4o both to generate reasoning chains and to evaluate them against GPT-4o-based gold standards, creating a circular evaluation loop.\n- The paper does not specify which MLLMs were used for flagging potentially incorrect reasoning steps.\n\nEvaluation Concerns:\n- Despite the multi-expert validation process, no inter-annotator agreement scores are reported.\n- Confidence intervals and significance tests for performance differences are not provided.\n- The number of experts involved and procedures for resolving disagreements are not described.\n\n\nConceptual Issue:\n- Counterintuitive Findings: The universally negative impact of reasoning across models raises questions about the benchmark’s design, the quality of the Chain-of-Thought implementation, and the validity of using GPT-4o as the evaluation reference."}, "questions": {"value": "- How do you address evaluation circularity when using GPT-4o to assess GPT-4o reasoning?\n- What are the inter-annotator agreement scores during expert validation?\n- Why do most models show negative reasoning impact, is this a CoT implementation issue or benchmark design problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OnEKx1rD9D", "forum": "S7KyLgHqJf", "replyto": "S7KyLgHqJf", "signatures": ["ICLR.cc/2026/Conference/Submission1571/Reviewer_28Mb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1571/Reviewer_28Mb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761224923944, "cdate": 1761224923944, "tmdate": 1762915818664, "mdate": 1762915818664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M3CoTBench, a benchmark designed to evaluate chain-of-thought (CoT) reasoning in multimodal large language models (MLLMs) for medical image understanding. M3CoTBench comprises a diverse dataset spanning 24 imaging modalities (X-rays, MRIs, endoscopy, etc.) and 13 task types, ranging from low-level tasks like image quality assessment to high-level clinical reasoning such as diagnosis and treatment planning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper introduces M3CoTBench, encompassing 24 imaging modalities to evaluate MLLMs' understanding capabilities across diverse medical imaging contexts.\n2. The benchmark introduces tailored metrics to assess reasoning quality across four dimensions: correctness of each reasoning step, efficiency cost, impact on final answer accuracy, and logical consistency—providing a more nuanced evaluation beyond traditional accuracy measures."}, "weaknesses": {"value": "1. M3CoTBench spans 24 modalities and 13 task types, but contains only 1,079 image-based QA pairs. Given this broad coverage, does each category have sufficient samples? The paper does not appear to provide per-category statistics.\n2. The benchmark’s dataset, while diverse, is relatively small (only 1079 Q&A pairs) compared to other medical VQA datasets, which may limit the statistical breadth of evaluation."}, "questions": {"value": "1. LLaVA-CoT exhibits relatively strong performance compared to Gemini 2.5 Pro. The authors attribute this to its architecture and training process, which emphasize structured reasoning chains while minimizing irrelevant or misleading steps. However, given that Gemini 2.5 Pro also incorporates thinking capabilities, what accounts for this performance difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dqup17tthS", "forum": "S7KyLgHqJf", "replyto": "S7KyLgHqJf", "signatures": ["ICLR.cc/2026/Conference/Submission1571/Reviewer_iKQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1571/Reviewer_iKQE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811119579, "cdate": 1761811119579, "tmdate": 1762915818493, "mdate": 1762915818493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M3CoTBench, a benchmark designed to evaluate Chain-of-Thought (CoT) reasoning in multimodal large language models (MLLMs) for medical image understanding. It constructs a diverse dataset of medical images with clinically grounded reasoning annotations and proposes multidimensional evaluation metrics covering correctness, efficiency, impact, and consistency. Experiments on both open- and closed-source models show that while CoT improves interpretability, it does not always enhance diagnostic accuracy, revealing limitations in current MLLMs’ clinical reasoning and highlighting the need for more trustworthy and efficient medical CoT systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an emerging yet underexplored topic: evaluating Chain-of-Thought reasoning in medical multimodal LLMs, which is both timely and relevant to advancing trustworthy medical AI.\n2. The benchmark is validated on a broad range of both open- and closed-source MLLMs, providing a well-rounded comparison that highlights current model limitations and practical challenges in clinical reasoning."}, "weaknesses": {"value": "1. The definition of CoT in the medical area is unclear. Although the paper claims that its Chain-of-Thought (CoT) formulation “mirrors clinicians’ cognitive workflow”, the reasoning template shown in the Appendix appears overly simplified. It typically only has four steps: examination type -> key features -> key conclusion -> additional analysis. It is unclear why this sequence represents a gold standard reasoning path in clinical diagnosis. Is it based on any references, such as guidelines in medicine?\n2. The justification for diverse reference reasoning paths is insufficient. The paper mentions that “multiple valid reference reasoning paths may exist” and evaluates by matching the generated path to the most similar reference. While this makes sense conceptually, it is unclear how the annotation process ensures both diversity and correctness of reference reasoning paths. In the medical domain, it remains questionable whether clinicians indeed exhibit substantially diverse CoTs. If so, where does this diversity arise? Is it in identifying different key features (Step 2) or in drawing different key conclusions (Step 3)?\n3. The Reasoning Impact evaluation simply measures the performance difference between models with and without CoT, which seems redundant. This metric does not provide new insight into reasoning quality.\n4. The reasoning progression across CoT steps is weak. From the provided CoT examples, the reasoning flow among steps is not clearly causal or hierarchical. Steps 3 and 4 appear to be direct deductions from Step 2, while Step 1 (examination type) is largely independent of the reasoning process itself. As a result, it is difficult to claim that the sequence truly reflects a step-by-step reasoning chain rather than a loosely connected checklist."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JkLIxE5gfE", "forum": "S7KyLgHqJf", "replyto": "S7KyLgHqJf", "signatures": ["ICLR.cc/2026/Conference/Submission1571/Reviewer_Ti8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1571/Reviewer_Ti8j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991004711, "cdate": 1761991004711, "tmdate": 1762915818327, "mdate": 1762915818327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}