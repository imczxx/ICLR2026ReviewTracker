{"id": "tJU1wP6bW7", "number": 242, "cdate": 1756732364817, "mdate": 1759898270263, "content": {"title": "Evolvable Safety Benchmarking: Multi-agent Pipeline for LVLMs", "abstract": "Large vision-language models (LVLMs) exhibit remarkable capabilities in vision and language tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process and static complexity that fail to keep pace with rapidly evolving model architectures and emerging risks. To address these limitations, we propose VLSafetyBencher, the first multi-agent system designed to automated LVLM safety benchmarking, which introduces four collaborative agents: preprocessing, cross-modal processing, augmentation, and post-sampling agents. With the optimized sampling algorithm at the sample level, VLSafetyBencher can be conveniently applied to benchmark construction, update, and sample evaluation. We conduct experiments on benchmark construction and updating tasks with VLSafetyBencher and evaluate the extensive LVLMs. Our results demonstrate that the automatically generated dataset effectively distinguishes model safety, with a safety rate disparity of nearly 70% between the most and least safe models. Ablation analyses further validate VLSafetyBencher's effectiveness.", "tldr": "", "keywords": ["LVLM Evaluation", "Safety", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81086c8a7c4b6bcdaec8e0313ce78d5aa6c05c51.pdf", "supplementary_material": "/attachment/3d393ef072e353b073f3e5377eef09c1e43d76c7.pdf"}, "replies": [{"content": {"summary": {"value": "The authors present VLSafetyBencher, an automated and evolvable safety benchmarking framework for Large Vision-Language Models. The system addresses limitations of existing benchmarks such as static datasets and manual annotation by employing a multi-agent pipeline that automatically generates, optimizes, and updates multimodal safety data. The framework includes four agents: a Preprocessing Agent that cleans and categorizes data into six risk types; a Cross-Modal Agent that creates image–question pairs following principles of modality dependency, complementarity, and conflict; an Augmentation Agent that diversifies and strengthens samples via paraphrasing and jailbreak-like transformations; and a Post-Sampling Agent that selects optimal samples by maximizing separability, harmfulness, and diversity. Benchmark quality is assessed through four metrics. Experiments show that VLSafetyBencher outperforms prior benchmarks like SafeBench and AutoBencher, achieving higher separability and diversity, while remaining fully automated and inexpensive. Evaluating 35 LVLMs reveals significant safety variation closed-source models like GPT-4o and Claude-Sonnet-4 achieve over 90% safety rates, while open-source models lag behind."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors introduce a novel and fully automated framework for LVLM safety benchmarking, which significantly reduces human effort and enables continuous, scalable evaluation as models evolve. This automation represents a clear advancement over existing static, manually curated benchmarks.\n\n2. The system is highly efficient and cost-effective, completing the entire benchmark generation process within a week at an extremely low cost (around $1.34) using only standard API calls. This makes it practical for iterative or large-scale deployment.\n\n3. The framework’s design for evolvability is conceptually impactful. By allowing dynamic regeneration and replacement of benchmark samples, it enables long-term adaptability to new threat patterns, model architectures, and emerging safety concerns."}, "weaknesses": {"value": "1. The framework poses a clear misuse risk since it can be repurposed to automatically generate large-scale harmful or adversarial content. While the authors emphasize safety benchmarking, they do not discuss any ethical safeguards, access control, or mitigation strategies to prevent malicious exploitation. This omission raises serious concerns about dual-use potential.\n\n2. The system heavily relies on LLM-based judgments for labeling and filtering, which may inherit or amplify the biases, inconsistencies, and hallucinations of the underlying models. Without human validation or bias correction, the resulting benchmarks risk reflecting model-specific moral or cultural biases rather than objective safety standards.\n\n3. The proposed evaluation metrics (MAD, MEAN, GAP, DIV) are intuitive but heuristic, lacking strong theoretical justification or empirical validation. The weighting parameters (α, β, γ) are manually tuned, and there is no adaptive or data-driven mechanism to optimize them.\n\n4. The reliance on closed-source APIs such as GPT-4o and Claude limits reproducibility and transparency, as other researchers cannot replicate results under identical conditions.\n\n5. The authors present a fully automated pipeline but omit any human evaluation or expert verification, which undermines the reliability of the generated benchmark and its alignment with human safety norms."}, "questions": {"value": "1. The framework uses LLMs to assess harmfulness and safety. How do the authors ensure that the model used for labeling does not bias the benchmark toward its own alignment characteristics? Have they tested whether using a different evaluator model changes the benchmark ranking?\n\n2. The Cross-Modal Agent is said to construct samples under three strategies modality dependency, complementarity, and conflict. Can the authors provide quantitative statistics on how many samples fall under each type, and whether any one type dominates the final benchmark composition?\n\n3. The benchmark quality metrics (MAD, MEAN, GAP, DIV) are primarily statistical. Was there any human evaluation or expert audit conducted to verify that the benchmark samples actually reflect real-world safety risks or harmful content?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The proposed framework can automatically generate, manipulate, and optimize harmful or adversarial multimodal content as part of its benchmarking process. While the authors intend this for safety evaluation, the same methodology could be repurposed to mass-produce unsafe, toxic, or misleading data, effectively serving as a harmful content generator. Moreover, the paper does not discuss access control, misuse prevention, or responsible release strategies, leaving open the risk of unethical or malicious application. In short, the framework provides valuable safety insights but also introduces dual-use risks that warrant careful ethical consideration and oversight."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NwGpo6UMzb", "forum": "tJU1wP6bW7", "replyto": "tJU1wP6bW7", "signatures": ["ICLR.cc/2026/Conference/Submission242/Reviewer_pLqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission242/Reviewer_pLqf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760599563292, "cdate": 1760599563292, "tmdate": 1762915478741, "mdate": 1762915478741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper utilizes multi-agent pipeline to synthesize harmful image-question pair and introduces the optimization-based sampling method to select high-value test data. Various experiments confirm the effectiveness of proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The framework is clear and easy to follow specific agent operation."}, "weaknesses": {"value": "This paper has several weaknesses.  \n\nFirst, as stated in the Framework, the Cross-Modal Agent groups samples into Modality Dependency, Complementarity, and Conflict. It is unclear whether this classification is 100% accurate. If some cases are misclassified, what consequences would these misclassified samples bring? If the classification is perfect, does that imply a super-powerful agent?  \n\nSecond, the Augmentation Agent includes various text and image augmentations. The authors should release which augmentation is most effective and provide rationale analysis. This would strengthen the paper’s solidity.  \n\nThird, for the POST-SAMPLING AGENT, the authors use three metrics: Separability, Harmfulness, and Diversity. However, the authors should rethink why these three metrics are chosen over others. Additionally, regarding the optimization weights for these three modules, Figure 3 does not clearly report the optimal weight combination. While the authors provide some discussion in Line 460, it is insufficient.  \n\nIn summary, I acknowledge that this paper is interesting and the authors have conducted various evaluation experiments. However, the rationale for the method and the discussion of ablation studies remain important."}, "questions": {"value": "Refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gfSj1y2aJf", "forum": "tJU1wP6bW7", "replyto": "tJU1wP6bW7", "signatures": ["ICLR.cc/2026/Conference/Submission242/Reviewer_XLQD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission242/Reviewer_XLQD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760756575866, "cdate": 1760756575866, "tmdate": 1762915478523, "mdate": 1762915478523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VLSafetyBencher, a fully automated multi-agent system for LVLM safety benchmarking, addressing high costs, staticity, and weak discriminative power of existing benchmarks. It uses four collaborative agents (Preprocessing, Cross-Modal, Augmentation, Post-Sampling) and an optimization-based sampling algorithm to select high-quality data. Experiments show it constructs benchmarks efficiently (1 week, $1.34 cost), outperforms baselines in metrics like GAP (≈70%), and effectively evaluates 35 LVLMs, revealing large safety disparities. Code and data will be released post-acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "First, it automates LVLM safety benchmarking via 4 collaborative agents, cutting construction time to ~1 week and cost to $1.34, far less than manual methods.\n\nSecond, its optimization-based sampling boosts discriminative power, showing a 70% safety rate gap between top and bottom LVLMs, outperforming baselines.\n\nThird, it enables dynamic benchmark updates, enhancing existing ones (e.g., SafeBench) by replacing low-efficiency samples to improve metrics."}, "weaknesses": {"value": "First, The paper has relatively low innovation, as its multi-agent framework and sampling methods build on existing automated benchmarking ideas.\n\nSecond, It fails to detail the agent system construction, with no supplementary info even in the appendix."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W779lyWTNw", "forum": "tJU1wP6bW7", "replyto": "tJU1wP6bW7", "signatures": ["ICLR.cc/2026/Conference/Submission242/Reviewer_aTJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission242/Reviewer_aTJA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381172880, "cdate": 1761381172880, "tmdate": 1762915478397, "mdate": 1762915478397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VLSafetyBencher, a multi-agent system designed to automate the construction and updating of safety benchmarks for large vision-language models (LVLMs). The framework integrates four agents—Preprocessing, Cross-Modal, Augmentation, and Post-Sampling—to perform data cleaning, harmful multimodal pair generation, augmentation, and optimization-based sampling. The authors formalize benchmark desiderata (separability, harmfulness, diversity) and propose an iterative selection algorithm to optimize these criteria. Experiments show that the generated benchmarks outperform prior works such as SafeBench, MLLMGuard, and AutoBencher, achieving greater discriminative power and construction efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Trendy and practically meaningful topic on LVLM safety evaluation  \n- Clear presentation and strong experimental validation  \n- Automated pipeline reduces human cost and enables benchmark updating  \n- Comprehensive ablation analysis and comparison with prior benchmarks"}, "weaknesses": {"value": "- Limited technical novelty \n- Dataset construction relies on existing collected data"}, "questions": {"value": "The paper proposes a well-structured and practically useful system for automating LVLM safety benchmark construction. The design of multiple agents and the optimization-based selection pipeline are clearly presented and empirically validated.  \n\nHowever, two major concerns limit the impact of this work at a top-tier venue: \n\nFirst, compared with prior agent-based benchmark construction frameworks such as AutoBencher (Li et al., 2024), the claimed innovation of this paper is not sufficiently substantial. AutoBencher already formulates benchmark generation as an automated agent pipeline, the proposed VLSafetyBencher essentially inherits the same high-level agent-based paradigm, with the main modification being the shift from textual to multimodal (LVLM) data and the transition from dataset-level optimization to sample-level. However, this change mainly reflects a granularity adjustment, rather than a conceptual or algorithmic breakthrough.  \n\nSecond, the dataset construction of VLSafetyBencher heavily depends on existing collected or filtered data sources rather than generating novel, risk-evolving, or task-driven safety samples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CAQOBDHeVT", "forum": "tJU1wP6bW7", "replyto": "tJU1wP6bW7", "signatures": ["ICLR.cc/2026/Conference/Submission242/Reviewer_nzLn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission242/Reviewer_nzLn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961561552, "cdate": 1761961561552, "tmdate": 1762915478266, "mdate": 1762915478266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}