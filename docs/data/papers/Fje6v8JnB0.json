{"id": "Fje6v8JnB0", "number": 17601, "cdate": 1758278075070, "mdate": 1759897165511, "content": {"title": "FLEX: A Largescale Multimodal, Multiview Dataset for Learning Structured Representations of Fitness Action Quality", "abstract": "Action Quality Assessment (AQA)—the task of quantifying how well an action is performed—has great potential for detecting errors in gym weight training, where accurate feedback is critical to prevent injuries and maximize gains. Existing AQA datasets, however, are limited to single-view competitive sports and RGB video, lacking multimodal signals and professional assessment of fitness actions. We introduce FLEX, the first large-scale, multimodal, multiview dataset for fitness AQA that incorporates surface electromyography (sEMG). FLEX contains over 7,500 multi-view recordings of 20 weight-loaded exercises performed by 38 subjects of diverse skill levels, with synchronized RGB video, 3D pose, sEMG, and physiological signals. Expert annotations are organized into a Fitness Knowledge Graph (FKG) linking actions, key steps, error types, and feedback, supporting a compositional scoring function for interpretable quality assessment. FLEX enables multimodal fusion, cross-modal prediction—including the novel Video→EMG task—and biomechanically oriented representation learning. Building on the FKG, we further introduce FLEX-VideoQA, a structured question–answering benchmark with hierarchical queries that drive cross-modal reasoning in vision–language models. Baseline experiments demonstrate that multimodal inputs, multi-view video, and fine-grained annotations significantly enhance AQA performance. FLEX thus advances AQA toward richer multimodal settings and provides a foundation for AI-powered fitness assessment and coaching.", "tldr": "", "keywords": ["Action Quality Assessment", "Video Understanding", "Computer Vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fe831978a19ae2fa3889d4482a6f442043ef587.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FLEX, a multimodal and multiview dataset for fitness action quality assessment focusing on weight-loaded exercises. It contains over 7,500 synchronized recordings of 20 exercises performed by 38 participants with varying skill levels, including RGB videos, 3D poses, sEMG, and physiological signals. The dataset adopts a biomechanically inspired annotation framework that divides each action into phases and key steps, uses a two-stage expert annotation process, and integrates a Fitness Knowledge Graph linking actions, errors, and feedback. FLEX also provides two benchmark tasks, FLEX-AQA and FLEX-VideoQA, to support multimodal and cross-modal learning. Overall, the paper provides a well-annotated dataset with multiple benchmarks, but the individual components’ value and research significance are insufficiently explored, resulting in limited generalizability and practical impact."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a large-scale **weight training dataset** that combines RGB, 3D pose, sEMG, and physiological data for weight training assessment.\n\n- The paper is **clearly organized**, with systematic explanations of data acquisition, annotation, and benchmark design.\n\n- The paper enables new research directions such as cross-modal prediction and biomechanically grounded learning, offering long-term value for applications in fitness assessment."}, "weaknesses": {"value": "The main weaknesses of the paper lie in scope limitation, insufficient justification, and weak experimental analysis. \n- Although the paper claims to target general fitness assessment, the dataset focuses **narrowly on weight-loaded exercises**, which limits its generality and potential impact; a clearer rationale for choosing this subdomain and comparisons with existing fitness datasets (e.g., FineGym, Human3.6M) are needed to establish necessity. \n- The motivation for collecting a new dataset rather than extending existing ones is underdeveloped; the authors should demonstrate that weight training poses **unique biomechanical or perceptual challenges** that existing datasets cannot capture. \n- While the dataset is multimodal and multiview, the paper **lacks sufficient justification and systematic analysis** to demonstrate the practical value of each modality and camera view. It remains unclear whether all modalities, such as sEMG, physiological signals, and five synchronized views, are truly necessary or feasible in real-world weight training scenarios. The study does not adequately discuss the trade-off between performance improvement and practical deployment cost. Moreover, the limited and simple ablation analysis fails to quantify the marginal utility of each modality or to justify which sensory inputs are most critical for accurate and efficient assessment.\n- The benchmark evaluation is limited: many baselines are outdated, and results are already near saturation, suggesting low challenge; comparisons with recent multimodal or transformer-based methods would strengthen the analysis. \n- The annotation process and participant statistics are **insufficiently detailed** (e.g., annotator expertise, gender, and skill distribution, balance across skill levels), which raises questions about bias and scalability."}, "questions": {"value": "- Beyond data collection, what are the deeper scientific questions or representation learning challenges this dataset aims to address? Without clearer theoretical or practical motivation, the contribution risks being perceived as incremental rather than fundamental.\n- Why focus exclusively on weight-loaded exercises? The paper claims to target general fitness assessment, yet the dataset scope is restricted to weight training. What unique research challenges or biomechanical properties make this subdomain essential and non-replaceable?\n- Many contributions (e.g., structured scoring, error labeling, Fitness Knowledge Graph) rely on manual annotation rather than model innovation. Could the authors clarify which aspects of AQA truly require additional annotations instead of leveraging better model architectures or self-supervised representations?\n- What is the concrete motivation for using five camera views and multiple physiological signals such as sEMG and respiration? How significant is each modality’s contribution in realistic deployment scenarios (e.g., home fitness or gym settings)? Is the marginal performance gain worth the complexity and cost?\n- Given the specialized setup, how can this dataset generalize to broader populations or different exercise types? Have the authors evaluated cross-subject or cross-action generalization to assess the dataset’s robustness?\n- What is the background of annotators, and how was inter-rater consistency measured? Are there potential biases in skill level, gender, or body type distribution among the 38 subjects that might affect evaluation fairness?\n- Most baseline methods are outdated and already achieve high performance. Could the authors explain how FLEX remains challenging for modern multimodal models, and whether additional experimental tasks or splits (e.g., cross-device, cross-load) could better demonstrate its research value?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "The dataset lacks sufficient information about participant diversity and annotator background, making potential data bias insufficiently addressed and raising concerns about its fairness and reliability."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qLvT7C8OLN", "forum": "Fje6v8JnB0", "replyto": "Fje6v8JnB0", "signatures": ["ICLR.cc/2026/Conference/Submission17601/Reviewer_vzMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17601/Reviewer_vzMp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760497607085, "cdate": 1760497607085, "tmdate": 1762927465735, "mdate": 1762927465735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FLEX, a new multi-view, multi-modal dataset for quality assessment of weight-loaded actions. The dataset contains 7,500 multi-view recordings of 20 weight-loaded exercises performed by 38 subjects, with synchronized RGB video, 3D pose, sEMG, and physiological signals. The authors also introduce FLEX-VideoQA, a question–answering benchmark for weight-loaded actions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- For the first time, the authors introduce an AQA (Action Quality Assessment) and VideoQA dataset specifically for weight-loaded exercises, which has the potential to benefit the research community and society.\n\n- Although it is not the only multi-view AQA dataset, there are only a few such datasets available, and this work could make a meaningful contribution to the field."}, "weaknesses": {"value": "1- Incomplete literature review: The related work section does not cover the key datasets in the area of multi-view, multi-modal action quality assessment. For example, the QMAR dataset [i], which is the first multi-view multi-modal AQA dataset, is not mentioned. As one of the few existing multi-view AQA datasets, the authors should compare FLEX’s features with QMAR.\n[i] F. Sardari et al., “Vi-net—view-invariant quality of human movement assessment,” Sensors, 2020.\n\n2- Lack of justification for multi-view setup: While they release a multi-view dataset for weight-loaded exercises, the main question remains unanswered: How does the multi-view data and setup help assess the quality of weight-loaded exercises? There are no experiments investigating this question. Furthermore, to support such investigations, the paper should also introduce a method that leverages the multi-view setup to demonstrate its advantages compared to a single-view setup, or employ a view-invariant methodsetup such as the one introduced in [i].\n\n3-Limited evaluation with AQA methods: The dataset should be evaluated using the most recent AQA methods [ii, iii, …], but only one relatively outdated method, TPT (2022), is applied (Table 3). The remaining methods used are primarily action recognition approaches, not quality assessment approaches.\n\n[ii] j. Xu, \"FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment\", CVPR 2024.\n[iii] K.  Zhou, \"Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization\", ECCV, 2024.\n\n4-Non-standard scoring function: The authors define their own score (i.e., compositional scoring function). However, in AQA, a standard scoring function must be used for evaluation. While their defined score may be useful for training, evaluation should be performed using a standard international score. Alternatively, both scores could be reported for completeness.\n\n5-Lack of clarity on score range: The paper does not explain the range or interpretation of the defined scores.\n\n6-Unclear use of additional signals: Although the dataset includes various physiological signals (e.g., heart rate), it is not clear how these are utilized or what benefits they bring.\n\n7-Unfair dataset comparisons (Table 1): The comparison with other AQA datasets is not fair. For example, the number of subjects and total dataset duration are not reported. The FLEX dataset includes 38 subjects, which is significantly less diverse than EgoExo, which involves over 700 participants. While FLEX has around 7,000 samples, the total video hours are not reported, making it unclear whether the dataset is truly large-scale.\n\n8-Unclear feedback computation: It is not clear based on what scores, metrics, or baselines the corrective feedback was computed. What is the joint baseline used?\n\n9-Evaluation by experts is missing: While the annotators are trained, both scores and QA annotations should be validated by domain experts after the annotation phase.\n\n10-the paper presentation needs a lot of improvements, for example:\n- They oversell the paper. e.g., “In comparison, existing datasets lack this multirepetition capture of weight training.” However, repetition is a standard practice in dataset collection. \n- Unnecessary implementation details are included, e.g., “Staff assisted subjects in wearing the customized vest,” which is not appropriate for the main paper of a top machine learning conference.\n- Misleading section titles and trivial tasks: For example, under “Action Segmentation,” the authors describe standard dataset preparation steps as if they were novel contributions.\n- Table caption issues: In Table 1, the meaning of “E” is not explained in the caption."}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tlGolp2MnS", "forum": "Fje6v8JnB0", "replyto": "Fje6v8JnB0", "signatures": ["ICLR.cc/2026/Conference/Submission17601/Reviewer_gH6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17601/Reviewer_gH6A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760804559122, "cdate": 1760804559122, "tmdate": 1762927463694, "mdate": 1762927463694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FLEX, an Action Quality Assessment dataset built from multimodal : five view videos, 3D pose, sEMG, and body metrics. The dataset is sizable—7,512 samples covering 20 exercises performed by 38 subjects with varied skill levels. Annotations are expert-curated and of high quality. The paper also presents the FLEX-VideoQA benchmark (about 30k QA pairs) and a video2EMG prediction task with baseline models. The experimental program has three components; each is reasonable and yields useful insights. The paper also addresses ethical considerations related to data collection and release. Overall, this is a good paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-instrumented dataset. The overview and setup details are clear.\n- Knowledge graph and supervision. The FLEX Knowledge Graph (FKG) links actions, key steps, error types, muscles, and feedback. The dataset annotations are well described and carefully produced.\n- Broad, well-organized experiments. The experimental suite is diverse and well structured, and the analyses are detailed.  The statements are detailed enough to slove potential ethical concerns"}, "weaknesses": {"value": "- Minor typos and inconsistencies. For example: line 340, “a models’ ability” should be “a model’s ability”; line 456, “signals provides” should be “signals provide”; line 286, “Concentration” should be “Concentric” to match Figure 1.\n- Insufficient introduction of evaluation metrics in the experiments section.\n- Potentially limited challenge. AQA results in Section 4.1 appear strong; this may reduce the dataset’s difficulty for current models. Consider discussing harder splits or protocols."}, "questions": {"value": "- Could the authors share a small anonymized subset or code via an anonymous GitHub repository to demonstrate data quality?\n- Could side information about participants (for example, age and height) be included—subject to privacy constraints—to broaden potential research uses?\n- In Table 2, MAE scores are very close across models. Can the authors explain why, and/or propose alternative metrics or tasks that provide better separation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a8m05q1r2q", "forum": "Fje6v8JnB0", "replyto": "Fje6v8JnB0", "signatures": ["ICLR.cc/2026/Conference/Submission17601/Reviewer_3nbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17601/Reviewer_3nbD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121171459, "cdate": 1762121171459, "tmdate": 1762927463126, "mdate": 1762927463126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLEX, the first dataset designed explicitly for fitness scenarios. It integrates multi-view videos, 3D poses, and other physiological data into an Annotated Action Quality Assessment (AQA) dataset. The multi-modal input, multi-view videos, and fine-grained annotations enhance AQA performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It integrates multi-view videos, 3D poses, surface electromyography (sEMG), heart rate/respiratory rate, and expert annotations, expanding the application scenarios of AQA.\n2. A structured Fitness Knowledge Graph (FKG) is constructed. Beyond providing scalar quality scores, the paper introduces a hierarchical annotation system covering Action Keysteps, Error Types, muscle activation relationships, and corrective feedback, which is further formalized into a knowledge graph."}, "weaknesses": {"value": "1.There is a lack of ablation experiments on the Fitness Knowledge Graph (FKG). Specifically, no ablation experiments were conducted to verify the specific contributions of each component in FKG—such as Action Keysteps, Error Types, and Feedback—to the performance of AQA or VideoQA. For instance, would the model performance drop significantly if only global quality scores (without the FKG structure) were used? This question directly relates to the practical necessity of FKG.\n2.In the AQA experiments of this paper, multi-modal fusion adopts the method of \"feature concatenation + relative muscle contribution calculation\". This fusion approach is relatively simple, and there is a lack of analytical experiments on other feature fusion methods.\n3.The computational cost of the experiments in this paper is high. Has consideration been given to how to reduce computational resources to meet the needs of lightweight deployment in the future?\n4. In Table 3, there is limited data on the performance of other methods on the FLEX dataset. Methods like CoRe and TPT are relatively early fine-grained action modeling approaches, so it is advisable to incorporate more recent work and conduct basic experiments for analysis."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "azubKLhgT6", "forum": "Fje6v8JnB0", "replyto": "Fje6v8JnB0", "signatures": ["ICLR.cc/2026/Conference/Submission17601/Reviewer_7RBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17601/Reviewer_7RBq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163387465, "cdate": 1762163387465, "tmdate": 1762927462624, "mdate": 1762927462624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FLEX, a pioneering multimodal, multiview dataset for fitness Action Quality Assessment (AQA), featuring over 7,500 recordings from 38 subjects performing 20 exercises with synchronized RGB video, 3D pose, sEMG, and physiological signals. It includes a Fitness Knowledge Graph for interpretable scoring and introduces FLEX-VideoQA, a benchmark for hierarchical video question-answering to enable cross-modal reasoning and AI-driven fitness coaching."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a new multimodal AQA dataset with over 7,000 samples, incorporating multimodal information such as synchronized surface EMG, RGB video, 3D joints, point clouds, and body metrics. Additionally, the dataset provides structured annotations and a Fitness Knowledge Graph. Based on the paper, the annotation quality appears high. Therefore, I believe that while the dataset is not particularly large, its superior quality still offers substantial value.\n- Building on this dataset, the authors developed a fine-grained video question-answering benchmark with hierarchical questions, spanning from coarse action recognition to detailed error diagnosis and causal feedback generation.\n- Furthermore, the authors explored multimodal, cross-modal, and biomechanically-oriented representation learning."}, "weaknesses": {"value": "- The proposed dataset includes information from multiple modalities, but the experimental section does not fully leverage these multimodal elements, and the inter-modal associations require deeper investigation. As a result, while the dataset inherently benefits from multimodality, this advantage is not adequately demonstrated in the experiments.\n- Although the authors introduce a new dataset and some methods, it is worth considering whether these methods possess general applicability and can be extended to other domains. After all, the dataset's scenarios are relatively narrow, focusing solely on fitness-related video QA tasks.\n- The paper does not clarify whether the proposed dataset will be open-sourced. If it is not made publicly available, the paper's contributions will be significantly undermined, as other researchers would be unable to access it for further studies and validation."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SNTwZxKrVC", "forum": "Fje6v8JnB0", "replyto": "Fje6v8JnB0", "signatures": ["ICLR.cc/2026/Conference/Submission17601/Reviewer_TsmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17601/Reviewer_TsmB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762788181363, "cdate": 1762788181363, "tmdate": 1762927462237, "mdate": 1762927462237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}