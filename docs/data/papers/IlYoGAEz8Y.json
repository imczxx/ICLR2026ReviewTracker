{"id": "IlYoGAEz8Y", "number": 1217, "cdate": 1756865692028, "mdate": 1759898220726, "content": {"title": "Equivariant Spherical Transformer for Efficient Molecular Modeling", "abstract": "Equivariant Graph Neural Networks (GNNs) have significantly advanced the modeling of 3D molecular structure by leveraging group representations. However, their message passing, heavily relying on Clebsch-Gordan tensor product convolutions, suffers from restricted expressiveness due to the limited non-linearity and low degree of group representations. To overcome this, we introduce the Equivariant Spherical Transformer (EST), a novel plug-and-play framework that applies a Transformer-like architecture to the Fourier spatial domain of group representations. EST achieves higher expressiveness than conventional models while preserving the crucial equivariant inductive bias through a uniform sampling strategy of spherical Fourier transforms. As demonstrated by our experiments on challenging benchmarks like OC20 and QM9, EST-based models achieve state-of-the-art performance. For the complex molecular systems within OC20, small models empowered by EST can outperform some larger models and those using additional data. In addition to demonstrating such strong expressiveness,we provide both theoretical and experimental validation of EST's equivariance as well, paving the way for new research in this area.", "tldr": "", "keywords": ["Graph Neural Networks", "Molecular Model", "Equivariant Model"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/246e2bdb84f2b92034f9b955d0ffc3f29ef985c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Equivariant Spherical Transformer (EST), a novel framework for 3D molecular modeling. The authors posit that existing Equivariant Graph Neural Networks (GNNs), which heavily rely on Clebsch-Gordan (CG) tensor product convolutions, suffer from limited non-linearity and expressiveness. EST aims to overcome this by operating in the Fourier spatial domain. The method works by transforming steerable group representations into a sequence of points on a sphere, then applying a Transformer-based architecture to model dependencies between these points.\n\nThe core architectural ideas are promising. I am open to increasing my score during the rebuttal phase if the authors can provide the necessary clarifications and more rigorous comparisons to fully substantiate their claims against other SOTA models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical analysis (Theorem 1) and empirical validation (Table 5) of how spherical sampling uniformity (Fibonacci lattice vs. e3nn grid) impacts $SO(3)$ equivariance is a valuable and rigorous contribution\n2. The \"hybrid mixture of experts\"  is a thoughtful design choice, allowing the model to explicitly balance the strict equivariance of steerable FFNs with the enhanced expressive power of spherical FFNs.\n3. The model demonstrates very strong results on the large-scale and challenging OC20 benchmark, outperforming several larger models and those trained with additional data."}, "weaknesses": {"value": "1.  The paper's most significant weakness is its flawed handling of GotenNet, a key state-of-the-art baseline that also avoids CG transforms.\n    - The justification provided, that GotenNet uses an \"extensive training schedule\", is methodologically unsound. It's well-established that different network architectures may require different training schedules (epochs, batch sizes, optimizers) to converge optimally. This, by itself, is not a valid academic reason to exclude a major baseline from a SOTA comparison.\n    - Furthermore, if the authors' unstated concern was computational cost, \"extensive schedule\" (i.e., 1000 epochs with a 32 batch size ) is not a valid proxy for this. The relevant metric is total wall-clock training/inference time. A model with a lower per-epoch cost could be more efficient overall, even with more epochs. The authors fail to provide this crucial efficiency comparison, making it impossible to evaluate their claims of efficiency and performance against a top competitor.\n        \n2. The comparison that _is_ provided in Appendix E.3 (Table 11) is flawed and does not constitute a fair evaluation.\n    - The authors compare against the 4-layer GotenNet, which is the smallest, weakest, and known-to-be-underparameterized version of that model. The original GotenNet paper's main results clearly show significant performance gains from its 6-layer (Base) and 12-layer (Large) models, which are conveniently ignored here.\n    - The comparison model, misleadingly named \"EST (with GATA)\", is not a clean comparison. It is, in fact, the full 4-layer GotenNet architecture (using its initialization, GATA message block, and EQFF) where the authors have added their spherical FFN component to run in parallel with GotenNet's existing EQFF block . This is a (GotenNet + EST components) hybrid, not a direct EST vs. GotenNet comparison.  The negligible performance difference shown in Table 11 on an already weak baseline demonstrates little, if any, benefit from the EST addition and can likely be attributed to a simple increase in parameters.\n    - The authors could have simply trained their proposed EST model with the \"extensive schedule\" for a direct comparison. The choice to instead create a convoluted, modified 4-layer GotenNet hybrid strongly suggests that the standard EST model, when trained under an equivalent schedule, did not achieve comparable SOTA results.\n\n3. Given that both architectures are transformer-based and use attention (GATA in GotenNet), a much more insightful and direct comparison would have been to integrate the proposed spherical attention/relative orientation embedding _directly into_ GotenNet's GATA module. This would have tested the EST's core components as a _replacement_ or _enhancement_ to an existing equivariant attention mechanism, rather than an appendage to a different block (EQFF).\n4. The \"plug-and-play\" claim is undermined by the OC20 experiments. The paper's SOTA results on OC20 (Table 1, Table 2) are not from the full EST architecture (Figure 3). Instead, they use a hybrid model where the EST's message block is replaced by the message block from Equiformer/EquiformerV2. This makes it impossible to isolate the contribution of the novel EST update block. The strong performance could be largely attributed to the borrowed Equiformer message passing, not the novel spherical transformer."}, "questions": {"value": "1. What is the performance (MAE) of the *full* EST architecture (using its native message block from Figure 3b) on the OC20 benchmarks, as opposed to the hybrid model reported in Tables 1 & 2?\n\n2. To quantify the \"hybrid mixture of experts\" trade-off, what is the MAE and latency (ms/batch) when using *only* steerable FFN experts versus *only* spherical FFN experts?\n\n3. How do the authors reconcile the model's strong theoretical expressiveness on synthetic n-fold symmetry tasks (Table 4) with its practical performance on the QM9 benchmark (Table 3), which is not SOTA on all tasks?\n\n4. What is the practical trade-off between the number of sampling points ($S$), model accuracy (MAE on QM9), and latency (ms/batch)?\n\n5. Could the authors provide end-to-end training and inference latency (ms/batch) for EST against *all* key SOTA baselines from Table 3, including Equiformer, EquiformerV2, and GotenNet, using identical hardware and batch sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TM3RHZBEEI", "forum": "IlYoGAEz8Y", "replyto": "IlYoGAEz8Y", "signatures": ["ICLR.cc/2026/Conference/Submission1217/Reviewer_mddm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1217/Reviewer_mddm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761331575670, "cdate": 1761331575670, "tmdate": 1762915712112, "mdate": 1762915712112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Equivariant Spherical Transform (EST), a new \"plug-and-play\" framework that applied a Transformer-like architecture to the Fourier spatial domain of group representations in order to achieve higher expressivity than tensor-product based equivariant model. EST preserves equivariance and achieves good performance on OC20 and QM9 benchmarks. The authors further provide theoretical support for their central claims about EST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Strengths\n- The authors provide a good explanation of the Spherical Fourier Transform and of their proposed method. The writing is overall easy to follow and understand. \n- The fact that EST is more expressive than tensor-product-based models is a very strong and novel contribution\n- EST can be integrated into existing model designs, making is a flexible approach that can build on existing works"}, "weaknesses": {"value": "### Weaknesses\n- The authors do not have a very convincing set of experiments. OC20 and QM9 are older and relatively saturated datasets, and most recent works on MLIPs are training and evaluating on the SPICE-MACE-OFF [1] and MPtrj datasets [2]. The paper is also missing comparisons to many recently developed MLIPs such as eSEN [3].\n- The authors do not attempt to train a larger scale \"foundation\" model based on EST or provide ablation experiments to demonstrate the scaling of the proposed method. While EST is a strong theoretical contribution, it may be the case that expressivity doesn't matter when the model is sufficiently large/training on diverse data. The paper would be more convincing if EST could outperform SOTA foundation models.\n\n\n[1] MACE-OFF: Transferable Short Range Machine Learning Force Fields for Organic Molecules, Kovács et al. (2023), https://arxiv.org/abs/2312.15211\n\n[2] CHGNet: Pretrained universal neural network potential for charge-informed atomistic modeling, Deng et al. (2023), https://arxiv.org/abs/2302.14231\n\n[3] Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction, Fu et al. (2025), https://arxiv.org/abs/2502.12147"}, "questions": {"value": "### Questions (related to above weaknesses)\n- Can the authors train EST on MPtrj and evaluate on Matbench Discovery?\n- Can the authors train a large model(s) to demonstrate the scalability of EST?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pDdZnPciCQ", "forum": "IlYoGAEz8Y", "replyto": "IlYoGAEz8Y", "signatures": ["ICLR.cc/2026/Conference/Submission1217/Reviewer_qRcS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1217/Reviewer_qRcS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699709603, "cdate": 1761699709603, "tmdate": 1762915711792, "mdate": 1762915711792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new architecture for equivariant modeling. They begin by presenting the concepts of spherical and spatial representations of functions defined on a sphere. Prior works such as SCN and ESCN apply only pointwise nonlinearities on grids, which limits computations to individual nodes. This paper proposes a novel self-attention mechanism that defines attention across pairs of nodes, thereby enhancing expressiveness. To preserve equivariance, the authors introduce a uniform spherical sampling strategy for the attention computation. Building on these ideas, they develop a new architecture that achieves good empirical results on OC20 and QM9."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed theoretical framework presents valuable insights that could substantially contribute to the future development of equivariant architectures."}, "weaknesses": {"value": "1. The primary weakness lies in the insufficient experimental evaluation. Specifically, the experiments lack efficiency comparisons against existing architectures such as Equiformer, eSEN, and Equiformer V2. Additionally, the introduction of the mixture-of-hybrid-experts module appears orthogonal to the proposed theoretical framework, which diminishes the perceived contribution of the spherical attention mechanism. It gives the impression that the theoretical innovation provides limited practical gains, while most of the performance improvement stems from unrelated components. To strengthen the paper, the authors should introduce minimal modifications to an established architecture and compare the efficiency/performance of the proposed method within that controlled setting. For instance, a variant such as “Equiformer + Spherical Attention” would offer a clearer assessment of the method’s effectiveness.\n2. Lack of results on the OC20 test set weakens the empirical validation. Moreover, Table 1 is difficult to interpret and confusing; the authors should follow the formatting used in the EscAIP paper by clearly separating training (All or All+MD) and test sets (val or test) to improve readability and comparability.\n3. The proof of the main theorems in Appendix C.1 lacks clarity. The authors should justify the drop of the rotation terms in Equation (34), lines 3 and 4, and explain how this leads to problems when the formulation is discretized.\n4. Theorem 1 asserts strict SO(3) equivariance under the condition that the sample set P is closed under arbitrary rotations. However, for any finite P, this condition cannot hold except in trivial cases: only the continuous sphere satisfies closure, whereas a finite grid does not. The paper instead relies on the notion that “uniform sampling approximates closure,” which yields **approximate**, rather than strict, equivariance. The authors should revise this claim accordingly.\n\nMinor Problems:\n1. Double citation on Gotennet.\n2.    In Eq. (11), you define $z_s = \\frac{2s-1}{S-1}$ and then use $\\sqrt{1-z_s^2})$. For $(s=S), (z_s>2)$, which is outside [-1,1]. I believe you intended something like $z_s = 1 - \\frac{2s-1}{S}) (or (\\frac{2s}{S}-1)$. Please fix this; otherwise sampling is ill-defined."}, "questions": {"value": "1. What is the time-complexity of the proposed spherical attention in terms of $L$?\n2. Can you provide a bound relating sampling discrepancy to equivariance error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pUtwFz75TB", "forum": "IlYoGAEz8Y", "replyto": "IlYoGAEz8Y", "signatures": ["ICLR.cc/2026/Conference/Submission1217/Reviewer_Ls5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1217/Reviewer_Ls5n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772718470, "cdate": 1761772718470, "tmdate": 1762915711624, "mdate": 1762915711624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EST (Equivariant Spherical Transformer), which applies Transformer-like attention to the Fourier spatial domain of group representations for molecular modeling. The authors claim this achieves higher expressiveness than tensor product-based methods while maintaining SO(3) equivariance through uniform spherical sampling. They evaluate on OC20 and QM9 benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.  Applying attention mechanisms in the spherical spatial domain is an interesting alternative to tensor product operations\n2. Testing on both OC20 and QM9 with multiple metrics"}, "weaknesses": {"value": "1. Undefined Notations and Poor Presentation:\n- \"EST (with GA)\" in Table 3 is never defined in the main text\n- Multiple undefined abbreviations: EwT (Table 2), PW-Linear, DTP\n- Inconsistent notation (S² vs S^2, multiple uses of C for different dimensions)\n2. Theorem 1: Claims \"strict SO(3)-equivariance\" but this is impossible with finite sampling. \n3. Fibonacci lattice sampling (Eq.~11): incorrect formula \\& missing definitions. The manuscript writes the FL coordinates as\n$\n\\vec p_s = \\big[ p_1 - z_s^2 \\cos(2s\\pi/\\lambda),\\; p_1 - z_s^2 \\sin(2s\\pi/\\lambda),\\; z_s \\big], \\quad\nz_s = \\frac{2s-1}{S-1}, \\quad \\lambda=\\frac{1+\\sqrt5}{2},\n$\nwhich does not match the standard FL parametrization; the symbol $\\(p_1\\)$ is undefined and the $\\(x,y\\)$ components lack the $\\(\\sqrt{1-z_s^2}\\)$ factor.\n4. Discrete inverse SFT and quadrature weights (Eq.12) lacks mathematical justification. Eq. 12 defines\n$\nY^{(l,m)*}(\\vec p) = \\lambda(l,m)\\, Y^{(l,m)}(\\vec p), \\quad\n\\lambda(l,m) = \\frac{1}{\\sum_s Y^{(l,m)}(\\vec p_s)^2},\n$\nwhich is unclear and nonstandard.\n5. Proposition 2: The proof in Appendix C.2, where the claim that the spherical Transformer’s function space “encompasses” that of CG tensor products, is informal and insufficient.\n6. The exclusion of GotenNet as a baseline from main results is highly problematic.GotenNet is reported only in Appendix E.3 via a hybrid “EST with GATA” variant, not as a main baseline. This weakens a fair comparison to a relevant SOTA method.\n7. B.1.3 (relationship between spherical harmonics and Wigner-D) — incorrect equality: The appendix states\n$D^{(l,m)}(R_{\\alpha,\\beta,\\gamma}) = \\sqrt{2l+1}\\, Y^{(l,m)}(\\vec p)$\nwhich conflates Wigner-D matrix entries with spherical harmonics.\n8. Some notation and clarity issues:\n- Symbol overload: The symbol $\\(R\\)$ is used both for $\\(\\mathbb{R}\\)$ reals and rotations. Please use $\\(\\mathbb{R}\\)$ for the reals and $\\(R\\in\\mathrm{SO}(3)\\)$ (or boldface) for rotations consistently. \n- Relative orientation embedding (Eq.~13): You augment queries/keys with $\\(\\vec p_{s}\\)$. Do you normalize $\\(\\vec p_s\\)$  before concatenation to avoid numeric scaling issues? Please state any normalization and explain how the term $\\(\\vec p_{s_i}^\\top \\vec p_{s_j}\\)$ behaves under discrete sampling. \n- Dimensions and shapes: Ensure the dimension counts for $\\(Y^{(0\\to L)}(\\vec p)\\)$ and the shapes in Eq.~8 and Eq.~28--32 are consistent across the manuscript."}, "questions": {"value": "Below are explicit questions that, if answered or addressed in the revision, will substantially improve clarity, correctness, and the fairness of comparisons.\n1. Fibonacci lattice (Eq.~11): Please confirm the exact FL parametrization used. \n2. Nyquist/bandlimit requirement: You mention a minimum of $\\((2L)^2\\)$ sampling points (p.~6). Do you mean $\\((L+1)^2\\)$ spherical harmonic coefficients or a different bound tied to your quadrature scheme? Please clarify the precise sampling requirement as a function of $\\(L\\)$. \n3. Why is GotenNet not included in the main comparison tables? Can you report EST with your standard message block, EST with GATA and GotenNet as separate rows in main table, and provide a short discussion on whether and how the message block choices influences performance, holding the update block constant? \n4. Can you provide rigorous justification for Equation 12?\n5. What is the actual computational complexity comparison with tensor products?\n6. How does discretization-induced equivariance breaking affect downstream task performance?\n7. Discrete equivariance error: Table~5 shows empirical equivariance errors for different sampling strategies. Can you provide a short explanation linking these errors to the sampling uniformity and the chosen $\\(w_s\\)$? For example, do errors decay with $\\(S\\)$ as expected, and how does the dynamics optimization affect the quadrature accuracy? \n8 Controlled ablation with GATA: Can you add a controlled ablation that fixes the message block (GATA) and compares the update block used in GotenNet vs EST MoE (and vice versa), so readers can isolate where gains arise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Yawfm60cw7", "forum": "IlYoGAEz8Y", "replyto": "IlYoGAEz8Y", "signatures": ["ICLR.cc/2026/Conference/Submission1217/Reviewer_DiC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1217/Reviewer_DiC2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950925071, "cdate": 1761950925071, "tmdate": 1762915711515, "mdate": 1762915711515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}