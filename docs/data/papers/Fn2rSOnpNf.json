{"id": "Fn2rSOnpNf", "number": 10655, "cdate": 1758178718458, "mdate": 1763724784583, "content": {"title": "SlotGCG: Exploiting the Positional Vulnerability in LLMs for Jailbreak Attacks", "abstract": "As large language models (LLMs) are widely deployed, identifying their vulnerability through jailbreak attacks becomes increasingly critical. Optimization-based attacks like Greedy Coordinate Gradient (GCG) have focused on inserting adversarial tokens to the end of prompts. However, GCG restricts adversarial tokens to a fixed insertion point (typically the prompt suffix), leaving the effect of inserting tokens at other positions unexplored. In this paper, we empirically investigate slots, i.e., candidate positions within a prompt where tokens can be inserted. We find that vulnerability to jailbreaking is highly related to the selection of the slots. Based on these findings, we introduce the Vulnerable Slot Score (VSS) to quantify the positional vulnerability to jailbreaking. We then propose SlotGCG, which evaluates all slots with VSS, selects the most vulnerable slots for insertion, and runs a targeted optimization attack at those slots. Our approach provides a position-search mechanism that is attack-agnostic and can be plugged into any optimization-based attack, adding only 200ms of preprocessing time. Experiments across multiple models demonstrate that SlotGCG significantly outperforms existing methods. Specifically, it achieves 14% higher Attack Success Rates (ASR) over GCG-based attacks, converges faster, and shows superior robustness against defense methods with 42% higher ASR than baseline approaches.", "tldr": "", "keywords": ["LLM", "Jailbreak", "Adversarial Attack", "Safe AI"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ec7d716a730d9c32671e937d510d1429684aaf7.pdf", "supplementary_material": "/attachment/5223b39bc9505fc4cf79f983d62c4f500485dcdf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SlotGCG, which extends GCG jailbreak attacks by inserting adversarial tokens at multiple vulnerable positions throughout prompts rather than only at the suffix. The method uses a Vulnerable Slot Score based on attention patterns to identify optimal insertion positions. Experiments on 6 LLMs show average 14% ASR improvement, faster convergence, and 42% higher robustness against defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. well-motivated problem: The systematic exploration of positional vulnerability is underexplored. \n\n2. Comprehensive empirical validation: Testing across 6 models × 4 attack variants × 4 defenses with consistent improvements demonstrates robustness of the approach.\n\n3. Practical efficiency: The method adds only 200ms preprocessing but achieves up to 10× faster convergence, making it immediately deployable as a drop-in enhancement to existing GCG-based methods."}, "weaknesses": {"value": "1. SlotGCG shows no improvement or degradation on Mistral-7B and Vicuna-7B in Table 1, but the paper provides no analysis of why positional vulnerability varies across architectures. This limits understanding of when the method applies.\n\n2. The observation that defenses can increase ASR  due to GPT-4 filtering during optimization suggests the evaluation methodology itself may be problematic, undermining confidence in the reported improvements.\n\n3. Some hyperparameters lack justification, e.g., why temperature T=8? What happens with other layer selections or temperatures?"}, "questions": {"value": "1. Can you characterize what architectural or training differences cause SlotGCG to fail on Mistral/Vicuna?\n\n2. What is the performance with (1) only lower layers, (2) only upper layers, (3) random layer selection?\n\n3. AutoDAN also uses flexible token placement. How does SlotGCG compare in effectiveness and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hvcbneD3Ff", "forum": "Fn2rSOnpNf", "replyto": "Fn2rSOnpNf", "signatures": ["ICLR.cc/2026/Conference/Submission10655/Reviewer_dH2T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10655/Reviewer_dH2T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738806104, "cdate": 1761738806104, "tmdate": 1762921908655, "mdate": 1762921908655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that jailbreaking susceptibility depends strongly on where adversarial tokens are inserted. It introduces a Vulnerable Slot Score (VSS) to rank token positions by “positional vulnerability,” and proposes SlotGCG, which allocates/optimizes adversarial tokens at high-VSS slots rather than only at the suffix. Across Llama-2/3-8B, Mistral-7B, Vicuna-7B, and Qwen-2.5, SlotGCG reportedly improves attack success rate (ASR) over several GCG-family baselines, converges in fewer iterations, and remains more effective against several input-filtering defenses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a clear and original problem framing, defining insertion slots, formalizing the Vulnerable Slot Score (VSS), and linking it to attention patterns to show that positional vulnerability is largely prompt dependent.\n* The method is attack-agnostic and simple, with a clear step-by-step presentation. The analysis is insightful, using random multi-position insertion and attention heatmaps that convincingly support the positional hypothesis.\n* The results show significant empirical gains, with large performance improvements and meaningful reductions in the number of optimization steps needed for success.\n* The attack appears more robust than prior methods against several defenses.\n* The writing is clear, well-structured, and easy to follow."}, "weaknesses": {"value": "1. The paper lacks an analysis of transferability across models. It remains unclear whether positional vulnerabilities are model-specific or primarily prompt-dependent. Evaluating SlotGCG as a black-box attack would provide valuable insight into this question.\n2. The evaluation is limited to AdvBench, while several newer jailbreak or safety datasets now exist [1-3]. Including additional benchmarks would strengthen the empirical claims and demonstrate broader robustness.\n3. The method is only tested within the GCG family. Other optimization-based attacks exist, and it is unclear whether the proposed position-finding process generalizes to them. Since the abstract claims applicability to “any” attack, evidence from beyond GCG is needed.\n4. The defense selection is weak. The Erase-and-Check (suffix) version is expected to perform poorly when there is no suffix, as it effectively just deletes the response. Evaluating only the SmoothLLM swap defense is also insufficient; since the attack produces more uniform attention maps, token swapping may be less effective. Other SmoothLLM variants (insert, patch) and stronger recent defenses [4] should be tested for a fair assessment. \n5. The reported preprocessing cost of “+200 ms” is not empirically demonstrated or discussed in detail. The paper should clarify how this value was obtained.\n6. Although the calculation of the Vulnerable Slot Score (VSS) is novel, the general idea that attack performance depends on token position has been explored up to some level in prior work [5-7]. These earlier studies should be acknowledged and discussed.\n\n***Minor remarks:***\n1. Table 5 presents very strong and important results, so it would be better placed in the main text rather than the appendix to highlight its contribution more clearly.\n2. Line 472 refers to “Table 3” for the VSS distribution, but this table is no related. This reference should be corrected.\n\n[1] Zeng, Y., Shen, T., Ding, Y., Zheng, L., Sun, Y., & Chen, H. (2024). JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models\n\n[2] Mazeika, M., Wei, A., Casper, S., Rafailov, R., Dragan, A. D., Finn, C., & Hadfield-Menell, D. (2024). HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal\n\n[3] Xu, W., Wang, X., Zhang, Z., & Li, M. (2023). ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation\n\n[4] Yi, S., Liu, Y., Sun, Z., Cong, T., He, X., Song, J., Xu, K., & Li, Q. (2024). Jailbreak Attacks and Defenses Against Large Language Models: A Survey\n\n[5] Wang, J., Li, H., Peng, H., Zeng, Z., Wang, Z., Du, H., & Yu, Z. (2025). Activation-Guided Local Editing for Jailbreaking Attacks.\n\n[6] Mu, J., Ying, Z., Fan, Z., Jing, Z., Zhang, Y., Yu, Z., & Zhang, X. (2025). Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?\n\n[7] Rocamora, E., Dubey, A., Jauhri, A., Pandey, A., Letman, A., Mathur, A., & Vaughan, A. (2024). Revisiting Character-Level Adversarial Attacks for Large Language Models."}, "questions": {"value": "1. Are token budgets (total adversarial tokens) matched across baselines in Table 1?\n2. How sensitive are results to VSS temperature, and number of slots selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CnwvSwJFnv", "forum": "Fn2rSOnpNf", "replyto": "Fn2rSOnpNf", "signatures": ["ICLR.cc/2026/Conference/Submission10655/Reviewer_99aE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10655/Reviewer_99aE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864508949, "cdate": 1761864508949, "tmdate": 1762921908291, "mdate": 1762921908291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SlotGCG, a positional variant of GCG that exploits positional vulnerability in LLMs. Instead of appending adversarial tokens as a suffix, SlotGCG identifies vulnerable token slots within the prompt using a lightweight Vulnerable Slot Score (VSS) derived from attention patterns, then inserts and optimizes attack tokens at those positions. The method is attack-agnostic and can be used as a plug-in front end to multiple GCG-style optimizers with minimal extra overhead. Experiments on several open-source models report higher ASR, faster convergence, and improved robustness under certain defenses, with success judged via automatic and human checks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty\n\nReframes jailbreak optimization from “suffix-only” to positional attacks by identifying vulnerable token slots via a lightweight attention-derived score (VSS) and inserting/optimizing adversarial tokens at those positions.\n\n\n2. Method is general, plug-and-play, and more efficient\n\n\nAttack-agnostic front end that can be attached to multiple GCG-based optimizers with minimal overhead.\nResults show faster convergence/fewer steps and higher ASR than standard suffix-only pipelines under comparable budgets.\n\n3. Good experimental coverage\n\nEvaluates across several commonly used open-source instruction models (e.g., Llama, Mistral, Vicuna, Qwen).\nAdapts to multiple GCG-based attack variants and compares under several defenses, demonstrating consistent gains."}, "weaknesses": {"value": "1. Threat model and usability boundaries\n\nThe core VSS metric depends on attention weights (upper-half layers from the after-chat template to adversarial tokens), which are typically unavailable in black-box/closed models. The paper does not clarify applicability in strict black-box settings or provide surrogate attack choice.\n\n2. Transferability is underexplored\n  - Cross-model transfer: Do attack prompts found on one model transfer to other models without further optimization (zero-shot transfer)?\n  - Seed sensitivity: How does optimization vary with different random seeds (initial tokens, sampling orders)? \n  - Context/system-prompt robustness: For the same target model, does changing the system prompt or different context affect ASR?\n\n3. Recency of attack targets\n\nExperiments focus on open-source instruction models (the newest being Qwen-2.5). There is no demonstration on newer/stronger/closed-source LLMs, limiting external validity.\n\n4. Hyperparameter choices lack justification\n\nThe effects of temperature in VSS, the precise definition of “upper-half layers,” and the impact of different after-chat template tokens are not detailed and analyzed. It remains unclear how sensitive VSS and final ASR are to these design choices.\n\n5. Confusion in section THE ROBUSTNESS OF SLOTGCG UNDER DEFENSE METHODS\n\nPerplexity Filter yields 0 ASR for all attack variants, yet the paper claims “Erase-and-Check yields the largest reduction in ASR.” This seems to appear inconsistent.\n\nThe paper attributes some failures to GPT-4 misclassification due to biases in the GPT-based filtering mechanism, but overall ASR is still measured by the same GPT-based judge. This creates a tension: if the judge is unreliable for filtering, why is it reliable for final success labels?\n\n6. Motivation and definition of VSS are hard to follow\n\nFigure 4 is used to motivate “developing a metric,” but VSS has not yet been defined at that point, making the figure difficult to interpret on first read."}, "questions": {"value": "1. White-box assumptions and transferability\n\n- Is SlotGCG a pure white-box attack (requiring attention weights) during both scoring and optimization?\n\n- If so, can the resulting adversarial prompts transfer to other models without further optimization (zero-shot cross-model transfer)? \n\n2. Effectiveness against deployed guardrails\n\nCan you please evaluate SlotGCG against current guardrails (e.g., Llama Guard or similar safety classifiers/filters)?\n\n3. Which VSS is shown in Figures 4 and 8?\n\nIn Figures 4 and 8, $\\text{VSS}^{\\text{final}}$ represents the VSS of which slot?\n\n4. Random Multi-Position Insertion in Figure 5\n\nWhat is the exact algorithm for **Random Multi-Position Insertion**? Why does random slot insertion without token optimization achieve faster convergence than GCG?\n\n5. Ablation on only insertion and token allocation via VSS\n\nCan you provide results for **VSS-based slot insertion only** (no token optimization), and compare them with **GCG-only token optimization** (no VSS-based slotting)? An ablation contrasting these two against the full SlotGCG would clarify each component’s contribution.\n\n6. Effect of the **token budget (m)**\n\nHow does the **token budget \\(m\\)** affect SlotGCG’s **ASR**, convergence speed, and stability? Please include curves or tables showing performance as \\(m\\) varies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0oomGIWqeP", "forum": "Fn2rSOnpNf", "replyto": "Fn2rSOnpNf", "signatures": ["ICLR.cc/2026/Conference/Submission10655/Reviewer_rRJL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10655/Reviewer_rRJL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905970529, "cdate": 1761905970529, "tmdate": 1762921907876, "mdate": 1762921907876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SlotGCG, a novel extension of gradient-based jailbreak optimization that explicitly models positional vulnerability in prompts. The key idea is to identify slots—token-level positions that are more susceptible to adversarial perturbation—using an attention-derived Vulnerable Slot Score (VSS). The method first probes each slot’s sensitivity, then assigns probabilistic insertion weights and integrates them into the GCG optimization loop. Experiments across multiple open-weight LLMs and defenses demonstrate that SlotGCG improves attack success rate, convergence efficiency, and robustness against defense mechanisms, while remaining lightweight and compatible with existing frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper connects positional token vulnerability with optimization-based jailbreaks, introducing VSS as a quantifiable measure of slot sensitivity. The slot-probing stage is lightweight and can be easily integrated into other attack pipelines, enhancing general applicability."}, "weaknesses": {"value": "1. Tokenizer dependence – As slots are token-based, specify the tokenizer used and discuss whether different tokenizers could affect slot boundaries or results.\n\n2. Optimality of Step 3 formula – It is unclear whether the slot-selection formula is optimal. Would selecting top-k slots and renormalizing yield different outcomes? Clarify if this is a tunable hyperparameter and analyze its effect on ASR and prompt coherence.\n\n3. Defense and baselines – The defense side lacks diversity and novelty. The chosen baselines and target models are relatively standard and dated. Including stronger or more recent defense baselines (e.g., [1] [2]) would strengthen the experimental credibility.\n\n4. Limited contribution – The method builds on optimization-based jailbreak attacks (e.g., GCG), yet its improvements appear easily neutralized by simple defense strategies. This raises the question of why such an optimization-based formulation is chosen in the first place. If the approach can be trivially mitigated, the paper should clarify what fundamental insight or practical benefit this “slot vulnerability” perspective contributes beyond existing optimization-based jailbreak methods.\n\n5. Target model and PPL results – Sec. 5.3 does not specify the target model, and the statement that “PPL mitigation is moderate” seems inconsistent with near-zero results. Please clarify both.\n\n6. Unclear notation – In Step 3 of Sec. 4, the variables fsi and S* are undefined. Add explicit notation or a brief symbol explanation for clarity.\n\n7. Minor textual error – Line 213 should mention three prompts instead of four. Please verify and correct.\n\n8. Slot normalization – In Sec. 3.1, slot indices are normalized by the longest prompt in the batch, which likely prevents values near 1.0. The motivation and comparison with per-prompt normalization should be clarified.\n\n[1] Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks\n\n[2] SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ge5elwQBYJ", "forum": "Fn2rSOnpNf", "replyto": "Fn2rSOnpNf", "signatures": ["ICLR.cc/2026/Conference/Submission10655/Reviewer_ELoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10655/Reviewer_ELoX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186802968, "cdate": 1762186802968, "tmdate": 1762921907464, "mdate": 1762921907464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}