{"id": "eWJbrssI9M", "number": 16840, "cdate": 1758269268228, "mdate": 1759897216470, "content": {"title": "FindMeIfYouCan: Bringing Open Set Metrics to $\\textit{near}$, $\\textit{far}$ and $\\textit{farther}$ Out-of-Distribution Object Detection", "abstract": "Recently, out-of-distribution (OOD) detection has gained traction as a key research area in object detection (OD), aiming to identify incorrect predictions often linked to unknown objects. In this paper, we reveal critical flaws in the current OOD-OD evaluation protocol: it fails to account for scenarios where unknown objects are ignored since the current metrics (AUROC and FPR) do not evaluate the ability to find unknown objects. Moreover, the current benchmark violates the assumption of non-overlapping objects with respect to in-distribution (ID) classes. These problems question the validity and relevance of previous evaluations. To address these shortcomings, first, we manually curate and enhance the existing benchmark with new evaluation splits---semantically $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ relative to ID classes. Then, we integrate established metrics from the open-set object detection (OSOD) community, which, for the first time, offer deeper insights into how well OOD-OD methods detect unknown objects, when they overlook them, and when they misclassify OOD objects as ID---key situations for reliable real-world deployment of object detectors. Our comprehensive evaluation across several OD architectures and OOD-OD methods show that the current metrics do not necessarily reflect the actual localization of unknown objects, for which OSOD metrics are necessary. Furthermore, we observe that semantically and visually similar OOD objects are easier to localize but more likely to be confused with ID objects, whereas $\\textit{far}$ and $\\textit{farther}$ objects are harder to localize but less prone to misclassification.", "tldr": "A rigorous benchmark for OOD object detection, introducing open-set metrics and semantically stratified splits, to precisely evaluate unknown object identification. Our work unmasks safety-critical insights that current metrics fail to reveal.", "keywords": ["Out-Of-Distribution Detection", "Open Set Detection", "Object Detection", "Benchmark", "Computer vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd0c159f8eb8d5d3c505dcb3cda188ef78e70106.pdf", "supplementary_material": "/attachment/a66a50eb0851b54b7883c746d7617fa55649efd9.zip"}, "replies": [{"content": {"summary": {"value": "This paper reveals flaws in the current OOD-OD evaluation protocol, noting that standard metrics fail to evaluate the ability to find unknown objects, at the same time the existing benchmark violates the assumption of non-overlapping in-distribution classes. To address these issues, the authors manually curate and enhance a benchmark with new semantic splits and integrate established open-set object detection metrics that offer deeper insights into how well methods detect unknown objects, when they overlook them, and when they misclassify them. The evaluation shows current metrics do not reflect actual unknown object localization, while observing that semantically similar OOD objects are easier to localize but more likely to be confused with ID classes, whereas far and farther objects show the opposite characteristics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The authors' removal of labeled instances with overlapping categories in the benchmark contributes to enhancing the validation accuracy of OOD-OD methods.\n\n2.The authors categorized the OOD datasets into new subsets based on semantic and visual similarity, which contributes to a fine-grained evaluation of OOD-OD method performance."}, "weaknesses": {"value": "1.The authors state that they \"properly evaluate the actual identification of unknown objects by integrating complementary metrics from the OSOD community,\" and that these OSOD metrics can \"measure when unknown objects are ignored, when they are detected, and when they are confounded with ID objects.\" However, since OOD-OD is fundamentally different from OSOD in its objectives, merely aiming to \"identify predictions that do not belong to the ID categories\". Why do the authors introduce metrics designed for a different task to \"capture the complete picture of model performance when encountering unknown objects\"? It is suggested for the authors to provide an explanation for this.\n\n2.The authors point out that \"a misconception that may be conveyed by these metrics is that a higher AUROC or lower FPR95 means better localization of OOD objects.\" However, neither of these metrics relates to localization capability. The AUROC serves to evaluate the overall performance of a classifier, while the FPR95 measures the false positive rate for out-of-distribution samples when the true positive rate for in-distribution (ID) samples is 95%. Why do the authors believe these metrics would lead readers to misinterpret them as indicators of \"better localization of OOD objects\"? It is suggested for the authors to provide an explanation for this.\n\n3.In Table 1, the authors present the percentage of \"Ignored objects\" in existing OOD-OD benchmarks, but do not provide the corresponding proportion for the proposed FMIYC benchmark. To demonstrate that the new benchmark effectively alleviates the \"Ignored objects\" issue, it is recommended that the authors supplement this with a comparative analysis.\n\n4.In the EXPERIMENTS AND RESULTS section, the purpose of the experiment on OBJECT DETECTION ARCHITECTURES remains unclear. Furthermore, the reason for incorporating post-hoc methods in the OUT-OF-DISTRIBUTION OBJECT DETECTION METHODS experiment is not sufficiently justified. It is recommended that the authors to explicitly elaborate on the objectives of these two experiments and explain how they collectively demonstrate the effectiveness of the proposed metric/benchmark.\n\n5.In the experimental results presented in Tables 4 and 5, several higher-is-better metrics (such as AUROC, R_U, and P_U) generally demonstrate a decreasing trend under the near-to-farther setup. However, nOSE is a lower-is-better metric. Why does this metric also exhibit a decreasing trend in the near-to-farther configuration? It is suggested for the authors to analyze and explain this observation.\n\n6.In addition to adopting OSOD metrics, the authors have introduced a new evaluation metric. However, neither the formulas for the established OSOD metrics nor the proposed nOSE are provided. It is recommended that the authors include the formal definitions of these metrics to help readers for understanding."}, "questions": {"value": "1.Why do the authors introduce OSOD metrics that differ from the objectives of the OOD-OD task to \"capture the complete picture of model performance when encountering unknown objects\"?\n\n2.Why do the authors believe that the AUROC and FPR95 metrics would lead readers to misinterpret them as indicating \"better localization of OOD objects\"?\n\n3.Why do the authors conduct experiments on \"mAP across architectures\" and post-hoc methods? How do these experiments contribute to validating the proposed metrics and benchmark?\n\n4.Why does the nOSE metric also show a decreasing trend in the near-to-farther experimental setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lS7GCcadQ1", "forum": "eWJbrssI9M", "replyto": "eWJbrssI9M", "signatures": ["ICLR.cc/2026/Conference/Submission16840/Reviewer_CM6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16840/Reviewer_CM6C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843235616, "cdate": 1761843235616, "tmdate": 1762926861696, "mdate": 1762926861696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discuss shortcomings of existing evaluation metrics for Out-Of-Distribution Object Detection (OOD-OD) benchmarks, and propose alternatives that they land from the close field of Open-Set Object Detection (OSOD).\nAdditionally, they create a new evaluation benchmark that separated unknown classes into near, far and farther.\nAlso, they combine OOD methods with object detectors and show that they have no clear advantage over one another.\nFinally, they show that conclusions drawn by existing OOD-OD metrics are invalid."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors indicate that (class-wise) evaluation of object detection methods, such as evaluated via mAP, is inappropriate.\n\n* The authors introduce post-hoc OOD methods into object detectors to perform open-set object detection.\n\n* The authors provide new evaluation benchmarks for OOD-OD that distinguish between semantically close, medium and far unknown classes."}, "weaknesses": {"value": "1. There are some unintuitive claims made by the authors:\n\n   a) In line 47, the authors claim that \"OSOD actively attempts to detect unknown objects\". This is completely wrong. For example, the evaluation metric in (Dhamija et al. 2020), which the authors cite for this claim, does not make used of $TP_o$.\n\n   b) In line 131, the authors state that a single confidence threshold $t^*$ is used to perform classification. This is incorrect since mAP actually indicates different thresholds for the different classes -- since they are evaluated individually.\n\n   c) In line 137, the authors discuss two cases: an unknown object is detected as one of the known classes (which corresponds to a false positive $FP_o$) or they are correctly ignored (corresponding to a true negative $TN_o$), which is ignored in all valid OSOR evaluation metrics. Hence, these two cases are two aspects of the same evaluation, and it is not clear why they indicate two different problems OOD-OD and OSOD.\n\n   d) In line 177, the authors state that people might believe that \"lower FPR95 means better localization of unknown objects\". The reviewer disagrees with this statement since detection/localization of unknown objects is in no way part of this evaluation. The bigger issue here is that we cannot compute a false positive **rate** since the number of detected (unknown or background) objects might differ between detectors.\n\n\n2. The proposed evaluation metrics are wrong, cumbersome, and do not follow their intuition:\n\n   a) The proposed Unknown Recall and Unknown Precision metrics (line 270) have the same issue as FPR95 and AUROC and are, therefore, meaningless. Researchers in the related field of open-set face recognition have solved this issue by defining false positives per image as absolute numbers [Kasim2024], similarly to the AOSE metric.\n\n   b) While the proposed normalized Open Set Error (nOSE) would be valid as it divides by a constant, its advantage over existing metrics is not obvious. On the other hand, it has the disadvantage that unknown objects need to be labeled -- which might be cumbersome since it is not clear what constitutes an unknown object and what not. For example, there is a street sign in the background of figure 2 -- is this an unknown object or not?\n\n   c) The purpose of the nOSE metric is also not fulfilled. While the authors claim that \"nOSE assesses the proportion of unknown objects detected as one of the ID classes\", this is not true. If background is detected and classified as ID class (such as in figure 2), this also counts into nOSE, so that the nOSE value can be larger than 1 and, therefore, this is not a proportion.\n\n\n\n\n3. The definition of the new benchmark is unintuitive:\n\n   a) It is not entirely clear why we need two separate sets of images, one containing known (plus some background and unknown) and one containing only unknown objects. The evaluation should concentrate on Ground Truth bounding boxes, not on an image level. Instead of \"removing overlaps\" (line 223), the authors should have labeled bounding boxes of known objects as such.\n\n   b) It is unclear why and how benchmarks are enriched by adding images from COCO and OpenImages (line 241) -- this process needs to be motivated and explained in more detail.\n\n   c) The definition of near, far and farther is misleading. Since ID datasets differ between far and farther (indicated in line 257), results cannot be compared between these evaluation protocols. However, in the conclusion they do exactly this: they compare far and farther.\n\n\n4. The evaluation could be improved:\n\n   a) While the authors correctly reject AUROC and FPR95 metrics for OOD-OD, they provide results for those in figure 4. This figure, if at all required, should be moved to the supplemental material.\n\n   b) Similarly, in line 431, the authors define VOS as the best method based on the invalid AUROC metric, which makes no sense.\n\n   c) In the figures, the authors use OpIm as abbreviation, which is introduced nowhere, and is easily confused with Oplm.\n\n   d) The authors used a single threshold (based on 95% TPR) to evaluate their system. While this is valid, it limits the evaluation to a specific operating point. Drawing curves for different operating points would provide more insights into the behavior of the models under different requirements.\n\n\n\n5. The Discussion section is merely useless since it just repeats what was already stated in previous sections.\n\n6. The citation style should be adapter throughout the paper, making proper use of \\cite, \\citep and \\citet.\n\n[Kasim2024] Kasim and others: \"Watchlist Challenge: 3rd Open-set Face Detection and Identification\", IJCB 2024."}, "questions": {"value": "A) The authors discuss shortcomings of the evaluation performed in one related paper (Du et al, 2022b). The reviewer agrees with these shortcomings. However, instead of rejecting the entire research direction of OD-OOD, the authors try to bring evaluation metrics from OSOD into OOD-OD. What is the importance of detecting unknown objects if we cannot classify them? Why do we need OD-OOD, in which scenarios would this be preferred over OSOD, which simply ignores unknown objects?\n\nB) While the authors claim (line 209) that \"Accurate localization of ground truth unknown objects is a critical aspect\", they do not provide any example where such localization would be required. The authors should discuss such scenarios clearly.\n\nC) Also, the proposed metrics do not fulfill their goals. It would be better to rely on existing metrics such as AOSE, or by dividing AOSE by the number of images in the dataset, to make it comparable across benchmarks. The authors need to defend their choices, or move to appropriate evaluation metrics.\n\nIf the authors could clarify the above-mentioned points, and change their paper accordingly, the reviewer would consider increasing their rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2NWTKO1LVG", "forum": "eWJbrssI9M", "replyto": "eWJbrssI9M", "signatures": ["ICLR.cc/2026/Conference/Submission16840/Reviewer_tFn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16840/Reviewer_tFn4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923605656, "cdate": 1761923605656, "tmdate": 1762926861388, "mdate": 1762926861388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a critical examination of current out-of-distribution object detection (OOD-OD) benchmarks. It argues that existing evaluation protocols are flawed because they allow models to ignore unknown objects without penalty. Metrics such as AUROC and FPR do not assess a model’s capability to detect or localize unknown objects. This work introduce FindMeIfYouCan (FMIYC), a  benchmark that eliminates dataset overlaps, categorizes out-of-distribution (OOD) samples into near, far, and farther tiers, and extends OSOD evaluation with PU, RU, APU, and a newly proposed nOSE metric. Experiments cover a diverse set of detectors including Faster R-CNN, YOLOv8, RT-DETR, and OWLv2 as well as standard OOD-OD baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents extensive experiments covering a range of modern object-detection architectures and strong OOD baselines, which strengthens the empirical evaluation. \n\nThe proposed stratification of semantic similarity (near vs. far) introduces valuable nuance into OOD assessment and allows for more fine-grained analysis than prior work. \n\nThe integration of OSOD metrics helps bridge the gap between OOD detection and open-set object detection frameworks, offering a more unified evaluation protocol."}, "weaknesses": {"value": "The discussion on why AUROC and FPR95 are insufficient for OOD object detection is unclear. It would be beneficial to include a concrete example illustrating how these metrics are computed in the OOD-OD setting, and then explicitly demonstrate why they fail to capture the nuances of this task.\n\nThe benchmark is constructed primarily from existing datasets, which raises concerns about originality. The work should better justify why repurposing existing data is sufficient and highlight what novel contributions arise beyond dataset aggregation.\n\nThe “near/far/farther” categorization is based on class-name heuristics, which may not reliably reflect semantic similarity. \n\nThe manuscript needs more discussion on why OSOD metrics are appropriate for OOD object detection. The conceptual relationship between OSOD and OOD-OD should be explained more thoroughly, and the advantages or limitations of adopting these metrics should be clarified."}, "questions": {"value": "Can FMIYC extend beyond VOC/COCO environments to real-world long-tail or continual-learning settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qavyYjxu00", "forum": "eWJbrssI9M", "replyto": "eWJbrssI9M", "signatures": ["ICLR.cc/2026/Conference/Submission16840/Reviewer_rPk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16840/Reviewer_rPk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991439695, "cdate": 1761991439695, "tmdate": 1762926860760, "mdate": 1762926860760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Here is a full review based on the revised submission.SummaryThis paper introduces FindMeIfYouCan (FMIYC), a new benchmark for Out-of-Distribution Object Detection (OOD-OD). The work identifies and corrects critical flaws in the existing evaluation protocol, namely the failure to penalize ignored unknown objects and the violation of the non-overlapping-class assumption. The FMIYC benchmark is built by meticulously curating new evaluation splits (\"near,\" \"far,\" and \"farther\") from established datasets (Pascal-VOC, BDD100k, COCO, OpenImages). It integrates metrics from the Open-Set Object Detection (OSOD) community ($AP_U$, $P_U$, $R_U$) and proposes a new metric, normalized Open Set Error (nOSE), to provide a more holistic evaluation. This revised version significantly expands the analysis to include modern architectures like YOLOv8, RT-DETR, and the Vision-Language Model (VLM) OWLv2, providing novel insights into their OOD performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses Fundamental Flaws: The paper clearly identifies and systematically corrects fundamental, long-standing issues in the standard OOD-OD evaluation. By removing semantic overlaps and using metrics that account for ignored objects, it provides a much more reliable and trustworthy evaluation framework.\n\n2. Nuanced \"Near/Far\" Splits: The stratification of OOD data into \"near,\" \"far,\" and \"farther\" splits is a key contribution. It enables a more fine-grained analysis, leading to important insights (e.g., semantically near objects are easier to localize but more often confused with ID classes).\n\n3. Adoption of OSOD Metrics: Integrating OSOD metrics ($AP_U$, $P_U$, $R_U$) and proposing nOSE is a major step forward. These metrics provide a much deeper understanding of model behavior (localization, misclassification, and omission) than traditional AUROC/FPR95, which this paper shows can be misleading."}, "weaknesses": {"value": "1. Dependency on Ground Truth Labels: The new metrics, while superior, introduce a significant practical dependency on exhaustive and correct ground truth (GT) labels for all unknown objects. This trade-off is a key limitation, as acquiring such annotations is a major bottleneck and contrary to the spirit of truly open-set evaluation, where unknowns are, by definition, unlabeled.\n\n\n2. Incremental Contribution: While the VLM analysis adds substantial value, the benchmark's core contribution is a refinement of existing work. It involves curating subsets from existing datasets and adopting metrics from the related OSOD field, rather than generating entirely new data or a fundamentally new evaluation paradigm."}, "questions": {"value": "Please refer to the above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vaJu2m0Tw0", "forum": "eWJbrssI9M", "replyto": "eWJbrssI9M", "signatures": ["ICLR.cc/2026/Conference/Submission16840/Reviewer_XELp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16840/Reviewer_XELp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073451257, "cdate": 1762073451257, "tmdate": 1762926860371, "mdate": 1762926860371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}