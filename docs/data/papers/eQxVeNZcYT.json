{"id": "eQxVeNZcYT", "number": 19956, "cdate": 1758300961489, "mdate": 1759897010743, "content": {"title": "Quantifying biases in LLM-as-Judge evals", "abstract": "The evaluation of large language models (LLMs) is increasingly performed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or autograders. While autograders offer a scalable alternative to human evaluation, they are not free from biases (e.g., favouring longer outputs or generations from their own model family). Here we propose a statistical framework based on Bayesian generalised linear models (GLMs) that enables researchers to address their primary research questions (e.g., LLM capability or risk assessment), while simultaneously identifying, quantifying and mitigating various biases in their autograders. Our approach can be applied to various evaluation formats (e.g., absolute scores or pairwise preferences) and augments traditional metrics (e.g., inter-rater agreement) by providing precise uncertainty estimates and clarifying sources of disagreement between graders. This framework also enables efficient counterfactual simulations without costly re-evaluation (e.g., assessing agreement after removing systematic biases). We demonstrate these capabilities through simulated examples, with all methods available in an open-source software package. Overall, we introduce a novel framework for autograder evaluation which allows researchers to detect, quantify and correct for various biases in a systematic way.", "tldr": "A novel framework for the evaluation of LLMs-as-a-judge which allows researchers to detect, quantify and correct for various biases in a systematic way.", "keywords": ["LLM evaluation", "LLM-as-a-Judge", "bias detection", "bayesian modeling"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75304cf71f6b52f89aa73dddb68b310ad473d648.pdf", "supplementary_material": "/attachment/022834fc64fc9aef8cba3333d386cc58312ee839.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces a Bayesian Generalized Linear Model (GLM) framework to identify and quantify biases in “LLM-as-a-judge” evaluation settings. It models grader, item, and interaction effects (e.g., self-bias, length bias, grader severity) within a unified probabilistic structure, offering posterior uncertainty estimates and counterfactual simulations to measure agreement after bias removal. All methods are implemented in the open-source HiBayes package. The approach is clearly explained through simulated examples and provides a statistically principled way to improve reliability in LLM evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and methodologically rigorous.\nIt proposes a novel, interpretable, and generalizable statistical framework that unifies multiple sources of evaluation bias under a single Bayesian model.\nThe inclusion of counterfactual simulations is particularly insightful, and the open-source implementation enhances reproducibility and practical relevance."}, "weaknesses": {"value": "While conceptually strong, the paper’s validation is limited to simulated data, which constrains its empirical credibility. The proposed framework should be tested on real-world evaluation datasets such as MT-Bench or Chatbot Arena to demonstrate robustness in noisy, large-scale conditions. \n\n\n**Significant issue:** The paper omits some crucial references [1]. They also identify the bias of LLM judges and propose measurement methods. A detailed comparison is necessary.\n\nFurthermore, the Bayesian hierarchical models used here may be computationally expensive for practical deployment. The authors should discuss convergence diagnostics, scalability, and possible approximations such as variational inference. Lastly, while the paper is generally clear, the statistical formalism may be difficult for non-expert readers; including a simplified diagram or intuitive summary of the model hierarchy could improve accessibility.\n\n[1] Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge” (ICLR 2025)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "291EOIMOd8", "forum": "eQxVeNZcYT", "replyto": "eQxVeNZcYT", "signatures": ["ICLR.cc/2026/Conference/Submission19956/Reviewer_QeBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19956/Reviewer_QeBC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761353041337, "cdate": 1761353041337, "tmdate": 1762932136536, "mdate": 1762932136536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a unified generalized linear modeling framework to decompose and quantify biases in LLM-as-judge evaluations. It models several sources of systematic deviation in grading. These include\n- a systematic shift in score ranges across graders, \n- self-bias where an LLM evaluator favors outputs from its own model family,\n- differences between human and LLM evaluators in average scoring levels, \n- item-level biases where certain prompts attract consistently higher or lower scores, and \n- length bias that favors longer responses.\n\nEach bias enters the linear model so the final assigned score can be explained as the effect of grader identity and its bias. The authors’ goal is to control these bias variables within the GLM, then assess agreement between human and LLM evaluators, seeking higher Krippendorff’s alpha once systematic grader shifts are removed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Mathematical framing of systematic bias. The paper formalizes and measures systematic biases of LLM evaluators and presents a way to control them within a unified GLM framework.\n\n2. Narrative presentation aids understanding. The storytelling style with concrete, step-by-step examples is unusual but effective; it made the methodology easier to follow."}, "weaknesses": {"value": "1. Lack of real-world validation. While the paper offers a solid statistical approach to LLM-as-Judge biases, it does not demonstrate how the framework works in practice or whether the theoretical claims reproduce on real evaluation tasks. As it stands, the contribution is largely a framework for shifting distributions via linear modeling under specific bias assumptions. It would be stronger with experiments showing, on concrete benchmarks, how much the method improves agreement with human judges (not just correlation) and under what conditions.\n\n\n2. Scope of applicability. Is the framework mainly effective when the LLM-as-Judge exhibits consistent, systematic biases (e.g., uniformly harsher scoring, or consistently down-scoring a particular item)? How robust is it when biases are item- or context-dependent in more irregular ways?\n\n\n3. Agreement vs. correlation for graded responses. For categorical labeling tasks, I agree that human–autograder agreement (e.g., Krippendorff’s alpha) is essential. But for graded/preference scoring, if the rank order per item and the overall trend/correlation (Spearman/Kendall) between human and LLM evaluators are high, isn’t that sufficient? Do we really need to chase high Krippendorff’s alpha in that setting, given natural rater-to-rater differences in scale use? This framework appears to require substantially more effort—why is it necessary there?\n\n\n4. Concrete use cases requiring this framework. In what evaluation scenarios is the proposed GLM-based correction strictly necessary compared to simpler pipelines (e.g., rater standardization + correlation/consistency analyses)? Please provide examples where improving alpha (agreement) beyond correlation materially changes conclusions about model quality or ranking."}, "questions": {"value": "Question 1. The results or example results (the figures in the main body) look extremely theoretical and idealistic. Are they actual results or hypothetical data? If they are from a real experiment, which LLM was used to generate them? Furthermore, it offers a solid statistical approach for addressing potential biases in LLM-as-Judge, but it is unfortunate that we cannot verify how this is implemented in practice or whether the theoretical hypotheses are truly confirmed.\n\nSmall suggestion 1. Use Evaluations in your title, rather than its abbreviation Evals.\n\nSuggestion 2. The effect size graphs (right panel) presented in Figures 1, 2, 3, 4, and 6 do not indicate how significant each confidence interval is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FQLkutCSfA", "forum": "eQxVeNZcYT", "replyto": "eQxVeNZcYT", "signatures": ["ICLR.cc/2026/Conference/Submission19956/Reviewer_7g6Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19956/Reviewer_7g6Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537260337, "cdate": 1761537260337, "tmdate": 1762932133830, "mdate": 1762932133830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a tutorial on applying generalized linear models (GLMs) for model LLM evaluations that use model or human judges. The paper demonstrates that GLMs can be used to model various evaluation setups (e.g. absolute scores vs pairwise preferences) and that the parameterization can be chosen to quantify biases including self-preference and length preference."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Overall this is a well-written tutorial, and the concepts are well-explained.\n- The examples cover quantification of common forms of bias (e.g. measuring judge leniency, self-preference, length preference).\n- The examples cover common LLM evaluation setups (e.g. absolute scores vs pairwise preferences)."}, "weaknesses": {"value": "- This paper is essentially a GLM tutorial paper (or recommendation paper) and does not present new methods. While this tutorial might be generally useful for LLM research practitioners, it may not be appropriate for this venue due to its lack of novelty.\n- The use of synthetic evaluation results in the examples makes the examples feel artificial. The examples would be more convincing if it used real evaluation results from an existing benchmark.\n- Nit: “graderLLM interaction” should be “grader-LLM interaction”, and “graderitem interaction” should be “grader-item interaction”"}, "questions": {"value": "- In examples 3 and 5, why do you choose a hierarchical GLM over a simpler GLM model? Do you have a general recommendation for researchers as to when a hierarchical GLM is appropriate?\n- Your examples show testing for only one or two kinds of bias at a time in these models. The more types of biases are tested at once, the more complex the model. How do you suggest that researchers approach this tradeoff?\n- The examples demonstrate that the researcher needs to have a good intuition for what kinds of bias to model for. There are other types of bias that were not included in the paper, such as position bias; there are possibly other sources of bias that the community is not yet aware of. How do you suggest that researchers approach these unknown unknowns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bGczn66WL4", "forum": "eQxVeNZcYT", "replyto": "eQxVeNZcYT", "signatures": ["ICLR.cc/2026/Conference/Submission19956/Reviewer_DZBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19956/Reviewer_DZBt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785864518, "cdate": 1761785864518, "tmdate": 1762932132653, "mdate": 1762932132653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Bayesian generalized linear model (GLM) framework for evaluating LLM-based autograders. The authors demonstrate how this statistical approach can simultaneously assess LLM capabilities while identifying and quantifying various biases in autograders, including self-bias, length bias, and grader severity differences. Through a series of simulated examples following a fictional researcher \"Florence,\" the paper illustrates how GLMs can be applied to both absolute scoring and pairwise preference evaluation formats.  I like the writing style."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does an excellent job of making Bayesian GLMs accessible to ML researchers who may not have strong statistical backgrounds. The progression from simple models to more complex hierarchical structures is pedagogically sound.\n\n2. The framework demonstrates flexibility in capturing various biases (self-bias, length bias, grader severity, item effects) within a unified statistical framework.\n\n3. I like the writing style, from the perspective of \"Florence\"."}, "weaknesses": {"value": "1. **Entirely based on simulated data**: This is the most critical weakness. Without validation on real LLM evaluation scenarios, we cannot assess whether the approach handles the complexities of actual evaluation data, such as:\n   - Non-linear relationships between features and outcomes\n   - Complex interaction effects beyond those modeled\n   - Robustness to model misspecification\n   - Computational scalability with large evaluation datasets\n\n2. **Missing critical technical details**:\n   - How were autograders prompted? What temperature/sampling settings?\n   - How was human evaluation conducted? Training protocols? Quality control?\n   - What datasets were these simulations based on, if any?\n   - What prior distributions were used and how sensitive are results to these choices?\n   - How long does model fitting take? What are computational requirements?\n\n3. **Inadequate related work coverage**: The paper misses significant recent work on LLM-as-judge biases and evaluation frameworks. Notably absent:\n   - \"Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge\" \n   - \"Humans or LLMs as the Judge? A Study on Judgement Bias\" (arXiv:2410.12784)\n   - JudgeBench, RewardBench and other specific LLM-as-a-Judge benchmarks\n\n4. **Unclear methodological contribution**: The paper doesn't clearly articulate what advantage GLMs provide over:\n   - Simpler statistical approaches (mixed-effects models, ANOVA)\n   - Existing bias detection methods\n   - Direct bias correction techniques\n   The focus on demonstrating that biases exist (which is known) rather than on the unique advantages of the GLM approach undermines the contribution.\n\n5. **No practical guidance or applications**:\n   - How should practitioners use this framework in real evaluation pipelines?\n   - Can the quantified biases be used to correct evaluation outcomes?\n   - What are the failure modes and limitations?\n\n6. **Organizational and structural issues**:\n   - The progression through Questions 1-5 feels arbitrary rather than principled\n   - Sections 2.1 and 2.3 both address human-autograder differences, separated by the self-bias discussion in 2.2\n   - The narrative device of \"Florence\" makes the paper read like a tutorial rather than a research contribution"}, "questions": {"value": "Where is Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2aufanFmT0", "forum": "eQxVeNZcYT", "replyto": "eQxVeNZcYT", "signatures": ["ICLR.cc/2026/Conference/Submission19956/Reviewer_hki8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19956/Reviewer_hki8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963815195, "cdate": 1761963815195, "tmdate": 1762932131533, "mdate": 1762932131533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}