{"id": "lle0aGQyQb", "number": 8572, "cdate": 1758091282272, "mdate": 1759897775834, "content": {"title": "Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation", "abstract": "Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches—such as prompt customization or fine-tuning—struggle to reason over implicit preferences, limiting real-world effectiveness. Recent “think-then-generate” methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose **FlyThinker**, an efficient “think-while-generating” framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions—allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.", "tldr": "", "keywords": ["LLM", "Personalization", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97e991c3f6e8ab0d92064425e25ac9d78347edcf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FlyThinker, an efficient “think-while-generating” framework. Unlike prior “reason-then-generate” approaches, FlyThinker employs a separate reasoning model that runs in parallel to dynamically guide the generation model. The framework achieves efficiency in both training and inference, and extensive experiments demonstrate its strong effectiveness and superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The concept of generating reasoning tokens and response tokens simultaneously is novel and intriguing to me.\n\n2. The idea of decoupling reasoning tokens from previously generated reasoning outputs is particularly interesting, as it enables a one-pass training process and significantly improves overall efficiency."}, "weaknesses": {"value": "1. The title of Figure 3 is somewhat misleading and should be revised to “Training Efficiency / Inference Efficiency.” Although the proposed method demonstrates shorter runtime, it relies on two separate models—the reasoning model and the generation model—which substantially increases memory consumption. The authors should therefore provide a comparison of the actual computational cost against other baselines to present a fair assessment.\n2. Reasoning models typically show the greatest advantages on more challenging tasks, such as mathematical or scientific reasoning (e.g., AIME24, GPQA) and coding tasks. It would be more insightful if the authors evaluated the proposed method on such demanding benchmarks, as this would more convincingly highlight the true effectiveness of the reasoning model.\n\nTypo: In line 94, the author redundantly includes an extra “First” after “Firstly.”"}, "questions": {"value": "As noted in the weaknesses above, please provide more details on the actual overall computational cost and evaluate the proposed method on a broader range of challenging tasks, such as mathematical or scientific reasoning and coding benchmarks, to better demonstrate its effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BvKM55Ya1l", "forum": "lle0aGQyQb", "replyto": "lle0aGQyQb", "signatures": ["ICLR.cc/2026/Conference/Submission8572/Reviewer_tgii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8572/Reviewer_tgii"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617395994, "cdate": 1761617395994, "tmdate": 1762920424162, "mdate": 1762920424162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose FlyThinker, a method for combining LLM reasoning and generation in a \"think-while-generating\" paradigm. In their approach, there are reasoner and generator models, where the reasoner is given the input and an in-progress generation, then produces a latent reasoning token that is fed to the generator to guide its response. They argue that this approach allows for efficient LLM personalization with latent reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed approach seems useful for efficiently combining reasoning and generation. It provides a simple way to align to user preferences.\n- The method is flexible and can be used with models of different sizes. It is particularly convenient that a small reasoning model can be combined with a larger generation model for greater efficiency.\n- The writing and presentation of the paper are clear. The method section in particular is very easy to read and clearly lays out the method."}, "weaknesses": {"value": "While the proposed approach is interesting, the evaluation experiments in their current form are not sufficiently comprehensive. \n1.  The evaluation is based only on simple automated metrics (ROUGE, BLEU, METEOR, BERT-Score). To get a full understanding of how much FlyThinker improves personalization, it would be useful to have a user study or an automatic LLM evaluation of preferred personalized outputs.\n2. The experiments are mostly limited to Qwen2.5-3B-Instruct. The small set of Qwen2.5-7B-Instruct experiments do not have any numbers on the axes, so it is difficult to tell how much FlyThinker improves performance over SFT. The experimental results would be strengthened by additional experiments with other models and filling out the Qwen2.5-7B-Instruct scores. \n3. From my understanding, compared to SFT, with this approach it is necessary to keep up to 2x the number of parameters in memory. The authors state that their method is efficient because both the reasoner and generator can perform inference simultaneously, but do not discuss the added memory required at training and inference time. Some discussion of training and inference memory requirements (in addition to the runtime results already included) would be appreciated."}, "questions": {"value": "- Figure 5 has no values in the axes, so it is hard to tell how significant the differences in scores actually are.\n- Could the authors clarify and further fill out the results in Figure 6? The authors claim that \"Moderate values (0.5-2) yield the best overall performance\", but do not test any intermediate values between 0.5 and 2. Also, there seems to be some instability in this range, especially for abstract generation. There are no values on the y axis for this graph, so it is difficult to tell how much $\\lambda$ affects performance.\n- It would be valuable to see some examples of personalized outputs produced by FlyThinker, for example for different reasoning model sizes or $\\lambda$ parameters. This would make it more clear how exactly these factors affect the outputs of the generator.\n\nMinor typo: line 180: \"tought\" instead of \"thought\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lQirMSv8AA", "forum": "lle0aGQyQb", "replyto": "lle0aGQyQb", "signatures": ["ICLR.cc/2026/Conference/Submission8572/Reviewer_uQ75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8572/Reviewer_uQ75"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689741222, "cdate": 1761689741222, "tmdate": 1762920423375, "mdate": 1762920423375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FlyThinker, a “think-while-generating” framework for personalized long-form text generation. FlyThinker employs a Reasoner that generates latent token-level reasoning signals and a Generator that dynamically integrates these reasoning signals into its token-level predictions, enabling parallel training and inference. Experiments on three tasks from the LONGLAMP benchmark, including Product Review, Abstract Generation, and Topic Writing, demonstrate that FlyThinker achieves improvements in both personalization quality and generation efficiency over several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The “think-while-generating” paradigm is well-motivated, and the overall methodology is simple and intuitive.\n2. The evaluation is comprehensive with multiple metrics, showing the effectiveness of the proposed method over baselines.\n3. FlyThinker achieves training and inference efficiency comparable to SFT, which is a major advantage relative to existing reasoning-augmented methods that typically incur higher latency."}, "weaknesses": {"value": "1. It is not clear whether the reported personalization results are based on the user-based split (testing on unseen users) or the temporal split (testing on later instances of seen users). Since these settings test different personalization abilities (cross-user vs. within-user), clarification or stratified results would make the findings more interpretable.\n\n2. While the paper ablates on Reasoner size, it is not clear how the Reasoner scales relative to the Generator, e.g., what would be the smallest Reasoner that still remains effective for different Generator sizes - would it be roughly 30%, or 50% of the Generator size? Insights into this would provide very helpful practical guidance for applying FlyThinker in real-world settings.\n\n3. Appendix H shows that adding reasoning tokens to both input and output positions yields the best performance. It is not clear how sensitive FlyThinker is to user history length, i.e., when each user has a lot of historical records with long-form generations, making the context very long. A discussion on this would strengthen the paper’s empirical insights.\n\n4. Are the learned latent reasoning tokens interpretable, or could they be used for downstream applications such as user clustering or user preference visualization? Some discussions on this would offer insights into the interpretability of the latent reasoning tokens."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uq0RLwwSRT", "forum": "lle0aGQyQb", "replyto": "lle0aGQyQb", "signatures": ["ICLR.cc/2026/Conference/Submission8572/Reviewer_vYSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8572/Reviewer_vYSZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701166748, "cdate": 1761701166748, "tmdate": 1762920423027, "mdate": 1762920423027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}