{"id": "8Vp3k3EEZi", "number": 16117, "cdate": 1758260226318, "mdate": 1759897260811, "content": {"title": "BSSR: Binarization and Sparsity For Image Super-Resolution", "abstract": "Lighter models and faster inference remain the focus in the field of image super-resolution. Quantization and pruning are both effective methods for compressing deep models. Unfortunately, existing approaches often optimize quantization and pruning independently: standalone binarization reduces storage but underutilizes sparsity, while N:M sparsity on weights accelerates inference but leaves high-bit storage overhead. Notably, no prior work has explored N:M sparse binary SR networks. In this paper, we combine quantization and sparsity to propose an extreme compression method for super-resolution tasks, namely BSSR. Within this framework, we introduce two key components:  Binarized N:M Sparse Quantizer (BSQ) and Binarized Sparse Gradient Adjuster (BSGA). Firstly, BSQ is a sparse binarization operation across dimensions, simultaneously performing activation and weight binarization while imposing N:M sparsity on weights, significantly reducing storage and computational resource requirements. Secondly, BSGA employs a learnable clipping interval and distinct gradient scaling factors for preserved and masked elements to overcome the non-differentiability of binarization and sparse masking, thereby enabling stable gradient propagation and improving training convergence in sparse binary networks. Extensive experiments on SR benchmarks demonstrate that BSSR achieves state-of-the-art performance with significant improvements in PSNR and SSIM over compression methods.", "tldr": "", "keywords": ["Image super resolution", "model pruning", "binary quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0facb4736dd7759f53806b3f026875ed83774fc3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents BSSR, a binarized and sparse model lighting method for SISR. The authors propose two components: (1) BSQ for binarizing and sparsing, (2) BSGA for gradient transfer. Experiments are conducted on SwinIR-light and MambaIRv2-light, and commonly used benchmarks. The results are SOTA compared with the combinations of binary quantization methods and sparsification methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing is clear and easy to follow.\n- The increase of PSNR and SSIM on five benchmarks is notable."}, "weaknesses": {"value": "- The experiments are all conducted on the light version of MambaIRv2 and SwinIR. Please provide the results on a bigger version, such as MambaIRv2_SR. Different versions share the same structure. These experiments can be easily conducted and make BSSR more convincing.\n-  The core motivation of BSSR is to reduce memory footprint and computation cost. However, the related results are not reported in the paper. Please provide the complexity of the BSSR, including (1) the required GPU memory during training and inference, (2) the number of parameters of the BSSR, and (3) the speedup ratio of BSSR compared with the MambaIRv2-light and SwinIR-light. \n- Please analyze the storage and computation overhead of Eq (3,4,5) during training and inference in detail.\n- The authors claim that BSGA achieves stable gradient propagation in lines 291~292. Please provide related results to demonstrate \"stable\"."}, "questions": {"value": "- The citing format is incorrect. Please use citep instead of cite.\n- What's the relationship between Eq (8) and the Adam optimizer? Adam does not update the parameters with Eq (8)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G90tJeQojf", "forum": "8Vp3k3EEZi", "replyto": "8Vp3k3EEZi", "signatures": ["ICLR.cc/2026/Conference/Submission16117/Reviewer_Fh3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16117/Reviewer_Fh3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405398859, "cdate": 1761405398859, "tmdate": 1762926291925, "mdate": 1762926291925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BSSR, a framework combining binarization and N:M sparsity for image super-resolution models. The method consists of two components: (1) the Binarized N:M Sparse Quantizer (BSQ), which applies activation and weight binarization along with group-wise N:M sparsity, and (2) the Binarized Sparse Gradient Adjuster (BSGA), which employs a learnable tanh surrogate gradient with different scaling for preserved and masked weights. The approach is evaluated on recent SR models (SwinIR-light and MambaIRv2-light), showing state-of-the-art performance, achieving consistent PSNR/SSIM gains compared to prior quantization and pruning baselines.\n\nThe paper tackles an important direction in extreme compression for SR, and the empirical results are strong. However, the lack of novelty, the absence of hardware-oriented benchmarks, and the limited scope to SR reduce the overall impact."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper evaluates the proposed method on recent lightweight and competitive SR models such as SwinIR-light and MambaIRv2-light, showing strong reconstruction performance improvements.\n\n2. The integration of binarization with N:M sparsity is timely and relevant, addressing efficiency concerns in super-resolution networks."}, "weaknesses": {"value": "1. The method is not SR-specific and could be applied to other tasks. Demonstrating generalizability (e.g., classification, detection) would strengthen the contribution.\n\n2. The motivation of N:M sparsity and binarization lies in hardware efficiency, yet no measurements of memory access, computation cost, or latency are reported. This missing evaluation is critical for assessing the practical value.\n\n3. The paper’s presentation is difficult to read due to overly long sentences and dense writing.\n\n4. N:M sparsity is supported only on Ampere GPUs, limiting deployment applicability.\n\n5. The main components (group-wise sparsity, tanh-based STE) are well-established techniques; combining them provides incremental novelty."}, "questions": {"value": "Please see Weakness and answers the concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns exisit"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ymQEJVWK17", "forum": "8Vp3k3EEZi", "replyto": "8Vp3k3EEZi", "signatures": ["ICLR.cc/2026/Conference/Submission16117/Reviewer_qFGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16117/Reviewer_qFGN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531797708, "cdate": 1761531797708, "tmdate": 1762926291468, "mdate": 1762926291468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose BSSR by combining binarization and N:M structured sparsity. Specifically, a Binarized N:M Sparse Quantizer (BSQ) is developed to simultaneously perform activation and weight binarization while imposing N:M sparsity on weights. Meanwhile, a Binarized Sparse Gradient Adjuster (BSGA) is introduced to employ learnable hyperbolic tangent function for end-to-end optimization. Experiments are conducted on widely-applied becnhmarks and the results demonstrate that BSSR achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experiments are conducted and the proposed method produces promising results.\n- This paper is well written and easy to follow."}, "weaknesses": {"value": "- Despite promising results, the technical novelty of the proposed method is limited. Network quantization, network binarization, and fine-grained pruning have been widely investigated for efficient image SR. In addition, the combination of these techniques has also been studied. From this perspective, the technical contribution is rather limited.\n\n- Several representation baselines ([c1-c2]) are missing in the performance evaluation. In seems these methods produces even superior performance (both quantitatively and visually) as compared to the proposed method. From this point of view, the effectiveness and superiority of the proposed method should be further validated.\n\n[c1] Lightweight Image Super-Resolution via Flexible Meta Pruning\n[c2] Flexible Residual Binarization for Image Super-Resolution\n\n- While this paper focus on efficient image SR, quantitative results on FLOPs, memory cost, and runtime are necessary to demonstrate the effectiveness of the proposed method.\n\n- Ablation experiments are not sufficient. As binarization and structured sparsity are combined in the proposed method, ablation experiments should be conducted to evaluate the accuracy and efficiency performance with only binarization or sparsity."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceXkcRh5rg", "forum": "8Vp3k3EEZi", "replyto": "8Vp3k3EEZi", "signatures": ["ICLR.cc/2026/Conference/Submission16117/Reviewer_q78U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16117/Reviewer_q78U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576775928, "cdate": 1761576775928, "tmdate": 1762926290646, "mdate": 1762926290646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an extreme compression framework, BSSR, for lightweight image super-resolution (SR), integrating sparsification and quantization method. BSSR comprises a binaried N:M sparse quantizer and a binarized sparse gradient adjustor, which together enhance the trainability of binarized-quantized networks. Experimental results demonstrate the effectiveness of the proposed strategy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper discusses the effective combination of two different compression strategies, which is a new perspective to solve the problem of lightweight SISR.\n2. The manuscript is clearly written with a well-organized structure. The proposed method is concise and effective, demonstrating practical applicability."}, "weaknesses": {"value": "1. Both grouped quantization and weight soft regularization are not new techniques. The authors need to highlight their contributions.\n\n2. The rationale for adopting grouped quantization remains unclear. As shown in Table 2, performance degrades with increasing group numbers, making it difficult to justify the use of grouped quantization.\n\n3. As the authors claim, \"BSSR eliminates the cumbersome process of channel search → channel pruning → fine-tuning while achieving superior results.\" Therefore, I am curious whether the proposed method can reduce training costs compared to single binarization or quantization methods.\n\n4. The experimental results lack comparisons with single methods, making it difficult to evaluate the trade-offs between performance and model size in the combined approach."}, "questions": {"value": "please refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0SzhYCUMF4", "forum": "8Vp3k3EEZi", "replyto": "8Vp3k3EEZi", "signatures": ["ICLR.cc/2026/Conference/Submission16117/Reviewer_Hd5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16117/Reviewer_Hd5x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924170860, "cdate": 1761924170860, "tmdate": 1762926290266, "mdate": 1762926290266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}