{"id": "yz2EM9EefE", "number": 19943, "cdate": 1758300836843, "mdate": 1759897011316, "content": {"title": "Preference Optimization via Key-step Error Exploration for Multi-step Reasoning in LLMs", "abstract": "Large Language Models (LLMs) have shown promising performance in various reasoning tasks, but still confront challenges when dealing with complex multi-step reasoning. Existing methods suggest using fine-grained preference signals to guide mathematical reasoning step by step. However, how to efficiently build a fine-grained preference dataset with correct and incorrect steps remains an open problem. To address this challenge, we propose an efficient method for preference optimization via Key-step Error Exploration (KEEP). Unlike previous methods that rely on extensive sampling of whole responses or predefined perturbations, KEEP implements a more controllable and lightweight step-level preference data construction. Specifically, KEEP designs a key step identification strategy to simplify data construction by focusing on critical steps of the reasoning path. Moreover, KEEP proactively explores the underlying errors on the key steps and speculatively remains high-valued errors for controllability. By focusing on key step error exploration, KEEP addresses a crucial gap in the efficient construction of fine-grained preference datasets. Extensive experiments on models from 7B to 70B show KEEP delivers up to a 9.5% performance gain across 6 mathematical reasoning benchmarks while reducing data generation costs by up to 10x. We further demonstrate KEEP's broad generality, showing strong performance on diverse domains including logic, code generation, and long-form QA across 8 distinct domains. Moreover, our analysis indicates KEEP's potential for training process supervision reward models (PRMs), which could effectively advance mathematical reasoning evaluation frameworks.", "tldr": "", "keywords": ["Direct Preference Optimization", "Multi-step Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cad5ca1ec404c114c1e1dcb3bdb0f3987724320b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a method for preference optimization through key-step error exploration, which comprises three stages: key step identification, proactive error exploration, and speculative filtering. Specifically, the paper employs PPL and entropy to identify key steps in the long-chain reasoning trajectories. It then proactively explores potential errors using GPT-4o, and applies speculative filtering based on rejection sampling to retain high-valued data. The resulting fine-grained preference dataset is subsequently used to train the model with step-dpo. \n\nThe paper conducts experiments on six mathematical reasoning benchmarks and other general reasoning benchmarks to evaluate the effectiveness and generality of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and easy to follow.\n\nThe paper combines PPL and information entropy to identify key steps, and also has a theoretical analysis.\n\nThe experimental results demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "The paper combines PPL and information entropy to identify key steps. Although the ablation study confirms the importance of key step identification, it does not clarify the respective contributions of these two factors.\n\nIn the proactive error exploration stage, although the paper generates incorrect steps at three different levels, it does not specify their respective impact in the experiments. Furthermore, the training process lacks clarification on how these data are utilized, nor does it explain how the proportions of these data are allocated in the experiments.\n\nAs shown in Tables 2 and 3, the proposed method KEEP appears to exhibit similar or even worse performance than RISE. It would be helpful to provide an explanation for these observations.\n\nAdditionally, the experiments in this paper are based on Llama-3.1-8B and Qwen2-7B as backbones. To more rigorously evaluate the method, it would be beneficial to adopt more advanced backbones such as Qwen3-8B and DeepSeek-R1-Distill-Qwen-7B, which already surpass the performance achieved by the KEEP in the paper.\n\nSeveral relevant studies on DPO, particularly step-wise DPO, such as IUPO[1], Selective-DPO [2], ConfPO[3] and Full-Step-DPO[4], have been proposed this year. However, the paper fails to incorporate these works in the related work section.\n\n[1] Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning, ACL 2025 \\\n[2] Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization, 2025.7 \\\n[3] ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization, 2025.6 \\\n[4] Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning, 2025.2"}, "questions": {"value": "see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "scIuQXUElQ", "forum": "yz2EM9EefE", "replyto": "yz2EM9EefE", "signatures": ["ICLR.cc/2026/Conference/Submission19943/Reviewer_kwVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19943/Reviewer_kwVS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813981785, "cdate": 1761813981785, "tmdate": 1762932111052, "mdate": 1762932111052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KEEP (Preference Optimization via Key-step Error Exploration) for multi-step reasoning in LLMs. Instead of sampling many whole responses or relying on noisy step annotations, KEEP builds step-level preference pairs by: (1) Key-step identification using a linear fusion of per-step perplexity (PPL) and contextual entropy to locate steps that are both uncertain and influential; (2) Proactive error exploration where a draft model generates simple / medium / complex incorrect variants at those steps; and (3) Speculative filtering that retains “high-value” pairs whose incorrect step has a high Prejected/Pchosen ratio, making them challenging and informative."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The PPL+entropy score targets the most impactful steps; the draft-model edits plus speculative filtering deliver diverse yet realistic negatives, avoiding template bias and high-temperature over-sampling costs (reported ≤10× cheaper).\n\n2. The method slots neatly into established step-wise preference optimization, with transparent formulas and training objective.\n\n3. Strong improvements across six math datasets and solid results at 70B+ scale on competition-level tasks (AIME24, Odyssey)."}, "weaknesses": {"value": "1. Fusion weights (α, β) and the key-step proportion (default 50%) are reasonable but still hyperparameters; robustness is partially studied, yet full calibration guidance across tasks/models is limited.\n\n2. KEEP often wins, but certain entries (e.g., Odyssey on Qwen2-72B vs. RISE) are close or slightly behind, suggesting sensitivity to base model/benchmark.\n\n3. The paper highlights diversity and pedagogical value, but finer-grained case studies where edits induce spurious reasoning drifts would clarify failure modes."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rlZeDFOGRK", "forum": "yz2EM9EefE", "replyto": "yz2EM9EefE", "signatures": ["ICLR.cc/2026/Conference/Submission19943/Reviewer_B6Vi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19943/Reviewer_B6Vi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833525731, "cdate": 1761833525731, "tmdate": 1762932110312, "mdate": 1762932110312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new controllable, lightweight, and scalable pipeline (i.e., KEEP) for step-wise preference data construction. It could efficiently scale up the step-wise preference dataset. It consists of three stages: 1) key step identification, 2) proactive error exploration, and 3) speculative filtering. Extensive experiments show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work innovatively proposes a construction pipeline for step-wise preference data. It first identifies critical reasoning steps, which can save large computational resources.\n2. Writing is clear.\n3. The experimental results are good."}, "weaknesses": {"value": "In the data pipeline, the stage of 'Proactive Error Exploration' uses a draft LLM to produce errors. This means that the data is off-policy, rather than on-policy. This requires that the data be diversified enough to cover the distribution of the errors that the current policy model is prone to making. However, in most cases, there are some mistakes that the constructed data cannot cover by drafting LLMs with simple rules. These mistakes would persist even after preference optimization.\nIn contrast, on-policy data (produced by the policy model itself) should work better, since it could expose comprehensively the mistakes that the policy model would make. \nMaybe the authors can modify this stage somehow to an on-policy strategy, and compare these two methods."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mDWRqmB0di", "forum": "yz2EM9EefE", "replyto": "yz2EM9EefE", "signatures": ["ICLR.cc/2026/Conference/Submission19943/Reviewer_XxsP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19943/Reviewer_XxsP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985422452, "cdate": 1761985422452, "tmdate": 1762932109304, "mdate": 1762932109304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For open-ended tasks such as search-based QA, it is challenging to directly assess the effectiveness of final results. The paper does not adequately address how such evaluations are conducted, which raises concerns about the reliability of the reported improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured with clear writing that facilitates comprehension of the proposed methodology.\n2. The proposed method is conceptually straightforward, and using entropy or perplexity to detect critical points in reasoning paths represents a general and reasonable approach.\n3. The paper provides extensive experiments across multiple task datasets, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The authors employ a separate draft LLM to expand paths and explore erroneous trajectories at key steps. This raises several concerns: (a) Does this introduce out-of-distribution (OOD) problems? (b) What are the additional computational costs? (c) Is this approach more effective than direct instance-level sampling? The paper lacks adequate experimental analysis to address these critical questions.\n2. Using entropy or perplexity to identify critical points in reasoning paths is not a novel contribution, and the paper does not provide substantial improvements to existing approaches in this regard.\n3. The combination of reasoning-based key step mining with DPO appears inefficient. A more principled approach would be to integrate this methodology with process supervision frameworks such as GRPO or PPO, which would be more naturally aligned with the step-wise nature of the proposed method."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HjXuMnwK5m", "forum": "yz2EM9EefE", "replyto": "yz2EM9EefE", "signatures": ["ICLR.cc/2026/Conference/Submission19943/Reviewer_6Sce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19943/Reviewer_6Sce"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997226167, "cdate": 1761997226167, "tmdate": 1762932108549, "mdate": 1762932108549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}