{"id": "Ah1630bQ5z", "number": 18331, "cdate": 1758286483251, "mdate": 1759897110509, "content": {"title": "Design Principles for TD-based Multi-Policy MORL in Infinite Horizons", "abstract": "Multi-Objective Reinforcement Learning (MORL) addresses problems with multiple, often conflicting goals by seeking a set of trade-off policies rather than a single solution. Existing approaches that learn many policies at once have shown promise in deep settings, but they depend on supervised retraining and carefully curated data, making them ill-suited for online and infinite-horizon tasks. Temporal-Difference (TD) methods offer a natural alternative, as they update policies incrementally during interaction, but current TD-based approaches are limited to small, episodic problems. In this work, we present design principles for extending TD-based multi-policy MORL to infinite horizons, realized in a framework that combines trajectory-based policy tracking, mechanisms for learning both predictable (stationary) and flexible (non-stationary) policies, techniques to avoid spurious dominance relations, and cycle detection to ensure well-defined long-term behavior. Through ablation studies, we show how each principle contributes to recovering diverse and reliable policies, providing a principled path toward scalable TD-based multi-policy methods in deep MORL.", "tldr": "", "keywords": ["Multi-Objective Reinforcement Learning", "Multi-Policy Learning", "Temporal-Difference Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5087583f333bff3d68262c155441d79cae8bcfa2.pdf", "supplementary_material": "/attachment/d22f0eeda0c36cf324d04d3f4a732f2aeb022505.zip"}, "replies": [{"content": {"summary": {"value": "MORL addresses multiple conflicting objectives by approximating the Pareto Front (PF)—a set of non-dominated policies representing trade-offs. The authors critique existing methods: supervised approaches such as Pareto Conditioned Networks (PCNs) require costly retraining and curated data, limiting online adaptation, while TD-based methods are confined to tabular, episodic tasks. They propose a trajectory-centric framework using colour-labelled transitions, tables for policy tracking, and mechanisms to handle stationary/non-stationary policies, spurious dominations and cycles. Ablation studies on an adapted DeepSeaTreasure environment validate the principles, demonstrating improved policy diversity and reliability. The paper concludes by positioning these principles as a foundation for deep RL extensions. Appendices detail algorithms and complexity analysis."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is structured as a series of \"design principles,\" and the methodology attempts to solve each identified problem in sequence (e.g., policy tracking, spurious domination, cycle detection). This is a logical way to build a complex algorithm.\n2.\tThe ablation study (Section 4) shows contribution of each \"design principle\" to the final performance (e.g., AS1 shows the need for policy tracking, AS5 shows the need for cycle detection). This provides clear empirical justification for the framework's internal components."}, "weaknesses": {"value": "1. Clarity and Presentation: The paper introduces a dense vocabulary of new terminology (e.g., \"color-label,\" \"swirl trajectory,\" \"Policy-Transition Table (PTT),\" \"Stationary Segments Mapping (SSM)\") without sufficient formal definition or high-level intuition. Section 3 is exceptionally difficult to follow, making the core methodology hard to assess, reproduce, or build upon.\n2. Disconnect Between Motivation and Future Extension: The paper motivates itself as laying a \"principled foundation for future deep-RL extensions.\" However, the entire methodology is tabular, and the experiments are confined to a single 2D gridworld. The paper makes no attempt to explain how complex, discrete structures like the PTT and SSM could ever be scaled to the high-dimensional, continuous state spaces required by deep RL. This makes the primary motivation feel unsupported.\n3. Insufficient Comparison to Baselines: While the internal ablation study is useful, the paper fails to compare its final algorithm against MORL baselines. Even if SOTA deep methods are unsuitable, comparisons against adapted tabular methods (e.g., Pareto Q-Learning https://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf) are necessary to benchmark the algorithm's actual performance, sample efficiency, and computational cost.\n3. Missing Theoretical Foundations (Markov Chains): The paper's unconventional treatment of \"cycles\" and \"swirls\" to manage infinite-horizon policies lacks rigor. A better analysis of infinite-horizon problems necessitates a connection to established Markov Chain theory (e.g., ergodicity, aperiodicity, etc.). The paper avoids this, making it unclear if the proposed cycle-detection and policy-tracking mechanisms are robust."}, "questions": {"value": "1. About the \"Color-Label\" Mechanism, how is the \"color-label\" c formally defined, generated, and stored? Is c a discrete integer? Does the space of c grow unboundedly as new trajectories and segments are discovered?\n2. The main premise is to build a foundation for deep RL. What is the explicit proposed path for scaling the PTT and SSM structures? Would this require approximating these discrete, graph-like tables with a GNN or a similar architecture? How would the \"color-label\" concept translate to a continuous state space?\n3. The distinction between \"cycles\" (stationary) and \"swirls\" (non-stationary) is central. Is a \"swirl\" (Fig 2a) simply a non-stationary policy that revisits states? How does the \"implicit\" temporal encoding (Fig 2b) offer a concrete advantage over a standard formulation that includes a time-step t or history h as part of the state?\n4. The paper states the environment (DeepSeaTreasure) was \"adapted to an infinite-horizon setting (agent goes to the initial state after the end of an episode).\" This sounds like an episodic task that is simply reset, not a true infinite-horizon, continuing task. Can the authors clarify this? If the task is truly episodic, it may undermine the paper's entire motivation about infinite-horizons.\n5. The algorithmic complexity for stationary policies in Appendix A.3 is much higher than that of non-stationary policies, while the authors stated ““they may be faster to learn” in lines 163–164. This sounds contradictory."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gHNYRgbcHO", "forum": "Ah1630bQ5z", "replyto": "Ah1630bQ5z", "signatures": ["ICLR.cc/2026/Conference/Submission18331/Reviewer_rxHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18331/Reviewer_rxHq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705187512, "cdate": 1761705187512, "tmdate": 1762928043803, "mdate": 1762928043803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for temporal-difference (TD)-based multi-policy multi-objective reinforcement learning (MORL) in the infinite-horizon setting.\nThe authors propose a color-coded representation of policies and trajectories, introduce mechanisms for policy tracking and stationary segment mapping, and formalize several “design principles” (Sections 3.1–3.6) aimed at preventing issues such as spurious domination and non-stationary credit assignment.\nThe framework is implemented in a tabular DeepSea Treasure environment, and eight ablation studies (AS1–AS8) are reported to justify individual design components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The notion of mapping stationary segments and detecting cycles to handle infinite horizons is conceptually novel and mathematically consistent.\n2. Despite being primarily theoretical, the paper supports every design element with an ablation (AS1–AS8), providing empirical intuition for why each component matters."}, "weaknesses": {"value": "1. The framework’s setting—colored trajectories, segments, and swirls—is unconventional and a bit under-explained.\nA concise motivating example illustrating would make the paper much more approachable.\n\n2. The framework remains fully tabular and is evaluated only on DeepSea Treasure.\nWithout evidence or discussion of scalability, it is difficult to assess real-world practicality.\n\n3. The paper devotes substantial space to figures and tables, leaving limited room for interpretation.\nFor instance, Table 1 lists all ablations but uses many internal terms unfamiliar to newcomers; more textual reasoning or summary commentary would be preferable.\n\n4. The Appendix follows immediately after References (page 11) without a clear break.\nMoving large tables (e.g., Table 1) to the appendix and separating these sections with \\section*{Appendix} would improve readability."}, "questions": {"value": "1. Could the authors provide a small running example early in Section 3 to clarify the role of colors and how they differ from conventional policy identifiers?\n\n2. How might the proposed tabular mechanisms (e.g., PTT, SSM) extend to function approximation or actor-critic settings?\n\n3. In the ablation results (Section 4), are the reported improvements statistically significant over multiple seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KgHCoOe12U", "forum": "Ah1630bQ5z", "replyto": "Ah1630bQ5z", "signatures": ["ICLR.cc/2026/Conference/Submission18331/Reviewer_dUff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18331/Reviewer_dUff"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970561392, "cdate": 1761970561392, "tmdate": 1762928043375, "mdate": 1762928043375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a trajectory-centric framework for Temporal-Difference (TD)-based multi-policy MORL that aims to achieve stable and interpretable behavior in infinite-horizon settings.\n\nUnlike prior deep multi-policy approaches such as Pareto Conditioned Networks (PCN), which depend on supervised retraining and curated data, the proposed framework incrementally learns multiple Pareto-optimal policies through TD updates.\n\nThe authors introduce a set of design principles—including trajectory-level policy tracking, unification of stationary and non-stationary policies, removal of spurious domination, and explicit cycle detection—to ensure reliable policy following and meaningful undiscounted returns.\n\nThrough eight ablation studies on the DeepSeaTreasure benchmark, each design element’s contribution to policy stability, interpretability, and diversity is analyzed.\n\nThe work positions itself as a conceptual and algorithmic foundation for future TD-based extensions to deep multi-objective reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper clearly articulates the gap between supervised deep MORL (e.g., PCN) and online TD-based methods and proposes a principled bridge through trajectory-centric design.\n- The trajectory-level policy-following mechanism (color labeling and Policy-Transition Table) provides a novel and interpretable way to maintain consistency across multiple Pareto-optimal policies.\n- The framework’s unified handling of stationary and non-stationary policies offers full Pareto-front recovery while maintaining predictable policy behavior.\n- The ablation results convincingly demonstrate the functional contribution of key components such as spurious domination correction, SSM, and cycle detection to stability and interpretability."}, "weaknesses": {"value": "- The paper does not compare against established baselines such as PCN, MPQ-Learning, or Pareto-DQN, all of which include experiments on broader environments (e.g., Minecraft-based or continuous-control benchmarks).\n- The evaluation is limited to the tabular DeepSeaTreasure environment, so scalability and generalization remain untested.\n- The theoretical grounding of the proposed principles is mostly heuristic; there is no formal convergence or optimality analysis supporting the modifications.\n- Experimental justification is largely qualitative, relying on hypervolume and visual coverage metrics rather than statistical performance comparisons.\n- Algorithmic structure is complex (color labeling, PTT, SSM, cycle detection), potentially limiting reproducibility and computational efficiency.\n- As a result, while the framework is conceptually interesting, it lacks both the theoretical justification and large-scale empirical evidence needed to confirm its practical advantage."}, "questions": {"value": "The proposed framework introduces several heuristic yet intuitively reasonable design principles (e.g., trajectory coloring, stationary-segment mapping, and spurious domination correction).\n\nWhile the motivation behind each component is clear, it remains uncertain why these heuristics consistently lead to better learning dynamics.\n\nCould the authors provide theoretical justification or empirical evidence—beyond ablation comparisons—that explains why these mechanisms are effective or under what conditions they provably improve convergence or Pareto-front coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A1THXoxHJ5", "forum": "Ah1630bQ5z", "replyto": "Ah1630bQ5z", "signatures": ["ICLR.cc/2026/Conference/Submission18331/Reviewer_Hujr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18331/Reviewer_Hujr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987596515, "cdate": 1761987596515, "tmdate": 1762928042814, "mdate": 1762928042814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multi-objective RL in the infinite-horizon regime and argues that TD-style, online updates can support learning and executing a set of Pareto policies when paired with specific design prescriptions. The method is trajectory-centric: it attaches color labels to transitions and uses a Policy-Transition Table to keep an agent on a chosen Pareto policy, handles both stationary and non-stationary behaviors, normalizes away length/frequency biases that cause spurious dominance, and detects/encapsulates cycles so undiscounted averages remain meaningful. Experiments are ablations on DeepSeaTreasure (MO-Gym) that isolate the effect of each ingredient rather than contrasting against external baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies the missing link between deep supervised MORL and TD-based online methods and proposes a structured framework to bridge them.\n2. The trajectory-centric policy-following mechanism offers a concrete and interpretable way to maintain consistent Pareto policies during learning.\n3. Integrating stationary and non-stationary policy handling allows comprehensive Pareto-front reconstruction under infinite-horizon settings.\n4. The ablation analysis effectively isolates how each design choice (e.g., SSM, cycle detection) contributes to policy stability and learning reliability."}, "weaknesses": {"value": "- The paper does not compare against established baselines such as PCN, MPQ-Learning, or Pareto-DQN, all of which include experiments on broader environments (e.g., Minecraft-based or continuous-control benchmarks).\n- The evaluation is limited to the tabular DeepSeaTreasure environment, so scalability and generalization remain untested.\n- The theoretical grounding of the proposed principles is mostly heuristic; there is no formal convergence or optimality analysis supporting the modifications.\n- While the framework is conceptually interesting, it lacks both the theoretical justification and large-scale empirical evidence needed to confirm its practical advantage."}, "questions": {"value": "1. The proposed principles are largely intuitive and heuristic. Can you clarify why they work in practice? In particular, could you provide theoretical analysis or empirical evidence beyond ablations that explains their actual effectiveness?\n2. How do the bias-correction terms (trajectory length, reward frequency) theoretically influence convergence or Pareto coverage?\n3. Compared to existing MORL baselines (PCN, MPQ-Learning), how do you expect the proposed framework to scale to larger or continuous domains such as Minecraft or complex control environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sc8Q3mZ0BI", "forum": "Ah1630bQ5z", "replyto": "Ah1630bQ5z", "signatures": ["ICLR.cc/2026/Conference/Submission18331/Reviewer_c5nB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18331/Reviewer_c5nB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999771748, "cdate": 1761999771748, "tmdate": 1762928042354, "mdate": 1762928042354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}