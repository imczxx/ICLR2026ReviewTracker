{"id": "QtfALPluAj", "number": 16216, "cdate": 1758261811402, "mdate": 1759897253972, "content": {"title": "mGRPO: Unlocking LLM Reasoning through Multilingual Thinking", "abstract": "As LLMs develop stronger multilingual capabilities, the long-standing English-centric bias is gradually diminishing. In some reasoning tasks, responses in non-English languages even surpass those in English. Existing approaches, such as majority voting or weighting across languages, have explored this potential but often fall short due to task complexity and suboptimal language selection. To investigate the role of language diversity in reasoning, we conduct a \\textit{Polyglot Thinking Experiment}, prompting models to answer each question in ten different languages or without any language restriction. Results show that non-English responses often achieve higher accuracy than English ones, and the best performance frequently emerges when the model is free to choose its response language. These findings suggest that LLMs benefit from a broader and more flexible multilingual thinking space. Building on this insight, we propose \\textbf{Multilingual Group Relative Policy Optimization (mGRPO)}, a reinforcement learning framework that improves LLM reasoning by generating multilingual preference data online using both language-constrained and unconstrained prompts. The model is optimized through group-wise reward comparisons based on accuracy and reasoning format.  Despite relying on only ~18.1k training examples without chain-of-thought supervision, mGRPO achieves consistent gains across four benchmarks: MGSM, MATH500, PolyMath, and X-CSQA, outperforming two base LLMs (Qwen2.5 and Llama3) by an average of 7.5\\% and obtains SOTA scores. These results highlight the value of multilingual thinking and demonstrate that mGRPO provides a lightweight yet effective approach to unlock reasoning potential in LLMs.", "tldr": "", "keywords": ["LLM Reasoning", "Multilingual Thinking", "GRPO"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9fd6f2537ad4ccecda1f1cd9ace1a19fe7eace6.pdf", "supplementary_material": "/attachment/ac8b861dae0070bf8a9d837e2a7f8b932aafc4e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MGRPO, a framework designed to enhance the reasoning ability of large language models (LLMs) through online multilingual preference data generation. At the core of MGRPO is the Polyglot Reasoning Generation Module, which guides the LLM to generate multilingual reasoning paths to optimize LLM. Experimental results demonstrate that MGRPO consistently improves performance across four reasoning benchmarks and two base models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "By explicitly generating multilingual preference groups, including one unconstrained output and several language-constrained outputs, the approach enables the model to leverage a wider and more diverse reasoning space than prior reinforcement learning-based methods, most of which remained English-centric. Building on this insight, this work incorporates multilingual reasoning into the rollout phase to further strengthen the model's reasoning capability."}, "weaknesses": {"value": "1. While the motivation to enhance LLM reasoning via multilingual thinking is commendable, the execution is fundamentally misaligned with this goal. The proposed methodology effectively reduces to cross-lingual thinking, rather than genuine multilingual reasoning.\n2. The proposed MGRPO framework appears to operate without an explicit constraint on the reasoning language, particularly under its optimal performance setting. While this language-agnostic approach aligns with a more flexible \"multilingual thinking\" concept, it introduces an ambiguity: we do not know which language is predominantly utilized for the internal Chain-of-Thought (CoT) generation."}, "questions": {"value": "1. In Figure 2, the specified-language settings are defined in the prompt. However, does the model actually follow these language settings during reasoning?\n2. In Section 5.3, this work examined token activations at each decoding step across all layers. The authors could further analyze the actual reasoning languages used by the model under its best-performing conditions for each target language.\n3. $\\text{MGRPO}_{lang}$ shows a decline in accuracy, particularly for low-resource languages.\nWhy does adding the language consistency reward lead to worse performance compared to the base model?\n\n\nTypos:\n1. Line 206, \"REWARD MODUL\" should be \"REWARD MODULE\"\n2. Line 965, \"espicially \" should be \" especially\"\n3. Line 965, \"low-resour\" should be \"low-resource\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1N9Askapmo", "forum": "QtfALPluAj", "replyto": "QtfALPluAj", "signatures": ["ICLR.cc/2026/Conference/Submission16216/Reviewer_EMqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16216/Reviewer_EMqQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550192544, "cdate": 1761550192544, "tmdate": 1762926377698, "mdate": 1762926377698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces mGRPO, an effective method for improving LLM reasoning performance by exposing the model to diverse languages during policy optimization, even if the resulting enhanced reasoning pathway ultimately defaults to generating answers via an internal, English-centric process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of the paper is its empirical demonstration that using multilingual training roll-outs within a reinforcement learning framework achieves SOTA accuracy across multiple multilingual reasoning benchmarks."}, "weaknesses": {"value": "1. The paper's central claim is to leverage \"multilingual thinking\".  However, the analysis section (Appendix H) explicitly reveals that the learned policy predominantly collapses back to English reasoning during inference. For German, French, Spanish, and Russian, the model achieves 0.0% to 0.4% consistency, indicating that regardless of the input language, the model reasons and responds in English. It is totally contradicts the proposed idea that model is performing reasoning in a diverse multilingual space. The English thinking makes the whole framework non-sense where the prompts are think in Swahili/Japanese/Russian. So, the core idea of \"multilingual thinking\" is not convincingly supported by the experimental evidence.\n\n2. Although the paper added a language consistency reward to encourage the model to think in the input language, the results demonstrate that this reward will be degraded the performance on reasoning tasks. While authors have considered the language unconsistency issue, why the proposed reward function (Section 3.2) not include this part is not well explained. \n\n3. The calculation of the language consistency reward is not the same with the paper which proposed GRPO. This paper assign the reward 1 or 0 based on whether the generated answer is in the same language as the input question. However, GRPO uses a more fine-grained approach by calculating the proportion of tokens in the generated answer that match the input language. It neglects the cases of code-mixing.\n\n4. After using the language consistency reward, the performance on reasoning tasks drops significantly (Table 13).  This drop is particularly severe in the low-resource languages (URL), where the accuracy plummets from 42.8 to 12.8. These results highlight a major flaw, that the only way the method achieves its best performance is by following the reasoning process to default to English, suggesting the core idea 'multilingual thinking' is fundamentally flawed.\n\n\n5. This method is not robust to different settings, e.g. the sensitivity to the roll-out number n. As shown in Table 12, the best choice of n varies across different models and languages, making it difficult to select a universal n for practical applications. \n\n6. For the experiments, the authors only compare the standard GRPO (English only) and mGRPO (multilingual roll-outs). However, the multilingual training data for mGRPO is generated online from a starting set of 1,703 English questions translated into nine languages (18,140 examples total). It lacks one important baseline where the standard GRPO policy is simply trained on a much larger, but still English-only, dataset of comparable size. This leaves open the possibility that the gains are due more to the quantity of the training data (translated questions acting as diverse reasoning prompts) rather than the mechanism of multilingual thinking policy optimization itself."}, "questions": {"value": "see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h6DurJwNLl", "forum": "QtfALPluAj", "replyto": "QtfALPluAj", "signatures": ["ICLR.cc/2026/Conference/Submission16216/Reviewer_BddF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16216/Reviewer_BddF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730869534, "cdate": 1761730869534, "tmdate": 1762926377099, "mdate": 1762926377099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the potential of multilingual thinking to enhance the reasoning capabilities of Large Language Models (LLMs). The authors first present a \"Polyglot Thinking Experiment\" demonstrating that (1) non-English responses can outperform English ones on reasoning tasks, and (2) an unconstrained setting, where the model freely chooses its response language, often yields the best performance. Building on this insight, the paper proposes Multilingual Group Relative Policy Optimization (mGRPO), a reinforcement learning framework. mGRPO adapts the GRPO algorithm by generating a group of responses for each prompt, including one from an unconstrained prompt and several from language-constrained prompts (randomly sampling different languages). This polyglot group is then used to compute group-relative advantages based on a simple reward function combining final answer accuracy and response format. The model is trained online using only 18.1k examples. Experiments on two base models (Qwen2.5-7B, Llama3-8B) across four multilingual reasoning benchmarks (MGSM, MATH500, PolyMath, X-CSQA) show that mGRPO significantly outperforms the base models and strong baselines, including a standard English-only GRPO, demonstrating the effectiveness of leveraging multilingual thinking for improving reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"Polyglot Thinking Experiment\" (Sec 1, Fig 2) provides a clear and compelling empirical foundation for the paper's entire premise. The finding that an unconstrained language setting often outperforms both English-only and specific language-constrained settings is a novel and significant observation.\n2.\tThe proposed mGRPO framework is a clever and original adaptation of GRPO. The core idea of constructing a \"polyglot reasoning set\" for group-wise optimization, mixing unconstrained and language-constrained rollouts, is a direct and principled response to the motivating experiment.\n3.\tThe method is lightweight. It relies on a very small training dataset (~18k examples) and, crucially, does not require supervised chain-of-thought (CoT) data or human-generated preference labels. The reward function is simple, rule-based, and computed online, making the approach highly scalable."}, "weaknesses": {"value": "1. While the unconstrained setting is central to the method's motivation and success, the paper provides limited analysis of what the model actually does in this setting. The text mentions (Sec 1, line 096) that \"the model typically reasons in English but borrows surface entities.\" This is a key detail. During inference (when the [LANGUAGE] token is empty), what languages does the model actually choose to reason in? Does this choice vary by the language of the input question? A more detailed analysis of the unconstrained behavior (e.g., a distribution of languages chosen for CoT during inference) would significantly strengthen the paper's claims about the model \"freely choosing\" its reasoning path.\n2. The simplicity of the reward function ($r \\in \\{0, 1, 2\\}$) is a strength (scalability) but also a potential weakness. It only rewards the final answer and the format. This means a response with a completely flawed reasoning path that luckily arrives at the correct answer (and is formatted correctly) receives the maximum reward (2), identical to a response with a perfectly sound reasoning path. While this \"ends-justify-the-means\" approach clearly works well, it relies on the model stochastically finding correct paths. The paper would be stronger if it discussed the potential limitations of this reward signal.\n3. The paper notes (Sec 5.1, 5.3) that the model's reasoning paths tend to converge to English during training and inference. While the logit lens analysis suggests this is a feature (fusing knowledge), it also raises a question: is the multilingual-thinking-as-scaffolding a temporary effect? If trained for much longer, would mGRPO's performance simply converge to that of the standard GRPO? The paper explores language consistency in Appendix H, but a discussion on the long-term training dynamics and whether the multilingual \"scaffolding\" benefit is persistent would be valuable."}, "questions": {"value": "1.\tFollowing up on Weakness #1: Could the authors provide a more detailed analysis of the model's behavior during inference in the unconstrained setting? For instance, when given a question in Thai (TH) on the MGSM benchmark, what language does the mGRPO-trained model predominantly use for its [Thinking] steps? Does this differ from the base model? This seems critical to understanding how mGRPO utilizes its \"flexible thinking space\" at test time.\n2.\tRegarding the reward function: The format reward FR is binary. Did the authors experiment with a more granular reward, for example, a small reward for just having [Thinking] or just #### [Answer], or a penalty for generating extraneous text (as explored in Appendix I for base models)?\n3.\tThe performance on Llama3-8B (Table 1) shows mGRPO provides a large boost over a weak base model, but the final scores (e.g., 68.11 on MGSM) are still lower than the mGRPO-trained Qwen model (75.93). This suggests the base model's multilingual capability is a strong prerequisite. Could the authors comment on how much \"base\" multilingualism is necessary for mGRPO to be effective?\n4.\tThe analysis of the roll-out number n in Appendix G (Table 12) is interesting. It seems n=4 or n=5 is optimal, and performance drops with more rollouts (e.g., n=8 or n=10), which is somewhat counter-intuitive. The authors attribute this to \"overexposure to low-resource languages.\" Does this imply that the quality of the non-English rollouts is often very low, and adding too many of them introduces more noise than signal to the group-relative advantage estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "obeUGnAzvL", "forum": "QtfALPluAj", "replyto": "QtfALPluAj", "signatures": ["ICLR.cc/2026/Conference/Submission16216/Reviewer_8cad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16216/Reviewer_8cad"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956447300, "cdate": 1761956447300, "tmdate": 1762926376000, "mdate": 1762926376000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multilingual Group Relative Policy Optimization (mGRPO), a reinforcement learning framework, designed to enhance reasoning capabilities of LLMs through multilingual thinking. Motivated by a preliminary experiment showing that multilingual reasoning can outperform monolingual (English-only) reasoning, the authors extend the standard GRPO framework by introducing a multilingual rollout strategy. During training, the model generates a set of multilingual reasoning trajectories under both language-constrained and language-unconstrained prompts, and optimizes them via GRPO. Experiments on four reasoning benchmarks—MGSM, MATH500, PolyMath, and X-CSQA, demonstrate consistent improvements over the baseline GRPO and other variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Insight with Preliminary Validation**. This paper has conceptual clarity and a well-motivated insight derived from the “Polyglot Thinking Experiment.” The preliminary analysis convincingly shows that (1) reasoning in certain non-English languages can outperform English; and (2) granting the model the freedom to choose its reasoning language leads to measurable gains. These findings provide direct empirical evidence for the motivation of leveraging multilingual thinking, making the subsequent methodological development logical.\n\n2. **In-depth Experimental Analysis**. Beyond reporting end-task performance, the paper includes a series of insightful analyses to interpret model behavior. The ablation studies highlight the complementary role of language-constrained and unconstrained reasoning paths, underscoring the necessity of both. The logit-lens analysis reveals that the mGRPO-trained model tends to integrate multilingual reasoning signals into an English-centric latent space, enabling it to perform high-quality reasoning with fewer non-English tokens. This offers an interpretable explanation for why the proposed method works."}, "weaknesses": {"value": "1. **Methodology is Rather Incremental** While the framework is conceptually sound, its algorithmic novelty is limited. The core contribution lies in a prompt-based multilingual data generation module on top of GRPO, while the optimization pipeline itself remains unchanged, still relying on the standard GRPO update and basic reward formulation. The work does not deeply explore richer forms of multilingual signals, such as logical consistency across languages or language-specific reasoning properties. As a result, the method feels more like a creative application of existing techniques rather than a new learning framework.\n\n2. **Baselines Could Be Expanded** Although the paper compares against several reasonable baselines, it omits some relevant recent works that extend GRPO like DAPO[1] or GSPO[2]. Including these would make the empirical evaluation more comprehensive and the claims more robust.\n\n3. **Eventual Reliance on English Reasoning** The analysis section notes that the trained model ultimately converges toward English reasoning during inference. The authors interpret this as knowledge fusion into an English-dominant latent space. However, this observation also reveals a limitation: the framework mainly leverages multilingual reasoning to strengthen English reasoning rather than fostering truly language-agnostic reasoning abilities. This weakens the central hypothesis that multilingual thinking inherently enhances reasoning. Moreover, it implies that the model may still struggle to reason effectively in specific non-English languages. While the appendix (mGRPO-lang) explores a mitigation strategy, the admitted trade-off in performance suggests that practical multilingual applicability remains limited.\n\n4. **Lack of Stability and Statistical Validation** Key experimental results (e.g., Table 1) appear to be reported from single runs, without standard deviation, confidence intervals, or statistical significance analysis. This omission raises concerns about the robustness of the reported improvements. Multiple independent runs with averaged scores and error bars would significantly strengthen the reliability and reproducibility of the results.\n\n[1] Dapo: An open-source llm reinforcement learning system at scale.\n\n[2] Group sequence policy optimization."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IvVvqtJ87h", "forum": "QtfALPluAj", "replyto": "QtfALPluAj", "signatures": ["ICLR.cc/2026/Conference/Submission16216/Reviewer_Bkj8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16216/Reviewer_Bkj8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975229602, "cdate": 1761975229602, "tmdate": 1762926375011, "mdate": 1762926375011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}