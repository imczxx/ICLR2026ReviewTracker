{"id": "zV1hR6wWFK", "number": 22545, "cdate": 1758332648836, "mdate": 1759896860294, "content": {"title": "Biasing the Future: Gaussian Attention for Sequential Decision-Making", "abstract": "Transformers have emerged as powerful sequence models for offline reinforcement learning (RL), but their reliance on purely self-attention mechanisms can limit their ability to capture fine-grained local dependencies and Markovian dynamics present in many RL datasets. In this work, we introduce a modified Decision Transformer architecture that incorporates a Gaussian-biased masked causal attention mechanism. By augmenting attention scores with a distance-aware bias, the model adaptively emphasizes temporally local relationships while still retaining the ability to capture long-range dependencies through self-attention. Experimental results on benchmark offline RL tasks show that our Gaussian-biased Decision Transformer achieves achieves state-of-the-art performance and notable gains over the standard DT, particularly in environments with strong Markovian structure. This demonstrates the importance of explicitly encoding locality into attention mechanisms for sequential decision-making.", "tldr": "", "keywords": ["Decision Transformer", "Offline reinforcementlearning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0818ad09cecddf0a859083f9e645dc00ee550eba.pdf", "supplementary_material": "/attachment/cd485d111a3f2169757356a3679b15430e1de387.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Gaussian-Biased Decision Transformer, which is a modification of the standard Decision Transformer (DT) architecture used in offline reinforcement learning (RL)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The Gaussian bias is a simple, mathematically grounded modification that adds strong inductive priors without increasing model complexity.\nThe approach can extend to any sequence modeling task requiring local sensitivity—beyond RL, potentially to NLP or time-series forecasting."}, "weaknesses": {"value": "The Gaussian width parameter is manually tuned. A **learnable or adaptive σ** could make the model more flexible across environments.\nThe paper lacks comparison with state-of-the-art DT-based methods, and the environments compared in this paper are incomplete. For example, in the antmaze environment, this paper only considers the simplest tasks without comparing more challenging tasks."}, "questions": {"value": "1. The abbreviation for \"Conditional Sequence Modeling\" has already been introduced in the previous paragraph, so there is no need to write out the abbreviation again.\n2. The abbreviation for \"Decision Conformer\" is DC, so why is the abbreviation for \"Decision ConvFormer\" also DC?\n3. typo: “Eq. equation 2”\n4. I understand the authors do not want to use multiplication, but why is addition used in Equation 5 rather than subtraction.\n5. In Equation 5, the value of $\\omega$ is a constant with respect to $\\sigma$, but why do the authors mention \"select its value using validation performance,\" which seems to suggest that $\\omega$ is a variable that changes during training?\n6. In line 84, since the authors mention \"recent states encode the most critical information for accurate decision-making, whereas distant states generally provide diminishingly relevant information,\" why not directly remove the distant information?\n7. The DT-based methods compared in the experimental section are not state-of-the-art algorithms. At the very least, to my knowledge, some of the following DT-based methods significantly outperform the method proposed in this paper.\n\n[1] Q-value regularized transformer for offline reinforcement learning\n\n[2] Q-value Regularized Decision ConvFormer for Offline Reinforcement Learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EoRbrtJQ6M", "forum": "zV1hR6wWFK", "replyto": "zV1hR6wWFK", "signatures": ["ICLR.cc/2026/Conference/Submission22545/Reviewer_vNpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22545/Reviewer_vNpi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761035769527, "cdate": 1761035769527, "tmdate": 1762942270407, "mdate": 1762942270407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a modified version of the Decision Transformer architecture for the sequential decision-making problem, aiming to balance long-term and short-term information when making decisions. To achieve this, the authors introduce a modification to the standard self-attention layer within the Decision Transformer. The contribution is demonstrated on a set of locomotion tasks from the D4RL benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of a bias term within the self-attention layer seems novel.\n- The experimental results show promise, improving over the baselines."}, "weaknesses": {"value": "1. The current presentation of the paper is poor: there are many typos, repeated words, and redefined abbreviations in the same section. Here are a few examples:\n- Duplicate \"achieves\" in L21.\n- Redefinition of Conditional Sequence Modeling (CSM) in the Introduction.\n- L188: \"Eq. equation.\"\n- L287: CSM -> Return-Conditioned Behavioral Cloning.\n- L94: Decision Conformer -> Decision ConvFormer\n- In lines 288-289, the baseline Q-learning Decision Transformer (QDT) is mentioned, but in Tables 1 and 2, the Online Decision Transformer (ODT) is reported.\n- What are the RvS and DS4 baselines in Table 1 and 2?\n\n2. Missing related work section, which makes the proposed method poorly situated in the literature.\n3. The proposed modification is somewhat incremental; the novelty over prior work [1][2] is not well described. \n4. Lack of analysis on key hyperparameters: $w$ used to control the decay rate. How is it effect to the ability of balancing long-term and short-term dependencies?\n5. Section 2.4 is missing.\n6. Experiments were conducted with 3 seeds, while prior work typically uses 5 seeds (e.g., Decision ConvFormer). This makes the results less reliable.\n7. Lack of discussion on the improvement of GDT over previous works such as Decision ConvFormer (DC) [1] and Long Short Decision Transformer (LSDT) [2].\n\n[1] Kim, Jeonghye et al. \"Decision convformer: Local filtering in metaformer is sufficient for decision making.\" ICLR 2024.\\\n[2] Wang, Jincheng et al. \"Long-short decision transformer: Bridging global and local dependencies for generalized decision-making.\" ICLR 2025."}, "questions": {"value": "1. What is the effect of key hyperparameters on GDT's performance?\n2. In the AntMaze navigation domain, how does GDT compare to DC and LSDT in antmaze-mediumplay and antmaze-medium-diverse?\n3. What is the computational cost of GDT compared to DC and LSDT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IjcOlWrWJN", "forum": "zV1hR6wWFK", "replyto": "zV1hR6wWFK", "signatures": ["ICLR.cc/2026/Conference/Submission22545/Reviewer_6i1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22545/Reviewer_6i1N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826265742, "cdate": 1761826265742, "tmdate": 1762942270041, "mdate": 1762942270041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Gaussian-biased Decision Transformer, which incorporates a Gaussian locality bias into the self-attention mechanism. The Gaussian bias adjusts attention logits based on temporal distance, emphasizing nearby timesteps while retaining the ability to capture long-range dependencies. Experiments on Mujoco and AntMaze benchmarks show improvements over prior Decision Transformer variants and value-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a simple, generalizable Gaussian bias mechanism\n- Antmaze experiments show meaningful improvements over DT baselines\n- The paper is clearly presented"}, "weaknesses": {"value": "- The Mujoco experiments in Table 1 show that the performance is only slightly better on average and in many environments many methods are bolded which seems to indicate the improvement is not significant\n- The contribution is incremental since local attention bias is not novel"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KGQsp5ijd6", "forum": "zV1hR6wWFK", "replyto": "zV1hR6wWFK", "signatures": ["ICLR.cc/2026/Conference/Submission22545/Reviewer_sSTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22545/Reviewer_sSTV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870881437, "cdate": 1761870881437, "tmdate": 1762942269317, "mdate": 1762942269317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}