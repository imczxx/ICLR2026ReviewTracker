{"id": "Kq9MMEynGW", "number": 17050, "cdate": 1758271602965, "mdate": 1759897201929, "content": {"title": "Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training", "abstract": "Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in music domain.", "tldr": "Pianist Transformer utilizes large-scale self-supervised pre-training on unlabeled MIDI data to achieve human-level expressive piano performance, overcoming the scaling limitations of previous supervised methods.", "keywords": ["Self-Supervised Learning", "Music Performance Rendering", "Model/Data Scaling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/956d4d951aa6005f8775f606efd4b22d1102b8b0.pdf", "supplementary_material": "/attachment/a29c5167104c3c82d3f30322557a44dbdb2186e6.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of expressive piano performance rendering, which aims to generate a piano performance with expressive features (note velocity, dynamic timing, and pedal) given the score-only information. The proposed model features an encoder-decoder Transformer architecture and is trained across two-stage. It is first pre-trained on large-scale, unaligned performance-only data based on a mask language modelling objective. It is then fine-tuned on paired score-performance data in a seq2seq fashion. This strategy addresses the problem of scarce paired data, which has long been found in this field, and ablation study demonstrates significant contribution by pre-training. The resulting model is named “Pianist Transformer,” which also demonstrates superior performance compared to existing baselines through objective and subjective evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-written paper, with problem and goal clearly depicted and methodology well illustrated. A few strengths of the paper can be summarized as follows:\n\n* **Unified Representation**: To make use of unaligned but more abundant data, the design of unified score/performance representation is well motivated in the paper. In both cases, note durations are represented as relative (inter onset) temporal units, which facilitate pre-training with performance-only data. \n\n* **Comprehensive Treatment**: The paper provides a thorough and complete treatment of the expressive performance rendering task. It not only describes the model design and training pipeline in detail, but also includes a practical post-processing step to ensure compatibility with standard DAW workflows. Technical details are well documented throughout.\n\n* **Demonstration Quality**: The attached synthesized demos sound convincing and musically natural, effectively supporting the evaluation results."}, "weaknesses": {"value": "Despite its merits, the reviewer would like to raise several points of concern, the clarification or improvement of which could further strengthen the paper.\n\n* **Baseline Comparisons**: While the paper includes two baseline models, both are relatively outdated. Incorporating more recent systems, such as (Borovik & Viro, 2023), would make the comparison more convincing and strengthen the evidence for the claimed performance improvements.\n\n* **Limited Case Study Scale**: In Sections 4.4.2 and 4.4.3, the subjective evaluation highlights individual case studies across different music styles. However, these analyses appear to be based on only one or two pieces per style (Baroque, Classical, Romantic), which may introduce sampling bias. A more robust analysis could involve evaluating a larger number of pieces per style (maybe with objective metrics to make it more practical) to ensure generalizability."}, "questions": {"value": "* **Metric Computation (Table 1)**: To the reviewer’s understanding, each metric (JS Div. and Inter.) is computed by comparing the model’s outputs against human performances. Are these metrics calculated per piece and then averaged, or are they derived from the aggregated distributions of all test pieces? Additionally, how are the “Human” results obtained in the table? (by comparing different human performances or through self-comparison?)\n\n* **Subjective Evaluation (Figure 3)**: Does P4 correspond to the Liszt piece included in the supplementary demos? Figure 3 shows that the model’s rating on P4 is fairly higher than the human performance, whereas in the Liszt demo, the reviewer finds the human rendition sufficiently better.\n\n* **Pedal Representation (Line 673)**: The explanation of the pedal encoding is somewhat unclear. Why are there four pedal tokens? And how are the 128 pedal values distributed among the four tokens?\n\n* **Sequence Length**: How long is the input/output token sequence during inference? Is the model capable of generating longer sequences beyond this limit, for example by autoregressive continuation or segment stitching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HoNrdkGR6l", "forum": "Kq9MMEynGW", "replyto": "Kq9MMEynGW", "signatures": ["ICLR.cc/2026/Conference/Submission17050/Reviewer_UJkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17050/Reviewer_UJkA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489374744, "cdate": 1761489374744, "tmdate": 1762927064549, "mdate": 1762927064549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Pianist Transformer, a model for expressive piano performance rendering. The authors propose (1) leveraging unaligned performance MIDI for pretraining, (2) introducing a unified representation for both score and performance MIDI, and (3) designing an efficient asymmetric Transformer architecture. Both objective metrics and subjective listening evaluations demonstrate the benefits of the proposed pretraining stage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The incorporation of self-supervised learning on unpaired performance MIDI data is meaningful and could be extended to other symbolic music tasks beyond piano rendering.\n2. The experimental section is thorough, particularly the design and analysis of the subjective evaluation."}, "weaknesses": {"value": "1. Two of the major claimed contributions overlap with prior work. Existing tokenization schemes such as REMI [1], MIDI-Like [2], CPWord [3], and Octuple [4] already represent pitch, duration, and velocity as discrete tokens. Moreover, the proposed note-level compression in the encoder resembles CPWord and Octuple designs, where each note is represented by a compression of fixed number of tokens (e.g., MusicBERT [4]). The authors should clarify how their unified representation provides new capabilities beyond these established methods.\n2. There is insufficient discussion on the efficiency-performance trade-off. In Section 4.5, the symmetric 6-6 architecture achieves lower pretraining validation loss than the proposed asymmetric 10-2 model, suggesting that efficiency gains may come at the cost of performance. However, there is no detailed comparison of their rendering performance (objective or subjective) or inference efficiency. Considering that the model is relatively small (~0.1B parameters), it is unclear whether efficiency should be a primary concern, and whether potential performance degradation, if any, is justified by the speedup.\n3. As one of the core components of the proposed framework, the pretraining setup lacks sufficient detail regarding the masking strategy; specifically, the mask rate, the masking pattern (e.g., random tokens vs. consecutive notes), and how these choices influence learning. An ablation study examining the impact of different mask rates on overall performance would strengthen the paper.\n\nReferences\n\n[1] Huang and Yang, “Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions,” ACM MM, 2020.\n\n[2] Oore et al., “This Time with Feeling: Learning Expressive Musical Performance,” Neural Computing and Applications, 2020.\n\n[3] Hsiao et al., “Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs,” AAAI, 2021.\n\n[4] Zeng et al., “MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training,” ACL-IJCNLP Findings, 2021."}, "questions": {"value": "Could the authors please addressing the following:\n\n1. What are the fundamental differences between the proposed unified MIDI representation and prior tokenizations (e.g., REMI, Octuple, CPWord)?\n2. The PDMX dataset contains many multi-instrument scores rather than solo piano parts. How are these multi-track scores handled during pretraining?\n3. During pretraining, data from both score-based (e.g., PDMX) and performance-based (e.g., POP909) MIDI are used. Are they explicitly distinguished in the model input? If not, how does the model learn to differentiate the reconstruction objective during pretraining from the rendering objective during inference when a score MIDI is provided?\n4. In Figure 3 (b), why does VirtuosoNet-ISGN perform worse than the “Score” baseline in most of the cases, despite being a strong model in previous literature?\n5. In Algorithm 1, the method requires aligned note pairs from score and performance sequences. How is this alignment established, especially under one-to-many senarios such as ornamentations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B0XBdwhhdN", "forum": "Kq9MMEynGW", "replyto": "Kq9MMEynGW", "signatures": ["ICLR.cc/2026/Conference/Submission17050/Reviewer_D9R1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17050/Reviewer_D9R1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818596141, "cdate": 1761818596141, "tmdate": 1762927064008, "mdate": 1762927064008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel piano performance rendering model together with a training strategy and a unified tokenization that handles both score and performance MIDI. Using this tokenization, the model is first pretrained with a masked language modeling objective that predicts masked attributes from either score or performance MIDI. It is then supervisedly fine-tuned to render performance from score. The experiments report superior performance over baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The tokenization and the overall pipeline are very well-designed and make sense in how they help with performance rendering tasks.\n2. The experiment design is clear and evaluates the model from multiple perspectives.\n3. The provided listening samples are very convincing, showing massive improvement compared to existing methods."}, "weaknesses": {"value": "1. Opening with an experiment figure feels a bit off and does little to support the narrative. The results in Figure 1, especially the variant without pretraining, should be detailed in the experiments section with fuller analysis.\n2. The paper could more intuitively explain token representations and model I/O at each stage. Small, concrete examples would improve readability.\n3. The paper’s relevance to the ICLR community is under-articulated; as written, it reads more naturally for a computer-music venue. Section 4.5’s discussion feels loosely connected to the core contributions. A more urgent question to ask is perhaps: by doing pre-training, which performance criteria benefit, compared to the one without pre-training. Also, would large-scale pretraining under the SSL objective be superior to supervised learning on large-scale transcribed/quantized audio?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6glhIfZDhA", "forum": "Kq9MMEynGW", "replyto": "Kq9MMEynGW", "signatures": ["ICLR.cc/2026/Conference/Submission17050/Reviewer_HbNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17050/Reviewer_HbNE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962911716, "cdate": 1761962911716, "tmdate": 1762927063444, "mdate": 1762927063444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}