{"id": "NgB3zf5uK1", "number": 14240, "cdate": 1758230984743, "mdate": 1759897381836, "content": {"title": "LOST: Low-rank and Sparse Pre-training for Large Language Models", "abstract": "While large language models (LLMs) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, sparsity is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and sparse components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose $\\textbf{LO}$w-rank and $\\textbf{S}$parse pre-$\\textbf{T}$raining ($\\textbf{LOST}$) for LLMs, a novel method that ingeniously integrates low-rank and sparse structures to enable effective training of LLMs from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise sparse components to complement the expressiveness of low-rank training.  We evaluate LOST on LLM pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Code will be made available.", "tldr": "A novel method for combining Low-rank module and Sparse module for Training LLMs.", "keywords": ["Large language models", "Low-rank", "Sparse", "Singular value decomposition", "Pre-training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8cc571601e64badd90da744946ac3e11ac09d971.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LOST (Low-rank and Sparse Training), a method for efficient large language model pre-training that combines low-rank factorization and structured channel-wise sparsity within each linear layer. The core idea is to perform an SVD of each weight matrix at initialization, use the top-r singular components for a low-rank representation, and utilize the residual singular subspace to select a small subset of channels forming a sparse complementary matrix. Experiments on LLaMA-style models ranging from 60M to 7B parameters demonstrate that LOST achieves comparable or better perplexity than full-rank and recent low-rank baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated decomposition: The proposed combination of low-rank and structured sparse components leverages complementary subspaces derived from SVD, offering a principled way to retain model expressivity while saving memory.\n2. Extensive ablation studies: The paper carefully analyzes the effects of rank, sparsity, combination coefficient γ, nonlinearity placement, and initialization schemes, supporting the design choices and validating the method’s robustness under different configurations.\n3. Practical efficiency gains: LOST achieves notable reductions in memory and computational costs with consistent or improved perplexity across several model sizes. The structured sparsity design is hardware-friendly compared to unstructured sparsity."}, "weaknesses": {"value": "1. Using only SVD for initialization without modifying the training process raises questions about the effectiveness of training at larger scales and for longer durations.\n2. The use of activation functions makes it difficult to merge parameter matrices in a manner similar to LoRA, which limits its application scenarios. Additionally, this method introduces more hyper parameters.\n3. The differences shown in ablation for different Wl and Ws strategies are not significant, casting doubt on the necessity of SVD and sparse approaches.\n4. Some comparisons rely on results from prior work, making it unclear whether all baselines share identical training data, training budgets, hyper parameters, and seeds.\n5. The main text focuses exclusively on pretraining perplexity; downstream fine-tuning results are only mentioned in the appendix without quantitative comparisons, limiting understanding of real-world benefits.\n6. The citation format contains widespread errors, with improper use of \\citet and \\citep commands."}, "questions": {"value": "1. In channel-wise sparse weight part, what is the difference between column sparsity and row sparsity, and has this distinction been experimentally compared?\n2. After using the activation function, why can we still do weight avg? (line 402)\n3. Can the sparse part also be represented as a special low-rank form of AB, for example, where A is learnable and B is a sparse matrix containing only 0s and 1s to place the results in specific channels.\n4. Why is the channel selected in the original matrix rather than in the W_comp in the method?\n5. In the LOST implementation of Attention, are the QKV projection matrices processed separately, and are the heads also processed separately?\n6. Have you tried the method on the MoE architecture?\n7. Why not further apply SVD and sparse to adjust weights during training (similar to GaLore), but only use them during initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IEa1Z0hg49", "forum": "NgB3zf5uK1", "replyto": "NgB3zf5uK1", "signatures": ["ICLR.cc/2026/Conference/Submission14240/Reviewer_ZVHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14240/Reviewer_ZVHK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491982177, "cdate": 1761491982177, "tmdate": 1762924693891, "mdate": 1762924693891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method called LOST, which aims to address the massive computational and memory overhead challenges encountered when pre-training LLMs from scratch. The authors cleverly combine low-rank and sparse structures, with its core innovation lying in \"co-design\" via SVD: the largest singular values are used to construct the low-rank component for capturing key information, while the remaining singular values are leveraged to build a channel-wise sparse component to compensate for the information lost in low-rank approximation. Experimental results demonstrate that across multiple model scales, LOST can significantly reduce resource consumption while achieving performance that equals or even surpasses that of full-rank models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to read.\n2. Significant efficiency and performance advantages. The core results (Table 1, Figure 1) show that while drastically reducing memory footprint, LOST achieves lower Perplexity than full-rank models across most model scales, demonstrating the notable superiority of the proposed method.\n3. The experiments cover a variety of model sizes ranging from 60M to 7B parameters, verifying the method’s universality and scalability. Comparisons are conducted with Full-Rank, LoRA, and a number of state-of-the-art low-rank pre-training methods (GaLore, LORO, CoLA, SLTrain), and the results show that LOST outperforms many baseline methods.\n4. The authors have conducted in-depth ablation studies on nearly every design detail of the model."}, "weaknesses": {"value": "1. There are only experimental results of pre-training on LLMs, with a lack of fine-tuning results on downstream tasks (e.g., GLUE). The fine-tuning hyperparameters for the GLUE dataset are provided in Table 13; however, the actual experimental results of the GLUE fine-tuning task are not presented in the main text.\n2. The authors only conducted experiments on models from the Llama family, and there is a lack of experimental results on LLMs with other architectures. It remains unclear whether LOST is also effective for other models.\n3. In Table 1, the PPL of LOST is significantly superior to that of the full-rank model on the small 60M-parameter model, but only slightly better on the 1B-parameter model. As the model scale increases, the advantage of LOST over the Full-Rank model becomes smaller."}, "questions": {"value": "1. How is the \"actual memory footprint\" in Table 2 measured? Can the authors provide more detailed specifics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VR0sDQBpFM", "forum": "NgB3zf5uK1", "replyto": "NgB3zf5uK1", "signatures": ["ICLR.cc/2026/Conference/Submission14240/Reviewer_h6tr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14240/Reviewer_h6tr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834277269, "cdate": 1761834277269, "tmdate": 1762924693476, "mdate": 1762924693476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LOST (LOw-rank and Sparse pre-Training), a new method to efficiently pre-train large language models from scratch. Its key idea is to \"co-design\" low-rank and sparse components so they are complementary. It initializes by performing a Singular Value Decomposition (SVD) on the full-rank weight matrices. The dominant singular values are used to create the main low-rank component. The remaining, smaller singular values are then used to construct a channel-wise sparse component that captures information lost in the low-rank truncation.\n\nThe method was evaluated on pre-training LLaMA models from 60M to 7B parameters. Experiments show that LOST achieves perplexity that is competitive with or even superior to full-rank models, while significantly reducing memory and computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled \"Co-Design\" Methodology: Instead of naively combining low-rank and sparse matrices, it uses Singular Value Decomposition (SVD) to purposefully create the sparse component from the residual singular values. This \"co-design\" ensures the two components are complementary from the start.\n2. Strong Empirical Performance: LOST demonstrates state-of-the-art results. It achieves perplexity scores that are competitive with, or even superior to (very suspicious they use different batch size and train steps in Table 10.) , standard full-rank pre-training on models up to 1B parameters. Furthermore, it clearly outperforms direct competitors in efficient pre-training, such as SLTrain, LORO, and GaLore, in like-for-like comparisons."}, "weaknesses": {"value": "(Please respond to the questions section)\n\n1. Limited Novelty Compared to SLTrain\n2. Weak Justification for Sparse Component Design\n3. Inconsistent and Unfair Experimental Comparisons"}, "questions": {"value": "This paper's core claims are undermined by concerns regarding limited novelty, weak justification for its methodology, and several inconsistencies in the experimental results.\n1. The paper's novelty appears incremental. The proposed framework, which combines a low-rank component with a sparse matrix defined by a predefined, static mask(column-wise), is functionally identical to the SLTrain method. The only significant difference is the initialization strategy for this sparse mask (using SVD residuals) and the low-rank factors. This raises the question of whether LOST is a new training method or, more accurately, a new initialization technique for an existing one.\n2. The paper's \"co-design\" hinges on creating the sparse component from the L2-norm of the columns of the residual matrix $W_{comp}$. First, the paper provides no strong theoretical or empirical justification for why this specific choice (column-wise L2-norm) is optimal over, for example, a row-wise selection or an L1-norm; Second, the ablation study in Table 3, which attempts to justify this, shows results that are not significant. For the 60M model, the perplexity for the proposed $SVD_{l2}^{rem}$ is 32.25, while $SVD_{l1}^{rem}$ is 32.33. A difference of 0.08 PPL is well within the standard deviation of such experiments and fails to convincingly argue for the superiority of this specific design choice.\n3. Several experimental results appear to be \"not normal\" and suffer from confounding variables or internal contradictions, making the paper's conclusions difficult to trust. 1. Unfair 7B Model Comparison: In Table 10, the 7B LLaMA experiment compares LOST (trained with a batch size of 8) to a Full-Rank Adam baseline (trained with a batch size of 4). Batch size is a critical hyperparameter, and this discrepancy makes the comparison unfair and invalid. The Full-Rank model is severely disadvantaged, and no reliable conclusion can be drawn; 2. Contradictory $\\gamma$ Ablation: This is the most serious concern. The $\\gamma$ parameter balances the low-rank and sparse components. As $\\gamma$ approaches 1, the sparse component's contribution $(1-\\gamma) \\cdot x_{[:,\\mathcal{I}]}W_{s}^{T}$ should vanish, and LOST's performance should converge to that of a LoRA-style model (with SVD initialization). But the results contradict this. For the 130M model, LOST with $\\gamma=0.9$ (mostly low-rank) achieves a perplexity of 24.51 (Table 9), however, the baseline LoRA model in Table 1 reports a perplexity of 33.92. This is a massive discrepancy. It strongly suggests that the \"LoRA\" baseline in Table 1 was trained with a standard (e.g., Kaiming) initialization, while the \"LOST\" model in Table 9 benefits from SVD initialization. This implies that the entire performance gain may be coming from the SVD initialization alone, not the novel sparse+low-rank co-design. This experiment fails to isolate the variable it claims to be testing.\n\nIn conclusion, I would not recommend accepting this work given the current quality and concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NbR4OdhMJj", "forum": "NgB3zf5uK1", "replyto": "NgB3zf5uK1", "signatures": ["ICLR.cc/2026/Conference/Submission14240/Reviewer_xG1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14240/Reviewer_xG1E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879822246, "cdate": 1761879822246, "tmdate": 1762924693162, "mdate": 1762924693162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a new pre-training strategy that takes low-rank and channel-wise sparse matrices as the model weights. A low-rank plus sparse matrix is a typical solution for approximating complicated matrices, such as the parameter matrix in a neural network. The method aims to retain full-rank expressivity while reducing parameters and memory. Experiments from 60M to 7B models on a classical pre-training corpus C4 show that the proposed methods outperform baselines with notable memory savings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Many Previous works have shown empirical results that large models exhibit substantial parameter redundancy. The success of LoRA also demonstrates that tuning the low-rank part of the weight matrix can effectively learn new things. Extending this idea to the pre-training is promising and could have a broad impact on the large model community."}, "weaknesses": {"value": "1. My major concern is the lack of a scaling law. When comparing architectures/optimization paradigms, scaling laws are a crucial and widely used tool. Different methods and settings often require different optimal hyperparameters, and relative rankings can flip as scale changes (e.g., attention variants such as MLA, GQA, linear attention exhibit size-dependent crossovers). Although this work reports results of multiple model sizes, the comparisons could still be confounded by hyperparameter suboptimality at each scale.\n2. It's better to analyze why the proposed method can beat the full-rank baseline in perplexity. In principle, a constrained parameterization should not outperform a full-rank baseline. I doubt the reason is that the constrained model regularizes better under non-optimal hyperparameters, early stopping, or limited training data. If the proposed method consistently outperforms the full-rank baseline, I think the author should provide more insights. \n3. The amount of pre-training tokens seems to be small relative to standard LLM pre-training. I do not mean training a model at trillions of tokens like the popular open-sourced model, but I doubt the loss curve has entered a plateau or a very slow region, making the observation biased. Conclusions based on limited training data may not extrapolate. Constrained models sometimes look strong in the undertrained regime but fall behind when trained to convergence. Looking at the scaling law at the step level, such as the L(S) estimation, could be a good way to analyze the proposed method.\n4. A minor issue: C4 is somehow out-of-date, conducting experiments with nemontron-cc or fineweb-edu could be a better choice."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3jOV7tIhx0", "forum": "NgB3zf5uK1", "replyto": "NgB3zf5uK1", "signatures": ["ICLR.cc/2026/Conference/Submission14240/Reviewer_a9CA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14240/Reviewer_a9CA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977784640, "cdate": 1761977784640, "tmdate": 1762924692726, "mdate": 1762924692726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}