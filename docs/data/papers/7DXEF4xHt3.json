{"id": "7DXEF4xHt3", "number": 9885, "cdate": 1758146389065, "mdate": 1759897689496, "content": {"title": "Adversarial Training for Process Reward Models", "abstract": "Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. \nHowever, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. \nWe introduce Adversarially Trained PRMs ($\\texttt{APRM}$), where\na Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them.\nThis interaction yields progressively harder negatives for $R$, improving it's robustness and generalization to novel errors without requiring manual step-level labels.\nAveraged across diverse mathematical reasoning benchmarks, $\\texttt{APRM}$ improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. $\\texttt{APRM}$ achieves gains of $+5.3$ pp on out-of-distribution tasks.", "tldr": "", "keywords": ["Process Reward Models", "LLM reasoning", "Adversarial Training"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca52c377ad854f29e663bb673f05e189c67ab162.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Adversarially Trained Process Reward Models (APRM). The core idea involves a two-player, general-sum game where a Generator learns to generate reasoning errors to deceive a PRM, which concurrently learns to detect them. Experimental results show that APRM improves solver accuracy on mathematical reasoning benchmarks compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper investigates an important and timely topic.\n2. The method is supported by theoretical analysis.\n3. Experimental results demonstrate the superior performance of the proposed method."}, "weaknesses": {"value": "1. The paper fails to cite and compare with [1], which also explores adversarial training for reward models. This omission makes it difficult to assess the precise novelty of this work relative to existing approaches.\n\n2. The evaluation is limited to using the PRM for inference-time supervision. It remains unclear whether the APRM can perform effectively as a reward signal in an RL post-training setup, which is a critical and common application for reward models.\n\nI am open to raising my score if these points are addressed.\n\n[1] Adversarial Training of Reward Models (COLM 2025)"}, "questions": {"value": "Please refer to the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5H3PuIf8e6", "forum": "7DXEF4xHt3", "replyto": "7DXEF4xHt3", "signatures": ["ICLR.cc/2026/Conference/Submission9885/Reviewer_jiVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9885/Reviewer_jiVq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580527166, "cdate": 1761580527166, "tmdate": 1762921351438, "mdate": 1762921351438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adversarially Trained Process Reward Models (APRM), a novel framework for training Process Reward Models (PRMs) that provide step-level supervision for LLM reasoning. The key innovation is formulating PRM training as a two-player, general-sum game between a generator and a reward model. Empirically, APRM outperforms strong baselines across multiple mathematical and scientific reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a clear limitation in existing PRM training (static data, poor generalization) and proposes an elegant, game-theoretic solution. The adversarial setup for generating an adaptive curriculum of hard negatives is a conceptually compelling idea.\n\n2. The method is evaluated thoroughly across diverse benchmarks and model families. The results are consistently strong, demonstrating not just better accuracy but also properties like robustness, scalability, and cross-domain generalization."}, "weaknesses": {"value": "1. The adversarial training process, involving two models trained with a combination of PPO and OGDA, is inherently more computationally expensive and complex than normal PRM training.\n\n2. The step level oracles introduced in the paper are sophisticated but imperfect automated validators that enables scalable training without human annotation, but also introduces dependence on its rule completeness and accuracy.\n\n3. A significant gap is the lack of comparison with a baseline that directly performs RL on the solver using the same automated oracle for improved reasoning. This makes it difficult to isolate the contribution of the complex adversarial game from the mere availability of automated step-level supervision using PPO. Also see Q1."}, "questions": {"value": "The proposed method already contains PPO on the solver model. Why not use direct RL to improve LLM reasoning using the step level rule based reward and the final correctness reward? Does it introduce reward hacking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bKUyRCEe2y", "forum": "7DXEF4xHt3", "replyto": "7DXEF4xHt3", "signatures": ["ICLR.cc/2026/Conference/Submission9885/Reviewer_nTz3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9885/Reviewer_nTz3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996759774, "cdate": 1761996759774, "tmdate": 1762921351190, "mdate": 1762921351190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adversarially Trained Process Reward Models (APRM), a framework that treats PRM training as a two-player general-sum game between (i) a generator that tries to produce plausible but wrong reasoning steps, and (ii) a process reward model that tries to detect those subtle errors. By letting the generator constantly generate harder negatives, the PRM is exposed to an adaptive curriculum rather than a fixed, static dataset, which the paper argues is the core weakness of current PRM training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The general-sum + Nash existence via Glicksberg is reasonable for the PRM and generator adversarial setup case.\n2. The inclusion of OGDA and regularization terms is justified with references to convergence theory.\n3. The evaluations cover multiple domains, datasets, and solvers, with consistent gains across both in-domain and out-of-distribution tasks."}, "weaknesses": {"value": "1. When summarizing synthetic data generation techniques, the authors miss works, MM-PRM, FG-PRM, and FreePRM.\n2. Maintaining two RL-trained LLMs (generator + PRM) is computationally expensive and less accessible than data-driven PRMs.\n3. APRM still struggles with certain logic or precondition-based reasoning failures that are difficult to model via token-level perturbations."}, "questions": {"value": "1. What mechanisms ensure that the generator explores fundamentally different reasoning errors rather than minor textual perturbations?\n2. Would a simpler hard-negative mining approach from failed solver traces achieve similar benefits without full adversarial RL? In other words, how much of the +3.4 pp is from adversariality instead of fresh and more complex data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X10IPOgtZw", "forum": "7DXEF4xHt3", "replyto": "7DXEF4xHt3", "signatures": ["ICLR.cc/2026/Conference/Submission9885/Reviewer_Y618"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9885/Reviewer_Y618"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065352567, "cdate": 1762065352567, "tmdate": 1762921350846, "mdate": 1762921350846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}