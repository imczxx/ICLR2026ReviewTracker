{"id": "zF0Cerwj5R", "number": 11027, "cdate": 1758187251211, "mdate": 1763658485934, "content": {"title": "LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have achieved significant advances in integrating visual and linguistic information, yet their ability to reason about complex and real-world scenarios remains limited.\nExisting benchmarks are usually constructed in a task-oriented manner, without a guarantee that different task samples come from the same data distribution. Therefore, they often fall short in evaluating the synergistic effects of lower-level perceptual capabilities on higher-order reasoning. To lift this limitation, we contribute Lens, a multi-level evaluation benchmark of multimodal reasoning with with 3.4K contemporary images and 60K+ human-authored questions covering eight tasks and 12 daily scenarios, forming three progressive task tiers, i.e., perception, understanding, and reasoning. One feature is that each image is equipped with rich annotations for all tasks. Thus, this data set intrinsically supports evaluating MLLMs to handle image-invariable prompts, from basic perception to compositional reasoning. In addition, our images have been   collected manually from social media, with $53$% published after Jan. 2025. We evaluate 15+ frontier MLLMs such as  Qwen2.5-VL,  InternVL3,  GPT-4o  and two reasoning models  QVQ-Max and Kimi-VL. Most models were released in 2025, and none of them achieve an accuracy beyond $60$% in the reasoning tasks. Furthermore, we propose the Self-Driven Multi-Expert Collaborative Framework (SMEC), a framework designed for MLLMs that simulates a panel of experts discussing and exchanging viewpoints via self-generated role-specific prompts. The experimental results confirm the existence of synergistic effects in a hierarchical task structure, where low-level tasks facilitate the reasoning of MLLMs on more complex, high-level tasks. Statistical analysis and ablation studies further demonstrate the comprehensiveness of our dataset and the superiority of our methodology.", "tldr": "", "keywords": ["MLLMs", "Multimodal reasoning", "Synergistic effects"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ca563cc2e04d280c253dc1a0fdc0aedc4f8ecaa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Lens, a benchmark for evaluating MLLMs on multi-modal reasoning and SMEC, a framework for MLLMs to solve the tasks with self-generated expert opinions. The Lens consists of eight tasks under three main categories: perception, understanding and reasoning. Images (3.4K) of the benchmark are collected manually from variety social media sources. The results of the experiments on Lens show that low-level visual tasks can affect the higher-order reasoning process. When the models apply SMEC for Scene Knowledge Inference task, the accuracy of the models increases with more iterations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "•\tThe data is manually filtered and annotated by human verifiers.\n\n•\tThe process of SMEC is explained very detailly.  \n\n•\tSynergistic effects evaluation provides detailed examination of cross-tasks performance."}, "weaknesses": {"value": "•\tThe overall writing quality of the paper could be improved. Some sections lack clarity, and certain sentences are difficult to understand.\n\n•\tWhile the paper highlights multi-level evaluation and integrated tasks as key contributions, these concepts are not sufficiently explained or exemplified in the paper.\n\n•\tThe paper does not clearly justify the need for introducing a new benchmark. Several existing benchmarks already include some of the tasks used in the paper. It remains unclear why extending existing benchmarks with additional tasks would not be sufficient to analyze cross-task interactions. \n\n•\tThe authors did not explain why the community needs a new benchmark. Other benchmarks have already included some tasks. By adding other task-related questions to the existing benchmarks and data, the same effect can be achieved. In that case, the evaluation of the synergistic effect does not require generating a new benchmark. \n\n•\tIt remains unclear how the proposed SMEC framework distinguishes itself from prior work in terms of novelty and contribution. The related work section does not address prior efforts involving expert-based or modular task-solving frameworks. \n\n•\tThe experimental validation of SMEC appears limited, as its performance is only evaluated on a single task (Scene Knowledge Inference). To validate its effectiveness, it would be valuable to test SMEC across other tasks and other datasets."}, "questions": {"value": "•\tIn which task is the interleaved image-text feature used? Figure 7 does not show the example. \n\n•\tThe second link on foot page opens Instagram webpage, not developer agreement. The reviewer didn’t understand the relation of the link and text in the paper. \n\n•\tDo the authors get legal permissions from social media platforms X, Instagram, Weibo and RedNote to use their data? Although some processes, like erasing facial information, have been done for privacy concerns, the images are still copyrighted. For example, the copyright regulations of RedNote state that “RedNote's trademarks, logos, and content are protected by intellectual property laws. You may not use our intellectual property without explicit permission.” According to these cases the authors should get permission from the platforms to use their data for research purposes. If permission has been granted for data use, including documentation of this in the Appendix, is recommended. \nRedNote: https://red-note.co/terms-of-service \n\n•\tIt would be valuable to report human performance on the Lens benchmark and the performance comparison between humans and models. \n\n•\tThe Related Work section does not clearly articulate how Lens differs from existing benchmarks, particularly in terms of visual and reasoning capabilities. It would be helpful to clarify what unique contributions or challenges Lens introduces that are not addressed by prior datasets.\n\nSuggestions:\n\n•\tThe organization of the paper could be improved. For instance, placing the 'Related Work' section in the Appendix makes it harder to assess the motivation and novelty, whereas some less critical elements like Figure 2 are included in the main text"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors collected images from social media platforms, X, Instagram, Weibo, and RedNote. Although they stated in the paper that \"During the collection process, we strictly complied with the copyright and licensing regulations of each platform, ensuring that data was collected only from publicly accessible posts and that no images were downloaded from sources explicitly prohibiting data reuse or redistribution.\", they only shared X's policy. The copyright regulations of RedNote state that “RedNote's trademarks, logos, and content are protected by intellectual property laws. You may not use our intellectual property without explicit permission.” According to this case, the authors might need to get legal permission from the platform to use their data for research purposes.\n\nRedNote: https://red-note.co/terms-of-service"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fAvk4dbc4m", "forum": "zF0Cerwj5R", "replyto": "zF0Cerwj5R", "signatures": ["ICLR.cc/2026/Conference/Submission11027/Reviewer_4Zew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11027/Reviewer_4Zew"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760648078354, "cdate": 1760648078354, "tmdate": 1762922207696, "mdate": 1762922207696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Lens,\" a new multi-level benchmark for evaluating multimodal large language models (MLLMs). The authors argue that existing benchmarks fail to assess how foundational perceptual skills (like object detection) contribute to higher-order reasoning. Lens addresses this by providing 3.4K contemporary images, each annotated for a hierarchy of tasks progressing from perception to understanding and reasoning. The paper evaluates over 15 recent MLLMs and finds that even top models struggle with the reasoning tasks, scoring below 60%. To address this, the authors also propose SMEC, a framework where an MLLM uses self-generated prompts to simulate a \"panel of experts\" to collaborate on complex reasoning, showing improved performance. A key finding is the confirmation of synergistic effects, where improving low-level perceptual tasks directly facilitates better performance on high-level reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Hierarchical Benchmark Design。 The \"Lens\" benchmark introduces a valuable multi-level structure (Perception, Understanding, Reasoning) where all tasks are annotated on the same set of images. This is a strength as it uniquely enables the evaluation of synergistic effects。\n\n2. Contemporary and Relevant Dataset. The dataset is built from 3.4K contemporary images manually collected from social media, with a large portion (53%) published after January 2025. This freshness is crucial for fairly evaluating modern MLLMs and mitigating the risk of data contamination from older, widely used training sets.\n\n3. Constructive Contribution with SMEC. Beyond just identifying a problem, the paper proposes a solution with the \"Self-Driven Multi-Expert Collaborative Framework\" (SMEC). This framework offers an innovative, tool-free method for enhancing MLLM reasoning by simulating a panel of experts. The positive results from SMEC add significant value to the paper."}, "weaknesses": {"value": "1. Limited to Static Images. As acknowledged by the authors, the benchmark is confined to static images. This scope does not capture the complexities of real-world multimodal reasoning, which often involves video, temporal sequences, audio, or long-form narrative understanding.\n\n2. Potential Impracticality of SMEC. The SMEC framework relies on an iterative, multi-step process of generating expert prompts and synthesizing answers. This implies a significant increase in computational overhead and latency at inference time, which could make the approach impractical for real-time applications. The paper demonstrates effectiveness but does not deeply analyze this efficiency trade-off.  For example, if using several small models, how about just using a larger powerful model? Under the same compute, which method is better?"}, "questions": {"value": "There is a formatting problem of the citations in the paper. To cite a paper, \\citep should be used instead of \\cite."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hraUXFarpQ", "forum": "zF0Cerwj5R", "replyto": "zF0Cerwj5R", "signatures": ["ICLR.cc/2026/Conference/Submission11027/Reviewer_d4LY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11027/Reviewer_d4LY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862796512, "cdate": 1761862796512, "tmdate": 1762922205336, "mdate": 1762922205336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Lens, a large-scale, multi-level benchmark for evaluating multimodal reasoning in large vision-language models (MLLMs). \n\nUnlike prior task-specific datasets, Lens uses a unified image set - 3.4K high-resolution, non-commercially licensed social media image - paired with over 60K human-authored questions across 12 real-world scenarios. Each image supports eight tasks organized into three progressive tiers: perception, understanding, and reasoning, enabling a structured evaluation of how lower-level perceptual skills contribute to higher-level reasoning, while minimizing the effects of data distribution across task categories.\n\nComprehensive experiments on 15+ state-of-the-art MLLMs, including GPT-4o, Qwen2.5-VL, InternVL3, and reasoning MLLMs like QVQ-Max, and Kimi-VL, reveal strong interdependencies between low-level tasks, like perception and high level tasks like reasoning.\n\nLastly, the paper also introduces Self-Driven Multi-Expert Collaboration (SMEC), a novel framework in which an MLLM simulates a panel of specialized agents via self-generated, role-specific prompts. SMEC can dynamically refine or even redefine these roles as needed to address a given instruction without external supervision. The authors demonstrate that this approach outperforms established baselines such as majority voting, self-reflection, and direct prompting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Hierarchical evaluation:\nThe benchmark’s three-tiered structure -  spanning perception, understanding, and reasoning - with a consistent data distribution across tasks, enables a more causal interpretation of how lower-level perceptual abilities influence higher-level reasoning (if any).\n\n2. Scale of human annotations:\nIn terms of human-authored data, the dataset is impressively large and aligns with the current scale of contemporary multimodal benchmarks.\n\n3. Up-to-date and well-curated dataset:\nThe dataset is well-defined and systematically structured, with 70% images sourced after November 2024 and 50% after January 2025. The images are high-resolution and reflect contemporary visual content, ensuring relevance to real-world scenarios beyond academic testbeds.\n\n4. Strong empirical analysis:\nOne of the paper’s strongest aspects lies in its thorough analysis and experiments - particularly the dataset exploration in Fig. 4, cross-task synthetic analysis in Fig. 6, and detailed discussions in Sections 2.3, 4.3, 4.4, 4.5.\n\n5. Interesting new SMEC framework:\nThe proposed Self-Driven Multi-Expert Collaboration (SMEC) is an interesting  idea where the same MLLM, prompted with diverse role-specific instructions, can collaboratively address complex reasoning tasks. It consistently outperforms direct prompting, self-reflection, and majority voting. This approach also raises an intriguing direction for future work on redundancy in self-reflection or voting setups - showing that structured role specialization may better harness the diverse knowledge and reasoning abilities embedded within a single MLLM?\n\n6. Clarity and presentation:\nThe paper is clearly written and easy to follow, with well-organized appendices that are rich in detail and effectively referenced throughout the main text."}, "weaknesses": {"value": "Weaknesses and Questions\n\n1. Limited scenario diversity: While the dataset spans three main scenarios -  education, city, and home - it would benefit from broader coverage. For example, incorporating workplace, outdoor, or social-interaction settings could better capture real-world multimodal reasoning. Additionally, introducing samples grounded in logical, mathematical, or scientific domains, following the same hierarchical design principles, would strengthen the benchmark’s comprehensiveness.\n\n2. Number and organization of tasks per tier: The benchmark currently features a relatively limited number of tasks within each tier. It is unclear whether this was an intentional design choice (e.g., grouping subtasks to facilitate downstream synergy analysis between task tiers) or simply a byproduct of human annotation diversity.\n\na) Did the authors design subtasks with cross-tier synergy in mind?\nb) How do they envision scaling the benchmark to include additional domains (e.g., logic, mathematics, science)?\n\n3. Clarifications on SMEC framework:\nThe Self-Driven Multi-Expert Collaboration (SMEC) framework is conceptually strong but would benefit from further detail and analysis. Section 3 and the appendix provide helpful descriptions, yet several aspects remain underspecified:\n\na) How exactly is the diversity metric defined and measured?\nb)  Why did the authors choose 3500 samples subset, and how representative it is of the entire benchmark? Do results in Table 3 scale to the entire dataset?\nc) What are the types of roles the model tends to generate, and which roles contribute most to performance gains? \nd) are roles domain-centric  (e.g., geometry, culture, ethics) or task-centric roles (e.g., perception vs. reasoning)? Which is better and what scenario? \ne)Could the framework be extended to multi-model setups, where different MLLMs act as specialized experts?"}, "questions": {"value": "Follow-up questions presented along with the weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Appendix section A.4 provides data privacy and copyright statement, but could be worth a double check to ensure the benchmark if released does not violate this."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndIHmnyEvb", "forum": "zF0Cerwj5R", "replyto": "zF0Cerwj5R", "signatures": ["ICLR.cc/2026/Conference/Submission11027/Reviewer_19vU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11027/Reviewer_19vU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935652604, "cdate": 1761935652604, "tmdate": 1762922204196, "mdate": 1762922204196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LENS, a multi-level benchmark for evaluating multimodal large language models on perception, understanding, and reasoning tasks within a unified dataset. LENS contains over 60K human-written questions built on 3.4K real-world images, enabling analysis of interdependencies among visual reasoning levels. The authors further propose SMEC, a self-driven multi-expert collaboration framework that improves reasoning performance without external tools. Experiments on 15+ open and closed-source models reveal strong correlations between perception and reasoning performance and highlight persistent challenges in complex reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LENS unifies eight well-defined tasks over shared images, allowing systematic analysis of inter-level dependencies.\n2. Ensures methodological transparency and ethical compliance through detailed dataset documentation and privacy handling."}, "weaknesses": {"value": "1. The boundaries between perception, understanding, and reasoning tasks are not clearly defined, and some tasks (e.g. SRC as a reasoning task) overlap in scope.\n2. Lacks qualitative or fine-grained error analysis, which limits understanding of how and why models fail across different levels.\n3. Font size in Figure 6 should be improved for better readability."}, "questions": {"value": "1. Could the authors clarify the rationale for categorizing SRC as a reasoning task, given that its scope appears to overlap with understanding-level tasks?\n2. Would it be possible for the authors to include or discuss more detailed qualitative error analyses to better illustrate common failure patterns and model limitations across different task levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NGhftqtoO6", "forum": "zF0Cerwj5R", "replyto": "zF0Cerwj5R", "signatures": ["ICLR.cc/2026/Conference/Submission11027/Reviewer_SK8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11027/Reviewer_SK8n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976058647, "cdate": 1761976058647, "tmdate": 1762922203533, "mdate": 1762922203533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LENS, a multi-level benchmark for evaluating multimodal large language models on perception, understanding, and reasoning tasks within a unified dataset. LENS contains over 60K human-written questions built on 3.4K real-world images, enabling analysis of interdependencies among visual reasoning levels. The authors further propose SMEC, a self-driven multi-expert collaboration framework that improves reasoning performance without external tools. Experiments on 15+ open and closed-source models reveal strong correlations between perception and reasoning performance and highlight persistent challenges in complex reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LENS unifies eight well-defined tasks over shared images, allowing systematic analysis of inter-level dependencies.\n2. Ensures methodological transparency and ethical compliance through detailed dataset documentation and privacy handling."}, "weaknesses": {"value": "1. The boundaries between perception, understanding, and reasoning tasks are not clearly defined, and some tasks (e.g. SRC as a reasoning task) overlap in scope.\n2. Lacks qualitative or fine-grained error analysis, which limits understanding of how and why models fail across different levels.\n3. Font size in Figure 6 should be improved for better readability."}, "questions": {"value": "1. Could the authors clarify the rationale for categorizing SRC as a reasoning task, given that its scope appears to overlap with understanding-level tasks?\n2. Would it be possible for the authors to include or discuss more detailed qualitative error analyses to better illustrate common failure patterns and model limitations across different task levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NGhftqtoO6", "forum": "zF0Cerwj5R", "replyto": "zF0Cerwj5R", "signatures": ["ICLR.cc/2026/Conference/Submission11027/Reviewer_SK8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11027/Reviewer_SK8n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976058647, "cdate": 1761976058647, "tmdate": 1763693961239, "mdate": 1763693961239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}