{"id": "EZ2TH4rPwG", "number": 8019, "cdate": 1758052040026, "mdate": 1759897813883, "content": {"title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs", "abstract": "Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our implementation will be made public upon acceptance.", "tldr": "", "keywords": ["Triton", "GPU", "acceleration", "block low-rank", "roofline"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cb0f93566852a0b91c3641c694996bfe48bad8d.pdf", "supplementary_material": "/attachment/e4c3d067d26889617afd162eb74239abfc4ef25f.zip"}, "replies": [{"content": {"summary": {"value": "The paper uses roofline analysis to show that block low-rank (BLR) methods achieve theoretical savings and optimize single-token inference, and on the other hand multi-level token inference becomes memory bound. To solve this, the paper introduces kernels written in Triton for Monarch and BLAST layers. The paper analyses memory constrained GPUs, Jetson Orin Nano, and A40. Their kernels deliver both speedups and model size compression over dense baselines in PyTorch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and well structured, making the technical contributions easy to follow. On quality, the empirical results are strong: the method achieves up to 3.76× speedup over the dense baseline and demonstrates an approximately 3× improvement in model size, indicating meaningful efficiency gains. The implementation choices further support robustness: the kernels are written in Triton, positioning the work to benefit from ongoing compiler and hardware-backend optimizations. On originality, the paper thoughtfully combines roofline modeling with Triton-level kernel design to guide sparsity-aware optimizations, an effective synthesis of performance analysis and low-level implementation practice. Finally, on significance and clarity, the roofline analysis is used not just descriptively but to justify concrete design decisions in the kernels, helping readers understand why the approach works and where it is likely to generalize. Overall, the paper offers a clear, well-motivated, and practically impactful contribution."}, "weaknesses": {"value": "Significance/motivation. The case for block-low-rank (BLR) matrices is under-motivated. It remains unclear how frequently BLR kernels arise in real-world workloads and whether practitioners deploy them at scale.\n\nEvaluation scope. All experiments use batch size = 1, which limits generality. Performance on modern accelerators often changes with batch size, sequence length, block size/rank, and tensor shapes. Results sweeping batch size, sequence length, and BLR configurations would make the findings more robust.\n\nBaselines. Comparisons appear limited to PyTorch implementations. For a fair performance picture, one could include vendor-optimized libraries, e.g., CUTLASS/cuBLASLt, TensorRT where applicable).\n\nPortability/generalization. Results focus on Jetson Orin Nano and A40. It is unclear whether the gains transfer to more common data-center GPUs , such as, A100, H100, B100 NVIDIA GPUs."}, "questions": {"value": "1. How exactly is the speedup evaluated? For example, [1] in Rule 1 suggests: “When  publishing parallel speedup, report if the base case is a single parallel process or best serial execution, as well as the absolute execution performance of the base case. “\nCould you report your base case of the dense baseline, and clarify your evaluation according to this suggestion of benchmarking?\n\n2. Your evaluation targets Jetson Orin Nano and A40. What performance should readers expect on A100/H100/B100 or similar widely used NVIDIA GPUs? Are there kernel or compiler assumptions that limit portability (e.g., tensor-core MMA variants, shared-memory pressure, register usage, or Triton autotuning constraints)?\n\n3. Since BLR can be used for training from scratch, do your findings carry over to training workloads? Can the proposed kernels be used for backward passes, and if so, what speedups versus dense baselines should one expect during pre-training or fine-tuning? If not currently supported, what are the blockers?\n\n[1] Hoefler, Torsten, and Roberto Belli. \"Scientific benchmarking of parallel computing systems: twelve ways to tell the masses when reporting performance results.\" Proceedings of the international conference for high performance computing, networking, storage and analysis. 2015."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7mMmFyQB62", "forum": "EZ2TH4rPwG", "replyto": "EZ2TH4rPwG", "signatures": ["ICLR.cc/2026/Conference/Submission8019/Reviewer_Ak8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8019/Reviewer_Ak8A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663182653, "cdate": 1761663182653, "tmdate": 1762920021535, "mdate": 1762920021535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies costly data movement operations when computing on GPU the matrix multiplication associated with structured matrices such as Block low rank (related to Monarch matrices) and BLAST matrices, in the context of inference with compressed foundation models (vision transformers, large language models, diffusion transformers). They tackle this issue by introducing kernel fusion to reduce the cost of these memory operations. Experiments show both per-layer and end-to-end speedup at inference compared to previous implementations of matrix multiplication with such structured matrices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper evaluates the method on the compression of a broad range of foundation models, demonstrating the general applicability of the proposed method. Reported speedups are obtained under realistic conditions, such as BF16 precision and multi-token inference, which enhances the practical relevance of the experiments.\n* The speedup analysis is conducted at both the layer and end-to-end levels, providing insights into the sources of efficiency gains. The trade-off between accuracy and inference speed is clearly presented in Table 1 and Figures 9 and 10, offering a valuable perspective on the method’s strengths and limitations relative to prior approaches, including low-rank and dense baselines."}, "weaknesses": {"value": "My main concern with this submission is its positioning with respect to a very close paper [1] that was not mentioned in the submission. [1] also proposes to reduce the cost of data movement operations when performing matrix multiplication on GPU with the so-called Kronecker-sparse factors. These matrices are typically involved in butterfly factorizations and in the Monarch matrices discussed in this submission. In [1], it is observed that the original implementation of Monarch matrix multiplication [2] has costly memory operations due to permutations (as claimed in the authors' submission). Then, [1] proposes a novel tiling that avoids the costs of these permutations, and implements it with a CUDA kernel to show effective speedup compared to previous baselines. This overall narrative appears highly similar to that of the present paper.\n\nI highly recommend the authors to clarify their positioning with resepct to [1]. What are the similarities, differences and novelties compared to [1]?\n\n[1] Gonon, A., Zheng, L., Carrivain, P., and Le, Q.T. Fast Inference with Kronecker-Sparse Matrices. In ICML, 2025.\n\n[2] Dao, T., Chen, B., Sohoni, N. S., Desai, A. D., Poli, M., Grogan, J., Liu, A., Rao, A., Rudra, A., and Ré, C. Monarch: Expressive structured matrices for efficient and accurate training. In ICML, 2022."}, "questions": {"value": "* What are the expected benefits of implementing the kernel using Triton vs. CUDA in this specific setting? \n* Does the proposed kernel implementation leverage tensorcores?\n* The experiments were carried on BF16, does the authors expect a speedup compared to baselines when using FP16 instead?\n* How much effort was devoted to hyperparameter tuning in Table 1 when comparing the accuracy of the various compressed foundation models? The significance of Table 1 may depend on the quality of the benchmarking protocol, since it is important to ensure that all methods are compared on equal footing, with proper tuning on a separate validation set."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BcAlEUO870", "forum": "EZ2TH4rPwG", "replyto": "EZ2TH4rPwG", "signatures": ["ICLR.cc/2026/Conference/Submission8019/Reviewer_cefX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8019/Reviewer_cefX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828744510, "cdate": 1761828744510, "tmdate": 1762920021151, "mdate": 1762920021151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the GPU kernel performance problem of block low-rank (BLR) compression for linear layers. It utilizes the roofline model to identify that the state-of-the-art BLR computations are memory bounded. It proposes two sets of fused Triton kernels for Monarch and BLAST algorithms and achieves good performance improvement than the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It explores the rarely studied problem of optimizing the GPU kernels of block low-rank compression.\n2. It presents a set of practical solutions for the problem."}, "weaknesses": {"value": "1. Given that the low-rank compression's accuracy is bad, it lacks a well discussed motivation of optimizing this kind of algorithm specifically.\n2. The roofline model analysis is not novel nor necessary. The performance bottleneck of the BLR computation is obvious from system aspect, that the matmul shape is quite small so that it becomes memory bounded (especially the intermediate K dimension of the matmul).\n3. The GPU kernel optimization is straight forward and does not have new contribution from the system aspect.\n4. It has wrong FLOP estimation of all the matmul problems in this paper."}, "questions": {"value": "1. From Table.1, when CF = 2x, the accuracy of BLR seems pretty bad. Specifically, if the CF = 2x for quantization (i.e., quantizing from float16 to 8-bit), the performance will be nearly lossless. It would be better to have a discussion of the advantages of using low-rank compression, especially when compared to quantization.\n2. It is well known that small matmul shape will lead to low compute efficiency, either parallelism problem when M/N is small, or software pipeline problem when K is small (assuming a MNK matmul problem). It seems the roofline analysis is not a new contribution.\n3. The CUTLASS library has rich examples of fused permutation and back-to-back matmul. The methodologies in this manuscript does not have new insight for the operation fusion problem.\n4. For a matmul of MNK problem (i.e., [M,K] * [K,N]), the FLOP is 2MNK including both the multiplication and accumulation instructions. This paper wrongly estimates it as MNK in Section 2.\n5. The model used in this paper is old.\n6. In Figure 3, why the low-rank version has a larger speedup when the batchsize is larger? As for batch size 1, it has reduced half of the memory traffic, but only achieves 1.5x speedup. Have you analyzed why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wnsjOsHryX", "forum": "EZ2TH4rPwG", "replyto": "EZ2TH4rPwG", "signatures": ["ICLR.cc/2026/Conference/Submission8019/Reviewer_ZYGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8019/Reviewer_ZYGz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911679030, "cdate": 1761911679030, "tmdate": 1762920019951, "mdate": 1762920019951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a memory-efficient acceleration framework for large transformer-based foundation models using block low-rank (BLR) compression techniques such as Monarch and BLAST. While BLR methods theoretically reduce FLOPs and model size, the authors show via roofline analysis that multi-token inference on GPUs like the Jetson Orin Nano and A40 often becomes memory-bound, limiting practical speedups. To address this, they design custom Triton kernels featuring partial fusion, operation reordering, and optimized memory layouts to minimize data movement overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Demonstrating up to 3.76× speedup and 3× model size reduction on resource-constrained GPUs (Jetson Orin Nano, A40) highlights real-world relevance for edge and low-memory environments.\n2. The kernel fusion method is clearly explained."}, "weaknesses": {"value": "1. The main issue lies in the limited novelty of the contribution. Using kernel fusion to reduce I/O overhead is a well-established engineering practice in modern LLM systems. State-of-the-art training frameworks such as Megatron-LM already include numerous fused operations that consistently outperform vanilla PyTorch implementations. Therefore, the proposed optimization should be viewed primarily as an engineering refinement rather than an academic innovation. For comparison, methods like FlashAttention [1] introduce genuine algorithmic innovations (e.g., the online softmax formulation). Moreover, FlashAttention targets the self-attention mechanism, which is a core and widely used component across almost all transformer architectures, making its impact general and far-reaching. In contrast, this paper focuses on specialized optimizations for Monarch and BLAST, which are relatively niche techniques within block low-rank modeling.\n\n2. Even as an engineering contribution, several concerns remain:\n\n    2.1 Code availability – the implementation is not released.\n\n    2.2 Backward support – it is unclear whether the custom kernels include backward computation. Moreover, the training results in Table 1 seems do not use the fused kernels, which raises questions about their applicability during training.\n\n    2.3 Experimental scope – all experiments are performed on low-memory GPUs (e.g., Jetson Orin Nano, A40). Evaluations on modern high-end GPUs such as A100 or H100 are necessary to demonstrate scalability and broader relevance.\n\n[1] Dao, Tri, et al. \"Flashattention: Fast and memory-efficient exact attention with io-awareness.\" Advances in neural information processing systems 35 (2022): 16344-16359."}, "questions": {"value": "Check above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NgCuOI58aa", "forum": "EZ2TH4rPwG", "replyto": "EZ2TH4rPwG", "signatures": ["ICLR.cc/2026/Conference/Submission8019/Reviewer_c6Dq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8019/Reviewer_c6Dq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965121651, "cdate": 1761965121651, "tmdate": 1762920018973, "mdate": 1762920018973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}