{"id": "1BVQQKL61w", "number": 4156, "cdate": 1757614519478, "mdate": 1759898050057, "content": {"title": "Unify in Isolation: Unified Data Synthesis for Divergent Multi-Stage Systems", "abstract": "As modern information processing systems grow increasingly complex, multi-stage composite frameworks are receiving heightened attention, as models with divergent optimization objectives are able to extract more heterogeneous information. However, the significant differences in optimization objectives across models in multi-stage frameworks make it difficult to achieve model-level unification. Inspired by the data-centric paradigm, we seek to develop a unified data synthesis framework applicable across diverse training objectives. Specifically, we introduce the Unified Data Synthesis system for multi-stage frameworks, which adaptively provides unified structured data of varying quality at different stages to consistently enhance overall recommendation data quality. Initially, UniDS utilizes Real Entropy to evaluate data quality and, through Metric-Oriented Gradient Comparison Theory, demonstrates that different stage objectives exhibit distinct sensitivity for Real Entropy. Subsequently, leveraging this difference, UniDS injects Real Entropy into user sequence segmentation via the Pattern Mining via Conditional Entropy module, aiming to mine interaction patterns among stages. Finally, UniDR establishes a unified, model-agnostic data generation architecture based on the Special Pattern-Token paradigm, which utilizes patterns separated by entropy, thereby simultaneously generating new data and core task representations. This approach ultimately achieves a unified multi-stage data generation paradigm. Extensive experiments on benchmark datasets demonstrate enhanced performance on each model in a multi-stage system, improved flexibility in feature synthesis, and superior stage adaptation. Our anonymized code is available at https://anonymous.4open.science/r/UniDS-9510/", "tldr": "", "keywords": ["Multi-Stage System", "Data Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9acca52f48e26a2c99cd95d686cd9ef00e1ce2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the unified data synthesis system for multi-stage frameworks. The framework is built on the metric-oriented gradient comparison theory and a conditional entropy-based pattern mining module and offers a real entropy metric and adaptive pattern extraction capabilities. The experiments are conducted on sequential recommendation data"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework can help improve data quality and shows performance improvement on benchmark datasets.\n2. The proposed framework is theorectically grounded.\n3. This paper reports extensive related works."}, "weaknesses": {"value": "1. The presentation of the paper can be improved. For example, the title suggests that the method is developed for multi-stage systems, but the method is actually developed for sequential recommendation. In Section 5 experiments, the blank between baseline citations is not there.\n\n2. It will be better if experiments can be conducted on more base models, other than HSTU and LLAMA2Rec. Some good baselines can be included such as SASRec and BERT4Rec.\n\n3. The motivation of using entropy-based method in the framework seems confusing and can be better explained."}, "questions": {"value": "Is the framework developed for sequential recommendation models for general multi-stage systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QpFFR50YmJ", "forum": "1BVQQKL61w", "replyto": "1BVQQKL61w", "signatures": ["ICLR.cc/2026/Conference/Submission4156/Reviewer_AcMh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4156/Reviewer_AcMh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761356049029, "cdate": 1761356049029, "tmdate": 1762917203393, "mdate": 1762917203393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniDS, a data synthesis system for multi-stage frameworks, and analyzes it on sequential recommendation task. This paper theoretically demonstrates that the optimization objectives at different stages of sequential recommendation exhibit distinct sensitivities to the Real Entropy metric. Based on this insight, this paper extracts low-entropy patterns from user sequences, converts them into pattern-token sequences, and trains a model to reconstruct these pattern-token sequences for data synthesis. Empirical studies are conducted to demonstrate the effectiveness of UniDS compared to existing data augmentation baselines, the influence of key hyperparameters, and the contribution of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides valuable insights by constructing a theoretical framework based on Real Entropy, which offers a principled explanation for the model's effectiveness.\n\n2. The experiments demonstrate consistent performance improvements across the recall, coarse-ranking, and fine-ranking stages of sequential recommendation.\n\n3. The theoretical findings can account for the experimental observations."}, "weaknesses": {"value": "1. The description of the Methodology, particularly Section 4.3 \"Uniform Dataset Generation,\" could be made clearer. For example, in Section 4.3.2 \"Uniform Data Generation Structure,\" a model architecture is designed to reconstruct the pattern-token sequences, and a loss function is proposed to train this model. However, relevant details regarding the hyperparameters of model architecture and training process are not provided.\n\n2. This paper uses LLaMA2Rec as the base model, which does not appear to be a method from a published work. If the authors fine-tuned the Llama 2 model themselves for the experiments, details of this fine-tuning process should be provided, along with a justification for using a fine-tuned Llama 2 model as the base model for sequential recommendation."}, "questions": {"value": "1. Since UniDS is designed as a model-agnostic framework, could the authors present experimental results with more base models—besides the generative HSTU and LLaMA2Rec—to further support the consistent effectiveness of the method across different models?\n\n2. What is the \"Entropy-Based Tokenizer\" mentioned in line 265?\n\n3. In line 268, it is stated that each $T_i$​ functions as a special token representing user attributes, session context, or domain-specific information. How are these special tokens obtained?\n\n4. Can you provide more explanation about \"the target sequential model $\\phi$\" in line 273? What is its role?\n\n5. Can you elaborate on $\\mathcal{L}_\\text{pretrain}$​ in line 277? Why is it mentioned at that point?\n\n6. There are several typos in the manuscript. Although they do not affect the technical quality, correcting them would improve the overall presentation. Specific instances include:\n    - Line 024 in the Abstract: \"UniDR\" should be \"UniDS\"\n    - Line 240: \"usersequence\" should be \"user sequence\"\n    - Line 249: \"recommwndation\" should be \"recommendation\"\n\n7. The authors provided an anonymous GitHub link, but as of the time of this review, the repository contains only an empty README.md file."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EBuhU8qlrE", "forum": "1BVQQKL61w", "replyto": "1BVQQKL61w", "signatures": ["ICLR.cc/2026/Conference/Submission4156/Reviewer_zzWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4156/Reviewer_zzWY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834655448, "cdate": 1761834655448, "tmdate": 1762917203159, "mdate": 1762917203159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniDS, a unified data synthesis framework for multi-stage systems. The method measures the entropy of data sequences to estimate their quality, then splits each sequence into smaller patterns using conditional entropy. These patterns are represented in a unified “Pattern–Token” format and used to generate new training data that can be shared across different stages. The goal is to make data from different stages more consistent and suitable for joint optimization. Experiments are conducted on multi-stage sequential recommendation tasks to show the effectiveness of this approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on an important problem — how to make data from different stages in multi-stage systems more consistent. This motivation is meaningful and clearly explained.\n2. Using conditional entropy to split and rebuild data is an interesting idea. It is creative to control data quality through entropy and use it to build a unified dataset.\n3. The authors run several experiments with different settings and show that their generated data can improve model performance. The results are consistent across multiple metrics."}, "weaknesses": {"value": "1. The paper says it gives a unified method for all multi-stage systems, but all tests are only on sequential recommendation. The authors do say this choice was for simplicity, but they still describe the method as general in the abstract and introduction. Later, the paper suddenly switches to user–item sequences, which feels strange and not consistent with the earlier claim. The method also depends on the sequence order when it calculates conditional entropy and splits the data. Other systems, like RAG or agent pipelines, do not have such a clear time order or item transitions, so the same method may not work there. \n\n2. Table 2 shows three numbers in the “Gen vs. Origin (%)” column, but the paper never explains what they mean. It is not clear if they are three different metrics, or three settings like generated data, original data, and mixed data. This column is important for understanding if the new data helps or not, but without clear meaning, the results are confusing. The authors should say exactly what each value stands for, how it was calculated, and show how much variation or significance there is. Right now, readers cannot tell where the improvement really comes from.\n\n3. Sequential recommendation depends on the real order of user actions. In UniDS, the data are cut into parts using conditional entropy and then joined or generated again. Even if each part keeps its small order, joining parts together can change the real sequence flow and the true transition between items. This may make the model learn fake patterns that do not exist in real data. The improvement might come from this data change, not from the claimed “unified data control.” The paper should test if the generated data keep the same sequence structure, and run control experiments that compare with only splitting or only generating, to make sure the results are fair."}, "questions": {"value": "1. How can the proposed entropy-based segmentation be applied to non-sequential tasks such as RAG or multi-agent pipelines?\n2. When you generate new sequences, how do you make sure the original order or real transition patterns are not broken?\n3. Is the entropy threshold chosen manually or learned automatically in the final application? Do you need to adjust it based on different datasets, domains or applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wtjo2ZUa7e", "forum": "1BVQQKL61w", "replyto": "1BVQQKL61w", "signatures": ["ICLR.cc/2026/Conference/Submission4156/Reviewer_C5Bm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4156/Reviewer_C5Bm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869704021, "cdate": 1761869704021, "tmdate": 1762917202926, "mdate": 1762917202926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniDS, a unified data synthesis framework for divergent multi-stage systems such as multi-stage recommendation pipelines. Instead of model-level integration, the authors focus on data-level unification via three components: (1) Real Entropy for data quality measurement; (2) Metric-Oriented Gradient Comparison Theory, revealing differential sensitivity of ranking metrics (AUC, NDCG, MRR) to entropy, and (3) Conditional Entropy-based Pattern Mining with a unified Pattern-Token generation scheme. The framework aims to generate data that simultaneously improves recall, coarse-ranking, and fine-ranking models. Experiments on MovieLens, Amazon Books, and KuaiRand datasets show that UniDS consistently outperforms data augmentation baselines (DR4SR, FRec, RepPad)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Unlike previous approaches that adapt multi-stage objectives by modifying model architectures (e.g., through multi-task learning), UniDS shifts the “unification” of multi-stage systems from the model level to the data level, achieving consistency through standardized input data — an exceptionally pioneering research direction.\nBoth the theoretical derivations and experimental design are rigorous. The choice of the Real Entropy metric is well-justified, and the conditional-entropy-based pattern mining leverages dynamic programming to optimize computational efficiency. The framework demonstrates stable performance across multiple datasets and architectures, accompanied by comprehensive ablation studies and threshold sensitivity analyses.\nAddressing an empirical  issue in industrial multi-stage recommendation systems, UniDS provides a plug-and-play data synthesis solution that enhances performance without requiring extensive architectural modifications. Furthermore, the framework shows promising potential for cross-domain extension, particularly to RAG and multi-stage LLM Agent scenarios."}, "weaknesses": {"value": "Although the paper claims that UniDS is applicable to various multi-stage systems, all experiments are confined to the sequential recommendation scenario. No preliminary experiments are conducted on other types of multi-stage tasks, resulting in a lack of cross-domain validation.\n\nThe paper does not report the computational overhead of UniDS compared with the baselines. For example, the conditional-entropy-based pattern mining requires iterative entropy computation, and the Transformer-based generator may increase training time. Key efficiency metrics such as inference speed and memory consumption are not discussed. \n\nDuring the pattern mining stage, whether to continue expanding a pattern is determined by the reduction in conditional entropy. However, the source or selection strategy of the threshold θ is not specified, and the paper does not analyze how θ affects runtime or the size of the search space. \n\nAlthough the theoretical framework references information-theoretic definitions, the actual computation relies on an LZ compression-based approximation, which deviates from strict information-theoretic entropy formulations. Moreover, no monotonicity calibration experiment is provided to verify the correspondence between Real Entropy and “true effective information.” Therefore, it remains unclear whether Real Entropy reflects a causal factor or merely a correlated phenomenon, undermining its credibility as the core indicator for cross-stage data quality unification."}, "questions": {"value": "How does UniDS perform in non-recommendation multi-stage systems, e.g., retrieval–generation pipelines or reinforcement learning agents?\n\nWhat is the computational overhead of Real Entropy estimation and pattern mining?\n\nCould the authors discuss how the entropy threshold θ is initialized and adapted dynamically?\n\nAre there any scenarios where UniDS underperforms compared to baselines? For example, in datasets with extremely low data density (e.g., users with only 5-10 interactions), does the Pattern Mining module fail to extract meaningful patterns, leading to performance degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CVyNyqVbUJ", "forum": "1BVQQKL61w", "replyto": "1BVQQKL61w", "signatures": ["ICLR.cc/2026/Conference/Submission4156/Reviewer_iGL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4156/Reviewer_iGL9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981863694, "cdate": 1761981863694, "tmdate": 1762917202630, "mdate": 1762917202630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}