{"id": "EYkPcogJxo", "number": 15303, "cdate": 1758250077429, "mdate": 1759897314752, "content": {"title": "Cognitive Structure Generation via Diffusion Models with Policy Optimization", "abstract": "Cognitive structure (CS), a student's construction of concepts and inter-concept relations, has long been recognized as a foundational notion in educational psychology, yet remains largely unassessable in practice. Existing approaches such as knowledge tracing (KT) and cognitive diagnosis (CD) simplify and indirectly approximate CS, but they intertwine representation learning with prediction objectives, limiting generalization, interpretability, and reuse across tasks. To address this gap, we propose Cognitive Structure Generation (CSG), a task-agnostic framework that explicitly models CS through generative modeling. Based on educational theories, CSG first pretrains a Cognitive Structure Diffusion Probabilistic Model (CSDPM) and then applies reinforcement learning with SOLO-based hierarchical rewards to align generation with genuine cognitive development. By decoupling cognitive structure  representation from downstream prediction, CSG produces interpretable and transferable cognitive structures that can be seamlessly integrated into diverse student modeling tasks. Experiments on four real-world datasets show that CSG yields more comprehensive representations, substantially improving performance while offering enhanced interpretability and modularity.", "tldr": "We propose Cognitive Structure Generation (CSG), a generative framework that learns cognitive structures and decouples representation from downstream prediction.", "keywords": ["Cognitive structure"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f10613f0ae1485da3a8f4fdb9a5618e1dfcb248.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work frames student modeling as Cognitive Structure Generation (CSG): given a student’s learning history, generate a personalized graph. The nodes encode concept mastery and edges encode inter-concept relations. It instantiates this with a Cognitive Structure Diffusion Probabilistic Model (CSDPM) trained in two stages: (i) pretraining on simulated cognitive structures inferred from logs via a rule-based procedure, and (ii) policy optimization of the reverse diffusion process to align generated graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Overall, I like the idea and the problem setting behind the proposed methods. Modeling dynamic, inter-concept graphs is both important and challenging,  especially given the limited data available per student, the lack of concrete evaluation methods, and the difficulty of optimizing over discrete structures. I also appreciate the effort to combine KT and CD, since prediction and interpretability are equally important in the educational domain.\n\n- The method’s use of a discrete graph diffusion model combined with policy optimization guided by the SOLO taxonomy is interesting. The pipeline, i.e., pretraining on simulated structures followed by reinforcement learning (RL) alignment, is novel within the education modeling space.\n\n- The paper is very well-presented. I enjoyed both the clarity of the writing and the quality of the figures."}, "weaknesses": {"value": "- The training signal for Stage-I comes from rule-based simulation computed from the same interaction logs used downstream, with Gaussian noise added t the Q-matrix for robustness. The mapping $f_{\\mathrm{UOC}}, f_{\\mathrm{UOR}}$ is basically weighted correctness; then values are rounded and one-hot encoded, which discards uncertainty and may bake in label-like information before KT/CD. A stronger justification or comparison against alternative simulators (Bayesian knowledge tracing, IRT-based posteriors, or human-elicited maps) is needed.\n- The 8:1:1 split at the interaction level seems somewhat inconsistent with the paper’s introduction section about “generalization” and “cold-start”. It would be informative to see how the models perform under smaller data regimes, especially given that several heuristics are already used so the method should work with small data. \n- The edge construction $f_{\\text {UOR }}$ uses co-occurrence within items as evidence of a relation between concepts. This conflates test design with student cognition; edge identifiability is questionable. Visual cases are interesting but anecdotal. A human-judged metric (or even prompt LLMs to evaluate edges) would make it more concrete.\n- Three datasets have only 20 items, and the largest uses 57 concepts but task heterogeneity is unclear. It’s hard to conclude that CSG scales to hundreds–thousands of concepts or to more realistic, noisy Q-matrices. Reported generation times are low, but training cost of diffusion + RL may be substantial.\n- The graph size is fixed (predefined by the number of concepts). In real-world educational settings, graph structures are dynamic: concept sets can expand or contract, and nodes often exist at different abstraction levels. For example, “linear algebra” as a field, “dot product” as a concept, and “specific exercises” as instances, which I think are all presented in student's mind. How does this method extend to these situations?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "niyj2TDnHj", "forum": "EYkPcogJxo", "replyto": "EYkPcogJxo", "signatures": ["ICLR.cc/2026/Conference/Submission15303/Reviewer_Aje3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15303/Reviewer_Aje3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493837504, "cdate": 1761493837504, "tmdate": 1762925601384, "mdate": 1762925601384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new student modeling framework named CSG, which aims to explicitly generate a student’s Cognitive Structure (CS) through a diffusion model. Unlike traditional KT and CD methods that implicitly model students’ concept mastery states, this work generates cognitive graphs to represent both the concepts and the construction process of relationships between concepts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The study formalizes the cognitive structure modeling problem as a graph generation task and introduces a diffusion model combined with a RL framework, which is innovative.\n2. Experiments are conducted on public datasets with comprehensive results. The proposed method significantly outperforms baselines across multiple metrics (AUC, ACC, RMSE). Ablation studies (V1–V5) further validate the effectiveness of each module.\n3. The generated cognitive structures improve performance on both KT and CD downstream tasks, demonstrating strong generalization ability of the learned structures."}, "weaknesses": {"value": "1. The quality of the cognitive structures is evaluated indirectly through KT/CD task performance, without qualitative validation of the generated structures themselves.\n2. There is no direct comparison with other graph generation paradigms (e.g., VAE, GraphGAN,), making it difficult to justify the necessity of the diffusion model.\n3. The rule-based “Cognitive Structure Simulation” component relies on handcrafted empirical formulas (e.g., Eq. (1)(2)) without verification against real student thinking patterns, which may introduce bias."}, "questions": {"value": "1. Are the rule functions (Eq. (1)–(2)) in the simulated cognitive structure still effective across different subjects or question types? Have the authors considered learning these weights from data instead of manually defining them?\n2. Could the authors visualize the temporal evolution of the generated cognitive graphs?\n3. Why did the authors choose the diffusion model over other generative methods such as VAE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7d6qXyNjP", "forum": "EYkPcogJxo", "replyto": "EYkPcogJxo", "signatures": ["ICLR.cc/2026/Conference/Submission15303/Reviewer_18YN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15303/Reviewer_18YN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707925901, "cdate": 1761707925901, "tmdate": 1762925599411, "mdate": 1762925599411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a way to generate personal latent representations called cognitive structures, using diffusion model. They show on several datasets that they outperform either knowledge tracing or cognitive diagnosis techniques."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is a unique approach that draws a link between knowledge tracing and cognitive diagnosis, two popular research communities in the literature.\nIt addresses the shortcomings that not everyone may have the same representations for a domain.\n\nThe mathematical description of the proposed approach is very clear."}, "weaknesses": {"value": "To me, the link with SOLO theory is a bit far-fetched: the fact that there would be exactly 5 levels is arbitrary. But that's a nice story.\n\nThe presentation contains many LLM-generated sentences: \"align generation with genuine cognitive development\". It is yet to be proven that the learned representations correspond to the actual cognitive development of students. \"authentic levels of cognitive growth\" is a bit too much too."}, "questions": {"value": "Could you please elaborate more about your use of LLMs for writing the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UkVXdLz5S1", "forum": "EYkPcogJxo", "replyto": "EYkPcogJxo", "signatures": ["ICLR.cc/2026/Conference/Submission15303/Reviewer_fqty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15303/Reviewer_fqty"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815170966, "cdate": 1761815170966, "tmdate": 1762925598544, "mdate": 1762925598544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Cognitive Structure Generation (CSG) task and a corresponding CSDPM method to address it, aiming to improve model generalization and interpretability. Experiments on Knowledge Tracing (KT) and Cognitive Diagnosis (CD) tasks demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to provide a unified method applicable to both KT and CD tasks and reports some empirical improvements."}, "weaknesses": {"value": "1. The paper overclaims novelty in defining the CSG task. Similar ideas have been explored in prior works such as MSKT (ESWA, 2024) and DiffCog (TLT, 2024).\n\n2. The interpretability analysis is not convincing. While interpretability is stated as a key contribution, the paper lacks quantitative metrics to support this claim. The main body include few interpretability analyses, most of which are placed in the Appendix.\n\n3. The paper provides no theoretical justification or proof to support the soundness of the proposed method.\n\n4. The computational cost of the proposed method is not discussed, leaving questions about its scalability and efficiency.\n\n5. It remains unclear how the method performs on large-scale datasets, especially those with a greater number of knowledge concepts."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8KJ3fSQAen", "forum": "EYkPcogJxo", "replyto": "EYkPcogJxo", "signatures": ["ICLR.cc/2026/Conference/Submission15303/Reviewer_pRBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15303/Reviewer_pRBx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065862663, "cdate": 1762065862663, "tmdate": 1762925598124, "mdate": 1762925598124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}