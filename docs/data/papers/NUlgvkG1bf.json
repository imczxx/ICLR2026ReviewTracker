{"id": "NUlgvkG1bf", "number": 9747, "cdate": 1758137789142, "mdate": 1763627583388, "content": {"title": "CRAMER: Control via Request-Aware Masking for Editing Recommenders", "abstract": "Sequential recommendation models, while powerful, have limited flexibility in responding to immediate user requests, making it difficult to adapt their recommendations to the user's timely interests. Unfortunately, existing user request adaptation methods often incur high computational overhead due to either 1) retraining the entire backbone network or 2) leveraging the inference ability of large language models (a.k.a. prompt engineering), limiting their applicability in large-scale recommendation services. This paper presents **C**ontrol via **R**equest-**A**ware **M**asking for **E**diting **R**ecommenders (**CRAMER**), a framework that takes users' natural-language requests to immediately change sequential recommendation models' behavior. Specifically, inspired by the model control theory, CRAMER treats user requests as control signals to modulate frozen backbone parameters through masking, achieving instant adaptation to diverse requests while avoiding costly retraining. Experiments on multiple large-scale benchmark datasets show that CRAMER outperforms four state-of-the-art request-aware baselines across multiple recommendation metrics while achieving minimal overhead. Moreover, the proposed framework exhibits enhanced controllability and cross-domain adaptability, establishing a new paradigm for request-aware sequential recommendation.", "tldr": "We propose CRAMER, a request-aware masking framework that adapts frozen sequential recommenders to natural-language requests in real time, achieving controllability and efficiency without retraining.", "keywords": ["Controllable Model Editing", "Request-Aware Masking", "Parameter-Efficient Adaptation", "Sequential Recommendation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77ae47a2c3a59c040c9b758f04df15dba5fe2ac3.pdf", "supplementary_material": "/attachment/82287a8fe1d9c2cf9c9f639ffb8bebc75673ad20.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents CRAMER, a lightweight and parameter-efficient framework that adapts Transformer-based sequential recommenders to natural-language user requests in real time. It encodes requests into semantic embeddings, projects them to sparse gate logits, and applies structured binary masks via Gumbel Top-k sampling. With a predictive loss and KL sparsity regularization, CRAMER achieves consistent improvements over request-aware baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Theoretically Grounded Control Mechanism. The variational formulation (ELBO with Bernoulli prior) explains sparsity and provides a clean, scalable training objective.\n2. Flexible and Modular Design. Supports multiple masking scopes and tuning strategies (frozen, partial, or full PLM), adapting to various compute budgets.\n3. Strong Empirical Validation. Evaluated on large-scale, text-rich real-world datasets with comprehensive ablations."}, "weaknesses": {"value": "1. Heavy reliance on pretrained language model (PLM) quality. Performance varies significantly with PLM choice—MiniLM lags behind ModernBERT by up to 15% in NDCG@10. This suggests CRAMER inherits PLM biases, domain gaps, and sensitivity to phrasing.\n2. Static maskable subsets limit expressiveness; lacks dynamic selection per request type CRAMER fixes the controllable scope globally, ignoring that different requests may require different control levers.\n3. Lack of case studies to clearly illustrate failure modes The paper reports strong average metrics but provides no qualitative failure analysis. For instance, how does CRAMER behave when requests are ambiguous or contain rare terms?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YtV78nnKQk", "forum": "NUlgvkG1bf", "replyto": "NUlgvkG1bf", "signatures": ["ICLR.cc/2026/Conference/Submission9747/Reviewer_cUQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9747/Reviewer_cUQE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891119556, "cdate": 1761891119556, "tmdate": 1762921242456, "mdate": 1762921242456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CRAMER addresses the inflexibility of sequential recommenders in adapting to immediate user natural-language requests while avoiding high computational overhead. It treats user requests as control signals to modulate frozen Transformer-based backbones (SASRec, BERT4Rec) via parameter masking, enabling real-time adaptation without retraining. Key steps include: (1) encoding requests into semantic embeddings using pretrained language models (PLMs); (2) projecting embeddings to sparse row-column gate vectors via Gumbel–Top-k sampling. Experiments on four datasets show CRAMER outperforms four state-of-the-art request-aware baselines, with minimal inference overhead and strong cross-domain adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Lightweight Control method: Unlike prior methods (retraining backbones, LLM prompt engineering) that incur high overhead, CRAMER uses request-driven parameter masking to modulate frozen models. This design achieves fast adaptation (no retraining) and minimal inference cost (comparable to the fastest baseline LLM-ESR), resolving the efficiency-adaptability trade-off in real-world recommendation systems.\n\n- Theoretically Grounded and Robust Design: The variational objective (with KL regularization) enforces sparse, stable masks, while STE enables end-to-end optimization of discrete gates. \n\n- Strong Empirical Generalization: CRAMER outperforms baselines in 93.75% of experiments across diverse domains. It also exhibits robustness to hyperparameters and maintains effectiveness on both small (ReDial) and large (KuaiSAR) datasets"}, "weaknesses": {"value": "- Ability to handle Ambiguous/Contradictory Requests: The framework assumes requests are semantically clear and align with either enhancing or negating historical preferences. It lacks mechanisms to resolve ambiguity (e.g., vague requests) or extreme user preference drift, potentially leading to suboptimal masking and misaligned recommendations.\n\n- Dependence on PLM Quality for Request Encoding: CRAMER’s performance heavily relies on the PLM used for request embedding—lightweight PLMs (e.g., MiniLM) underperform due to limited semantic capture, limiting deployment in resource-constrained scenarios where only small PLMs are feasible.\n\n- No Analysis of Mask Interpretability: While CRAMER claims \"fine-grained control,\" it provides limited analysis of how masks map to request semantics. Without interpretability, it is hard to debug failures"}, "questions": {"value": "See weakness. \n- How would CRAMER adapt to ambiguous or contradictory requests?\n\n- How does CRAMER perform on cold-start users/items? Since it relies on historical user sequences to generate meaningful masks, it may fail for users with limited history or items with limited interaction data"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JDBXM7JtHY", "forum": "NUlgvkG1bf", "replyto": "NUlgvkG1bf", "signatures": ["ICLR.cc/2026/Conference/Submission9747/Reviewer_Cwki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9747/Reviewer_Cwki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897783770, "cdate": 1761897783770, "tmdate": 1762921241773, "mdate": 1762921241773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In sequential recommendation, the recommender system predicts the next item that the user is likely to interact with. In some cases, users also submit natural language queries while interacting with the platform (often search queries, e.g. \"blue dresses\" or \"shower curtains\"), but these queries are typically ignored by the sequential recommendation system. The goal of this work is to use these queries to adapt the outputs of the sequential recommender system. Since the recsys is typically already trained and fixed, rather than changing the underlying model, the authors propose a masking strategy. In particular, in their method, the request is encoded and then converted into a binary mask that is applied to either the output weights of the attention layers or the feed-forward weights of the transformer. The idea is simple (in a good way), though the paper’s current presentation makes it appear more complex than necessary and would benefit from clearer exposition."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- The masking idea is simple and straight-forward\n- The method is a computationally efficient to adapt a fixed sequential transformer-based recommender system, and is on par in runtime to all baselines\n- The authors test 5 methods (including theirs) on 4 datasets, and find that their method outperforms the baselines in all settings in terms of NDCG, MRR, HR"}, "weaknesses": {"value": "# Clarity\nThe paper’s clarity could be substantially improved. For example, the proposed masking method is actually quite straightforward: request-specific masks are learned to adapt the fixed transformer model, and these masks are trained end-to-end by maximizing the likelihood of recommending relevant items. However, I found the discussion of a variational lower bound in Section 3.2 confusing since the authors use Gumbel-top-k to sample binary masks. Typically, one would then use the straight-through estimator, not variational inference. And in fact, the method does ultimately use the straight-through estimator and does not perform variational inference. I suggest removing the discussion of variational inference, at least from the main text, as it adds unnecessary complexity without clear benefit. Similarly, the KL regularization term does not need to be motivated from a variational inference perspective.\n\n# Experiments\n- The authors state that in \"45 out of 48 experiments, CRAMER showed statistically significant improvements over the second best result\", but they do not perform any multiple testing corrections. Please update the paper with corrections for multiple testing.\n- For the efficiency results in Table 2, which backbone is this with?  It would be helpful to expand the table to include results for both backbones, and to report the runtime of the vanilla SASRec and BERT4Rec backbones themselves. This would clarify how much of the runtime is attributable to the query-aware method versus the backbone.\n\n# Limitations\nThe authors do not provide any discussion of limitations in the paper. Please update the paper with a discussion of limitations. Here are a few examples of topics to consider adding, though the authors should add their own points beyond what I list here:\n- The authors say in the introduction that \"natural-language requests may emphasize or even contradict historical\npreferences, requiring the model to dynamically balance immediate intent with long-term behavior patterns\". However, in practice, they train their model to predict historical interactions conditional on the query. This raise the question of how much the queries can steer the model to items that users have truly not engaged with in the past.\n- Relatedly, the method requires the platform to already have a good way of catering to the user's query (e.g. by changing the pool of items that are received), otherwise the historical interactions used for training will not truly reflect the user's intent\n- General limitations of transformer-based sequential models, e.g., support for extremely large item inventories and generalization to new items"}, "questions": {"value": "One of the key premises of the paper is that the sequential recommender system cannot be retrained to incorporate user queries, which serves as an important motivation for this work. However, I am unclear about how this aligns with real-world practice, where recommender systems are often used alongside search queries. For example, when a user searches for something like \"cat food,\" is it typical for the system to first retrieve a set of relevant items and then constrain the sequential recommender to only select from within that set? If so, while the current paper focuses solely on the sequential recommender in isolation, this combined approach of constraining the item pool based on the query and then applying the recommender could serve as a practical and relevant baseline for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ujrM0oyM1m", "forum": "NUlgvkG1bf", "replyto": "NUlgvkG1bf", "signatures": ["ICLR.cc/2026/Conference/Submission9747/Reviewer_MGLn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9747/Reviewer_MGLn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928916395, "cdate": 1761928916395, "tmdate": 1762921241266, "mdate": 1762921241266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CRAMER (Control via Request-Aware Masking for Editing Recommenders), a novel, parameter-efficient framework designed to enable sequential recommendation models to respond instantaneously to natural-language user requests. The core innovation is treating the user request as a control signal used to modulate the parameters of a frozen Transformer backbone via structured, sparse masking, drawing inspiration from model control theory. CRAMER avoids the computational overhead associated with full fine-tuning or heavyweight Large Language Model (LLM) inference. The methodology employs a lightweight request encoder, converts the semantic embedding into sparse row–column gate vectors using the Gumbel–Top-k trick, and trains this mapping using a variationally motivated objective featuring a KL sparsity regularizer. Empirically, the authors demonstrate that CRAMER consistently outperforms four state-of-the-art request-aware baselines across multiple large-scale benchmark datasets and two foundational Transformer architectures (SASRec and BERT4Rec)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Superior Empirical Performance: CRAMER demonstrates robust and consistent state-of-the-art performance, outperforming four competing request-aware baselines (Query-SeqRec, BLaIR, LLM-ESR, REARANK) across all four benchmark datasets (ReDial, KuaiSAR, Beauty, CDs&Vinyl) and both frozen backbones (SASRec and BERT4Rec). Notably, CRAMER achieved statistically significant improvements over the second-best result in 45 out of 48 experiments (93.75%).\n\n2. Exceptional Efficiency and Scalability: The framework is highly efficient, achieving minimal computational overhead compared to retraining or LLM-based approaches. CRAMER’s average inference runtime is 0.018 seconds per request, which is comparable to the fastest baseline (LLM-ESR at 0.016s) but drastically faster than heavyweight methods like REARANK (9.256s), making it highly suitable for real-time recommendation deployment.\n\n3. Novel and Parameter-Efficient Control Paradigm: CRAMER introduces a new paradigm for request-aware sequential recommendation by framing the natural language request as a fine-grained control signal applied via masking to a frozen backbone. This masking approach, utilizing structured row-column gating, is an expressive yet lightweight mechanism that minimizes the number of trainable parameters required for adaptation"}, "weaknesses": {"value": "1. Sensitivity to Mask Sparsity (ρ): The drop ratio ρ is a crucial hyperparameter, and performance deteriorates significantly at extreme values. Optimal performance requires tuning ρ based on dataset characteristics—smaller ρ (denser masks) is better for large, data-dense datasets, while larger ρ (sparser masks) is preferable for small datasets to prevent overfitting. This suggests a non-trivial tuning requirement for new application domains.\n\n2. Dependence on High-Capacity PLMs: The performance of CRAMER is shown to rely heavily on the quality and capacity of the Pretrained Language Model (PLM) used for initialization of the request encoder $E_{ϕ_{enc}}$. While RoBERTa and ModernBERT yield the best results, lightweight models like MiniLM consistently underperform, indicating that robust semantic extraction is a prerequisite for effective control via masking.\n\n3. Use of a Variationally Inspired Surrogate Objective: The authors acknowledge that the practical training objective is not a strict Evidence Lower Bound (ELBO) maximization. Because the forward pass uses a hard k-hot Gumbel–Top-k sampler, while the KL regularizer assumes independent Bernoulli gates, the resulting objective is a \"variationally inspired surrogate\". A deeper discussion or boundary analysis concerning the theoretical gap introduced by this necessary approximation would strengthen the paper.\n\n4. Training Complexity of Discrete Optimization: The method relies on sophisticated techniques (Gumbel–Top-k sampling and Straight-Through Estimators (STE)) to handle discrete mask generation and gradient flow. Furthermore, the performance is sensitive to the annealing schedule for the STE temperature $\\tau$, with cosine decay found to be superior. These elements add overhead and complexity to the training setup compared to fully continuous optimization methods.\n\n5. Incomplete Large-Scale Validation: While the paper claims superiority on \"large-scale\" benchmarks, the three largest datasets used (KuaiSAR, Beauty, and CDs\\&Vinyl) were randomly downsampled due to computing budget limitations. Although necessary for the authors, this compromises the ability to definitively evaluate the method's performance and scalability claims on the full, unreduced datasets."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0V8nIp4Olq", "forum": "NUlgvkG1bf", "replyto": "NUlgvkG1bf", "signatures": ["ICLR.cc/2026/Conference/Submission9747/Reviewer_VS3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9747/Reviewer_VS3F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973337617, "cdate": 1761973337617, "tmdate": 1762921240803, "mdate": 1762921240803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}