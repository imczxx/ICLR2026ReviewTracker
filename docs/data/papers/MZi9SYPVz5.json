{"id": "MZi9SYPVz5", "number": 24698, "cdate": 1758359442912, "mdate": 1759896754115, "content": {"title": "Enhancing Visual Token Representations for Video Large Language Models via Training-free Spatial-Temporal Pooling and Gridding", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly advanced video understanding tasks, yet challenges remain in efficiently compressing visual tokens while preserving spatiotemporal interactions. Existing methods, such as LLaVA family, utilize simplistic pooling or interpolation techniques that overlook the intricate dynamics of visual tokens. To bridge this gap, we propose ST-GridPool, a novel training-free visual token enhancement method designed specifically for Video LLMs. Our approach integrates Pyramid Temporal Gridding (PTG), which captures multi-grained spatiotemporal interactions through hierarchical temporal gridding, and Norm-based Spatial Pooling (NSP), which preserves high-information visual regions by leveraging the correlation between token norms and semantic richness. Extensive experiments on various benchmarks demonstrate that ST-GridPool consistently enhances performance of Video LLMs without requiring costly retraining. \nOur method offers an efficient and plug-and-play solution for improving visual token representations.\nOur code is available in [https://anonymous.4open.science/r/ST-GridPool-85BE](https://anonymous.4open.science/r/ST-GridPool-85BE).", "tldr": "Our training-free method, ST-GridPool, boosts Video LLM performance and efficiency by intelligently compressing visual tokens based on their spatiotemporal importance.", "keywords": ["Visual Token Representation", "Video Understanding", "Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a32119d51c7b783cf45f7e597aa27891964f0c71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ST-GridPool, a training-free method to enhance visual token representations for Video LLMs, addressing the challenge of efficient token compression while preserving spatiotemporal interactions. It integrates two core components: Pyramid Temporal Gridding (PTG) for multi-grained temporal feature capture and Norm-based Spatial Pooling (NSP) for preserving high-semantic regions via token norms. Evaluated on 6 benchmarks (e.g., VideoMME, MVBench) with LLaVA-OneVision-7B and LLaVA-Video-7B, ST-GridPool achieves consistent performance gains without retraining, while reducing inference cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper tackles a critical research problem about visual token enhancement for Video LLMs. The proposed ST-GridPool method is effective. It offers a training-free, plug-and-play framework that improves visual token representations without adding extra computational cost, a very appealing design choice.\nThe paper is well positioned within the literature and clearly explains how it differs from prior work. The motivation is convincing, the narrative flows nicely, and the writing is clear throughout. It’s an easy and enjoyable read.\nCapturing multi-grained spatiotemporal interactions across varying temporal lengths and leveraging the positive correlation between token norms and semantic importance is a really interesting idea.\nThe results are solid, covering diverse tasks, different backbones, and a range of metrics and hyperparameters. The experimental section is thorough and leaves little doubt about the method’s effectiveness."}, "weaknesses": {"value": "The paper demonstrates the overall computational advantages (inference time and memory) of ST-GridPool. However, it does not provide a detailed breakdown of the computational cost for the PTG and NSP components individually. Furthermore, the potential interaction between these two modules is not discussed, e.g. how PTG's multi-scale processing affects the norm distribution of the input to NSP.\n\nThe Pyramid Temporal Gridding (PTG) module generates a summary token for each temporal segment and then uses it to update the token of the last frame in that segment. This update operation seems potentially destructive, as it replaces the representation of an actual, observed frame with a synthetic, interpolated summary. This design choice is not clearly motivated over less destructive alternatives, e.g., appending the summary token.\n\nThe NSP component is explicitly designed to prioritize high-importance regions (i.e., salient objects) and suppress low-importance backgrounds. This strategy could be detrimental in tasks that require holistic scene understanding or reasoning about subtle interactions within the background regions, which are now actively suppressed."}, "questions": {"value": "1. Given the model's clear overall efficiency gains, could you provide a more detailed computational cost breakdown contributed by the PTG and NSP modules individually?\n2. Why do you choose to make the PTG summary token overwrite the last frame's token instead of a less destructive alternative like appending?\n3. How does your method balance the trade-off between prioritizing high-importance regions and preserving potentially critical, low-importance backgrounds in practical video understanding tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "d5AQsVMMvu", "forum": "MZi9SYPVz5", "replyto": "MZi9SYPVz5", "signatures": ["ICLR.cc/2026/Conference/Submission24698/Reviewer_VLyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24698/Reviewer_VLyg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834917001, "cdate": 1761834917001, "tmdate": 1762943166178, "mdate": 1762943166178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a well-motivated and effective training-free method, ST-GridPool, for enhancing visual token representations in Video LLMs. The approach is simple yet intuitive, combining PTG and NSP to improve performance without retraining while also reducing computational overhead. The empirical validation is extensive, with strong ablation studies supporting the design choices."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Both components are well-justified and technically sound. PTG provides an intuitive hierarchical approach to model events at different time scales. NSP is motivated by both intuitive and quantitative analysis, which is a clever insight for preserving important information during compression.\n\n2. A comprehensive evaluation is conducted, covering both long and general video understanding tasks. The method is tested across multiple backbone models, demonstrating its generalizability and effectiveness.\n\n3. The paper is mostly well-written, clearly structured, and easy to follow, with helpful visualizations that aid in understanding the proposed approach."}, "weaknesses": {"value": "1. The method for updating the token sequence in Pyramid Temporal Gridding involves overwriting the last frame token of a segment with a generated summary token. This specific design choice should be justified.\n\n2. The computational analysis focuses on the net efficiency gains, but offers limited insight into the additional computational cost (e.g., latency, memory) introduced by the ST-GridPool module itself.\n\n3. A deeper theoretical explanation for why token norms (specifically L_2 norm) reliably correlate with semantic importance in this context would be beneficial."}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jkiGPtoQYj", "forum": "MZi9SYPVz5", "replyto": "MZi9SYPVz5", "signatures": ["ICLR.cc/2026/Conference/Submission24698/Reviewer_JydV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24698/Reviewer_JydV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835866284, "cdate": 1761835866284, "tmdate": 1762943165951, "mdate": 1762943165951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ST-GridPool, a practical training-free method to enhance visual tokens for Video LLMs. The approach is technically sound, combining a hierarchical temporal gridding strategy (PTG) and a saliency-aware spatial pooling mechanism (NSP). The method demonstrates consistent performance improvements on LLaVA-family models across six benchmarks and compelling gains in computational efficiency. The empirical evaluation is extensive, including strong ablations and comparisons to other token reduction methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed ST-GridPool method is training-free and plug-and-play, offering an efficient solution to enhance existing Video LLMs without the need for costly retraining. It demonstrates consistent performance improvements across various backbones on all six evaluated video understanding benchmarks, with notable gains in Long Video Understanding.\\\n2.The motivation behind Norm-based Spatial Pooling is well-founded, supported by a clear analysis that establishes a positive correlation between token norms and semantic saliency, contributing to the method's effectiveness.\\\n3.The empirical evaluation is comprehensive, featuring extensive ablation studies on individual components, key hyperparameters, and design choices such as pooling shape and gridding strategy."}, "weaknesses": {"value": "1.The performance depends on several non-trivial hyperparameters, including the number of temporal levels, norm order, and temperature. Balancing these parameters effectively is crucial for their successful application in real-world scenarios.\\\n2.The paper does not clearly show whether PTG provides complementary benefits or duplicates functionality already present in recent Video LLMs.\\\n3.The NSP module relies heavily on an empirical correlation between token norm and semantic importance, yet offers limited empirical comparison or grounding.\\\n4.The experimental section primarily focuses on quantitative results but lacks deeper analytical discussion. The paper does not explore why the proposed ST-GridPool achieves varying degrees of improvement across different benchmarks or tasks.\\\n5.There are some typos in the paper. E.g., the percentage gains reported for \"LLaVA-Video-7B + Ours\" are identical to those reported for \"LLaVA-OneVision-7B + Ours\"."}, "questions": {"value": "1.Is it possible to explore an adaptive mechanism that automatically determine the optimal values of non-trivial hyperparameters in your method?\\\n2.How does the proposed PTG interact with or differ from the built-in temporal reasoning mechanisms already present in modern Video LLMs such as NVILA?\\\n3.Could other unsupervised indicators serve as complementary or alternative signals to the token norm?\\\n4.How to interpret the observed trend of a larger improvement for Long Video Understanding tasks compared to General Video Understanding tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W2EI4Wt3bm", "forum": "MZi9SYPVz5", "replyto": "MZi9SYPVz5", "signatures": ["ICLR.cc/2026/Conference/Submission24698/Reviewer_BLb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24698/Reviewer_BLb5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840007221, "cdate": 1761840007221, "tmdate": 1762943165738, "mdate": 1762943165738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ST-GridPool, a training-free visual token compression method tailored for video understanding with multimodal large language models. ST-GridPool introduces Pyramid Temporal Gridding to construct multi-granularity temporal grids of visual tokens and employs Norm-based Spatial Pooling to further reduce token count while preserving informative regions. The method is evaluated across multiple video understanding benchmarks, demonstrating its effectiveness"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written and well-structured, making it easy to follow.\n* The proposed approach is intuitive and straightforward to implement; its plug-and-play design enhances practical applicability across different MLLMs.\n* In terms of performance, ST-GridPool demonstrates strong potential, consistently outperforming or matching existing token reduction methods across multiple benchmarks."}, "weaknesses": {"value": "* The core idea of ST-GridPool bears strong resemblance to prior training-free methods such as TS-LLaVA (which combines gridding with token sampling) and IG-VLM (which relies solely on image gridding). However, comparisons with these approaches are relegated to the appendix and lack sufficient implementation details. Given that ST-GridPool appears to be a more complex integration of similar components, a more rigorous and fair comparison is warranted, ideally involving hyperparameter tuning for the baselines (e.g., number of grids, token compression ratios, or retained visual token counts) to ensure a level playing field. Without such controlled experiments, it is difficult to assess the true contribution of the proposed design.\n* The efficiency analysis currently only compares LLaVA-Video with and without ST-GridPool. While it is unsurprising that adding a token compression module reduces computational cost, this alone does not demonstrate relative superiority. To substantiate its efficiency claims, ST-GridPool should be benchmarked against other token reduction methods under identical settings. \n* As shown in Table 1, the performance gains from integrating ST-GridPool are sometimes marginal. A more granular breakdown of results, e.g., by question type, video duration, or scene complexity on benchmarks like VideoMME and LongVideoBench, would help identify failure modes or scenarios where the method underperforms. Additional ablation studies and evaluation on more diverse datasets would further strengthen the analysis.\n* The last row of Table 1 appears to be duplicated and should be corrected in the final version."}, "questions": {"value": "Please refer to the weaknesses. I'll update my rating accordingly based on the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t8UaNIxQrn", "forum": "MZi9SYPVz5", "replyto": "MZi9SYPVz5", "signatures": ["ICLR.cc/2026/Conference/Submission24698/Reviewer_35ED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24698/Reviewer_35ED"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982248958, "cdate": 1761982248958, "tmdate": 1762943165503, "mdate": 1762943165503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}