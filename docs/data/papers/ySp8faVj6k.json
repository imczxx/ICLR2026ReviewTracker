{"id": "ySp8faVj6k", "number": 794, "cdate": 1756818363516, "mdate": 1763279377647, "content": {"title": "SAQ: Stabilizer-Aware Quantum Error Correction Decoder", "abstract": "Quantum Error Correction (QEC) decoding faces a fundamental accuracy-\nefficiency tradeoff. Classical methods like Minimum Weight Perfect Matching\n(MWPM) exhibit variable performance across noise models and suffer from poly-\nnomial complexity, while tensor network decoders achieve high accuracy but\nat prohibitively high computational cost. Recent neural decoders reduce com-\nplexity but lack the accuracy needed to compete with computationally expensive\nclassical methods. We introduce SAQ-Decoder, a unified framework combining\ntransformer-based learning with constraint aware post-processing that achieves\nboth near Maximum Likelihood (ML) accuracy and linear scalability. Our ap-\nproach combines a dual-stream transformer architecture that processes syndromes\nand logical information with asymmetric attention patterns, and a novel differ-\nentiable logical loss that directly optimizes Logical Error Rates (LER) through\nsmooth approximations over finite fields. SAQ-Decoder achieves near-optimal\nperformance, with error thresholds of 10.99% (independent noise) and 18.6%\n(depolarizing noise) on toric codes that approach the ML bounds of 11.0% and\n18.9% while outperforming existing neural and classical baselines in accuracy,\ncomplexity, and parameter efficiency. Our findings establish that learned decoders\ncan simultaneously achieve competitive decoding accuracy and computational ef-\nficiency, addressing key requirements for practical fault-tolerant quantum com-\nputing systems.", "tldr": "", "keywords": ["Deep learning", "Quantum Information", "Error Correcting Codes"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f6b1f084297ce8f0c1bd6990838a578905ac63e.pdf", "supplementary_material": "/attachment/afd4ba6a9a0c254ef1f5fd6c66aa25801b647db5.zip"}, "replies": [{"content": {"summary": {"value": "I would like to note that I am not familiar with the quantum computing or quantum machine learning domain, and therefore I do not have the necessary background to properly evaluate this submission. While I can assess general aspects of clarity and structure, I am not qualified to judge the technical novelty, correctness, or significance of the paper within the context of quantum research.\n\nI respectfully suggest that this paper be reassigned to a reviewer with expertise in quantum algorithms or quantum information, to ensure a fair and accurate evaluation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "See Summary"}, "questions": {"value": "See Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "jtlN8dWH3q", "forum": "ySp8faVj6k", "replyto": "ySp8faVj6k", "signatures": ["ICLR.cc/2026/Conference/Submission794/Reviewer_xQqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission794/Reviewer_xQqG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742911972, "cdate": 1761742911972, "tmdate": 1762915606197, "mdate": 1762915606197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAQ-Decoder. It is a neural network approach for QEC that addresses the fundamental accuracy efficiency trade off in QEC decoding. The method combines a dual-stream transformer architecture with a novel differentiable logical loss function and a post-processing algorithm, constraint projected null space descent. The approach achieves near optimal error thresholds while maintaining linear computational complexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  Strong empirical performance. The decoder achieves very good results on toric codes, also approaching maximum bounds while outperforming both classical methods and recent neural approaches.\n2. Novel architectural design. The dual-stream transformer with asymmetric attention patterns is well-motivated for QEC.\n3. Differentiable logical loss. Paper provides a rigorous mathematical derivation of a differentiable approximation to the discrete GF(2) constraints, enabling end to end training that directly optimizes logical error rates rather than bit error rates."}, "weaknesses": {"value": "1. The experiment limited to code distances up to 10. For practical fault-tolerant quantum computing should be larger.\n2. The paper lacks comparison with recent strong baselines. \n3. While the method clams applicability to any stabilizer code family, but experiments are limited to surface codes."}, "questions": {"value": "1. The ablation studies don’t clearly show CPND’s contribution versus simpler projection. Can you provide a direct comparison of logical error rates with/without CPND?\n2. Does decoder require retraining for different physical error models?\n3. What are the convergence guarantees for CPND?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "erN2ot01s3", "forum": "ySp8faVj6k", "replyto": "ySp8faVj6k", "signatures": ["ICLR.cc/2026/Conference/Submission794/Reviewer_wwv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission794/Reviewer_wwv5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749218008, "cdate": 1761749218008, "tmdate": 1762915606028, "mdate": 1762915606028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a transformer-based quantum decoder, called the SAQ-decoder, with several innovative features: dual-stream transformers, a differentiable logical loss, and constraint-aware post-processing. The proposed decoder achieves near-optimal accuracy for topological codes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The dual-stream transformer architecture is intriguing, and the experimental results appear very promising."}, "weaknesses": {"value": "1.\tIt’s unclear what the final output of the proposed decoder actually is. Is it \\hat{l}, \\hat{e}, or the output of the CPND? Based on the appendix, it seems the final output used for evaluating LER is from the CPND, but this should be clarified more clearly in the main text. Also, e^{pred} above Eq. (13) appears to represent the final output, but this is not explicitly stated.\n2.\tTo my knowledge, other transformer-based decoders for QEC already exist, such as alphaQubit. These should be cited, and the differences explained. Additionally, the dual-stream transformer architecture seems similar to CrossMPT for classical codes, which also uses cross-attention. That work is worth referencing as well.\n3.\tThe MLP ϕb_\\phi seems technically identical to the FFN in Kai (2022, PRL), as it takes the syndrome vector and outputs logical information. This similarity should be acknowledged.\n4.\tThe asymmetric structure of the dual streams is not intuitive. Why does the syndrome stream (which contains local information) influence the logical stream (which contains global information)? The opposite direction seems more meaningful. Without empirical results, can the authors illustrate the reasoning behind this design? Moreover, since the syndrome stream is not affected by the logical stream, does this mean the error vector is derived directly from the input syndrome? If so, how does this differ from QECCT?\n5.\tIs the gain over QECCT mainly due to the novel transformer architecture or the post-processing via CPND? To make a fair comparison, the SAQ-decoder should also be compared to QECCT without CPND.\n6.\tThe non-differentiability of the loss function L(e_{\\text{true}} + e_{pred}) has already been addressed by Liu (2019, PRL). Could you explain the difference?\n7.\tThe authors use MWPM as the standard classical decoder. However, BP+OSD is also a well-known and widely used decoder for quantum codes and should be included in the comparison.\n8.\tThe computational complexity is only discussed using Big-O notation. It would be more informative to also provide numerical comparisons, such as FLOPs or inference time.\n9.\tRegarding parameter efficiency, the SAQ-decoder still relies on syndrome information, which scales quadratically with the lattice size LLL. Therefore, I think it cannot avoid quadratic scaling."}, "questions": {"value": "In Fig. 6, are the authors testing the effect of both masking and the global token? Does the \"no mask\" label mean only the global token is used, and \"mask only\" means only masking is applied? Is SAQ-decoder the version with both features? The labeling is somewhat confusing. Also, it would be helpful to include results where neither masking nor the global token is used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y3sYQhClS3", "forum": "ySp8faVj6k", "replyto": "ySp8faVj6k", "signatures": ["ICLR.cc/2026/Conference/Submission794/Reviewer_ehw9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission794/Reviewer_ehw9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896873872, "cdate": 1761896873872, "tmdate": 1762915605872, "mdate": 1762915605872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SAQ-Decoder proposes a stabilizer-aware decoder for Quantum error codes, that combines existing transformer-based decoding approaches with a constraint-inducing post-processing. The key idea is to use a dual-stream representation - one for syndromes and one for logical information. These streams are then processed by a transformer, which is followed by a lightweight constraint-preserving post-processing stage that enforces exact syndrome consistency.\n\nThe architecture attains near-ML thresholds on toric codes, while maintaining linear scaling in syndrome size and strong parameter efficiency. The method outperforms MWPM and prior neural baseline QECCT in both accuracy and scalability. Similar gains hold on rotated surface codes, indicating generalization across codes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well-motivated and seems technically sound, combining learned decoding with constraint-preserving post-processing is a clear and sensible idea.\n\n2. The results shown are strong: the decoder achieves near-ML thresholds on toric and rotated surface codes while maintaining linear scaling and good parameter efficiency.\n\n3. The approach seems to generalize well across different noise models and code families."}, "weaknesses": {"value": "I do not have sufficient background in quantum error correction to fully assess the novelty of the proposed method relative to prior decoders.\nWhile the approach appears reasonable and the empirical performance is impressive, I am unable to evaluate the theoretical aspects of stabilizer enforcement or the loss formulation."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "a2x4YK3LQS", "forum": "ySp8faVj6k", "replyto": "ySp8faVj6k", "signatures": ["ICLR.cc/2026/Conference/Submission794/Reviewer_H6iu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission794/Reviewer_H6iu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958925077, "cdate": 1761958925077, "tmdate": 1762915605477, "mdate": 1762915605477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}