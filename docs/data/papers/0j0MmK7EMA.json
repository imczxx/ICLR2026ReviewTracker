{"id": "0j0MmK7EMA", "number": 13107, "cdate": 1758213653984, "mdate": 1763432099798, "content": {"title": "SimpleFold: Folding Proteins is Simpler than You Think", "abstract": "Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks}. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.", "tldr": "We tackle protein folding as it was a text-to-3D generative model", "keywords": ["Generative models", "protein structure prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31a972b822a8958da262c7bf338ae56d133b793c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SimpleFold, a novel protein folding model based on flow-matching and a standard Transformer architecture. The central thesis challenges the necessity of complex, domain-specific architectural components (like MSAs, explicit pair representations, triangle updates, equivariant modules) prevalent in state-of-the-art models such as AlphaFold2. SimpleFold adopts a generative approach, treating folding as analogous to text-to-image generation, mapping amino acid sequences (encoded via a frozen PLM, ESM2-3B ) to all-atom 3D coordinates using only standard Transformer blocks with adaptive layers conditioned on time. The authors scale SimpleFold up to 3 billion parameters and train it on a large dataset combining PDB experimental structures and approximately 9 million distilled structures . On standard benchmarks (CAMEO22, CASP14), SimpleFold-3B achieves performance competitive with SOTA baselines. Notably, it demonstrates strong performance in ensemble prediction tasks, which is often challenging for models trained with deterministic objectives. The work suggests that scale and general-purpose architectures might suffice for learning complex folding patterns, opening alternative design avenues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core argument—that domain-specific inductive biases might be replaceable by scale and general architectures—is highly stimulating and challenges conventional wisdom in protein folding. If validated further, this could significantly simplify model design in structural biology and beyond.\n- SimpleFold's reliance solely on standard Diffusion Transformer blocks makes the architecture remarkably simple compared to those with attention bias in AF3. This generality facilitates leveraging advances from the broader Transformer ecosystem and potentially simplifies implementation and optimization (like flash-attn and deepspeed).\n- The paper provides a clear demonstration of performance scaling with model size (up to 3B parameters) and training compute on folding benchmarks. This empirical evidence supports the claim that scaling is a viable path for improving performance with this simpler architecture. The scaling with data size is also shown."}, "weaknesses": {"value": "- While competitive, SimpleFold-3B does not consistently surpass the very best models like AlphaFold2, especially on the challenging CASP14 benchmark. This suggests that current domain-specific designs and/or MSA information still provide an edge, particularly for difficult targets.\n- SimpleFold 1.6B and 3B models achieve accuracy comparable to (or on par with) ESMFold. However, their inference speed is noted to be slower. This presents a critical trade-off, and the justification for using these models over ESMFold is unclear given this substantial speed limitation."}, "questions": {"value": "- How frequently do SimpleFold's predictions contain steric clashes or distorted covalent geometries compared to models like AlphaFold2 (before relaxation) or other generative models? Is a relaxation step necessary/used for evaluation?\n- Given the performance gap on CASP14, do the authors believe further scaling (model size, data) can close this gap, or are there fundamental limitations to the purely data-driven approach without explicit biases for certain protein families or topologies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VnYwPDirfB", "forum": "0j0MmK7EMA", "replyto": "0j0MmK7EMA", "signatures": ["ICLR.cc/2026/Conference/Submission13107/Reviewer_WgMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13107/Reviewer_WgMN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908308048, "cdate": 1761908308048, "tmdate": 1762923833360, "mdate": 1762923833360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SimpleFold, a flow-matching based protein folding model that relies solely on general-purpose transformer blocks for efficient structure prediction. In contrast to prior methods that employ triangular updates, explicit pair representations, or multi-objective training, SimpleFold utilizes standard transformer blocks enhanced with adaptive layers and is trained with a generative flow-matching objective augmented by a structural loss term. Evaluated on standard folding benchmarks, the SimpleFold-3B model—trained on both distilled structures and PDB data—achieves performance competitive with state-of-the-art baselines. Additionally, the model demonstrates strong capability in ensemble prediction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tSimpleFold effectively removes complex components commonly used in previous protein folding models—such as MSA processing, pairwise representations, and triangle modules—leading to significant computational speedup while maintaining competitive prediction accuracy.\n\n•\tThe model shows promising results in ensemble generation, highlighting its potential for capturing conformational diversity.\n\n•\tThe paper is clearly structured and easy to follow."}, "weaknesses": {"value": "•\tThe scope of SimpleFold remains limited to single-chain protein structure prediction. Given the emergence of efficient and generalizable predictors such as ESMFold and Protenix-Mini [1]—which are also capable of predicting biomolecular complexes—SimpleFold does not demonstrate a clear advantage in either efficiency or accuracy, raising questions about its practical added value.\n\n•\tThe overall framework constitutes a relatively straightforward integration of existing techniques from computer vision and protein modeling (e.g., flow matching and transformer adaptations), and the core technical innovation appears incremental.\n\n[1] Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM"}, "questions": {"value": "•\tSimpleFold appears to adopt training structures and strategies similar to prior flow-based folding methods such as AlphaFlow or ESMFlow. Could the authors provide further insight or ablation studies to explain why SimpleFold is more effective at recovering multi-state protein conformations? Is this capability attributable to the architectural design, the training objective, or the data used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IXbOZCAd8F", "forum": "0j0MmK7EMA", "replyto": "0j0MmK7EMA", "signatures": ["ICLR.cc/2026/Conference/Submission13107/Reviewer_iwqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13107/Reviewer_iwqT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928284878, "cdate": 1761928284878, "tmdate": 1762923832959, "mdate": 1762923832959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SimpleFold, a flow-matching generative model for protein folding. It uses a standard transformer architecture to generate full-atom structures, conditioned on embeddings from a frozen ESM2-3B protein language model (PLM). This approach notably avoids the explicit pair representations and triangular updates common in models like AlphaFold. The largest 3B parameter model, trained on approximately 9 million distilled and experimental structures, achieves performance that is competitive with, but not superior to, existing state-of-the-art baselines on standard folding and ensemble generation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength is **architectural simplification**. The work demonstrates that a general-purpose transformer can achieve comparable performance to more complex, bespoke architectures, provided it is leveraged at a large scale and conditioned on a powerful pretrained PLM. The clear empirical validation of scaling laws for both model and data size is a useful, albeit expected, finding."}, "weaknesses": {"value": "**Limited Novelty**: The approach is highly derivative. It combines a standard transformer with a known flow-matching objective and, most importantly, relies heavily on the powerful ESM2-3B PLM. These attempts have already appeared in previous work and are not surprising; they cannot be called new folding methods.\n\n**Insignificant Performance**: In the context of the rapidly advancing protein design field, the results are not a significant breakthrough. The model achieves \"competitive\" or \"comparable\" performance, failing to clearly outperform established baselines like AlphaFold2 or ESMFold. This incremental result does not demonstrate a clear advantage."}, "questions": {"value": "See **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eImzOZmgZI", "forum": "0j0MmK7EMA", "replyto": "0j0MmK7EMA", "signatures": ["ICLR.cc/2026/Conference/Submission13107/Reviewer_MgVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13107/Reviewer_MgVM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964681336, "cdate": 1761964681336, "tmdate": 1762923832506, "mdate": 1762923832506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment to All Reviewers"}, "comment": {"value": "To all reviewers:\n\nWe thank reviewers for all their feedback, which we believe will help clarify the value of SimpleFold. Here we summarize the main updates:\n\n- A new section has been added to the appendix with evaluations on de novo and orphan proteins to show the performance of SimpleFold compared to AlphaFold2 and ESMFold. \n- We have answered concerns from reviews regarding novelty and performance of SimpleFold compared with existing approaches. \n\nWe are happy to engage in respectful scientific discussions with all reviewers.\n\nRegards,\n\nAuthors of Submission 13107"}}, "id": "UBCtIMk9Tm", "forum": "0j0MmK7EMA", "replyto": "0j0MmK7EMA", "signatures": ["ICLR.cc/2026/Conference/Submission13107/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13107/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission13107/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763434425240, "cdate": 1763434425240, "tmdate": 1763434425240, "mdate": 1763434425240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Kind request for discussion"}, "comment": {"value": "Dear reviewers, AC and SACs,\n\nWe appreciate your time reviewing our submission. Following reviewers suggestions we have updated our submission and ran new experiments that help clarifying the value of SimpleFold.  We have put substantial effort to complete our rebuttal early in the process and we wanted to send a note to kindly remind you that we are happy and eager to engage in discussions.  \n\nAuthors"}}, "id": "nJka1uxVHt", "forum": "0j0MmK7EMA", "replyto": "0j0MmK7EMA", "signatures": ["ICLR.cc/2026/Conference/Submission13107/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13107/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission13107/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763757321398, "cdate": 1763757321398, "tmdate": 1763757321398, "mdate": 1763757321398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}