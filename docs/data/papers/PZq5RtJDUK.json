{"id": "PZq5RtJDUK", "number": 12594, "cdate": 1758208845314, "mdate": 1763302287670, "content": {"title": "AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment", "abstract": "Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks—distortion detection, distortion analysis, tool selection, and tool execution—coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.", "tldr": "", "keywords": ["Agent", "Image Quality Assessment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a824038da0a5f5a12b875608510774d72690f44e.pdf", "supplementary_material": "/attachment/e11f3f3c885fe778eb6535a05445a14bcd1eebd3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional image quality assessment (IQA) tools. The AgenticIQA framework consists of three agents: a planner, an executor, and a summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool execution, and the summarizer integrates this evidence to produce an IQA score with an accompanying quality explanation. In addition, the authors introduce the AgenticIQA-200K training dataset, a large-scale instruction dataset tailored for IQA agents, as well as AgenticIQA-Eval, a benchmark for assessing the planning, execution, and summarization capabilities of IQA agents. In experiments, AgenticIQA trained on AgenticIQA-200K demonstrates moderate performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Using a continually advancing general-purpose baseline VLM to build an agentic workflow that outputs both IQA scores and their accompanying rationales is a very reasonable approach.\n- A system that provides IQA scores together with explanations can be highly useful in real-world applications.\n- Enabling the use of approximately 30 IQA metrics as tools increases the flexibility of the proposed method."}, "weaknesses": {"value": "At a high level, the proposed AgenticIQA architecture appears reasonable, but there are many aspects in the architectural details and experimental results that are difficult to evaluate positively.\n\nRegarding the distortion detection module\n- The distortion types used (\"Blurs\", \"Color distortions\", \"Compression\", \"Noise\", \"Brightness change\", \"Sharpness\", \"Contrast\") may suffice for qualitative descriptions of image quality, but it has not been thoroughly validated whether they are appropriate enough for zero-shot IQA score prediction.\n\nAbout the performance of IQA Agent (Table 1)\n- The proposed Qwen2.5-VL* model trained on AgenticIQA-200K underperforms the baseline Qwen2.5-VL in the Executor-Tool. How should this be interpreted?\n- Based on the table 1, one would expect the best performance by using InternVL2.5 as the Planner, Q-Instruct and Qwen2.5-VL as the Executor (for Distortion and Tool, respectively), and LLaVA-OneVision as the Summarizer. Is there a specific reason for not adopting this configuration and instead using a single model, Qwen2.5-VL, for the Planner–Executor–Summarizer?\n\nAbout the performance of IQA Scoring (Table 2)\n- A fair comparison is difficult because there is no zero-shot methods.\n- Although the IQA score performance is low, it would be beneficial to leverage the strengths of the proposed method and present a quality reasoning as a qualitative evaluation.\n- The absence of baseline Qwen2.5-VL results makes accurate assessment difficult (it appears in Tables 1 and 3).\n\nAbout the performance of IQA Interpretation (Table 3)\n- The proposed Qwen2.5-VL* shows only a 0.03% improvement over the baseline Qwen2.5-VL, which casts doubt on the effectiveness of the method."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PWRcTUfJnZ", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["ICLR.cc/2026/Conference/Submission12594/Reviewer_2xhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12594/Reviewer_2xhB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663775187, "cdate": 1761663775187, "tmdate": 1762923439765, "mdate": 1762923439765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Alert to Suspected Academic Misconduct"}, "comment": {"value": "I wish to alert all readers and reviewers to the author team's significant suspicion of academic misconduct. Critical concerns persist regarding the usability of their dataset and the reproducibility and authenticity of all quantitative findings. This conclusion is drawn from a thorough and in-depth analysis of their previous work.\n\nThe team's prior publication is: Yu Tian, Yixuan Li, Baoliang Chen, Hanwei Zhu, Shiqi Wang, and Sam Kwong. \"AI-generated Image Quality Assessment in Visual Communication.\" AAAI Conference on Artificial Intelligence, 2025, pp. 7392-7400.\n\nI invite you to refer to https://github.com/ytian73/AIGI-VC/issues/2 for a detailed review of their past research.\n\nWhile I cannot rule out the possibility that the current paper may be an exception, the author team's historical practices raise substantial suspicion of academic misconduct."}}, "id": "paZ8PWbyQG", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["~Vezina_Carlam1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Vezina_Carlam1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763011772104, "cdate": 1763011772104, "tmdate": 1763011772104, "mdate": 1763011772104, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JwVoPf7fBR", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["ICLR.cc/2026/Conference/Submission12594/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763302286888, "cdate": 1763302286888, "tmdate": 1763302286888, "mdate": 1763302286888, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgenticIQA, a novel agentic framework for image quality assessment (IQA) that combines the strengths of traditional score-based IQA methods and VLM-based approaches. AgenticIQA decomposes the IQA process into four subtasks (distortion detection, distortion analysis, tool selection, and tool execution) orchestrated by a planner, executor, and summarizer. The paper also introduces AgenticIQA-200K, a large-scale instruction dataset for training IQA agents, and AgenticIQA-Eval, a benchmark for evaluating VLM-based IQA agents. Experimental results demonstrate that AgenticIQA outperforms strong baselines in both scoring accuracy and explanatory alignment."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Framing IQA as a multi-agent reasoning task is innovative and intellectually coherent with current LLM agent trends.\n2. AgenticIQA-200K and AgenticIQA-Eval are valuable community resources likely to promote reproducibility and future research.\n3. Experiments span human comparison, multiple datasets (TID2013, BID, AGIQA-3K, LLVisionQA), and multiple backbones (GPT-4o, Qwen2.5-VL), demonstrating robustness of the proposed agentic framework.\n4. The paper is generally well-written and organized. The structure and logic are clear, figures are illustrative, and tables are easy to interpret."}, "weaknesses": {"value": "1. The ablation focuses mainly on score fusion (HVS weighting vs. uniform). It would be stronger to also report results when removing specific agents (e.g., planner only / executor only).\n2. Figure 3 shows runtime but lacks comparison to single-pass VLM baselines in terms of efficiency/accuracy trade-off.\n3. The paper focuses on the successes of AgenticIQA. A brief discussion of potential failure cases or limitations would be beneficial."}, "questions": {"value": "In addition to the aforementioned weaknesses, I have two additional questions:\n\n1. Line 304: \"synthetically degrade them with one or two randomly sampled distortions following the protocol of (You et al., 2024a)\" – Please describe the types of distortions used in this synthetic degradation process.\n2. It would be helpful to include a baseline that executes all available tools on the input image and feeds their outputs into the VLM model for prediction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KFWpo7rPpz", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["ICLR.cc/2026/Conference/Submission12594/Reviewer_1cHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12594/Reviewer_1cHU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679730502, "cdate": 1761679730502, "tmdate": 1762923438852, "mdate": 1762923438852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgenticIQA, a novel framework for image quality assessment (IQA) that addresses several limitations in current approaches. Traditional IQA models rely on fixed models that produce scalar quality scores or language-driven models that lack precision. AgenticIQA aims to bridge this gap by integrating traditional IQA tools with vision-language models (VLMs), creating a dynamic, query-aware system that enhances both scoring accuracy and interpretability. The framework operates through three components: planner, executor, and summarizer, which coordinate to detect and analyze distortions, select and apply IQA tools, and provide explanations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the AgenticIQA framework, which combines traditional image quality assessment methods with vision-language models (VLMs) that allow the system to adapt to different queries and images.\n2. The paper introduces the AgenticIQA-200K dataset for training VLMs and the AgenticIQA-Eval benchmark.\n3. The modular design of AgenticIQA allows independent handling of tasks (distortion detection, analysis, tool selection, etc.), offering greater flexibility and scalability compared to traditional integrated systems."}, "weaknesses": {"value": "1. The AgenticIQA system requires significant computational resources due to its use of multiple agents and complex tasks, which may limit its efficiency in practical scenarios.\n2. From the performance of IQA Scoring compared to baseline IQA methods, its performance is somewhat lower than that of other methods.\n3. The paper does not conduct separate ablation studies on the individual modules of AgenticIQA (planner, executor, and summarizer), failing to fully verify the contribution of each module to the overall performance.\n4. While the ablation studies demonstrate the framework's advantages, testing on complex distortion types and real-world scenarios is insufficient.\n5. Besides, the submission format clearly does not meet the ICLR submission requirements, where the margin is too narrow."}, "questions": {"value": "1. For the performance of AgenticIQA in Table 2, it shows lower results on some datasets compared to its counterpart methods. Please clarify it.\n2. Others see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FMTMKfxgzU", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["ICLR.cc/2026/Conference/Submission12594/Reviewer_YsSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12594/Reviewer_YsSd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753643553, "cdate": 1761753643553, "tmdate": 1762923438414, "mdate": 1762923438414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AgenticIQA, a modular agent-based framework for Image Quality Assessment (IQA) that integrates traditional perceptual models with Vision-Language Models (VLMs) in a planner–executor–summarizer architecture. The approach decomposes the IQA process into subtasks (distortion detection, analysis, tool selection, and execution), coordinated by agents that perform reasoning and planning dynamically. The authors also release AgenticIQA-200K, a dataset designed for training and aligning VLMs with IQA-specific reasoning, and AgenticIQA-Eval, a benchmark to evaluate such systems. Experiments show that the framework outperforms both traditional IQA methods and VLM-based systems in accuracy and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel framing – The idea of applying an “agentic” planning-execution-summarization paradigm to IQA is conceptually interesting and relatively unexplored. It takes cues from recent trends in LLM-based agents and adapts them to a low-level visual assessment domain, which is novel.\n\n2. Comprehensive setup – The work is well-engineered: the framework, dataset, and benchmark together form a coherent ecosystem. The paper details how agentic reasoning helps modularize the IQA pipeline, which may inspire further work in interpretable assessment tasks.\n\n3. Strong baselines and evaluation – The experiments are extensive, comparing with both classical (SSIM, LPIPS, DISTS, etc.) and recent VLM-based systems (Q-Instruct, Q-SiT). The authors also evaluate both the agentic reasoning ability (via multiple-choice benchmarks) and the final IQA accuracy.\n\n4. Interpretability – Producing explicit, human-readable explanations is a clear plus, and aligns with the growing emphasis on transparency in perceptual quality tasks.\n\n5. Dataset contribution – AgenticIQA-200K could be a useful community resource, especially if released with clear licensing and reproducibility."}, "weaknesses": {"value": "1. Overly complex for the task – While the agentic formulation is creative, IQA is arguably a simpler regression problem compared to domains that benefit from multi-agent reasoning. The planner–executor–summarizer decomposition might feel over-engineered for predicting perceptual scores, adding unnecessary complexity without clear evidence that planning itself (rather than more data or supervision) provides the gains.\n\n2. Limited interpretability validation – The claim of “interpretable” output is mostly qualitative. There is no human study to confirm that the generated explanations are actually more understandable or faithful to perceptual reasoning.\n\n3. Computation overhead – The multi-agent design increases latency and resource usage substantially (as the paper admits). This could make practical deployment difficult for real-world systems that require fast, consistent predictions.\n\n4. Ablations are incomplete – The ablation analysis is rather shallow. For example, it’s unclear how much each agent contributes to the overall gain. Would a single monolithic model trained on the same data achieve similar results?\n\n5. Marginal quantitative improvement – In some datasets (e.g., AGIQA-3K), the improvement over baselines is modest. The additional architecture and training effort may not be justified by the relatively small gains.\n\n6. Data generation pipeline ambiguity – The AgenticIQA-200K dataset appears to rely heavily on synthetic instructions and outputs generated via GPT-4o. This raises concerns about data originality, annotation quality, and possible circularity in evaluation (since GPT-4o is also used in the experiments)."}, "questions": {"value": "1. How sensitive is the system to the choice of underlying VLM backbone? Does the agentic design generalize if the base model is weaker (e.g., <7B models)?\n\n2. Are the explanations consistent and faithful, or are they post-hoc justifications?\n\n3. Can the agentic decomposition be learned end-to-end rather than hand-coded into planner/executor roles?\n\n4. How does the system handle conflicting cues from different IQA tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B7fxz6GDbH", "forum": "PZq5RtJDUK", "replyto": "PZq5RtJDUK", "signatures": ["ICLR.cc/2026/Conference/Submission12594/Reviewer_GY9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12594/Reviewer_GY9x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919965324, "cdate": 1761919965324, "tmdate": 1762923437963, "mdate": 1762923437963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}