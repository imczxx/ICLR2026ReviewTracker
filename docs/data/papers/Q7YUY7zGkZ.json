{"id": "Q7YUY7zGkZ", "number": 8983, "cdate": 1758105645919, "mdate": 1763290319929, "content": {"title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "abstract": "From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 7\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search.", "tldr": "", "keywords": ["LLM Evaluation", "Info-Seeking Benchmark", "Search Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4f98be6ea2f4fab960e3a5e6a2c4b6047e83471.pdf", "supplementary_material": "/attachment/a06955dc9ec49476eaf161898bae40c978eb0ada.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces WideSearch, a benchmark for broad, multi-entity, verifiable information-seeking by LLM agents. Each task pairs a query with a predefined table schema.\nThe agent is required to 1) identify the complete entity set and 2) fill attributes for each entity using web tools. \nA five-stage, human-in-the-loop curation pipeline ensures difficulty, verifiability, and temporal stability.\nLLM-as-judge is used as an automated evaluator for semantically variable cells. \nExperiments on a few models show very low table-level success, while item-level F1 can be much higher with test-time scaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly scoped evaluation setting: The paper frames `wide information seeking` as distinct from general deep research style tasks, emphasizing breadth, completeness, and structured outputs rather than deep synthesis of a single topic.\n2. Rigorous curation: The five-stage pipeline is thoughtful and practical for benchmark validity.\n3. Hybrid automatic evaluation that mixes exact or approximate rules with LLM-as-judge, including column types and primary keys for deterministic alignment. This is a careful table-centric evaluation."}, "weaknesses": {"value": "1. Although 18 topics are covered, the distribution still shows heavier representation in Business/Finance/Arts & Culture. A sensitivity analysis showing robustness of conclusions across domains/languages would be useful.\n2. To the best of my knowledge, Manus is the only product claiming wide research. Is there any result from that system?"}, "questions": {"value": "1. Fonts are too small for Fig.1 and Fig. 2\n2. In line 476, note that the left quote is wrong.\n3. Too many numbers in Table 1. Could you bold important numbers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FjbJnziJZ8", "forum": "Q7YUY7zGkZ", "replyto": "Q7YUY7zGkZ", "signatures": ["ICLR.cc/2026/Conference/Submission8983/Reviewer_tbLU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8983/Reviewer_tbLU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760536780120, "cdate": 1760536780120, "tmdate": 1762920713593, "mdate": 1762920713593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WideSearch, a benchmark targeting “agentic broad information-seeking” tasks where an LLM agent must find a complete set of entities and fill a structured table from the live web (e.g., “all DOJ attorney openings in a date window”). The dataset contains 200 manually curated questions (100 EN / 100 ZH) spanning 15 domains, built with a five-stage curation pipeline to ensure breadth, verifiability, and temporal stability, and scored with a hybrid, table-alignment evaluation (rule checks + LLM-as-judge). Benchmarking 10 systems shows very low table-level success (most ≈0%; best single/multi-agent ≈7%), while humans achieve ~20% in single-annotator mode; the gap stems from strict completeness requirements rather than difficulty of single facts (item-level F1 can be high with retries). Multi-agent frameworks improve F1 via divide-and-conquer but still rarely achieve perfect tables, highlighting deficiencies in planning, reflection, and evidence use."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, it's a very good work that extends the frontiers of evaluating newly emerged deep research systems and tools.\n\n1. This paper proposes a novel dataset aimed at breadth and completeness across many entities. Rigorous five-stage human-centered curation ensures realistic and verifiable tasks.\n\n2. Results expose a fundamental limitation of current agents—completeness at scale—and show multi-agent setups help but don’t solve it, offering a concrete target for future research."}, "weaknesses": {"value": "1. The evaluation protocol might not be robust enough. This paper uses markdown format as a protocol, applies several fuzzy or exact matches to number/date/urls, and finally applies LLM-as-a-judge to evaluate complex answers. This paradigm replies on pre-crafted ground truths and would be sensitive to those queries whose answer might change with time (for example, the top-selling electrical toothbrush in the past week).\n\n2. The current success criterion may be too brittle, which requires perfect table equality; small, arguably negligible deviations lead to failure, which can conflate usability with strict exactness.\n\n3. Tasks are claimed “temporally invariant”, yet the benchmark relies on the live web; concrete procedures for refreshing ground truths and handling site changes are under-specified."}, "questions": {"value": "1. What exactly do the source queries come from? Are they purely curated by human annotators or modified from existing data sources (like NaturalQuestions)? Are there any time-sensitive queries (for example, the top-selling electrical toothbrush in the past week)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fod8ezDweB", "forum": "Q7YUY7zGkZ", "replyto": "Q7YUY7zGkZ", "signatures": ["ICLR.cc/2026/Conference/Submission8983/Reviewer_29pm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8983/Reviewer_29pm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691027190, "cdate": 1761691027190, "tmdate": 1762920713254, "mdate": 1762920713254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WideSearch, a benchmark designed to evaluate search agents on broad information-seeking tasks, focusing on collecting atomic factual data across many entities that are objectively verifiable. The benchmark comprises 200 manually curated tasks (100 English, 100 Chinese). It also evalutes a broad set of LLMs and the LMs under agent frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark is unique from previous ones, which extend the commonly used browsecomp-style simple-answer questions to broad information gathering."}, "weaknesses": {"value": "- Most importantly, even though the paper's motivation is very clear and reasonable, it remains a straightforward extension to the existing browsecomp-style benchmarks (from simple fact to a list of fact). Despite its potential usefulness (given the success of browsecomp), it's hard to admire from a research perspective. Also, it's still constrained to this very specific type of tasks, for the convenience of evaluation.\n\n- Regarding the experiments: \n  - 1) the tasks appear to be too challenging for simple LLMs (with searches) because they usually only execute very limited search steps. This makes related analysis trivial to know. And it's also not necessary to call them as \"single agent\" \n    2) The tasks appear to be more suitable for Deep Research agents such as Deep Research from OpenAI/Gemini/Grok/Qwen/Kimi/.... But according to the content in the appendix, the evaluted systems are more or less from self-implemented agent scaffoldings, while leaving those important frontier deep research systems omitted? The paper could be much more benefited by including experiments on more frontier agents and their analysis.\n\n- Writing and experiments: might as well include more details in the main content."}, "questions": {"value": "- It's suggested to include a table of comparison to the recent deep research benchmarks on key dimensions.\n- The paper would be benefited by including more evaluated agents, and at least OpenAI Deep Research, which is usually the leading agent across recent benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZCRZm6r8Jq", "forum": "Q7YUY7zGkZ", "replyto": "Q7YUY7zGkZ", "signatures": ["ICLR.cc/2026/Conference/Submission8983/Reviewer_dPH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8983/Reviewer_dPH5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979286700, "cdate": 1761979286700, "tmdate": 1762920712857, "mdate": 1762920712857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WideSearch, a benchmark designed to evaluate the reliability and comprehensiveness of LLM-based search agents in large-scale information-gathering tasks. WideSearch assesses agents on tasks requiring exhaustive and structured collection of numerous atomic facts across multiple entities. The benchmark consists of 200 manually curated queries (100 English, 100 Chinese) spanning 18 diverse domains, each paired with a well-defined schema and gold-standard table. \n\nA five-stage human-in-the-loop curation pipeline ensures that all tasks are complex, objectively verifiable, and reliant on real web tool usage. The evaluation framework integrates rule-based scoring with LLM-as-a-judge scoring, achieving over 97.8% agreement with human judgment.\n\nThe authors further perform an extensive benchmarking study across state-of-the-art agentic systems, revealing severe limitations in current models—particularly in maintaining completeness and fidelity at scale—highlighting the urgent need for more advanced planning, reflection, and evidence-grounding capabilities in next-generation agents."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper tackles an underexplored yet practically crucial dimension of agent evaluation—broad, high-fidelity information gathering—which complements existing reasoning- and synthesis-oriented benchmarks. The conceptual framing of WideSearch as the “breadth” counterpart to DeepSearch and DeepResearch is clear, coherent, and well-motivated.\n\n(2) The five-stage human-in-the-loop curation pipeline is rigorous and systematic, ensuring that all tasks are complex, verifiable, and genuinely dependent on real web search. The inclusion of both English and Chinese queries further enhances the benchmark’s generality and cross-lingual coverage.\n\n(3) The evaluation and analysis are comprehensive and insightful. The experiments benchmark over ten single-agent, multi-agent, and end-to-end commercial systems using multiple aggregation metrics (Avg@N, Pass@N, Max@N), providing a well-rounded assessment of agent reliability at scale. The error taxonomy—separating advanced agentic failures (planning, reflection, evidence use) from basic operational errors—is particularly insightful. The test-time scaling analysis further isolates completeness, rather than retrieval accuracy, as the key bottleneck, offering a concrete and data-driven direction for future work."}, "weaknesses": {"value": "The benchmark construction and evaluation are solid in general. It would be great if the authors could further conduct quantitative analysis regarding the major challenges and failure patterns mentioned in sections 4.1 and 4.2."}, "questions": {"value": "(1) The paper reports 97.8% agreement between LLM-as-judge and human evaluation, which is strong. Could you provide more details on the specific disagreement cases? It would be great if the authors could provide more insights into this. \n\n(2) The benchmark relies on live web data. How do you plan to ensure its stability over time? Is there a plan for versioning or periodic re-validation as the web content evolves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TIoNTP5RPZ", "forum": "Q7YUY7zGkZ", "replyto": "Q7YUY7zGkZ", "signatures": ["ICLR.cc/2026/Conference/Submission8983/Reviewer_kg32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8983/Reviewer_kg32"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244919566, "cdate": 1762244919566, "tmdate": 1762920712471, "mdate": 1762920712471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to reviewers, ACs, SACs, and PCs"}, "comment": {"value": "In response to the specific comments from the reviewers, we have provided corresponding answers and made certain revisions to the paper. The revised parts are highlighted in blue, as detailed below:\n\n1.We have enlarged Figure 1 and Figure 2 to present the content more clearly.\n\n2.We have emphasized the key numbers in Table 1: the highest and the second-highest values are displayed in bold and underlined.\n\n3.We have added a discussion in Section 3.3 regarding the inconsistencies between LLM-judge evaluations and human evaluations.\n\n4.We have added a discussion in Section 4.1 about the failure modes of advanced agent capabilities.\n\n5.We have added a quantitative analysis of basic failure modes in Section 4.2 and Table 7.\n\n6.We have added a detailed comparison of different benchmarks in Section 6.1 and Table 5."}}, "id": "wm7Bg3RIBb", "forum": "Q7YUY7zGkZ", "replyto": "Q7YUY7zGkZ", "signatures": ["ICLR.cc/2026/Conference/Submission8983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8983/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission8983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763290088063, "cdate": 1763290088063, "tmdate": 1763290088063, "mdate": 1763290088063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}