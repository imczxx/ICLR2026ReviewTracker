{"id": "kt51kZH4aG", "number": 17755, "cdate": 1758280150410, "mdate": 1759897155924, "content": {"title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model", "abstract": "Successful generalist Vision-Language-Action (VLA) models that rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders with an enhanced encoding pipeline, enjoying both scalability and simplicity. Evaluated across 6 simulation environments as well as 3 real-world robotics platforms, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves state-of-the-art performance over a sweep of benchmark suites, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks.", "tldr": "", "keywords": ["robotics; vision language action model; prompt learning; heterogeneous pretrainining"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b74d5d61a9cda8a71eeb3fca50d7e328950f3830.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the significant challenge of training a single, generalist Vision-Language-Action (VLA) model on large-scale, heterogeneous, and cross-embodiment robotic datasets. The authors propose **X-VLA**, a flow-matching-based VLA architecture. The core contribution is a **\"Soft Prompt\" mechanism**, which assigns a unique set of learnable embeddings to each distinct data source (i.e., hardware configuration). These prompts condition a standard Transformer backbone, enabling it to handle the heterogeneity arising from different embodiments while learning a generalist policy. The model is trained using a two-phase pretraining and domain adaptation strategy. The authors demonstrate that their 0.9B parameter model, X-VLA-0.9B, achieves new state-of-the-art (SOTA) performance across a comprehensive suite of six simulation benchmarks and three real-world robot platforms , and shows remarkable parameter-efficient finetuning (PEFT) capabilities."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Clear Problem, Elegant Solution:** The paper tackles a critical and timely problem in large-scale robotics: how to effectively leverage diverse, heterogeneous datasets. The proposed \"Soft Prompt\" mechanism  is an elegant and simple solution that integrates cleanly into a standard Transformer architecture. This approach is more deeply integrated than using domain-specific action heads (Fig 2a)  and more flexible and scalable than relying on hand-crafted language prompts (Fig 2c).\n\n2.  **Exceptional Empirical Results:** This is the paper's most significant strength. The evaluation is both comprehensive and highly convincing.\n    * The model achieves SOTA results on an impressively broad set of **6 simulation benchmarks** (Libero, Simpler, VLABench, RoboTwin-2.0, Calvin, and NAVSIM). The performance gains shown in Table 2 are substantial (e.g., 95.8% on Simpler-WidowX vs. 71.9% prior SOTA; 87.3% on NAVSIM vs. 81.7% prior SOTA).\n    * The work is validated on **3 distinct real-world robotic platforms** (WidowX, Agilex, AIRBOT), demonstrating strong real-world applicability. This includes impressive performance on a challenging dexterous cloth-folding task.\n    * The introduction and planned release of the new **\"Soft-Fold\" dataset** for dexterous manipulation is a valuable contribution to the community.\n\n3.  **Strong Adaptation and Parameter Efficiency:** The parameter-efficient finetuning (PEFT) results are a key highlight. The ability to achieve performance comparable to a fully finetuned 3B parameter model ($\\pi_{0}$) while tuning only 9M parameters (1% of the 0.9B model) is a powerful demonstration of the model's capabilities. This strongly supports the claim that the backbone learns a truly \"embodiment-agnostic\" generalist policy.\n\n4.  **Thorough Analysis and Ablations:** The paper does an excellent job of justifying its design choices.\n    * Figure 4 provides a direct comparison against other methods for handling heterogeneity, showing that soft prompts lead to more stable training and lower validation error.\n    * Table 1 offers a clear ablation path, demonstrating the positive contribution of each key component (e.g., soft-prompt, scaling up, two-step adaptation).\n    * The scaling plots in Figure 5 are promising, showing consistent improvement with model size, data diversity, and data volume, with no sign of saturation.\n    * The T-SNE visualization in Figure 8 provides compelling qualitative evidence that the soft prompts are successfully capturing meaningful, embodiment-specific information."}, "weaknesses": {"value": "1.  **Limited Methodological Novelty of Soft Prompts:** The core mechanism, \"Soft Prompting,\" is not a new invention. It is a well-established technique from the NLP community (e.g., Lester et al., 2021)  and has been used in multi-task and multi-domain learning , as the paper acknowledges. The novelty here lies in its *application* to the *cross-embodiment robotics* domain. While the *results* are novel and SOTA, the *methodological* leap is more of an effective and well-executed application rather than a new fundamental mechanism.\n\n2.  **Clarity on Soft Prompt Implementation:** Key details about the soft prompt's architecture are not fully clear in the main text. Figure 1 shows them feeding into the Transformer block, but the exact mechanism (e.g., concatenation, addition) is ambiguous. This is clarified in Appendix C / Figure 10, which shows the \"Soft Prompt\" tokens are concatenated with the multi-modal tokens and control tokens. Moving this crucial implementation detail from the appendix to the main methodology section would significantly improve the paper's clarity and self-containedness.\n\n3.  **Ambiguity in T-SNE Visualization:** The T-SNE plot in Figure 8  is a key piece of qualitative evidence, but it's not entirely clear what is being plotted. The legend lists 7 data sources , but there are many dots for each source. Given that Figure 5 (center) explores \"Prompt Length\", it's plausible that the prompt is a *sequence* of tokens. If so, does each dot in the T-SNE plot represent a single token from that sequence? Clarifying this is important for correct interpretation."}, "questions": {"value": "1.  The soft prompt is described as \"a set of learnable embeddings\" , and Figure 5 (center) explores \"Prompt Length\". Does this confirm that each \"soft prompt\" is a *sequence* of learnable tokens (e.g., a matrix of size $L \\times D_{model}$)? If so, what prompt length was used for the final X-VLA-0.9B model?\n\n2.  Regarding the T-SNE visualization in Figure 8: What do the individual dots represent? If the prompt is a sequence of tokens, does each dot represent one token from that prompt sequence, for its corresponding data source?\n\n3.  The two-step adaptation procedure  involves (1) prompt warm-up and (2) joint policy adaptation. The PEFT experiments (Table 3)  use LoRA. Is this LoRA applied *in addition* to the two-step prompt tuning? As a baseline, did the authors try *only* tuning the new soft prompt (while keeping the entire 0.9B backbone frozen), which would be a more standard \"prompt tuning\" PEFT approach?\n\n4.  In the architecture (Section 4.1) , the paper mentions disentangling streams, with a pretrained VLM for the main view/language and a \"shared vision backbone\" for auxiliary views. Is this \"shared vision backbone\" the *same* vision encoder from the VLM, or is it a separate vision encoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8E1Jv8ZAmj", "forum": "kt51kZH4aG", "replyto": "kt51kZH4aG", "signatures": ["ICLR.cc/2026/Conference/Submission17755/Reviewer_F3Do"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17755/Reviewer_F3Do"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923983479, "cdate": 1761923983479, "tmdate": 1762927597446, "mdate": 1762927597446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes X-VLA, a vision-language-action model that leverages \"soft prompts\" (learnable embeddings of heterogeneous hardware platforms) to effectively address heterogeneity across different robot platforms when learning multi-task policies. The method learns domain-specific soft prompt embeddings that encode individual embodiments, with a shared VLM backbone that utilizes the soft prompts to ultimately generate high-quality actions across multiple embodiments and tasks. The model is pretrained on 290K demonstrations from seven robot platforms that include both single-arm and dual-arm manipulation setups. Experimental evaluations show strong, state-of-the-art performance across the benchmarks, which include LIBERO, SIMPLER, CALVIN, and others."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper conducts extensive and thorough experimental evaluation which includes six simulation benchmarks and three real-world robotic platforms. A variety of prior methods are compared against, including $\\pi_0$, OpenVLA, Octo, ACT, and several other VLAs (UniVLA, GR00T-N1, FLOWER, UniVLA, etc.).\n* The results across all settings are consistently strong, nearly outperforming all other prior methods and even matching $\\pi_0$ (3B parameters) with a PEFT version that use only 9M trainable parameters.\n* The experimental analysis on various components of X-VLA is comprehensive, with various ablations and a study on scaling behavior across model and dataset size. Further, the qualitative and quantitative discussions on the effects of the learned soft prompts are informative and support the claims that the author makes regarding the ability to encode heterogeneous morphologies in a shared structured latent space.\n* The paper also introduces the Soft-Fold bimanual cloth folding dataset with 1,200 demonstrations.\n* The uncut video on the project website showcasing two hours of t-shirt folding demonstrates impressive, robust performance of X-VLA across an extended period of time.\n* I appreciate the transparent discussion of failed attempts to address heterogeneity across mixed data sources presented in Appendix E. Reporting unsuccessful attempts may provide useful information to readers."}, "weaknesses": {"value": "* Soft prompts, the core contribution of the paper, is borrowed from prior natural language processing research (Lester et al., 2021), which suggests limited technical novelty. While the application of soft prompts to vision-language-action models in robotics is novel, the paper reads more like an engineering project that obtained a strong model through various careful techniques employed during training.\n* Some important details are missing: How many tokens are used in the soft prompts, how is the number chosen, and how does performance vary with respect to the number of tokens used in the soft prompt? In addition, how is the temporal downsampling rate for pretraining chosen, and how does that affect policy performance? Further, what effect does the prompt warm-up phase discussed in Section 4.2.1 have on final performance? The paper would be strengthened by additional discussions addressing these questions."}, "questions": {"value": "* How does X-VLA perform under OOD generalization settings?\n* When does X-VLA fail during deployment, and what do the failure modes look like qualitatively?\n* Can a soft prompt be used for zero-shot deployment on an unseen robot if the soft prompt was trained for a seen robot that is very similar to the new robot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ojWNXilMM", "forum": "kt51kZH4aG", "replyto": "kt51kZH4aG", "signatures": ["ICLR.cc/2026/Conference/Submission17755/Reviewer_R55g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17755/Reviewer_R55g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989021116, "cdate": 1761989021116, "tmdate": 1762927596887, "mdate": 1762927596887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes X-VLA policy for learning across different robot setups using soft prompts for each data source. The pipeline is trained with a flow-matching objective for action chunk generation. The training involves first pretraining on ~290k episodes across seven setups, then prompt warm-up with frozen backbone for new embodiment and finally joint PEFT (LoRA) that tunes ~1% of parameters. The evaluation shows strong performance across six simulation benchmarks and three real-robot platforms. The learned soft prompts cluster based on hardware in t-SNE plot."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of soft prompts is motivated well, and is shown to be better than the language templates and HPT-style projections. \nThe overall proposal seems a scalable solution, as it is based on standard transformer with shared params for encoders and validation errors suggest room to grow. \nThe paper explores how reduced LR leads to better performance for their experiments and report consistent gains in performance across the simulated benchmarks."}, "weaknesses": {"value": "Prompts are queried by dataset/hardware ID; this assumes reliable domain labels during training and may risk overfitting to source IDs unless carefully regularized. There are no experimental evidence of zero-shot generalization to unseen tasks with known robots and unseen robots.\n\nWhile the experiment results has a wide coverage, the current focus is on tabletop tasks with focus on the end-effector movements, and do not require much reasoning of the differences in the embodiment. It is unclear if the proprioceptive states outputs are actually needed for the evaluation tasks.   \n\nThe results use PEFT with 1% tunable params that is promising for on-robot finetuning, but this has been compared on quite saturated Libero benchmark. The results reported have no error bars or confidence intervals."}, "questions": {"value": "What does the prefix \"high-dimensional\" observation stream and \"low-dimensional\" proprioceptive–action stream mean? Can you clarify what is it in comparison to, like does it have more frames for history or multi-perspective viewing? How big are the \"low-dimensional\" proprioceptive–action vectors, and how do they differ with different embodiments?\n\nAre the results statistically significant? \n\nHow useful are the learned soft prompts? While the paper provides a t-SNE plot to show how embeddings cluster by hardware, it is unclear how important they are for reusing in a new instance of a task. For example, if a soft prompt embedding is used with another task same embodiment or another embodiment, what is the performance degradation on the task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1R2u91oZCq", "forum": "kt51kZH4aG", "replyto": "kt51kZH4aG", "signatures": ["ICLR.cc/2026/Conference/Submission17755/Reviewer_PCNL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17755/Reviewer_PCNL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033515493, "cdate": 1762033515493, "tmdate": 1762927596353, "mdate": 1762927596353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}