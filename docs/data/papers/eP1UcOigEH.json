{"id": "eP1UcOigEH", "number": 584, "cdate": 1756750452856, "mdate": 1759898252206, "content": {"title": "Learning Fine-grained Parameter Sharing via Sparse Tensor Decomposition", "abstract": "Large neural networks attain cutting-edge performance on many tasks, yet their sheer size hinders deployment on resource-constrained devices. Among existing compression approaches, parameter sharing remains relatively unexplored. In this paper, we introduce Fine-grained Parameter Sharing (FiPS), a unified compression framework that combines parameter sharing, tensor decomposition, and sparsity for achieving optimal compression. FiPS compresses transformers by factorizing MLPs concatenated across layers into a shared low-rank basis with sparse, layer-specific projection matrices. Both components are initialized by singular-value decomposition (SVD) and jointly optimized with block-wise reconstruction error minimization. As a result, FiPS enables compression of a variety of Vision Transformers (ViTs) and Large Language Models (LLMs) by 20–50% with negligible degradation in quality. Finally, we combine FiPS with Quantization Aware Training (QAT) to obtain state-of-the-art compression results on GEMMA-2 models. These results establish fine-grained parameter sharing as a practical route to compact, high-performance transformer models.", "tldr": "FiPS is a sparsity-driven parameter-sharing method that combines shared bases, sparse factors, and quantization-aware training to compress ViTs and LLMs substantially while preserving accuracy and perplexity.", "keywords": ["Deep Learning", "Model Compression", "Efficient Deep Learning", "Parameter Sharing", "Sparsity", "Tensor Decomposition", "Quantization", "Quantization Aware Training", "QAT", "Vision Transformers", "ViT", "DeiT-B", "SWIN-L", "Large Language Models", "LLM", "Gemma2", "LLaMa3.1", "LLaMa"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23c96fe5980134d2f28241fc09dd355558509787.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a fine-grained parameter sharing (FiPS) technique for efficient deployment of large Transformer networks. A singular value decomposition is performed on a concatenation of multiple weight matrices to identify the shared bases and coefficients, followed by a sparsification process to further reduce the number of parameters. A weight matrix is then expressed by a sparse combination of shared bases. The parameter sharing strategy is demonstrated with experimental results on vision models and language models, achieving substantial parameter reduction with minimal accuracy loss."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides thorough analyses of the method selections, which are clearly stated and easy to follow. In Section 2 and 3, several options for the joint SVD, pruning, and weight tying were presented, followed by intuitive and representative analyses that support the choices that were made in the paper.\n2. The ImageNet-1k accuracy in Table 1 is well preserved even when the parameter budget is low. `FiPS` also improved the accuracy of LLM from the non-parameter-shared low-rank model.\n3. The writing was clear and easy to follow."}, "weaknesses": {"value": "I lean toward rejection because (1) the paper is missing an important baseline, (2) the novelty of the proposed method is limited, and (3) the experimental results are not strong enough to overcome the aforementioned limitations.\n\nRegarding (1) and (2), the paper should discuss `Basis Sharing` (Wang et al., 2025) and compare `FiPS` with it in the experimental results. `Basis Sharing` also studied a fine-grained parameter sharing technique based on the acativation-aware joint SVD. Although there are some technical differences between `FiPS` and `Basis Sharing`, the core ideas of two works are almost identical. Also, I did not find any points that make `FiPS` more effective or practical than `Basis Sharing` when factorizing the weights into shared bases and individual coefficients. For example, in the activation-aware factorization, `Basis Sharing` uses the Cholesky decomposition of the feature matrix, whereas `FiPS` factorizes the activation via gradient descent, which is much slower.\n\nI acknowledge that sparsifying the decomposed matrices can be thought as a technical contribution of `FiPS`. That said, it is unconvincing that pruning the SVD results alone justifies acceptance of the paper because (1) the idea of sparse singular vectors or principal components were already well studied (e.g., SparsePCA (Zou et al., 2006)), and (2) I did not find any technical challenges or theoretical/practical advancements presented in this paper when sparsifying the SVD results. The GMP method was chosen heuristically based on an analysis, which the community may appreciate, but again, it is not a significant enough contribution to justify acceptance. Similar arguments hold for QAT.\n\nMoreover, due to the absence of comparison between `FiPS` and `Basis Sharing` , it is unclear whether `FiPS` produces a significant accuracy improvement compared to `Basis Sharing`. It is also questionable whether the sparse bases are more practical than the non-sparse version, since `FiPS` did not accelerate the network on a GPU when the batch size is one. \n\nIn addition to (Wang et al., 2025), the paper is missing many of recent works, including (David-Hay and Wolf, 2024; Bae et al., 2024), which could help position the paper in the literature.\n\n* Wang et al. \"Basis sharing: Cross-layer parameter sharing for large language model compression.\" ICLR 2025.\n\n* Zou et al. \"Sparse principal component analysis.\" Journal of Computational and Graphical Statistics 15.2 (2006).\n\n* David-Hay and Wolf. \"Dynamic layer tying for parameter-efficient transformers.\" ICLR 2024.\n\n* Bae et al. \"Relaxed recursive transformers: Effective parameter sharing with layer-wise lora.\" ICLR 2024."}, "questions": {"value": "1. In Section 4.2, what sparsity pattern was used for the language model compression? Was it also 2:4 sparsity?\n2. How much speedup can `FiPS` produce when it is applied to the Llama and the Gemma models on a GPU?\n3. How much accuracy / speedup gain can `FiPS` make from `Basis Sharing`? Also, is there any significant contribution made in this paper from `Basis Sharing`? Please discuss any items that should be highlighted.\n\nMinor comment:\n\n- Mixing the parameter budget and the compression ratio is somewhat confusing and might cause unnecessary misunderstandings—consider using only the compression ratio in the main text and give the MLP-specific parameter budget in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bZBoMJ8a85", "forum": "eP1UcOigEH", "replyto": "eP1UcOigEH", "signatures": ["ICLR.cc/2026/Conference/Submission584/Reviewer_JGJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission584/Reviewer_JGJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761185599541, "cdate": 1761185599541, "tmdate": 1762915553430, "mdate": 1762915553430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Fine-grained Parameter Sharing (FiPS), a compression framework that combines parameter sharing, tensor decompositions, and sparsity to achieve efficient model compression. The technique can be summarized using Figures 1 and 5: the fully connected (FC) layers of the MLP blocks across different layers are concatenated and compressed using truncated SVD. After obtaining the initial shared factors, these factors are optimized using a calibration dataset while enforcing sparsity in the projection matrices. An optional final step allows fine-tuning the compressed model to recover any lost performance. FiPS demonstrates superior performance compared to baselines while remaining both memory- and computation-efficient."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The major strength of this paper is its performance. It significantly outperforms the baselines across a wide range of compression ratios and on multiple network architectures, including ViTs. They also demonstrate that FiP effectively compresses models while enhancing both memory efficiency and computational speed."}, "weaknesses": {"value": "I believe one of the major weaknesses of this paper is its organization and presentation. In its current form, the paper is somewhat difficult to follow. I highlight several examples below:\n\n- The subsections in Section 2 feel orthogonal to one another, and it is unclear how they connect conceptually. The section transitions abruptly from sparsity induction in the factors, to optimal weight concatenation, to parameter sharing. In addition, Line 137 begins with “we investigate parameter sharing across multiple layers…,” and the next section is titled “Parameter Sharing Across Layers,” which makes the flow misleading.\n- Lines 225–226 refer to Net2Net, but do not provide any explanation of what Net2Net is. Since the comparison is used to explain how growing the factor matrices is similar but different from Net2Net, a brief description would help the reader understand the relevance.\n- Figure 5 is cited twice in the main manuscript, but the figure itself appears only in the appendix, which disrupts the reader’s ability to interpret the discussion.\n\nWhile the experimental results are strong, the methodology would be difficult for others to adopt if the main algorithmic choices are not presented clearly. I suggest reorganizing the paper to introduce the high-level design decisions first (e.g., sparsity induction in $U$, $V$, or both; the choice of full long-axis concatenation, etc.) and then move the reasoning and supporting analysis for each decision to the appendix."}, "questions": {"value": "- The authors also suggest that this approach could naturally extend to attention layers. Is there a particular reason why the paper focuses on compressing only the MLP layers? Are MLP layers inherently more compressible? I may have missed it, but I did not see this choice explained clearly in the manuscript.\n- There are now many compression techniques, particularly those that use structured matrices such as Monarch or BLAST [1]. Is there a reason why the authors chose the specific baselines used in the paper and did not compare against methods based on structured matrices?\n\n---\n\n[1] \"BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference\", Changwoo Lee, Soo Min Kwon, Qing Qu, Hun-Seok Kim. NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wAGXmSMyT3", "forum": "eP1UcOigEH", "replyto": "eP1UcOigEH", "signatures": ["ICLR.cc/2026/Conference/Submission584/Reviewer_qcdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission584/Reviewer_qcdY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882388564, "cdate": 1761882388564, "tmdate": 1762915553217, "mdate": 1762915553217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed to compress the MLP (also known as FFN) modules in transformer-based models through grouping and sharing parameters between adjacent layers. The weights in shared layers are concatenated, decomposed (by SVD), and then sliced accordingly to form the initial shared base U and layer-specific projection matrix Vi. Furthermore, U and Vi are optimized using the activations X collected from a small calibration set to minimize ||WX - UVX||. Sparsity will also be enforced into Vi at this stage. For high compression ratio cases, an end-to-end fine-tuning may be needed to recover performance lost. This proposed method could be combined with QAT, as demonstrated in Table 4."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. perform experiments with both vision and LLM models.\n2. provide useful supplemental information, e.g. error increases with depth at fixed rank (Fig. 3a) hence the adaptive sparsity along depth (Fig. 4b), also the impact of SVD initialization vs random initialization for U and Vi."}, "weaknesses": {"value": "**1. Estimate of potential speed-up at inference stage**\n\nAs illustrated by Eq. 1, this method at inference stage will replace a W@X matrix multiplication with a U@V@X computation. The shape of these operations are [d, p] @ [p, seq_len] and [d, r] @ [r, p] @ [p, seq_len], respectively, hence, the corresponding FLOPs are *(d x p x seq_len)* and *(d x r x p + d x p x seq_len)*. Even though the first matmul could be HW-accelerated due to the sparsity in V, the total number of operations seems to be greater than the original operation. Author may want to add a brief paragraph before Section 4 to clarify the inference benefit of this proposed method.\n\n\n**2. FiPS combined with QAT may not be the best option for reduced precision**\n\nDue to the constraint in computation resources, modern LLMs are rarely QAT'ed in practice. To better assess the compatibility of the proposed method with reduced precision, it would make more sense to utilize popular post-training quantization methods like GPTQ (aka OPTQ) or GPTQv2 (aka GPTAQ) or similar economic PTQ methods in Table 4. More importantly, Table 4 only shows that FiPS + QAT at 8x compression is not as bad as INT2. But it still doesn't demonstrate that FiPS + quantization is a viable approach. Maybe an example with decent perplexity at 4X compression would be closer to what author intended to highlight.\n\nIn addition, if Table 4 intended to demonstrate the effectiveness of FiPS at high compression ratio (when combined with quantization), it should also benchmark with quantized model + LoRA or SVD compensation approaches, e.g. EoRA and CLoQ. Those methods are known to provide very good performance under high compression ratio. At least a brief discussion/comparison of those methods should be included in Section 4.2 QAT paragraph to provide a clearer picture for the readers.\n\n\n**3. Typo that causes confusion**\n\nOn Line 368, author stated that the results in Table 3 is using \"a 20% compression ratio\", which means the model size is reduced to 80% of the original size, according to Line 101. However, the description of Table 3 states \"...evaluate LLama-7b and LLama-3.1-8b at 20% of their size...\" Based on the content of the table, readers may be able to infer that the correct model size should be at 80%. But this inconsistency could give a misunderstanding of the model performance at first glance. Please correct this typo, if that's the case. \n\nother minor typos or missing info:\n- Line 368, Table 3 instead of Table 4.2\n- Line 373, Table 4 instead of Table 4.2.\n- Line 350, \"... all FC layers from MLPs within the same group are concatenated.\" but didn't mention the grouping was treated as a hyperparameter and should refer to Appendix A.7."}, "questions": {"value": "please see weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b1pEnN9qgd", "forum": "eP1UcOigEH", "replyto": "eP1UcOigEH", "signatures": ["ICLR.cc/2026/Conference/Submission584/Reviewer_FZad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission584/Reviewer_FZad"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164648455, "cdate": 1762164648455, "tmdate": 1762915553056, "mdate": 1762915553056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Fine-grained Parameter Sharing (FiPS)**, a transformer compression framework combining low-rank factorization, inter-layer parameter sharing, and sparsity. The authors group MLP parameters across layers, perform SVD truncation to obtain a shared low-rank basis $U$ and sparse, weight-specific projection matrices $V_i$, and then fine-tune them to match the original model’s activations while enforcing sparsity on $V_i$. This design promotes efficient weight reuse across layers and reduces redundancy with minimal loss in representation quality. The method is evaluated on both **Vision Transformers** and **LLM tasks**, showing favorable compression–accuracy trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths:**\n\n* The method is easy to follow, and the authors explain the technique clearly with sufficient technical detail and a dedicated algorithm section.\n* The authors evaluated their approach on both vision and language tasks, although on limited baselines."}, "weaknesses": {"value": "**Major Weaknesses:**\n\n* The method is **limited to compressing only the MLP layers**, without addressing the attention (QKV) projections. This is a substantial limitation, as many recent baselines including SVD-based and low-rank attention methods, compress both components for a more complete transformer compression framework. (SVD LLM, Dobi SVD, etc.)\n* The ablation section can get significantly improved, as the authors rely only on textual descriptions of what happens when certain components are changed, without providing clear tables or quantitative comparisons. This makes it difficult to assess the relative contribution of each design choice.\n* The **ViT baseline in Table. 1 is insufficient and outdated**. More recent methods such as *DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge (CVPR 2025)* should be included for a fair comparison. Also many cells in Table. 1 contain missing entries, reducing the completeness of the evaluation.\n* The approach to **layer grouping is mostly based on trial and error**, with no analytical or principled method for determining optimal grouping strategies, which limits the practicality of the technique.\n\n\n**Minor Weaknesses**\n- **Figure 2** is too large, with oversized legends and unclear labeling; descriptive titles should replace numeric legend entries.  \n- **Figure 1** is poorly placed and not informative without any captions, reducing its utility in understanding the method.  \n- **Figure 5** is misplaced within the reference section and disrupts the paper’s structure."}, "questions": {"value": "**Questions:**  \n\n- Could the authors clarify **what the reported x% compression rate represents** in the baseline tables? Does it refer to total model compression or only the MLP layers? I believe this detail needs be more emphasized in the experimental section. \n- How are **compression rates computed** based on the sparsity ratio? Specifically, how are the sparse \\(V\\) matrices stored, and how is the overall compression budget determined?  \n- During inference, is the method implemented using lookup tables from the $U$ matrices or through general matrix multiplications, and how does this the method affect inference latency compared to SVD-based compression methods (SVDLLM) where FLOPS are reduced solely because of the low rank multiplications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aZuRP3kjoA", "forum": "eP1UcOigEH", "replyto": "eP1UcOigEH", "signatures": ["ICLR.cc/2026/Conference/Submission584/Reviewer_bePR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission584/Reviewer_bePR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169655988, "cdate": 1762169655988, "tmdate": 1762915552880, "mdate": 1762915552880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}