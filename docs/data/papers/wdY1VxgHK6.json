{"id": "wdY1VxgHK6", "number": 18895, "cdate": 1758291825785, "mdate": 1759897074818, "content": {"title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from over-optimization where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator), a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking by injecting trap instructions to trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.", "tldr": "We present IFD, a training framework that balances instruction difficulty and enforces intent alignment. It exposes shortcut behaviors and significantly improves both robustness and accuracy.", "keywords": ["Large Language Model", "Instruction Following", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d42efff8262943528d2417250c3f923f00fd76b.pdf", "supplementary_material": "/attachment/013db22bc293c0632571ed3a3edb5cc7423c6c2d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes IFDecorator, a framework that augments Reinforcement Learning with Verifiable Rewards (RLVR) for instruction following. RLVR can still be prone to reward hacking, where satisfying instruction intent is bypassed. The authors introduce three synergistic components to mitigate reward hacking:\n\n* Cooperative-Adversarial Data Flywheel – iteratively generates and filters instruction–verification pairs with difficulty control based on solver pass rates rather than constraint counts.\n* IntentCheck – a “decorator” module that explicitly extracts the instruction’s core intent and verifies whether model responses fulfill it, mitigating over-optimization.\n* Trip Wires – diagnostic trap instructions designed to quantify and analyze reward hacking tendencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Comprehensive and credible evaluation: multiple model families, scales, and benchmarks.\n* Addresses a genuine bottleneck—reward hacking in RLVR—more directly than previous “early-stop” fixes.\n* Maintains or slightly improves general reasoning and code performance (GA stable).\n* Practical utility: a clean wrapper for existing RLVR pipelines."}, "weaknesses": {"value": "* Conceptual novelty modest: All three components have strong precedents (curriculum flywheels, intent verification, trap-based auditing). \n* Trip Wire coverage narrow: mostly format/placeholder exploits—ignores semantic or contextual gaming (e.g., misleading content that passes human-style checks).\n* Evaluation bias: Many metrics depend on LLM-as-a-judge evaluators (potential leakage).\n* Limited interpretability: The paper reports lower Hack Hit Rate but does not analyze why certain patterns decline—are models truly more aligned or just penalized for those forms?"}, "questions": {"value": "* How consistent are IntentCheck judgments when re-run with a different seed or model (e.g., Claude vs Qwen judge)?\n* Do models trained with IntentCheck transfer to new constraint types not seen in training (e.g., temporal ordering)?\n* How do results compare against mixing RLVR and RLHF rewards at equal compute (Pyatkin et al., 2025)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ajcg5sMkIW", "forum": "wdY1VxgHK6", "replyto": "wdY1VxgHK6", "signatures": ["ICLR.cc/2026/Conference/Submission18895/Reviewer_wybW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18895/Reviewer_wybW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337742479, "cdate": 1761337742479, "tmdate": 1762930866829, "mdate": 1762930866829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IFDecorator, a framework that augments Reinforcement Learning with Verifiable Rewards (RLVR) for instruction-following LLMs. The method addresses two known issues in RLVR4IF: (1) naive difficulty estimation and (2) reward hacking through verification shortcuts. IFDecorator introduces three synergistic components:\n\n1. Cooperative-Adversarial Data Flywheel for co-evolving instruction-verification pairs with difficulty control.\n\n2. IntentCheck, a bypass verifier that ensures alignment to the instruction’s intent.\n\n3. Trip Wires, diagnostic probes that quantify reward hacking tendencies.\nExperiments on IFEval and FollowBench demonstrate improved instruction-following ability and reduced reward hacking, with minimal degradation of general abilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty and conceptual clarity:\nThe idea of wrapping RLVR with an intent-aware decorator and diagnostic tripwires is conceptually elegant and practically relevant. Unlike previous RLVR or RLHF hybrids, IFDecorator explicitly disentangles intent alignment from verification correctness, which directly targets reward hacking.\n\n2. Strong motivation and connection to AI safety:\nThe work clearly situates itself within the literature on Goodhart’s Law and reward hacking, linking practical RLVR challenges to core safety concerns. This contextualization is rare and well justified.\n\n3. Comprehensive framework:\nThe paper offers a complete system from data generation to evaluation. The cooperative-adversarial data flywheel for instruction evolution is an interesting extension of curriculum or self-play ideas."}, "weaknesses": {"value": "1. Insufficient analysis of IntentCheck mechanism:\nThe paper doesn’t deeply analyze how IntentCheck extracts and represents “intent.” The prompt-based approach may risk circularity if the same LLM family is used for both generation and evaluation. A qualitative or error-type analysis of IntentCheck failures would strengthen claims about robustness.\n\n2. Limited novelty in components:\nWhile the integration is well-motivated, each individual part (data flywheel, intent verification, trap-based evaluation) draws on existing paradigms. The paper’s main contribution is more engineering synthesis than a new algorithmic principle. Some reviewers might find the “decorator” framing slightly overstated.\n\n3. Trip Wires evaluation scope:\nThe diagnostic captures only a few exploit types (placeholders, repetition, formatting). With 37.5% recall, it may underestimate hacking frequency. The paper could discuss scalability of Trip Wires to more nuanced or semantic exploit behaviors.\n\n4. Comparisons and baselines:\nAlthough comparisons to UltraIF and VerIF are included, it’s unclear how hyperparameters, dataset sizes, and judge strengths were normalized. More transparent cost–performance comparisons would help, especially versus recent open-source RLVR variants."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SXxzHz0gSB", "forum": "wdY1VxgHK6", "replyto": "wdY1VxgHK6", "signatures": ["ICLR.cc/2026/Conference/Submission18895/Reviewer_4SQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18895/Reviewer_4SQ3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965097405, "cdate": 1761965097405, "tmdate": 1762930866274, "mdate": 1762930866274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IFDecorator, a framework that wraps reinforcement learning with verifiable rewards for instruction following (RLVR4IF) to make training more sample-efficient and to mitigate reward hacking. The system combines (1) a cooperative–adversarial data flywheel that evolves instructions by empirical difficulty using measured pass rates, (2) an IntentCheck module that verifies whether model responses satisfy the core intent of instructions, and (3) Trip Wires that expose and quantify reward-hacking behavior. On benchmarks such as IFEval and FollowBench, IFDecorator improves adherence to instruction semantics and reduces exploitative behaviors, particularly in self-alignment settings using large open-source models like Qwen2.5-32B."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a concrete and increasingly relevant issue through a simple, modular solution that integrates easily into existing training pipelines.\n\n- Replaces naive constraint-counting with pass-rate–driven adaptive filtering, a principled way to balance challenge and solvability in evolved datasets.\n\n- IntentCheck enforces semantic fidelity beyond rule-level correctness, and Trip Wires provide a tangible diagnostic for detecting reward hacking.\n\n- Consistently improves instruction-following performance while reducing reward hacking, with the self-alignment experiment demonstrating the potential of verifiable self-judging."}, "weaknesses": {"value": "- Relies on an LLM-driven EXTRACTINTENT step to decompose each instruction into intent, context, input, and constraints, but this process is not validated for accuracy or consistency (e.g., no human agreement or inter-run checks). The paper shows downstream effectiveness (IntentCheck lowers hacking) but does not establish IntentCheck reliability as a decomposition method.\n\n- The cooperative–adversarial flywheel depends on empirical pass rates and fixed thresholds (e.g., $\\tau_\\text{low}=0.0$, $\\tau_\\text{high}=0.5$) to classify tasks as too easy, too hard, or acceptable, yet no per-iteration analysis, sensitivity study, or visualization of pass-rate dynamics is provided. The only related ablation disables difficulty control, leaving threshold tuning unexplored.\n\n- IntentCheck and several evaluation components rely on LLM judges—primarily Qwen2.5-32B-Instruct (with a 7B variant) for judging, plus GPT-4o for FollowBench open-ended scoring and for discovering reward-hacking patterns, so cross-judge or non-LLM verifiers are still not demonstrated, and robustness to genuinely different verifiers remains unclear.\n\n- The Trip Wires detector is tuned for high precision (93.5%) but has low recall (37.5%), so it likely undercounts reward hacking. Broader pattern coverage and human correlation would strengthen the claim.\n\n- The pipeline leans heavily on Qwen2.5-32B as the judge/data synthesizer. A 7B variant works but reduces GA, and cheaper or smaller verifiers (e.g., Llama-2-13B) are not analyzed in depth, raising questions about reproducibility and generality."}, "questions": {"value": "- How accurate/reliable is the LLM-based EXTRACTINTENT decomposition? Do you have any human agreement or consistency checks to support IntentCheck, beyond the effectiveness ablation?\n\n- How do pass rates evolve across flywheel iterations with the chosen thresholds (e.g., $\\tau_\\text{low}=0.0$, $\\tau_\\text{high}=0.5$)? Is the method sensitive to these thresholds?\n\n- Since most judging uses Qwen2.5-32B (and a 7B variant) and GPT-4o is used for some evaluation, how robust are the results to alternative, weaker, or non-LLM verifiers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FLDE06O5IN", "forum": "wdY1VxgHK6", "replyto": "wdY1VxgHK6", "signatures": ["ICLR.cc/2026/Conference/Submission18895/Reviewer_ThLb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18895/Reviewer_ThLb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966377235, "cdate": 1761966377235, "tmdate": 1762930865323, "mdate": 1762930865323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IFDecorator, a wrapper around RL with verifiable rewards for instruction following. It has three parts: a cooperative-adversarial data flywheel to curate hard but solvable instruction-verification pairs, IntentCheck to verify core intent beyond surface constraints, and Trip Wires to diagnose reward-hacking via trap instructions and a hack hit rate metric. Experiments show higher IFEval (e.g., 87.43% for Qwen2.5-32B-Instruct-IFD) and better FollowBench, while reducing hacking tendencies, with human study supporting Trip Wires precision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of decorating RLVR with intent verification plus independent diagnostics is quite original and practical. IntentCheck directly targets the gap between constraint satisfaction and intent fulfillment, and Trip Wires formalize hack probing with HHR.\n- The paper is well-written with clear binary reward formulation and careful hybrid verification. The motivating examples and framework figure make the failure modes and fixes easy to grasp.\n- The paper conducts extensive experiments with multiple ablations and a human study."}, "weaknesses": {"value": "- IntentCheck and soft-criteria rely on a judge model. More cross-judge validation would be stronger. \n- For Trip Wires, human eval shows high precision but only 37.5% recall.\n- Although Trip Wires are training-independent, repeated evaluation could still invite Goodhart effects"}, "questions": {"value": "- How does IFDecorator perform if Trip Wires cover new patterns unseen during development?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZAlZJHXjek", "forum": "wdY1VxgHK6", "replyto": "wdY1VxgHK6", "signatures": ["ICLR.cc/2026/Conference/Submission18895/Reviewer_eMTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18895/Reviewer_eMTT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762638251340, "cdate": 1762638251340, "tmdate": 1762930864430, "mdate": 1762930864430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach to enhance instruction following capabilities of large language models (LLMs), but it suffers from over-optimization where LLMs exploit verification shortcuts without aligning to the actual instruction intent. We introduce Instruction Following Decorator (IFDecorator), a framework that wraps RLVR for instruction following into a sample-efficient and robust pipeline. It consists of three components: a cooperative-adversarial data flywheel that co-evolves instruction-verification pairs, generating progressively challenging training samples; IntentCheck, a bypass module that circumvents verifications and directly assesses whether LLM responses align with instruction intent; and Trip Wires, a novel diagnostic tool using strategically designed trap instructions to quantify and capture exploitation behaviors. Extensive experiments validate our approach, with our Qwen2.5-32B-Instruct model achieving 87.43% accuracy on IFEval, outperforming larger models like GPT-40, while human evaluation confirms Trip Wires achieve high precision in detecting genuine hacking. Crucially, Trip Wires show our method significantly reduces reward hacking tendencies and generalizes across different model architectures and scales. We will release models, code, and data for future research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper designs a framework called IFDecorator that successfully addresses the long-standing challenge of gauging instruction difficulty by leveraging a cooperative-adversarial flywheel.\n\n2. It proposes the IntentCheck and Trip Wires methods, which effectively mitigate over-optimization and reward hacking in RLVR4IF tasks; I find this direction a particularly interesting angle for RLVR-based instruction-following alignment.\n\n3. Models trained with the approach demonstrate good results across a wide range of parameter scales."}, "weaknesses": {"value": "1. My central concern is generalization. IFEval and FollowBench consist largely of verifiable instructions, offering limited evidence that the approach will generalize to real-world instructions—especially restrictive role-play prompts that are hard to verify. This raises substantial doubts about the method’s effectiveness on non-verifiable instructions. In addition, several challenging instruction-following benchmarks—such as ComplexBench, Multi-IF, FoFobench and InfoBench—are not covered.\n\n2. A second major concern is the overlap between the paper’s instruction/verification evolution process and prior work like AUTOIF, which weakens the novelty. Moreover, the “instruction evolution” relies heavily on the thresholds τ_low and τ_high; the resulting difficulty seems highly sensitive to these empirical settings, and the paper lacks fine-grained experiments to justify them.\n\n3. Experimentally, the setup mirrors AUTOIF but omits direct comparisons with key baselines (AUTOIF, UltraIF, Conifer, etc.), which is inadequate. Reviewing recent instruction-following papers, I did not see this method establishing a clear performance advantage, which casts doubt on its true contribution. Finally, using only Qwen2.5-32B-IT for the self-alignment setting is insufficient to demonstrate the effectiveness of self-alignment."}, "questions": {"value": "See Weakness and following questions.\n\n1. The paper should spell out in detail how it differs from closely related work such as AUTOIF and UltraIF, and it should include direct performance comparisons with those baselines.\n\n2. It is unclear how much overlap there is between IntentCheck and the hybrid verification scheme. Is it really necessary to run both checks for every query?\n\n3. The use of Trip Wires in RL training needs clarification. If Trip Wires do not affect the reward, do they actually influence the training process? The current exposition is not sufficiently clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LWgiDTnWsW", "forum": "wdY1VxgHK6", "replyto": "wdY1VxgHK6", "signatures": ["ICLR.cc/2026/Conference/Submission18895/Reviewer_WDg6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18895/Reviewer_WDg6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762856475275, "cdate": 1762856475275, "tmdate": 1762930863659, "mdate": 1762930863659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}