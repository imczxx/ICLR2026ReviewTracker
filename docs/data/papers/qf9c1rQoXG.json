{"id": "qf9c1rQoXG", "number": 10969, "cdate": 1758185846259, "mdate": 1759897617460, "content": {"title": "Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning", "abstract": "While behavior cloning with flow/diffusion policies excels at learning complex skills from demonstrations, it remains vulnerable to distributional shift, and standard RL methods struggle to fine-tune these models due to their iterative inference process and the limitations of existing workarounds. In this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on the key insight that discretizing the flow-matching inference process via a fixed-step Euler scheme inherently aligns it with the variational Jordan–Kinderlehrer–Otto (JKO) principle from optimal transport. SWFP decomposes the global flow into a sequence of small, incremental transformations between proximate distributions. Each step corresponds to a JKO update, regularizing policy changes to stay near the previous iterate and ensuring stable online adaptation with entropic regularization. This decomposition yields an efficient algorithm that fine-tunes pre-trained flows via a cascade of small flow blocks, offering significant advantages: simpler/faster training of sub-models, reduced computational/memory costs, and provable stability grounded in Wasserstein trust regions. Comprehensive experiments demonstrate SWFP's enhanced stability, efficiency, and superior adaptation performance across diverse robotic control benchmarks.", "tldr": "", "keywords": ["Reinforcement Learning; Flow Model; Diffusion Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3175ce40498a8b024a33dc00be192901bedc19d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework for fine-tuning a flow-based policy.\nIn the proposed framework, the transport from the actions generated by an initial policy to those of the optimal policy is modeled based on the Jordan–Kinderlehrer–Otto (JKO) scheme.\nThe optimal policy is approximated by a Boltzmann distribution derived from entropy-regularized reinforcement learning (RL).\nThe transport from the initial policy to the optimal policy is modeled in a step-wise manner; that is, when the flow from the initial policy to the optimal policy is discretized into $N$ steps, $N$ models are trained to estimate each transition.\n\nThe proposed method was evaluated on manipulation tasks in the Franka-Kitchen and RoboMimic environments, where it outperformed the baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Modeling the transport from the initial policy to the optimal policy for policy refinement appears to be novel.\n\n- The experiments demonstrate the advantages of the proposed approach."}, "weaknesses": {"value": "- The presentation lacks clarity, and the paper is not well-organized.\n\n- The experimental evaluation is limited and requires additional experiments.\n\nIf I understand correctly, the transport from the actions generated by the initial policy to those of the optimal policy is approximated using step-wise models. However, I am not fully certain whether this interpretation is correct—please clarify if my understanding is mistaken.\n\nWhile the JKO scheme is introduced in the paper, the update rule appears to correspond to a standard entropy-regularized RL objective function with a Wasserstein distance–based regularization term. It is therefore unclear why the JKO scheme is emphasized. Are the actions generated by the initial policy pushed forward through a process similar to Stein variational gradient descent?\nIn addition, the procedure for solving the objective in Equation (19) is not clearly described, which makes it difficult to reproduce the proposed method.\n\nAlthough the proposed approach was evaluated on manipulation tasks (Franka-Kitchen and RoboMimic), evaluations on locomotion benchmarks such as D4RL or OGBench are missing. Since algorithmic behavior often differs between manipulation and locomotion domains, I recommend including additional experiments on locomotion tasks.\n\nMoreover, Flow Q-Learning is now a well-known and relevant baseline, as it also employs a flow-based policy and addresses offline-to-online RL problems. A comparison with Flow Q-Learning [R1] would therefore strengthen the paper.\n\n[R1] Flow Q-Learning. Seohong Park, Qiyang Li, Sergey Levine. ICML 2025.\n\n*Minor Comments*\n\n- Line 207: “can written as” → “can be written as”\n\n- Line 114: “dynamic of” → “dynamics of”"}, "questions": {"value": "- Please confirm whether my understanding is correct: is the transport from the actions generated by the initial policy to those of the optimal policy approximated by step-wise models?\n\n- Please explain in detail how the objective in Equation (19) is solved in the proposed method.\n\n- Please include comparisons with Flow Q-Learning and experiments on locomotion tasks."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "I am also reviewing Submission 691, titled “Value Gradient Flow: Behavior Regularized RL without Regularization.”\nI noticed that the idea presented in this paper is highly similar to that of Submission 10969. Both submissions share the same main approach: a flow policy is initialized using a given dataset to model the behavior policy, and the transport from the actions generated by the initial policy to those of the optimal policy is learned. The optimal transport is modeled using the Jordan Kinderlehrer Otto (JKO) scheme.\n\nThere are, however, some differences between the two submissions. In Submission 691, the update rule, which is analogous to Stein Variational Gradient Descent, is clearly described, and the regularization based on limiting the transport budget is explained. In addition, the gradient flow appears to be modeled using a single model. The method is evaluated on both locomotion and large language model (LLM) fine tuning tasks.\nIn contrast, in Submission 10969, the update rule is not clearly described, and the transport is modeled with step wise models. The method is evaluated on manipulation tasks.\n\nIn my view, the differences between these two papers are minimal. If they are authored by the same group, these submissions may constitute a dual submission of essentially the same work. However, if the authors are different, then this overlap is less concerning.\nAs I do not have access to the author identities, I am simply raising a flag to alert the Area Chairs and Program Chairs to this potential issue. I will also raise a similar flag for Submission 691."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jek0ATE3Gh", "forum": "qf9c1rQoXG", "replyto": "qf9c1rQoXG", "signatures": ["ICLR.cc/2026/Conference/Submission10969/Reviewer_sK8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10969/Reviewer_sK8u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204505130, "cdate": 1761204505130, "tmdate": 1762922161731, "mdate": 1762922161731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn flow policies using RL. Unlike previous work in behaviour cloning that learns flow fields that match the demonstration trajectories, this paper allows the flow field to be optimised against an explicit objective function. It does so tractably by using the JKO time discretisation scheme to generate incremental improvements to the flow field to match a maximum-entropy energy field encoding the reward function."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Overall this paper proposes an interesting and novel approach to learning flow fields. The paper is mostly clearly written (with occasional typos), and the paper is careful to take the reader from the original problem statement (above equation 3) through to the policy improvement step in equation 15.\n\nThe experimental results show that the technique performs well on several baselines."}, "weaknesses": {"value": "The primary concern I have is how practical is the technique. I believe the authors when they say that recursive gradient computations are not tractable, but it is not clear to me how tractable each update of equation 15 is. It also the case that the learning process involves learning a soft critic for the target along side the policy -- the learned Q function is presumably updated after each JKO optimisation update. The Q function in SAC is usually updated using TD learning -- is that straight forward given this parameterisation? \n\nA second concern I have is the novelty of the technique. Flow-based RL is not exactly novel, and there have been previous results such as Reinflow (Zhang et al 2025) and Flow q-learning (Park et al 2025). These are very recent, so I don't view this as a huge problem but it would be good to provide some kind of comparison in the response. \n\nGiven that the technique is fundamentally distribution matching between the free-energy functional of equation 3 and the Q functional of the policy, it is not clear if there are constraints on the reward function. Are there kinds of problems where this technique will not succeed?\n\nThe authors describe in the introduction that they view RL as a fine-tuning step on top of behaviour cloning, but many RL problems do not have an initial human-provided solution and the \"fine-tuning\" must actually be performed from scratch. It is not clear how strong the gradients are when pushed through the distribution matching.\n\nIt is interesting that the SWFP technique does not uniquely dominate all techniques in figure 4 --- it achieves comparable performance to the best technique in every domain and in some domains is the clear best performer (e.g., CALVIN, kitchen-complete-v0) but is not better than DPPO in Can and not better than IDQL for kitchen-partial-v0 --- the authors do not in fact describe figure 4, and so provide no insight into where SWFP is in fact a superior technique."}, "questions": {"value": "Is the Q function learning for the soft critic updated after each JKO update? Do we have to do anything different to recover a TD step? \nAre there kinds of problems where this technique will not succeed?\nGiven that SWFP does not outperform other techniques in some domains, what are the characteristics of problems where SWFP is superior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uvqNmwZ37x", "forum": "qf9c1rQoXG", "replyto": "qf9c1rQoXG", "signatures": ["ICLR.cc/2026/Conference/Submission10969/Reviewer_3kYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10969/Reviewer_3kYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875985831, "cdate": 1761875985831, "tmdate": 1762922161296, "mdate": 1762922161296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates reinforcement learning as an iterative refinement process grounded in optimal transport and flow-matching theory. It interprets policy optimization as a Wasserstein gradient flow over probability distributions, where each update step minimizes a composite objective consisting of an energy functional (negative expected return with entropy regularization) and a transport regularization term. The authors derive a practical algorithm, termed Stepwise Flow Policy (SWFP), that discretizes this gradient flow into tractable updates and implements it using conditional flow-matching parameterizations. The method represents policies as learnable stochastic transport maps and performs block-wise optimization akin to proximal iterations in the space of distributions. The paper connects this formulation to existing soft policy iteration and diffusion-policy approaches, provides theoretical grounding through the Jordan–Kinderlehrer–Otto (JKO) scheme, and demonstrates implementation details for both policy and value updates within standard continuous-control benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I am not an expert in the generative-policy literature, so my comments are based primarily on the content of this submission itself.\n\n- The motivation of the paper is clearly articulated: in online policy iteration, it is generally infeasible to back-propagate through a large number of inference steps of flow-based policies. The authors address this limitation by proposing an alternative formulation based on Wasserstein gradient flow. (However, I have some reservations about this aspect, as noted in the weaknesses section below.)\n\n- The introduction of a parallel training scheme that leverages replay-buffer data could be a significant contribution to online policy improvement for generative policies, assuming that prior work has not already addressed this direction.\n\n- The derivation from Wasserstein gradient-flow theory to the practical implementation seems technically sound, and I did not observe any hand-wavy reasoning or questionable design choices."}, "weaknesses": {"value": "- Empirical evaluation: The empirical evaluation appears somewhat unfair. While SWFP is tuned over a grid of hyper-parameters, the baselines are not subjected to comparable hyper-parameter search, as stated in the appendix. This imbalance in tuning effort makes it difficult to attribute the reported performance gains solely to the proposed method.\n\n- On the motivation and possible baseline: Given that the policy network used in experiments is a relatively small MLP and the SWFP block size is only five, it should generally be feasible to back-propagate through the entire recursive sampling process. This makes the stated motivation, avoiding back-propagation through many inference steps, feel somewhat less compelling in the current experimental scale. Nonetheless, the topic remains relevant and important, especially considering the scaling trend in robotic foundation policies. To strengthen the paper, the authors could consider adding a baseline that directly back-propagates through the sampling process, which might serve as an approximate upper bound for online iterative refinement of generative policies."}, "questions": {"value": "- L135 ``wherein $u_t(x\\mid x1)$ becomes tractable by defining explicit conditional probability paths from $x_0$ to $x_1$, such as OT-paths or linear interpolation paths'': Could the authors clarify which was actually used in the implementation?\n\n\n- L246 ``To solve the above problem, we first model the continuous-time diffusion process by a partial differential equations (PDE), i.e., the Fokker-Planck equation, $\\partial \\rho(t, x) / \\partial t = \\nabla (\\nabla U(x) \\rho(t, x)) + \\beta \\nabla^2 \\rho(t, x)$'': I’m curious about the motivation for introducing the Fokker–Planck equation here. Is this formulation commonly used in generative-policy models, or is it mainly used as a convenient connection to the JKO scheme? Would it be helpful to provide further clarification or justification for this modeling choice in the paper?\n\n- Figure 3: If I understand correctly, the target reward landscape consists of eight Gaussian components, each representing a high-reward region. In the right subfigure, however, the particles appear to concentrate around three modes (with one dominant and two minor clusters). Does this suggest that SWFP may not fully capture multimodal distributions? Ideally, one might expect convergence toward all eight modes. Could the authors clarify whether this behavior is expected or a limitation of SWFP. If it's incorrect, could the authors design another example showing how multi-model behavior can be well-captured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tcS9xZk8vk", "forum": "qf9c1rQoXG", "replyto": "qf9c1rQoXG", "signatures": ["ICLR.cc/2026/Conference/Submission10969/Reviewer_oc5Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10969/Reviewer_oc5Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762719407897, "cdate": 1762719407897, "tmdate": 1762922160670, "mdate": 1762922160670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}