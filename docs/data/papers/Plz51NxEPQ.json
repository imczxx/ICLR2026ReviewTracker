{"id": "Plz51NxEPQ", "number": 12107, "cdate": 1758205703343, "mdate": 1759897532825, "content": {"title": "ATM: Adaptive Time Series Tokenization with Semantic Modeling", "abstract": "Recent advances in time series forecasting have achieved remarkable success, yet two critical challenges remain underexplored: the limitations of fixed-length patching strategies and the lack of semantic-level modeling. Fixed-length patches struggle to capture heterogeneous temporal patterns and often truncate temporal patterns, while existing methods largely rely on data-driven statistical patterns without semantic guidance. We propose Adaptive Time Series Tokenization with Semantic Modeling (ATM), a novel framework designed to address these issues. ATM introduces a Temporal Tokenization Module, which consists of two interrelated components: the Time Tokenizer, which adaptively partitions time series to preserve meaningful patterns (e.g., full cycles or peaks), and the Semantic Tokenization Regularization, designed to ensure semantically coherent temporal partitioning. In addition, ATM incorporates Semantic-Aware Modeling, where a Semantic Extractor enriches patches with latent semantic information and a Semantic Modeler captures hierarchical dependencies from local temporal patches to global sequence structures, enhanced by a Mixture-of-Experts module for diverse pattern modeling. Extensive experiments show that ATM consistently surpasses state-of-the-art methods in both long-term and short-term forecasting, demonstrating its effectiveness and strong generalization ability. Code is available at https://anonymous.4open.science/r/ATM.", "tldr": "We propose ATM, an adaptive time-series patching framework that incorporates latent semantic information for multivariate time-series forecasting.", "keywords": ["time series forecasting; patching; adaptive tokenization; semantic modeling"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac0f9e8cfd9e2be9e064b43d6c2fe14e3990f750.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on designing a new adaptive patch method for time series forecasting, and proposes a Adaptive Time Series Tokenization with Semantic Modeling (ATM) model. Experiments are carried out based on seven real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Tokenization is a critical step in deep learning models for time series modeling. Thereby, the investigated problem is good and worth exploring. \n\n2. Source codes are provided for reproducibility."}, "weaknesses": {"value": "1. Some relevant works about adaptive patching are not discussed, e.g., [1]\n\n[1]. DeformableTST: Transformer for time series forecasting without over-reliance on patching. NIPS 2024.\n\n2. Some recent baselines are missed in experiments, e.g., Timemixer and Timemixer++.\n\n3. The datasets are limited, maybe adding some traffic datasets, e.g., PeMS could make the experimental parts more solid."}, "questions": {"value": "1. The author spent a considerable amount of space discussing the drawbacks of the fixed patch, but what are its advantages? Can it, to some extent, ensure that the information strength between tokens is consistent? This way, there won't be any deviation or collapse when calculating attention. If the varying-length patch method is used, then the amount of information carried by each token will be different. Will this lead to an information imbalance during the attention process?\n\n2. In the field of vision, there are some methods involving deformable convolutions, which might be related to adaptive patches. Could the authors elaborate on further discussions and comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3HaceTbPF", "forum": "Plz51NxEPQ", "replyto": "Plz51NxEPQ", "signatures": ["ICLR.cc/2026/Conference/Submission12107/Reviewer_Q5Hm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12107/Reviewer_Q5Hm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760887497005, "cdate": 1760887497005, "tmdate": 1762923073796, "mdate": 1762923073796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this study, the authors investigate the tokenisation of time series in the context of forecasting. In particular, they propose to divide time series into (1) patches of varying length and (2) patches that are semantically meaningful. Experiments are conducted across 10 tasks, involving short-term and long-term forecasting. An ablation study is conducted to investigate the importance of the proposed components."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is well structured.\n2) The authors conduct forecasting experiments on established datasets, including 4 short-term and 6 long-term forecasting tasks, and an ablation study to evaluate their method."}, "weaknesses": {"value": "Related Works:\n\n1) The authors do not discuss the most recent works in the field of time series tokenisation. For instance, works on domain-specific tokenisation [1] and wavelet-based tokenisation [2] should be included to provide a representative overview of the current literature. \n\nPresentation: \n\n2) The authors propose ATM with its multiple components, however, the presentation lacks clarity. For instance, Figures 1 (overall architecture), 2 (Time Tokeniser), and 3 (Semantic Extractor and Semantic Modeler) are difficult to follow. \n\nExperiments: \n\n3) The authors compare the proposed tokeniser against several baselines, however, they do not include foundational models. Particularly, general-purpose models (e.g. OTIS [1] and MOMENT [3]) and foundational forecasting models (e.g. MOIRAI [4] and Chronos [5]) should be included as baselines to ensure a fair comparison of the proposed method. \n\n4) As the authors only report scores for a single seed (2025), the study does not guarantee robustness of the results. Mean and standard deviation across multiple seeds should be reported in Tables 1, 2, and 3. \n\nReproducibility:\n\n5) The authors do not support reproducibility by making their code publicly available for evaluation. \n\nDiscussion:\n\n6) The discussion of the experiments is too limited. The experiments section only provides numbers without their interpretation, while the conclusion section only provides a brief summary of the abstract.  \n7) The authors do not discuss the limitations of their work. \n\n___\n[1] Turgut, Özgün, et al. \"Towards generalisable time series understanding across domains.\" arXiv preprint arXiv:2410.07299 (2024).\n\n[2] Masserano, Luca, et al. \"Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization.\" ICML (2025).\n\n[3] Goswami, Mononito, et al. \"MOMENT: A Family of Open Time-series Foundation Models.\" ICML (2024).\n\n[4] Woo, Gerald, et al. \"Unified training of universal time series forecasting transformers.\" ICML (2024).\n\n[5] Ansari, Abdul Fatir, et al. \"Chronos: Learning the Language of Time Series.\" TMLR (2024)."}, "questions": {"value": "1) What is the motivation behind using patches of varying length?\n2) How do the authors (a) define and (b) quantify semantic information within a patch? Does semantic information vary with respect to the time series domain? If this is the case, how does it affect the tokenisation across domains, e.g. for medical recordings and weather recordings?\n3) What is the motivation behind using a SMAPE loss and L1 loss in short-term and long-term forecasting, respectively? \n4) How do the authors determine $\\lambda$ in Equation (12), which balances the $L_\\text{fore}$ and $L_\\text{STR}$ loss terms?\n5) Have the authors tried using their approach to different types of tasks, such as classification or regression? For instance, domain-specific tokenisation proposed in [1] proves beneficial across classification, regression, and forecasing tasks, further advancing the field of time series analysis. \n\n___ \n[1] Turgut, Özgün, et al. \"Towards generalisable time series understanding across domains.\" arXiv preprint arXiv:2410.07299 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZrfS4lMyq", "forum": "Plz51NxEPQ", "replyto": "Plz51NxEPQ", "signatures": ["ICLR.cc/2026/Conference/Submission12107/Reviewer_J1di"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12107/Reviewer_J1di"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579080602, "cdate": 1761579080602, "tmdate": 1762923073324, "mdate": 1762923073324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework that combines adaptive patching and semantic modeling to improve time series forecasting. It features a learnable patching module (TTM) that generates variable-length patches and a Semantic Tokenization Regularization (STR) mechanism to encourage semantic coherence. The model show strong empirical results across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed approach is well-motivated. The goal of avoiding temporal truncation in fixed patching is valid and important.\n* ATM achieves competitive performance across several datasets, showing clear gains over strong baselines like PatchTST."}, "weaknesses": {"value": "* The paper overlooks highly relevant concurrent research such as TimeCAT (dynamic grouping), which target the same limitation of fixed patching, so the claim that this area is “underexplored” is misleading.\n* The STR uses cosine similarity, a weak statistical proxy, and the paper fails to convincingly justify or validate that it captures genuine temporal **semantics**."}, "questions": {"value": "* Why is TimeCAT (ICLR2025 submission) not cited or compared? Both address adaptive segmentation, so a direct comparison seems essential.\n* How much of the performance gain comes from the SAM module (especially its MoE) versus the Time Tokenizer itself?\n* Is there any experiment showing that STR captures real-world semantic differences rather than statistical similarity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "giAa6ZrAGZ", "forum": "Plz51NxEPQ", "replyto": "Plz51NxEPQ", "signatures": ["ICLR.cc/2026/Conference/Submission12107/Reviewer_qdb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12107/Reviewer_qdb5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998628658, "cdate": 1761998628658, "tmdate": 1762923072955, "mdate": 1762923072955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}