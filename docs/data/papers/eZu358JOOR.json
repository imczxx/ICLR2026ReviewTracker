{"id": "eZu358JOOR", "number": 13427, "cdate": 1758217742002, "mdate": 1759897438152, "content": {"title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing", "abstract": "Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data.\nTo address this critical bottleneck, we built EditReward, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. EditReward demonstrates superior alignment with human preferences in instruction-guided image editing tasks.\nExperiments show that EditReward achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new EditReward-Bench, outperforming a wide range of VLM-as-judge models. Furthermore, we use EditReward to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates EditReward's ability to serve as a reward model to scale up high-quality training data for image editing. EditReward with its training dataset will be released to help the community build more high-quality image editing training datasets to catch up with the frontier ones.", "tldr": "We present EDITREWARD, trained on EDITREWARD-DATA (200K human preference pairs), and introduce EDITREWARD-BENCH, setting new state-of-the-art in image editing evaluation and data curation.", "keywords": ["Image Editing", "Reward Model", "Generative Model Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/172b90ff3a00c484240e67a2f226c7b12ce39210.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents EDITREWARD, a human-aligned reward model for instruction-based image editing. The authors build EDITREWARD-DATA, a 200K-scale preference dataset annotated along two dimensions (instruction following and visual quality), train a vision–language-based reward model with a multi-dimensional uncertainty-aware ranking loss, and propose EDITREWARD-BENCH for evaluation. EDITREWARD achieves strong results on GenAI-Bench, AURORA-Bench, and ImagenHub, and shows practical utility by improving data curation for training image editors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important and timely challenge: aligning open-source image-editing models with human preference.\n- Introduces EDITREWARD-DATA, a high-quality 200K, pair dataset with expert human annotations.\n- Proposes a multi-dimensional uncertainty-aware ranking loss, a thoughtful extension of prior single-dimension reward modeling.\n- Demonstrates strong results across several public benchmarks and a practical downstream benefit for data curation.\n- Writing, figures, and comparisons are clear and thorough."}, "weaknesses": {"value": "- **Data reuse and potential leakage.** Section 2.1 states that EDITREWARD-DATA is constructed from several existing editing benchmarks (GEdit-Bench, ImgEdit-Bench, MagicBrush, AnyEdit, EmuEdit). Because these benchmarks contain test splits in their original form (see also Fig. 2b), it remains unclear whether EDITREWARD’s training pool includes samples originating from those test sets. Clarifying this is critical to ensure that the reward model was not exposed to data overlapping with any public evaluation benchmarks. A simple leakage or overlap analysis would strengthen the claims of fair evaluation.\n- **Benchmark independence.** The paper reports results on GenAI-Bench, AURORA-Bench, and ImagenHub, which appear to be distinct from the training sources. However, an explicit statement confirming the absence of overlap (in images or prompts) between EDITREWARD-DATA and these benchmarks would improve transparency.\n- **Limited discussion of annotation reliability.** Although the authors mention a rigorous protocol and Likert-scale ratings, quantitative inter-annotator agreement or consistency metrics are not reported. Including such statistics (e.g., Cohen’s κ) would reinforce the dataset’s claimed quality.\n- **Lack of qualitative visual comparison.** While the quantitative metrics show consistent gains, the paper would benefit from direct visual comparisons between edits generated using datasets curated by EDITREWARD and those curated by other reward models or full unfiltered datasets. Such examples would make the improvements more interpretable and tangible, especially for assessing human alignment.\n- **Limited analysis of failure cases.** The experiments report strong quantitative results but give little qualitative analysis of where the model fails (e.g., when human judgment and reward disagree). Showing even a few representative failure patterns would strengthen interpretability.\n- **Reproducibility details.** Training parameters are described briefly, but access to scripts or validation splits would be essential to reproduce results once the dataset is released.\n- **Data annotation transparency and demographics.** Although the authors state that annotations were collected by “trained experts” following a standardized protocol, the paper provides no concrete details about the number of annotators, their demographic or institutional backgrounds, training procedures, or compensation. These factors are critical for assessing annotation quality, potential bias, and fairness. Greater transparency here would strengthen the dataset’s credibility and ethical grounding.\n- **Limited dataset examples and analysis.** Since EDITREWARD-DATA is a major contribution, the paper would benefit from showing more representative examples from the dataset, including the range of editing instructions, image types, and annotation cases. This would help readers better understand dataset diversity, annotation granularity, and potential biases. A few visual samples or statistics beyond Fig. 2 would strengthen the dataset description.\n- **Lack of qualitative illustration of the filtering process.** The paper describes reward-guided data selection but does not provide examples of what EDITREWARD retains or rejects. Visual examples of this filtering effect would make the model’s behavior and alignment properties much clearer."}, "questions": {"value": "1. Were any samples from the test splits of the original benchmarks (e.g., MagicBrush or GEdit-Bench) included in EDITREWARD-DATA’s training pool?\n2. Did the authors verify that EDITREWARD-DATA does not overlap with GenAI-Bench, AURORA-Bench, or ImagenHub (in images, prompts, or instructions)?\n3. Could the authors share inter-annotator agreement statistics to quantify annotation reliability?\n4. Could the authors clarify the licensing and redistribution permissions for EDITREWARD-DATA, given that it combines multiple public datasets (e.g., MagicBrush, AnyEdit, EmuEdit) that may have differing usage terms?\n5. Could the authors provide qualitative visual comparisons (e.g., side-by-side edited images) showing how edits differ when the same model architecture is trained on (a) the original dataset and (b) the EDITREWARD-curated subset? This would help readers understand the qualitative nature of the claimed alignment improvements.\n6. Could the authors provide more details about the human annotation process, including the number of annotators, their training and demographic diversity, and how they were compensated? Were any measures taken to ensure diverse perspectives or reduce annotator bias in the preference judgments?\n7. Could the authors include more representative examples from EDITREWARD-DATA (e.g., a sample of editing instructions, before/after images, and associated annotations) to illustrate dataset diversity and label consistency?\n8. Since the paper uses the reward model for data ranking and filtering, could the authors show qualitative examples of what types of samples are retained or filtered out by EDITREWARD to provide intuition about its learned preferences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e4DmjP6hjW", "forum": "eZu358JOOR", "replyto": "eZu358JOOR", "signatures": ["ICLR.cc/2026/Conference/Submission13427/Reviewer_1h4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13427/Reviewer_1h4X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760601029424, "cdate": 1760601029424, "tmdate": 1762924053028, "mdate": 1762924053028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper: EDITREWARD: A Human-Aligned Reward Model for Instruction-Guided Image Editing.\nContributions: The authors (1) construct EDITREWARD-DATA—a 200K expertly annotated, multi-dimensional human preference dataset for instruction-guided image editing; (2) train EDITREWARD, a VLM-based reward model with a multi-dimensional, uncertainty-aware ranking loss that predicts Gaussian scores per dimension (Instruction Following, Visual Quality); (3) introduce EDITREWARD-BENCH, a stricter evaluation with multi-way (K=2/3/4) preference ranking; and (4) demonstrate a data curation application where EDITREWARD filters a noisy dataset to improve a strong editor (Step1X-Edit) beyond training on the full set. SOTA results are reported on GenAI-Bench and AURORA-Bench; competitive on ImagenHub; strongest on the proposed EDITREWARD-BENCH (Table 2). The pipeline and data construction are summarized in Fig. 1 and §2; loss and training details in §3.2–§3.3; ablations in Table 4–5; curation results in Table 3."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "High-quality human supervision at scale. EDITREWARD-DATA provides ~200K expert preference pairs with two-dimension rubric; construction protocol and statistics are detailed in §2 and Fig. 2/Table 1—a step up from noisy, pseudo-labeled datasets. \nPrincipled reward modeling. The Gaussian per-dimension scoring and uncertainty-aware pairwise loss (Eq. 2–5) are well-motivated, capturing ambiguity and disentangling \"follow the instruction\" vs. \"visual quality.\" The tie-disentanglement augmentation further leverages ambiguous labels (§3.3, Fig. 5). \nStrong empirical validation. New SOTA alignment with human preferences on GenAI-Bench (65.72) and AURORA-Bench (63.62); best overall on EDITREWARD-BENCH with multi-way tuples (Table 2). \nActionable downstream impact. Using EDITREWARD to filter ShareGPT-4o-Image and fine-tune Step1X-Edit yields higher GEdit-Bench overall (7.086 vs 6.780 full-set), showing quality>quantity for editing training data (Table 3). \nTransparent ablations and generality. Loss/head/aggregation comparisons (Table 4), backbone scaling (Table 5), and dataset/tie studies (Table 7) indicate robustness and portability."}, "weaknesses": {"value": "Tool/backbone dependence not fully stress-tested. While two VLM backbones are tried (Qwen2.5-VL-7B, MiMo-VL-7B), broader tool-swap sensitivity (alternative VLMs, different visual encoders, cropping policies) is missing; this matters because reward transferability across labs/tooling is crucial (cf. Table 5 limited scope). \nBenchmark coupling and generalization. EDITREWARD-BENCH is curated from the same candidate pool as training sources (§2.2). Although splits are held-out, more OOD editing distributions (e.g., style-centric edits or long-tail compositional instructions) would better test generalization. Style edits are known gaps in related evaluators and are not emphasized here. \nAggregation choice/per-axis reporting. The mean aggregation across dimensions often performs best (Table 4), but policies like pessimistic min may align better with certain safety-critical uses. A task-conditioned aggregation or calibrated composite is not explored. \nHuman-label reliability bounds. The paper describes expert protocols and cross-checks yet gives few quantitative IAA statistics for EDITREWARD-DATA (A.2). Providing per-dimension IAA would contextualize achievable upper bounds (as done for ImagenHub). \nLimited analysis of failure modes. Qualitative examples exist (Fig. 3), but a taxonomy of consistent reward failures (e.g., counting, fine geometry, subtle semantics) with confusion patterns would guide future reward design."}, "questions": {"value": "Tool-swap robustness: How does EDITREWARD perform if you replace the VLM backbone with a different family (e.g., Idefics2 or LLaVA-Next) and/or swap the image encoder? Can you report Δaccuracy on Table 2 metrics under such swaps? \nAggregation policy: Can you provide use-case-conditioned aggregation (e.g., pessimistic min for safety-critical; mean for general editing) with calibrated thresholds, and report trade-offs on Table 2? \nAnnotator agreement: Please report IAA (e.g., Spearman or Krippendorff's α) per dimension for EDITREWARD-DATA and EDITREWARD-BENCH to bound the attainable ceiling and compare to \"Human-to-Human\" lines (Table 2). \nOOD editing types: Have you tested style-centric or text-in-image/OCR edits and failure rates? If not, could you include a small OOD slice or note expected behavior? \nCuration efficiency: For Table 3, what are the compute costs (GPU-hours) of scoring and filtering 46k examples, and how sensitive is the improvement to the top-K cutoff (e.g., 10k vs 20k vs 30k)?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "Although the work is an evaluator/reward model, it amplifies editing capability by enabling higher-quality training data. Please clarify: (i) dataset licenses and rights for all sources; (ii) filtering of PII/people and sensitive content; (iii) any watermark/provenance tagging for curated outputs; (iv) mitigation for misuse (deepfakes). The Ethics and Reproducibility statements are present but could include explicit license texts and allowlists/denylists."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "39swNOhasc", "forum": "eZu358JOOR", "replyto": "eZu358JOOR", "signatures": ["ICLR.cc/2026/Conference/Submission13427/Reviewer_i644"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13427/Reviewer_i644"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654236051, "cdate": 1761654236051, "tmdate": 1762924050521, "mdate": 1762924050521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EditReward, a specialized reward model for  instruction-guided image editing, designed to help open-source models catch up with proprietary giants. The authors argue the main bottleneck is the lack of high-quality training data. Their solution is to first build a massive dataset with over 200,000 expert-annotated preference pairs, uniquely scored on both \"Instruction Following\" and \"Visual Quality\". They use this to train EditReward, a reward model that shows superior alignment with human preferences, outperforming even models like GPT-5 on several benchmarks. Critically, they demonstrate its practical value by using it to filter a noisy dataset, significantly improving a downstream editing model's performance. The work provides a new dataset, model, and benchmark to accelerate open-source development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High-Quality, Large-Scale Human Data. It introduce a 200K expert-annotated dataset which is a massive undertaking and a huge contribution to the community. \n2. Instead of a single score, separating \"Instruction Following\" from \"Visual Quality\" is brilliant. It provides a much richer, disentangled signal for the model to learn from, which is crucial for editing tasks where these two aspects can be in conflict.\n3. A More Challenging Benchmark(EditReward-Bench) with multi-way ranking is a forward-thinking move. It pushes the evaluation standard higher, forcing models to be more consistent and discerning."}, "weaknesses": {"value": "1. The Cost of Quality: The whole project relies on \"trained experts.\" This is expensive and hard to scale. While they've created a great resource, it doesn't solve the fundamental problem of how to continue generating high-quality preference data affordably.\n\n2. Are two dimensions enough? \"Instruction Following\" and \"Visual Quality\" are great, but what about more abstract concepts like \"creativity,\" \"style preservation,\" or \"compositional harmony\"? Complex edits might require more than two axes to be judged properly."}, "questions": {"value": "1.  The \"tie-disentanglement\" trick for training is very clever. Does this teach the model that some dimensions are more important than others in \"tie-breaker\" situations? For example, could it learn to favor an image with better visual quality over one with slightly better instruction following when the overall scores are similar?\n\n2. Given that expert annotation is a bottleneck, have you considered using Edit reward itself in a semi-supervised or self-training loop to label a much larger, un-annotated dataset, effectively creating a \"student\" reward model that learns from your expert one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hKwqFPHn3U", "forum": "eZu358JOOR", "replyto": "eZu358JOOR", "signatures": ["ICLR.cc/2026/Conference/Submission13427/Reviewer_iwg3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13427/Reviewer_iwg3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833973348, "cdate": 1761833973348, "tmdate": 1762924049505, "mdate": 1762924049505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EditReward, a vision-language model (VLM)-based reward model tailored for image editing tasks, aiming to better align with human preferences regarding editing outcomes. The authors construct a large-scale, high-quality human-annotated preference dataset, train an EditReward model based on VLMs, and introduce a new, more challenging multi-choice preference ranking benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. EditReward-Data is currently one of the largest and most diverse preference datasets in the image editing domain, offering significant practical value.\n2. EditReward achieves state-of-the-art or near-SOTA performance across multiple public and newly proposed benchmarks.\n3. The paper is clearly written, and the figures effectively illustrate the methodology and workflow."}, "weaknesses": {"value": "1. The paper evaluates only Instruction Following and Visual Quality, but overlooks the preservation of unedited regions, which is a critical aspect of image editing. Many edits may unintentionally alter details outside the target area. \n2. VLMs often struggle with fine-grained spatial reasoning, yet image editing demands pixel-level precision. How does EditReward mitigate potential inaccuracies when evaluating spatially sensitive edits (e.g., object positioning, resizing, or local geometric changes)?\n3. Using VLM-based approaches as reward models incurs substantial inference overhead, limiting practical deployment.\n4. Although the paper mentions quality control procedures for human evaluation (e.g., training, calibration, spot-checking), it does not report inter-annotator agreement metrics, making it difficult to assess the reliability of the annotations."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5IWJKuLdo", "forum": "eZu358JOOR", "replyto": "eZu358JOOR", "signatures": ["ICLR.cc/2026/Conference/Submission13427/Reviewer_7xvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13427/Reviewer_7xvL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918277523, "cdate": 1761918277523, "tmdate": 1762924048828, "mdate": 1762924048828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}