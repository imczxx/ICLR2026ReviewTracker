{"id": "kW5hSRG5wq", "number": 21688, "cdate": 1758320532827, "mdate": 1759896908625, "content": {"title": "The Curious Case of AdamW", "abstract": "AdamW is ubiquitous in deep learning, yet its behavior remains poorly understood. We analyze its dynamics through the lens of dynamical systems and show that AdamW admits an *implicit objective*: its fixed points coincide with the stationary points of a constrained and regularized optimization problem. However, not all of these fixed points are stable under AdamW’s dynamics, and stability depends sensitively on curvature, weight decay, and momentum parameters. Even in simple one-dimensional settings, AdamW can exhibit surprisingly complex behavior: equilibria may be unstable and trajectories can fall into persistent limit cycles. We further extend the analysis to higher dimensions, deriving sufficient conditions for stability, and validate empirically that when AdamW converges in neural network training, it converges to stable equilibria. These results clarify what optimization problem AdamW is associated with, when convergence can be expected, and how its curious dynamics could inspire the development of more reliable optimization algorithms in the future.", "tldr": "AdamW’s fixed points match a regularized implicit objective, but many are unstable. This misalignment makes AdamW non-convergent on simple functions.", "keywords": ["AdamW", "optimization", "convergence", "dynamical systems", "stability analysis"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f3e27313103e97652bee2827bb4cf0ab802809c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the dynamics of AdamW in the deterministic setting by deriving its continuous-time limit as an ordinary differential equation (ODE). Through this formulation, the authors analyze the convergence properties of AdamW and demonstrate that its equilibria coincide with the stationary points of an implicit objective function that differs from the original loss. This leads to the insight that AdamW effectively optimizes a modified problem, and that even in simple settings, its trajectories may fail to converge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper contributes to the growing line of work that analyzes optimization algorithms through their continuous-time counterparts, an increasingly valuable and insightful perspective. It provides a rigorous analysis of AdamW in the deterministic setting, offering a clear theoretical characterization of its dynamics. The authors illustrate their findings with well-constructed examples that reveal non-convergent behavior even on simple, controlled objective functions."}, "weaknesses": {"value": "**Weaknesses**\n\nThe following weaknesses are listed in no particular order of importance or severity:\n\n1. **Lack of connection to prior continuous-time literature.**  \n   The paper overlooks a substantial body of prior work analyzing optimization algorithms through continuous-time formulations (ODEs and SDEs). This literature provides the theoretical groundwork for interpreting optimizers as dynamical systems and should be discussed to position the present contribution more clearly.  I added some references below, also encompassing ODEs: I suggest the authors carry out a thorough review of these papers and look for more recent ones as well. For an accessible entry point, I recommend the Related Works and Appendix A of [1], which offer a representative overview of (tens and tens of) works using continuous-time models for optimizations. Although that reference focuses mainly on SDEs, many of the works it cites also include ODE analyses directly relevant to this discussion. The absence of this contextualization makes the paper appear somewhat disconnected from its theoretical lineage.\n\n2. **Overly restrictive deterministic setting.**  \n   The analysis is limited to the deterministic regime, leading to an ODE model that ignores gradient noise and cannot capture the interaction between weight decay, stochasticity, and loss curvature. As shown in [1], an SDE formulation of AdamW naturally extends this model and reduces to the proposed ODE when noise is removed.  \n   In particular, Theorem 3.12 in [1] explicitly derives the SDE of AdamW and demonstrates that even mild stochasticity can qualitatively change its dynamics. This raises the question of how the results presented here would generalize to the stochastic setting, which is the one that actually governs practical optimization.\n\n3. **Conceptual inconsistency in the experiments.**  \n   The experiments in Figure 5 integrate the ODE using a Runge–Kutta (RK4) scheme. This is conceptually problematic: a continuous-time model is valuable only insofar as it provides insights into the discrete optimizer it approximates. Integrating the ODE numerically does not validate the behavior of AdamW itself.  \n   As the authors acknowledge, gradient flow converges independently of step size or smoothness, which underscores that ODEs cannot capture discrete-time stability phenomena. To validate the theoretical insights, the experiments should be rerun on AdamW directly, not on its continuous-time approximation.\n\n4. **Questionable experimental relevance.**  \n   I replicated the experiment in Figure 1 using the same hyperparameters as the authors and confirmed the non-convergent behavior. However, optimizing a simple quadratic function with AdamW is not a meaningful or representative test case—no practitioner would use such a setup. The example is illustrative but unrealistic, and the hyperparameters were not tuned.  \n   More generally, convergence is not a particularly relevant metric in modern deep learning, especially in large-scale contexts such as LLM training, where optimization intentionally remains far from stationary points.\n\n5. **Triviality of the main conceptual claim.**  \n   The observation that decoupled weight decay shifts the optimizer’s equilibria away from the minima of the original loss is not new and has long been known for SGD with weight decay as well. While the formal characterization presented here is neat, it adds little conceptual novelty and does not lead to actionable guidance or any principled modification of AdamW.\n\n6. **Limited practical relevance.**  \n   Despite focusing on a widely used optimizer, the paper offers no concrete takeaways for practitioners. The analysis remains largely theoretical and provides limited insight into how AdamW behaves in realistic training regimes or how its limitations could be mitigated in practice.\n\n**In summary**, I find the work promising in scope but limited in impact:\n1. The literature review is incomplete.  \n2. The setup is overly restrictive and should be extended using the SDE formulation of AdamW proposed in [1].  \n3. The experimental design contains a conceptual flaw.  \n4. The work provides little practical insight for real-world optimization.\n\n**[1]** *Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise.*  \nEnea Monzio Compagnoni, Tianlin Liu, Rustem Islamov, Frank Norbert Proske, Antonio Orvieto, Aurélien Lucchi.  \n*International Conference on Learning Representations (ICLR), 2025.*\n\n---\n**Relevant prior work on ODE/SDE analyses of optimization algorithms**\n\n1. **Helmke, U. & Moore, J. B. (1994).**  \n   *Optimization and Dynamical Systems.* Springer London.  \n   — Classical textbook connecting continuous-time dynamical systems and optimization via gradient flows.\n\n2. **Su, W., Boyd, S., & Candès, E. (2014).**  \n   *A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights.*  \n   *Advances in Neural Information Processing Systems.*  \n   — Foundational ODE model for Nesterov acceleration; initiated the modern line of continuous-time analyses.\n\n3. **Li, Q., Tai, C., & Weinan E. (2017).**  \n   *Stochastic modified equations and adaptive stochastic gradient algorithms.*  \n   *International Conference on Machine Learning (ICML).*  \n   — Derives stochastic modified equations for stochastic gradient algorithms, laying the foundation for weak ODE/SDE approximations.\n\n4. **Li, Q., Tai, C., & Weinan E. (2019).**  \n   *Stochastic modified equations and dynamics of stochastic gradient algorithms I: Mathematical foundations.*  \n   *Journal of Machine Learning Research, 20(1): 1474–1520.*  \n   — Provides a rigorous mathematical foundation for the weak SDE approximations of SGD and related methods.\n\n5. **Orvieto, A. & Lucchi, A. (2019).**  \n   *Continuous-time models for stochastic optimization algorithms.*  \n   *Advances in Neural Information Processing Systems 32.*  \n   — Introduces a general ODE/SDE formalism for analyzing SGD, momentum, and adaptive algorithms, establishing the link between discrete-time optimizers and their continuous-time limits.\n\n---\n\n*These works collectively constitute the theoretical lineage of continuous-time analyses of optimization methods. Including them would anchor the AdamW ODE analysis within its broader ODE/SDE context, highlighting both deterministic and stochastic generalizations.*"}, "questions": {"value": "**Questions**\n\n1. **On the role of $\\epsilon$ and connection to SignSGDW.**  \n   When $\\epsilon = 0$, the implicit regularization in the objective vanishes. While this choice is not standard in practice, it raises the question of whether practitioners might actually benefit from setting $\\epsilon = 0$. This parameter contributes to the stability of the algorithm, yet it also seems to drive the optimizer away from its original objective.  \n   The case is intriguing because for $\\beta_1 = \\beta_2 = \\epsilon = 0$, AdamW effectively reduces to a “SignSGDW”-like scheme. According to the analysis presented in the paper, this would imply that SignSGDW minimizes the original loss function, which seems questionable. In practice, however, this regime appears to behave quite differently.\n\n2. **On the effect of $\\epsilon = 0$ in practice.**  \n   Using the following hyperparameters  \n   $ \\eta = 10^{-4},\\quad \\beta_1 = 0.99,\\quad \\beta_2 = 0.9,\\quad \\lambda = 1.0,\\quad \\epsilon = 0,\\quad T = 20000$,\n   I observe almost perfect convergence to $x = 0.5$ on the same quadratic example. This contrasts with the non-convergent behavior reported in Figure 1. Could the authors comment on why setting $\\epsilon = 0$ appears to *improve* convergence in this case?\n\n3. **On convergence of the ODE.**  \n   What specific technical elements are still missing to establish convergence guarantees for the AdamW ODE under suitable assumptions (e.g., convexity, smoothness, or bounded trajectories)? A short discussion of these obstacles would help clarify the current theoretical frontier.\n\n4. **On the equal-$\\beta$ regime.**  \n   The case $\\beta_1 = \\beta_2$ appears practically relevant [2]. Since the paper observes that this setting simplifies the stability condition, I suggest expanding the discussion and empirical analysis of this regime, possibly providing explicit confirmation in experiments.\n\n[2] “In Search of Adam’s Secret Sauce” (Orvieto & Gower, 2023)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B7revGTabI", "forum": "kW5hSRG5wq", "replyto": "kW5hSRG5wq", "signatures": ["ICLR.cc/2026/Conference/Submission21688/Reviewer_WZfq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21688/Reviewer_WZfq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760524297452, "cdate": 1760524297452, "tmdate": 1762941890729, "mdate": 1762941890729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the implicit bias of AdamW, covering the case where the stability constant $e\\neq 0$. As a complement to [1], this work derives the implicit objective function and corresponding constraints. In addition, it discusses the stabilities of equilibria. Under the one-dimensional case, it proves that the local convexity of the implicit object is not sufficient to guarantee the stability, and establishes the exact conditions instead. Under the high-dimensional case, it proposes a potential Lyapunov function under certain assumptions. \n\n[1]. Xie and Li, Implicit bias of AdamW: $\\ell_\\infty$ norm constrained optimization"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. In fact, studying the stability of equilibrium is both interesting and important for people's understanding of different optimizers. Specifically, given that the prior study [1] only characterizes the limit point when assuming the existence of such limit points, this work provides a good start of another question: whether such a limit point exists?\n\n2. The conclusion that the local convexity of the implicit objective is not sufficient to guarantee the stability is quite surprising. I believe people can get more insights from this work, if this conclusion can be extended to more general cases."}, "weaknesses": {"value": "1. While I do feel this is an interesting work, its first conclusion regarding the implicit objectives of AdamW, is somewhat incremental, specifically given the prior works [1, 2]. \n\n2. In fact, I find that the additive term of the implicit objective is quite sensitive to the scale of $e$, which is omitted in the prior study [1]. Given that this constant is usually very small, like $1e-8$ in practice, I'm skeptical about the difference between the conclusion of this work and that of [1]. I do believe authors should add discussions in their Remark 4.4. From my perspective, the effect of the scale of $e$ is much important than the other two cases. In addition, I also wonder when $e$ is positive but sufficiently small, what is the conclusion of this paper? Could the author provide some quantitative analysis?\n\n3. Besides the effect of $e$, the major conclusion of this paper only covers the one-dimensional case, which significantly limits its generality. Under the multi-dimensional case, the construction of the Lyapunov function is based stronger assumption of existence of some matrices. In addition,  I can not see any insights or implications of such Lyapunov functions. Compared to the prior work [2], which successfully constructs that the objective function is a minimizer of their Lyapunov function, the conclusion of this paper for the multi-dimensional case is quite unsatisfactory. \n\n4. As a paper studying the implicit bias of AdamW, I believe it misses the discussions of several relatively related works, like [3, 4, 5, 6]. Specifically, [3] contains a continuous characterization of the moment term and derives a continuous solution. I'm not certain whether such a conclusion can benefit this work. \n\n[2]. Chen et al. Lion secretly solves a constrained optimization: as Lyapunov predicts.\n\n[3]. Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.\n\n[4]. Wang et al. Does momentum change the implicit regularization on separable data?\n\n[5]. Zhang et al. The implicit bias of Adam on separable data. \n\n[6]. Cattaneo et al. On the implicit bias of Adam."}, "questions": {"value": "Could the authors explain Figure 4? I can not understand why the stationary point of a function can serve as an axis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rk7WUfj54O", "forum": "kW5hSRG5wq", "replyto": "kW5hSRG5wq", "signatures": ["ICLR.cc/2026/Conference/Submission21688/Reviewer_sEDy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21688/Reviewer_sEDy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761220629614, "cdate": 1761220629614, "tmdate": 1762941890428, "mdate": 1762941890428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies theoretical properties related to the convergence of AdamW, a commonly used optimizer for training deep neural networks. The authors show that AdamW has an implicit objective, which aligns to the fixed points of AdamW. However, the authors find that the local minima of this implicit object are not always stable for AdamW. They demonstrate this in simple cases where the optimizer does not converge, but oscillates. Finally, the authors provide some preliminary analysis for parameters of more than 1-dimension, showing that - in the case where there is convergence of AdamW - the associated Jacobian of the implicit objective is stable."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper was unusually well written. I found it easy to follow and well motivated, which is lacking in many papers, especially some theory papers. \n\n2. As noted above, the motivation was strong. I think it is clear that this work builds on other literature and moves the understanding of state-of-the-art (or at least widely used) optimizers forward. I believe it will be work that the community at large will be interested in. \n\n3. The counter-examples were very clear and demonstrated the theory nicely. The authors do a good job pointing out that these examples are of theoretic interest (given their somewhat artificial nature), which helped ensure that these results were not misrepresented. \n\n4. I liked the figures that show how the bounds depend on different aspects of the problem (Fig. 4). As a non-theorist, I always find myself wanting these kinds of plots and it was helpful to have it. \n\n5. I thought the argument for why considering the continuous time setting was good. Sometimes when reading that kind of analysis, my first thought it is why care about something continuous when we know some of the most interesting properties of DNN optimizers arise from the discrete time setting. But the argument that convergence should happen for useful optimizers at least in the simplified continuous time setting was convincing, especially given the non-convergence results."}, "weaknesses": {"value": "I identified no major weaknesses. I also didn't really identify any points that I thought could be significantly improved upon. My only suggestions are therefore rather minor: \n\n1. It would be useful to provide a little more context about what all the parameters are. I know the intended audience are people already familiar with Adam, but just a reminder of what $\\beta_1$, $\\beta_2$, $\\lambda$, and $e$ are would be helpful. Especially because in first reading this I thought $e$ was the mathematical constant. \n\n2. Related to the above, Algorithm 1 is never referenced. Maybe briefly reviewing AdamW by going through Algorithm 1 would be good. \n\n3. In Figure 3, the caption and the x-axis labels overlap. Making the figure a little smaller would remove that problem. \n\n4. The authors point out that their results bring up something of a paradox - AdamW fails on very simple problems but is the workhorse of the machine learning community. I think this is very interesting and more discussion on this would be interesting and increase the impact of the work. \n\n5. Seeing the examples where AdamW fails to converge (and moves along a limit cycle), I was reminded of the fact that people have found that optimizers of DNNs often do oscillate at the end of training (e.g., \"On the generalization of learning algorithms that do not converge\" Chandramoorthy et al. 2022 - note this is not my paper). Perhaps DNNs trained with AdamW also exhibit this kind of behavior? In that case, your analysis of failure to converge/sensitivity holds even for the way more complex case of actual DNNs, but convergence to a limit cycle is not a \"bad\" thing. Just a thought."}, "questions": {"value": "1. Do DNNs trained with AdamW exhibit any kind of oscillatory behavior, even if they seemingly \"converge\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FGeiijLooy", "forum": "kW5hSRG5wq", "replyto": "kW5hSRG5wq", "signatures": ["ICLR.cc/2026/Conference/Submission21688/Reviewer_6v1r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21688/Reviewer_6v1r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604390112, "cdate": 1761604390112, "tmdate": 1762941890149, "mdate": 1762941890149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the dynamics of AdamW, with a special focus on the dynamics of a continuous-time analogue of AdamW. It begins by describing the fixed points of the AdamW update, and then analyzes the conditions under which these fixed points are stable. It uses these to construct instances where AdamW converges to a limit cycle. However, they verify that these stability conditions do seem to be verified at reasonable values of $\\lambda$ using MLP experiments on MNIST."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Given the complexity of the dynamical system, the stability analysis is fairly clean\n- The authors directly address prior convergence analyses of AdamW and reconcile them with the results in the paper (lines 454-465)\n- The authors include plots of limit cycles and non-convergence which verify the stability conditions"}, "weaknesses": {"value": "- I'm confused about the following two notions of convergence in this paper: (1) for any $\\epsilon$, there exists a setting of the hyperparameters such that $\\liminf L(w) < \\epsilon$ (or converge to an $\\epsilon$-stationary point) and (2) for a fixed set of hyperparameters, $L(w) \\to 0$. For example, RMSProp doesn't satisfy (2) on $L(x) = x^2$ because it converges to the period 2 orbit $x = \\pm \\eta/2$, but it does satisfy (1) because we can take $\\eta$ sufficiently small. Under (1), to argue that AdamW is distinct from Adam, it's not sufficient to show that you converge to a limit cycle, so long as it is possible to pick the hyperparameters in such a way that the limit cycle is arbitrarily small. My understanding is that the paper's focus is for what parameters $w$ and hyperparameters $a,b,\\lambda$ are the dynamics of the continuous-time AdamW flow (eq 4) locally stable, which appears separate from convergence/non-convergence.\n\n- In light of the above comment, it's not necessary for equation 1 to hold at convergence. For example, RMSProp converges to an orbit of period 2, rather than a fixed point of the dynamics. It's not clear to me why AdamW would converge to a single fixed point of the Adam update, or why this is necessary for convergence under criterion (1) above.\n\n- The paper focuses on the continuous time dynamics in eq. 4 under the assumption that empirical learning rates are \"small\" but it does not empirically justify this assumption. There is some evidence to the contrary. For example, AdamW is known to exhibit the edge of stability phenomenon (Cohen et al. 2022 \"Adaptive Gradient Methods at the Edge of Stability\") at practical learning rates which implies that the dynamics of the discrete-time AdamW are very far from those of eq. 4. Appendix A.4 does discuss the discrete time system by analyzing local stability around a fixed point of $z = T(z)$. However, as above it is not justified why such a fixed point is reached or why this is necessary for convergence under (1).\n\n- The citation [Bock & Weiß 2022 \"Non-Convergence and Limit Cycles in the Adam optimizer\"] seems highly relevant – it studies non-convergence of Adam and shows convergence to potentially unstable period 2 orbits and limit cycles."}, "questions": {"value": "- Could you generalize this analysis to check when AdamW converges to a stable period 2 orbit rather than a stationary point? As in [Bock & Weiß 2022 \"Non-Convergence and Limit Cycles in the Adam optimizer\"]\n- Does there always exist a setting of the hyperparameters that forces the size of the limit cycle to $0$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2OHUGn9oWQ", "forum": "kW5hSRG5wq", "replyto": "kW5hSRG5wq", "signatures": ["ICLR.cc/2026/Conference/Submission21688/Reviewer_Fkeb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21688/Reviewer_Fkeb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931353513, "cdate": 1761931353513, "tmdate": 1762941889891, "mdate": 1762941889891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}