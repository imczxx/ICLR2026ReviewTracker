{"id": "OzfHVorKMW", "number": 21427, "cdate": 1758317468422, "mdate": 1759896922447, "content": {"title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs", "abstract": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely on methods from cryptography such as zero-knowledge proofs (ZKPs), but these add significant computational overhead, and remain infeasible for use for large models. In this work, we develop a new insight -- that given a method for performing \\emph{private} LLM inference, one can obtain forms of \\emph{verified} inference at marginal extra cost. Specifically, we propose three new protocols, each of which leverage privacy-preserving LLM inference in order to provide different guarantees over the inference that was carried out. Our approaches are cheap, requiring the addition of a few extra tokens of computation, and have little to no downstream impact. As the fastest privacy-preserving inference methods are typically faster than ZK methods, the proposed protocols also improve verification runtime. Our work provides novel insights into the connections between privacy and verifiability in LLM inference.", "tldr": "We present three protocols that utilize privacy-preserving inference methods to obtain verified inference in LLMs.", "keywords": ["privacy", "verifiability", "trust", "smpc", "zk"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee5f8944bb4c7e088b5fe64b3e793453be4e5c10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focus on the problem of verifiability of hosted LLM inference services. Different from prior works that use heavy ZKP technologies, the authors propose to verify the LLM output by inserting random tokens to the user prompt or adding random noises to the token embeddings, where the output logits can be significantly different if a different (typically smaller) model is used. The user-side perturbation is made oblivious to the server owing to the assumed underlying MPC-based inference. The proposed methods exhibit both theoretical and statistical resilience to attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The problem of verifying MLaaS is an increasingly important direction.\n- The idea of build verification upon MPC-based secure inference is interesting."}, "weaknesses": {"value": "- Table 2 only shows experiments for one single forward pass. However, the proposed approaches  require continual user interaction at every decoding step. Runtime for a full generation is need to showcase the introduced additional cost.\n- Protocol 1 and Protocol 2 both inherently require user interaction for every step of token decoding. This is not reasonable in real-world deployment,\n- Protocol 3, which is claimed to be non-interactive, also has a significant limitation. For any models with instruction-following, the key appending does not work."}, "questions": {"value": "Please refer to the weakness. I belive the current limitations for each protocol is significant and not applicable to real-world deployment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qhnmsI3qZi", "forum": "OzfHVorKMW", "replyto": "OzfHVorKMW", "signatures": ["ICLR.cc/2026/Conference/Submission21427/Reviewer_NATw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21427/Reviewer_NATw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301141537, "cdate": 1761301141537, "tmdate": 1762941762735, "mdate": 1762941762735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical practical challenge in remote large language model (LLM) inference: untrusted third-party providers may substitute expensive, high-capacity models with cheaper, weaker alternatives (e.g., replacing LLaMA-70B with LLaMA-7B) to cut costs, while users lack mechanisms to verify the authenticity of the computation. Existing verification tools (e.g., zero-knowledge proofs, ZKPs) are infeasible for LLMs due to prohibitive computational overhead.  The authors propose three distinct protocols that leverage privacy-preserving inference to provide targeted verification guarantees. The work aims to advance both practical verification tools for remote LLM inference and theoretical understanding of privacy-verifiability links in AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses a high-priority problem in LLM deployment with a novel, practical insight: leveraging private inference to enable low-overhead verification. \n2. The link between private and verified inference is a key innovation."}, "weaknesses": {"value": "1. The presentation of this paper is not good enough, and it is a little hard to understand how it works. Maybe a schematic graph helps\n2. What are connections between three protocols? How does this paper relate to existing works?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "qqGLXaKCBP", "forum": "OzfHVorKMW", "replyto": "OzfHVorKMW", "signatures": ["ICLR.cc/2026/Conference/Submission21427/Reviewer_eGgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21427/Reviewer_eGgA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840391259, "cdate": 1761840391259, "tmdate": 1762941761633, "mdate": 1762941761633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the intersection between privacy-preserving computation and verifiable inference for large language models (LLMs). It argues that when privacy-preserving techniques such as Secure Multi-Party Computation (SMPC) or Fully Homomorphic Encryption (FHE) are already used for model inference, these mechanisms can be extended to provide verifiable computation almost for free. \n\nTo support this idea, the paper proposes three novel protocols: Logit Fingerprinting, Logit Fingerprinting with Noise, and Key Appending. Experimental results show that these methods can detect dishonest inference efficiently and run much faster than zero-knowledge proof (ZK)–based approaches.\n\nSince I am not an expert in security or cryptographic verification, I may not be able to fully assess the depth or novelty of the proposed mechanisms. However, from a general machine learning perspective, the idea of connecting privacy and verification in LLM inference seems potentially impactful."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work presents a creative and timely exploration of a relatively unstudied relationship between privacy and verifiability in large model inference. \n\n2. The motivation is clear, as the growing reliance on third-party model hosting introduces both privacy and integrity risks.\n\n3. The three proposed protocols provide a spectrum of practical trade-offs between interaction cost, computational efficiency, and verification strength."}, "weaknesses": {"value": "1. The paper seems to rely mostly on empirical results rather than formal proofs, and it is not clear how strong the guarantees are compared with existing cryptographic approaches.\n\n2. The evaluation, while comprehensive in experiments, could include more discussion of practical deployment aspects such as latency, communication overhead, and integration into existing systems. It is also not entirely clear how well the proposed methods scale to long or interactive LLM sessions.\n\n3. For readers without a strong security background, it would be helpful to have clearer explanations of key assumptions (for example, what it means for a provider to be “honest but curious,” or how the privacy mechanism ensures non-collusion)."}, "questions": {"value": "- Could the authors clarify, in more intuitive terms, what kind of verification guarantee each protocol provides compared with standard cryptographic verification? For instance, are these probabilistic or statistical guarantees?\n- How do the proposed methods perform in more realistic, multi-turn generation settings where verification cannot be repeated at every step?\n- To what extent can these protocols be combined with lighter-weight security assumptions such as trusted execution environments (TEEs)?\n- From an engineering perspective, how difficult would it be to integrate these protocols into existing inference frameworks used in industry?\n- Could the authors discuss more concretely what kind of real-world scenarios or users would most benefit from these methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SWIH7qlUUI", "forum": "OzfHVorKMW", "replyto": "OzfHVorKMW", "signatures": ["ICLR.cc/2026/Conference/Submission21427/Reviewer_Y7kc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21427/Reviewer_Y7kc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989081655, "cdate": 1761989081655, "tmdate": 1762941760915, "mdate": 1762941760915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "he submission proposes several mechanisms to ensure the verifiability\nof privacy-preserving LLM inferences. By \"verifiability,\" the user who\nsupplies private input x can verify that the output y is legitimately\nobtained by computing M(x). In the FHE scenario, the user may send a\nciphertext ct = Enc(x); the inference provider homomorphically evaluates\nM on ct, and the user decrypts the resulting ciphertext to obtain y.\nWhile one could naively achieve verifiability by using zero-knowledge\nproofs, this solution tends to incur significant overhead.\n\nThe submission explores alternative and more lightweight methods:\n\n- In Protocol 1, the user inserts randomly selected sentinel tokens into\nthe tokenized prompt at random positions. The user then observes the\noutput logit vector at all token positions and compares it against the\nprecomputed cached logits for model M.\n\n- Protocol 2 is a variant of Protocol 1, which adds randomly sampled\nnoise to the token embeddings.\n\n- In Protocol 3, the user appends a randomly generated key to the prompt\nwith explicit instructions asking to repeat the key in the response. The\nuser then checks that the response includes the same key."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The submission explores alternative solutions to verifiable LLM\ninference. Since zero-knowledge proofs for verifiable LLM are\nprohibitively expensive due to the underlying cryptographic operations,\nproposing a new paradigm is a promising direction."}, "weaknesses": {"value": "Unfortunately, the submission does not adequately justify the security\nof the proposed methods.\n\n- Protocol 3 does not provide a meaningful guarantee. Since the user\nprompt always follows the same format, in the FHE setting, any malicious\ninference provider can identify the location of the key in the text. By\napplying suitable homomorphic operations, an adversary can single out\nthe ciphertext containing the key and append it to an arbitrary output\ntext. Thus, this methodology barely meets the requirement of verifiable\ncomputation.\n\n- The authors present some experiments in the SMPC setting, where a\nprompt is secret-shared among multiple parties and at least one behaves\nhonestly. However, if the remaining parties are dishonest, one should\nsimply use SMPC with **active security** to achieve verifiability, as\nthe protocol can then tolerate any misbehavior. This means that as soon\nas any dishonest party deviates from the protocol by trying to evaluate\nsecret shares of the private prompt on an incorrect circuit (i.e., wrong\nmodel), the honest party can detect such cheating behavior.\n\n- In general, the paper does not formally define what kind of\n\"verifiability\" is guaranteed by the proposed solutions. I recommend\nthat the authors make the security notion mathematically precise. For\nexample, in the context of ZKP, verifiability can be formulated in a\ngame-based manner: for any input $x$ and model $M$, and any\n(computationally bounded) adversary $A$ outputting a proof $\\pi$ and\n(encryption of) $y$, the probability $\\Pr[M(x)\\neq y \\land V \\text{\naccepts } \\pi]$ is negligible. SMPC with active security also formally\nguarantees that either $M(x)=y$ is correctly computed (or otherwise the\nprotocol simply aborts if it does not have the guaranteed output\ndelivery property), even in the presence of malicious parties. Both\nparadigms provide formal security proofs against _any_ adversarial\nstrategy. In contrast, the submission only checks security with respect\nto _specific attack strategies_, which is generally not a sound\nmethodology for analyzing security."}, "questions": {"value": "- Although the main goal of the paper is verifiability, the paper does\nnot provide any formal security notion as opposed to ZKP and SMPC. Could\nthe authors define a formal security definition similar to (knowledge)\nsoundness of ZKP or active security of SMPC?\n\n- If one were to use SMPC to achieve privacy-preserving LLM inference,\nwouldn't verifiability be trivially ensured by employing actively secure\nSMPC? Note that actively secure SMPC does not necessarily require\nzero-knowledge proofs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2bn4KYfzN3", "forum": "OzfHVorKMW", "replyto": "OzfHVorKMW", "signatures": ["ICLR.cc/2026/Conference/Submission21427/Reviewer_qDjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21427/Reviewer_qDjq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097198729, "cdate": 1762097198729, "tmdate": 1762941760080, "mdate": 1762941760080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}