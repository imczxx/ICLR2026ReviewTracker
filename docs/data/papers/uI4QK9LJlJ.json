{"id": "uI4QK9LJlJ", "number": 17710, "cdate": 1758279602810, "mdate": 1759897159028, "content": {"title": "Less Forgetting Learning: Memory-free Continual Learning Classification", "abstract": "Continual Learning (CL) refers to a model's ability to sequentially acquire new knowledge across tasks while minimizing Catastrophic Forgetting (CF) of previously learned information. Many existing CL approaches face scalability challenges, often relying heavily on memory or a model buffer to maintain performance. To address this limitation, we propose \"Less Forgetting Learning\" (LFL), a memory-free CL framework for class and task incremental learning classification that does not rely on any memory buffer.\n\nThe LFL adopts a stepwise freezing and fine-tuning strategy. Different components of the network are trained in separate stages, with selective freezing applied to preserve critical knowledge. The framework leverages knowledge distillation to strike a balance between stability and plasticity during learning. Building upon this foundation, LFL+ incorporates an under-complete Auto-Encoder (AE) to preserve the most informative features. In addition, the LFL+ addresses the bias toward new classes in the classification head.  Extensive experiments on three benchmark datasets show that LFL achieves competitive performance while requiring only 2.53% of the model buffer used by state-of-the-art methods. In addition, we propose a new metric designed to assess CL's plasticity-stability trade-off better.", "tldr": "The paper proposes a memory-free continual learning approach based on knowledge distillation.", "keywords": ["Continual Learning", "Neural Networks", "Catastrophic Forgetting", "Class Incremental Learning", "Task Incremental Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9ca02ffffe4189e4143df82007df9fe1a904acc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a continual learning framework, LFL, and its enhanced version, LFL+, to mitigate catastrophic forgetting. The core contribution lies in its stepwise freezing and training strategy. The model is decomposed into a shared feature extractor and task-specific heads. When learning a new task, LFL controls the updates of different components through several training phases. First, the shared parameters and old task head are frozen while only the new task head is trained to adapt to the new task; next, the new task head is frozen, and the shared parameters and old task head are aligned using soft targets; finally, a joint fine-tuning phase is performed to consolidate all learned knowledge. LFL+ further integrates an autoencoder to constrain and preserve essential feature representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on an important and widely discussed problem in the field of continual learning and proposes a memory-free approach.\n2. The paper is generally clearly written, making it easy to follow the main ideas."}, "weaknesses": {"value": "1. The Introduction and Related Work sections provide a broad overview of Continual Learning. While this is useful for context, they fail to frame and motivate the specific contribution of this work. It isn't easy to discern the key differences between this paper and other memory-free continual learning methods.\n2. The proposed 4- or 5-phase training paradigm lacks details on how each phase is conducted. In the experiments, only a generic grid search is reported. Given that even standard end-to-end training methods can exhibit substantial variability, the absence of practical guidance on hyperparameter selection in this multi-phase framework limits its generalizability and practical applicability to other domains.\n3. LFL may heavily rely on a similar dataset or task settings. One of the core ideas of LFL is to use the output produced by the old model on new data as soft targets. However, if the data distributions of the old and new tasks differ significantly, the targets generated by the old model on the new data may be meaningless or even noisy. In such cases, forcing the new model to fit these noisy signals may actually degrade performance. \n4. The shared encoder is described as a “teacher,” but in practice, it serves as a shared representation for both old and new tasks. During the third training phase, if the shared encoder is updated and its internal representations drift, this may lead to irreversible degradation in performance on the old tasks.\n5. Although the autoencoder introduced in LFL+ is intended to preserve feature representations, it also requires training a separate autoencoder for each task. Since autoencoder training itself is a relatively complex and sensitive optimization process, incorporating it into an already complicated five-phase training pipeline makes the overall approach difficult to justify in practice.\n6. Many recent studies have shown that storing a small number of representative samples (i.e., a small fixed memory buffer) is generally acceptable and can mitigate catastrophic forgetting in a simple and highly effective manner, with much lower implementation difficulty and computational cost. LFL incurs significant engineering complexity to address a problem that could likely be solved more efficiently with a small memory buffer and straightforward replay."}, "questions": {"value": "Please refer to the Weakness section for detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SYIQGRv3fq", "forum": "uI4QK9LJlJ", "replyto": "uI4QK9LJlJ", "signatures": ["ICLR.cc/2026/Conference/Submission17710/Reviewer_M85P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17710/Reviewer_M85P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760558284414, "cdate": 1760558284414, "tmdate": 1762927547674, "mdate": 1762927547674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Continual learning (CL) deals with the incremental training of neural networks from a data stream. Owing to the nature of gradient-based optimization, catastrophic forgetting (CF) occurs: the performance on old tasks drops as newer tasks are learned. To mitigate forgetting, authors propose knowledge distillation by using the output of a network from the end of the previous task to regularize the updates on the current task. They pair this with a slight parameter expansion method of adding task-specific heads. They demonstrate the strong performance from their method on common CL datasets and in two settings (class- and task-incremental learning)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- studied both CIL and TIL\n- extensive set of experiments"}, "weaknesses": {"value": "- limited novelty: method is a combination of existing strategies (mainly, knowledge distillation with task-specific heads)\n- Paper focuses on memory-free CL, but benchmarks against a large set of memory-based methods (which are outperformed mostly tho) -- authors are encouraged to add 2 to 3 recent memory-free methods into their comparison\n- Plasticity stability ratio has priorly been introduced [1] (please comment on differences)\n- Presentation and description of the algorithm should be updated for improved readability; there are a lot of subscripts, superscripts, underline, etc.\n- Its unclear how the method works in CIL: There are no task-IDS available to select the head, because only one head is expanded in CIL. So, there is only one head available in CIL, which makes doing steps 2, 3, 4 impossible?\n- ablation study on the individual components/ steps are missing"}, "questions": {"value": "- How does the method work in the CIL setup? \n- line 301: what is the cross-validation set? Wouldnt it require a memory buffer?\n\n**Suggestions:**\n- Table 2 does not add, maybe move to appendix\n- use correct \\cite commands (currently misses brackets around references in the text, eg line 37)\n- improve figure 1: integrate legend into figure\n- I think that the authors do not make themselves a favour by benchmarking agains memory-based methods. I'd suggest sticking to the memory-free regime for a fairer comparison, and benchmark agains memory-free methods only (such as already done with EWC)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HockFXLrID", "forum": "uI4QK9LJlJ", "replyto": "uI4QK9LJlJ", "signatures": ["ICLR.cc/2026/Conference/Submission17710/Reviewer_9HJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17710/Reviewer_9HJx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569991701, "cdate": 1761569991701, "tmdate": 1762927547239, "mdate": 1762927547239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Less Forgetting Learning (LFL), and its enhanced variant LFL+, which are memory-free Continual Learning (CL) methods designed to mitigate Catastrophic Forgetting without relying on an explicit memory buffer. LFL involves a multi-step freezing and fine-tuning training protocol incorporating knowledge distillation using previous target signals. The enhanced variant, LFL+, incorporates an auto-encoder to preserve task-relevant features, along with a bias correction mechanism to mitigate classification head and bias toward new classes. Experiments on CIFAR-100, Tiny-ImageNet and ImageNet-1000 under Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL) show competitive performance compared to recent buffer-based methods,  with improved scalability and shorter training time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method addresses a well-know problem in CL, namely the need for effective methods that do not rely on the use of a buffer to store samples from previous tasks. \n\n- The method contributes a clear and simple stepwise freezing and fine-tuning procedure addressing the stability-plasticity dilemma in CL\n\n- In the LFL+ variant, the use of the autoencoder to preserve the features of the previous task is reasonable, because it overcomes the problem of forgetting on the autoencoder itself, as it is not continual trained on the sequence of tasks, but re-trained task by task only on recent features. \n\n- The bias correction mechanism addresses class imbalance in a principled manner to enhance performance in class incremental learning.\n\n- Comprehensive empirical evaluation compares LFL and LFL+ against a range of strong baselines and across multiple standard benchmarks."}, "weaknesses": {"value": "- While the overall paper is generally easy to follow, the Method section (Section 3) is often unclear and ambiguous. For instance, the statement \"the new task head is kept frozen, while the feature extractor and the previously learned task head are updated during training after random initialization\" (line 227) creates confusion between random initialization and parameter fine-tuning. Ambiguity is also introduced through figures and pseudo-code notation, which do not always clarify the flow between \"randomly initialized/updated/fine-tuned parameters\", and frozen parameters. Clearer language and explicit step-by-step descriptions would help readers precisely reproduce the pipeline.\n\n- The “memory-free” claim is repeated frequently and highlighted as a key strength versus competitors. However, while LFL/LFL+ do not store real data exemplars from previous tasks, they do require a temporary buffer to store logits from previous models for knowledge distillation. This memory requirement, although far less than storing raw images, is intrinsic to the approach and not explicitly stated.\n\n- The discussion of cross-validation details (used for bias correction in LFL+) is not adequately described, limiting reproducibility.\n\n- Unclear training procedure: the paper does not explicitly indicate a fixed number of epochs per task in the training procedure. The description refers generically to “training until convergence” (line 217) for each step or task, without providing precise details on the epoch count per task. Therefore, the paper seems to suggest that the number of epochs per task is treated as a hyperparameter defined empirically or by convergence conditions, not strictly defined in the experimental protocol.\n\n- In the experimental setup, using different buffer sizes for Class-IL and Task-IL methods may introduce a form of bias in the evaluation, making results hard to compare “on equal ground\". As a buffer-free method, LFL/LFL+ results are relevant and competitive, but choosing different amount of memory for buffer-based methods in Task-IL could artificially disadvantage them. In addition, comparison between Task-IL results in this paper and those in the DER++ paper reveals differences for several shared methods, raising concerns about reproducibility.\n\n- Section 4.3 (Training Time Comparison) raises further concerns:\n\n i) The reported training time difference between CL-IL and Task-IL is surprisingly large. For buffer-based methods like DER++, the training pipeline is virtually identical for CL-IL and Task-IL, except for access to task-ID at inference, which should not substantially affect runtime. Such large training time differences may only be justified if different numbers of training epochs were used, which again links back to ambiguity about the training protocol.\n\nii) The training time difference is attributed to buffer management and dynamic model complexity. While generally reasonable, buffer size mainly impacts specific methods. For example, DER++ retrieves the same number of buffer samples per stream batch regardless of buffer size.\n\niii) It is unclear how LwF, which essentially corresponds to step 1 of the proposed method, can have longer training time than LFL on CIFAR-100."}, "questions": {"value": "Please address and resolve the concerns raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xLPmvwXW7N", "forum": "uI4QK9LJlJ", "replyto": "uI4QK9LJlJ", "signatures": ["ICLR.cc/2026/Conference/Submission17710/Reviewer_q7UX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17710/Reviewer_q7UX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748832925, "cdate": 1761748832925, "tmdate": 1762927546789, "mdate": 1762927546789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work develops a Less Forgetting Learning (LFL) paradigm to balance plasticity and stability in continual learning. Specifically, LFL is performed with four steps, each of which is based on the previsous output. Besides. LFL+ integrates an additional Auto-Encoder for feature retention further. Extensive results on CIFAR and ImageNet datasets show the effectiveness of the proposed LFL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work aims to devise a exemplar- free continual learning method, which does not rely on memory buffer. \n\nThe implementation details on the four steps in LFL are provided, and it might be easy to reproduce the method properly.\n\nThe experimental evaluations are comprehensive and thorough. Besides, training cost is compared as well."}, "weaknesses": {"value": "The techinal contributions are not significant, as the proposed LFL is mainly composed of several freezing and fine-tuning steps. It is hard to identify the new insights on continual learning. \n\nAfter reading the four steps in LFL, I am confused by the relations between them. Specifically, it is unclear why step 2 and step 3 are separated from each other. In general, it is reasonable to fine-tune the feature extractor and train the new classifier at the same time, given the knowledge distillation regularization. It is needed to clarify the strength of learning them in two steps. Moreover, what is the motivation behind step 4? \n\nThe objective loss function for step 5 (as in Eq.17) is too complex, which may make it hard to adapt to various scenarios."}, "questions": {"value": "Some experimental results are not convining. For example, in Table 9, the ablation model with step 2 even achieves 38.92 accuracy score, which outperforms SI and EWC, and competes with LwF, as shown in Table 3. It is hard to understand about why step 2 can bring such remarkable improvements, as it is simply freezing the backbone and training new classifier from scratch."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gyvS5aw6AM", "forum": "uI4QK9LJlJ", "replyto": "uI4QK9LJlJ", "signatures": ["ICLR.cc/2026/Conference/Submission17710/Reviewer_N6JE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17710/Reviewer_N6JE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822202518, "cdate": 1761822202518, "tmdate": 1762927546408, "mdate": 1762927546408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}