{"id": "cj1MAx7lKs", "number": 20769, "cdate": 1758309891412, "mdate": 1763582246119, "content": {"title": "OpenApps: Simulating Environment Variations to Measure UI Agent Reliability", "abstract": "Reliability is key to realizing the promise of autonomous UI-agents, multimodal agents that directly interact with the apps humans use, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments---often clones of existing apps--- which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent’s ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than 50\\% across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from 63\\% to just 4\\% across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations.", "tldr": "We introduce a new environment, OpenApps, for generating thousands of versions of apps to test UI agent reliability.", "keywords": ["reinforcement learning", "agents", "envrionment", "reliability"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ddf0a86573a33287fb65d79dfdb37f4fa954fb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a light-weight version of a benchmark for GUI agents, which presents high reproducibility and easy/cost-effective evaluations. Especially, the authors focused on evaluating the reliability of the agents across UI variants. Across six applications that are popularly employed in many routines, authors configure various combinations of UI diversities, constituting more than 10,000 independent cases. Based on the cases, the vulnerability of agents is revealed, and their qualitative analysis is presented."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "On top of all, I believe that this work is highly notable because a light-weight benchmark for GUI agents is a missing yet demanding block in this community. I describe several other strengths of this work.\n1. Important problem: the reliability of agents in practice is a highly crucial problem. I believe that this work represents an important step forward for the community.\n2. Robust success detection logic: the authors put considerable effort into creating success detectors that can work around diverse UI variations. I believe that the efforts that the authors put into this part should be recognized, as this is known to be a challenging problem.\n3. Extensive experiments: the volume of the experiments is notable. I believe that such a massive numbers of experiments make the results presented more reliable.\n4. Interesting observations: the authors also present a detailed behavioral analysis. These observations highlight the shortcomings of current agents, enabling the design of future solutions."}, "weaknesses": {"value": "I discuss the weaknesses of this paper, including questions and suggestions.\n1. Mobile device control benchmarks: while the paper presents fruitful discussion on web benchmarks, discussion on mobile device control benchmarks [1,2,3,4] (which also overlaps with OS-level benchmarks) is absent. Notably, B-MoCA [2] and AndroidWorld [3] both tackle the robustness of the agents, where the former one demonstrates the feature of UI variations and degradation of agents’ performance with respect to the variations, similar to this work. Yet, I do think that this work presents unique and distinct features from prior works (e.g., focus on easy reproduction), which I hope to read in the revision. \n2. HTML diversity: while the use of FastHTML is desirable, I question if it would have negative effects in terms of HTML diversity. To elaborate, I think evaluating the agents with diverse formats can be more appealing if the agents take text input.\n3. Intentionally misleading description: I worry this is out of scope in this work. There are many works tackling the robustness of the agents in an adversarial manner [5]. However, from my understanding, the UI variations are a ‘natural’ perturbation challenging the agent's robustness rather than ‘intentional’ (line 188-189). Such features should be handled differently, in my opinion.\n4. Simplicity of tasks: the proposed test suite suffers from both (1) a lack of diversity and (2) a lack of challenges. There is not enough headroom for improvements in this benchmark for the state-of-the-art agent (i.e., GPT-4o), which I assume would be a bigger problem with the recent agent (e.g., GPT-5). I suggest diversifying the tasks in terms of both volumes (i.e., more than 15 task templates) and difficulties.\n\nI do believe that this work has a strong potential to be a highly noteworthy work that can function as a standardized benchmark.\n\n--- \n\nReferences:\n\n[1] Rawles et al., “Android in the Wild: A Large-Scale Dataset for Android Device Control” (2023).\n\n[2] Lee et al., “Benchmarking Mobile Device Control Agents across Diverse Configurations” (2024).\n\n[3] Rawles et al., “AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents” (2024).\n\n[4] Zhang et al., “LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation” (2024).\n\n[5] Wu et al., “Dissecting Adversarial Robustness of Multimodal LM Agents” (2025)."}, "questions": {"value": "For brevity, I included questions and suggestions in the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RwzBirdG1b", "forum": "cj1MAx7lKs", "replyto": "cj1MAx7lKs", "signatures": ["ICLR.cc/2026/Conference/Submission20769/Reviewer_5Ghj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20769/Reviewer_5Ghj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761697143014, "cdate": 1761697143014, "tmdate": 1762934202622, "mdate": 1762934202622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenApps, a novel, lightweight, open-source ecosystem designed to measure a new dimension of reliability for autonomous UI-agents: performance fluctuations across app variations. Current evaluations rely on fixed environments, failing to capture how changes in app design, appearance, or content affect agent success. OPENAPPS provides six common apps (calendar, maps, messenger, etc.) that are highly configurable via simple Python/YAML files, enabling large-scale, reproducible experiments (over 10,000 trials conducted). The study finds that while reliability within a fixed app is stable, reliability across app variations fluctuates drastically (e.g., Kimi-VL-3B's success rate varied from 63% to 4%), and that failure modes like looping and hallucination are highly environment-dependent. This highlights app variation as a critical, overlooked axis of agent reliability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly identifies the critical gap between testing on fixed clones and real-world deployment, where app style, content density, and language constantly change. This significantly advances the utility of reliability metrics.\n\n2. OpenApps is designed for parallel, large-scale study (single CPU, < 10MB memory, pure Python). This low overhead is a crucial enabler for large-scale RL training and comprehensive testing that previous VM-based or complex web-clone environments could not afford.\n\n3. Beyond average success rates, the study provides valuable diagnostics on how environment changes induce specific failure modes."}, "weaknesses": {"value": "1. The paper focuses exclusively on 15 simple, short-horizon tasks (e.g., adding a to-do item). While justified to isolate reliability, the performance of agents on complex, multi-app workflows (which is the ultimate promise of UI-agents) remains unevaluated. Future work needs to extend the task set to long-horizon, multi-step tasks to form a comprehensive reliability benchmark.\n2. The study primarily varies each app factor (appearance or content) independently. The authors acknowledge that interactions between multiple variations (e.g., dark theme + German language + dense content) could expose novel failure modes. A brief initial exploration of combined variations would strengthen the argument.\n3. The paper provides excellent diagnostics but offers limited concrete proposals for agent development (e.g., does VLM fine-tuning on synthetic variants solve the problem? Should contrast-boosting pre-processing be used for dark themes?). While the goal is diagnosis, suggesting a simple architecture or training technique that could leverage the synthetic data would enhance the paper's prescriptive value."}, "questions": {"value": "1. The flexibility of OpenApps allows generating training data across thousands of versions. Did the authors explore whether fine-tuning a model (e.g., UI-TARS) on a synthetic dataset generated by OpenApps could significantly improve its overall deviation (Figure 5) compared to training only on a single fixed version?\n\n2. The paper notes GPT-4o performs highly when given simplified AX tree representations along with the screenshot. Which specific content variations (e.g., German language, adversarial text) affected the reliability of AX tree parsing more than the visual recognition, or was the failure primarily a model-level reasoning issue?\n\n3. The reward function is deterministic. For complex, open-ended tasks, a continuous partial reward is often more useful. Could the authors confirm that the Python logic of OpenApps could support easily pluggable, continuous reward functions (e.g., L2 distance between the current state vector and the target state vector) for future RL training efforts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PdLXnPAWQo", "forum": "cj1MAx7lKs", "replyto": "cj1MAx7lKs", "signatures": ["ICLR.cc/2026/Conference/Submission20769/Reviewer_1P4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20769/Reviewer_1P4o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948386668, "cdate": 1761948386668, "tmdate": 1762934201329, "mdate": 1762934201329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenApps, a lightweight and scalable benchmark designed to evaluate the **reliability of multimodal UI agents under app-environment variations**. It exposes an overlooked weakness in current evaluation protocols—agents’ fragility to minor visual or structural changes in otherwise identical tasks. The framework is technically sound, reproducible, and empirically comprehensive across seven models. Results show dramatic reliability drops (up to 50% across app versions) and reveal new failure behaviors such as looping and hallucination. The contribution is both **timely and impactful**, offering a reproducible infrastructure that can underpin future robustness research.\n\nWhile methodologically solid, the work would benefit from a **clearer theoretical framing of “reliability,” richer task diversity, and improved presentation**. The current tasks are simple and limited to single-app workflows; including long-horizon tasks would strengthen generality. Despite these limitations, the paper’s novelty, execution quality, and potential community value are strong."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper identifies a key blind spot in existing agent benchmarks: reliability across app variations. Unlike prior environments that focus on fixed app clones, OpenApps systematically quantifies how changes in design and content affect UI-agent performance. This is a genuine conceptual contribution to the field of multimodal agent evaluation.\n2. OpenApps is implemented in pure Python and runs on a single CPU, removing the heavy dependencies of prior environments (e.g., emulators, Docker containers, or large memory requirements). This design choice makes it widely accessible and reproducible.\n3. The authors conduct 10,000+ trials across seven state-of-the-art agents, including GPT-4o, Claude Sonnet, Kimi-VL, Qwen-VL, and UI-TARS. The scale and comprehensiveness of these experiments convincingly demonstrate the practical significance of app variation as a factor in agent reliability.\n4. Results are striking: task success can fluctuate by more than 50% across app versions, and specific models show massive degradation (e.g., Kimi-VL drops from 63% to 4%). The paper also documents behavioral shifts such as looping and hallucination, revealing that environmental variability induces new failure modes.\n5. OpenApps can be extended for safe training, adversarial robustness testing, or sim2real transfer studies. The discussion section articulates a credible roadmap for future research directions, showing awareness of the broader ecosystem."}, "weaknesses": {"value": "1. **Task Simplicity and Limited Scope**\n    - The current experiments focus on **15 simple tasks** (e.g., adding a to-do item). While the paper demonstrates large variability even on these, such simplicity limits conclusions about generalization to *complex or long-horizon* tasks seen in real-world apps.\n2. **Insufficient Theoretical Framing of “Reliability”**\n    - While empirical results are strong, the paper lacks a deeper theoretical formalization of *reliability across app variations*—for example, framing it as an expected reward stability problem under distributional shifts could strengthen the conceptual rigor."}, "questions": {"value": "+ How are app variations generated — random parameter changes, manually curated modifications, or rule-based templates? Could the process introduce artificial correlations that models might exploit?\n+ How do you ensure that a task instance in version A is semantically equivalent to the same task in version B (e.g., identical goal, only UI difference)?\n+ Did you observe any model types that adapt better to variations (e.g., multimodal LLMs vs. RLVR UI-trained agents)? If so, what might explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBlG3cORVl", "forum": "cj1MAx7lKs", "replyto": "cj1MAx7lKs", "signatures": ["ICLR.cc/2026/Conference/Submission20769/Reviewer_CTPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20769/Reviewer_CTPq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076652983, "cdate": 1762076652983, "tmdate": 1762934200407, "mdate": 1762934200407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the question of reliability of UI agents when the underlying environments change (without affecting overall functionality). The key idea is to create multiple appearance and content variations of a set of UI apps and measure agent performance across these variations. The authors find that all models have significant variance in performance, with larger models being relatively more stable compared to smaller models. The analysis further illustrates that certain specific types of variations in the environment lead to consistently larger drops in agent performance (e.g. using darker themes for apps leads to consistent failures)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The question of reliability is an important one, especially when considering how dynamic real world apps often are. Assessing agent performance under these variations is quite useful.\n\n- The analysis shows that multimodal models have large variability in performance, raising important questions about their suitability in practical settings, where the underlying environments change. \n\n- This benchmark could be a useful additional evaluation for multimodal agent solutions in addition to the existing benchmarks."}, "weaknesses": {"value": "The main issue I have with this work is that the motivations for the variations is not adequately justified and the choices for curation are not well explained. \n  - Content variations seem somewhat arbitrary. If one were to include misleading descriptions and adversarial perturbations performance is going to drop. What is the point of this exercise? There is not much in terms of motivation for why these were done and what specific ways in which these were created. Why use German translations? Why not others? \n  - Similarly the choice of stylistic variations also appears somewhat arbitrary and adversarial. It would have been much more natural to target the most frequently used variances alongside some of the more rarely used variations."}, "questions": {"value": "- I appreciate the state based evaluation and the rigorous evaluation. However, it seems like partial progress towards the task is not accounted for in the current analyses. Is there a way to extend the analysis to partial progress? \n\n- How are the within app fluctuations observed? Are these deviations computed across all queries? Or averages of standard deviations over multiple attempts for each query?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1WRazLG6wU", "forum": "cj1MAx7lKs", "replyto": "cj1MAx7lKs", "signatures": ["ICLR.cc/2026/Conference/Submission20769/Reviewer_MLr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20769/Reviewer_MLr5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117592107, "cdate": 1762117592107, "tmdate": 1762934199495, "mdate": 1762934199495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We’re grateful to the reviewers for their careful consideration of our work. We’re thrilled to see reviewers appreciated the \n\n- **Importance of agent reliability across app variations**: reviewers commented on the “highly crucial problem” [5Ghj] we study, noting “The question of reliability is an important one, especially when considering how dynamic real world apps often are ” [MLr5]. Compared to existing work reviewers noted “the paper identifies a key blind spot in existing agent benchmarks” [CTPq]  “significantly advances the utility of reliability metrics” [1P4o].\n- **Low overhead of OpenApps making large scale agent research accessible**: reviewers mentioned our work is “highly notable because a light-weight benchmark,” calling out “OpenApps is implemented in pure Python and runs on a single CPU, removing the heavy dependencies of prior environments” [CTPq]. Reviewers noted the importance of these design choices in terms of scale:  “this low overhead is a crucial enabler for large-scale RL training and comprehensive testing that previous VM-based or complex web-clone environments could not afford” [1P4o]\n- **Systematic experiments with striking findings**: “authors conduct 10,000+ trials across seven state-of-the-art agents” [CTPq] and that “the volume of the experiments is notable”[5Ghj].  Reviewers characterized the findings as “Results are striking: task success can fluctuate by more than 50% across app versions” [CTPq] “valuable diagnostics on how environment changes induce specific failure modes.” [1P4o] “this work has a strong potential to be a highly noteworthy work that can function as a standardized benchmark.” [5Ghj]. \n\nReviewers made several suggestions regarding the complexity of tasks, app variation coverage, and presentation of our work. Thanks to their suggestions, we’ve made a considerable effort to revise our manuscript based on their feedback and run three new experiments: \n\n1. **New more complex, multi-step tasks with partial rewards**: three reviewers, 1P4o, n5Ghj, CTPq suggested we consider more complex long-horizon tasks. Based on their feedback, we implemented 10 new multi-step tasks (see Section E.2). For these complex tasks we also incorporate reviewers’ MLr5 and 1P4o feedback to include partial rewards. We find multi-step tasks to be more challenging for agents.\n2. **New experiments covering popular app variation font, color, and language choices**: based on reviewers MLr5 and CTPq’s suggestions, these experiments extend our coverage of app variations to popular choices found on the web (see Section E.3)\n3. **New experiments studying interactions across app variations**: based on reviewer 1P4o’s suggestion, these experiments highlight OpenApps scale to generate even more app versions by considering combinations of app variations (see Section E.1).\n\nWe’ve also updated our manuscript with a theoretical grounding of reliability across app variations in terms of distribution shifts in Section 4. All revisions are highlighted in blue to ease reviewers’ burden. We once again thank reviewers for their outstanding suggestions that we believe have greatly improved the quality of our work. With reviewers’ feedback incorporated, we believe OpenApps can serve as a foundation to the community for accessible multimodal agent reliability research."}}, "id": "2Zq6mOPc87", "forum": "cj1MAx7lKs", "replyto": "cj1MAx7lKs", "signatures": ["ICLR.cc/2026/Conference/Submission20769/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20769/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission20769/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763582706102, "cdate": 1763582706102, "tmdate": 1763582706102, "mdate": 1763582706102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}