{"id": "JS6FwZJwFn", "number": 1137, "cdate": 1756848256213, "mdate": 1763037912785, "content": {"title": "Simplex–FEM Networks (SiFEN): Learning a Triangulated Function Approximator", "abstract": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents $f:\\mathbb{R}^d \\to \\mathbb{R}^k$ as a globally $C^r$ finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most $d+1$ basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-$m$ Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate $M^{-m/d}$ with $M$ mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.", "tldr": "Simplex-FEM Networks (SiFEN): a neural finite-element model that learns a simplicial mesh and local polynomials for single-simplex inference, controllable $C^r$ smoothness, FEM-rate approximation, improved calibration, and lower inference latency.", "keywords": ["SiFEN", "finite elements", "simplicial mesh", "piecewise polynomial", "barycentric coordinates", "Bernstein-Bezier", "local function approximation", "geometric deep learning", "representation learning", "calibration", "approximation rates"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/435f77c3b6a978a3761673c4b10692eb3bd46fb4.pdf", "supplementary_material": "/attachment/008e47aecc599f0a65ce197adbc2f9a684305d54.zip"}, "replies": [{"content": {"summary": {"value": "A trial to replace multi-layer perceptrons and Kolmogorov--Arnold networks with finite elements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Finite elements are inherently local and supported by a well-established theoretical foundation. The author’s claims of faster inference and improved interpretability are reasonable given these properties."}, "weaknesses": {"value": "* The paper opens and closes with excessive jargon, which obscures the main ideas.\n* The computing environment is insufficiently described (see Table 6).\n* The expected $M^{-r/d}$ convergence rate is not clearly presented in Figure 10.\n* Appendices D--K are mathematically dense but lack formal proofs.\n* No qualitative evaluations are provided---only error curves. For example, Appendix S.5 mentions the Gibbs phenomenon and localized shocks, yet only (L^2) errors are reported. At least (L^\\infty) metrics should have been included for the Gibbs phenomenon.\n* The style does not match with the reference latex template. (e.g., see **Jaderberg et al., 2015** and compare the fonts for \"Under review as a conference paper at ICLR 2026\")."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YnGEUVb7yD", "forum": "JS6FwZJwFn", "replyto": "JS6FwZJwFn", "signatures": ["ICLR.cc/2026/Conference/Submission1137/Reviewer_Hctk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1137/Reviewer_Hctk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016366629, "cdate": 1761016366629, "tmdate": 1762915688088, "mdate": 1762915688088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5w8ryKhL08", "forum": "JS6FwZJwFn", "replyto": "JS6FwZJwFn", "signatures": ["ICLR.cc/2026/Conference/Submission1137/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1137/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763037912156, "cdate": 1763037912156, "tmdate": 1763037912156, "mdate": 1763037912156, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SiFEN and alternative to traditional MLPs or KANs. The core idea is a learned finite–element predictor that is globally continuous and sparse by construction - activating only one simplex and at most d+1 basis functions per input. The authors propose training scheme for these primitives by learning the mesh, coefficients, and an optional invertible warp with shape regularization, coverage via semi–discrete OT, and differentiable local flips for topology improvement."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Using FEM primitives seems an interesting avenue for learnable primitives, in particular, as alternative to current MLPs or KANs. The paper also goes in a lot of detail regarding its derivations etc."}, "weaknesses": {"value": "- results are on toy tasks; in principle this is okay for a theoretical paper but in this case I'm having real trouble extrapolating the approximation / fitting tasks to real-world scenarios\n- the presentation, in particular the result section, is not good. Much of the text is isolated bullet points and very difficult to follow. \n- Fig 1. graphs are very difficult to interpret / axes labels too small fonts; formatting of tables (e.g., 4.3.) is poor - in a sense this follow through out the paper which makes it for an out-of-domain expert difficult to judge.\n\nOverall, my main issue is that I am having a hard time validating the results - based on the current presentation, I am struggling to properly evaluate the benefits over existing approaches in common, real-world tasks."}, "questions": {"value": "My main question would be how the results could translate from basic fitting tasks to real-world learning problems, and how to best showcase this. In principle, I would've expected that this should be feasible to demonstrate with modern differentiable learning frameworks.\n\nAt the same time, I would like to highlight that this paper is pretty much out of my area. I tried my best to follow the derivations but it is not straightforward. In comparison to similar papers out of this area, the presentation makes this also difficult in addition to my lack of background knowledge in this space. From a pure paper presentation standpoint, this could be improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xT2XcY6RxQ", "forum": "JS6FwZJwFn", "replyto": "JS6FwZJwFn", "signatures": ["ICLR.cc/2026/Conference/Submission1137/Reviewer_u4Fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1137/Reviewer_u4Fy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942875953, "cdate": 1761942875953, "tmdate": 1762915687969, "mdate": 1762915687969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a comprehensive and well-structured paper introducing Simplex-FEM Networks. The authors have effectively presented a novel, theoretically grounded, and empirically strong alternative to standard MLPs and recent models like KANs, particularly excelling in areas requiring explicit geometry and sharp interfaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea is rooted in classical Finite Element Methods (FEM), providing a solid foundation with an established approximation rate. This theoretical backing is a significant advantage over many heuristic neural network architectures.\n\n- The use of $C^r$ continuity constraints (specifically $C^0$ and partial $C^1$) allows explicit control over the smoothness of the predicted function, which is crucial for approximating functions with sharp features or discontinuities.\n    \n- SiFEN consistently matches or surpasses MLPs and KANs at matched parameter budgets. Notably, it shows marked improvement in calibration (lower ECE/Brier scores). Training the mesh vertices, triangulation, and an optional geometric warp ($\\Phi_\\theta$) allows the model to adapt its approximation basis to the data distribution and function complexity."}, "weaknesses": {"value": "- The paper clearly acknowledges that mesh complexity scales poorly in high dimensions (the theoretical rate $M^{-m/d}$ gets very small for large $d$ unless $M$ is enormous). While the warp $\\Phi_\\theta$ is proposed as a mitigation strategy (flattening curvature or reducing effective dimension), the effectiveness of this for extremely high dimensions ($d \\gg 50$) remains a primary practical concern. \n\n- The performance is highly dependent on effective regularization for shape regularity and coverage. Degenerate simplexes and poor vertex initialization are explicit failure modes. The reliance on semi-discrete OT for coverage is sophisticated and may increase training complexity compared to simpler baselines.\n\n- Table 17 breaks down the inference latency into \"Point location\" and \"Local polynomial eval.\" The point location is currently implemented via kd-tree + walk or BVH. What are the computational challenges for the point location step specifically in high dimensions ($d=10, 50$)? Is there a computational limit on $M$ in high dimensions?\n\n- The interpretability section focuses on the learned mesh structure (e.g., error vs. boundary crossings). The warp $\\Phi_\\theta$ is crucial to performance and is interpretable as a monotone triangular map. Can the authors offer a visual interpretation of the learned warp $\\Phi_\\theta$ on a 2D synthetic task?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RvAHScvpFP", "forum": "JS6FwZJwFn", "replyto": "JS6FwZJwFn", "signatures": ["ICLR.cc/2026/Conference/Submission1137/Reviewer_3qki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1137/Reviewer_3qki"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762334726176, "cdate": 1762334726176, "tmdate": 1762915687782, "mdate": 1762915687782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}