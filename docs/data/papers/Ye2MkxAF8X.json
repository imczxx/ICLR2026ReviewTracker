{"id": "Ye2MkxAF8X", "number": 18610, "cdate": 1758289476006, "mdate": 1759897092223, "content": {"title": "Benchmarking Gaslighting Negation Attacks Against Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to conversational adversarial inputs. In this paper, we systematically study gaslighting negation attacks—a phenomenon where models, despite initially providing correct answers, are persuaded by user-provided negations to reverse their outputs, often fabricating justifications. We conduct extensive evaluations of state-of the-art MLLMs across diverse benchmarks and observe substantial performance drops when negation is introduced. Notably, we introduce the first benchmark GaslightingBench, specifically designed to evaluate the vulnerability of MLLMs to negation arguments. GaslightingBench consists of multiple-choice questions curated from existing datasets, along with generated negation prompts across 20 diverse categories. Throughout extensive evaluation, we find that proprietary models such as Gemini-1.5-flash and GPT-4o demonstrate better resilience compared to open-source counterparts like Qwen2-VL and LLaVA, though even advanced reasoning-oriented models like Gemini-2.5-Pro remain susceptible. Our category level analysis further shows that subjective or socially nuanced domains (e.g., Social Relation, Image Emotion) are especially fragile, while more objective domains (e.g., Geography) exhibit relatively smaller but still notable drops. Overall, all evaluated MLLMs struggle to maintain logical consistency under gaslighting negation attack. These findings highlight a fundamental robustness gap and provide insights for developing more reliable and trustworthy multimodal AI systems.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Gaslighting negation attack", "benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45ec773745c9f95af2fa2a50edc98df5eacf840e.pdf", "supplementary_material": "/attachment/6b9e45f1f0bf50e20b44a2db913cc9152b8eeb45.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents the first systematic study of “gaslighting negation attacks” against Multimodal Large Language Models (MLLMs). It defines these attacks as conversational manipulations where models, initially correct, are persuaded by user-provided negations to revise their answers incorrectly—often with fabricated justifications. The authors conclude by emphasizing that gaslighting negation attacks represent a distinct, underexplored adversarial failure mode, highlighting the need for fine-grained alignment and calibration strategies to enhance robustness and trustworthiness in multimodal AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces “gaslighting negation” as a new class of conversational attack, distinct from jailbreak or prompt injection. It’s a subtle yet impactful vulnerability, especially in real-world dialogue contexts.\n\n2. Proprietary models (Gemini-1.5-flash, GPT-4o, Claude-3.5) outperform open-source ones (Qwen, LLaVA) but still degrade notably.\n3. Figure 7 (p.8) illustrates models contradicting earlier correct answers—sometimes even producing hallucinated justifications (“I apologize, the color is red”)—clearly conveying the behavioral risk.\n4. Includes supplementary experiments on question type sensitivity (Appendix A.1) and negation phrasing effects (Figure 8, p.15)."}, "weaknesses": {"value": "1. The explanation of why over-alignment induces gaslighting behavior is qualitative.\n2. The paper exposes the vulnerability well but provides no mitigation strategies, even conceptually, e.g., calibration, adversarial training, debate-style reinforcement\n3. The study does not explore internal attention or activation traces to explain why negation overrides factual grounding—especially relevant for multimodal reasoning.\n\n4. Minor stylistic issues (e.g., “ne￾gation,” “conversational negation attack”) indicate OCR artifacts or typesetting errors. Figures are informative but occasionally crowded."}, "questions": {"value": "1. Have you examined whether negation causes greater changes in text–vision attention layers or in decoder self-attention?\n2. How might calibration-aware decoding mitigate confident hallucination?\n3. How consistent are generated negation prompts across linguistic forms, e.g., “not,” “no,” “never”?  \n4. Would “self-consistency” or “chain-of-verification” decoding strategies resist these attacks?  \n5.  Any differences in model responses to explicit vs. implied negation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqYZkfR2Du", "forum": "Ye2MkxAF8X", "replyto": "Ye2MkxAF8X", "signatures": ["ICLR.cc/2026/Conference/Submission18610/Reviewer_bXJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18610/Reviewer_bXJC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535806391, "cdate": 1761535806391, "tmdate": 1762928323805, "mdate": 1762928323805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an underexplored but important vulnerability of Multimodal Large Language Models (MLLMs): their tendency to reverse correct answers when given with user-provided negations, a phenomenon termed gaslighting negation attacks. The authors introduce GaslightingBench, a benchmark of 1,287 multimodal multiple-choice questions over 20 categories, to evaluate robustness to such attacks. They evaluate on multiple proprietary (GPT-4o, Gemini-1.5-flash, Claude-3.5-Sonnet, Gemini-2.5-Pro) and open-source (Qwen2-VL, LLaVA) models across several established multimodal datasets (MMMU, MMBench, MathVista, ChartQA, etc.), analyzing pre- and post-negation performance. Results show significant performance drop after the gaslighting attack."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The gaslighting attack on multimodal LLMs are under-explored (although it has been extensively studied under text-only LLMs)."}, "weaknesses": {"value": "### **Major**\n\n1. **Over-simplified gaslighting prompt type:** The paper only studies direct negation and short-answered gaslighting prompt. However, I think this type of gaslighting prompt may be over-simplified, and less practical:\n    - In this work, the gaslighting prompts are all directly telling the LLMs the (incorrect) answer. However, since LLMs are trained to follow user instructions. If the user directly tells the LLM what the answer should be, then it is expected that the LLM should consider the user input in the first place. The more proper gaslighting prompt should be questioning/debating as studied in existing 'gaslighting' attack in text-only domain (e.g. [1,2]), or negations with more explanation (e.g. CoT [2]).\n    - In many cases when the original context is not sufficient and the MLLM is making prediction based on certain prior, if the user directly provides the (incorrect) answer, it is expected that the MLLM should change mind. For instance in the left most example in Figure 5, the MLLM predicts \"professional\" relation based on the people's outfits. However, if the user directly tells the MLLM they are in \"family\" relation, then it is expected that the MLLM should follow user's input, as it has no further prior knowledge what the relation is.\n2. **Incomprehensive gaslighting type:** Based on the above consideration, I feel the proposed benchmark lacks some comprehensiveness:\n    - Currently it only includes negation style prompt. I think it is important to also include questioning/debating style gaslighting.\n    - Currently it focuses mostly on short-answered gaslighting attack without explanation. I think it is worthwhile to study how the model behaves when the incorrect explanation (e.g. CoT) is provided along with the gaslighting input.\n3. **Evaluate on more open-sourced models:** Currently the evaluation of open-sourced models is conducted on qwen2-vl-72b and llava1.6-7b. I think more evaluation are needed:\n    - qwen2-vl-72b and llava1.6-7b are very different in size and backbones, making the evaluation un-controlled and hard to draw conclusions. For instance, if you want to study the effect of LLM sizes on gaslighting attack, you should ablate on qwen2-vl-2b, 7b and 72b, etc.\n    - Both qwen2-vl-72b and llava1.6-7b are not RL-finetuned. I think it's worthwhile to evaluate over RL-finetuned MLLMs such as internvl2.5/3/3.5, gemma3 and qwen2.5/3, many of which are available before ICLR paper deadline.\n3. **Missing some more in-depth analysis:** \n    - The paper lacks a deeper analysis of how the model reverse decisions: no probing of attention patterns/representation shifts/intermediate reasoning traces etc.\n    - I feel there could be more case studies to analyze when model predicts incorrectly after the gaslighting attack. Are they hard negative? Or are actually false negative (e.g. Figure5 left most)?\n4. **Discussion on mitigation is minimal:** given that gaslighting is not a new topic in LLM literature, and there have been studies on how to mitigate [2], I feel it may be necessary to provide some baseline approaches to mitigate the impact of such attack, along with the benchmark.\n\n### **Minor**\n1. Please consider using /citep instead of /cite to put citations in parentheses for better readability.\n\n 1 Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate. ACL 2023\n\n 2 Aligning Large Language Models for Faithful Integrity Against Opposing Argument. AAAI 2025"}, "questions": {"value": "The paper is clearly written and straightforward, so I do not have additional questions. Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "41e0OajBAu", "forum": "Ye2MkxAF8X", "replyto": "Ye2MkxAF8X", "signatures": ["ICLR.cc/2026/Conference/Submission18610/Reviewer_1fXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18610/Reviewer_1fXH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577519523, "cdate": 1761577519523, "tmdate": 1762928323420, "mdate": 1762928323420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GaslightingBench, a novel benchmark designed to systematically evaluate the vulnerability of Multimodal Large Language Models (MLLMs) to gaslighting negation attacks—a form of adversarial input where models are misled into reversing their initially correct answers through user-provided negations, often fabricating justifications in the process. The authors conduct extensive experiments across multiple MLLMs (both proprietary and open-source) and existing multimodal benchmarks, demonstrating significant performance drops when negation is introduced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses an under-explored but critical issue—negation-induced inconsistency in MLLMs—and introduces the first dedicated benchmark (GaslightingBench) for evaluating this vulnerability.\n\n2.The study evaluates a wide range of MLLMs across multiple datasets and question formats, providing a thorough and comparative analysis of model robustness.\n\n3. Rigorous Methodology:The evaluation pipeline is well-structured, including negation generation, post-processing, and careful dataset curation. The use of multiple negation styles (neutral, anger, authority) adds depth to the analysis."}, "weaknesses": {"value": "1.Benchmark Bias Toward MCQs: GaslightingBench is primarily based on multiple-choice questions, which may not fully capture the complexity of real-world adversarial interactions or free-form reasoning.\n\n2. Different real-world complexity are not considered:The study focuses on controlled benchmarks; it does not test how gaslighting attacks perform in more dynamic, multi-turn, or real-world conversational settings.\n\n3. Lack of Mitigation Strategies or insight. The paper identifies the problem but does not propose or evaluate methods to mitigate gaslighting attacks, which would have strengthened its practical impact."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QuLYQwgIiB", "forum": "Ye2MkxAF8X", "replyto": "Ye2MkxAF8X", "signatures": ["ICLR.cc/2026/Conference/Submission18610/Reviewer_9rHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18610/Reviewer_9rHf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625165620, "cdate": 1761625165620, "tmdate": 1762928322922, "mdate": 1762928322922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}