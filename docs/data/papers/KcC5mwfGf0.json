{"id": "KcC5mwfGf0", "number": 19476, "cdate": 1758296577000, "mdate": 1759897036918, "content": {"title": "GRL-SNAM: Geometric Reinforcement Learning with Differential Hamiltonians for Navigation and Mapping in Unknown Environments", "abstract": "We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping in unknown environments. GRL-SNAM differs from traditional SLAM and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates navigation and mapping as coupled dynamics on generalized Hamiltonian manifolds: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise under Differential Policy Optimization (DPO). A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on 2D deformable navigation tasks, where a hyperelastic robot learns to squeeze through narrow gaps, detour around obstacles, and generalize to unseen environments. We evaluate GRL-SNAM on procedurally generated 2D deformable–robot tasks comparing against local reactive baselines (PF, CBF, staged DWA) and global A* references (rigid, clearance-aware) under identical stagewise sensing constraints. GRL-SNAM shows superior path quality while using the minimal map coverage, preserves clearance, generalizes to unseen layouts, and demonstrates that Hamiltonian-structured RL enables high-quality navigation through minimal exploration via local energy refinement rather than global mapping.", "tldr": "We propose a geometric RL method that navigates and maps using only local sensing, leveraging Hamiltonian dynamics and differential policy optimization to adapt quickly under dynamic, deformable conditions", "keywords": ["Reinforcement Learning", "generalized Hamiltonian manifolds", "Differential Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c6e601ce146fd6aefcd113266c8c14c537f8357.pdf", "supplementary_material": "/attachment/4cc2984fe346958bdbd1cac8bb5b5ee66a8c032e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces GRL-SNAM, a geometric reinforcement learning framework for simultaneous navigation and mapping in unknown environments. The key contribution is formulating navigation as energy minimization on Hamiltonian manifolds rather than using traditional value-based RL. The system decomposes navigation into three independent policies, i.e., sensor, frame planning, and shape reconfiguration, which operate at different timescales and are unified through a shared Hamiltonian energy formulation. Instead of Bellman-based value iteration, the approach uses Differential Policy Optimization (DPO) for purely feedforward control. Empirically, the method is evaluated on 2D deformable robot navigation where a hyperelastic ring navigates through cluttered environments. It achieves near-optimal path quality with SPL=0.95 while using minimal map coverage at 10.7%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of Hamiltonian mechanics with RL presents a conceptually promising approach. The idea of avoiding value function optimization for feedforward control is innovative and the physical grounding provides strong inductive bias for navigation tasks.\n2.The paper develops a thorough and solid mathematical framework with clear definitions of multi-policy architecture, meta-Hamiltonian formulation, and provides theoretical guarantees for stability, symplectic preservation, and sample complexity.\n3. The focus on achieving high-quality navigation with minimal environment mapping addresses a practical concern in SNAM that has received limited attention in prior work.\n4. The paper provides detailed mathematical derivations, implementation specifics, and comprehensive baseline descriptions in the appendices."}, "weaknesses": {"value": "1. Although the paper provides substantial theoretical contribution, the experimental validation is limited. The evaluation is restricted to 2D environments with a single robot type. This somewhat limits the generalizability and impact of the work.\n2. For presentation, frequent jumps between abstract concepts and implementation details disrupt flow, making it hard to follow. For example, Section 3.1 discusses abstract Hamiltonian optimization theory, including energy deposition by policy, then Section 3.2 abruptly introduces specific hyperelastic robot model, followed by Section 3.3 which jumps back to introduce policy abstraction."}, "questions": {"value": "1. Regarding the reason of not comparing with modern RL methods, the authors claim that \"DPO has already been shown to outperform state-of-the-art policy learners\". However, this justification is insufficient since those comparisons were on different tasks. Could the authors provide stronger justification for this?\n2. Could the authors provide inspirations on extending the proposed work to more complex environment setup, like 3D or even real world?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XIAtOzaQQ0", "forum": "KcC5mwfGf0", "replyto": "KcC5mwfGf0", "signatures": ["ICLR.cc/2026/Conference/Submission19476/Reviewer_aWJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19476/Reviewer_aWJS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454613323, "cdate": 1761454613323, "tmdate": 1762931386161, "mdate": 1762931386161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRL-SNAM, a geometric reinforcement learning framework for simultaneous navigation and mapping in unknown environments. The key idea is to formulate navigation as a differential Hamiltonian system, embedding geometric priors directly into the policy dynamics without explicit value iteration or Bellman backups. By treating perception, mapping, and control under a unified Hamiltonian energy structure, the agent maintains spatial coherence and adaptively balances attraction toward goals with repulsion from obstacles.\nThe framework claims to bridge local and global navigation by allowing a continuous transition between locally reactive potential-based behaviors and globally consistent geometric flows, learned through reinforcement of energy-minimizing trajectories. Experimental validation includes multiple 2D simulated navigation tasks, showing improvements in mapping efficiency, trajectory smoothness, and robustness compared to traditional potential fields, CBF, and A*-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper reframes RL-based navigation as energy minimization on a manifold governed by Hamiltonian flows, avoiding discrete value propagation while retaining long-term optimality through implicit geometric consistency. This yields a clear bridge between local reactive control (as in potential fields) and global goal-directed planning (as in RL-based exploration).\n\nThe idea of using differential Hamiltonians introduces an interesting way to encode conservation and stability into DRL, improving interpretability and safety, which is sometimes the main concern in local navigation in unstructured environments.\n\nBy avoiding explicit value iteration, GRL-SNAM positions itself between model-free RL (which learns global policies but struggles locally) and geometric controllers (which react locally but lack long-term planning). This hybrid viewpoint is theoretically meaningful and could influence future works in hierarchical RL for navigation.\n\nExperiments show GRL-SNAM achieves nearly full success rates with lower mapping ratios, suggesting more efficient local exploration and better global coverage with limited sensing."}, "weaknesses": {"value": "*Unclear role of learning vs. hand-crafted structure*\n\nAlthough stated as “reinforcement learning,” the optimization primarily tunes Hamiltonian parameters rather than learning a policy through interaction in the RL sense. The boundary between learning and analytic control design could be better clarified.\n\n*Insufficient analysis of global consistency*\n\nThe claim that the system enables global navigation without explicit mapping is not rigorously validated. It would be sufficient to show more whether the Hamiltonian field can produce loop-consistent paths or handle long-horizon navigation under partial observability.\n\n*Limited comparison to DRL baselines*\n\nWhile comparisons to potential fields and CBF controllers are provided, there’s no quantitative evaluation against or clarifications on any learning-/DRL-based navigation baselines (e.g., model-based/model-free, hierarchical RL, waypoint-/motion-based). Without these, it is unfair to assess whether the proposed framework is competitive in learning efficiency and generalization. Some examples include:\n- Tai, Lei, Giuseppe Paolo, and Ming Liu. \"Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation.\" 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017.\n- Zhelo, Oleksii, et al. \"Curiosity-driven exploration for mapless navigation with deep reinforcement learning.\" arXiv preprint arXiv:1804.00456 (2018).\n- Liang, Jingsong, et al. \"Context-aware deep reinforcement learning for autonomous robotic navigation in unknown area.\" Conference on Robot Learning. PMLR, 2023.\n- Sathyamoorthy, Adarsh Jagan, et al. \"Terrapn: Unstructured terrain navigation using online self-supervised learning.\" 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022.\n\n*Limited discussion on exploration strategies*\n\nThe “minimum mapping ratio” metric is novel, but it’s not clear how exploration is coordinated globally, whether the Hamiltonian field inherently guides global coverage, or if it only reacts locally to perceived gradients."}, "questions": {"value": "How does GRL-SNAM maintain global goal consistency when only local energy gradients are available? Is there a mechanism analogous to global value propagation (e.g., a learned potential update or memory-based field fusion)?\n\nThe learning process seems to optimize parameters of the Hamiltonian rather than a standard RL policy. Could the authors clarify whether the reward signal still drives policy gradient updates, or if this is a purely energy-based optimization (akin to unsupervised RL or active inference)?\n\nHow does the system behave in environments with dynamically changing obstacles or shifting goals? Does the energy-based formulation adapt online, or does it require retraining?\n\nCould the authors compare with DRL-based navigation baselines (e.g., model-based/model-free, hierarchical RL, waypoint-/motion-based)? It would strengthen the claim that GRL-SNAM achieves similar or better coordination without an explicit hierarchy.\n\nGiven the Hamiltonian structure, could this framework naturally integrate with learned world models (e.g., latent-space dynamics) for true model-based RL on manifolds or navigation in 3D environments? It seems well-suited for that direction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ABXiyR6liz", "forum": "KcC5mwfGf0", "replyto": "KcC5mwfGf0", "signatures": ["ICLR.cc/2026/Conference/Submission19476/Reviewer_eoAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19476/Reviewer_eoAa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582548929, "cdate": 1761582548929, "tmdate": 1762931385686, "mdate": 1762931385686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GRL-SNAM, a framework claiming to move beyond reinforcement learning (RL) by formulating navigation and mapping as Hamiltonian energy minimization on manifolds. It introduces “Differential Policy Optimization” (DPO) and claims that navigation arises as a gradient flow over learned Hamiltonians, evaluated on 2-D deformable-robot navigation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Physically grounded formulation:\nConnects reinforcement learning with Hamiltonian optimal control, offering a structured and interpretable energy-based framework.\n\n\n2. Geometric and multi-scale design:\nPreserves energy and manifold structure through symplectic updates and coordinates multiple policies across different temporal scales.\n\n\n3. Applicability to deformable robots:\nDemonstrates potential in continuous and deformable robotic control, extending beyond traditional rigid-body navigation methods."}, "weaknesses": {"value": "1. Theoretical Contributions Are Mischaracterized\n\nAlthough the paper repeatedly claims to go beyond RL or beyond Bellman optimization, the theoretical formulation remains within standard optimal control and RL principles:\nThe proposed Hamiltonian is written as:\n\n$\nH(q, p) = K(p) + P(q)\n$\n\nwhich directly corresponds to the Pontryagin Hamiltonian in optimal control with a linear dynamic system without drift.  \nIts gradients are:\n\n$$\n\\nabla_p H = f(x, u), \\quad \\nabla_q H = \\nabla_x V.\n$$\n\nis equivalent to the value-function gradient in the Hamilton–Jacobi–Bellman (HJB) framework.\n\nThus, the “Hamiltonian gradient” proposed here produces the same optimal policy direction as the value gradient in standard optimal control and RL.\n\nClaims that this method is “beyond Bellman” or “feedforward” are conceptually misleading — it effectively re-expresses the value-function gradient field under a different parameterization.\n\nHence, the theoretical novelty is overstated, and the method is a rebranding of existing value-based interpretations of RL/HJB rather than a fundamentally new control principle.\n\n2. Experiments Are Limited and Not Convincing\n\nThe experiments focus exclusively on simple 2-D environments with “a hyperelastic ring navigating among several circular obstacles”\nWhile the paper reports high success rates, these setups are not challenging:\n\nNavigation among a few separated obstacles is a standard and well-solved problem; methods like [1] (and even classical potential field methods) already achieve similar results.\n\nThe authors compare only against simple baselines (PF, CBF, A*, DWA) but not against modern RL or geometric planning baselines in complex environments.\n\nTo substantiate the claimed scalability, the authors should evaluate on realistic indoor navigation benchmarks such as Habitat, Gibson, or Matterport3D, where partial observability, long horizons, and mapping uncertainty actually challenge RL and control approaches.\n\nWithout such experiments, the empirical section does not demonstrate generalization or scalability.\n\n[1] Optimal obstacle avoidance based on the Hamilton–Jacobi–Bellman equation"}, "questions": {"value": "Several notational and structural issues hinder readability:\n\nKey symbols in the reward/energy equations (Eq. 2) are undefined, including functions epsilon() and b()"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SA1XxmtZsu", "forum": "KcC5mwfGf0", "replyto": "KcC5mwfGf0", "signatures": ["ICLR.cc/2026/Conference/Submission19476/Reviewer_8Pzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19476/Reviewer_8Pzq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962644237, "cdate": 1761962644237, "tmdate": 1762931385413, "mdate": 1762931385413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}