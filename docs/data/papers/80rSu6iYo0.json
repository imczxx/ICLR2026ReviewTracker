{"id": "80rSu6iYo0", "number": 5310, "cdate": 1757898697846, "mdate": 1759897981905, "content": {"title": "Test Time Scaling of Diffusion Model via Flow Matching Corrector", "abstract": "Scalable self-improving capability is a desirable property of generative models, as it enables performance improvement with additional computational resources rather than requiring more training data. Existing approaches typically rely on external reward signals to fine-tune generative models and improve the generation. In this paper, we propose the Scalable Self-Improving Correction (SSI-Corr) framework, which requires neither new training data nor external rewards. Instead, SSI-Corr trains a corrector that directly aligns the sample generation process with the target distribution. Our method is supported by theoretical analysis and scales effectively with available computational resources. In the experiment, we demonstrate that SSI-Corr improves FID scores by 27\\% and 14.3 \\% on a pre-trained unconditional CIFAR10 DDPM with ancestral and DDIM samplers respectively.", "tldr": "", "keywords": ["Diffusion Model", "ML", "Test-Time Scaling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92df3f0787201fcc0a7b5f8dcdde64f04d0acfcf.pdf", "supplementary_material": "/attachment/f5919edcfa8a3aed58a23d753e89e40e59adfe23.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes SSI-Corr, a “self-improving” framework for diffusion models that inserts a flow-matching corrector at a chosen timestep  to transport samples from the sampler’s biased marginal  toward the target marginal . Practically, the corrector is trained with synthetic trajectories from the base sampler and noisy data pairs constructed from the original dataset; at inference, an ODE for the learned velocity field is solved at step . The authors claim test-time and training-time scalability and report CIFAR-10 FID improvements for DDPM (e.g., from 6.872 to 4.963 by roughly doubling total steps) and a 14.3% gain over DDIM. They provide an error decomposition and a KL bound arguing that reducing  at step  should improve the final sample distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear formulation that decouples correction from the base sampler and can be slotted at arbitrary timesteps; the method is conceptually simple (one extra ODE solve). \n\nTheory-backed motivation: a clean error analysis (initial bias, score error, discretization) and a KL bound illustrating when the corrector helps; also a note that the  corrector is near-linear in expectation."}, "weaknesses": {"value": "1. Limited scope of evaluation: only unconditional CIFAR-10 is shown. No higher-resolution datasets (e.g., ImageNet-64/256, CelebA-HQ) or text-to-image settings, making scalability/generalization claims hard to assess. \n\n2. Self-improvement framing is overstated: the method still uses the original dataset to construct targets ; it is not data-free (just no new data). The paper should temper the claim and discuss limitations when access to data is restricted. \n\n3. Baselines feel weak/incomplete: comparison is mainly to predictor–corrector (Langevin) and an initial-bias tweak; no head-to-head against modern fast samplers / correctors (e.g., DPM-Solver variants, UniPC, consistency models, or recent flow-matching refinements) under matched compute. The fairness of comparing “one-shot” correction at a single  to PC applied after every step is also not fully justified. \n\n4. Compute accounting is unclear: the principal gains come when total sampling steps roughly double; wall-clock impact of the extra ODE solve, solver tolerances, and memory footprint are not reported. Training cost vs. improvement (especially at large ) also lacks a thorough cost–benefit analysis."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wb1C5muPar", "forum": "80rSu6iYo0", "replyto": "80rSu6iYo0", "signatures": ["ICLR.cc/2026/Conference/Submission5310/Reviewer_2zaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5310/Reviewer_2zaJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640903012, "cdate": 1761640903012, "tmdate": 1762917999547, "mdate": 1762917999547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Scalable Self-Improving Correction (SSI-Corr) method to address three intrinsic sources of error in diffusion models: initial distribution mismatch, score estimation error, and discretization error.\nThe proposed approach trains an additional model to bridge the gap between the true distribution (ground truth) and the model distribution (obtained from the base diffusion sampler) at a specific diffusion timestep $t$. This corrector model is trained using the well-known flow matching loss.\nThe paper theoretically establishes an explicit bound on the sampling error of this method and empirically demonstrates its effectiveness through experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a principled approach based on flow matching to mitigate key sources of error inherent to diffusion models.\n\n- It is not restricted to a specific sampler and can be applied to any diffusion sampling process, regardless of the underlying architecture or parameterization.\n\n- It provides strong theoretical foundations, clearly deriving the specific error bounds achieved by the proposed algorithm."}, "weaknesses": {"value": "- The method introduces an additional ODE solving step, inevitably increasing the total sampling time.\n\nMoreover, since the ODE solver uses the same architecture as the base score model, it is computationally heavy.\n\nEven for a relatively simple setup such as CIFAR-10 with a lightweight score model, the proposed framework requires an additional model of equal size.\n\nThus, it is unclear whether the method can be scalably applied to large and powerful diffusion models [1,2]. [2] has cifar-10 experiment.\n\n- Experiments are conducted only on small-scale datasets, and comparisons are made mainly with older baselines.\n\nIt remains uncertain whether the proposed method would still be effective when applied to larger datasets or more recent diffusion architectures beyond DDPM and CIFAR-10.\n\n-----------\n[1] Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\n\n[2] Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator"}, "questions": {"value": "- In Figure 3(a), the FID score appears to increase as training progresses, suggesting a lack of convergence.\n\nThis raises concerns about whether the proposed method is actually improving the sample quality as intended.\n\nThe observed instability might be due to the non-optimal training of the corrector at specific timesteps, but a more detailed explanation would be helpful.\n\n- The proposed method appears general enough to be applicable to distillation models as well. \n\nDo the authors have any plans for additional experiments or evaluations in such settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MPsYPUGAmo", "forum": "80rSu6iYo0", "replyto": "80rSu6iYo0", "signatures": ["ICLR.cc/2026/Conference/Submission5310/Reviewer_4TfU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5310/Reviewer_4TfU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842974126, "cdate": 1761842974126, "tmdate": 1762917999215, "mdate": 1762917999215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to improve the generative modelling of diffusion models post-training by learning an auxiliary flow matching model that maps the generated distribution to the true data distribution. They train this auxiliary model in practice using the original training data and synthetic data generated by the diffusion model *at a specific noise level*.  At inference, the flow matching model is used at the aforementioned noise level to \"correct\" the generated samples, diffusion generation then continues as normal."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I believe the core idea is reasonable and may have potential in practical generative modelling applications with diffusion models. This sort of approach may be interesting to explore in test-time distribution shifting (e.g. adjusting the generative style of a frozen/black box model post-training)."}, "weaknesses": {"value": "The paper load for ICLR this year has been large, and so I have not been able to spend as much time as I would like on reviewing. I encourage the authors to correct any errors/misunderstandings I may have with regards to the paper. \n\n1. **Understanding the approach**\n    1. The approach applies a very general interpolation between the generated and data distribution via random pairing at train time, which to me would potentially suggest large transformations during the correction step. However, the actual effect (Figure 1) seems to be visually imperceptible. The authors do not provide any insight on why this is the case. Why does the learnt velocity make only very small changes?\n2. **Poor presentation**  \n    1. Figures are out of order, e.g. Figure 4 appears in the text before Figure 3, leading to confusion when reading.\n    1. Figures are poorly formatted with text inside figures being tiny and close to illegible.\n    1. Writing is unpolished e.g. in the introduction, \"current works ... either rely on additional reward models for guidance.\" \n3. **Weak experiments**\n    1. The authors only benchmark on CIFAR-10. That is to say a single, small-scale (32x32) dataset. I am not saying that CIFAR is necessarily a bad benchmark for generative modelling, however, using *only* CIFAR-10 is not enough. This is especially problematic since the authors motivate their work from the perspective of test-time scaling, which is a concept that is primarily relevant in large-scale foundation model deployment, where relevant experiments should be at the scale of high-resolution text-to-image generation. \n    1. The lack of any FID-50K results makes it hard to compare the results to the literature."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jVM7bHRBlv", "forum": "80rSu6iYo0", "replyto": "80rSu6iYo0", "signatures": ["ICLR.cc/2026/Conference/Submission5310/Reviewer_dJp8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5310/Reviewer_dJp8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918078665, "cdate": 1761918078665, "tmdate": 1762917998919, "mdate": 1762917998919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Scalable Self-Improving Correction (SSI-Corr), a drop-in framework for diffusion models that replaces generation–verification schemes with a flow-matching corrector trained to align the sampler’s marginal with the ground-truth marginal at a chosen timestep t. The authors provide a clean error decomposition and a bound showing that the final divergence is controlled by the corrector’s mismatch at t plus residual score/discretization terms accumulated after t. Empirically, SSI-Corr improves FID on a pre-trained CIFAR-10 DDPM by ~27% with the ancestral sampler and ~14.3% with DDIM, and exhibits both test-time and training-time scalability via the choice of correction timestep and solver steps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple and easy to plug in to the current diffusion models.\n- The introduction of flow matching to the self-improving corrector is new\n- Several experiments are conducted to demonstrate the main claims\n- Implementation and evaluation details are reported clearly enough to replicate the results."}, "weaknesses": {"value": "- Incremental core novelty. The main technical part—using flow matching to train a corrector at a chosen timestep—leans on established techniques; much of the machinery (least-squares flows, OU marginals, sampler integration) is also adapted from prior work.\n- The writing could be improved; for example, the discussion of training and testing scalability appears before these terms are clearly defined.\n- The proposed corrector can overfit training data (the paper notes this qualitatively). Please quantify with train/val curves vs dataset size, and report generalization across seeds/samplers.\n- The experiments only focus on CIFAR-10 dataset and DDIM samplers. It would be interesting to see how the corrector perform on other datasets/samplers."}, "questions": {"value": "- In the data sampling process from the distribution p_t, why is Gaussian noise added, and how are its mean and variance determined?\n- In Algorithm 1, the sampler time horizon T is never used. Should t/h-1 be T/h-1 ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tqeVhUkavb", "forum": "80rSu6iYo0", "replyto": "80rSu6iYo0", "signatures": ["ICLR.cc/2026/Conference/Submission5310/Reviewer_TZ1Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5310/Reviewer_TZ1Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943610617, "cdate": 1761943610617, "tmdate": 1762917998580, "mdate": 1762917998580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}