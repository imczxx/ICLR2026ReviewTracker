{"id": "ja0tZJS3hM", "number": 19408, "cdate": 1758295992123, "mdate": 1759897040666, "content": {"title": "Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff", "abstract": "Knowledge distillation is often considered a compression mechanism when judged on the resulting student's accuracy and loss, yet its functional impact is poorly understood. In this work, we quantify the compression capacity of knowledge distillation and the resulting knowledge transfer from a functional perspective, decoupling compression from architectural reduction, which provides an improved understanding of knowledge distillation. We employ hypothesis testing, controls, and random control distillation to understand knowledge transfer mechanisms across data modalities. To rigorously test the breadth and limits of our analyses, we explore multiple distillation variants and analyse distillation scaling laws across model sizes. Our findings demonstrate that, while there is statistically significant knowledge transfer in some modalities and architectures, the extent of this transfer is less pronounced than anticipated, even under conditions designed to maximise knowledge sharing. Notably, in cases of significant knowledge transfer, we identify a consistent and severe asymmetric transfer of negative knowledge to the student, raising safety concerns in knowledge distillation applications. Across 12 experimental setups, 9 architectures, and 7 datasets, our findings show that knowledge distillation functions less as a compression mechanism and more as a data-dependent regulariser with a negative asymmetric payoff.", "tldr": "We show that knowledge distillation can provide statistically significant knowledge transfer; when significant non-marginal knowledge is transferred, there is a negative asymmetric payoff towards negative teacher knowledge shared with the student.", "keywords": ["Functional Perspective", "Knowledge Distillation", "Randomised Control Trial", "Distillation Scaling Laws", "Adversarial Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/999f457f37425ee987abb3b3e008be2d3054c2b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discusses the negative aspects of knowledge distillation (KD). Through various observations, the authors argue that KD itself is merely a regularizer and might even introduce detrimental tradeoffs. They compare naive KD with Random Control Distillation (RCD), suggesting that simply injecting noise into labels might be sufficient. Furthermore, they highlight KD's drawbacks – such as backdoor transfer or significant transfer of false positives (errors) – asserting that because these phenomena occur consistently, the utility of KD needs reconsideration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper addresses important cautionary points regarding the use of KD."}, "weaknesses": {"value": "Frankly, I feel that most of the claims are largely exaggerated. For instance, the RCD methodology doesn't differ significantly from the randomized teacher experiments they mentioned. Moreover, the adversarial transfer experiment ultimately boils down to whether backdoors are transferred, a claim already known, particularly in the context of LLMs.\n\nAdditionally, I find it difficult to agree with the issue raised about false positives (error transfer). The difference between the SIDDO baseline's accuracy and the teacher's true accuracy essentially represents the ceiling for improvement in correct prediction agreement. Conversely, for incorrect predictions, the teacher has already made errors, and the potential for the student to mimic these errors represents a much less restricted setting (a higher or less relevant ceiling).\n\nFurthermore, learning from incorrect predictions (dark knowledge) is part of KD's original intention. Confidence-based methodologies, which avoid learning from incorrect predictions due to the problems the authors point out, already exist. It's difficult to agree that their claims are truly novel.\n\nAlso, their methodology was applied only to very small datasets. They should demonstrate whether their claims hold true on at least ImageNet."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zfkmDNagyc", "forum": "ja0tZJS3hM", "replyto": "ja0tZJS3hM", "signatures": ["ICLR.cc/2026/Conference/Submission19408/Reviewer_oDU5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19408/Reviewer_oDU5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705278095, "cdate": 1761705278095, "tmdate": 1762931325912, "mdate": 1762931325912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the mechanism of Knowledge Distillation (KD), arguing that its benefits stem from as being a data-dependent regularizer. Through controlled experiments, the authors demonstrate that the performance gains from KD can be replicated even with randomized teacher outputs, suggesting that the primary advantage stems from the regularization effect of soft labels. Another finding is a negative asymmetry that students inherit teacher errors more readily than its correct knowledge, meaning flawed teacher predictions are disproportionately amplified. The work argues that auditing teacher quality is essential, as applying KD to an imperfect model can significantly compromise the safety and reliability of the student."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are conducted based multiple modalities, architectures and datasets.\n2. The mechanism of knowledge distillation (KD) is of great interest to the community because it involves model compression, representation learning, and transfer of inductive biases between deep models.\n3. The negative asymmetric error transfer is an interesting finding."}, "weaknesses": {"value": "1. Although multiple datasets are used, whether the conclusion can be generalized to huge teacher/student language models needs further verification.\n2. The claim that KD functions as a data-dependent regularizer is not novel, as existing studies have already established its connection to label smoothing (Random Control Distillation in this paper).\n3. While the transfer of teacher errors is concerning, the paper does not deeply examine why this happens and how to prevent it from happening."}, "questions": {"value": "1. Most of the models adopted in this paper are pretty small. How are the experiment results on large models?\n2. How can we prevent the teacher model from transferring bad knowledge to the student?\n3.The temperature parameter has a significant impact on KD performance. How does the choice of temperature affect the validity of the paper’s conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KD5WlM5HmK", "forum": "ja0tZJS3hM", "replyto": "ja0tZJS3hM", "signatures": ["ICLR.cc/2026/Conference/Submission19408/Reviewer_BrDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19408/Reviewer_BrDg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828191856, "cdate": 1761828191856, "tmdate": 1762931325467, "mdate": 1762931325467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Knowledge distillation has become a popular paradigm for compressing larger models into smaller models. This paper aims to improve our understanding of knowledge distillation through experiments in controlled settings, challenging the framework of knowledge distillation as a framework of knowledge transfer. The paper employs randomized control trials to study distillation across data modalities, i.e., the teacher replaced by noise that is subsequently fed to the student through the loss function. Then, they compute several alignment metrics between student and teacher, such as activation distance, JS divergence, prediction disagreement, etc., and also accuracy for these two sets of experimental setups. Experiments are performed across multiple experimental setups architectures, and datasets, aiming to show that knowledge distillation functions less as a compression mechanism and more as a data-dependent regulariser with an asymmetric payoff."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n-- Several experimental setups have been considered in the paper. Experiments span multiple datasets and architectures, using compute worth more than a thousand hours. \n\n-- Fig. 1 is nice and clarifies the setup well.\n\n-- The paper has also studied multiple modalities. The nuances of knowledge distillation in different modalities are interesting.\n\n-- Adversarial transfer in language models, some mutual information-based measures, and distillation scaling laws have been included."}, "weaknesses": {"value": "Weaknesses\n\n-- While the extensive experiments are quite appreciated, I have some concerns/confusion about the main claim of the paper and the conclusions being drawn from the experiments. The terms \"assymetric payoff\" , \"knowledge transfer\", and \"negative knowledge transfer\" are quite confusing. Is it sufficient to contend that knowledge distillation does not transfer knowledge since the similarity measures do not increase? It is an optimization with two disagreeing terms anyway, and can be sensitive to the choice of hyperparameters. I do not fully understand the main argument about assymetric payoff and why it is not knowledge transfer.\n\n-- The metrics used to compute \"knowledge transfer\" simply rely on the empirical alignment between teacher and student. It is unclear to me why similarity/alignment between teacher and student alone is a good measure of knowledge transfer since this doesn't consider anything specific about the task at hand. Separately, accuracy is only task-specific and doesn't consider both teacher and student together. Shouldn't an ideal measure of knowledge transfer capture what information about the task is transferred from teacher to student? Some information-theoretic metrics are introduced, but they are still analogous to either direct similarity/alignment or to accuracy. The measures are not about the task-specific alignment/similarity between teacher and student.\n\n--  In setups where randomized control distillation has higher accuracy, how is the performance of the student without distillation at all (regularizer=0)? It seems the accuracies in these cases are still kind of close, e.g., 0.952, 0.954, and 0.957. Similarly, 0.605, 0.604, and 0.607. Could the small jump in randomized control distillation be from some small overfitting that is avoided? \n\n-- Some of these settings seem to be cases where distillation itself might not make much difference (unless I am mistaken). What about setups where students with and without distillation show a big gap in accuracy? Please point me to it if already done.\n\n-- How to know if the student has the full capacity as the teacher? What is the measure of student capacity?"}, "questions": {"value": "Q1. I am confused about the main claim. Why is asymmetric payoff wrong/surprising and not knowledge transfer?\n\nQ2. Why is similarity between teacher and student a good measure of knowledge transfer? It will be interesting to consider measures that capture task-specific alignment/similarity rather than just similarity.\n\nQ3. What about setups where students with and without distillation show a big gap in performance? Please point me to it if already done.\n\nQ4: How to know if the student has the full capacity as the teacher? What is the measure of student capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ph7XlgJtHv", "forum": "ja0tZJS3hM", "replyto": "ja0tZJS3hM", "signatures": ["ICLR.cc/2026/Conference/Submission19408/Reviewer_mT8m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19408/Reviewer_mT8m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870402831, "cdate": 1761870402831, "tmdate": 1762931325035, "mdate": 1762931325035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}