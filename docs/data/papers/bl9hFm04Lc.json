{"id": "bl9hFm04Lc", "number": 1768, "cdate": 1756916450662, "mdate": 1759898187812, "content": {"title": "Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs", "abstract": "Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train \\model, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 15+ LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.", "tldr": "", "keywords": ["Human Study; Reliable LLM; Public Deliberation; Computational Social Science; Large-Scale Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb73ef4109dd19e7f50d7fcf8ccf70a549d55b4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LLMs should theoretically be capable of supporting deliberation by summarizing discussions in ways that are appropriate for use by policymakers, but no large-scale evaluations exist to benchmark their ability to do so. In response to this, the authors introduce DeliberationBank, a large-scale human-grounded dataset of crowdworker opinions and summary evaluations. They then train automatic evaluators on this dataset, finding that they comfortably outperform zero-shot LLM-as-a-judge, and use it to assess LLM summarization performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem space is well-motivated, and their contributions (the dataset DeliberationBank and the DeliberationJudge model) are highly useful contributions to this subfield. In particular, the large-scale data collection will be very useful once published.\n- Well-designed experiments make a strong case that off-the-shelf LLMs are insufficient as-is for the summarization task due to limited neutrality and representativeness. These are supplemented by detailed order analysis.\n- Thorough ablations for the judge design - it’s interesting that DeBERTa does better even than more recent LLMs."}, "weaknesses": {"value": "- Limited discussion of interannotator agreement when collecting human judgments. Many of the annotation dimensions seem highly subjective, and it would be useful to verify that there is high IAA between crowdworker judgments to verify that their judgments can be used as ground-truth values for the dataset. For example, it seems plausible that crowdworkers would have limited understanding of what kind of summaries would be useful for policymakers. \n- The minority representation case study seems pretty limited - only running on two topics, given the high inter-topic variance cited in Section 4.2, seems like it may not give the full picture. I’m also unconvinced that self-reports are the best way to classify opinions as minority or non-minority, as participants may not be well-calibrated about others’ opinions. The authors note significant past work on extracting classes of opinions from deliberation datasets - why wouldn’t a similar automatic approach work here?"}, "questions": {"value": "- How did you choose the full spectrum of topics in Appendix B1?\n- Why is Spearman’s used in some places and Pearson’s in others?\n- How do you envision others leveraging your dataset and model in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QyWU3Sl5v3", "forum": "bl9hFm04Lc", "replyto": "bl9hFm04Lc", "signatures": ["ICLR.cc/2026/Conference/Submission1768/Reviewer_pDNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1768/Reviewer_pDNN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283366621, "cdate": 1761283366621, "tmdate": 1762915884635, "mdate": 1762915884635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DELIBERATIONBANK, a large-scale dataset for studying large language models (LLMs) in the context of public deliberation summarisation, and introduces DELIBERATIONJUDGE, a fine-tuned DeBERTa model designed to evaluate summaries across four human-centered dimensions: representativeness, informativeness, neutrality, and policy approval. The dataset combines 3,000 free-form opinions from ten societal deliberation topics with 4,500 human annotations rating summaries generated by 18 different LLMs. Using this benchmark, the authors analyse systematic weaknesses in deliberation summarisation (e.g., underrepresentation of minority perspectives, input-order sensitivity) and demonstrate that their fine-tuned judge model achieves higher correlation with human judgments and greater efficiency compared to general-purpose LLM evaluators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The creation of DELIBERATIONBANK fills an important gap in large-scale, human-grounded evaluation of deliberation summarisation, a topic with high social and policy relevance.\n* The evaluation across 18 LLMs, multiple topics, and controlled input scales provides a thorough empirical picture of current model capabilities and weaknesses.\n- The study explicitly examines minority opinion coverage, a dimension rarely addressed in summarisation benchmarks, adding ethical and societal depth.\n* The human evaluation design (rating + comparison tasks) is clear, systematic, and well-documented, yielding high-quality supervision data.\n- The paper is well-written and easy to follow, with clear figures and a well-structured argument."}, "weaknesses": {"value": "* The main modelling component, DELIBERATIONJUDGE, is a straightforward fine-tuning of DeBERTa with a regression head and Huber loss. While practical, it introduces no methodological innovation beyond supervised fine-tuning. It would be good if the authors could highlight generalisable technical components of their work that might be useful for the community.\n\n- The train/test split (random 80/20) likely leads to substantial overlap in topics, question types, and summarisation styles, so the model’s generalisation to unseen deliberations or unseen LLM summarisers is unclear. Other political debate datasets such as X-Stance (Vamvas et al. 2020) make clear distinctions between topics and questions, i.e., topics in train are not in test. This does not become apparent from this work.\n    \n- The study identifies fairness and minority-representation gaps but does not probe why these biases arise or how to mitigate them, which limits the conceptual insight.\n\n-  The use of a fine-tuned DeBERTa judge resembles prior “LLM-as-a-judge” work, differing mainly by domain rather than by technique."}, "questions": {"value": "1. How distinct are the train and test sets in terms of deliberation topics and summarisation models? Can the authors show that the topics handled in train and test are genuinely different and train is not leaking information to test?\n    \n2. Did the authors evaluate performance on held-out questions or unseen summarisers to test generalisation beyond the training distribution?\n    \n3. Could the approach be extended to explicitly model deliberative diversity (e.g., stance-conditioned evaluation or contrastive objectives)?\n    \n4. What interpretability analyses, if any, were conducted to understand what linguistic or semantic cues DeliberationJudge relies on?\n    \n5. Beyond efficiency, how does the judge perform when used for ranking or selection of summaries, rather than scoring them in isolation?\n\n\n\nI find this paper interesting and valuable as a dataset and benchmark contribution, but technically limited in terms of modelling innovation. The empirical analysis is careful and socially relevant, but the methodological core (a DeBERTa fine-tune) does not offer new ideas or generalisable techniques. I recognise the value of a deliberation dataset, therefore I would currently rate it as **4 (Borderline / Reject)** but would increase my score if the authors strengthen the framing as a dataset/benchmark paper and clarify the out-of-distribution evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWfzATjmDI", "forum": "bl9hFm04Lc", "replyto": "bl9hFm04Lc", "signatures": ["ICLR.cc/2026/Conference/Submission1768/Reviewer_wcNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1768/Reviewer_wcNP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908836862, "cdate": 1761908836862, "tmdate": 1762915884283, "mdate": 1762915884283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contribution is to train DeliberationJudge, a fine-tuned DeBERTa model that, given a LLM-generated summary of opinions on a topic, and a specific individual's opinion on that topic, scores on various dimensions the extent to which that individual's opinion is captured by the summary. The training dataset is based on a large-scale dataset of human opinions and ratings for 10 different topics. In Section 3.3, they show that DeliberationJudge outperforms out-of-the-box LLM-based approaches. In Section 4, they use DeliberationJudge as a primitive to measure the abilities of different LLMs to write summaries of peoples' opinions on the various topics. Finally, Section 5 highlights that minority opinions may not always be captured by these summaries."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "S1. The dataset collected, DeliberationBank, is extremely comprehensive, involving thousands of participants' opinions, ratings, and comparisons. It's also valuable that a broad array of different topics are considered.\n\nS2. The general research approach of training and validating DeliberationJudge, and then using it as a primitive to evaluate LLM-generated summaries, is sound."}, "weaknesses": {"value": "W1. The list of the paper's contributions says (Ln 106-7): \"We conduct a rigorous and comprehensive study of LLM summarizers that surfaces systematic biases (e.g., minority-stance under-coverage, order/verbosity sensitivity)\" but I do see not any analysis of order or verbosity sensitivity in the paper. I can only find a mention in connection to Figure 25 in the appendix, but Figure 25 does not directly appear to establish any relevant claims regarding order or verbosity bias. \n\nW2. For the study on minority opinions, it is unclear the extent to which participants' self-assessment of their minority status is accurate. The analysis would be more convincing if it instead directly determined whether a participant holds a minority opinion (this should at least be possible for \"Tarrif Policy\", a binary question). \n\nW3. It would be helpful to put these contributions in context with the closely related literature on LLM-assisted deliberation that does not appear to be cited, most notably \"Fine-tuning language models to find agreement among humans with diverse preferences\" (2022) and \"Generative Social Choice\" (2023). Both papers build systems that fill a similar role to DeliberationJudge: in the first paper it is the reward model, and in the second paper it is the discriminative query. And then both papers use these systems to evaluate LLM-generated statements. \n\nW4. In terms of presentation, many aspects of the experimental design are explained using what I would view as an unnecessary amount of mathematical notation. To give one example, the equation on Line 141-2 strikes me as unnecessary. The prose would be clearer if things were described more directly."}, "questions": {"value": "Q1. Judge performance in Figure 3 and 4 is reported in terms of correlation coefficient, which can be difficult to interpret. (For example, systems that systematically give wildly over- or underestimates would still get perfect scores.) What do the results look like if instead L1 accuracy is reported? \n\nQ2. In Figure 23, since the order of summaries is randomized, why are the histograms not symmetric about the midpoint? Does this have implications about the reliability of the human annotaters? \n\nQ3. Does DeliberationJudge outperform a few-shot baseline (particularly with a strong LLM such as GPT-5)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GL6vDrJKSB", "forum": "bl9hFm04Lc", "replyto": "bl9hFm04Lc", "signatures": ["ICLR.cc/2026/Conference/Submission1768/Reviewer_Dtym"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1768/Reviewer_Dtym"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942665038, "cdate": 1761942665038, "tmdate": 1762915884031, "mdate": 1762915884031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of evaluating large-scale public deliberation summarization, in which LLMs tend to underrepresent minority opinions and exhibit biases. They introduce DELIBERATIONBANK, a large-scale, human-grounded benchmark comprising 3,000 free-form opinions on ten deliberation questions and 4,500 human annotations evaluating summaries on four dimensions: Representativeness, Informativeness, Neutrality, and Policy Approval. They fine-tune a DeBERTa-based model called DELIBERATIONJUDGE, which achieves much stronger alignment with human judgmentsUsing DELIBERATIONJUDGE, they benchmark 18 LLMs and conduct detailed analyses of performance factors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-structured and adresses an important and underexplored area: fairness and representation in deliberative AI summarization for policy contexts. The authors provide a comprehensive benchmark which is large, well-structured, and grounded in human annotations. The fine-tuned DeBERTa model achieves impressive correlation with human ratings, outperforming all general-purpose LLM judges. The authors evaluate 18 diverse models across four human-centric criteria."}, "weaknesses": {"value": "Small Comments:\n- in Figure 1 under \"Judge Model Training\" it says \"Indivisual Opinion\" instead of \"Individual Opinion\"\n- 143: two periods at the end of the sentence"}, "questions": {"value": "Did you try other judges (e.g., RoBERTa, Longformer, or Mistral 7B) to assess whether DeBERTa is uniquely suited?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RKsweVwgMJ", "forum": "bl9hFm04Lc", "replyto": "bl9hFm04Lc", "signatures": ["ICLR.cc/2026/Conference/Submission1768/Reviewer_GiWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1768/Reviewer_GiWi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943764962, "cdate": 1761943764962, "tmdate": 1762915883739, "mdate": 1762915883739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}