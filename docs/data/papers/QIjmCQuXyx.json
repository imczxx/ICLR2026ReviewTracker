{"id": "QIjmCQuXyx", "number": 5473, "cdate": 1757913272823, "mdate": 1763533242934, "content": {"title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.", "tldr": "We propose FixingGS, a training-free framework that leverages diffusion-based distillation to significantly improve sparse-view 3D reconstruction with superior visual quality and cross-view consistency.", "keywords": ["Gaussian splatting", "novel view synthesis", "3D reconstruction", "Diffusion models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9c88939004be829ecb3d9440af64503c41b57e60.pdf", "supplementary_material": "/attachment/50d49d6bded4bb20a5d69c222e09d1da28fd6c97.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a framework FixingGS for enhancing the sparse-view 3DGS, which includes the training-free score distillation of the diffusion model, and the adaptive progressive enhancement strategy for the guidance of the diffusion prior. Experiments on different datasets show FixingGS achieves improvements compared to several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well-written, and the readers can easily understand the points authors want to present.\n* The proposed adaptive progressive enhancement strategy seems intresting for me."}, "weaknesses": {"value": "* One of the main part of this work \"training-free score distillation\" is not novel enough, and I think it is a quite normal technique for extracting the diffusion knowledge.\n* Although the proposed adaptive progressive enhancement strategy seems intresting, it's more of an engineering skill than a fundamental innovation.\n* The experimental results are hard to prove the effectiveness of the proposed method compared to other methods, e.g., FixingGS only achieves the improvement of +0.003 SSIM on the DL3DV-10K dataset compared to FSGS, which does not utilize the external diffusion models, but utilizes the monocular depth prediction model as its main external supervision.\n* The ablations are also hard to prove the effectiveness of the proposed FixingGS. The experimental results in Table 3 show limited improvements of the proposed modules."}, "questions": {"value": "There are no special questions. See weaknesses for further discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ai99cGIa8H", "forum": "QIjmCQuXyx", "replyto": "QIjmCQuXyx", "signatures": ["ICLR.cc/2026/Conference/Submission5473/Reviewer_4Fn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5473/Reviewer_4Fn1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396261507, "cdate": 1761396261507, "tmdate": 1762918082679, "mdate": 1762918082679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "HKSlDnkZlC", "forum": "QIjmCQuXyx", "replyto": "QIjmCQuXyx", "signatures": ["ICLR.cc/2026/Conference/Submission5473/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5473/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763533242099, "cdate": 1763533242099, "tmdate": 1763533242099, "mdate": 1763533242099, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets sparse-view novel view synthesis under challenging cases. It proposes an iterative loop: render → diffusion repair → reliability gating via a PSNR threshold → adaptive pose shifting toward better-constrained regions → feed repaired views back into training. The idea is practically motivated, and the pipeline is clearly written. Reported results show improvements over several baselines on standard image-quality metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Practical motivation & clear pipeline.** Using diffusion as a prior to “repair” sparse-view renderings and feeding the signal back into 3DGS is a sensible, implementable loop.\n2. **Handling sparse and blurred scenes.** The paper targets areas where sparse view reconstruction is particularly prone to failure. This is a real problem with obvious practical pain points, not just a small indicator stacking under normal conditions.\n3. **Clarity-wise:** the manuscript is clearly written, with well-structured methodology, detailed explanations, and intuitive visualizations that enhance understanding"}, "weaknesses": {"value": "**1. Hard-coded PSNR threshold (η) lacks justification, parameter sweep, or visual evidence**\nThe gating rule “trigger APE if PSNR(I_fix, I_extra) < η” is central to the proposed safety mechanism. However, the rationale for using PSNR instead of alternative similarity measures such as SSIM, LPIPS, or a composite metric is not explained. The method for selecting η (fixed or adaptive) is also unclear. The paper provides no histogram of PSNR values, no ablation over η (for example, 18, 20, 22, 24, 26, or 28 dB), and no visual examples demonstrating why low PSNR corresponds to unreliable outputs.\n\n**2. Supervision configuration and hyperparameter ablations are insufficient**\nThe study fixes a single “pseudo-label alignment” configuration, where diffusion-repaired images are used as targets under a pixel-level loss. Several key hyperparameters, including t₀, the global weighting term ω, and optional perceptual terms, are fixed without robustness analysis. This limits the understanding of how sensitive the approach is to supervision choices.\n\n**3. Consistency metrics and failure analysis are limited**\nThe paper repeatedly claims that the proposed method improves cross-view consistency and mitigates multi-view inconsistency. Table 3 presents an ablation comparing “with APE” and “without APE” using standard reconstruction metrics (PSNR, SSIM, and LPIPS), which demonstrates overall quality gains but does not substantiate the claimed mechanism. Appendix G introduces a cross-view consistency metric (for example, TSED) evaluated on DL3DV, which is informative but has two limitations: (i) it is not included among the main results or applied to other benchmarks, and (ii) it is not linked to mechanism-specific ablations such as with versus without APE under TSED, or under different reliability thresholds η. Moreover, only successful cases are shown, without any examples of failure modes such as diffusion hallucinating textures that fail to remain consistent across nearby views.\n\n**4. Lacking Training Details**\nDiffusion-based backbones typically require input resolutions aligned with network strides (for example, multiples of 8). The paper does not specify the native resolutions used for DL3DV or other datasets, nor does it describe the preprocessing strategy applied to satisfy this constraint. It is also unclear whether isotropic resizing, center cropping, or reflection padding was used, and whether the same pipeline was applied to all baselines. These choices can substantially affect image sharpness, field of view, and fairness across comparisons.\n\n---\nI have listed my concerns, and the score will be adjusted based on the author's response."}, "questions": {"value": "Please refer to Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bLN3LCRkVu", "forum": "QIjmCQuXyx", "replyto": "QIjmCQuXyx", "signatures": ["ICLR.cc/2026/Conference/Submission5473/Reviewer_q4vs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5473/Reviewer_q4vs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400761716, "cdate": 1761400761716, "tmdate": 1762918082223, "mdate": 1762918082223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aim to address the artifact issues in 3D Gaussian Splatting (3DGS) when reconstructing from sparse viewpoints. Recent methods that enhance 3DGS with generative priors suffer from the problem of using \"lagging\" diffusion priors that are updated at fixed intervals, leading to inconsistent supervision. The proposed method, FixingGS, introduces a \"training-free\" score distillation approach that continuously leverages a pre-trained diffusion model (Difix) throughout the optimization process to provide more accurate and cross-view consistent guidance. Additionally, an Adaptive Progressive Enhancement (APE) scheme is proposed to improve under-constrained regions by identifying unreliable views and generating new training samples using multiple reference views.\nExperiments on the DL3DV-10K and Mip-NeRF 360 datasets demonstrate superior quantitative and qualitative results compared to existing SOTA methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is easy to implement and follow.\n- The proposed method APE shows good performance in the experiments, achieving better results than existing SOTA methods on the DL3DV-10K and Mip-NeRF 360 datasets, the ablation studies demonstrate the effectiveness of each component of the method."}, "weaknesses": {"value": "- This paper suffers from lack of novelty. The proposed method does not make significant improvements over Diffx3D+, mainly replacing the \"lagging\" diffusion prior in DiffGS with a \"training-free\" score distillation approach, without any improvements on SDS itself. There is no modification during the distillation process. The claimed score distillation introduces higher computational overhead during training (as it requires calling the Diffx3D+ model at every training step), while the choice of Diffx3D+ seems to be merely for efficiency balance.\n- The experimental settings are not reasonable. The comparison is made with the official Diffx3D+ results, but if the claim is that the \"lagging\" diffusion prior in Diffx3D+ is problematic, why not simply reduce the update interval of Diffx3D+ to make it update the diffusion prior more frequently? This could also address the \"lagging\" issue. The paper does not provide relevant comparative experiments. Additionally, the paper should compare with more methods that use SDS.\n- The APE method uses a heuristic approach to select unreliable views, which is relatively crude. The authors could consider improving it by referencing adaptive methods like FisherRF\n\n[1] Active View Selection and Mapping with Radiance Fields using Fisher Information"}, "questions": {"value": "- Why not reduce the update interval of Diffx3D+ to make it update the diffusion prior more frequently, instead of introducing a new score distillation method? Have you conducted any experiments to compare this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNjyCbRgce", "forum": "QIjmCQuXyx", "replyto": "QIjmCQuXyx", "signatures": ["ICLR.cc/2026/Conference/Submission5473/Reviewer_fGZm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5473/Reviewer_fGZm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538635645, "cdate": 1761538635645, "tmdate": 1762918081925, "mdate": 1762918081925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper integrates the SDS loss into 3D reconstruction with diffusion priors, achieving superior results compared to using diffusion-refined images directly. To address unreliable views, it further introduces the APE module, which leverages multiple reference views with slightly shifted camera poses to enhance reconstruction consistency. Experimental results show that APE effectively improves the overall reconstruction quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work is the first to apply the SDS loss for novel view synthesis, demonstrating state-of-the-art performance. The results clearly show that leveraging diffusion gradients provides a more effective supervision signal than directly using diffusion-refined images, highlighting a significant novelty and contribution in bridging diffusion priors with 3D reconstruction.\n\n2.The proposed APE module effectively enhances the performance of the diffusion + 3DGS pipeline, particularly under unreliable or challenging view conditions.\n\n3.The paper is well written, logically structured, and easy to follow."}, "weaknesses": {"value": "1. Based on Equation (5), the essential difference between the proposed SDS loss and DiFix3D lies in the additional gradient path that passes through the diffusion model. This extra gradient allows 3DGS to receive guidance not only from the pixel-space reconstruction loss but also from the diffusion model’s internal score field, which theoretically provides distribution-level supervision. However, the paper does not analyze why or how this additional gradient benefits the 3D reconstruction process. For instance, it remains unclear whether the score-based gradient improves geometry consistency, enhances perceptual realism, or simply acts as a regularization term. \n\n2. The novelty of this work appears somewhat incremental. Integrating the SDS loss into a 3DGS pipeline has already been explored in prior works such as DreamGaussian [1] . Moreover, it is well known that SDS loss often introduces blurring artifacts and degrades high-frequency fidelity, which was well studied in VSD[2]. The authors should make their framework more solid—e.g., by analyzing how their pipeline mitigates the typical limitations of SDS or by demonstrating clear advantages beyond a simple replacement of the loss function.\n\n3.Using SDS loss alone does not automatically make the pipeline training-free. The framework remains training-free only if the diffusion prior is a frozen, general pretrained model. Once a task-specific or fine-tuned diffusion (e.g., DiFix3D+) is used, the pipeline effectively relies on prior training and thus cannot be strictly regarded as training-free.\n\n4.As stated in the paper, the SDS loss is applied throughout the entire training process, which is expected to substantially increase the overall training time due to repeated diffusion inference and backpropagation through the score network.\n\nHere is the citation:\n[1] Tang, J., Ren, J., Zhou, H., Liu, Z., & Zeng, G. (2023). Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653.\n[2] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., & Zhu, J. (2023). Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36, 8406-8441."}, "questions": {"value": "1. Does the SDS loss make the scene more blurry?\n\n2 Why does 3DGS outperform many methods on the DL3DV dataset?\n\n3.Since the SDS loss is applied throughout the entire training process, have you tried doing the same for DiFix3D+?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WvanVyHrJP", "forum": "QIjmCQuXyx", "replyto": "QIjmCQuXyx", "signatures": ["ICLR.cc/2026/Conference/Submission5473/Reviewer_E9KJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5473/Reviewer_E9KJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633604220, "cdate": 1761633604220, "tmdate": 1762918081671, "mdate": 1762918081671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}