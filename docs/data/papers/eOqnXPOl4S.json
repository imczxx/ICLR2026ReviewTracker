{"id": "eOqnXPOl4S", "number": 11678, "cdate": 1758203032648, "mdate": 1759897561655, "content": {"title": "Reinforcement Learning with Missing Context to Mitigate Reward Hacking from Training Only on Golden Answers", "abstract": "Reinforcement learning (RL) for reasoning has achieved remarkable progress in recent years. However, much of this progress has been evaluated in overly idealized settings. In most existing benchmarks, problems are deterministic, carefully curated, and fully specified. \nWhile such settings make evaluation straightforward, real-world reasoning tasks are often underspecified, lack crucial contextual information, or even contain misleading premises.  Hence, we argue that most current RL training paradigms based on verifiable rewards amount to an implicit form of reward hacking.  Our experiments show that many state-of-the-art reasoning models tend to overcommit to producing a single definite answer, even when the problem is inherently underspecified. To address this gap, we propose \\emph{Reinforcement Learning with Missing Context} (RLMC), a framework that explicitly trains models on problem instances with missing, underspecified, or incorrect context.  We construct a large-scale RL dataset of 120K queries by intentionally synthesizing such imperfect questions, encouraging models to identify uncertainty, make reasonable assumptions, and reason effectively under incomplete information.  Experimental results show that RLMC-trained models exhibit substantial gains in robustness, reduced hallucinations, and improved overall reasoning capabilities compared to baselines trained only on fully specified tasks. \nWe further introduce \\textsc{Reasoning Beyond the Given} (RBG), a benchmark designed to evaluate whether models can detect missing or inconsistent information and proactively elicit clarifying input from users. Evaluation on \\textsc{RBG} fully exposes current models' limitations in handling imperfect problem statements.  Code, \\textsc{RBG} and train data will be fully released at \\url{https://anonymous.4open.science/r/RLMC-RBG}.", "tldr": "", "keywords": ["reinforcement learning", "benchmark", "large language models", "conversational llm", "chatbots", "synthetic data", "reward hacking"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52af8c4776f09d5d89b2034806443c57491ca713.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a new framework and benchmark to evaluate and improve the abiltiy of answer unclear questions (missing context)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- RLMC significantly improves robustness, reduces hallucinations, and enables models to recognize uncertainty and seek clarification when context is missing.\n\n- if opensource, the benchmark will help researcher explore this direction."}, "weaknesses": {"value": "- The code repo is empty.\n\n- The general reasoning ability will decrease after RLMC. There is clear trade-off between HRB and final performance."}, "questions": {"value": "- What will be the performance if introduce a small finetuned classification model to decided whether the question is ideal.\n\n- How is the proportional division in the section “A Partition of the Trajectory Space” determined?\n\n- It will be better to add a comparsion to RLVR/SFT only methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mfX7l4mpmP", "forum": "eOqnXPOl4S", "replyto": "eOqnXPOl4S", "signatures": ["ICLR.cc/2026/Conference/Submission11678/Reviewer_owYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11678/Reviewer_owYq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621501827, "cdate": 1761621501827, "tmdate": 1762922728846, "mdate": 1762922728846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies an issue in current RL paradigms for reasoning tasks - reward hacking due to training only on golden answers. Instead, the authors posit that the models should abstain when they do not know, ie, if the problem is inherently underspecified. They introduce Reinforcement Learning with Missing Context (RLMC), a method to reduce reward hacking by incorporating synthetically generated questions that lack key context (generated synthetically by altering knowledge graphs). They also introduce the Hypothetical Reasoning Benchmark (HRB) to evaluate robustness to missing context. Compared against other baselines, the proposed method performs well on HRB and IDK score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The problem is clear and well-motivated; the current paradigm with both datasets and training results in models being encouraged to answer questions even that they do not know. This work seeks to solve this - can models ask for more information?\n* The paper has a nice initial section which justifies current issues with reward hacking, and connects this to the need for missing context training data and evaluation.\n* The paper is well-written and easy to follow\n* The proposed method is intuitive and technically sound. It is described well in Figure 2. The simplicity is a plus - easy to implement and understand, while doing well compared to a few baselines. \n* The reasoning graph construction is motivated well and clear. There are helpful additional studies which have interesting findings on best practices for this construction."}, "weaknesses": {"value": "* The authors should include a related work section to clarify novelty relative to AmbigQA, SQuAD 2.0, and prior RLHF studies. Without it, is unclear how the framing is different \n* Minor, but some text in the figures is hard to read at times. For example, in Figure 3. \n* Minor, but there are some awkward phrasings which can be clarified, ie, \"Utilizing Reasoning Graph and Condition Perturbation for Missing-Context Problems Synthetic\"\n* It would be nice to see that the method works well on at least one other model. Optionally, would also be nice to show how well this works either on 1) a smaller model or 2) a larger model.\n* The section on the hypothetical reasoning benchmark is not discussed enough in the main text. It is in the appendix with no link to it (I had a hard time finding it). In addition, it is missing citations to some of the claims (ie, \"Current reasoning benchmarks are largely confined to well-posed problems, failing to assess model robustness to imperfect, real-world information.\"). \n* While there is the IDK-RL baseline, there could be more uncertainty-calibrated baselines, which do not require any additional training. There are several options here, for example, using LM introspection itself, using the log probs, etc."}, "questions": {"value": "* Can the framing of this task be considered a subset of ambiguous questions, ie, as mentioned in AmbigQA?\n* Is there potential relevance of this method for safety/alignment?\n* Is there any validation of the synthetic data generated? If not, how to maintain high-quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DoBwiUHDep", "forum": "eOqnXPOl4S", "replyto": "eOqnXPOl4S", "signatures": ["ICLR.cc/2026/Conference/Submission11678/Reviewer_M9UW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11678/Reviewer_M9UW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879538270, "cdate": 1761879538270, "tmdate": 1762922728440, "mdate": 1762922728440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a flaw in current RLVR setups where the models learn to produce confident but unsupported outputs on underspecified questions. The authors propose a framework that generates synthetic underspecified problems via a reasoning-graph pipeline and use it to create a dataset of such questions. In addition, the authors suggest a reward function for training models over this dataset and experiment with it."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic of unanswerable or underspecified questions is timely and important. It addresses a real limitation of RLVR.\n\n- The proposed synthetic data pipeline for generating missing-context problems is novel, well-engineered, and general enough to be applied across domains."}, "weaknesses": {"value": "- While the problem you identified is real, I do not think it is a reward hacking. If all training problems satisfy $\\mathcal T(s')=\\{\\tau_g\\}$ then the gradients in Eq. 3 and Eq. 4 are equivalent. In that case, the “reward hacking” effect seems more an artifact of environment design than true reward misalignment (reward hacking is typically defined as exploiting the reward in unintended ways).\n\n- The RLMC reward design feels heuristic and not grounded. The choice of behavioral categories (SH, EA, Abs, Cond, Elicit) and their assigned numeric rewards seems ad hoc. Where do these values come from? Why these categories and not others (e.g., partial solution, answering with uncertainty quantification)? A more principled justification or sensitivity analysis is needed\n\n- The missing-context pipeline is the key contribution, but there’s no rigorous evaluation showing that the generated questions are indeed high quality. For example, what is the success rate of the frontier model at creating the reasoning path tree? It is mentioned that “despite their tendency to hallucinate when answering, leading LLMs excel at judging the unanswerability of Missing Context problems under the guidance of a specific judge prompt.” Can you please provide more information? Numbers?\n\n- Please report 95% confidence intervals for your results, particularly on smaller datasets such as MATH-500 (where CI is usually ±3%) it’s hard to assess the statistical significance of Table 1’s improvements."}, "questions": {"value": "- Maybe it is a small thing, but I think section 2 is missing an important distinction between a reasoning trace and a final answer. RLVR does not maximize the likelihood of a single golden trace, but of every trace that ends up in the golden solution. There can be an infinite number of such reasoning traces. The claim presented in this section should be that there can be multiple “valid” answers, not traces. \n\n- Equation 4 assumes a uniform distribution over T(s′). In practice, some valid answers or reasoning paths should be more likely than others. Even the authors later weigh them differently. This inconsistency should be clarified.\n\n- Is the IDK-RL baseline trained on the same question mixture as RLMC? It is not clear from the text\n\n- The paper claims RLMC lies on the Pareto frontier of HRB vs. IDK score, but by inspection IDK-RL also seems to occupy a frontier point? just at a different trade-off"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z3SWgAWfVi", "forum": "eOqnXPOl4S", "replyto": "eOqnXPOl4S", "signatures": ["ICLR.cc/2026/Conference/Submission11678/Reviewer_ircM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11678/Reviewer_ircM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925478703, "cdate": 1761925478703, "tmdate": 1762922728068, "mdate": 1762922728068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of excessive idealization in existing reasoning reinforcement learning paradigms trained on \"ground truth,\" which leads to \"reward hacking.\"\nTo address this issue, the paper proposes the \"Reinforcement Learning with Missing Context (RLMC)\" framework. It synthesizes problems with missing or incorrect context through a reasoning-graph-guided pipeline (constructing a large-scale training dataset of 120K queries) and designs a structured reward system to encourage models to perform hypothetical reasoning. Meanwhile, it introduces the \"Hypothetical Reasoning Benchmark (HRB)\" to evaluate models' ability to handle incomplete information. Experimental results show that models trained with RLMC outperform baselines on HRB while maintaining strong general reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretically analyzes the causes of the reward hacking phenomenon, providing a theoretical framework for subsequent research on \"how to design RL objectives more in line with real-world needs.\"  \n- Proposes the RLMC (Reinforcement Learning with Missing Context) framework. It synthesizes high-quality problems with missing context through a DAG-based pipeline and designs a structured reward system, effectively alleviating the reward hacking phenomenon.  \n- Constructs a benchmark for systematically measuring models’ hallucinations and advanced hypothetical reasoning capabilities under missing or inconsistent context, addressing the lack of such evaluations in the current research community.  \n- Empirically demonstrates that \"hypothetical reasoning is a more advanced reasoning capability than IDK (I don’t know) behavior,\" which provides insights for the advanced training of large language models’ reasoning abilities."}, "weaknesses": {"value": "- The evaluation is insufficient. The authors claim that RLMC is more adaptable to real-world scenarios, but when evaluating general capabilities, this paper mainly focuses on math-related metrics. It is suggested that the authors supplement more evaluation on general tasks, such as LiveBench [1] (logic, coding, and language comprehension), ANAH [2] (knowledge-based QA), and SciBench [3] (multi-domain scientific problem-solving), to further illustrate the effectiveness of this method in real-world scenarios.\n\n[1] White C, Dooley S, Roberts M, et al. Livebench: A challenging, contamination-free llm benchmark[J]. arXiv preprint arXiv:2406.19314, 2024, 4.\n\n[2] Ji Z, Gu Y, Zhang W, et al. Anah: Analytical annotation of hallucinations in large language models[J]. arXiv preprint arXiv:2405.20315, 2024.\n\n[3] Wang X, Hu Z, Lu P, et al. Scibench: Evaluating college-level scientific problem-solving abilities of large language models[J]. arXiv preprint arXiv:2307.10635, 2023."}, "questions": {"value": "- The anonymous repository link provided by the author seems to be invalid. Is there an updated version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeovHPCsbw", "forum": "eOqnXPOl4S", "replyto": "eOqnXPOl4S", "signatures": ["ICLR.cc/2026/Conference/Submission11678/Reviewer_S2Xz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11678/Reviewer_S2Xz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987836618, "cdate": 1761987836618, "tmdate": 1762922727668, "mdate": 1762922727668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}