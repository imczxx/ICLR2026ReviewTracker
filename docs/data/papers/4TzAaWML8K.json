{"id": "4TzAaWML8K", "number": 5840, "cdate": 1757939397497, "mdate": 1759897950206, "content": {"title": "Towards Enhanced Image Generation via Multi-Modal Chain of Thought in Unified Generative Models", "abstract": "Unified generative models have shown remarkable performance in both text and image generation. When faced with image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, we find that direct T2I generation limits unified generative models in handling complex compositional instructions. Such instructions frequently occur in realistic application scenarios. Although this is a vital issue, existing works predominantly focus on improving the basic image generation capability of unified generative models. While improvements in basic image generation can contribute to complex image generation to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems in a step-by-step manner, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first introduce Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts based on function. In this way, FoXperts disentangles the potential conflicts in current mainstream modality-oriented designs and provide a sound foundation for CoT. When introducing CoT, the first question is how to design a CoT approach specifically for complex image generation. To this end, we emulate a human-like artistic workflow---planning, acting, reflection, and correction---and propose the Multimodal Chain of Thought (MCoT) approach, since the data here involves multiple modalities (text and image). In response to the subsequent challenge---how to design an effective MCoT training paradigm---we develop a multi-task joint training paradigm that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm overcomes the difficulty and impracticality of collecting consistent multi-step data tuples for training. Extensive experiments demonstrate that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable quantitative improvements in complex image generation.", "tldr": "", "keywords": ["image generation", "generative model", "multimodal chain of thought"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a154bcc28b726d727c63bd43320b0899f53f8559.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical limitation in contemporary unified generative models: their struggle with ​complex compositional image generation​ (e.g., multi-object scenes, spatial relationships). The authors identify that direct text-to-image (T2I) generation is insufficient for these challenges and propose a novel reasoning-based paradigm inspired by Chain-of-Thought (CoT).\n\nThe authors introduce ​FoXperts, an expert-parallel architecture that assigns experts based on function:1. A unified ​Linguistic Expert​ for text. 2. A dedicated ​Semantic Vision Expert​ for visual understanding tasks. 3. A dedicated ​Generative Vision Expert​ for visual generation tasks. Also, they propose ​MCoT, a four-step (Planning, Acting, Reflection, Correction) reasoning framework that emulates a human artistic workflow. The proposed model, ​FoX​ (1.3B parameters), demonstrates highly competitive performance across diverse benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of applying a Chain-of-Thought reasoning process to complex image generation is inspiring. The functionality-oriented expert architecture presents a fresh alternative to mainstream modality-oriented designs, effectively addressing a fundamental conflict in multimodal modeling.\n\n2. The experimental evaluation is thorough. The paper validates its approach across a wide range of well-established benchmarks for both image generation and understanding, demonstrating the high performance of their model.\n\n3. By moving from one-shot generation to a reasoned, multi-step process, it enhances the reliability and controllability of models for complex tasks. The proposed training paradigm is particularly significant as it provides a practical solution to a major data availability challenge."}, "weaknesses": {"value": "1. What was the original intention behind using the VAE encoder for the \"Image for Understand\" component? Why not using a model with richer semantic features, like CLIP?\n\n2. The overall training and inference process is multi-staged. What is the rationale for integrating and training these stages within a single model? For the same workflow, what are the advantages compared to using two expert models (e.g., a VLM for understanding and a generative model for creation)? For instance, can the understanding and generation tasks mutually enhance each other?\n\n3. Although the total parameter of the model  is 1.3B, the entire image generation workflow is relatively long and time-consuming. How is the trade-off between performance and efficiency balanced?\n\n4. Can the \"reflection and correction\" process be performed multiple times? If so, how does it affect the model's performance? Does the long-context issue impact this process? Could the authors provide some illustrative examples?\n\n5. If an error occurs during the reflection stage, it will inevitably lead to mistakes in the subsequent correction. This may even result in a corrected image that is poorer than the initially generated one? How to prevent this issue as much as possible?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pt8a94TUBe", "forum": "4TzAaWML8K", "replyto": "4TzAaWML8K", "signatures": ["ICLR.cc/2026/Conference/Submission5840/Reviewer_4655"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5840/Reviewer_4655"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379424003, "cdate": 1761379424003, "tmdate": 1762918297694, "mdate": 1762918297694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the task of complex image generation by firstly introducing a unified generative model named FoX, which consists of FoXperts that disentangle experts by functions of generation and understanding. It also proposes an MCoT method to address image generation as a multi-step process of planning, acting, reflection, and correction, with a multi-task joint training paradigm to train the model without consistent multi-step data. Experiments are conducted on image generation and understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The model shows satisfying performance with a small number of parameters.\n2. The generated images seem to effectively solve the complex image generation task, judging from the qualitative results.\n3. The overall writing is clear and easy to follow."}, "weaknesses": {"value": "1. The proposed method shows limited novelty compared with previous work. Assigning experts by functionality, the core contribution of FoX, is already introduced by BAGEL[1]. Using CoT in image generation is also present in works like T2I-R1[2], GoT-R1[3], Uni-CoT[4] but are not discussed in this paper. The main difference between them and the proposed MCoT lies in the layout planning, but this is somewhat confined to the compositional image generation task (e.g., T2I-CompBench) and cannot be extended to more general scenarios.\n2. Benchmarks used in multimodal understanding are not sufficient. The experiments only consider MME-P, MMBench, and VQAv2. Commonly used ones like MMMU, MM-Vet, TextVQA, and InfoVQA are missing.\n3. The model scale is still small (1.3B). It is unclear whether the proposed method can be scaled to larger models.\n\n\n[1] Emerging Properties in Unified Multimodal Pretraining\n\n[2] T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT\n\n[3] GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning\n\n[4] Uni-CoT: Towards Unified Chain-of-Thought ReasoningAcross Text and Vision"}, "questions": {"value": "1. Baselines in Table 2 are too old. What about recent models?\n2. Can MCoT be extended to more general reasoning-related image generation benchmarks like WISE[5], T2I-ReasonBench[6] and PhyBench[7]?\n3. Line 879: why is Semantic Visual Expert initialized from Generative Visual Expert? Is there an ablation on initializing from scratch or from Linguistic Expert (like in [8])?\n4. Some writing issues:\n- Line 170-172: I suppose these papers released in 2024 are not concurrent.\n- Line 874: Qwne -> Qwen\n- Table 9: No textual explanation. Better explain that the results are on T2I-CompBench.\n\n[5] WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation\n\n[6] T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation\n\n[7] PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models\n\n[8] Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5Sn2dCDNyn", "forum": "4TzAaWML8K", "replyto": "4TzAaWML8K", "signatures": ["ICLR.cc/2026/Conference/Submission5840/Reviewer_cMFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5840/Reviewer_cMFY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555983291, "cdate": 1761555983291, "tmdate": 1762918297234, "mdate": 1762918297234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FoXperts, a unified generative model with a Functionality-oriented eXperts architecture that mitigates function-domain conflicts in modality-oriented designs, while seamlessly integrating both understanding and generation across textual and visual modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The method is simple and easy to understand.\n* MCoT splits complex drawing into four quick passes—plan, execute, reflect, refine—each targeting one goal so error drops round-by-round.\n* FoXperts assigns “seeing” and “painting” to two separate vision experts, plus a language expert, eliminating internal conflict and giving later iterations a solid base."}, "weaknesses": {"value": "* The paper mainly combines an expert architecture with a unified understanding–generation model. Technically, it divides the visual expert into two parts—semantic understanding and generation—which is rather straightforward. Essentially, it does not differ significantly from common mixture-of-experts models, so the innovation seems to lie more in integration than in architectural novelty.\n* The proposed MCoT adopts a four-step process, which appears to be a standard approach in Chain-of-Thought (CoT) methods, without specific adaptations for image generation tasks.\n* The idea of a planning → execution → reflection → revision process has already been reflected in existing CoT-based visual-language models (e.g., CoT-VLA), yet the paper lacks comparisons or discussions regarding these related works.\n* Although the paper proposes a four-step process, it does not explain why such a complex procedure is necessary, as opposed to simpler two-stage (e.g., planning + generation) or three-stage designs.\n* The paper claims that “a single visual expert leads to functional conflicts,” but provides neither experimental evidence nor theoretical analysis to demonstrate the existence or impact of such conflicts on performance.\n* The paper argues that “direct T2I generation cannot handle complex compositional instructions,” yet current models like SD3 and DALL·E 3 already show strong performance in complex scene generation. The authors do not sufficiently justify why introducing CoT is essential rather than further optimizing existing generative models."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MvvOfqvIIK", "forum": "4TzAaWML8K", "replyto": "4TzAaWML8K", "signatures": ["ICLR.cc/2026/Conference/Submission5840/Reviewer_pLvw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5840/Reviewer_pLvw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814416016, "cdate": 1761814416016, "tmdate": 1762918296685, "mdate": 1762918296685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study introduces a unified generative model that improves complex image creation through a functionality-oriented expert design and a multimodal chain of thought process. The model separates visual understanding and generation to strengthen both abilities and follows a four-step reasoning workflow of planning, acting, reflection, and correction. A multi-task training scheme enables each step to be learned independently without costly supervision. Experiments on several benchmarks show clear gains in compositional accuracy and visual quality, highlighting the value of combining functional expert design with stepwise reasoning for better image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The separation of generation and understanding experts brings performance gains, which shows potential in unified MLLMs.\n2. The multi-task joint training paradigm enables efficient learning of each reasoning step without requiring expensive multi-step supervision, making the approach scalable in terms of data efficiency.\n3. The proposed FoX achieves best results on most generation benchmarks, which shows the effectiveness for this framework."}, "weaknesses": {"value": "1. All experiments are conducted using the Qwen2 0.5B backbone without comparisons across larger scales or alternative architectures, leaving uncertainty about the method’s scalability and general applicability.\n2. The proposed MCoT framework introduces multiple reasoning steps (planning, acting, reflection, correction), which likely increase inference time and computational cost compared with direct text-to-image generation."}, "questions": {"value": "1. The multi-step MCoT process (planning, acting, reflection, correction) likely increases inference time. Can the authors provide quantitative comparisons of inference latency and memory usage versus baseline text-to-image generation?\n2. Could the authors clarify whether the proposed FoX and MCoT framework can generalize to other backbone architectures or larger models beyond Qwen2 0.5B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5zFhhPaBU", "forum": "4TzAaWML8K", "replyto": "4TzAaWML8K", "signatures": ["ICLR.cc/2026/Conference/Submission5840/Reviewer_Zoat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5840/Reviewer_Zoat"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924520096, "cdate": 1761924520096, "tmdate": 1762918296065, "mdate": 1762918296065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}