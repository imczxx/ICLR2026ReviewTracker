{"id": "2eL6yXLCh8", "number": 9751, "cdate": 1758138191906, "mdate": 1759897700588, "content": {"title": "Hyden: A Hybrid Dual-Path Encoder for Monocular Geometry of High-resolution Images", "abstract": "We present a hybrid dual-path vision encoder (Hyden) for high-resolution monocular depth, point map and surface normal estimation, surpassing state-of-the-art accuracy with a fraction of the inference cost. The architecture pairs a low-resolution Vision Transformer branch for global context with a full-resolution CNN branch for fine details, fusing features via a lightweight MLP before decoding. By exploiting the linear scaling of CNNs and constraining transformer computation to a fixed resolution, the model delivers fast inference even on multi-megapixel inputs. To overcome the scarcity of high-quality high-resolution supervision, we introduce a self-distillation framework that generates pseudo-labels from existing models at both lower resolution full images and high-resolution crops—global labels preserve geometric accuracy, while local labels capture sharper details. To demonstrate the flexibility of our approach, we integrate Hyden and our self-distillation method into DepthAnything-v2 for depth estimation and MoGe2 for surface normal and metric point map prediction, achieving state-of-the-art results on high-resolution benchmarks with the lowest inference latency among competing methods.", "tldr": "We propose a hybrid dual-path encoder design for monocular geometry prediction of high-resolution images, which achieves SOTA results with low inference latency.", "keywords": ["3D vision", "Monocular Depth Estimation", "Monocular Surface Normal Estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eab27caa571ee535281d2ab16647e0cbdc3bdc3f.pdf", "supplementary_material": "/attachment/79f26fee8f2054407e3510d07a3d3ed6f2c6a1fb.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents a conv-transformer dual-path framework for high resolution depth estimation. The transformer processes lower resolution input whereas the conv processes higher one. After fusing the features, the model provides high resolution depth. Due to the scarcity of HR depth GT in the wild, the paper adopts a self-distillation method for labeling. Experiments show promising results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dual-branch design is well-motivated and the overall framework is technically sound.\n\n- The framework shows strong performance compared with current sota methods.\n\n- The paper is well-writen and it's easy for me to follow the ideas."}, "weaknesses": {"value": "- It would be better to add discussions about papers sharing the same ideas. For self-distillation: consider [1]. For dual branch, consider [2]. From my point view, discussing comparisons with [1] is kind of crucial as [1] was released 8 months ago and it was a trending paper. Though Hyden additionally presents a good architecture, the self-distillation is another major contribution claimed by the paper. [2] is an old paper but using the same dual-branch plus feature-level fusion idea. It would be good to add it in the related work.\n\n    [1] Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator\n    [2] DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation\n\n- Since the scale and shift alignment is not perfect (is kind of noisy as far as I see), it would be better to present some clues about the error introduced by this alignment process, giving readers more insights about the upper bound of this method. It would benifit future potential follow-up works.\n\n- One follow-up question: what if we use the way generating pseudo labels for direct evaluation? Given a HR image, we use DAV2/Moge for coarse prediction. Then, we crop patches from the HR image, use DAV2/Moge for patch-wise prediction. Finally, we align the patch-level prediction with coarse one and ensemble predictions to get final depth. I know this method leads to a super long inference time, but it would be good to have this one in the paper to indicate the performance and inference time gap.\n\n- It's hard to reproduce the experimental results, majorly because the introduction to the training data is vague. Since the unlabeled data can be from any dataset, why don't use public available ones for the sake of potential follow-up works? Also, it would be good to include the results with different size of training data.\n\n- I was wondering how many parameters are frozen vs, trainable in the architecture. Given a frozen transformer, I assume that only convolutional encoder and the fusion decoder are trainable."}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SqE7PL1kUv", "forum": "2eL6yXLCh8", "replyto": "2eL6yXLCh8", "signatures": ["ICLR.cc/2026/Conference/Submission9751/Reviewer_R67Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9751/Reviewer_R67Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760876006808, "cdate": 1760876006808, "tmdate": 1762921246165, "mdate": 1762921246165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a method for extending a dense predictor (based on ViT) to high resolution, applied to depth, surface normals and point maps prediction.  Given an already-trained network on a medium resolution (e.g. 518x518), the network is extended with a convnet (that scales linearly with image area) by fusing features from the original global network and the convnet.  This second-stage model is trained using a self-supervised method that predicts pseudolabels on crops using the original model, and aligns the crops to the global image prediction to form the training targets.\n\nNotably, for any of the higher resolutions, the ViT's resolutions is held constant at 518.  This takes advantage of two properties/assumptions of the image captures: (1) the camera captures a scene framed by the image extent, so while increased resolution supplies more details, the world scene in the image doesn't change and can be handled by the ViT at fixed res; and (2) crops of the image are themselves scenes with smaller (or more distant) objects that are represented well enough in the original training set to be accurately predicted.\n\nThe method is incorporated into DepthAnything-v2 and MoGe2, with evaluations showing excellent improvements in latency vs resolution, accuracy measures and visual details at high res up."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is an exceptionally simple method with great results, naturally combining the strengths of ViT and convnets to form a performant system.  The self-supervised training technique leverages existing models and data, making this extension mechanism fairly general.  I also found the presentation and writing very clear (except in a couple parts mention below), I much enjoyed reading it."}, "weaknesses": {"value": "While this is an excellent extension method, it requires an already performant model to distill from.  And in particular, the original model must be performant at its full original (518) res on crops which may have a different scene framing or subjects from the original images.  While these are indeed the case for the applications here, it would be good to see this discussed as a requirement."}, "questions": {"value": "To what extent do the ViT predictions of the global image vs crop disagree?  Some is of course inherently necessary, details and boundaries not present at lower res.  But are there times when the two scales disagree in larger areas?  How often does that occur and does it need to be handled in the alignment or by filtering?\n\nDo you think this could be used for semantic segmentation and/or panoptic segmentation?  In particular, crops may or may not have enough of the objects present to identify them, depending on their size.  So the alignment between crops and global scale may need to account for that, as well as the possibility that the smaller crops' segmentations by the original model could output finer segmentation levels (e.g. car globally vs car/door/window in crops) depending on the model.\n\n3.2.3:  I think this description is little confusing with the description of injecting into a support set, though I think I understand it.  Equivalently, though, is this the same as cropping the student SxS predictions and comparing against the corresponding teacher labels for the crop, summing over the crops in the task loss?\n\nFig 1 plots:  what are the stars in the plots?  also this plot is a little too small, I needed to zoom in a lot to read it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dr4KBD4u55", "forum": "2eL6yXLCh8", "replyto": "2eL6yXLCh8", "signatures": ["ICLR.cc/2026/Conference/Submission9751/Reviewer_2xjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9751/Reviewer_2xjn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669884779, "cdate": 1761669884779, "tmdate": 1762921245758, "mdate": 1762921245758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HYDEN is a high-resolution monocular depth, point map, and surface normal estimation model that achieves state-of-the-art accuracy with substantially reduced inference cost. Its architecture combines a low-resolution Vision Transformer (ViT) branch for global context with a full-resolution CNN branch for fine-grained details, and employs a self-distillation framework to address the scarcity of high-quality ground-truth supervision. Integrated into leading models such as DepthAnything-v2 and MoGe2, HYDEN delivers top performance on high-resolution benchmarks while providing significantly lower inference latency compared to competing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First of all, the key idea is interesting. The hybrid dual-path design effectively combines global ViT features and local CNN features without incurring the quadratic cost typical of pure ViT models. The modular encoder design makes HYDEN adaptable across multiple downstream tasks and backbones such as DA2 or MoGe2.\n- Also, HYDEN achieves substantial speed-ups while preserving accuracy, making it practical for deployment in real-time or resource-constrained settings (e.g., robotic perception, self-driving planning etc.). This contrasts with prior methods that trade efficiency for high-resolution detail.\n- Lastly, the model’s performance on zero-shot benchmarks highlights the robustness of the self-distillation framework and the hybrid encoder’s scalability."}, "weaknesses": {"value": "- While the proposed framework is well designed and practically effective, its technical novelty is relatively limited. The core components such as the ViT–CNN fusion and pseudo-label-based self-distillation largely build upon existing approaches. The contribution primarily lies in the integration and scalability of existing techniques rather than introducing fundamentally new architectural or algorithmic innovations.\n- Although the component-wise ablation studies are well organized, the paper lacks sufficient detail on the design rationale and training loss formulations for each module, making full reproduction of the method challenging.\n- The effectiveness of the self-distillation pipeline relies heavily on the quality of the teacher model. However, the paper does not extensively discuss how weaker or biased teachers affect performance or convergence stability."}, "questions": {"value": "- How sensitive is the self-distillation performance to the teacher model’s quality or prediction bias? Have the authors experimented with weaker teachers or ensemble teachers?\n- Have the authors considered applying HYDEN to video sequences or multi-view inputs to assess temporal coherence in predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eZWU8vnm3u", "forum": "2eL6yXLCh8", "replyto": "2eL6yXLCh8", "signatures": ["ICLR.cc/2026/Conference/Submission9751/Reviewer_yCEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9751/Reviewer_yCEW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084080276, "cdate": 1762084080276, "tmdate": 1762921245328, "mdate": 1762921245328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hybrid dual-path vision encoder for high-resolution geometry prediction. In addition, the authors employ off-the-shelf models to generate low-resolution depth for the entire image and high-resolution depth for local patches, which serve as the ground truth to supervise the network training. Experiments demonstrate that the proposed method achieves state-of-the-art performance and efficiency on the high-resolution geometry estimation benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of hybrid dual-path architecture and self-distillation is simple and effective.\n\n2.  The proposed method achieves state-of-the-art performance and efficiency."}, "weaknesses": {"value": "1.  The novelty of the proposed hybrid dual-path encoder appears limited, as FlashDepth also adopts a similar methodology of fusing features from different resolution branches for high-resolution prediction. What is the ​fundamental difference​ in the encoder design between the proposed method and FlashDepth?\n\n2.  The novelty of the proposed self-distillation also appears limited. Using cropping for self-distillation is also a common practice in self-supervised learning. In the field of depth estimation, Distill Any Depth [1] similarly employs a local-global distillation strategy based on cropping. A clear discussion of the core distinctions between the proposed method and Distill Any Depth is required.\n\n3.  In the ablation study, the local crop loss is key to achieving fine-grained depth estimation. I'm curious to see what would happen if we only used the local crop loss and did away with the global loss altogether.\n\n[1] Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator, arXiv preprint arXiv: 2502.19204"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qP1fFzxojR", "forum": "2eL6yXLCh8", "replyto": "2eL6yXLCh8", "signatures": ["ICLR.cc/2026/Conference/Submission9751/Reviewer_nhZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9751/Reviewer_nhZ4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131056532, "cdate": 1762131056532, "tmdate": 1762921244739, "mdate": 1762921244739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}