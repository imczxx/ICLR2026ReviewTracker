{"id": "FWk3xTf6Il", "number": 8390, "cdate": 1758081146227, "mdate": 1759897788117, "content": {"title": "BOW: Reinforcement Learning for Bottlenecked Next-word Prediction", "abstract": "Large language models (LLMs) are typically pretrained with next-word prediction (NWP), which yields strong surface fluency but places limited pressure on models to form explicit reasoning before emitting tokens. \nWe study whether shifting the supervision signal can better elicit explicit reasoning and, more broadly, strengthen models’ general reasoning capability.\nWe present BOttlenecked next-Word exploration (BOW), a RL formulation of NWP that inserts an intermediate reasoning bottleneck. Instead of predicting the next word directly from context, the policy model must first generate a next-word reasoning trajectory. A frozen scorer then assigns this trajectory a soft, distributional reward equal to the probability of the gold next token conditioned solely on the trajectory to guide the RL optimization.\nWe also propose an optional L1-style regularizer on the reward to discourage “name-the-answer” shortcuts.\nAcross ten benchmarks, a brief BOW adaptation phase on Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct improves zero-shot reasoning and outperforms strong continual-pretraining baselines, including an RL variant with a hard, binary reward and a supervised finetuning approach with augmented data, by nearly 5\\% on average, while achieving the top result in 7 of 10 intrinsic NWP evaluations.\nThese results indicate that BOW is a viable alternative to vanilla NWP, inducing explicit next-word reasoning and strengthening general reasoning ability.", "tldr": "", "keywords": ["Language Modeling", "Large Language Models", "LLM Reasoning", "Next Word Prediction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33c1de5978f263d8a56645c46c42f0d9bd43809c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BOttlenecked next-Word exploration (BoW), a method to fine-tune Large Language Models (LLMs) in the Reinforcement Learning (RL) formulation.  BoW extends the standard next-word prediction (NWP) training pipeline with an intermediate reasoning bottleneck, and assigns the reward according to the likelihood of the gold next token conditioned the trajectory. This formulation naturally aligns with Reinforcement Learning (RL) and can be optimized using existing policy gradient methods such as GRPO. The authors further introduce an L1-style regularizer to avoid reward hacking. Experiments on Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct demonstrate the effectiveness of propose method on several benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written.\n2. The proposed method outperforms baselines on 7 out of 10 benchmarks\n3. The author does comprehensive analysis on the algorithms."}, "weaknesses": {"value": "1. **Unclear method name**: the method \"Bottlenecked next-World exploration (BOW)\" in terms of both \"bottleneck\" and \"next-world exploration\". \n    - As discussed is related work and Section 3.2, \"bottleneck\" is just a reasoning trajectory before the final prediction. Therefore, I don't see any benefit of renaming the \"reasoning trajectory\" into \"bottleneck\"\n   - From my experience, \"next-world prediction\" refers to the pretraining task of LLM, where the loss is applied to all the tokens in the sentence. In comparison, BOW only considers the probabilty of \"gold token\", which is a form of outcome reward.\n2. **Lack of Novelty**: based on the discussion above, BOW is a method of Reinforcement Learning with outcome reward, which has been studied on many existing works. Particularly, the hard reward (HR) formulation corresponds to standard RLVR, while the soft-reward has also been studied on papers such as [1]\n3. **Training is done on Instruction-tuned model**: The experiments are conducted on instruction-tuned model, which has been fine-tuned on some of the evaluation sets. This might also partially expains that why the improvement of BOW against Vallina is not substaintial.\n\n[1] VeriFree: Reinforcing General Reasoning without Verifiers. arXiv preprint arXiv:2505.21493"}, "questions": {"value": "1. What's the difference between BOW and RLVR (HR in Table 1) and this method [1]?\n2. Why in Table 1, the performance of HR degrades compared to Vallina?\n3. What would be the performance on BOW if trained on Base model? \n\n[1] VeriFree: Reinforcing General Reasoning without Verifiers. arXiv preprint arXiv:2505.21493"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j10IV2G538", "forum": "FWk3xTf6Il", "replyto": "FWk3xTf6Il", "signatures": ["ICLR.cc/2026/Conference/Submission8390/Reviewer_DG9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8390/Reviewer_DG9i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562472054, "cdate": 1761562472054, "tmdate": 1762920294436, "mdate": 1762920294436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reinforcement learning framework for training large language models (LLMs), called Bottlenecked Next Word Exploration (BOW). Unlike standard next-token (word) prediction, the model is prompted to generate a reasoning trajectory that analyzes the next-token prediction based on the previous tokens. A frozen scorer (also an LLM) evaluates the trajectory by computing the likelihood probability of the true next token, conditioned on both the context and the generated trajectory. The policy model (the LLM being trained) is optimized using Grouped Reward Policy Optimization (GRPO).\n\nThe authors conduct experiments on two LLMs, Qwen2.5-7B-I and LLaMA3.1-8B-I, with LLaMA3.1-8B-I also being the frozen scorer. The model is trained on narratives from the murder mystery domain, and the data are filtered to exclude context–next-word pairs where the next tokens do not require reasoning to infer. The proposed method is compared against the base LLM and several other training approaches, including selective language modeling, hard reward, and thoughts of words. The evaluation covers various general reasoning benchmarks as well as intrinsic next-word prediction tasks. Overall, the proposed method shows improvements over the original models on most tasks and outperforms other methods according to the reported results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper generally tackles an interesting and important question about the training signals in LLMs and contributes to the line of research that explores RL as an alternative. Among the work that uses RL to post-train LLMs for incentivizing reasoning ability, the paper has some originality in applying it to a token or word level and in using a soft reward based on the reasoning trajectory rather than the final answer.\n\n2. The paper is written with good quality and does a good job of clearly presenting what it does. The authors structure the paper well and provide detailed explanations of their design choices.\n\n3. The experimental setting is generally valid. The compared methods and evaluated datasets seem comprehensive, and they also provide detailed ablation studies to examine effects such as reward regularization, different scorers, and the effect of training data filtering."}, "weaknesses": {"value": "1. My main concern is the validity of the experimental comparisons. From Table 2, almost all baseline methods SLM, ToW, HR reduce the performance of the vanilla untrained LLMs on most datasets, especially for LLaMA3.1 8B I. I expect this happens because the authors rerun these methods by training models on the same, very limited murder mystery domain, which may cause the model to overfit and perform worse on other datasets. If the authors followed the original papers, the scores should not be that low. So even if the proposed method is better than other methods, the improvement could be due to less overfitting rather than enhanced reasoning ability, especially considering that the gains over the vanilla model are marginal, and in several cases, performance even drops a bit. This also leads me to question the validity of the training data. I understand the murder mystery data may demand reasoning, but there are other domains, such as math and coding. Why do the authors not train on a more diverse set of data? And for a fair comparison, why not keep the training closer to the baseline methods rather than adapting it to your data?\n\n2. Besides, compared with a more popular RL tuning pipeline such as in the GRPO paper, this work applies RL tuning at a more fine grained token level, and the reward computation is also different. It is not clear to me from the paper why these two aspects are important. In the experiments, not all tokens are used, only selected tokens, which may be trivial, and results could vary across domains. I would be interested to see, for example, beyond the murder mystery domain, whether for math problems doing RL at the token level instead of on the final answer is better. Similarly for the reward model. I do see from the baselines that there is a hard reward comparison, but it is still not clear to me why a soft reward is better, and why not let the policy model directly output the answer after the reasoning trajectories and use that soft or hard as the reward. There are several design choices, but I am not clear why the proposed one is the best.\n\n3. (Minor points.) From the abstract and introduction, the paper seems to frame the approach as completely shifting the supervision signal from next token prediction to an RL framework, but the experiments look more like a post training technique, since it needs LLMs trained with next token prediction as a good starting point for both the policy and the reward models. Stating this clearly could reduce confusion. Also, the paper does not discuss its limitations, and I recommend adding a section on that. Clearly stating when the method works and when it does not would bring more benefits to the community."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SnvOXXNexq", "forum": "FWk3xTf6Il", "replyto": "FWk3xTf6Il", "signatures": ["ICLR.cc/2026/Conference/Submission8390/Reviewer_LQAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8390/Reviewer_LQAb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931024633, "cdate": 1761931024633, "tmdate": 1762920293827, "mdate": 1762920293827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new reinforcement learning framework, **Bottle-necked Next-Word Exploration (BOW)**, as an alternative to standard next-word prediction (NWP) for large language models. Instead of directly predicting the next word, BOW forces the model to first generate an explicit **reasoning trajectory**, that evaluates how well the trajectory supports the correct next token. An **L1-style reward regularizer** is introduced to discourage shortcut behaviors like “naming the answer” and to encourage more general reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive Evaluation and Analysis** – The paper includes detailed ablations (scorer choice, regularization, data filtering) and human studies, demonstrating robustness of the method’s behavior.\n\n2. **Novel Soft Reward** – The soft, probabilistic reward may offer smoother and denser feedback than hard binary rewards, improving exploration efficiency and stability during training.\n\n3. **Empirical Performance Gains** – BOW outperforms strong baselines (RPT, ToW, SLM) across several reasoning benchmarks and majority of intrinsic NWP evaluations."}, "weaknesses": {"value": "1. **Lack of Cost Analysis** – This is a big concern. Generating reasoning trajectories for next token prediction is computationally expensive; the paper does not quantify training time, GPU cost, or sample efficiency relative to simpler continual-pretraining methods.\n\n2. **Narrow Experimental Domain** – Training data seem to come from *murder-mystery narratives*, a very specific genre. This raises the question of how the proposed model performs well on generic reasoning tasks, such as coding, mathematics, or dialogue. It would be useful to have a detailed description of training stages. For each stage, we need to compare the training cost and the training domain used side by side with the simple autoregressive baselines or SLMs for clarity."}, "questions": {"value": "1. How costly is the proposed approach compared to vanilla next token prediction baseline?\n2. How does the performance of the proposed approach generalize to out-of-domain reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tT1GqZ0haW", "forum": "FWk3xTf6Il", "replyto": "FWk3xTf6Il", "signatures": ["ICLR.cc/2026/Conference/Submission8390/Reviewer_viTt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8390/Reviewer_viTt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963476270, "cdate": 1761963476270, "tmdate": 1762920293403, "mdate": 1762920293403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reformulates the next-word prediction in LLMs as a reinforcement learning problem, instructing the policy model to output a reasoning chain that carries critical information about the gold next word information. Rewards are assigned from a separate frozen LLM scorer, which predicts the next word based on the given reasoning chain, outputting a soft probability score. The method, named bottlenecked next word exploration (BOW), achieves encouraging reasoning performance on several benchmarks and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of converting next-word prediction into a reinforcement learning issue is interesting, which also provides a way to scale RL up.\n* The paper is generally well written and easy to understand.\n* Experiments are based on several models and benchmarks, making the results more convincing."}, "weaknesses": {"value": "* It’s unclear how BOW scales as training data increases: the experiments are only based on one data setup.\n* The hard reward baseline performs much worse than the vanilla baseline, which may be misleading."}, "questions": {"value": "In Dong et al. (2025)’s work, the hard reward also delivers encouraging performance, but in the experiments, this method achieves much worse performance even than the vanilla baseline. Why would this happen? Is this caused by sub-optimal optimization?\n\nAnother way to understand the effectiveness of hard reward is to convert the soft score in BOW to binary, such as if the score is larger than some threshold (like 0.2), the model gets a positive reward of 1, otherwise 0. This would better illustrate how hard and soft reward works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bBi0SV7s1U", "forum": "FWk3xTf6Il", "replyto": "FWk3xTf6Il", "signatures": ["ICLR.cc/2026/Conference/Submission8390/Reviewer_q7pv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8390/Reviewer_q7pv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043094358, "cdate": 1762043094358, "tmdate": 1762920292984, "mdate": 1762920292984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}