{"id": "uSIjOhmKtX", "number": 3442, "cdate": 1757427473094, "mdate": 1759898089690, "content": {"title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "abstract": "We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.", "tldr": "A Generation model can be a representation-agnostic 3D compressor by bridging latent spaces", "keywords": ["3D", "Generation", "Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa5705431617cdde9c764095509fdc48d4e24aaf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is about a 3D compression method leveraging arbitrary combinations of existing 3D encoder models and generation models. Two mapping networks are trained jointly: a forward mapping from encoded latent to the compressed form, and a reverse mapping from the compressed form to the latent space that can be decoded by the generator. Experiments are presented with various encoder-generator combinations, showing very high compression rates at good quality preserverance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a valuable approach to a relevant problem. In particular the flexibility of combining various encoder and generation models is a good feature. The results are impressive considering the high compression ratio."}, "weaknesses": {"value": "1) Using the word \"generator\" is a bit misleading in my point of view, since it seems rather the \"decoder\" that is used. A \"generator\" usually does not take a latent vector as input (a generator generates a 3D object based on images or text or unconditionally). Based on the explanations of the paper, it seems that a decoder model of the generator is used, which takes a generated latent vector and decodes it into a 3D object. For example, LION is using latent diffusion, and consists of a diffusion model and a decoder that transforms the latent vector back to a point cloud. I assume, only the decoder is used and not the diffusion model? This should be clarified.\n2) Related to 1, a suitable baseline would be just using the encoder and decoder of a generative method, e.g. LION. Almost all 3D generation methods (except for SDS-loss based methods) use latent diffusion and therefore inheretly have an encoder and decoder model. None of them is considered as a baseline.\n3) Some related works are not mentioned, e.g. \"ROAD: Learning an Implicit Recursive Octree Auto-Decoder to Efﬁciently Encode 3D Shapes\".\n4) There is always a trade-off between quality and compression ratio, as can be seen in the tables where sometimes other methods outperform Squeeze3D in terms of quality. Would it be possible to show a Pareto frontier that shows this trade-off for different methods? This would give more insights into how different methods preserve quality at various compression ratios."}, "questions": {"value": "1) Does the method really use generative models (e.g. diffusion models) or just decoders?\n2) Is it possible to tune the compression ratio of Squeeze3D for the best quality-CR balance by varying the size of z_comp?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JqJZr8mQrP", "forum": "uSIjOhmKtX", "replyto": "uSIjOhmKtX", "signatures": ["ICLR.cc/2026/Conference/Submission3442/Reviewer_Gi28"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3442/Reviewer_Gi28"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594131403, "cdate": 1761594131403, "tmdate": 1762916727177, "mdate": 1762916727177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Squeeze3D**, a compression framework that leverages *pre-trained 3D generative models* as implicit priors to achieve **extreme compression of 3D data** (meshes, point clouds, radiance fields). Instead of training a dedicated encoder–decoder architecture for each 3D format, Squeeze3D introduces two lightweight mapping networks to bridge the latent space of an existing 3D encoder and that of a generative model."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Novel idea of using pre-trained 3D generative models as *neural compressors*.  \n- Ablation on Gram loss demonstrates meaningful design motivation for avoiding degenerate latent collapse.\n- Flexibility across 3D formats (meshes, point clouds, RFs) enhances practical applicability."}, "weaknesses": {"value": "**Visual Quality Concerns**\n- The qualitative 3D reconstruction quality is **visibly weak**.  \n  - In Fig. 4, even the GT renderings of meshes appear low-quality, making it difficult to assess compression performance reliably. More examples using clean, high-fidelity GT meshes are necessary.  \n  - In Fig. 6, reconstructed NeRFs show inferior results compared to VQRF and SparsePCGC, especially regarding specular highlights—Squeeze3D fails to retain reflections that other methods preserve.\n\n**Incomplete Evaluation Metrics**\n- Compression is fundamentally a trade-off between **storage and computation**. The paper only compares latency (ms) but omits **compute cost metrics**, especially FLOPs or GPU memory usage during compression/decompression, these resource requirement are as important as time cosumption.\n\n**Writing and Presentation Issues**\n- Numerous writing issues lower the perceived quality of the paper. Examples include:  \n  - Inconsistent figure caption formatting [Figure 2: Overview of **our** Method], [Figure 3: Training Squeeze3D.], [Figure 4: Qualitative mesh compression results.].  \n  - Numeric precision is inconsistent across tables (e.g., CR values with arbitrary decimal lengths such as *11.28* vs. *2187.0748*).  \n  - Typos remain, such as L358: *“splits used fro”*."}, "questions": {"value": "1. **Quality of GT Meshes**: Can you provide high-quality GT mesh renderings for comparison to allow fair evaluation of geometric fidelity?  \n2. **Compute Efficiency**: Can you report FLOPs or GPU energy cost of compression/decompression to better reflect computation–storage trade-offs?  \n3. **Generator Limitation**: Since reconstruction quality is capped by generator ability, how does the method generalize with stronger generators (e.g., recently released high-fidelity 3D diffusion models)?  \n4. **Failure Cases**: Can you include explicit failure cases and analysis, particularly where the mapping network fails to reconstruct geometry outside the generator’s prior distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DlgOprhtI5", "forum": "uSIjOhmKtX", "replyto": "uSIjOhmKtX", "signatures": ["ICLR.cc/2026/Conference/Submission3442/Reviewer_QYUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3442/Reviewer_QYUA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737142388, "cdate": 1761737142388, "tmdate": 1762916726139, "mdate": 1762916726139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces  a novel framewor to compress 3D data by leveragin pre-trained 3D VAE. Squeeze3D utilizes trainable mapping networks to bridge the latent spaces between a pre-trained 3D encoder and a pre-trained 3D generator."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is capable of compressing various 3D representations, including meshes, point clouds, and radiance fields. And the reported compression ratios appear to be high."}, "weaknesses": {"value": "1. Motivation: The primary weakness is the lack of clear motivation for the compression goal. The paper is centered on using a generative model as a compressor, but it fails to convincingly articulate the downstream applications or the practical necessity of compressing the outputs of these generative models. The significance of this compression technique needs to be better justified.\n\n2. Incomplete Experimental Validation: The experiments primarily focus on comparing the reconstruction quality of Squeeze3D against previous compression methods. However, the evaluation does not investigate the performance impact of the compression method compared with the original generation model. It is essential to demonstrate that the compressed representations can achieve minimal performance loss compared to the original, uncompressed latent codes.\n\n3. Efficiency and Generality: The current method requires separate, dedicated training of the mapping networks for every different pre-trained generative model. This significantly undermines the efficiency and general applicability of the method, as it does not offer a single, unified compression model.\n\n4. Technical Novelty: The technical contribution is limited, primarily involving the introduction of two mapping networks designed to connect the latent spaces. The novelty and complexity of this architectural contribution should be further highlighted and contrasted against existing latent space manipulation techniques."}, "questions": {"value": "1. Can the compressed latent space derived from Squeeze3D provide any positive benefit (beyond just storage) to the generative model itself? For instance, does the compression process help in regularization, improving the robustness of the latent space, or aiding in subsequent training/fine-tuning tasks?\n\n2. For such a high-level compression ratio, what is the quantitative degradation in reconstruction quality (using metrics like LPIPS or FID) when comparing the compressed latent representation to the original uncompressed latent representation? A more detailed trade-off analysis between compression rate and quality is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BSVik13Wag", "forum": "uSIjOhmKtX", "replyto": "uSIjOhmKtX", "signatures": ["ICLR.cc/2026/Conference/Submission3442/Reviewer_W6ps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3442/Reviewer_W6ps"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823279821, "cdate": 1761823279821, "tmdate": 1762916725708, "mdate": 1762916725708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework to use existing pre-trained models to produce highly compressed 3D data. The paper claims to have 3 contributions. \n1. The first framework to use pre-trained generative models for compression\n2. Showcases the ability to establish correspondences in the latent space between neural architectures that vary widely in structure, optimization goals, and data distributions.\n3. Highlight the flexibility of the framework to work with different encoders, 3D generators, and different 3D representations.\n\nThe proposed framework utilizes a pair of pre-trained models, an encoder and a 3D generator, to provide the training data for the framework.  The synthetic data is then used to train a pair of Forward and Reverse Mapping networks between the different latent spaces, with an intermediate compressed representation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides great Qualitative and Quantitative results, which provide a significant increase in compression ratio and similar 3D views compared to the ground truth.\n2. Introduction of gram loss to prevent dimension collapse\n3. Flexibility of architecture to use different neural architectures and 3D representations"}, "weaknesses": {"value": "1. Very little/no experimentation with non-synthetic data.\n2. Minimal set of Out of Distribution compression examples.\n3. Current experiments are biased towards the 3D generator distribution. The framework seems to be learning to compress the generator's distribution and may not generalize well."}, "questions": {"value": "1. Does this framework allow choosing between different compression ratios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fhtl7AukrM", "forum": "uSIjOhmKtX", "replyto": "uSIjOhmKtX", "signatures": ["ICLR.cc/2026/Conference/Submission3442/Reviewer_mx4C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3442/Reviewer_mx4C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971346243, "cdate": 1761971346243, "tmdate": 1762916725147, "mdate": 1762916725147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}