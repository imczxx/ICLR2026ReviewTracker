{"id": "2WQAEQLq3E", "number": 18624, "cdate": 1758289556006, "mdate": 1759897091088, "content": {"title": "Scalable Evaluation of Language Models with Generated Games", "abstract": "We present gg-bench, a collection of generated game environments designed to evaluate the reasoning capabilities of language models. gg-bench is synthetically generated by (1) using an LLM to write game descriptions in natural language, (2) using the same LLM to implement each game in code, and (3) training RL agents via self-play on the generated games. We evaluate models based on their winrate against these RL agents by prompting them with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: general-purpose LLMs (GPT-4o, Claude 3.7 Sonnet) achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models (o1, o3-mini, DeepSeek-R1) achieve average winrates of 31-36%. Additionally, because gg-bench is a data generating process rather than a static benchmark, new evaluation instances can be created at will. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.", "tldr": "We present a pipeline for generating games that can be used to evaluate LLMs", "keywords": ["language models", "benchmarks", "reasoning", "environments", "games"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50c880d48e0af3d59ed4af0b1f4080b1850b68e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new benchmark designed to test the reasoning and generalization abilities of large language models (LLMs) using synthetically generated two-player games. Instead of relying on static datasets, gg-bench is a data-generating process: an LLM creates novel game descriptions, writes corresponding Gym environments in Python, and self-trains RL agents to act as opponents. Models under evaluation then play against these agents, with win rate as the performance metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clear, well-organized, and readable, making a technically complex topic accessible to a broad research audience. It effectively addresses an important challenge in AI evaluation: how to measure generalization and reasoning in language models beyond static benchmarks."}, "weaknesses": {"value": "- The evaluation framework depends on RL-trained agents as the sole measure of difficulty. \n\n- The authors could have conducted human evaluations on a small, representative subset of games. Such human-in-the-loop assessment would help contextualize RL agent performance, reveal potentially flawed game designs, and provide a richer reference point for what constitutes robust reasoning and generalization.​\n\n- While the paper reports mean win rates for both RL agents and LLMs, it would be helpful if the authors explicitly compared RL agent performance to theoretical or empirical maximum scores for each game, normalizing achievements and clarifying the difficulty landscape. As for evaluation details, the paper states that each language model plays 30 games against the RL agent for every game in the benchmark (i.e., 30 trials per game), with the main result reported as the average win rate across the 126 games. However, there is no indication that scores are explicitly normalized relative to RL maxima, and RL agent scores are not directly shown in comparison to maximum possible scores.\n\n- The paper does not clearly indicate on which specific games or categories language models perform better or worse. This makes it difficult to analyze the types of strategies or reasoning that differentiate successful LLMs, and may obscure systematic model strengths or weaknesses across different gameplay mechanics.\n\n- The current evaluation restricts LLMs to picking moves turn-by-turn. An alternative approach, having LLMs generate complete code for game-playing policies or strategies, could explore their capacity for high-level planning, algorithm synthesis, or generalization at the program level. This could provide deeper insights into their actual reasoning abilities and offer a richer evaluation setting."}, "questions": {"value": "Which types of games or strategies do current LLMs perform best on, and what does this suggest about the strengths and weaknesses of modern language models?​\n\nHow might the evaluation change if LLMs were asked to synthesize complete agents or policy code, rather than only making step-by-step move selections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DlC2ZfnnNV", "forum": "2WQAEQLq3E", "replyto": "2WQAEQLq3E", "signatures": ["ICLR.cc/2026/Conference/Submission18624/Reviewer_eDCL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18624/Reviewer_eDCL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414569274, "cdate": 1761414569274, "tmdate": 1762928337745, "mdate": 1762928337745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces gg-bench, a scalable benchmark of LLM-generated, two-player, turn-based games intended to evaluate LLM reasoning capabilities via gameplay against RL agents. The core problem addressed is that fixed, human-curated test sets saturate and risk contamination, making it hard to assess true out-of-distribution generalization. gg-bench generates games by first generating a game description/rulebook, then generating the code for the game as a Gym environment, and finally a PPO-based self-play agent is trained as the baseline to beat. The benchmark includes 126 filtered games (from an initial 1000). Evaluations show non-reasoning models perform significantly worse (~9% win rate over RL agent) compared to reasoning models (~30%)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark is interesting, and the generation pipeline is useful for evaluating LLMs.\n- Scalable engineering effort is a plus.\n- Can provide meaningful empirical signals, with proper in depth analysis.\n-"}, "weaknesses": {"value": "- Low novelty \n- No concrete analysis of reasoning, which is the main position of the paper. There is insufficient ablations, investigation, analysis of model performance on the benchmark in relation to reasoning capabilities, which is the main axis that the benchmark evaluates. It is unclear if these games need reasoning, or if solutions are often illogical or unreasonable. \n- LLMs are zero-shotted on the environment and evaluated against a trained model. While this isn't inherently bad, the empirical evaluation misses ablating to what degree LLMs can perform well. It lacks depth of study. \n    - For example, ablations on multiple attempts might be valuable. The LLM could play multiple rounds, and learn from each round. This may also extend to testing how it reasons over past performance, etc.\n- Information assymetry between RL agent, which also uses MCTS at inference time. This blends with the above point.\n- Arguably, a human baseline is missing.\n- Inference compute cost limits public usage, with high API costs needed to evaluate models and provide analysis."}, "questions": {"value": "I only ask that the authors review my weaknesses above and either clarify any issues or provide content to fill in identified gaps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6x2oKkLIS3", "forum": "2WQAEQLq3E", "replyto": "2WQAEQLq3E", "signatures": ["ICLR.cc/2026/Conference/Submission18624/Reviewer_qibi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18624/Reviewer_qibi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714513852, "cdate": 1761714513852, "tmdate": 1762928337281, "mdate": 1762928337281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces gg-bench, a novel benchmark designed to evaluate Large Language Models (LLMs) as reinforcement learning (RL) agents in diverse two-player games. The core idea is to leverage LLMs themselves to generate game descriptions and their corresponding Gym environment implementations, creating a scalable pipeline for task creation. To establish evaluation baselines, the authors train RL agents (using PPO with self-play) for each generated game. A crucial filtering step retains only those games where a trained RL agent can be clearly outperformed by a stronger version of itself (win rate >80%), ensuring the existence of a meaningful performance gap. The final benchmark consists of 126 such games. LLMs are then evaluated by their win rates against the \"weaker\" RL agent in each game. The results show that reasoning-capable models (e.g., OpenAI o1) significantly outperform standard ones, highlighting the importance of explicit reasoning for complex planning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The use of RL upper-bound checks (PPO vs. PPO) for filtering is a major strength. It provides an objective criterion to ensure that each game in the benchmark is non-trivial yet solvable, and has a clear performance target, which enhances the reliability of the evaluation.\n\n2) Evaluating LLMs against trained RL agents, rather than random or rule-based bots, creates a more challenging and informative testbed for strategic reasoning and planning abilities.\n\n3) The significant performance gap between reasoning models (GPT-o1) and standard models provides strong empirical evidence for the value of chain-of-thought reasoning in complex decision-making tasks."}, "weaknesses": {"value": "1) While the benchmark is theoretically scalable due to automated game generation, the practical scalability is limited. The need for manual intervention in feature engineering, state representation design, hyperparameter tuning, and debugging for each unique game makes the process of adding new tasks labor-intensive. The claim of \"scalability\" may therefore be more aspirational than fully realized in current practice.\n\n2) The filtering mechanism favors games that are amenable to PPO-style deep RL. Games with sparse rewards, long horizons, or requiring symbolic reasoning might be filtered out even if they are excellent tests for LLMs, leading to a potential bias towards \"RL-friendly\" tasks.\n\n3) The paper does not provide sufficient details on how the neural network architectures for the PPO agents are standardized or adapted across the vastly different game environments. This lack of transparency makes it difficult to assess the true level of automation and generalizability of the training process."}, "questions": {"value": "1) Were there common failure modes during the PPO training phase? For instance, did many games fail the filtering step due to non-convergence, instability, or all agents performing at chance level?\n\n2) The paper selects the \"weaker\" RL agent as the opponent. Have you experimented with evaluating LLMs against the stronger RL agent? If so, what were the win rates, and did they suffer from a \"ceiling effect\" as hypothesized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zzAbHdjmKo", "forum": "2WQAEQLq3E", "replyto": "2WQAEQLq3E", "signatures": ["ICLR.cc/2026/Conference/Submission18624/Reviewer_b7uH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18624/Reviewer_b7uH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818012884, "cdate": 1761818012884, "tmdate": 1762928336673, "mdate": 1762928336673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces gg-bench, a new scalable benchmark for evaluating the reasoning capabilities of LLMs. The core problem it addresses is the saturation and potential contamination of static benchmarks. Instead of a fixed set of tasks, gg-bench is a data generating process that uses an LLM to create new, unique two-player strategy games.\n\nThe process consists of three main stages:\n- Game Generation: An LLM (here o1) is prompted to create natural language descriptions (rules, objectives, mechanics) for new games.\n- Implementation: The same LLM is then prompted to implement these games as Python Gym environments.\n- Agent Training: Reinforcement learning (RL) agents are trained via self-play (using PPO) on these generated games to create competent opponents.\n\nLLMs are then evaluated by having them play against these trained RL agents. The model is given the game description, the current board state, and a list of valid moves, and must choose the best action. The primary metric is the winrate against the RL agent.\nThe key findings are that gg-bench is challenging and effectively differentiates between model classes. The paper provides a clear analysis of the generated games' diversity, code quality, and the strategic failures of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This benchmark is challenging and could serve as a testbed for very strong models, which is important nowadays since many evaluations get quickly saturated.\n- The framework is inherently scalable. As language models become more capable, they can be used to generate more complex and difficult games, ensuring the benchmark's longevity and continued relevance. \n- The paper is very well-written and easy to follow. The figures are clear and highly effective. Figure 1 provides a good overview of the pipeline, and Figure 4's breakdown of a failed game trajectory is helpful."}, "weaknesses": {"value": "- The authors are transparent about the high cost of both generating the dataset ($1162 with o1) and running the evaluations ($2547 for o1). This presents a significant barrier to reproducibility and adoption for academic labs or independent researchers, potentially limiting the benchmark's widespread use.\n- The paper notes that some implementations contained hard-coded details (for example a list of prime factors). This raises the question of how often the generated code might deviate from the natural language rules in ways that could be exploited by an RL agent but would be opaque to an LLM reading the rules. This could affect the fairness of the evaluation."}, "questions": {"value": "- Please correct the typo in the title \"Lanugage\"\n- Regarding the scalability, how would you scale 10x your benchmark? What would you target?\n- Have you experimented with different RL algorithms or training regimes for the opponent agents? How can you be sure that the observed winrates reflect the LLMs' general strategic ability rather than their ability to exploit patterns specific to PPO agents?\n- Do you have statistics on why games were filtered out? For instance, what percentage failed due to timeouts versus execution errors? This could illuminate the types of complex games that the current framework is systematically excluding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WK6hZ5Kx04", "forum": "2WQAEQLq3E", "replyto": "2WQAEQLq3E", "signatures": ["ICLR.cc/2026/Conference/Submission18624/Reviewer_Jcg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18624/Reviewer_Jcg8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851585017, "cdate": 1761851585017, "tmdate": 1762928336039, "mdate": 1762928336039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents gg-bench,  which is a collection of generated game environments via LLMs. To be more specific, each game environment in gg-bench is generated by the following two steps. First, using an LLM to write game descriptions in natural language. Second, using the same LLM to implement each game in code. Afterwards, the reasoning capabilities of LLMs can be evaluated by playing them against RL agents that are trained via self-play on the generated games. Some experimental results are presented with regard to the winrates of several LLMs against the RL agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- using LLMs to generate game benchmarks is interesting.\n\n- gg-bench is challenging to existing SOTA LLMs, as what has been shown in the experiments.\n\n- the paper is very easy to follow."}, "weaknesses": {"value": "- The technical contribution of the paper is to some extent insignificant. It is more of an interesting application of LLMs than a research paper. I would expect that some research questions have been answered, or some new knowledge has been generated. \n\n- Since the game environments are generated using LLMs, it seems difficult to compare results across different papers, since different papers may use different LLMs. Even based on the same LLMs, the inherent randomness of LLM generation process may incur large variance to the evaluation process. \n\n- gg-bench is only limited to two player zero-sum games."}, "questions": {"value": "- Why gg-bench is so interesting or important that future LLMs should be tested on? What benefits/contributions does gg-bench bring to the exisitng community on the evaluation of LLMs reasoning abilities.\n\n- How to ensure that results on gg-bench across different papers are comparable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iiq1Z8KCEB", "forum": "2WQAEQLq3E", "replyto": "2WQAEQLq3E", "signatures": ["ICLR.cc/2026/Conference/Submission18624/Reviewer_y8Sy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18624/Reviewer_y8Sy"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896578305, "cdate": 1761896578305, "tmdate": 1762928335605, "mdate": 1762928335605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}