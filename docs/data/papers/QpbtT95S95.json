{"id": "QpbtT95S95", "number": 22492, "cdate": 1758331819680, "mdate": 1759896862982, "content": {"title": "Compute-Optimal Quantization-Aware Training", "abstract": "Quantization-aware training (QAT) is a leading technique for improving the accuracy of quantized neural networks. Previous work has shown that decomposing training into a full-precision (FP) phase followed by a QAT phase yields superior accuracy compared to QAT alone. However, the optimal allocation of compute between the FP and QAT phases remains unclear. We conduct extensive experiments with various compute budgets, QAT bit widths, and model sizes from 86.0M to 2.2B to investigate how different QAT durations impact final performance. We demonstrate that, contrary to previous findings, the loss-optimal ratio of QAT to FP training increases with the total amount of compute. Moreover, the optimal fraction can be accurately predicted for a wide range of model sizes and quantization widths using the tokens-per-parameter-byte statistic. From experimental data, we derive a loss scaling law that predicts both optimal QAT ratios and final model performance across different QAT/FP compute allocation strategies and QAT bit widths. We use the scaling law to make further predictions, which we verify experimentally, including which QAT bit width is optimal under a given memory constraint and how QAT accuracy with different bit widths compares to full-precision model accuracy. Additionally, we propose a novel cooldown and QAT fusion approach that performs learning rate decay jointly with quantization-aware training, eliminating redundant full-precision model updates and achieving significant compute savings. These findings provide practical insights into efficient QAT planning and enable the training of higher-quality quantized models with the same compute budget.", "tldr": "The optimal fraction of quantization-aware training compute (vs. pretrain stage) increases with total compute budget. We derive scaling laws to predict optimal allocation and model loss, enabling higher-quality model training with the same compute.", "keywords": ["quantization-aware training", "QAT", "neural network quantization", "compute optimization", "scaling laws", "large language models", "LLMs", "model compression", "compute budget allocation", "training efficiency", "model optimization", "quantized neural networks", "efficient deep learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f88d31a288f75f8bb16b93ac756a00fdab8255e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends scaling-law analysis to the two-stage full-precision & QAT pipeline, asking how to split a fixed training budget between full-precision pretraining and quantisation-aware training. Through hundreds of runs across model sizes (86M–2.2B) and bit-widths, the authors show that the loss-optimal QAT fraction increases with scale, and that this dependency is well captured by a simple _tokens-per-parameter-byte_ statistic. They fit a unified loss scaling law (over ($N$, $D_{fp}$, $D_{qat}$, $B$) that includes pure-QAT and FP/QAT interaction terms, and use it to predict optimal QAT ratios as well as when low-bit QAT can match FP loss. The study also proposes a practical “cooldown & QAT fusion” training schedule (merging LR decay with QAT) and reports small but consistent savings in “wasted tokens” at moderate/high bit-widths. Overall, the paper moves the literature from “QAT should be ~10%” heuristics to compute-aware prescriptions grounded in scaling behaviour, while relying on a strong, modern QAT baseline (ParetoQ)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "All in all, this is a solid experimental paper that answers fundamental questions in LLM deployment with a sound methodology. I enjoyed how the authors used their loss scaling curves to answer interesting questions in their discussion. For example, the question of what the optimal QAT bitwidth per model and training budget is has been a topic in the community for years. The main strengths are:\n\n* The paper addresses a very significant and unanswered question in LLM inference and of increasing importance for LLM deployment. What budget should be allocated to QAT vs FP training? No previous work has answered this important question in experimentally sound ways, and this work paves the way for significant future work.\n* The loss scaling laws are solid, based on hundreds of experiments, and provide strong transparency on the fit of the curve. The introduction of the token-per-byte statistics is also a very useful metric capturing the idiosyncrasies of QAT. The vast corpus of QAT experiments makes for a solid experimental methodology.\n* A very extensive appendix, including many results, experimental set-ups, and curves."}, "weaknesses": {"value": "* The paper is somewhat too long given the level of novelty and contribution, and as a result, the central narrative becomes hard to follow. While each section presents interesting findings, the overall flow sometimes feels like a set of independent conclusions about QAT rather than a single, coherent story. I found myself re-reading parts to connect the results to the main scaling-law argument. \n\n* Section 7 reads as an addendum: it explores LR-scheduler fusion, which is interesting but tangential to the main scaling-law contribution, and the reported gains are relatively small and bit-width dependent. I would think this should be part of another paper or appendix.\n\n* The full-precision loss regularisation used to fit the unified law is crucial to interpreting Fig. 5 and the FP-vs-QAT comparisons, but it’s only described in Appendix E. Similarly, the “wasted token count” metric is under-explained and would benefit from a formal definition and more apparent motivation."}, "questions": {"value": "* Figure 1 (right), add $D_{fq}$ and $D_{qat}$ along the axes and include a loss heatmap in the legend\n* Equation 2: more discussion and motivation for the forms chosen related to QAT.. What is pure QAT penalty, and why does it take form? * * Why does FP/QAT interaction take this form? Are these grounded in theory or chosen for fitting reasons? Did you do any ablation studies on the form?\n* Appendix D. Figure 9: I did not see any discussion or explanation on why the theoretical and experimental optima diverge so much at the lowest $D_{total}$ iso curves for 2,4, and 6 bits for the smallest model of 86M.\n* Line 277: wasted token count: I had to read the definition multiple times to understand it. Could you add an equation? \n* Line 322: iso-flop. Capitalise flop because it’s hard to understand it when it’s in lower case.\n* Figure 5: Make the colour of the bitwidths 5 and 1 more distinct. It’s very hard to tell which curve corresponds to each bitiwidthth\n* Section 7: wasted unfused total tokens: could you maybe add an equation? I don’t fully understand how you reach that number."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSBOjyC7RY", "forum": "QpbtT95S95", "replyto": "QpbtT95S95", "signatures": ["ICLR.cc/2026/Conference/Submission22492/Reviewer_1gxP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22492/Reviewer_1gxP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674853423, "cdate": 1761674853423, "tmdate": 1762942239781, "mdate": 1762942239781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the optimal resource allocation ratio between Full-Precision (FP) and Quantization-Aware Training (QAT) phases. Through extensive LLM experiments, the authors found that the optimal QAT fraction increases with the total compute budget and introduced tokens-per-parameter-byte as a key metric to accurately predict this optimum. Loss Scaling Law is proposed to predict the performance of QTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper establishes a novel loss scaling law capable of predicting both the optimal QAT fraction and final model performance across varying bit widths and allocation strategies, demonstrating high practical utility. The proposed method integrates learning rate decay with QAT, effectively eliminating redundant model updates and improving both computational efficiency and accuracy."}, "weaknesses": {"value": "Experiments are conducted on a specific LLM architecture（llama2）, and the scaling law parameters may require refitting for other model types. Different types of models may exhibit varying degrees of tolerance to quantization, particularly those with inherent redundancy."}, "questions": {"value": "How sensitive are the fitted parameters to the range of model sizes and token counts used in the experiments? Can the authors provide an uncertainty analysis? While fine-tuning the FP model, how would the optimal QAT fraction for the final QAT phase shift? There might be a distribution difference between pre-training phase and fine-tuning phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TJTlQ1ad2l", "forum": "QpbtT95S95", "replyto": "QpbtT95S95", "signatures": ["ICLR.cc/2026/Conference/Submission22492/Reviewer_w7Wt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22492/Reviewer_w7Wt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829879730, "cdate": 1761829879730, "tmdate": 1762942239515, "mdate": 1762942239515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is focused on QAT fine tuning which is done after full-precision (FP) phase pre-training.\nQAT bit widths, and model sizes from 86.0M to2.2B to investigate how different QAT durations impact final performance. \nThey show that contrary to previous findings, the loss-optimal ratio of QAT to FP training increases with the total amount of compute.\nThis work derive a loss scaling law that predicts both optimal QAT ratios and final\nmodel performance across different QAT/FP compute allocation strategies and\nQAT bit widths.  Additionally, they propose a novel cooldown and QAT fusion approach that performs learning rate decay jointly with quantization aware training, eliminating redundant full-precision model updates and achieving significant compute savings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Authors run multiple experiments for fitting generating scaling law into proposed function and demonstrated small prediction error. They showed that the optimal QAT fraction is not a fixed percentage but rather increases with the total compute budget, specifically with the tokens-per-parameterbyte statistic. This challenges the previous conclusion that 10% is universally optimal for QAT length relative to total training length.\n\nThey proposed a loss scaling law that captures the optimal QAT fraction phenomenon and\nmodels the final expected loss of the FP and QAT.\n\nProposed a novel approach: cooldown & QAT fusion—a scheme where learning rate decay is performed jointly with quantization-aware training, eliminating redundant fullprecision updates and achieving better accuracy for the same token count."}, "weaknesses": {"value": "This work is focused on a specific LLM architecture and data sets, and exact results may differ for different model types different QAT methods and data.\nOnly int quantization explored, it would be interesting to see fp4 and fp8 results.\nLimited size LLM are explored."}, "questions": {"value": "Do you reset or keep optimizer state, from pre-trained model, when apply QAT fine tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4jjmkGxOjl", "forum": "QpbtT95S95", "replyto": "QpbtT95S95", "signatures": ["ICLR.cc/2026/Conference/Submission22492/Reviewer_cjQW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22492/Reviewer_cjQW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977251004, "cdate": 1761977251004, "tmdate": 1762942239305, "mdate": 1762942239305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors talk about the scaling law for quantization aware training as well as the impact of the fraction for quantization aware training compared to the full-precision pretraining. The topic seems to be interesting and the authors provide some experiments to demonstrate the results. However, the paper is very badly written and results presentation is very bad (e.g., the figures are very difficult to read and the conclusions made from the figures are difficult to follow or should be arguable)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The topic and problem proposed seems to be interesting, and the final conclusion or results, if correct as the author claimed, might be useful. Especially, the conclusion about how to determine the optimal fraction for quantization aware training, and also the possibility to eliminate redundant full-precision training via novel learning rate scheduling, might be useful.\n- Regarding the techinical contribution, the authors first study the optimal fraction allocated for QAT and its relationship with the tokens-per-parameter-byte. Afterwards, they propose the loss scaling law for optimal QAT fraction and model the final expected loss of the FP and QAT pipeline. They finally propose a novel approach of cooldown and QAT fusion, where learning rate cooldown in FP training and learning rate warmup in QAT are fused, and thus redundant full-precision updates can be eliminated."}, "weaknesses": {"value": "- The writing and results presentation are not good. Figure1 is difficult to read.\n- Improvement in Table 2 might not be strong enough."}, "questions": {"value": "In Figure 1 left, the y-axis is QAT tokens-per-byte, and x-axis is Total tokens-per-byte. The author would like to demonstrate that the optimal QAT fraction increases with the full training tokens-per-parameter-byte (line 092 in the original paper). To demonstrate this, the y-axis should be the ratio between QAT tokens-per-byte to the total tokens-per-byte, instead of only list the two end ratios in the figure. Could the author provide the plot with ratio as the y-axis to prove the conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nnwj5ZvNZJ", "forum": "QpbtT95S95", "replyto": "QpbtT95S95", "signatures": ["ICLR.cc/2026/Conference/Submission22492/Reviewer_Hsq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22492/Reviewer_Hsq3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997959511, "cdate": 1761997959511, "tmdate": 1762942239063, "mdate": 1762942239063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}