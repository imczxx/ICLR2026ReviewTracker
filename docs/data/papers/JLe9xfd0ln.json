{"id": "JLe9xfd0ln", "number": 9777, "cdate": 1758139823265, "mdate": 1763668922506, "content": {"title": "Expert Merging in Sparse Mixture of Experts with Nash Bargaining", "abstract": "Existing expert merging strategies for Sparse Mixture of Experts (SMoE) typically rely on input-dependent or input-independent averaging of expert parameters, but often lack a principled weighting mechanism. In this work, we reinterpret expert merging through the lens of game theory, revealing cooperative and competitive dynamics among experts. Based on this perspective, we introduce Nash Merging of Experts (NAMEx), a novel framework that incorporates Nash Bargaining into the merging process, enabling more balanced and efficient collaboration among experts. Additionally, we incorporate complex momentum into NAMEx to accelerate expert propagation with theoretical guarantees for convergence. Extensive experiments across language modeling, text classification, image classification, and zero-shot robustness under data corruption show that NAMEx consistently outperforms competing methods while integrating seamlessly with popular MoE architectures. Finally, we demonstrate NAMEx’s scalability by applying it to large-scale systems, including Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both zero-shot and fine-tuning settings.", "tldr": "NAMEx uses Nash bargaining and complex momentum to merge experts more fairly and efficiently, outperforming prior methods across tasks.", "keywords": ["Mixture of Experts", "Game Theory"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffaa5420bb6a8e56dd8831cd9925da6d6754a974.pdf", "supplementary_material": "/attachment/c95f1f5573ac667450f5225a1be10b7214f306bd.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript introduces Nash Merging of Experts (NAMEx), a novel, theoretically-grounded framework for compressing Sparse Mixture of Experts (SMoE) models. The core innovation is the reinterpretation of the merging process as a cooperative game, utilizing Nash Bargaining to determine the optimal, balanced parameter weighting for merging two or more experts. The authors claim this approach, coupled with a mechanism for enhanced convergence (complex momentum), leads to significant reduction in the SMoE's memory and compute footprint while preserving performance. The work is highly relevant to the scalable deployment of large sparse models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The most significant strength is the introduction of a game-theoretic perspective (Nash Bargaining) to expert merging.\n* The work directly addresses the scalability and deployment challenge of large SMoE models by enabling significant model compression. The results demonstrating reduced footprint while maintaining performance are very compelling."}, "weaknesses": {"value": "* While the memory/compute reduction is a strong result, the paper must clarify the computational overhead of the NAMEx merging process itself. Since Nash Bargaining requires solving an optimization problem to determine the merge point, how does the time taken for merging compare against simpler methods like magnitude pruning or basic parameter averaging? A clear complexity analysis or runtime comparison is needed.\n* The core of Nash Bargaining relies on the definition of the experts' utility functions and the disagreement point (status quo). The manuscript needs to dedicate more detail to explaining why the chosen utility function accurately captures an expert's \"value\" or \"specialized knowledge,\" and how it relates to the loss landscape or data distribution."}, "questions": {"value": "* Could the authors explicitly detail the mathematical expression of the utility function, $u_i(\\theta)$, for a single expert $i$ within the merging context? How is this function derived from the expert's parameters or its performance on a held-out dataset?\n* Please provide a clear, detailed equation for the \"complex momentum\" and explain how it differs from standard momentum methods (like Nesterov or Adam momentum). Does it specifically counteract issues arising from the non-convexity introduced by the merging objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6jqqSck3zq", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Reviewer_kguY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Reviewer_kguY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399442797, "cdate": 1761399442797, "tmdate": 1762921264992, "mdate": 1762921264992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework, Nash Merging of Experts (NAMEx), for merging Sparse Mixture of Experts (SMoE) models into a single dense model for efficient deployment. Departing from traditional heuristic-based averaging, NAMEx models expert merging as a multi-agent cooperative game. It applies the Nash Bargaining Solution (NBS) from game theory to compute fair and Pareto-optimal merging weights that reflect each expert's contribution. Furthermore, to address the slow convergence of prior expert-propagation methods, the authors integrate complex momentum, resulting in the NAMEx-Momentum variant. The method's effectiveness is demonstrated through extensive experiments on language, text, and vision tasks, as well as on large-scale models like DeepSeek-MoE 16B and Qwen1.5-MoE 14B."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength lies in reframing expert merging as a Nash bargaining game. This provides a principled, non-heuristic solution for combining experts, borrowing a sophisticated tool from multi-task learning to solve a practical problem in model deployment.\n\n The paper identifies a key weakness in prior work (EP-CAMEx), namely its slow convergence and suboptimal performance. The introduction of complex momentum is a targeted solution that demonstrably improves convergence speed and stability, as shown in the empirical analysis (e.g., Figure 5).\n\n* **Comprehensive and Strong Validation:** The experimental validation is extensive and a significant strength. The method is tested across multiple modalities (language, vision), on robustness benchmarks (ImageNet-A/O/R), and scales up to 16B parameter models, showing consistent improvements over baselines. This broad validation builds strong confidence in the method's generalizability and practical utility.\n* **Clarity:** The paper is well-written and clearly organized. Figure 2, for instance, provides an excellent visual comparison between NAMEx, CAMEx, and EP-CAMEx, making the architectural contribution easy to understand."}, "weaknesses": {"value": "W1.  The core technical contribution is a clever *combination* of two very recent works: the expert propagation framework from (EP-CAMEx and the NBS) [1,2] . optimization from a multi-task learning paper. While highly effective, this might be seen as a successful application rather than a fundamental theoretical invention.\n\n\nw2.The NAMEx method requires iteratively solving the NBS equation, which introduces significant computational overhead. The appendix (Table 11) shows a 6.8x increase in runtime for a per-layer update. While the final model (NAMEx-Full-Mom) seems to mitigate this by using fewer iterations (as per Appendix F.1), this trade-off between accuracy and cost is a notable weakness.\n\nw3.The \"bargaining budget\" (number of NBS iterations) is a critical new hyperparameter, but its treatment is confusing. The text mentions \"20 steps\" in one place and \"2 iters\" in another. The paper lacks a clear ablation study on how this budget (e.g., 2 vs. 5 vs. 20 iterations) affects both final performance and computational cost.\n\nw4.The paper justifies its choice of utility function by drawing an analogy between the domain vector $\\tau_i$ and a task gradient. This is intuitive but remains an analogy."}, "questions": {"value": "1.  **Overhead and Hyperparameters:** For the key results in the main paper (e.g., Table 2), how many NBS iterations were *actually* used for the best-performing model, NAMEx-Full-Mom (was it 2 or 20)? What is the concrete wall-clock time overhead of your final, optimized model during training compared to the EP-CAMEx baseline?\n2.  **NAMEx vs. NAMEx-Full:** The superior performance of NAMEx-Full (recomputing $\\alpha$ at each layer) over NAMEx (reusing the first layer's $\\alpha$) seems to confirm that layer-wise dynamics (as hinted in Figure 1) are critical. Does this not imply that the basic \"NAMEx\" variant is a methodologically flawed ablation, as it ignores the very dynamics the paper observes?\n3.  **Choice of Disagreement Point:** The paper follows prior work in setting the \"disagreement point\" to 0 (no update). Did you consider other, perhaps more natural, disagreement points for this specific problem, such as \"standard average merging\" (the main heuristic baseline)?\n4.  **Role of the Curvature Matrix $M_i$:** Why was the curvature matrix $M_i$ explicitly removed from the NBS optimization step (Eq. 9, line 1) but kept in the final expert update step (Eq. 9, line 2)? If the game-theoretic solution itself ignores curvature, what role does $M_i$ play in the final update?\n\n\n----\n\nReferences\n\n\n\n[1] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017, 2022. \n\n[2] Viet Dung Nguyen, Minh Nguyen Hoang, Rachel Teo, Luc Nguyen, Tan Minh Nguyen, and Linh Duy Tran. CAMEx: Curvature-aware merging of experts. In The Thirteenth International Conference on Learning Representations, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l3xyb0YTFr", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Reviewer_eQrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Reviewer_eQrG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826091589, "cdate": 1761826091589, "tmdate": 1762921264365, "mdate": 1762921264365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nash Merging of Experts (NAMEx), a novel framework for expert merging in Sparse Mixture of Experts (SMoE) architectures. Diverging from conventional input-dependent or averaging strategies, NAMEx reinterprets expert merging through the lens of game theory, specifically leveraging the Nash Bargaining Solution (NBS) to derive principled merging coefficients. This approach models expert merging as a cooperative-competitive game, using expert domain vectors (deviations from a base expert) as utility functions to ensure a fair and efficient optimal agreement.\nA key extension, NAMEx-Momentum, integrates complex momentum to accelerate convergence and enhance stability during the base expert propagation across SMoE layers, addressing observed slow convergence issues in prior methods like EP-CAMEx. The authors provide theoretical guarantees for the convergence of NAMEx-Momentum."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is exceptionally well-written and clear. The authors effectively motivate the need for a principled merging strategy by highlighting the competitive and cooperative dynamics observed across different SMoE layers and architectures (Figure 1, Figures 6, 8, 9). Key concepts like the Nash Bargaining Solution and its adaptation to multi-task learning are concisely reviewed. The definition of NAMEx (Definition 3.3) and the algorithms (Algorithm 1 and 2) are clearly presented. The comprehensive experimental results are organized logically into tables, with the best performing variants (NAMEx-Full results) highlighted clearly.\n\nThis paper offers a highly original contribution to the Sparse Mixture of Experts:\n- Novel Game-Theoretic Framework: NAMEx introduces the first game-theoretic interpretation of expert merging, leveraging Nash Bargaining to move beyond heuristic or input-independent averaging schemes. This principled method for balancing cooperation and competition among experts is highly original.\n- Enhanced Convergence via Complex Momentum: The successful integration of complex momentum into the expert propagation mechanism (NAMEx-Momentum) accelerates convergence and provides needed stability, particularly addressing limitations found in EP-CAMEx. The theoretical convergence guarantee further enhances this contribution.\n\nThe central claims—that expert merging can be framed as a bargaining game and that NBS provides a principled solution—are adequately supported with both theoretical derivation and comprehensive empirical evidence.\nThe paper formalizes NAMEx as the Nash solution to the Bargaining of Expert Merging Problem, derived from the Nash product maximization objective. The authors provide a proof sketch, which establishes the Nash Bargaining equation $G^\\top G \\alpha = 1/\\alpha$ for computing the optimal update direction. Furthermore, the introduction of complex momentum is backed by a convergence guarantee.\nThe empirical methodology is thorough, testing NAMEx and its momentum variants against baselines across four distinct domains. The consistent performance superiority shown across small, medium, and large-scale models confirms the effectiveness and scalability of the method. The supplementary analysis showing NAMEx yields faster and more stable convergence compared to EP-CAMEx (Figure 5) validates the motivation for using complex momentum.\n\n**Originality and Significance**: The core innovation of framing expert merging as a bargaining problem solved via the Nash Bargaining Solution is highly original and offers a principled alternative to existing heuristics. This perspective enables a derived weighting mechanism ($\\alpha$) rather than a heuristic one.\n\n**Quality of Results**: NAMEx variants consistently deliver superior performance across various benchmarks.\n\n**Clarity and Insight**: The paper is clear in its definitions and provides insightful empirical analysis, such as visualizing how NAMEx steers outcomes closer to the Pareto surface than linear averaging (Figure 11). The investigation into complex and quaternion momentum further pushes the boundaries for stability in expert propagation.\n\n**Theoretical Foundation**: The theoretical proof of convergence for NAMEx-Momentum underpins the method's stability and provides a foundation for future analysis."}, "weaknesses": {"value": "I don't find any major weakness in the paper. \n\nHere are two typos. Please fix\n- Line 365: \"present\" -> \"presents\"\n- Line 1187 \"hese” -> \"these\""}, "questions": {"value": "Please provide a more detailed theoretical or empirical justification for the removal of the curvature matrix $M_i$ in the calculation of the propagation update $\\Delta E^{(l)}$ in NAMEx (Eqn 7) compared to EP-CAMEx (Eqn 3). Does the Nash Bargaining process implicitly account for the necessary geometry, or is this simplification merely an artifact of aligning with the existing bargaining framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZdpwPfS7Pc", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Reviewer_2Qyz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Reviewer_2Qyz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885955228, "cdate": 1761885955228, "tmdate": 1762921263950, "mdate": 1762921263950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"Nash Merging of Experts\" (NAMEx), a novel method for merging experts in Sparse Mixture-of-Experts (SMoE) models. It reframes expert merging as a game-theoretic problem, using the Nash Bargaining solution to calculate merging weights. This principled approach models the complex cooperative and competitive dynamics between experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Applies game theory (Nash Bargaining) to expert merging.\n\n- Consistently outperforms baselines across multiple experiments.\n\n- Deeply investigates key components like layer-by-layer bargaining and momentum."}, "weaknesses": {"value": "- What impact does this 20-step bargaining budget have on the quality of the solution? If the budget is increased, will the performance improve further?\n\n- In the second line of Eq. 9, why not also use Nash Bargaining to guide the calculation of $\\hat{E}_m$?\n\n- It should be compared with more vision MoE models on ImageNet."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HnFrAGFGxa", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Reviewer_zXWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Reviewer_zXWC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916958769, "cdate": 1761916958769, "tmdate": 1762921263156, "mdate": 1762921263156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a token-wise expert merging strategy of MoE, mainly to replace the conventional sparse routing for better performance. The paper adopted the Nash Bargaining Solution (NBS) to enhance the collaboration among experts during the expert merging process. The paper provides theoretical and empirical support to justify the claim."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposal of NBS in expert merging seems novel\n2. The paper provides theoretical justifications behind the proposed method\n3. The method improves performance over previous related expert-merging methods (e.g., SMEAR, CAMEx, EP-CAMEx)"}, "weaknesses": {"value": "1. The main advantage of the MoE-based models is their training efficiency. It has been established in the literature that MoE models generally achieve similar performance with significantly lower training FLOPs due to their sparse training. However, the proposed expert-merging method (and possibly previous methods also) doesn't employ the sparse routing. For example, in equation (7) of the paper, the update of the base expert uses all $N$ experts and their curvature matrices. Therefore, there is an uncertainty about whether the proposed expert-merging process sacrifices the training-efficiency advantage of MoE. As we can see, in Table 12 of the Appendix, the SMoE has lower training FLOPs than NAMEx. Therefore, it is uncertain whether the advantage of NAMEx appears for extra training FLOPs or from the delicate design.\n\n2. Another advantage of SMoE is its capability of maintaining a constant inference FLOPs with the increase of the number of experts. It is not clear whether the advantage remains in the proposed expert-merging method."}, "questions": {"value": "Can the authors clarify whether the empirical advantage of expert-merging arises from extra training FLOPs or from the proposed design? A training FLOPs equivalent result can be a good way to clarify that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FGISlSK7Gn", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Reviewer_q8Yj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Reviewer_q8Yj"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993738272, "cdate": 1761993738272, "tmdate": 1762921262802, "mdate": 1762921262802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision"}, "comment": {"value": "In addition to correcting typos and notation issues, we have revised the paper to incorporate all reviewers’ comments and suggestions. The major updates we made are listed below and highlighted in magenta in the revised manuscript.\n\n1. We conducted compute-matched FLOPs/throughput experiments and added Top-2 MoE parameter counts and WikiText-103 results in Table 1.\n2. We added Top-2 MoE performance results on the GLUE benchmark in Table 2.\n3. In addition, we added SMEAR and Top-2 MoE training compute in Table 13 (Appendix F.1).\n4. We added clarification that we tested the mean as the disagreement point and found negligible differences, so we retain 0 as the stable default, with the comparison reported in Table 4 of the revised manuscript.\n5. We provided a clear ablation (Table 8) and corresponding analysis (Section 5) showing that NAMEx is largely insensitive to the “bargaining-budget” hyperparameter, with only marginal performance differences across NBS iteration counts.\n\nWe will update this summary in our next response once we have obtained additional experimental results."}}, "id": "F5vf53o8Ul", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626873749, "cdate": 1763626873749, "tmdate": 1763641893334, "mdate": 1763641893334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision"}, "comment": {"value": "In addition to correcting typos and notation issues, we have revised the paper to incorporate all reviewers’ comments and suggestions. The major updates we made are listed below and highlighted in magenta in the revised manuscript.\n\n1. We conducted compute-matched FLOPs/throughput experiments and added Top-2 MoE parameter counts and WikiText-103 results in Table 1.\n2. We added Top-2 MoE performance results on the GLUE benchmark in Table 2.\n3. In addition, we added SMEAR and Top-2 MoE training compute in Table 13 (Appendix F.1).\n4. We added clarification that we tested the mean as the disagreement point and found negligible differences, so we retain 0 as the stable default, with the comparison reported in Table 4 of the revised manuscript.\n5. We provided a clear ablation (Table 8) and corresponding analysis (Section 5) showing that NAMEx is largely insensitive to the “bargaining-budget” hyperparameter, with only marginal performance differences across NBS iteration counts.\n6. We conducted an additional comparison with another vision MoE model, NAMEx vs. ACMoE [1], on ImageNet and included the results in Table 15 of Appendix F.4 in our revised manuscript.\n\n**References**\n\n[1] Stefan Nielsen et al. \"Tight clusters make specialized experts.\" (ICLR 2025)"}}, "id": "F5vf53o8Ul", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626873749, "cdate": 1763626873749, "tmdate": 1763668783879, "mdate": 1763668783879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1)"}, "comment": {"value": "Dear AC and reviewers,\n\nThanks for your thoughtful reviews and valuable comments, which have helped us improve the paper significantly. We are encouraged by the endorsements that: **(1)** our game-theoretic framework to handle the complex dynamics of expert merging problem and our solution through Nash Bargaining Solution (NBS) is novel (Reviewer q8Yj, zXWC, 2Qyz, kguY) and principled (Reviewer 2Qyz, eQrG); **(2)** Our incorporation of momentum, specifically complex momentum, improves both the convergence speed and the stability of the expert-merging process (Reviewer 2Qyz, eQrG); **(3)** The proposed framework is supported by  theoretical justifications for future analysis (Reviewer q8Yj, 2Qyz); **(4)** The empirical methodology and evidence are comprehensive and thorough, demonstrating consistent performance gains over previous related expert-merging methods (Reviewer q8Yj, zXWC), across multiple modalities and benchmark (Reviewer eQrG), for small-, medium-, and large-scale models , which confirm both the effectiveness and the scalability of the proposed method (Reviewer 2Qyz). We have updated our submission based on the reviewers' feedback, and **we have highlighted our revision in magenta**.\n\nThe most common concern towards our framework is the computational overhead of the Nash Bargaining problem, the associated speed–accuracy trade-offs of our proposed models, their impact on training efficiency, and whether the empirical gains stem from additional training FLOPs. We have provided detailed explanations to resolve this concern in each reviewer-specific rebuttal and added further theoretical complexity analysis and empirical evaluations in the revised manuscript. We also summarize our response below.\n\nFirst, we would like to clarify that the SMoE baseline shown in Table 12 of the original manuscript (Table 13, Appendix F.1 in the revised manuscript) uses top-1 routing. We used top-1 routing because NAMEx, CAMEx, and EP-CAMEx all operate with a single active expert (the merged expert). However, top-1 is not the standard setting in modern MoE systems. In practice, models use top-2 or top-K routing (K = 8, 16, …) as in GShard [1] and DeepSeek-MoE [2]. \n\nTo address your concern, we additionally compare against top-2 SMoE in Table 1, Table 2, and Table 3 below. We also included these results in Table 1–2 in Section 4 and Table 14 in Appendix F.1 of the revised manuscript.\n\n**Experimental Results Summary**\n\nAcross GLUE tasks and WikiText-103 perplexity, NAMEx and NAMEx-Full consistently outperform CAMEx and EP-CAMEx, with NAMEx-Full-Mom achieving the best performance overall.\n\nImportantly:\n\n- **SMoE (top-2)** provides small accuracy gains but with **substantially higher compute**\n(train throughput drops **19.5%**, from 22,236 $\\rightarrow$ 17,898 tok/s).\n- **NAMEx and NAMEx-Full** improve accuracy purely through expert-space merging, **without increasing routing cost or activating more experts per token**. NAMEx throughput decreases by only around **1.1%** relative to SMoE (top-1), and is **22.9%** faster than SMoE (top-2).\n\n**Table 1**. Training compute and throughput. Inference is unchanged relative to baselines.\n| Model                     | Train TFLOPs | Infer TFLOPs | Train Throughput (tok/s) |\n|---------------------------|--------------|---------------|----------------------------|\n| SMoE                      | 13.95        | 4.65          | 22,236                     |\n| **SMoE (Top-2)**        | 18.32        | 7.44          | 17,898                     |\n| SMEAR                      | 13.95        | 4.65          | 22,236                     |\n| CAMEx                     | 14.30        | 4.65          | 21,982                     |\n| EP-CAMEx                  | 14.25        | 4.65          | 21,982                     |\n| NAMEx                     | 14.25        | 4.65          | 21,995                     |\n| **NAMEx-Full**            | **14.25**    | **4.65**      | **21,897**                 |\n| EP-CAMEx-Mom              | 14.25        | 4.65          | 21,872                     |\n| **NAMEx-Full-Mom**        | **14.25**    | **4.65**      | **21,783**|"}}, "id": "WkJaj6fg27", "forum": "JLe9xfd0ln", "replyto": "JLe9xfd0ln", "signatures": ["ICLR.cc/2026/Conference/Submission9777/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9777/Authors"], "number": 22, "invitations": ["ICLR.cc/2026/Conference/Submission9777/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763661040214, "cdate": 1763661040214, "tmdate": 1763661040214, "mdate": 1763661040214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}