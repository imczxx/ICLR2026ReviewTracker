{"id": "VAhhaq02hj", "number": 19689, "cdate": 1758298418968, "mdate": 1759897025794, "content": {"title": "Know When to Fold 'Em: Predicting an LLM-Judge for Efficient but Performant Inference", "abstract": "Large language models (LLMs) face a fundamental trade-off between computational efficiency (e.g., number of parameters) and output quality, especially when deployed on computationally limited devices such as phones or laptops. One way to address this challenge is by following the example of humans and have models ask for help when they believe they are incapable of solving a problem on their own; we can overcome this trade-off by allowing smaller models to respond to queries when they believe they can provide good responses, and deferring to larger models when they do not believe they can. To this end, in this paper, we investigate whether models can predict---prior to responding---how an LLM judge would score their output. We evaluate three approaches: zero-shot prediction, prediction using an in-context report card, and supervised fine-tuning. Our results show that larger models (particularly reasoning models) demonstrate good zero-shot prediction abilities, while smaller models require in-context report cards or fine-tuning for reliable predictions. While the effectiveness varies across datasets, both approaches can substantially improve smaller models' prediction accuracy, with fine-tuning achieving mean improvements up to 52\\% across datasets. These findings suggest that models can learn to predict their own performance limitations, paving the way for more efficient and self-aware AI systems.", "tldr": "LLMs are capable of predicting how an LLM judge would score their response to a query without needing to respond first.", "keywords": ["Agentic AI", "Large Language Models", "LLM-as-a-Judge", "Self-Reflective Models", "Supervised Fine-Tuning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/712a4639ded58ec3aac343d9e08c9f5624276183.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This is a well-executed paper that tackles the practical challenge of balancing an LLM's computational efficiency with its output quality. The core idea is to empower smaller, faster models to \"know what they don't know\" by having them predict, before generating a full response, how an LLM-judge would score their potential answer. If the predicted score is poor, the query can be deferred to a larger, more capable model. The paper compellingly evaluates three methods for this pre-hoc prediction: zero-shot, an in-context \"report card,\" and supervised fine-tuning. The authors demonstrate that while large models have some innate self-assessment ability, smaller models do not. However, both the report card and fine-tuning methods dramatically improve this capability, with fine-tuning showing the most promise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a core, real-world problem. The trade-off between cost/latency and quality is a primary concern for anyone deploying LLMs, and the proposed \"deferral\" system is an elegant solution.\n\n- The concept of an in-context \"report card\" is a particularly clever, training-free approach. It’s a great interim solution for closed-weight models where fine-tuning isn't an option."}, "weaknesses": {"value": "- The study relies on a single LLM-judge (Llama 3.3 70B). While the judge's evaluations were confirmed to be stable, it would be great to see how the prediction models hold up against different judges (e.g., GPT-4o, Claude 3.5 Sonnet, or even a human panel). Would a model fine-tuned to predict a Llama-judge also be able to predict a GPT-judge?\n\n- The SFT approach is clear, but it would be helpful to have a more detailed discussion on the cost of creating this fine-tuning dataset. It requires generating responses from all models and then running the expensive judge model on them. A brief analysis of this \"setup cost\" would make the SFT method's practicality even clearer.\n\n- The full, practical implementation of the deferral system isn't explored. It would be fantastic if you could add experiments showing the actual end-to-end performance. For example, a \"small model + SFT predictor + large model\" system vs. just using the large model, showing the blended cost-per-query and overall quality score.\n\n- The system seems to imply a binary \"answer or defer\" choice based on \"great/ok/bad.\" It would be interesting to explore a more granular system. For instance, could a predicted \"ok\" score trigger a simpler, cheaper intervention (like a RAG query) rather than a full deferral to the most expensive model?\n\n-  For the in-context report card method, it would be great to see a more direct analysis of the token overhead vs. the accuracy gain. How many tokens does the report card add to the context, what's the added latency from that, and what is the \"break-even\" point where the time saved by not generating a bad answer equals the time spent processing the report card?\n\n- There are red \"REDACTED\" comments at the end of sections like acknowledgement or authors contribution"}, "questions": {"value": "- Your judge rubric is quite general. Did you experiment with how prediction accuracy changes if the rubric is made more specific or complex? For instance, if the judge was asked to only score for \"factual accuracy\" and ignore tone.\n\n- Beyond just prediction accuracy (correctly guessing \"great,\" \"ok,\" or \"bad\"), did you look at the model's calibration? For example, when the SFT model predicts \"bad\" with high confidence, is it almost always correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bCcSlXJHwP", "forum": "VAhhaq02hj", "replyto": "VAhhaq02hj", "signatures": ["ICLR.cc/2026/Conference/Submission19689/Reviewer_nGf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19689/Reviewer_nGf2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460557325, "cdate": 1761460557325, "tmdate": 1762931531308, "mdate": 1762931531308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether LLMs can anticipate how an external LLM-based judge would evaluate their answers before actually generating them. The authors investigate three strategies: (1) zero-shot prediction, (2) in-context “report card” prompting that summarizes past performance, and (3) supervised fine-tuning using the hindsight trick. Results show that larger reasoning models display reasonable self-assessment ability even in zero-shot settings, while smaller models benefit significantly from contextual report cards or fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses how to make LLMs self-aware enough to know when to ask for help. This is both conceptually interesting and relevant for efficient LLM deployment.\n\n2. The proposed approaches are well-motivated and systematically compared across diverse datasets and model sizes.\n\n3. The authors provide some detailed empirical results and ablations (e.g., per-category analysis on MMLU-Pro) that reinforce their claims."}, "weaknesses": {"value": "1. All experiments rely on a single LLM judge (Llama-70B). This raises questions about generalization to different evaluators or judging paradigms.\n\n2. Since both judge and fine-tuning signals ultimately depend on LLM-generated labels, there is no ground truth accuracy. Also, the number of classification types is small (only 3), making the classification problem seem easy.\n\n3. The report cards are generated on a training set, and then given to the testing set as part of the prompt. As this paper considers the i.i.d. case testing, this may give too much shortcut for the problem, making it hard to tell how much improvement is coming aside from just aligning the distribution of the test and training set."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kvv5ZKpvT6", "forum": "VAhhaq02hj", "replyto": "VAhhaq02hj", "signatures": ["ICLR.cc/2026/Conference/Submission19689/Reviewer_f7ts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19689/Reviewer_f7ts"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840932243, "cdate": 1761840932243, "tmdate": 1762931530773, "mdate": 1762931530773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a pre-self-assessment mechanism that enables large language models (LLMs) to predict, prior to generation, how an LLM-based judge would evaluate their responses. It explores three strategies—zero-shot, in-context report cards, and supervised fine-tuning—and demonstrates that smaller models can be effectively calibrated to route queries to larger models only when needed, achieving efficient inference under resource constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Methodological novelty\n\nThe report card approach is innovative. It consolidates joint judge evaluations across multiple models and datasets into concise textual performance summaries, eliminating the need for expensive per-query judge calls. By combining hindsight relabeling with supervised fine-tuning (SFT), the method effectively repurposes existing judge scores as supervision signals. An ablation study (Appendix E) comparing joint versus isolated judging further shows that joint evaluation enhances score diversity.\n\n2. Broad & reproducible evaluation\n\nThe study presents extensive experiments across five diverse datasets (MedQA, LongFact, AIME’24, SciCode, MMLU-Pro) and eleven models ranging from 0.9B to 120B parameters, including recent reasoning architectures such as Llama-4 Scout and DeepSeek-R1. The release of complete prompts (Appendix C) and the detailed judge rubric reinforces the work’s reproducibility and transparency.\n\n3. Strong empirical findings\n\nFine-tuning yields substantial improvements, with the some model achieving a +52 percentage point gain in prediction accuracy. Even large non-reasoning models benefit from contextual report cards. Notably, prediction accuracy increases with query difficulty, suggesting that task complexity itself can serve as an informative signal for adaptive model routing."}, "weaknesses": {"value": "1. Reliance on a Single Judge Model\n\nThe study depends exclusively on a single LLM judge (Llama 3.3 70B) for all evaluations, raising concerns about evaluation bias and potential overfitting to one model’s judgment criteria. Without comparisons across multiple judges or human evaluations, the generality and robustness of the proposed approach remain uncertain.\n\n2. Methodological Simplicity and Incomplete Framework\n\nThe three proposed methods—zero-shot probability prediction, contextual report card prompting, and supervised fine-tuning—are conceptually straightforward, relying on techniques such as prompt engineering and fine-tuning that have been widely explored in prior work. Moreover, the paper does not clearly specify how queries predicted as “bad” are routed to larger models, leaving the proposed self-assessment-based routing framework incomplete for real-world deployment.\n\n3. Unrealistic Assumption of Report Card Availability\n\nThe report card method assumes access to detailed historical performance summaries for each model across multiple datasets. In practical settings, such comprehensive records are rarely available, particularly for unseen data. This assumption limits the method’s applicability and generalization potential.\n\n4. Lack of Baselines and Comparative Analysis\n\nThe paper omits key baselines such as uncertainty modeling and self-evaluation approaches. Without these comparisons, it is difficult to assess the relative improvement, effectiveness, or novelty of the proposed techniques."}, "questions": {"value": "1. The paper discusses agent in both PRELIMINARIES and RELATED WORK, but the main methods and contents of the paper do not seem to involve agent?\n\n2. The PRELIMINARIES section contains some redundant details. The description of LLM architectures is not directly relevant to the paper’s central research question—predicting LLM judge scoring. It may be beneficial to simplify this section and focus only on the components essential to understanding the proposed methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pZ01q1dQBw", "forum": "VAhhaq02hj", "replyto": "VAhhaq02hj", "signatures": ["ICLR.cc/2026/Conference/Submission19689/Reviewer_ycqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19689/Reviewer_ycqw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889479897, "cdate": 1761889479897, "tmdate": 1762931530017, "mdate": 1762931530017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether language models can predict LLM judge scores before generating responses, enabling efficient routing where small models handle easy queries and defer hard ones to larger models. This is practically relevant for 2025's on-device AI deployment trends.\nThe study tests three approaches across 11 models and 5 datasets: zero-shot prediction, report cards with historical performance, and fine-tuning. Key finding: reasoning models show inherent self-awareness while small models achieve up to 52% improvement with fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Timely practical problem addressing real deployment challenges with comprehensive experiments across medical, mathematical, coding, and factual domains.\n2. Strong empirical findings demonstrating that small models can learn their limitations, providing actionable baselines for production systems.\n3. Well-documented reproducible methodology with extensive ablations."}, "weaknesses": {"value": "1. Zero technical innovation: zero-shot is basic prompting, report cards are standard in-context learning, fine-tuning is vanilla supervised learning from 2022.\n2. All tasks have objective answers (multiple choice, math solutions, code correctness). No subjective tasks like creative writing or advice-giving where evaluation is ambiguous.\n3. Single judge (Llama 3.3 70B) creates uncertainty whether models learn true self-awareness or just memorize one judge's preferences.\n\nCritical Questions\n\n- For LongFact success: what drives correct predictions? Is it response length estimation, keyword matching, or topic familiarity? Need 10-20 concrete examples showing why model correctly predicted \"great\" versus \"bad\" with feature analysis.\n- For AIME math problems: how do models assess difficulty? Is prediction based on problem length, mathematical terminology, or actual computational complexity? Requires stratified analysis by difficulty level showing models correctly identify when they fail on olympiad problems but succeed on algebra.\n- Which evaluation criteria drive predictions? The rubric includes accuracy, relevance, clarity, formatting. Do models fail when answers are correct but poorly formatted? When verbose but accurate? Need ablation isolating each criterion's influence."}, "questions": {"value": "- RAG integration: does retrieval-augmented generation improve prediction accuracy on knowledge-intensive queries by providing reference context?\n- Federated learning: can distributed edge devices collaboratively build report cards without sharing raw data, learning when to defer to cloud models based on collective experience?\n- Cross-judge generalization: train on Judge A, test on Judge B and human ratings to distinguish true self-awareness from judge-specific overfitting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Io2l8nR4Bk", "forum": "VAhhaq02hj", "replyto": "VAhhaq02hj", "signatures": ["ICLR.cc/2026/Conference/Submission19689/Reviewer_rVge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19689/Reviewer_rVge"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946300968, "cdate": 1761946300968, "tmdate": 1762931529516, "mdate": 1762931529516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}