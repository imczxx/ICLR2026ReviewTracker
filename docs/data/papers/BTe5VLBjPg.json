{"id": "BTe5VLBjPg", "number": 17070, "cdate": 1758271817990, "mdate": 1759897200250, "content": {"title": "VITA: Vision-to-Action Flow Matching Policy", "abstract": "Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning mechanisms to incorporate visual information during the generative process,  incurring substantial time and memory overhead. To reduce the complexity, we develop VITA~({\\bf VI}sion-{\\bf T}o-{\\bf A}ction policy), a noise-free and conditioning-free policy learning framework that directly maps visual representations to latent actions using flow matching. VITA treats latent visual representations as the source of the flow, thus eliminating the need of conditioning. As expected, bridging vision and action is challenging, because  actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equations) solving steps.  We evaluate VITA on 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA outperforms or matches state-of-the-art generative policies, while achieving $1.5{\\times}$-$2.3{\\times}$ faster inference compared to conventional methods with conditioning.", "tldr": "We present Vision-To-Action flow matching policy, a noise-free, conditioning-free framework, that evolves latent visual representations into latent actions via flow matching for efficient visuomotor control.", "keywords": ["Flow Matching", "Robot Learning", "Imitation Learning", "Robotics", "Robotics Policy", "Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb0175b7c76b0a2cf39cb95093ee2cd9b8150e22.pdf", "supplementary_material": "/attachment/a591d934773a3665aafed3167e928ed5bcfdde54.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Vision-to-Action Flow Matching Policy (VITA), which maps visual representations directly to latent actions using flow matching. Their approach is noise-free and conditioning-free, proposed to reduce time and space complexity. They use an action autoencoder to up-sample action representations to have the same high dimensionality as visual representations. Their approach is simple and relatively novel, achieving state-of-the-art results while being faster."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper's approach is simple and relatively novel in performing flow matching directly from visual representations to action latents. \n2. VITA matches state-of-the-art generative policies\n3. VITA has a faster inference speed relative to other methods, in part due to its simplicity.\n4. The method diagrams they use are simple and understandable (Figures 1 and 2). Their approach is well-motivated and their ablation of FLD and FLC is informative."}, "weaknesses": {"value": "1. The evaluations seem relatively sparse, both in terms of the number of baselines compared and the tasks. For example, how well does FM (or one of the other policies) do on the real-world tasks?\n2. The paper alluded to VITA having lower memory footprint than other methods, especially those that use visual conditioning. Can you quantify/estimate this comparison?\n3. (minor) Citation formatting issues. For example, many cited papers lack parentheses in the first paragraph of the introduction."}, "questions": {"value": "1. I think the paper would be improved by a little more elaboration on the advantages of being noise-free and conditioning-free, and why others have not got this to work successfully in the past.\n2. If VITA used a Transformer/DiT instead of an MLP at roughly the same number of parameters, what would be its latency? (referencing Table 3)\n3. Does Table 3 latency include the visual conditioning time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "59jPfp7vv1", "forum": "BTe5VLBjPg", "replyto": "BTe5VLBjPg", "signatures": ["ICLR.cc/2026/Conference/Submission17070/Reviewer_2nF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17070/Reviewer_2nF7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871750555, "cdate": 1761871750555, "tmdate": 1762927081600, "mdate": 1762927081600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper introduces a noise-free, conditioning-free framework that directly maps visual inputs to latent actions using flow matching, eliminating complex denoising and conditioning steps. By combining an action autoencoder with a flow latent decoding mechanism, VITA bridges vision–action dimensional gaps and stabilizes training. Implemented with simple MLPs, it achieves state-of-the-art performance and 1.5×–2.3× faster inference on simulated and real-world robotic tasks compared to conventional diffusion and flow-based policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper proposes novel solution to the timely and important problem, which seems to be quite significant and useful to the researchers and practitioners. \n\n- Paper itself is cleanly written, explaining the motivation behind the method ingredients. All claims are supported by evidence and additional ablations are provided to further support the proposed contributions. VITA performs on par with the baselines, while being much faster on inference, which is essential for real world applications. Paper provides sufficient information for reproducibility, including hyperparameters. \n\n- Overall, I do not have major concerns regarding the proposed method and think that results are significant."}, "weaknesses": {"value": "I think the paper has two weaknesses. \n\nFirstly, it seems to me that the motivation is not sufficiently explained. After all, why is it important to flow directly from images into actions, rather than from noise with visual conditioning? Why does it introduces complexity? Where does the additional overhead come from? Given that this is not analysed further in the paper, I believe that a simple citation (e.g. line 55) of prior work is insufficient. I advise the authors to elaborate on their reasoning in greater detail.  \n\nSecond, It seems to me that the introduction of MLP-backbone is rather sudden and not particularly justified, in the sense that it is an additional confounder to the main method, which invalidates a fair comparison of approaches specifically for predicting actions. And there are no ablations on that. What if the main speed up (or even performance gains) come from the simpler backbone? How would baselines perform with similar backbones? If baselines will fail but VITA is not, what is the main reason?\n\nI can not answer this questions from the current paper results and this is critical."}, "questions": {"value": "1. Can authors provide std or (better) confidence intervals for Table 3? It should be possible, given that you used more than one random seed.\n2. Why set of baselines differ between experiments (Table 1 & Table 3)? Won't the results differ from other conditioning methods, e.g. AdaLN, cross-attention, FiLM? \n3. I think this line needs clarifications: “we use vector representations for both vision and action, further reducing time and space overhead”. To my knowledge, most existing methods represents observations as vectors… What is the difference specifically and why it reduces time overhead?\n4. Can baselines be trained with MLP backbones? Would they be faster or still slower than the proposed VITA?\n\nMisc:\n\ntypo on 81 line “collapse of targe latent action”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jzIfQPiSDI", "forum": "BTe5VLBjPg", "replyto": "BTe5VLBjPg", "signatures": ["ICLR.cc/2026/Conference/Submission17070/Reviewer_cek7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17070/Reviewer_cek7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989322885, "cdate": 1761989322885, "tmdate": 1762927081205, "mdate": 1762927081205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework that maps visual observations directly into latent action spaces through a flow-matching approach, avoiding traditional denoising or conditioning pipelines. By integrating an action autoencoder with a latent flow decoder, the proposed method (VITA) aligns vision and action representations and improves training stability. Implemented with lightweight MLP networks, it achieves comparable results to diffusion-based and flow-based policies while delivering faster inference across simulated and real-world control benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of simplifying visuomotor policy learning by removing conditioning mechanisms is potentially interesting. The paper is generally clear in presentation, and experiments are competently executed. The implementation details are sufficiently documented, and the inclusion of ablation studies is appreciated."}, "weaknesses": {"value": "The main conceptual motivation is underdeveloped. The authors state that removing conditioning simplifies the process, yet the argument remains superficial. It is not clear why direct flow from vision to actions should be advantageous, or what specific drawbacks the previous conditioning-based methods introduce. Without a stronger analysis, the contribution feels somewhat incremental.\n\nThe use of MLP backbones further complicates interpretation of results. Introducing such a lightweight architecture changes the comparison dynamics, and without explicit ablations, it’s unclear whether performance and efficiency gains originate from the proposed framework or simply from the reduced model complexity. This limits the credibility of the reported advantages.\n\nOverall, while the approach is clean and runs efficiently, it does not convincingly establish the necessity or distinctiveness of the proposed design choices."}, "questions": {"value": "(1) Have the authors evaluated whether the choice of an MLP backbone influences the observed improvements? Specifically, if the same MLP architecture were used for the baseline methods, would their inference times and performance still trail behind VITA? How much of the reported speed-up is due to the backbone rather than the proposed flow-matching formulation itself?\n\n(2) Have the authors evaluated whether the choice of an MLP backbone influences the observed improvements? Specifically, if the same MLP architecture were used for the baseline methods, would their inference times and performance still trail behind VITA? How much of the reported speed-up is due to the backbone rather than the proposed flow-matching formulation itself?\n\n(3) Why do the sets of baselines differ between experiments (e.g., Table 1 vs. Table 3)? What was the rationale for omitting certain conditioning-based approaches such as AdaLN, FiLM, which seem directly relevant to the claimed simplifications? Are the results expected to hold under a unified baseline setup, or were baselines adjusted for computational or implementation reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uu9Wpt8E6p", "forum": "BTe5VLBjPg", "replyto": "BTe5VLBjPg", "signatures": ["ICLR.cc/2026/Conference/Submission17070/Reviewer_od34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17070/Reviewer_od34"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995966692, "cdate": 1761995966692, "tmdate": 1762927079066, "mdate": 1762927079066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about VITA, a noise-free and conditioning-free policy learning framework that attempts to directly map visual representations to latent actions using flow matching. The authors introduce an action autoencoder to align the dimensionalities of the vision and action spaces and propose FLD to stabilize the joint training by backpropagating the reconstruction loss through the ODE solving steps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- I like the core idea of using the visual latent space as the source distribution for the flow matching process. Removing the explicit conditioning mechanisms (like cross-attention or FiLM) that are standard in diffusion/flow policies simplifies the architecture and naturally leads to faster inference.\n\n- The resulting architecture is lightweight. It is compelling that an MLP-only network for the flow matching and decoding components (following the ResNet encoder) can handle complex bimanual tasks like those in the ALOHA suite."}, "weaknesses": {"value": "- I find the claim of being \"conditioning-free\" (L014) slightly misleading. While explicit conditioning modules are removed, the flow is inherently conditioned on the visual input because the visual latent is the source distribution (z0). The velocity field must learn the transport from this specific starting point. This feels more like implicit conditioning via the ODE initial state rather than a fundamental removal of conditioning.\n\n- The approach seems heavily constrained by the architecture. The MLP-only implementation relies on a global average-pooled ResNet-18 feature vector. This severely compresses spatial information and likely bottlenecks the policy's ability to handle tasks requiring fine-grained spatial understanding. It is unclear how this approach would scale if we switched to representations that retain spatial information (e.g., spatial tokens), which often require more complex backbones like U-Nets or Transformers, potentially nullifying the efficiency gains.\n\n- The experimental results are mixed and the baseline comparisons are questionable. VITA underperforms conventional FM on 'Slot Insertion' (0.76 vs 0.83) and 'PourTest Tube' (0.79 vs 0.84). Furthermore, the baseline performance is suspiciously low in some cases (e.g., DP achieving 0.00 on ThreadNeedle), even though the authors mention increasing training steps for DP and ACT significantly. This makes it difficult to gauge the true strength of VITA.\n\n- The necessity of FLD introduces a significant hidden cost. FLD requires backpropagation through the ODE solver steps during training. This substantially increases the computational graph complexity and memory requirements during training. The paper focuses heavily on inference speed, but the practical training overhead of FLD is not discussed or quantified.\n\n- The efficiency gains presented in Table 3 mix architectures and conditioning methods. VITA (MLP) is compared against FM implemented with DiT or U-Net. A fairer comparison would be conventional FM using the same MLP architecture and a simple conditioning mechanism to isolate the benefit of the noise-free approach from the architecture choice."}, "questions": {"value": "- Regarding the tasks where VITA underperforms FM, what is the hypothesis for this performance drop? Does the reliance on the globally pooled visual feature bottleneck performance on these precision tasks?\n\n- What is the actual overhead in training time and memory usage when enabling FLD?\n\n- Why did Diffusion Policy fail so completely on ThreadNeedle and HookPackage despite extended training? Were the hyperparameters for the baselines thoroughly tuned?\n\n- In Section 4.3, paper mentions that a pre-trained and frozen action AE is ineffective. Could you provide the empirical results for this ablation?\n\n- Have you experimented with representations that retain more spatial information instead of the global average-pooled vector? If so, would the MLP architecture still be sufficient?\n\n- How does VITA handle multimodal action distributions? In conventional diffusion/FM, stochasticity comes from the initial noise sampling. In VITA, the process seems largely deterministic once the image latent z0 is fixed. How is multimodality captured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KRKf5Cxqyk", "forum": "BTe5VLBjPg", "replyto": "BTe5VLBjPg", "signatures": ["ICLR.cc/2026/Conference/Submission17070/Reviewer_7Vvz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17070/Reviewer_7Vvz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000199341, "cdate": 1762000199341, "tmdate": 1762927078745, "mdate": 1762927078745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}