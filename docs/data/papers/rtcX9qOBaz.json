{"id": "rtcX9qOBaz", "number": 5470, "cdate": 1757912864260, "mdate": 1759897972637, "content": {"title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications", "abstract": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross scenario tasks , and less than 50% success rate on single scenario tasks. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications.", "tldr": "", "keywords": ["llm agent", "tool use", "multi-turn interaction", "real-world application"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f716a936a8ec66fee5c4ffcebfdfe3c498cda39.pdf", "supplementary_material": "/attachment/cea80d09551948daa1d1b3c487f8156c10cb25c4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VitaBench, a benchmark designed to evaluate LLM-based agents in real-life applications. VitaBench formalizes task complexity along three dimensions—reasoning, tool, and interaction—and proposes a rubric-based sliding window evaluator to assess multi-turn trajectories. With the extensive experiments conducted, the author show that even the best current models achieve only around 30% success in cross-scenario tasks, underscoring the remaining challenges in developing capable real-world LLM agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-organized and clearly written.\n\n- This paper addresses the gap in the absence of evaluations of LLM Agents in real-world settings.\n\n- The proposed task complexity formalization provides a reasonable way to quantify the task difficulties."}, "weaknesses": {"value": "- Although the paper claims to benchmark “LLM-based agents” in real-world scenarios, the evaluation is actually conducted on single LLM models acting as direct function-calling agents, rather than on established agent frameworks or architectures.\n\n- The benchmark evaluates current models as static agents without adaptation or learning. It remains unclear how VitaBench can guide or support the training of future agent models beyond static evaluation."}, "questions": {"value": "Have the authors considered evaluating existing agent frameworks？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fBgpqxiUQm", "forum": "rtcX9qOBaz", "replyto": "rtcX9qOBaz", "signatures": ["ICLR.cc/2026/Conference/Submission5470/Reviewer_zsYD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5470/Reviewer_zsYD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746905856, "cdate": 1761746905856, "tmdate": 1762918081739, "mdate": 1762918081739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VitaBench, a benchmark designed to evaluate agents across three interactive daily-life scenarios: food delivery, in-store consumption, and online travel services. The benchmark comprises 100 cross-scenario tasks, 300 single-scenario tasks, and 66 tools. Agent performance is assessed using a rubric-based sliding window evaluator to handle long-horizon trajectories. Experiments on multiple LLMs demonstrate the difficulty of the benchmark, as the best-performing model achieves only a 30% success rate in the cross-scenario setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well-written and clearly describes the benchmark and evaluation results.\n2.\tThe benchmark focuses on realistic, real-world scenarios, making it valuable for guiding real agent development. Its components, including the user simulator, evaluator, and hyperparameter settings, are shown to be robust in the reliability analysis.\n3.\tThe paper provides thorough analysis of the evaluation results, from aspects of reasoning complexity, task complexity, and interactive complexity."}, "weaknesses": {"value": "1.\tAlthough the benchmark is well designed, there already exist many benchmarks addressing similar tasks. It is unclear how much new insight VitaBench contributes beyond existing efforts.\n2.\tThe sliding window evaluator is intended to manage long trajectories, but it raises the question of whether evaluation should occur over the entire trajectory. An alternative approach could be to extract key outcomes (e.g., booking results) and apply the rubrics to those results directly. Additionally, the permanent satisfaction criterion across windows may fail when an agent meets a rubric early but changes later."}, "questions": {"value": "1.\tCould you provide more details about the tools? Specifically, what are the 66 tools, and are their returned values (e.g., store_info) real or simulated?\n2.\tDo variations in simulated user attributes or communication styles influence the results differently?\n3.\tWhat does the “score” in Table 4 represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f0OeMHCHdo", "forum": "rtcX9qOBaz", "replyto": "rtcX9qOBaz", "signatures": ["ICLR.cc/2026/Conference/Submission5470/Reviewer_boYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5470/Reviewer_boYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847981312, "cdate": 1761847981312, "tmdate": 1762918081424, "mdate": 1762918081424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VitaBench, a new benchmark designed to evaluate LLM-based agents on complex, interactive tasks grounded in real-world, \"life-serving\" applications. The benchmark spans three domains (food delivery, in-store consumption, online travel) and comprises 66 tools, 100 cross-scenario tasks, and 300 single-scenario tasks. The authors motivate their work by arguing that existing benchmarks lack the necessary complexity in terms of information volume, tool interdependencies, and dynamic user interaction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies a critical need for more challenging and realistic agent benchmarks. VitaBench represents a significant step up in scale and complexity compared to prior work, pushing the evaluation frontier toward long-horizon. The ambition to create a \"life-serving simulation environment\" is good and moves the field in the right direction.\n\n2. The proposed rubric-based sliding window evaluator is a  practical solution to the problem of evaluating long-form agent trajectories with LLMs."}, "weaknesses": {"value": "1. The benchmark's entire pipeline is built on LLMs. The agent is an LLM, the user simulator is an LLM (gpt-4.1), and the final evaluator is another LLM (claude-3.7-sonnet). While the authors conduct reliability studies (Section 5.1), this \"LLM-evaluating-LLM-interacting-with-LLM\" setup raises concerns about potential biases and circularity. The benchmark may inadvertently measure how well one model's behavior and style align with another's, rather than objective task success. For example, the user simulator's \"cooperative\" nature might favor agents with similar inherent tendencies, and the evaluator might be more lenient towards reasoning patterns similar to its own. This methodological concern is significant and undermines the objective grounding of the benchmark's results.\n\n2. The simulator is prompted to faithfully convey all the points from a pre-written, detailed instruction set (page 13, \"Must ensure every detail from instructions is mentioned\"). Real users are not like this. They forget their own constraints, provide contradictory information (\"I want the cheapest option... no, not that one, it looks bad\"), and change their minds midway through a task. A key skill for a real agent is navigating this human inconsistency, which VitaBench's user doesn't seem to model. The paper notes that user behavior like \"impatience\" is explicitly prompted (e.g., \"If the agent repeats the same question... show impatience,\" line 719). This is a scripted reaction. A real impatient user might interrupt, abandon the conversation, or start making demands outside the original scope. Moreover, The agent can make as many tool calls as it wants without penalty. In reality, API calls can have monetary costs, and excessive interaction turns frustrate the user and increase the probability of abandonment. **I think the benchmark tests an agent's ability to follow a complex conversational script and using tools rather than handle truly emergent, unpredictable human behavior thus not a realisttic benchmark.**"}, "questions": {"value": "The benchmark is explicitly focused on \"Chinese contexts\" (Appendix B, line 646), while many of the top-performing models evaluated (like the GPT and Claude series) are developed by Western companies and primarily trained on English-language and Western-centric data.Many reasoning tasks are steeped in cultural common sense. For example, the user request in Figure 3 mentions booking a dinner for a \"10th anniversary celebration\" near the \"Huangpu River.\" While anniversary is a universal concept, the specific expectations around such an event, typical restaurant choices, or etiquette might differ. A model trained on Western data might make different assumptions than one with deeper knowledge of modern Chinese urban culture. The example trajectory's mention of a \"Tangshan time-honored restaurant\" is a perfect example of a culturally specific concept (老字号, lǎo zì hào) that an English-centric model might only understand at a surface level. I was wondering if author considering this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Sh2wvGz9i", "forum": "rtcX9qOBaz", "replyto": "rtcX9qOBaz", "signatures": ["ICLR.cc/2026/Conference/Submission5470/Reviewer_Bj6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5470/Reviewer_Bj6N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862858068, "cdate": 1761862858068, "tmdate": 1762918081038, "mdate": 1762918081038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VitaBench, a real-world–oriented agent benchmark spanning three domains (delivery, in-store consumption, OTA).\nThe tasks require multi-turn dialogue, temporal/spatial reasoning, proactive clarification, and tool chaining. Results show strong difficulty: ~30% success on cross-scenario and <50% on single-scenario tasks with various state-of-the-art models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear three-axis task-complexity framework (reasoning/tool/interaction) and cross-scenario composition without domain-specific policies.\n2. The data are collected from authentic platform data.\n3. Rubric + sliding window evaluator with reported human agreement (κ≈0.828) and ablations justifying design choices.\n4. Comprehensive experiments and error taxonomy (reasoning dominates) study,"}, "weaknesses": {"value": "1. Compute/latency opacity: Many-turn trajectories and 4-run protocol imply high cost; paper lacks concrete cost accounting per task/model.\n2. The main results are based on only one basic function calling agent. It is unknown how different agent design would affect the results."}, "questions": {"value": "1. What is the cost analysis of different models? \n2. Is any other agents/techniques being tested? Can agent design affect results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hg7amprVRE", "forum": "rtcX9qOBaz", "replyto": "rtcX9qOBaz", "signatures": ["ICLR.cc/2026/Conference/Submission5470/Reviewer_jGAh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5470/Reviewer_jGAh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945433047, "cdate": 1761945433047, "tmdate": 1762918080571, "mdate": 1762918080571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}