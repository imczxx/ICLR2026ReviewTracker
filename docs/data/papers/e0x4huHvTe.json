{"id": "e0x4huHvTe", "number": 18524, "cdate": 1758288739593, "mdate": 1759897098299, "content": {"title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback", "abstract": "Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. One of the popular approaches for this task is ControlNet, which introduces an auxiliary conditioning module into the architecture. To improve alignment of the generation image and control, ControlNet++ proposes a cycle consistency loss to refine correspondence between controls and outputs, but restricts its application to the final denoising steps, while the main structure is introduced at an early stage of generation. To address this issue, we suggest InnerControl -- a training strategy that enforces spatial consistency across all diffusion steps. Specifically, we train lightweight control prediction probes — small convolutional networks — to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. We prove the efficiency of such models to extract signals even from very noisy latents and utilize these models to generate pseudo ground truth controls during training. Suggested approach enables alignment loss that minimizes the difference between predicted and target condition throughout the whole diffusion process. Our experiments demonstrate that our method improves control alignment and fidelity of generation. By integrating this loss with established training techniques (e.g., ControlNet++), we achieve high performance across different condition methods such as edge, segmentation and depth conditions.", "tldr": "", "keywords": ["Diffusion Model", "Controllable Generation", "ControlNet"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0045302629c8886f1547557964585635ddb90ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces InnerControl, a novel training strategy to improve spatial alignment in ControlNet-based text-to-image diffusion models. While existing methods like ControlNet++ enhance controllability by applying cycle consistency losses, they primarily focus on late denoising steps, neglecting the early stages where spatial structure begins to form. This limitation leads to misalignments between input control signals (e.g., edge maps, depth, segmentation) and the final generated images, especially when extending supervision to early steps causes visual artifacts.\n\nTo address this, the authors propose leveraging intermediate UNet features across all denoising steps, not just the final ones. They train lightweight, timestep-conditioned convolutional probes to predict control signals (e.g., depth, edges) directly from these intermediate features. These probes are used to generate pseudo ground truth controls during training, enabling an alignment loss that enforces consistency between the predicted and target control at every diffusion timestep.\n\nThe key insight is that intermediate diffusion features encode spatial information even in noisy early stages, making them more reliable than one-step image predictions for supervision. This allows InnerControl to avoid artifacts while improving control fidelity and image quality.\n\nMain Contributions of this paper can be summarized:\n\nA new training objective that enforces consistency between input controls and intermediate diffusion features throughout the entire denoising process, including early steps.\n\nThe proposed method improves upon reward-based approaches like ControlNet++ and CTRL-U, achieving better alignment (e.g., 5.6% lower RMSE for depth, 5.6% higher mIoU for segmentation) without degrading image quality.\n\nThe alignment loss can be integrated into existing frameworks, offering a plug-and-play enhancement for various control types (edge, depth, segmentation)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s core contribution—leveraging intermediate diffusion features for control alignment across all denoising steps—is both novel and timely. While prior works like ControlNet++ and CTRL-U focused on late-stage alignment via reward losses, this work identifies and addresses a critical temporal gap: the early denoising stages, where spatial structure emerges but is overlooked. The idea of training lightweight, timestep-conditioned probes to extract control signals from noisy intermediate features is creative and departs significantly from the dominant paradigm of using single-step image predictions for supervision. This approach also draws inspiration from recent work on diffusion features for vision tasks (e.g., Readout Guidance), but applies it in a new context: training-time alignment for controllable generation, not post-hoc analysis or guidance.\n\n\nThis work has broad implications for the controllable generation community. By showing that early-stage supervision is not only possible but beneficial, it challenges a key assumption in prior reward-based methods and opens a new dimension for training diffusion-based controllers. The proposed alignment loss is model-agnostic and can be plugged into existing frameworks, making it immediately useful for practitioners. Furthermore, the paper bridges two previously disconnected lines of work: diffusion feature representations (used for vision tasks) and controllable generation, showing that the former can enhance the latter. This cross-pollination is likely to inspire follow-up work in areas like video generation, multi-modal control, or real-time editing. Finally, by removing the temporal limitation of prior reward losses, InnerControl sets a new standard for fine-grained, training-based control in diffusion models.\n\n\nThe paper conducts thorough empirical validation across multiple control tasks (depth, edge, segmentation) and datasets (MultiGen-20M, ADE20K). The ablation studies are well-designed, showing that the proposed alignment loss improves both control fidelity (RMSE, SSIM, mIoU) and image quality (FID), even when applied to early denoising steps—a setting where prior methods fail due to artifacts. The authors also carefully isolate the contribution of their method by integrating it into existing pipelines (ControlNet++, CTRL-U) and showing consistent improvements, demonstrating modularity and generalizability. The use of timestep-conditioned probes is validated with quantitative comparisons against standard discriminative models (e.g., DPT), showing superior stability and accuracy across the diffusion trajectory."}, "weaknesses": {"value": "The alignment module is essentially a “conv head on U-Net decoder” whose structure and channel-fusion logic are borrowed wholesale from Readout Guidance (Readout Guidance: Learning Control from Diffusion Features).  The only new twist is timestep conditioning, which is already standard in diffusion literature.  Consequently, the architectural contribution feels thin.\n\nAll four tested conditions (depth, HED, LineArt, segmentation) are dense, pixel-to-pixel maps.  The paper never tackles sparse or semantic controls—e.g., bounding boxes, key-points, open-vocabulary phrases, or 3-D poses—where mis-alignment is often discrete rather than ℓ₂ error.  The probes may fail when the signal is a set of 10 key-points rather than a 512×512 map.  A concrete fix is to add at least one sparse task (e.g., COCO key-point conditioning) and report PCK or OKS instead of RMSE/SSIM.  If the head must be redesigned (e.g., heat-map → coordinate regression), the authors should discuss how much extra engineering is required and whether the same head still applies.\n\nTable 6 shows that segmentation needs alignment only in the second half of the trajectory, while depth needs all steps.  The paper offers no principle for choosing the interval; practitioners must grid-search per task.  This reintroduces a meta-hyper-parameter that the method claimed to remove. \n\n\nEvery training step now runs the probe and computes an extra loss on every timestep."}, "questions": {"value": "The alignment head is taken almost verbatim from Readout Guidance (RG).  What architectural changes did you introduce beyond RG’s weighted-sum bottleneck + timestep embedding?  If the only delta is the loss function, please state so explicitly.\n\n\nDid you try the same head on SD-XL or on a DiT-based diffusion model?  If it fails, how do you reconcile the claim of “model-agnostic” alignment?\n\nTable 6 indicates that segmentation needs alignment only in [450,980], whereas depth needs [0,920].  Did you attempt to learn these intervals instead of grid-search?  If learned, describe the algorithm; if not, please concede this hyper-parameter overhead.\n\n\nAll four conditions are dense pixel maps.  Have you tried sparse controls—COCO key-points or bounding boxes—where the probe must output 17 heat-maps or 4 numbers instead of 512×512 maps?  \n\n\nYou report 6 h on 8×H100 but not the slow-down factor relative to ControlNet++.  Please provide:\n(a) images/sec per GPU for both runs,\n(b) peak memory delta, and\n(c) whether gradient checkpointing was enabled for the probe.\n\n\nThe probe is discarded at inference.  Did you explore using it for on-the-fly correction?  For example, after 50 DDIM steps, if probe-RMSE >τ, inject an auxiliary gradient and continue sampling.  What τ values work?  Does quality improve or collapse?\n\nYou argue that intermediate features are more reliable than one-step x₀-predictions.  Can you provide a minimal toy experiment that quantifies this?  For example, plot mutual information between (a) intermediate features and ground-truth depth, and (b) single-step x₀ and ground-truth depth, across noise levels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wS9lrixMGw", "forum": "e0x4huHvTe", "replyto": "e0x4huHvTe", "signatures": ["ICLR.cc/2026/Conference/Submission18524/Reviewer_5Wbu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18524/Reviewer_5Wbu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698521503, "cdate": 1761698521503, "tmdate": 1762928215937, "mdate": 1762928215937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation in ControlNet training: the inability to enforce spatial control alignment during early diffusion steps. Prior methods (like ControlNet++) use a reward loss ($\\mathcal{L}_{reward}$) that fails in early steps because it relies on inaccurate, blurry single-step predictions ($x_0'$), causing image artifacts.\n\nThe proposed method, InnerControl, solves this by changing the signal source. Instead of using the $x_0'$ prediction, it pre-trains lightweight, timestep-conditioned \"probes\" ($\\mathbb{H}(\\cdot, t)$) to extract control signals directly from the UNet's intermediate features (the \"inner voice\").\n\nThis extracted signal is stable and accurate even at high noise levels, enabling a new alignment loss that can be applied across the entire diffusion process without degrading image quality. The final training combines the stable, all-step with the strong, late-step. This approach achieves state-of-the-art control fidelity (e.g., 5.6% RMSE drop in depth) while maintaining or improving image quality (FID)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core strength is its clear identification and experimental validation of why prior reward losses (like ControlNet++) fail. The analysis showing that the signal source (the blurry one-step $x_0'$ prediction) is the root cause of early-step instability is a critical insight.\n2. The idea of using the UNet's \"inner voice\" (intermediate features) is an elegant solution. The pre-trained probe ($\\mathbb{H}$) is shown to be a far more robust signal extractor than standard models (like DPT) operating on $x_0'$.\n3. The method is clever in combining the new stable loss with the old strong loss. It doesn't just replace $\\mathcal{L}_{reward}$; it complements it. The alignment loss provides a stable baseline control across all steps, while the reward loss provides a strong correction at the end. The ablations (Table 2) clearly show this combination is superior to either loss alone.\n4. The method achieves its primary goal: it measurably improves control alignment (SOTA on RMSE, SSIM, mIoU) without the image quality (FID) trade-off that plagued prior methods when applied to early steps."}, "weaknesses": {"value": "1. The proposed method requires a new, non-trivial pre-training stage for the $\\mathbb{H}$ probe. This must be done for every control type (depth, segmentation, HED, etc.), and each probe requires its own specific training dataset (e.g., ADE20K for segmentation). This increases the overall pipeline complexity and data requirements. The paper could be strengthened by discussing the generalization of these probes or a more data-efficient way to train them.\n2. The final loss function is a complex combination of three losses, each with weights ($\\alpha$, $\\beta$) and, critically, task-specific timestep schedules (e.g., depth alignment is applied at [0, 920], segmentation at [450, 980]; reward is 400 steps vs. 200). This suggests the method may be difficult to tune for a new, unseen control type. The paper would be more impactful if it provided a clearer methodology or ablation for how these optimal schedules were determined, or if it showed that a single, unified schedule can work well.\n3. The results in Table 1 show that while InnerControl consistently wins on controllability (e.g., SSIM), it sometimes loses to CTRL-U on image quality (e.g., FID for LineArt and Segmentation). This suggests the trade-off is not fully resolved. The paper should discuss this trade-off more directly. Is there a \"knob\" (e.g., the $\\beta$ weight) that could balance this, allowing a user to trade a bit of control for better FID?\n4. The paper's premise is that the $x_0'$ prediction is unreliable. The chosen solution is to find a new signal source (intermediate features). An alternative, unexplored path would be to improve the $x_0'$ prediction itself. For example, would using a more sophisticated sampler's $x_0'$ prediction (like DDIM) or a multi-step $x_0'$ prediction provide a stable-enough signal for $\\mathcal{L}_{reward}$? A small ablation on this could strengthen the paper's claim that intermediate features are the necessary solution."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKioJk3uUf", "forum": "e0x4huHvTe", "replyto": "e0x4huHvTe", "signatures": ["ICLR.cc/2026/Conference/Submission18524/Reviewer_e6tR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18524/Reviewer_e6tR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942386907, "cdate": 1761942386907, "tmdate": 1762928215245, "mdate": 1762928215245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposing a new training strategy called InnerControl, which is an enhancement to existing ControlNet-style architectures for text-to-image diffusion models that improves spatial alignment between conditioning inputs (like edge maps, depth, or segmentation) and generated images. \n* The key idea is to get consistency signal feedback during whole generation process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper observes that prior approaches only focus on the final generation results, leading to slow and delayed feedback during training.  \n- To address this, the paper introduces the prediction of a pseudo \\\\( x_0 \\\\), which is then decoded into an image via a VAE. This enables the extension of the consistency loss to every diffusion step."}, "weaknesses": {"value": "* the cost is expensive because we have to do the vae decode for each step.\n\n* the contribution compared with controlnet++ is only the step level feedback, which is a trival trick since 2023.\n\n* the improvement is marginal."}, "questions": {"value": "* In Table 1, for FID of Depth Map, it seems ControlNet has best performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PJAjwQ3Sjf", "forum": "e0x4huHvTe", "replyto": "e0x4huHvTe", "signatures": ["ICLR.cc/2026/Conference/Submission18524/Reviewer_PrLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18524/Reviewer_PrLh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974366181, "cdate": 1761974366181, "tmdate": 1762928214853, "mdate": 1762928214853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}