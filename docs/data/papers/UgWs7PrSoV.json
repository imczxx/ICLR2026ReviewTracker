{"id": "UgWs7PrSoV", "number": 24097, "cdate": 1758352659615, "mdate": 1759896781869, "content": {"title": "AutoBaxBench: Bootstrapping Code Security Benchmarking", "abstract": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks and vulnerabilities to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBench, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBench to construct entirely new tasks and release them to the public, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing under USD 10.", "tldr": "We present a method to generate new secure code generation benchmarks from scratch.", "keywords": ["large language model", "large language models", "LLM", "code generation", "code security", "security", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/233047e1f3135099d98626efcb71a7c1e1b3aec6.pdf", "supplementary_material": "/attachment/c2405b451ec55f7ed3ad7e8d3dbe41986d2e5ae4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AutoBaxBuilder, an LLM-based framework for automatically generating code security benchmarks, including new tasks, functional tests, and end-to-end exploits, extending BaxBench. The method employs an agentic orchestration pipeline that iteratively refines scenarios, solutions, and security tests through execution feedback and self-critique. The generated tasks are compared agains BaxBench tests, demonstrating strong agreement and improved exploit coverage. Experiments across state-of-the-art LLMs reveal low secure-pass rates, underscoring the challenge of generating secure code."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a fully automated pipeline for constructing code-security benchmarks with minimal human intervention.\n2. AutoBaxBench shows strong empirical alignment with BaxBench and deeper exploit coverage."}, "weaknesses": {"value": "1. The paper shows limited novelty beyond automation.\n2. The design has potential contamination and bias on benchmark generation and evaluation."}, "questions": {"value": "The paper presents an interesting design; however, in its current form, it is not strong enough to be accepted.\n\n1. The framework mainly extends BaxBench by adding an orchestration layer using stronger LLMs. The algorithmic design follows established LLM-agent paradigms without introducing new security-testing methodologies or vulnerability detection. In essence, the contribution lies in automation rather than conceptual innovation, resulting in limited novelty.\n\n2. AutoBaxBench uses GPT-5 as an orchestration LLM to generate scenarios, test cases and exploits. It iterates on solutions generated by  GPT-5, Claude-4 Sonnet, DeepSeek-R1, and Qwen-Coder 480B. This design introduces the risk of data contamination or model-family bias, since the benchmark generator and evaluated models share overlapping reasoning patterns or prompt structures. A rigorous benchmark should ideally remain model-agnostic, constructed via a fixed, independent toolchain. Could the paper clarify how contamination is mitigated? And what are the performance of GPT-5, Claude-4 Sonnet, DeepSeek-R1, and Qwen-Coder 480B when tested on AutoBaxBench?\n\n3. It remains unclear how many tasks AutoBaxBench implements. The original BaxBench benchmark includes 392 executable tasks across 28 scenarios, while AutoBaxBuilder reports only 40 new “scenarios.” Figure 3 compares AutoBaxBench and BaxBench results side-by-side, but this comparison may be misleading: the total number of test instances are not matched or reported; The difficulty and coverage distributions may not align; And it’s unclear whether the same subset of scenarios is used for comparison.\n\n4. No human study or expert verification is provided for the generated functional and security tests. The paper claims that AutoBaxBench produces “more thorough” or “better” exploits, but this conclusion relies solely on quantitative metrics. A small-scale audit by security experts—evaluating a random sample of generated exploits for correctness, realism, and exploitability—would substantially strengthen the paper’s credibility.\n\nMinor:\n– There is an extraneous “s” above “functional requirement” in Figure 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EYzgEVKnMC", "forum": "UgWs7PrSoV", "replyto": "UgWs7PrSoV", "signatures": ["ICLR.cc/2026/Conference/Submission24097/Reviewer_utRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24097/Reviewer_utRV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972940000, "cdate": 1761972940000, "tmdate": 1762942936350, "mdate": 1762942936350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents AUTOBAXBUILDER, an LLM-orchestrated pipeline that automatically creates realistic backend application tasks for secure-coding evaluation, with OpenAPI specs, functional tests, and end-to-end exploits. It validates the generated tests against BAXBENCH, finding close agreement on functionality and stricter security coverage, then releases AUTOBAXBENCH with 40 new scenarios across Easy/Medium/Hard splits. Results show low secure-pass rates for strong models on the hardest split, highlighting a real gap in secure code generation; the whole pipeline is low-cost and scalable."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Realistic, application-level evaluation (multi-endpoint REST backends) instead of toy, single-function settings; language/framework-agnostic via HTTP testing. \n- Agentic, execution-in-the-loop generation of scenarios, tests, and exploits with iterative refinement and sanity checks (e.g., OpenAPI validation). \n- Tightens security assessment relative to BAXBENCH. Higher exploit coverage and more sensitive tests. \n- Scalable & economical: ~2 hours per scenario and a few USD each; three difficulty tiers enable broad model evaluation. \n- Clear writing and very strong structure; figures explain the system cleanly.\n- Appendix is comprehensive, with detailed statistics, vulnerability analyses, and case studies"}, "weaknesses": {"value": "- While the evaluation is extensive, a brief discussion of failure cases where the pipeline struggles (e.g., scenario types it can’t yet reliably generate) would strengthen understanding of current limitations.\n- The paper focuses on task generation and evaluation results; a short qualitative example walkthrough (prompt → refinement → exploit) in the main text could make the pipeline’s behavior more concrete for readers."}, "questions": {"value": "- The methodology could be extended beyond web server back-ends, right?\n- What about generalizing to other languages like JavaScript (Node.js) or Java? There will be different attack surface being exposed with different languages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mysK5Cqjew", "forum": "UgWs7PrSoV", "replyto": "UgWs7PrSoV", "signatures": ["ICLR.cc/2026/Conference/Submission24097/Reviewer_y8WS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24097/Reviewer_y8WS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973384885, "cdate": 1761973384885, "tmdate": 1762942936083, "mdate": 1762942936083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends BAXBench but takes a fully automated approach, where instead of humans designing realistic backend security challenges (scenarios, tests, exploits), the authors propose a pipeline which uses an orchestrator LLM (GPT-5) to invent new backend scenarios, generate functional test cases to ensure the scenario is solvable, and generate security exploits that must break at least one solution (but not all), and to ensure the vulnerability is real, validates via iteration until constraints are satisfied. They claim this allows creation of new benchmark tasks in <2 hours, costing <$10, and that task quality is comparable to human-designed ones.\n\nThe system inherits its CWE grounding from BaxBench and claims to cover the same 13 high-severity CWE classes. They even report cases where AutoBAXBench identifies additional CWE types that the original human-authored tests missed (e.g., OS command injection instead of only path traversal)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•  important challenge: the high human labour of building security benchmarks like BAXBENCH. Proposing the first end-to-end automated pipeline (AutoBAXBENCH) that generates backend scenarios, functional tests, and security exploits from scratch using agentic LLMs.\n•  alignment with existing benchmarks: evaluated on BAXBENCH scenarios, with authors ensuring that the generated benchmarks cover the same 13 high-severity CWE classes. They also show that CWE coverage and per-scenario vulnerability counts match or exceed the original expert-authored tests.\n•  quantitative & qualitative evaluation: 6 model families benchmarked across difficulty levels (Easy/Medium/Hard) with pass@1 and sec_pass@1 results reported to be comparable to BAXBENCH. They also include a manual qualitative analysis demonstrating cases where the automated tests detect realistic additional vulnerabilities (e.g., OS command injection in FileSearch) that were not captured by BAXBENCH.\n•  produces more thorough attacks than the human baseline on some tasks, eg. detecting memory exhaustion via crafted arithmetic expressions and concurrent loads, whereas BAXBENCH only used basic symbolic misuses.\n•  scalability and cost-effectiveness: can produce a new benchmark task (scenario + functional tests + exploit tests) in under 2 hours and <$10 USD"}, "weaknesses": {"value": "* limited and unsystematic human validation: The paper  does include a “manual security test analysis” and present a few examples (e.g., OS injection in FileSearch, resource exhaustion in Calculator). This indicates some human inspection occurred. However, this qualitative evaluation is anecdotal (approx. 6 scenarios), lacks methodological transparency (no sampling strategy, number/expertise of reviewers, rubric, or inter-rater agreement), and is not reproducible. In summary, the qualitative evaluation appears to consist only of the authors manually inspecting a small number of generated tests, and as a result, the claim that AutoBAXBench “reproduces or outmatches expert-written tests” is not fully substantiated.\n* figures 4 & 5 show correlation between AutoBAXBENCH and BAXBENCH outcomes, but this only demonstrates consistency, not correctness. It's possible that AutoBAXBENCH incorrectly flags additional “vulnerabilities” due to over-strict or misaligned tests.\n* risk of falsely accepted pipeline steps: several stages rely on LLM self-judgment and automated execution checks. Without human auditing, it is unclear whether functional tests fully capture the intended behaviour, whether vulnerabilities are genuine, or whether CWE labels are always accurate."}, "questions": {"value": "* who performed the manual analysis? How many experts, what were their credentials, and was it blinded/independent?\n* do you have a defined rubric (scenario realism, spec–test alignment, exploit realism, CWE mapping)? Please share it and inter-rater agreement stats if available.\n* for the “extra” exploits (the 6 scenarios), were these reproduced against human-written reference implementations (not just LLM solutions)?\n\nThe appendix includes a helpful scenario-generation case study (SVG badge) that demonstrates the pipeline’s adaptive test and exploit iteration. However, this single simple example is insufficient evidence by itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HWvyPvTXhQ", "forum": "UgWs7PrSoV", "replyto": "UgWs7PrSoV", "signatures": ["ICLR.cc/2026/Conference/Submission24097/Reviewer_4Ydm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24097/Reviewer_4Ydm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070641559, "cdate": 1762070641559, "tmdate": 1762942935630, "mdate": 1762942935630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As LLMs become widely used in software engineering, assessing the correctness and security of their generated code is increasingly important, yet existing manually crafted benchmarks are limited and unsustainable. To address this, the paper introduces AutoBaxBench, a fully automated framework that generates security benchmarking tasks and tests using LLMs, incorporating plausibility checks and end-to-end exploit generation. Evaluations show that AutoBaxBench produces high-quality benchmarks comparable to expert-created ones, enabling new task generation in under two hours at a cost of less than $10."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important and timely problem of generating security benchmarking tasks and tests using LLMs. These in turn can be used to automatically evaluate correctness and security of LLM-generated code against those tasks and tests.\n\n2. The evaluation shows that AutoBaxBench is on par with the manually created predecessor, BaxBench, in terms of correctness and security trends for a variety of different LLMs."}, "weaknesses": {"value": "1. The presentation of the paper needs significant improvement. I found the main Algorithm 1 to be highly incomprehensible\n- What is the desired goal of Steps 2 and 3?  How do LLMs help accomplish those goals? Even if LLMs can only approximately achieve those goals, it would be good to know what the ideal outcome is.\n- Why are two different LLMs used for M and M_s? Is the intention to use the best model (like GPT-5) for M, but weaker LLMs for M_s?\n- Is the pseudo code of various functions in Algorithm 1 (such as refine_tests, refine_solutions, etc.) provided somewhere? I was only able to find informal descriptions in Section 3.\n- A running example showing what is generated at each step would also be helpful.\n\n2. AutoBaxBench would be most useful when one wants to improve a specific LLM.  How can one achieve this? Suppose the target LLM is M_c.  How should one pick M and M_s in Algorithm 1, and why?\n\n3. There is hardly any discussion in the paper about the kinds of languages, tasks, and correctness/security properties that AutoBaxBuilder supports. Section 2 notes \"Each such combination defines a language-independent task, which can readily be evaluated in 14 frameworks across 6 programming languages.\" What are these frameworks and languages?\n\nMinor: the terminology AutoBaxBuilder vs. AutoBaxBench is confusing. I think you are proposing the former, a benchmark builder, and you have generated one instance of the benchmark called AutoBaxBench. But the paper uses these terms interchangeably, e.g. calling the framework itself AutoBaxBench in the abstract."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CwEmliK59X", "forum": "UgWs7PrSoV", "replyto": "UgWs7PrSoV", "signatures": ["ICLR.cc/2026/Conference/Submission24097/Reviewer_RoZm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24097/Reviewer_RoZm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131240290, "cdate": 1762131240290, "tmdate": 1762942934887, "mdate": 1762942934887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}