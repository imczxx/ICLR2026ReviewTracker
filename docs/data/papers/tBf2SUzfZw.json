{"id": "tBf2SUzfZw", "number": 840, "cdate": 1756820641609, "mdate": 1759898239351, "content": {"title": "Visual Jigsaw Post-Training Improves MLLMs", "abstract": "Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While *vision-centric* post-training is crucial for enhancing MLLMs’ intrinsic understanding of visual signals, current post-training paradigms are predominantly *text-centric*, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce **Visual Jigsaw**, a generic *self-supervised* post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Self-supervised Learning", "Post-training", "Reinforcement Learning", "Visual Jigsaw"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c398ec8af3c81ac30f73a97cf7462c8803a32b5.pdf", "supplementary_material": "/attachment/bafae2edb1af8d0a3b057850bc652b99c06275e1.zip"}, "replies": [{"content": {"summary": {"value": "This paper considers Visual Jigsaw as a general self-supervised task for post-training. Specifically, it has three variants, including image jigsaw, 3D jigsaw (ranking from the closest to the farthest), and video jigsaw. Empirically, initialized from Qwen2.5-VL-7B-Instruct, the proposed method achieves improvements over baselines separately. Moreover, when incorporating with a text-centric reasoning MLLM (ThinkLite-VL), the proposed image jigsaw post-training task still brings improvements on vision-centric benchmarks and maintains the performance on reasoning-oriented benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. The motivation of this paper is clear and reasonable.\n3. Empirical studies are partially sufficient."}, "weaknesses": {"value": "1. The proposed three types of jigsaw tasks are evaluated *separately*. It is strongly encouraged to combine them and find out whether they can work together.\n2. Qualitative examples of the reasoning pathways of both (1) when solving the proposed jigsaw problems and (2) answering multiple-choice questions are missing. Moreover, the underlying reason why this pre-text task is beneficial for VQA seems to be missing. An analysis of the reasoning pathways might be helpful.\n\nSome suggestions on the claims:\n1. The claim \"While these jigsaw-style approaches provide structural ordering signals, they have generally shown weaker performance compared to more dominant approaches\" is actually wrong. There are actually some great works [1, 2, 3] that demonstrate jigsaw-like self-supervised pre-training tasks are at least comparable with contrastive learning and masked image modeling. Therefore, adding discussions on these works and clarifying the claim is important.\n2. The claim\"Current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning\" is also not that appropriate. Recent o3-like approaches are definitely not text-centric reasoning methods. Discussions should include both early approaches like CogCoM [4] and Dyfo [5], recent approaches like DeepEyes [6], VGR [7], Pixel-Reasoner [8], and TreeVGR [9]. The advantages of visual jigsaw compared with these o3-like approaches should be discussed.\n\n**References**\n\n[1] Position prediction as an effective pretraining strategy. ICML, 2022\n\n[2] DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions. NeurIPS, 2023.\n\n[3] Location-aware self-supervised transformers. WACV, 2024.\n\n[4] CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning. ICLR 2025.\n\n[5] Dyfo: A training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. CVPR 2025.\n\n[6] DeepEyes: Incentivizing\" Thinking with Images\" via Reinforcement Learning. arXiv 2025.\n\n[7] Vgr: Visual grounded reasoning. arXiv 2025.\n\n[8] Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv 2025.\n\n[9] Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv 2025."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BTp6ZCEg3S", "forum": "tBf2SUzfZw", "replyto": "tBf2SUzfZw", "signatures": ["ICLR.cc/2026/Conference/Submission840/Reviewer_kYxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission840/Reviewer_kYxc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760516104050, "cdate": 1760516104050, "tmdate": 1762915623816, "mdate": 1762915623816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Link to Jigsaw-R1"}, "comment": {"value": "Thanks for sharing this interesting work! I enjoyed reading it and I’m glad to see continuous interest in rule-based RL and structured pretext tasks in multimodal learning.\n\nI’m one of the authors of *Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles* (Wang et al.; preprint May 2025; accepted to TMLR Oct 2025), which you already cite as related work. Thank you. Our paper also introduces jigsaw puzzles as a rule-based visual RL pretext task for post-training multimodal LLMs, and studies their impact on generalization and reasoning. To the best of our knowledge, Jigsaw-R1 was the first to use jigsaw puzzles in this specific “MLLM + rule-based RL post-training” setting.\n\nFrom a reader’s perspective, the connection between the two works could be made a bit more explicit. Since both papers study jigsaw-based RL for MLLMs, some experimental comparison on overlapping settings, would help the community understand the relative strengths of each approach.\n\nThis is meant purely as a constructive suggestion. I’m very happy to see follow-up work in this direction, and I think your extensions to video and 3D, along with the graded reward and more complex ordering tasks, provide useful perspective on how structured visual pretext tasks can improve MLLM perception under rule-based RL."}}, "id": "8K47fygNVJ", "forum": "tBf2SUzfZw", "replyto": "tBf2SUzfZw", "signatures": ["~Junyi_Zhu1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Junyi_Zhu1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission840/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763376688956, "cdate": 1763376688956, "tmdate": 1763377006954, "mdate": 1763377006954, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel post-training task called Visual Jigsaw, which encourages MLLMs to learn the inherent order within visual inputs. An RL-based supervision strategy is employed to train the model. Experiments on image, video, and 3D benchmarks demonstrate that the proposed method significantly improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper presents an effective post-training framework for enhancing MLLMs.\n\n2.\tThe idea of leveraging a self-supervised objective to strengthen the visual understanding capability of MLLMs is novel and promising.\n\n3.\tThe proposed approach achieves strong performance across diverse benchmarks, including image, video, and 3D tasks.\n\n4.\tThe manuscript is well structured, clearly written, and easy to follow."}, "weaknesses": {"value": "1.\tThe Visual Jigsaw task employs a relatively simple supervised objective. Although the experimental results across multiple benchmarks are promising, the underlying mechanism of how this form of supervision contributes to model improvement remains unclear. It would strengthen the paper to include an analysis of what is learned during post-training and how this supervision enhances visual comprehension.\n\n2.\tThe experiments are conducted on only a single MLLM. Including additional baseline models would help demonstrate the generalizability and robustness of the proposed Visual Jigsaw framework.\n\n3.\tThe Visual Jigsaw post-training is performed using specific individual datasets rather than the original MLLM training data. It would be helpful to clarify the rationale behind this choice and provide guidelines for selecting appropriate post-training data."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k8PR57k1CZ", "forum": "tBf2SUzfZw", "replyto": "tBf2SUzfZw", "signatures": ["ICLR.cc/2026/Conference/Submission840/Reviewer_uvM4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission840/Reviewer_uvM4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292088585, "cdate": 1761292088585, "tmdate": 1762915623434, "mdate": 1762915623434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Visual Jigsaw , a self-supervised post-training framework designed to enhance the intrinsic visual understanding of MLLMs. The core idea is to formulate a visual ordering task, the model is presented with shuffled parts of a visual input and must generate a text-based permutation to reconstruct the original order. This approach cleverly leverages RLVR, specifically using GRPO, to optimize the model. The authors instantiate this method across images, videos, and 3D data . Extensive experiments on a wide range of benchmarks show that this post-training stage significantly improves fine-grained perception, temporal reasoning, and 3D spatial understanding, demonstrating the method's generality and effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an elegant and effective idea. It repurposes a classic self-supervised task (jigsaw puzzles) into a post-training stage for modern MLLMs. The framework is simple, requires no architectural modifications or extra generative modules, and is broadly applicable."}, "weaknesses": {"value": "The improvement effect of some benchmarks is not significant."}, "questions": {"value": "1. Have you tried expanding on the harder puzzle tasks (like 4x4)?\n2. Given that some improvements are modest, the results would be more credible if variance  of evaluation are reported."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uLQHSzVvIv", "forum": "tBf2SUzfZw", "replyto": "tBf2SUzfZw", "signatures": ["ICLR.cc/2026/Conference/Submission840/Reviewer_znHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission840/Reviewer_znHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653656289, "cdate": 1761653656289, "tmdate": 1762915623050, "mdate": 1762915623050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Visual Jigsaw, a self-supervised post-training framework designed to enhance vision-centric understanding in MLLMs. The method formulates a general visual ordering task, partitioning and shuffling images, videos, or 3D inputs and requires the model to predict the correct permutation via natural-language output without modifying architecture or adding visual generators. Experiments on Qwen2.5-VL show improvements across fine-grained perception, temporal reasoning, and 3D spatial understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The jigsaw ordering task provides a simple yet effective way to reinforce visual perception in MLLMs, avoiding generative components.\n\n- Applying the post-training principle to images, videos, and 3D data convincingly demonstrates versatility.\n\n- Evaluation on various benchmarks covering visual perception, temporal reasoning, and 3D geometry provide good empirical support.\n\n- SFT vs RL comparison, jigsaw-difficulty analysis, and transfer to reasoning models (ThinkLite-VL) strengthen the claims.\n\n- The paper is well written and figures are intuitive."}, "weaknesses": {"value": "- The paper has limited conceptual novelty. The idea extends classical self-supervised “jigsaw” pretexts (Noroozi & Favaro 2016) into RL post-training. The novelty mainly lies in adapting it to MLLM post-training rather than the task itself.\n\n- Missing related works of reconstruction-based methods in MLLMs and discussion in paper: Recent works like X-Former[1] explicitly combines contrastive and masked-reconstruction objectives with frozen encoder and decoders to improve visual understanding during pre-training with less data. In contrast, Visual Jigsaw uses structural ordering without dense pixel reconstruction. The paper should discuss this design trade-off in detail, why post-training with ordering might achieve similar benefits, and whether the two paradigms (reconstruction vs ordering) could be complementary.\n\n- The paper omits discussion on training overhead for Jigsaw compared to SFT.\n\n- Missing comparison to SoTA for image and video benchmarks.\n\n- The paper could further analyze reward sensitivity and scalibility (beyond 3×3 grid/6-clips).\n\n- All results rely on Qwen2.5-VL; would be good to show on other model architectures like LLaVA/Blip to confirm generality\n\n- Missing qualitative analysis. Visual reasoning traces (< think > outputs) could better substantiate the claim of improved visual understanding.\n\n\n\n[1] X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs. Sirnam Swetha, Jinyu Yang, Tal Neiman, Mamshad Nayeem Rizve, Son Tran, Benjamin Yao, Trishul Chilimbi, Mubarak Shah. ECCV 2024"}, "questions": {"value": "- How sensitive are the improvements to the number of jigsaw pieces (K)? Does performance saturate or degrade with grid size (4×4, 5×5) grids or > 6 video clips?\n\n- The reward combines partial correctness with a discount factor γ = 0.2. How sensitive are results to this value? Did the authors explore other reward formulations ?\n\n- What is the total cost of post-training ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0GgEX5XAv5", "forum": "tBf2SUzfZw", "replyto": "tBf2SUzfZw", "signatures": ["ICLR.cc/2026/Conference/Submission840/Reviewer_Dhxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission840/Reviewer_Dhxc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975551573, "cdate": 1761975551573, "tmdate": 1762915622904, "mdate": 1762915622904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}