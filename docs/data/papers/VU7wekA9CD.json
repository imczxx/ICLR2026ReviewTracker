{"id": "VU7wekA9CD", "number": 23219, "cdate": 1758340953244, "mdate": 1763755955569, "content": {"title": "Membership Privacy Risks of Sharpness Aware Minimization", "abstract": "Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This is an intriguing phenomenon, as conventional belief posits that higher membership privacy risk is associated with poor generalization. We investigate this phenomenon by running extensive analysis on memorization and influence scores. Our results corroborate that SAM is capable of capturing atypical subpatterns, leading to higher memorization scores. On the other hand, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Paradoxically, this reliance on majority features incurs higher variance on output probability or confidence for unseen samples, improving membership privacy. We model SAM under a perfect interpolating regime and theoretically show that SAM has an inherent property of reducing variance, provably resulting in higher MIA advantage for confidence and likelihood ratio attacks.", "tldr": "Sharpness Aware Minimization is more vulnerable to membership privacy attacks than SGD although it generalizes better due to the property of memorizing atypical subclass features.", "keywords": ["membership inference attack", "sam", "sharpness aware minimization", "memorization", "benign overfitting"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7dc4bd8a6b34f87741d0dbab7a09b8fcf4b8cab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the vulnerability of neural networks trained using algorithms that improve generalisation, such as the Sharpness-Aware Minimisation (SAM), to membership inference attacks (MIAs). It provides empirical evidence that suggests SAM-optimised networks are more vulnerable to MIAs when compared to SGD-optimised networks. The author(s) also provide theoretical justification for the high MIA vulnerability of SAM-optimised networks despite improved generalisation, since that goes against the conventional expectation where high MIA vulnerability is associated with low generalisation (and vice-versa)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents evidence that suggests that optimising models using SAM encourages memorisation of atypical samples (mid-range memorisation scores), which contribute to their improved generalisation.\n- The author(s)' proposed metric of influence entropy $\\mathcal{I}_{ent}$ in Eq(6) helps verify the claim that SAM enhances generalisation by encouraging the network to memorization more atypical samples compared to SGD-based optimisation (as seen in Figure 4(a)). \n- The empirical results show that the claims in the paper are not model-dependent but extend to different experimental settings.\n- The theoretical justification for the observed phenomenon of improved generalisation leading to high memorisation when optimising a model with SAM is well described."}, "weaknesses": {"value": "- Computing memorisation scores using Feldman and Zhang's [1] method is computationally expensive as it often requires (re)training hundreds of models. Why did the author(s) not use Ye et al.'s [2] more efficient version to compute these scores?\n- Use of balanced accuracy as the attack metric and not TPR at fixed FPR [3], which informs more about an attack's ability to correctly identify membership signal at [preferably] low FPR (or low chances of predicting a member as a non-member).  \n- It does not use SOTA MIAs such as LiRA [3] / Quantile-MIA [4] / RMIA [5] to measure the sensitivity of SAM-optimised networks, which are known to provide better estimates of a model's MIA vulnerability compared to attacks that rely on predetermined thresholds, such as the Entropy- or Confidence-based attacks. There is also the case that the theoretical results for Theorem 2 depend on a single [data-dependent] threshold, whereas a factor contributing to the success of SOTA MIAs is that they incorporate sample-level thresholds. \n\n[1] Feldman, V., and Zhang, C. “What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation.” NeurIPS 2020.\n\n[2] Ye, J. et al. \"Leave-One-Out-Distinguishability in Machine Learning.\" ICLR 2024.\n\n[3] Carlini, N. et al. \"Membership Inference Attacks From First Principles.\" SP 2022.\n\n[4] Zarifzadeh, S. et al. “Low-Cost High-Power Membership Inference Attacks.” ICML 2024.\n\n[5] Bertrán, M. et al. \"Scalable Membership Inference Attacks via Quantile Regression.\" NeurIPS 2023."}, "questions": {"value": "**Questions**: I would urge the author(s) to address the weaknesses detailed above. I am amenable to updating my initial assessment thereafter.\n\n**Suggestions**: I suggest the following edits to improve the presentation of the paper:\n- Minor Suggestion #1: Memorisation scores are measured w.r.t. samples, so this statement, \"Motivated by this connection, we analyse the memorisation scores of SAM-trained models...\", is somewhat misleading. It would be better to amend it to frame it w.r.t. individual samples.\n- Minor Suggestion #2: Can you report the correlation coefficient between mem_SAM and mem_SGD for Figure 1(b) and 1(c)?\n- Minor Suggestions #3: Lines 350-352 are written in a complicated and difficult-to-read manner. It would be best to rewrite it focusing on one relationship (for example, lower number bucket is associated with higher memorisation and vice versa)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JDRKvmufEu", "forum": "VU7wekA9CD", "replyto": "VU7wekA9CD", "signatures": ["ICLR.cc/2026/Conference/Submission23219/Reviewer_g2BK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23219/Reviewer_g2BK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487029337, "cdate": 1761487029337, "tmdate": 1762942565581, "mdate": 1762942565581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals a critical trade-off in Sharpness-Aware Minimization (SAM): while it improves model generalization, it also increases privacy risk by making models more vulnerable to Membership Inference Attacks (MI). The authors demonstrate that this occurs because SAM's performance gains stem from its enhanced ability to memorize rare sub-patterns within the training data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow.\n\n- The finding of the relationship between the SAM and memorization is interesting and may shed light to privacy defense.\n\n- The theoretical analysis is interesting and insightful."}, "weaknesses": {"value": "+ (Major) Experimental Evidence. The central claim regarding SAM's heightened privacy risk is not yet fully convincing. The results in Table 1 show that SAM's privacy risk (measured by ASR) is not consistently or significantly higher than that of SGD (in Purchase-100 and Texas-100). Besides, Table 3 reports only the best attack accuracy, unlike Table 1 which shows results for all MIA methods.\n\n+ (Major) Uncertain connection between ASR and memorization score. The link between the memorization score and the Membership Inference Attack (MIA) success rate is presented as a given. However, this relationship appears to be an assumption rather than an empirically demonstrated fact. This connection should be either validated or more cautiously framed as a hypothesis.\n\n+ (Major) What can the observation bring to the future work on MIA or SAM? A more precise presentation of the study's potential implications for relevant fields would substantially strengthen the significance of this research.\n\n+ (Minor) The solidness of the experiments would be strengthened by evaluating a wider range of modern MIA methods to ensure the findings are not specific to the selected attacks. \n\n+ (Minor) The experiments are primarily conducted on small datasets. It is unclear if the observed privacy trade-offs persist on larger, more complex benchmarks such as ImageNet.\n\n## Score Justification\n\nThe paper is well-organized and introduces valuable theoretical analysis. However, the experimental evidence currently feels incomplete, which undermines the strength of its conclusions. Besides, the most significant shortcoming is the lack of a clear discussion on the broader impact and implications of these findings for the community. While briefly mentioned in the conclusion, a detailed discussion on how this new understanding of SAM's properties (especially concerning long-tailed data) can influence future research and practice is crucial."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ax38bKV1wU", "forum": "VU7wekA9CD", "replyto": "VU7wekA9CD", "signatures": ["ICLR.cc/2026/Conference/Submission23219/Reviewer_2e1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23219/Reviewer_2e1z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810817495, "cdate": 1761810817495, "tmdate": 1762942564226, "mdate": 1762942564226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a counterintuitive yet rigorously studied phenomenon: Sharpness-Aware Minimization (SAM), an optimization method known for improving generalization, unexpectedly increases vulnerability to membership inference attacks (MIAs). Through extensive experiments on multiple datasets and theoretical analysis, the authors demonstrate that SAM’s generalization gains stem from structured memorization of atypical subclass patterns(e.g., rare features in long-tailed distributions), which simultaneously enhances test performance and privacy risks. The work challenges the conventional belief that better generalization implies lower privacy risk, offering novel insights into the trade-offs between optimization, generalization, and privacy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers novel insights into the trade-offs between generalization and privacy, which challenges the conventional belief that better generalization implies lower privacy risk.\n2. The authors combine empirical evidence with theoretical guarantees to support their claims. The consistent results across datasets and models strengthen their conclusion.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. While the paper highlights SAM’s privacy vulnerability, it would be good to propose or evaluate defensive strategies to mitigate this risk to improve practical applicability.\n2. The theoretical analysis relies on a simplified linear model and strong assumptions (e.g., perfect interpolation). It lacks both theoretical and empirical validations on more advanced non-linear architectures like Transformer-based or diffusion models."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fYgVO1b0RU", "forum": "VU7wekA9CD", "replyto": "VU7wekA9CD", "signatures": ["ICLR.cc/2026/Conference/Submission23219/Reviewer_U9Jc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23219/Reviewer_U9Jc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921141639, "cdate": 1761921141639, "tmdate": 1762942563522, "mdate": 1762942563522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conventional belief that improved model generalization implies better membership privacy by investigating the SAM algorithm. The authors surprisingly find that SAM-trained models, despite achieving better generalization than standard SGD, may be more vulnerable to MIAs. The authors hypothesize this occurs because SAM selectively learn atypical but generalizable sub-patterns more effectively than SGD. This enhanced memorization of rare features improves performance on atypical test samples, but consequently increases the privacy risk by making training samples more distinguishable. The paper empirically validates this mechanism by analyzing memorization and influence scores and provides a theoretical framework showing how a model that better captures minority features can simultaneously achieve high generalization and high MIA vulnerability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper first states that SAM increases vulnerability to MIAs.\n2. The paper is exceptionally clear, presenting its counter-intuitive finding and complex hypothesis in a logical, well-structured, and easy-to-follow manner."}, "weaknesses": {"value": "1. The authors claim to challenge the conventional assumption that improved generalization implies stronger privacy. However, this insight has already been discussed in previous works [1,2,3,4].  \n2. The paper's claims are based solely on the original SAM algorithm. It is unclear whether these findings on increased privacy risk generalize to other sharpness-aware optimizers (e.g., ASAM, GSAM). An investigation into these variants is recommended.  \n3. As a privacy metric, accuracy has been criticized in many works [4,5]. It is recommended to provide TPR at a low FPR and evaluate the privacy risk using recent MIA methods, such as LiRa and RMIA [4,6].\n4. The \"anti-alignment\" assumption means a model that gets better at classifying the majority subclass inherently gets worse at classifying the minority subclass, forcing a direct trade-off. This is a very strong and specific setup.\n\n\n\n[1] \"Understanding membership inferences on well-generalized learning models\" (arXiv, 2018)\n\n[2] \"When does data augmentation help with membership inference attacks\" (ICML, 2021)\n\n[3] \"Bounding Information Leakage in Machine Learning\" (Neurocomputing, 2023)\n\n[4] \"Membership Inference Attacks From First Principles\" (SP 2022)\n\n[5] \"Evaluations of Machine Learning Privacy Defenses are Misleading\" (CCS, 2024)\n\n[6] \"Low-Cost High-Power Membership Inference Attacks\" (ICML, 2024)"}, "questions": {"value": "I do not understand why Assumption 4 is reasonable. Could you explain it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T3Uc6XWdIx", "forum": "VU7wekA9CD", "replyto": "VU7wekA9CD", "signatures": ["ICLR.cc/2026/Conference/Submission23219/Reviewer_4axt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23219/Reviewer_4axt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982194892, "cdate": 1761982194892, "tmdate": 1762942563051, "mdate": 1762942563051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}