{"id": "GQugc1J2kW", "number": 3113, "cdate": 1757336783901, "mdate": 1763037484701, "content": {"title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "abstract": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web. The benchmark spans four levels: Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. To assess both effectiveness and efficiency, we further propose the Efficiency–Quality-Aware (EQA) metric, which measures task success alongside action redundancy. Extensive evaluations reveal that precise visual grounding is the critical determinant of performance, underscoring the advantages of modular designs with specialized grounding modules. Moreover, all agents suffer from substantial inefficiencies, frequently completing tasks with excessive steps despite eventual success. Performance also degrades on complex or cross-application tasks, exposing weaknesses in memory, planning, and adaptive reasoning. By providing broad coverage, standardized protocols, and novel metrics, MMBench-GUI establishes the first comprehensive foundation for advancing GUI agent research.", "tldr": "A hierarchical benchmark for evaluating GUI automation agents across six platforms.", "keywords": ["GUI Agent", "Benchmark", "Computer Use"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4d785248cfaf014050ab7df25dcbc61a57d83fe0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new benchmark for CUA agents, which supports multiple platforms. The work is technically solid and well-done, providing a structured way to assess agents across perception, grounding, automation, and multi-application collaboration tasks. The tests of various schemes on this benchmark also sound reasonable, providing essential information for researchers in this field to further explore."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is one of the first to unify GUI agent evaluation across platforms and hierarchical capability levels, through low-level perception (understanding, grounding) to high-level reasoning (automation, collaboration). I like the four-level evaluation structure.\n\nThe proposed EQA metric also sounds good, which could address an issue ignored by prior success-only metrics.\n\nThe experiments on this benchmark by including major open-source and proprietary VLM/LLM systems are also well done, which is beneficial to the research on CUA agents. And also, it's good to see the results show that accurate visual grounding is the key bottleneck."}, "weaknesses": {"value": "The paper’s primary contribution is the benchmark design, not new algorithmic or modeling techniques. It's useful to the CUA research, but it's hard to say if its technical contributions are sufficient as an ICLR paper. \n\nAnd also, prior works like OSWorld already explore parts of these capabilities, and the paper’s novelty lies mainly in unifying them, not in entirely new data or task types."}, "questions": {"value": "In addition to the novelty issue, the authors are also suggested to add more systematic discussions on the models/solutions' generalization capabilities evaluated with this benchmark, e.g., performance on unseen apps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzxLNFGNLa", "forum": "GQugc1J2kW", "replyto": "GQugc1J2kW", "signatures": ["ICLR.cc/2026/Conference/Submission3113/Reviewer_ri1F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3113/Reviewer_ri1F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546934345, "cdate": 1761546934345, "tmdate": 1762916557345, "mdate": 1762916557345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear AC, reviewers,\n\nThank you very much for your hard work and careful review. We have carefully gone through your comments and agree that the current writing and chapter organization indeed need further refinement. Some details regarding the data construction also need to be described in greater depth. Your review has been very inspiring for us, and we would like to express our sincere gratitude for your insightful feedback. After considering your comments and the whole timeline of rebuttal period, we decide to withdraw our paper to make further polishment.\n\nBest regards,\nAuthors"}}, "id": "9mfqevg821", "forum": "GQugc1J2kW", "replyto": "GQugc1J2kW", "signatures": ["ICLR.cc/2026/Conference/Submission3113/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3113/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763037483727, "cdate": 1763037483727, "tmdate": 1763037483727, "mdate": 1763037483727, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on evaluating GUI automation agents through a hierarchical benchmark, MMBench-GUI, which spans six platforms and four capability levels, introducing an Efficiency–Quality-Aware (EQA) metric to measure task success and efficiency across more than 8,000 tasks. It does not propose any new model, algorithm, or representation-learning insight."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Benchmark covering six major platforms and four hierarchical capability levels.\n2. Introduces an Efficiency–Quality-Aware (EQA) metric combining accuracy and efficiency.\n3. Offers extensive empirical results across a wide range of proprietary and open-source GUI agents.\n4. Proposes a transparent and well-documented data construction process."}, "weaknesses": {"value": "1. **No contribution to learning representations or algorithms.**  \n   The work is purely an evaluation benchmark with no proposed model, training method, or learning insight, misaligned with ICLR’s core focus on representation learning.\n\n2. **Limited conceptual novelty.**  \n   The four-level hierarchy (understanding → grounding → automation → collaboration) closely mirrors existing works such as OSWorld, ScreenSpot-Pro, and UI-TARS, amounting to a reorganization rather than a conceptual advance.\n\n3. **Lack of benchmark reliability evidence.**  \n   The paper does not report human baselines, inter-annotator agreement, or quality-control statistics, making it unclear whether the dataset accurately reflects human task understanding.\n\n4. **No analysis of learned representations.**  \n   The paper reports only performance metrics and provides no investigation into internal representations, generalization behavior, or failure structures of the evaluated models.\n\n5. **Evaluation is descriptive rather than explanatory.**  \n   While the results show performance differences, there is no deeper causal or diagnostic analysis that connects outcomes to representational or architectural factors.\n\n8. **Primarily an engineering contribution.**  \n   The main value lies in data collection and system integration rather than new scientific understanding or learning principles.\n\n9. **Efficiency metric (EQA) adds limited theoretical value.**  \n   Although formally defined, EQA is a straightforward success-vs-step measure and offers minimal conceptual advancement beyond existing efficiency metrics."}, "questions": {"value": "1. **Interpretation of EQA metric.**  \n   The EQA metric measures combined efficiency and accuracy, but its practical meaning is unclear. Could the authors provide examples illustrating how EQA changes agent ranking compared to success rate alone, and whether this correlates with human judgment of efficiency?\n2. **Cross-application task design.**  \n   For L4 “Task Collaboration,” can the authors describe how multi-application coordination is simulated? Are tasks executed in real interactive environments, or in scripted virtual contexts?\n3. **Scalability and potential for training use.**  \n   Is it feasible to scale MMBench-GUI beyond the current size, particularly for the higher-level (L3 and L4) tasks? Additionally, could the benchmark be adapted or extended for *training* purposes—for example, to support representation learning or reinforcement learning of GUI agents—rather than evaluation only? If so, what learning objectives or architectures would benefit from it?\n4. **Human performance reference.**  \n   Did the authors measure human accuracy or efficiency on any subset of tasks? Such baselines would help calibrate the benchmark’s difficulty and interpret model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yFSKMrMUPB", "forum": "GQugc1J2kW", "replyto": "GQugc1J2kW", "signatures": ["ICLR.cc/2026/Conference/Submission3113/Reviewer_oqiV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3113/Reviewer_oqiV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726556394, "cdate": 1761726556394, "tmdate": 1762916556651, "mdate": 1762916556651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMBench-GUI, a well-designed hierarchical benchmark that evaluates GUI agents across four levels and six platforms, with 8000+ tasks, and a novel and valuable EQA efficiency metric. The comprehensive evaluation framework provides useful insights for the community. The extensive experiments across 10+ models yield actionable findings about visual grounding bottlenecks and efficiency gaps. While the work relies on existing benchmarks and LLM-generated data, and some findings are expected, the systematic unification, broad platform coverage, and practical contributions make this a useful benchmark paper that will benefit GUI agent research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The four-level framework naturally decomposes GUI agent capabilities from basic understanding to complex collaboration, enabling fine-grained diagnosis.\n\nS2: The benchmark spans all major platforms (Windows, macOS, Linux, iOS, Android, Web) and has a balanced data distribution, addressing a gap in existing work.\n\nS3: Testing 10+ models with detailed analysis across platforms and difficulty levels provides useful insights for the community."}, "weaknesses": {"value": "W1: L1/L2 rely heavily on automated generation via Claude/GPT models, with limited details on quality control using manual sampling. For manual sampling, what percentage of LLM-generated questions were rejected during manual review? What specific issues were found? More details could be provided.\n\nW2: The continuous-time integral formulation of EQA (Equation (11)) is somewhat opaque. Have the authors compared EQA to other efficiency metrics? How sensitive is it to the choice of M=101 evaluation points? Could the authors show that EQA correlates with human judgments of efficiency?"}, "questions": {"value": "Q1: How were the 15 and 50 step limits determined? What happens with other budgets?\n\nQ2: For L3/L4, what percentage of tasks are newly created vs. curated from existing benchmarks? Can the authors clarify the contribution breakdown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wz9uyif43", "forum": "GQugc1J2kW", "replyto": "GQugc1J2kW", "signatures": ["ICLR.cc/2026/Conference/Submission3113/Reviewer_ddvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3113/Reviewer_ddvL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795525504, "cdate": 1761795525504, "tmdate": 1762916556185, "mdate": 1762916556185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark to evaluate the various capabilities required in GUI agents. The task is built in a hierarchical manner, from understanding to perception, to two stages of action prediction and planning. The first stage (L3) deals with single-platform tasks, and the second stage (L4) deals with multi-platform tasks. The authors evaluate several GUI agents on their benchmarks and through analysis, the authors point out several areas where GUI agents underperform."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The papers exhaustively cover different abilities of GUI agents and diverse and different platforms. The motivation to do this hierarchically is sound.\n2. The authors introduce MacOSArena, which includes 70 tasks for MacOS, is a novel contribution. This is in addition to L1 and L2 images they curate and the tasks they create for the same.\n3. The paper analyzes the efficiency of GUI agents in performing tasks in addition to task success. Such a type of evaluation is much needed."}, "weaknesses": {"value": "1. My main point of concern is that this benchmark does not give new insights regarding GUI agents. I do agree that unifying different tasks in one framework is useful, but I don't see how doing so allows authors to come up with new insights (please see my comments below to know my problems with the tasks). Overall, I feel several works already point out the issues that the authors learn from this benchmark, which brings an important question as to its practical utility.\n\t1. For L1, a related work is GUI-World [1] already covers a plethora of platforms and several software applications, and tests both image and video models on a number of prediction, reasoning, and captioning tasks.\n\t2. For L2, ScreenSpotv2, ScreenSpot-Pro [3] and UI-Vision [4] already have combined over 100 different software applications covered. Moreover, for the authors' claims on direct instructions L195, Ui-vision has functional and spatial instructions that go beyond direct instructions. \n\t3. For L3 and L4, the tasks are already in the existing benchmark, and some works analyse models' weakness across these as they are very popular benchmarks (except MacOS, which I already pointed to in the strengths)\n\tThere might be other relevant works that I missed. Moreover, many of the insights the paper provides are also covered in related works. All this makes me wonder about the need for a new benchmark (see problems listed below for more details) especially given that most of L3 and L4 are from already existing tasks. If the authors could provide a clear and convincing argument in this regard, it would be really helpful.\n\t**Note:** I don't mean to say this negatively. I know that the authors have put a lot of effort into this work, and I appreciate and respect that. But from a conference point of view, I believe these questions need to be addressed for a benchmark.\n2. Where do the authors get their images from? Currently, L921-928 only tells us that the screenshots are obtained manually from diverse software applications, but neither the coverage of these applications nor their sources is described. This hinders the transparency. Did the authors pick the images themselves? If so, what criteria were used, and how did the authors ensure that there was no bias?\n3. **Some concerns around the L1-L4:**\n\t1. For L1 and L2 tasks, there is too much reliance on LLMs to create the tasks. This risks biases of the LLMs into the annotation. For example, for the L1 tasks, the LLM might ask questions around elements that it is confident about, which might cause biases in the initial distribution of questions/instructions.\n\t2. The performance of models on L1-L2 is quite high. For L2, the basic category, which I believe to be the most important category, as in realistic cases, agents would receive direct information from a planner or human, the performance is quite high. The authors have not evaluated more recent models like [5], [6], which further raises the question of how high the accuracy might go.\n\t3. How much manual labelling was done for the L2 task? And if a limited number of UI elements were annotated, what motivated this choice?\n\t4. For the MacOSArena, could the authors give more details regarding how the tasks were created?\n4. Most of the meat of the paper is in the appendix. I understand that the authors have a lot of content to present, but even the main contribution, which is the EQA metric, is in the appendix. This makes the read a little tiring as one needs to go to the appendix to understand all the details (including the source of images). I believe the details regarding the source and metrics are very important for a benchmark paper and need to be summarised appropriately in the main text. One suggestion here is to shorten the task definitions as these are not novel and already known, and let the new readers refer to the appendix for more details on the task definition.\n5. There is no detailed comparison with related benchmarks. A good table highlighting what new additions the authors present could be helpful.\n\n[1] Chen et al. GUI-World: A Dataset for GUI-Oriented Multimodal Large Language Models\n\n[2] Cheng et al. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents\n\n[3] Li et al. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use\n\n[4] Nayak et al. UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction"}, "questions": {"value": "Please refer to the weakness for more questions. The question below is a clarification I wanted.\n1. I had a question around the EQA metric. From my understanding, this metric is sensitive to the order of the tasks performed, as the order determines T_k and S_k. Am I right here? If so, then how is this a reliable metric, especially since the tasks are ordered randomly and the agents can't choose the order? Do all the people who evaluate the tasks have to run them in a particular order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eAybJygFfN", "forum": "GQugc1J2kW", "replyto": "GQugc1J2kW", "signatures": ["ICLR.cc/2026/Conference/Submission3113/Reviewer_hau5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3113/Reviewer_hau5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762536985400, "cdate": 1762536985400, "tmdate": 1762916555714, "mdate": 1762916555714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}