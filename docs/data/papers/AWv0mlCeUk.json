{"id": "AWv0mlCeUk", "number": 3252, "cdate": 1757387269803, "mdate": 1759898099578, "content": {"title": "Discourse-Aware Retrieval-Augmented Generation via Rhetorical Structure Modeling", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an important means for enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages as flat and unstructured text, which prevents the model from capturing structural cues and constrains its ability to synthesize dispersed evidence and to reason across documents. Although a few recent approaches attempt to incorporate structural signals, each remains restricted to shallow representations such as entity graphs or dependency edges and thus fails to capture hierarchical discourse organization. To overcome these limitations, we propose Discourse-RAG, a structure-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk rhetorical structure theory (RST) trees to capture local coherence hierarchies and builds inter-chunk rhetorical graphs to model cross-passage discourse flow. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Discourse-RAG achieves a new state-of-the-art ROUGE-L score of 42.4 on ASQA dataset and improves LLM Score by 12.79 points over standard RAG on Loong benchmark. These findings underscore the important role of discourse structure in advancing retrieval-augmented generation.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "Discourse-Aware Generation", "Rhetorical Structure Theory", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c5cd95f5796bff9c5b978f844cfaab6d8954fc6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Discourse-RAG, a retrieval augmented generation pipeline that makes the model explicitly use discourse structure. It first parses each retrieved chunk into an RST like tree and then links chunks with rhetorical relations to show support, elaboration, or conflict. A final planning stage guides generation using this structure. In evaluation it outperforms standard RAG and other structure aware baselines on long context QA, ambiguous QA, and scientific summarization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- It uses one pipeline where chunk level discourse trees feed into a cross chunk graph, and both are used to guide generation.\n\n- The method is tested on three tasks, with two Llama models, in both open and closed settings, and it beats 2025 RAG baselines, including on ASQA.\n\n- The ablations, noise tests, and chunk size tests show that the method improves across different settings."}, "weaknesses": {"value": "- While training-free, the pipeline requires multiple LLM calls per query (RST parsing per chunk, O(k²) pairwise relation inference, planning, generation). Cost grows quickly with larger k and the paper should discuss the latency and token counts.\n\n- I noticed that all LLM benchmarks use Llama 3.x models. Since Qwen was already used for embeddings, why not include Qwen models in the main comparisons as well?\n\n- Discourse quality is not validated. All trees and relations come from an LLM prompt rather than a parser with known accuracy. If the LLM segments poorly the whole pipeline can weaken.\n\n- Relation set may be too big. The method uses many fine grained discourse labels but does not show which ones actually help. A smaller set might work the same.\n\n- Evaluation scope is narrow. Results are mostly on English, long context, clean inputs. It is unclear how well this works on multilingual or noisy data."}, "questions": {"value": "- Did you evaluate the accuracy of your LLM-generated RST trees against gold-standard annotations (e.g., on RST-DT)? \n\n- Have you considered integrating neural discourse parsers or non-RST frameworks (e.g., entity grids, coherence models)?\n\n- In cases where Discourse-RAG underperforms standard RAG (if any), what are the failure modes? Are they due to incorrect rhetorical parsing, poor planning, or something else?\n\n- Beyond automatic metrics (ROUGE, LLM Score), was there any human assessment of coherence, faithfulness, or readability? LLM-based scoring can be biased."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vDBUZMXodt", "forum": "AWv0mlCeUk", "replyto": "AWv0mlCeUk", "signatures": ["ICLR.cc/2026/Conference/Submission3252/Reviewer_sbPP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3252/Reviewer_sbPP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848173113, "cdate": 1761848173113, "tmdate": 1762916629983, "mdate": 1762916629983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Discourse-RAG, a novel training-free framework designed to address a key limitation in standard Retrieval-Augmented Generation (RAG): the tendency to treat retrieved documents as a flat, unstructured \"bag of facts.\" This \"flat structure\" problem leads to intra-chunk structural blindness and inter-chunk coherence gaps, hindering the model's ability to synthesize evidence and reason."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper identifies a clear and important limitation of standard RAG (its \"flat structure\") and proposes a novel, linguistically-grounded solution that directly addresses it.\nS2. The method achieves state-of-the-art performance on multiple, diverse benchmarks (long-doc QA, ambiguous QA, summarization), demonstrating its effectiveness and generalization ability.\nS3. The paper is clear, well-illustrated, and reproducible thanks to the detailed appendices."}, "weaknesses": {"value": "W1. This method is computationally expensive. The proposed pipeline requires an enormous number of LLM inference calls per query. As described in the methodology, for a top-k retrieval, the framework k calls for intra-chunk RST tree construction, k * (k – 1) calls for inter-chunk rhetorical graph construction and 1 call for planning.\nW2. The entire framework's success is predicated on the LLM's ability to function as a high-quality, zero-shot RST parser. This capability is assumed, not proven."}, "questions": {"value": "Q1: Why did the authors not include an intrinsic evaluation of the RST parser against a gold-standard dataset? How can we be confident that the generated structures are faithful and not just plausible-sounding hallucinations that happen to guide the LLM?\nQ2: Did the authors compare the full, complex RST parsing against simpler structural signals? For example, what is the performance if only explicit discourse markers (e.g., \"however\", \"because\", \"in contrast\") are used to build the inter-chunk graph, without any RST tree parsing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "alvEqc4pf0", "forum": "AWv0mlCeUk", "replyto": "AWv0mlCeUk", "signatures": ["ICLR.cc/2026/Conference/Submission3252/Reviewer_kqc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3252/Reviewer_kqc4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924326527, "cdate": 1761924326527, "tmdate": 1762916628947, "mdate": 1762916628947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Discourse-RAG, a retrieval-augmented generation framework that explicitly models intra- and inter-chunk rhetorical structures using Rhetorical Structure Theory (RST) and rhetorical planning to improve coherence and factual consistency in long-context reasoning. It demonstrates strong empirical results on multiple benchmarks (Loong, ASQA, and SciNews) across varying document lengths, outperforming several state-of-the-art RAG baselines. While the approach is somewhat heavy and empirically oriented, its clear performance gains and conceptual novelty justify acceptance, provided reproducibility and efficiency details are strengthened."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of introducing rhetorical trees and inter-chunk discourse graphs into RAG is original and conceptually well-motivated, bridging discourse analysis and generative reasoning.\n2. The method is tested on diverse, long-context benchmarks with detailed ablations and robustness studies (chunk size, Top-k, noise, and structure perturbations), giving credibility to the empirical claims.\n3. Discourse-RAG outperforms strong baselines (StructRAG, MAIN-RAG, RQ-RAG) in both accuracy (LLM Score, EM) and factuality (SummaC, SARI), particularly on large-context and noisy retrieval scenarios."}, "weaknesses": {"value": "1. Both intra- and inter-chunk discourse structures rely on LLM prompting for RST parsing, raising concerns about reproducibility, cost, and stability.\n2. While results show improvements, the paper doesn’t deeply explore why rhetorical modeling helps or how structural cues propagate through the generator beyond surface correlations.\n3. The multi-agent setup (parsing, graphing, planning) introduces significant preprocessing latency and complexity, which may limit real-time or large-scale deployment; no efficiency analysis is reported."}, "questions": {"value": "1. Provide a quantitative evaluation of RST parsing accuracy and its impact on final performance (e.g., noise sensitivity to incorrect discourse trees).\n2. Include a runtime and cost comparison versus other RAG systems (e.g., StructRAG, MAIN-RAG) to demonstrate practical feasibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ldXKBKeLsi", "forum": "AWv0mlCeUk", "replyto": "AWv0mlCeUk", "signatures": ["ICLR.cc/2026/Conference/Submission3252/Reviewer_R11v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3252/Reviewer_R11v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015270652, "cdate": 1762015270652, "tmdate": 1762916628278, "mdate": 1762916628278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Discourse-RAG, a rag framework that explicitly models discourse structures. It works via a three-stage pipeline: 1) constructing intra-chunk RST trees to identify core vs. supporting information, 2) building inter-chunk rhetorical graphs to model relationships, 3) using a discourse-aware planning module to generate a blueprint for the final answer. Experiments on ASQA, Loong, and SciNews benchmarks show that Discourse-RAG achieves strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, and gets strong performance with good baselines.\nThe method can be plugged in any setup without any fine-tuning. \nThe components are ablated."}, "weaknesses": {"value": "There is no analysis on how the method scales (in terms of cost (tokens) and latency) with higher top-k settings."}, "questions": {"value": "What are the tradeoff with offline indexing and creation of the RST trees for the whole dataset?\nHow does the method scale in terms of latency, tokens with respect to higher top-k, different chunk sizes, bigger documents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pUHDLMfhZz", "forum": "AWv0mlCeUk", "replyto": "AWv0mlCeUk", "signatures": ["ICLR.cc/2026/Conference/Submission3252/Reviewer_FdYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3252/Reviewer_FdYo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762879655579, "cdate": 1762879655579, "tmdate": 1762916626431, "mdate": 1762916626431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}