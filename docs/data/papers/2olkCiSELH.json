{"id": "2olkCiSELH", "number": 4522, "cdate": 1757696486601, "mdate": 1759898028371, "content": {"title": "The Geometry and Topology of Circuits: the Manifolds of Modular Addition", "abstract": "The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition.\nIn this work, we show that this is not the case, and that both the Clock and the Pizza circuits implement the same algorithm via topologically and geometrically equivalent representations.\nOur methodology goes beyond the interpretation of individual neurons and weights.\nInstead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity.\nThis method reveals that each learned representation is a manifold that we can study utilizing tools from topology.\nBased on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.", "tldr": "We quantitatively and qualitatively show that the manifolds learned by neural networks trained on modular addition are universally the entire input space manifold or projections of it.", "keywords": ["mechanistic interpretability", "representation learning", "geometry", "topology", "manifolds", "universality", "platonic representation hypothesis"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e47f259c65c453c01827638bd3018e3674387b49.pdf", "supplementary_material": "/attachment/4ee9ee3734913673dd552d5e25a1f1ba4e094949.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates internal representations of networks trained to perform modular addition. Prior work claims that networks either learn a Clock or Pizza circuit. The paper shows that for a number of networks, the manifold at early layers is a torus which slowly morphs to be more circle-like in later layers (including a disk) although not completely a circle. This is shown first theoretically, then empirically through a number of analyses."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper's key strength is the depth of the empirical analysis. The authors consider a number of techniques to probe the representations learned by the networks including PCA, activation strengths, PAD and Betti numbers. This is all done for a number of different architectures. The theoretical analysis also seems sound and is convincing.\n\nOverall, the paper would likely be of significance to those in mechanistic interpretability, particularly those studying modular addition."}, "weaknesses": {"value": "In my view, the paper doesn't have many weaknesses. My main critique is on clarity: sections 4.1 and 5 could benefit from more intuition provided for the theorem and proposed analyses. For instance, it's not immediately obvious what the factorization structure of X in Theorem 4.1 has to do with geometry. Similarly, in the description of PAD, the significance of a strong diagonal is never explicitly explained. I would encourage the authors to use their extra page in the revision to more slowly walk through these harder-to-understand areas of the paper.\n\nMinor points:\n- \\mathbf Z and \\mathbb Z appear to be used inconsistently in places (as well as for T in the appendix)\n- It's unclear why bold font is used for E and Z but not for any other symbols\n- The color scheme for visualizing numbers from 0-58 is not cyclic (it looks like viridis); I recommend using a cyclic color scheme\n- Figure 6 text is a bit small, especially for the legend"}, "questions": {"value": "- Please clarify sections 4.1 and 5\n- See minor points above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QckPuQyLKt", "forum": "2olkCiSELH", "replyto": "2olkCiSELH", "signatures": ["ICLR.cc/2026/Conference/Submission4522/Reviewer_9zL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4522/Reviewer_9zL4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760489341295, "cdate": 1760489341295, "tmdate": 1762917422298, "mdate": 1762917422298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the modular addition benchmark, showing that models previously thought to learn distinct “Clock” and “Pizza” circuits in fact converge to geometrically and topologically equivalent manifolds. Using the simple neuron model (from McCracken et al., 2025), the authors derive closed-form expressions for layer-1 preactivations and prove that under mild assumptions on phase distributions, all networks learn either a disc (vector-addition) or torus manifold. They then verify these findings empirically via Phase Alignment Distributions (PAD) and Betti number analyses across hundreds of networks, concluding that the supposed circuit diversity of Zhong et al. (2023) arises from superficial differences.\nThe work further argues that the universality hypothesis—that similar networks learn similar circuits—remains valid, thus unifying previously divergent mechanistic interpretations of modular arithmetic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Theoretical clarity: Derivation of closed-form preactivation manifolds (Theorem 4.1) provides a clear and elegant link between Fourier structure and representation geometry.\n\nUnified interpretation: The paper offers a coherent view showing that previously “distinct” circuits (Clock vs Pizza) are mathematically equivalent, reinforcing the universality hypothesis.\n\nMethodological novelty: Introduces quantitative topology tools (Betti distributions, PAD + MMD metrics) for large-scale circuit comparison.\nEmpirical rigor: Evaluates 703 trained models across multiple architectures, giving the conclusions strong empirical grounding.\n\nConceptual impact: Connects manifold geometry, phase statistics, and interpretability—potentially inspiring similar analyses for other arithmetic circuits."}, "weaknesses": {"value": "Missing connection to real LLMs: https://arxiv.org/abs/2406.03445 shows that similar Fourier-modular features emerge in large pretrained transformers, yet this paper does not discuss whether such real representations share the same torus or disc topology. Integrating this perspective could help demonstrate broader relevance beyond synthetic tasks.\n\nInsufficient engagement with prior findings: https://arxiv.org/pdf/2311.07568 and https://arxiv.org/pdf/2402.09469 already show that linear superposition of $\\cos a$ and $\\cos b$ can yield $\\cos(a+b)$ without attention and also characterize attention outputs in Fourier space. A deeper comparison or reconciliation with these works would make the analysis more convincing.\n\nLimited methodological novelty: The TDA methods (Betti numbers, persistent homology) are established tools; the contribution mainly lies in their application to this particular setting.\n\nQuestionable realism: The MLP-Concat baseline, while illustrative, trivially produces a torus and is rarely used in realistic architectures. Clarifying its motivation would strengthen the empirical design."}, "questions": {"value": "How does your explanation differ from https://arxiv.org/pdf/2311.07568, which showed that $\\cos(2\\pi f(a+b))$ can emerge from linear mixing of $\\cos a$ and $\\cos b$ with activation function? Maybe activation function in real model can also get  $\\cos(2\\pi f(a+b))$. Hence it may not contradict to the clock algorithm.\n\nIn https://arxiv.org/pdf/2402.09469, attention outputs are sparse in Fourier space—does this align with your observations? Are the attention outputs closer to $\\cos a + \\cos b$ or to $\\cos(a+b)$?\n\nHave you examined whether similar disc or torus topologies appear in pretrained language models, such as those analyzed in https://arxiv.org/abs/2406.03445?\n\nBeyond applying standard Betti-number analysis, what new theoretical insight or interpretability perspective does your TDA pipeline provide?\n\nWhat motivates including MLP-Concat, given that it trivially yields a torus (four degrees of freedom) and is not common in modern architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m0HijZreTn", "forum": "2olkCiSELH", "replyto": "2olkCiSELH", "signatures": ["ICLR.cc/2026/Conference/Submission4522/Reviewer_3muT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4522/Reviewer_3muT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761259623089, "cdate": 1761259623089, "tmdate": 1762917422075, "mdate": 1762917422075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a topological approach to interpretability in modular addition. Under the assumption of simple neurons, it is theoretically established that activations lie on a disc (when phases are in sync) or on a torus (when phases might be independent). The authors then perform experiments to shed light on the geometry of the activations. Based on their empirical analysis, they argue that previously identified mechanisms Pizza and Clock are in fact the same algorithm, restoring the universality hypothesis."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The question of universal representations is important and modular addition is a good setup to study this question.\n- The topological/geometric approach is interesing and sheds light on the representations learned by neural networks.\n- The paper studies a diverse and comprehensive set of four architectures: MLP-add, MLP-concat, Attention 0.0 (transformers with constant attention), Attention 1.0 (transformers with learnable attention).\n- The paper combines theoretical analysis with a comprehensive suite of experiments.\n- The theoretical analysis appears correct (although the assumptions are up for debate)."}, "weaknesses": {"value": "1. The paper uses ambiguous language that might lead the reader to confuse neural network architectures and the mechanisms learned by these networks. For example, in lines 116-118, the terms Clock and Pizza are introduced as two different architectures, when in fact they are different mechanisms learnable by the same architecture (the standard transformer block: MLP + Softmax Attention). In lines 151-154, the authors are making a statement seemingly about archictures, when in reality they are refering to specific learned networks. I would suggest that the authors renouce the term \"architecture\" in favour of just \"network\", while clarifying that they are refereing to a specific mechanism learned by a specific architecture.\n2. The numbers in the terms \"Attention 0.0\" and \"Attention 1.0\" probably reffer to the $\\alpha$ constant introducted by Zhong et al. (2023), but this is not described here. I would recommend renaming \"Attention 0\" and \"Attention 1\" to \"Constant-Attention\" (or \"Uniform-Attention\") and \"Learnable Attention\", respectively.\n3. The use of the letter E in eqs 2 and 3 is confusing by suggesting an embedding vector, or at least an activation, but after reading the paper several times this appears to be just a high level explanation (at least for the clock).\n4. Most importantly, the idea that the clock algorithm can be reduced to the same \"pizza disk\" is very problematic. The Clock algorithm uses the attention mechanism to perform trigonometric operations. However, these operations require a linear attention regime (not softmax). Therefore, it is expected that most attention scores would concentrate closely around zero, where the application of softmax is approximately linear. However, this also implies that the output of attention would be approximately an average sum (as in the pizza), with the entire clock mechanism is encoded in the variance from this average. Therefore, it is not surprising that a PCA of the Clock reveals a pizza disk. However, a closer look at the plots reveals that the Clock PCA is not identical to the Pizza PCA: there is a lot of \"noise\" (hidden information) in the Clock plot. The same can be said about the sum of post-relu activations: we can see in Figure 3 that Attention 1.0 contains an additional (albeit weak) signal that is not present in the Attention 0.0. It is exactly this faint signal that is essential to the Clock algorithm. I strongly suspect that all of these apparant similarities would vanish if we considered the activation vectors projected into the row-space of the unembedding matrix.\n5. The claim that \"the Pizza and the Clock algorithms are actually the same algorithm\" is **misleading**. Setting aside the weaknesses of the empirical analysis, the pizza and the clock are well understood mechanisms that are known to perform different computations. This papers reads to me like an argument that *quick sort* and *merge sort* are actually the same algorithm because they share some similarities. I believe this claim is greatly weakening this paper. The paper would be more interesting if the topological approach was used instead to shed light on the similarities and differences of the clock and the pizza. For example, can we characterize the complete topology of the clock algorithm beyond simple neurons?"}, "questions": {"value": "1. What exactly is meant by pre-activations at layer1? Is it the vectors from eqs 2 and 3?\n2. Exactly what amount of the variance is described by the top-2 principal components for each network?\n3. What do the plots looklike if we were to study the activations projected to the row-space of the unembedding matrix?\n4. What do the 3rd and 4th principal components look like?\n5. How much do the principal components 1-4 align with the unembedding matrix?\n6. Does the torus topology of MLP-cat undermine the universality hypothesis?\n7. Are the authors willing to tone down the claim that \"the clock and pizza are the same algorithm\", for example, to something like \"topological similarities between the clock and the pizza\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YsVl1zAvij", "forum": "2olkCiSELH", "replyto": "2olkCiSELH", "signatures": ["ICLR.cc/2026/Conference/Submission4522/Reviewer_yR58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4522/Reviewer_yR58"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587775317, "cdate": 1761587775317, "tmdate": 1762917421727, "mdate": 1762917421727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}