{"id": "MWIv2vRbE9", "number": 19976, "cdate": 1758301165188, "mdate": 1759897008901, "content": {"title": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training", "abstract": "Vision Mamba (e.g., Vim) has successfully been integrated into computer vision, and token reduction has yielded promising outcomes in Vision Transformers (ViTs). However, token reduction performs less effectively on Vision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a high loss of key knowledge tokens and a drop in performance, making it not a good solution for enhancing efficiency. Token merging, which preserves more token information than pruning, has demonstrated commendable performance in ViTs, but vanilla merging performance decreases as the reduction ratio increases either, failing to maintain the key knowledge and performance in Mamba. Re-training the model with token merging, which effectively rebuilds the key knowledge, enhances the performance of Mamba. Empirically, pruned Vims, recovered on ImageNet-1K, only drop up to 0.9% accuracy, by our proposed framework R-MeeTo in our main evaluation. We show how simple and effective the fast recovery can be achieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17 minutes, and Vim-S only drop 1.3% with 1.2 $\\times$ (up to 1.5 $\\times$) speed up in inference.", "tldr": "Mamba models have different token pruning robustness. Marginal enrichment phenomenon is the key difference. Mamba's SSMs modules keep information in marginal tokens (end of scanning).", "keywords": ["Mamba", "Token Efficiency"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8384c7461cf4487ea921d4d941e9ac3b0d5cd9a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces token merging to Vision Mamba. The core: (1) due to its sequential state space structure, Mamba is more sensitive to token pruning than Transformers; (2) Token merging preserves more key knowledge than pruning; (3) A short retraining phase can effectively restore performance.\n\nAlthough the empirical results are convincing, given recent and highly relevant work, novelty is limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The discussion on token merge and token prunning for Mamba is interesting. This work bring some insights to the efficient vision mamba design."}, "weaknesses": {"value": "The token merging strategy is wildly explore by well-known untrained method on Vision Transformers. Adaptation to Mamba seems like a natural extension. The contribution is limited."}, "questions": {"value": "What`s the difference between a recent works such as \"Training-free Token Reduction for Vision Mamba\", \"Sequential Token Merging: Revisiting Hidden States\", which also merge tokens for mamba. Could the authors illustrate original contribution of this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x19uRqx0zH", "forum": "MWIv2vRbE9", "replyto": "MWIv2vRbE9", "signatures": ["ICLR.cc/2026/Conference/Submission19976/Reviewer_RNij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19976/Reviewer_RNij"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761313630315, "cdate": 1761313630315, "tmdate": 1762932868391, "mdate": 1762932868391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies token reduction for Vision Mamba models. The work is practically motivated and proposes a simple general design that consistently recovers most accuracy while cutting FLOPs and improving throughput. The authors propose R-MeeTo with token merging and then perform a short round of re-training. The empirical study includes Vim tiers and VideoMamba. The theoretical section is provided to offer the intuitive lens  but hinges on strong assumptions and interaction-information manipulations that are not fully justified."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple and easy to follow. The merging and re-training mechanisms are not complex and R-MeeTo yields smaller drops at matched or lower FLOPs. This indicates competitive effectiveness at comparable cost. The experiment tables also show that the throughput improves across GPUs with modest accuracy loss. Settings are detailed and this aids replication.\n2. Experiments are designed across image and video models and results on Kinetics-400 show consistent behavior and FLOP reductions with small drops. Ablation experiment results are relatively abundant.\n3. Visualizations results suggest semantically coherent merges (Figs. 5, 6) improving method intuition to some extent."}, "weaknesses": {"value": "1. Theoretical assumptions and derivations need tighter justification. Assumptions are strong and unvalidated empirically. Consequences for misspecification are not analyzed in Sec. 2.3. This affects technical soundness. The proof of Theorem 1 uses interaction information decompositions with limited rigor. Proposition 2 (no dependency before t) may be an oversimplification for selective SSMs due to the diverse scan order and layer settings of common models such as Vim model. This impacts correctness and clarity.\n2. While pruning baselines are compared, there is no direct comparison against prior merging methods adapted, e.g., ToMe-style variants.\n3. Some experimental designs are not rigorous. For example, ViT's token interaction method is global attention, so the conclusion that shuffle has no effect may not need to be verified.\n4. The writing needs careful proofreading. For example, the abstract sentence \"Our key insight is that a quick round of retraining after token merging yeilds robust results across various compression ratios\" is missing a period, and the typo \"dropa\" is in line 73.\n5. Hypotheses were listed as a main contribution."}, "questions": {"value": "1. How to accurately define the concept of the term \"knowledge\" in the paper?\n2. For other questions, please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KlvUVaf0lX", "forum": "MWIv2vRbE9", "replyto": "MWIv2vRbE9", "signatures": ["ICLR.cc/2026/Conference/Submission19976/Reviewer_bp9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19976/Reviewer_bp9X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927852293, "cdate": 1761927852293, "tmdate": 1762932863572, "mdate": 1762932863572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes R-MeeTo (Re-training Merged Token), a simple and effective framework to accelerate Vision Mamba models through token merging followed by lightweight retraining. Unlike pruning-based token reduction methods that cause severe performance drops due to the sequential dependency and information imbalance inherent to Mamba’s state space structure, R-MeeTo merges similar tokens to preserve key knowledge and then retrains the model for a few epochs to recover lost performance. The paper provides both theoretical analyses based on information bottleneck theory and extensive empirical validation. Experiments on ImageNet-1K show that R-MeeTo can recover up to 35.9% accuracy within minutes of retraining, achieving up to 1.5× inference speedup with minimal accuracy degradation across multiple Vision Mamba variants (Vim-Ti/S/B). Additional tests on VideoMamba confirm the method’s generality and scalability across different hardware and data modalities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and explains a key inefficiency in token pruning for Mamba models.\n\n2. The paper provides a clear information-theoretic analysis.\n\n3. The propsed model achieves better results than the pruning baseline."}, "weaknesses": {"value": "1. I'm concerning that the paper is comparing with only weak baselines. For example, the Transformer baseline is still DeiT with only 81.8% base-sized model accuracy on ImageNet, which is outdated. A reasonable ViT-Base performance on ImageNet should be over 83.0% (e.g., DeiT-III [1]). Does the conclusion \"Mamba is more sensitive that Transformers for pruning\" still hold true for DeiT-III?\n\n2. The scalability is unknown. The paper only conduct experiments with base-level models with less than 100M parameters. Does it still work for larger models? I understand that your baseline architectures Vim, VideoMamba, PlainMamba do not have large models, but actually there have already been many recent studies training Mamba at a larger scale like MambaVision [2] and MambaReg [3]. Intuitively, larger models need more input information so I suspect the token pruning/merging approaches cannot work well on them.\n\n3. A clear computation–accuracy trade-off **across all model sizes** is very important for a paper on token pruning or reduction. Although I did not find such a figure or table in the paper, I roughly estimated the trade-off myself. According to your results, after merging 42% of the tokens, Vim-S achieves only 72.9% accuracy. Assuming FLOPs scale linearly with token length (a common assumption for Mamba), the corresponding FLOPs would be around 2.94G, which is still much higher than Vim-Ti’s 1.45G FLOPs, yet the accuracy is significantly lower (76.1% → 72.9%). This raises a question: why not simply reduce the model size (e.g., use Vim-Ti) to achieve better efficiency at similar or even higher accuracy? Such a trade-off appears much more favorable.\n\n4. Besides the lack of experiments on larger model sizes, another limitation of this paper is that it only focuses on a moderate sequence length. If the image resolution is increased or decreased—resulting in a much longer or shorter sequence length—would the conclusions in this paper still hold? It remains unclear whether the proposed method and its observed trends generalize across different sequence lengths.\n\n5.  The formatting of the paper needs significant improvement. Many tables are floating beside the main text, and the excessive use of \\vspace makes it difficult to visually distinguish tables from the surrounding paragraphs. It appears that the authors did not carefully adjust the layout—the current manuscript looks as if it was forcefully converted from a two-column template to the single-column ICLR format, resulting in poor readability.\n\n[1] Touvron, Hugo, Matthieu Cord, and Hervé Jégou. \"Deit iii: Revenge of the vit.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2022.\n\n[2] Hatamizadeh A, Kautz J. Mambavision: A hybrid mamba-transformer vision backbone[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 25261-25270.\n\n[3] Wang F, Wang J, Ren S, et al. Mamba-Reg: Vision Mamba Also Needs Registers[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 14944-14953."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lFUfvPPhAX", "forum": "MWIv2vRbE9", "replyto": "MWIv2vRbE9", "signatures": ["ICLR.cc/2026/Conference/Submission19976/Reviewer_GQRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19976/Reviewer_GQRu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963935563, "cdate": 1761963935563, "tmdate": 1762932833113, "mdate": 1762932833113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the inference efficiency of Vision Mamba (Vim) models. The authors first make two observations: (1) Vision Mamba models are significantly more sensitive to token pruning than standard Vision Transformers (ViTs), leading to severe performance degradation , and (2) Token merging  preserves more information than pruning, but also suffers from performance drops at high compression ratios.   \n\nBased on these observations, the paper's central claim is that training-free token reduction is not a good solution for Vision Mamba. The proposed method, R-MeeTo (Re-training Merged Token) , is therefore a simple two-step process: (1) apply token merging, and (2) conduct a very fast (minutes-long) re-training phase to \"rebuild the key knowledge\" and recover the lost performance. The paper heavily emphasizes the speed of this re-training step as a primary benefit."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely and practical problem: improving the computational efficiency of Vision Mamba, a promising new architecture.\n\nThe initial diagnosis of the problem is correct. The field has indeed observed that Vision Mamba's sequential nature makes it highly sensitive to token reduction methods designed for ViTs.   \n\nThe proposed solution (merge + fast retrain) is simple and practical, and the reported re-training times (e.g., 5-17 minutes)  are impressive."}, "weaknesses": {"value": "The paper's entire argument and central claim hinge on the assertion that retraining is necessary because \"training-free is not a good solution for... Mamba\". This claim is unsubstantiated and invalidated by the authors' failure to cite or compare against MTR (Mamba Token Reduction) , a SOTA framework specifically designed for this problem and explicitly advertised as \"training-free\".   \n\nInvalidated Conclusion: Because of this omission, the paper fails to prove its central thesis. It has not shown retraining is necessary; it has only shown that its implementation of merging (which might be suboptimal) can be fixed with retraining. It is possible that the training-free MTR  outperforms R-MeeTo (with retraining), which would render this paper's contribution moot."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxQGm4wor0", "forum": "MWIv2vRbE9", "replyto": "MWIv2vRbE9", "signatures": ["ICLR.cc/2026/Conference/Submission19976/Reviewer_wcPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19976/Reviewer_wcPc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240139154, "cdate": 1762240139154, "tmdate": 1762932801058, "mdate": 1762932801058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}