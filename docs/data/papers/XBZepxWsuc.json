{"id": "XBZepxWsuc", "number": 11550, "cdate": 1758201510705, "mdate": 1763729468285, "content": {"title": "From Two to One: Harmonizing Attention and Feature Debiasing for Multivariate Time Series Forecasting", "abstract": "Multivariate time series forecasting (MTSF) models based on Transformers have shown remarkable success in various applications, such as energy management, weather forecasting, and traffic monitoring.\nHowever, due to the complex and intertwined correlations among variates, Transformer-based methods often fail to precisely model the interactions among series, leading to limited performance improvement.\nIn this paper, we rigorously investigate and establish the phenomenon of feature oversmoothing in Transformer-based forecasters through a theoretical analysis.\nTo this end, we then propose \\textbf{FADformer}, a frequency-aware debiasing framework, which harmonizes the low- and high-frequency components of attention and feature maps to capture fine-grained patterns for accurate forecasting.\nSpecifically, we design two plug-and-play modules using the Fourier transformation, where i) AttnDeb rescales high-frequency weights within attention modules to mitigate the low-pass limitation and ii) FeatDeb injects inductive feature bias into residual connections to amplify the important high-frequency signals.\nExtensive experiments on challenging real-world datasets show the superiority of our FADformer over existing state-of-the-art methods, in terms of both forecasting performance and generalization ability.", "tldr": "We propose FADformer, a frequency-aware debiasing framework, which harmonizes the low- and high-frequency components of attention and feature maps to capture fine-grained patterns for accurate forecasting.", "keywords": ["Time Series Forecasting", "Frequency Debiasing"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d0460ac24e4d1515c93ea8aa4eec43f40574785.pdf", "supplementary_material": "/attachment/cddb683ca510edd4dfef8260b9865c53122751ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel attention mecanism for transformer for time series forecasting. Motivated by the empirical observation that attention oversmooth frequences, i.e., acts as a low-frequency filter which might hinder performance, FADformer is proposed with a frequency-aware debiasing module to preserve all the information for forecasting. Large scale experiments are conducted showing the improvement brought by FADformer on common time series forecasting benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Experiments are comprehensive and showcase the benefits of the approach\n- The ETTh2 analysis is simple yet intuitive to show the low-frequency filtering\n- The theoretical analysis is sound\n- The proposed approach is shown to be effective over a wide range of benchmarks and a large ablation study is conducted to confirm its robustness"}, "weaknesses": {"value": "I list below what I believe are weaknesses but I would be happy to get corrected if I misunderstood some parts of the work.\n\n- The observed filtering pattern seems to be related to rank collapse which has been theoretically and empirically studied in prior works [1, 2, 3]. I believe those are important work that are not discussed in the current paper.\n- Notably, Thm 3.2 seems very close to [1, section 2.2] which is not cited.\n- In particular, in [3], the authors study the rank collapse in transformer based models for time series forecasting, and propose using a sharpness-aware optimizer to solve the issue. It would be interesting to add this model as a baseline or at least discuss it given that the proposed approach solves a similar issue (oversmoothing / filtering). \n\nOverall, the proposed approach is interesting and the results showcase its benefits however, there is missing works to be discussed for a better positioning of the paper in the literature.\n\n*References*\n\n[1] Dong et al. Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. ICML 2021\n\n[2] Noci et al. Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse. NeurIPS 2022\n\n[3] Ilbert et al. SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel Wise Attention. ICML 2024ecasting"}, "questions": {"value": "- How does the proposed approach scale with the increase in sequence length and/or horizon?\n- In definition 3.1, multivariate time series are described as independent channels however in practice the features can be correlated (otherwise there would be no need to do multivariate forecasting). Could the authors please elaborate on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WpzfoLrmxV", "forum": "XBZepxWsuc", "replyto": "XBZepxWsuc", "signatures": ["ICLR.cc/2026/Conference/Submission11550/Reviewer_A378"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11550/Reviewer_A378"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775091042, "cdate": 1761775091042, "tmdate": 1762922641427, "mdate": 1762922641427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FADformer, a Transformer-based forecasting model that aims to tackle oversmoonthing and frequency learning bias issue in Transformers (which is introduced in Fredformer, KDD24). The work introduces two plug-in debiasing modules: (i) AttnDeb, which rescales high-frequency attention responses to mitigate the low-pass filtering nature of self-attention, and (ii) FeatDeb, which re-amplifies high-frequency signals in the residual connections to alleviate feature degeneration. The method achieves performance gains on 13 MTSF benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Fair feature learning in the frequency domain is an important research topic in time series forecasting, and the idea of addressing oversmoothing is intuitively appealing.\n\nThe paper is easy to follow and the empirical evaluation is extensive."}, "weaknesses": {"value": "While the paper presents frequency bias as an important observation, very recent works such as Fredformer (KDD’24) and FilterNet (NeurIPS’24) already address selective amplification or reweighting of high-frequency components in Transformers. Also, works like FreDF (ICLR’25) discussed the frequency modeling in the forecasting task. The introduction and related works overlaps with their motivation narratives, but these works and technical differences with them are not sufficiently discussed or contrasted. This makes the motivation feel partially rediscovered rather than newly formulated. In general, the motivation is unclear and seems like this is an incremental work.\n\nThe oversmoothing issue here is closely tied to the spectral imbalance story already explored in the above frequency-aware papers. it is unclear what is fundamentally new compared to prior FFT-based decomposition + reweighting strategies. I remember Fredformer already proposed this fft-ifft backbone with frequency decomposition learning. What are the technically new solution or contributions in this paper?\n\nThe theoretical section argues that effective rank can mitigate degeneracy, but the proposed method relies on FFT-based re-scaling, not directly on the theoretical update rule. The theory supports residual scaling in the abstract, but does not explain why a Gaussian decomposition for attention or a Top-K decomposition for features is the correct or optimal instantiation. The conceptual link between Proposition 3.4 and the implemented modules remains loose. Sometimes Top-K is an empirical way that cannot ensure the selection is always satisfied and easily influenced by noise. How to evaluate its effectiveness?\n\nWhile the authors acknowledge several frequency-domain modeling methods in the introduction, the experimental baselines do not include any of these frequency modeling methods. Most comparisons are made only against common time-domain models (e.g., iTransformer, PatchTST), which directly conflicts with the paper’s claim that time-domain modeling is insufficient. Given that the proposed motivation closely aligns with Fredformer, including at least it or more representative frequency modeling baselines is essential for a fair and convincing evaluation.\nMoreover, a deeper ablation (e.g., per-frequency reconstruction error, variance of gradients across layers) would help clarify the real causal effect."}, "questions": {"value": "Please kindly refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SG5q0jwv2J", "forum": "XBZepxWsuc", "replyto": "XBZepxWsuc", "signatures": ["ICLR.cc/2026/Conference/Submission11550/Reviewer_92CY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11550/Reviewer_92CY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794170698, "cdate": 1761794170698, "tmdate": 1762922640548, "mdate": 1762922640548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FADformer, a Transformer-based framework that introduces two frequency-aware debiasing modules (AttnDeb and FeatDeb) to mitigate oversmoothing in multivariate time series forecasting. The method combines Fourier-based reweighting of attention and feature components and is supported by a theoretical discussion using effective rank analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well organized, making it easy to follow.\n2. The topic of addressing frequency bias and oversmoothing in Transformer-based time series models is timely and aligns with current trends in time-series representation learning."}, "weaknesses": {"value": "1. In the Related Work section, the paper lacks an in-depth discussion of existing studies on closely related topics, such as Fredformer and Amplifier. Although the authors briefly mention Fredformer, they fail to provide a clear and insightful comparative analysis. As for Amplifier, it is not mentioned at all, which reflects an insufficient literature review and a lack of thorough investigation into this topic by the authors.\n\n2. This paper belongs to the category of frequency-domain models, yet the experimental section lacks comparisons with other frequency-domain baselines.\n3. Regarding Table 2, the performance improvement brought by Debiasing is not significant.\n4. Line 418: the reference to Table 5 is incorrect; it should be Table 4.\n5. Lines 427–428: In the statement “First-K defines the first K lowest elements of the Fourier transform as low-frequency components of features,” — it is unclear what “lowest” specifically refers to."}, "questions": {"value": "1、Comments on Figure 1:\n- (1) The upper figure of Figure 1(a) does not provide any meaningful insight.\n- (2) The phenomena illustrated by the lower two subfigures of Figure 1(a) have already been investigated in the Amplifier[1] paper.\n- (3) Is the situation shown in the lower two subfigures of Figure 1(a) exclusively caused by the self-attention mechanism?\n- (4) For Figure 1(b), please clarify which Transformer-based forecaster was used in the visualization experiment.\n- (5) The conclusion “Figure 1(b) suggests that the correlations predicted by Transformer-based forecasters are mainly concentrated on and near the diagonal, where there is a substantial portion of the low-frequency characteristics” does not make sense: First, the correlations on the diagonal are self-correlations (a variable with itself), which are always equal to 1.000 and thus irrelevant to the topic discussed in this paper. Second, the claim that the correlations are near the diagonal cannot be reasonably inferred from the figure.\n\n2、Line 016–017: “Transformer-based methods often fail to precisely model the interactions among series” — What is the specific experimental or theoretical evidence supporting this statement?\n\n3、Definition 3.3 (Effective Rank) appears to be a direct copy of Definition 3.1 (Effective Rank) from CONTRANORM[2] (ICLR 2023). Is such a practice acceptable? Similarly, Equation (4) in this paper is almost identical to Equation (8) in CONTRANORM, and Proposition 3.4 closely resembles Proposition 1 from the same work. These similarities raise serious concerns about the theoretical contribution and originality of this paper.\n\n4、Regarding Figure 3 (The Architecture of FADformer), I have two questions:\n- (1)\tAttnDeb separates the attention map into low-frequency and high-frequency components. In FADformer, should other neural network components—such as Linear layers or MLP modules—also undergo a similar separation into low- and high-frequency parts?\n- (2)\tFeatDeb obtains low- and high-frequency components through spectral truncation. Why doesn’t AttnDeb adopt this straightforward and intuitive approach as well?\n\n\n[1] Amplifier: Bringing Attention to Neglected Low-Energy Components in Time Series Forecasting (AAAI 2025)\n\n[2] CONTRANORM: A CONTRASTIVE LEARNING PERSPECTIVE ON OVERSMOOTHING AND BEYOND (ICLR 2023)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9j85EYXOh", "forum": "XBZepxWsuc", "replyto": "XBZepxWsuc", "signatures": ["ICLR.cc/2026/Conference/Submission11550/Reviewer_oSXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11550/Reviewer_oSXr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840571822, "cdate": 1761840571822, "tmdate": 1762922639891, "mdate": 1762922639891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the tendency of Transformer-based forecasters to oversmooth temporal and cross-variate information, offering both a theoretical explanation and empirical evidence. To address this, the authors propose two lightweight frequency-aware debiasing modules—one applied to attention and one to residual features—and integrate them into a general Transformer backbone. The resulting model exhibits consistent improvements across standard multivariate forecasting benchmarks and can also serve as a plug-in for several existing architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear characterization of oversmoothing in Transformer-based forecasters, supported by both theoretical analysis and intuitive visualizations.\n2. The proposed modules are simple, lightweight, and easy to integrate into existing architectures without altering their design, which makes the approach practical.\n3. The plug-in results on TimeBridge, TQNet, and Leddam show that the idea generalizes reasonably well beyond the chosen backbone."}, "weaknesses": {"value": "1.  Several strong recent baselines are missing, especially newer frequency-based or mixture-of-experts models, which makes it harder to gauge the relative improvement. Our analysis experiments already include TimeBridge and TQNet. Would it be possible to include performance comparisons against these models in the main results, along with CycleNet, SparseTSF, and other related methods?\n\n2. Many improvements over reported baselines are <1%. Without confidence intervals or repeated runs, the strength of the claim is hard to judge. Could we add more datasets or benchmarks to validate further the effectiveness of our experiments, such as GIFT-Eval?\n\n3. The theoretical analysis is helpful but does not directly justify the specific design choices (e.g., Gaussian kernel, Top-K amplitude selection), which makes the connection between theory and the final modules somewhat loose.\n\n4. While the paper emphasizes oversmoothing as a primary limitation, the empirical section does not isolate whether reducing oversmoothing itself is responsible for the observed gains."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ot1zh4Bpez", "forum": "XBZepxWsuc", "replyto": "XBZepxWsuc", "signatures": ["ICLR.cc/2026/Conference/Submission11550/Reviewer_eEMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11550/Reviewer_eEMP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982103104, "cdate": 1761982103104, "tmdate": 1763012846970, "mdate": 1763012846970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}