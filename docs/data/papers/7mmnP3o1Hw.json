{"id": "7mmnP3o1Hw", "number": 8865, "cdate": 1758100311473, "mdate": 1762927302086, "content": {"title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting", "abstract": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.", "tldr": "We propose a novel training framework for zero-shot image matching based on 3DGS.", "keywords": ["image matching", "gaussian splatting", "zero-shot generalization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4d73f4fc662e240e34e56ec3c3e7cc70371141e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes MatchGS, which leverages 3D gaussian splatting (3DGS) to generate pseudo-labels for zero-shot image matching. The key idea is to first reconstruct the images using 3DGS, then remove outliers in the corresponding generation and render image pairs with matching pseudo-labels for training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using 3DGS to generate image matching pseudo-labels is interesting.\nThe paper writing is clear."}, "weaknesses": {"value": "1. The method largely follows the experiment setup of GIM, however, unlike GIM where the same method is applied to both sparse, semi-dense and dense matching methods, MatchGS only applies the proposed data to semi-dense methods.\n- This raises the problem of generalization, since theoretically I dont see why the generated pseudo-labels cannot be applied to other method types, e.g., GIM only has sparse/semi-dense labels, it still can be applied to improve dense methods.\n- This also raises the concern of the performance, since the current SOTA matching methods are all dense methods, the limited applicability also indicates that the proposed method might not be able to push the real SOTA.\n\n2. Even in the semi-dense domain, the proposed method did not surpass the performance of GIM (see GIM_loftr and MatchGS+loftr) in table 3, which raises further concerns on the actual benefit given that GIM pipeline is much simpler (without the need of 3D reconstruction)."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IZdtSscu8j", "forum": "7mmnP3o1Hw", "replyto": "7mmnP3o1Hw", "signatures": ["ICLR.cc/2026/Conference/Submission8865/Reviewer_JnBE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8865/Reviewer_JnBE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522826154, "cdate": 1761522826154, "tmdate": 1762920628505, "mdate": 1762920628505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "uO7wc34muQ", "forum": "7mmnP3o1Hw", "replyto": "7mmnP3o1Hw", "signatures": ["ICLR.cc/2026/Conference/Submission8865/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8865/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927300754, "cdate": 1762927300754, "tmdate": 1762927300754, "mdate": 1762927300754, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use gaussian splatting networks to create unlimited amounts of paired correspondence data for training.\n\nThey first show that they can get good performance in comparison to other methods and then explore how the number of scenes impacts the results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes an interesting and scalable way to obtain paired data for correspondence training. In particular, they propose to use gaussian splatting in order to create high quality synthetic pairs and want to demonstrate that this means they have unlimited data with which to train a downstream model.\n\nThey compare a model trained with their approach to other methods for 2D correspondence, showing they get good results on 0-shot generalisation using MegaDepth and the ZEB dataset.\n\nIn order to leverage gaussian splatting, they train a model to give more robust correspondences and be robust to the noise that can be introduced by such an approach."}, "weaknesses": {"value": "1. This paper does not quite fulfill its aims. The main aim is to see how using synthetic data can lead to unlimited data that can be used to demonstrate the value of such data. However, they generally seem to have fixed the amount of data to 70/245 scenes in Table 4. instead, it would be better to show data scaling -- how much do things improve as more and more scenes are sampled as opposed to a relatively small amount of 245 and only looking at two points. We would want to see that things continue to improve as the dataset gets bigger and bigger and we would a higher granularity to be able to see how quick gains are and how they plateau as we saturate at some data amount.\n\n2. While they do a bit better than other semi-dense methods in the ZEB setup -- it is not consistent. And given that their point is the utility of their synthetic data -- I think they should have a graph showing how the amount of data impacts performance and then the performance of other methods to make it clear whether, with more data, we expect their setup to do consistently better or not. Finally, it would be interesting to understand if the improvements are the new points they find or the method -- could they train LoFTR / ELoFTR on their data and then compare results to what they have ?\n\n3. The paper is not very clear for someone not familiar with the area\na.  -- why do they ignore dense methods -- I assume because they are slower but there is no computational comparison to be very convincing. \nb. -- What is the AUC over in Table 2 / 3 -- I think pose but again not said ?\n\nGiven these two substantive weaknesses, I am not convinced that this paper is ready for acceptance."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xkEHfWxMlG", "forum": "7mmnP3o1Hw", "replyto": "7mmnP3o1Hw", "signatures": ["ICLR.cc/2026/Conference/Submission8865/Reviewer_YAC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8865/Reviewer_YAC1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759542718, "cdate": 1761759542718, "tmdate": 1762920628196, "mdate": 1762920628196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of sparse matching datasets, which limits generalizability and restricts viewpoint distribution. They solve this by leveraging 3D Gaussian Splatting to render images from arbitrary user-specified camera trajectories without requiring hand-crafted annotations or sensor data. Through 3D-2D consistency matching, they enhance the matching network and demonstrate improved zero-shot matching performance compared to existing matchers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Utilizing 3D Gaussian Splatting to overcome the limitation of restricted camera trajectories in existing datasets is an interesting direction.\n- To ensure geometric consistency in 3D Gaussian Splatting, introducing depth loss and planar Gaussians that enforce consistent geometry for matching is an intuitive and effective approach.\n- By leveraging 3D Gaussians, they achieve additional 2D-3D consistency matching and propose injecting 3D information into the model."}, "weaknesses": {"value": "### Weaknesses:\n\n- **Unclear parameter ranges for camera perturbations:** What are the specific ranges of $\\Delta R$ and $\\Delta t$, and what scale is applied to the intrinsics? If these ranges are small, the claim that this dataset generation pipeline can produce extreme viewpoint changes may be overstated.\n- **Concerns about 3DGS rendering quality:** I am curious about the rendering quality of the 3D Gaussian Splatting. To my knowledge, depth loss is sufficient in few-shot settings but may be inadequate for full-sequence settings because depth foundation models do not provide perfect depth estimates, and scale ambiguity can become problematic when processing full image sequences [1,2,3]. If the rendering quality is low, using synthetic datasets might be a more feasible approach for generating extreme viewpoint changes.\n- **Potential for mixed dataset training:** I wonder whether there is a performance improvement when mixing the original dataset with your generated dataset. While I understand the table shows results for fair comparison, it would be valuable to see these results. Additionally, if performance improves with your dataset, would this approach also benefit dense matchers such as DKM[4] or RoMa[5], which show superior results compared to semi-dense matching methods?\n- **Comparison with other augmentation methods:** How does this dataset generation approach compare to other augmentation paradigms[6] that create extreme viewpoint changes? Have alternative methods been explored that might yield better results?\n\n### Minor Weakness:\n\n- **Clarity of Equation (8):** The notation in Equation (8) is difficult to understand. Revising the q_{i}^{A,B} notation would improve readability.\n\n### References\n---\n[1] Li, Jiahe, et al. \"Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024.\n\n[2] Zhang, Chi, et al. \"Hierarchical normalization for robust monocular depth estimation.\" Advances in Neural Information Processing Systems 35 (2022): 14128-14139.\n\n[3] Chung, Jaeyoung, Jeongtaek Oh, and Kyoung Mu Lee. \"Depth-regularized optimization for 3d gaussian splatting in few-shot images.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024.\n\n[4] Edstedt, Johan, et al. \"DKM: Dense kernelized feature matching for geometry estimation.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[5] Edstedt, Johan, et al. \"Roma: Robust dense feature matching.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024.\n\n[6] Ma, Jiahao, et al. \"Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction.\" *arXiv preprint arXiv:2506.23863* (2025)."}, "questions": {"value": "Questions are listed in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3vokCiRtE5", "forum": "7mmnP3o1Hw", "replyto": "7mmnP3o1Hw", "signatures": ["ICLR.cc/2026/Conference/Submission8865/Reviewer_Kpbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8865/Reviewer_Kpbb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987342332, "cdate": 1761987342332, "tmdate": 1762920627857, "mdate": 1762920627857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MatchGS, which aims to leverage 3DGS for generating data for image matching. MatchGS first adopts plane-based gaussians with additional depth regularization to learn geometrically consistent gaussians, and renders the scene in multiple views to generate image pairs with projected 3D gaussians serving as GT correspondence. Furthermore, the paper extends upon ELoFTR and LoFTR, by incorporating the information from 3D gaussian primitives. This is done in both coarse and fine levels, where the coarse level alignment aligns 2D features from the matching network and 3D features from a PointTransformerV3, while fine alignment adds a decoder head to predict the gaussian primitives from a local patch from the feature map."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a self-supervised method, based on 3D gaussians, to generate data for training LoFTR-like models. The core motivation in this, being generating data in multiple views with gaussians for zero-shot performance, is very convincing.\n\n2. The paper shows clear gains over LoFTR and ELoFTR despite being in a zero-shot setting, showing the effectiveness of proposed 3DGS-based learning of image matching."}, "weaknesses": {"value": "1. The coarse and fine alignment strategy, despite being claimed complementary (L.265) does not show complementary results at all. In fact, the results for fine-level alignment drops in all cases and the method does not seem to incorporate it in its final design. If so, why is it introduced in the methodology section as a component?\n\n\n2. Frankly, the paper is quite hard to follow. At first glance, it is unclear that how the GT correspondences are established for the generated data, where the reviewer assume the projected gaussian centers in two images to serve as the GT. The framework also has many components and loss functions, most of them only having brief justification and missing details."}, "questions": {"value": "1. Although the authors find the gain to be \"underwhelming\" for extra sampling with the 2x results, it actually seems to be a quite significant margin. It is understandable that further scaling the number of samples can be limited by the storage that the data takes, but showing more scalability with the samples would greatly highlight the core motivation for the data generation pipeline. Could the authors show additional results with more samples, perhaps with some ways to reduce the storage usage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfOgRXhfTX", "forum": "7mmnP3o1Hw", "replyto": "7mmnP3o1Hw", "signatures": ["ICLR.cc/2026/Conference/Submission8865/Reviewer_MX7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8865/Reviewer_MX7F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998315350, "cdate": 1761998315350, "tmdate": 1762920627520, "mdate": 1762920627520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}