{"id": "fZ64NwiBpt", "number": 20539, "cdate": 1758307219583, "mdate": 1759896972335, "content": {"title": "Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining", "abstract": "Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal—URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.", "tldr": "", "keywords": ["LLM pretraining", "efficient LLMs", "metadata"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27a47c4f1f1ef18cff4ac184e57570ccb54e5e53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- It is widely believed that prepending metadata helps LLMs learn latent cluster structure during pre-training; however, evidence beyond URL tags has been limited.\n- The authors systematically investigate which metadata works best and find that fine granularity is key to accelerating LLM pre-training.\n- Specifically, they compare several kinds of metadata (URL; coarse- vs. fine-grained quality scores and domain labels) and contrast prepending with appending.\n- They show that fine-grained metadata yields larger gains than coarse-grained metadata, and that appending can also accelerate training.\n- To probe the mechanism, they further analyze URL metadata: they observe an “attention sink” toward the URL prefix, yet the literal prefix content itself is not important.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- They compare metadata across five configurations (varying granularity and including non-URL metadata) and evaluate both prepending and appending strategies.\n- Their experimental analysis is multifaceted:\n  - (i) They evaluate a broad suite of benchmarks;\n  - (ii) They explore combinations of different metadata types;\n  - (iii) They measure probing accuracy for latent cluster prediction;\n  - (iv) They analyze attention scores and distances between attention patterns;\n  - (v) They report perplexity and gradient norms.\n- It is also interesting that QS-coarse model outperforms QS-fine model when the task is irrelevant to metadata. \n\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "weaknesses": {"value": "- The motivation for studying metadata granularity could be clarified further; otherwise, it risks seeming trivial to prefer fine-grained metadata whenever available. (See the question section.)\n- The discussion in lines 347–353 may already address this concern; if so, please make this explicit—e.g., by emphasizing the key argument—and, if possible, provide any additional supporting rationale.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "questions": {"value": "- Isn’t it trivial to prefer fine-grained metadata? Did you consider hypotheses under which (i) granularity does not matter, or (ii) coarse metadata might be preferable for some reason?\n- It appears that [1] (which you cite in line 106) also compares metadata at different granularities—they state, “We compare the results by varying the depth of prepended metadata…” in Section 3. Do you see substantive similarities between your results and theirs?\n- Can we expect that using both \"prepending\" and \"appending\" in the same training sequence would further boost pre-training?\n\n[1] Higuchi, R., Kawata, R., Nishikawa, N., Oko, K., Yamaguchi, S., Kobayashi, S., ... & Suzuki, T. (2025). *When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars.* arXiv:2504.17562.\n\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ot4Ee842zJ", "forum": "fZ64NwiBpt", "replyto": "fZ64NwiBpt", "signatures": ["ICLR.cc/2026/Conference/Submission20539/Reviewer_7BEe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20539/Reviewer_7BEe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392986946, "cdate": 1761392986946, "tmdate": 1762933959143, "mdate": 1762933959143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic empirical study of how different metadata types (URL, quality, domain) influence LLM pretraining.\n\n\nThrough controlled pretraining runs and a suite of probing and attention analyses, the paper finds that fine-grained metadata consistently accelerates learning and shapes latent representations, while coarse metadata contributes little.\n\nNotably, URL metadata enhances stylistic and quality-related features but also introduces attention-sink behavior.\n\nThe work further explores “learnable meta tokens” as a self-organizing latent conditioning mechanism.\n\nThe study is systematic, clearly written, and practically relevant to LLM data curation pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a broad and well-controlled comparison across five metadata types and multiple usage paradigms (prepending, appending, prediction).\n\n2. Demonstrates that fine-grained metadata yields measurable pretraining efficiency gains, informing real-world LLM data curation.\n\n3.  Includes diverse diagnostic views: loss curves, gradient stability, attention visualization, and probing of latent representations.\n\n4. Writing and figures are clear; each section presents concrete “observations” that summarize key findings."}, "weaknesses": {"value": "1. Limited robustness under real web-scale noise.\n\n    - The paper’s findings rely on moderately curated corpora (e.g., FineWeb-Edu), where metadata fields are clean and semantically aligned with the text.\n    - On truly raw web data, where a large fraction of pages contain boilerplate, encoding errors, or misaligned metadata, the assumed correlation between metadata and content weakens.\n\n2. The learnable meta-token experiment is under-specified and likely reflects optimization or statistical effects rather than true semantic abstraction.\n    - Since the tokens are inserted randomly and unsupervised, any observed clustering by “quality” could simply result from correlations between quality scores and superficial statistics such as document length or domain frequency, rather than genuine latent metadata inference.\n\n3. Potential artifact in attention-sink analysis.\n    - The paper attributes performance differences to an “attention sink” on URL prefixes, but the phenomenon is likely positional rather than semantic.\nBecause all metadata are prepended, prefix tokens naturally dominate early-layer attention regardless of content.\nAn append or randomized-position control would likely remove this effect. Without such controls, the claim remains unsubstantiated."}, "questions": {"value": "1. Lack of quantitative definition of metadata informativeness\n    - The paper repeatedly claims that fine-grained metadata is more beneficial than coarse-grained metadata, yet this distinction is treated qualitatively. There is no quantitative measure of metadata informativeness (e.g., entropy, number of distinct buckets, mutual information with the text, or token-level perplexity gain).\nWithout defining an “information budget,” it is difficult to generalize the conclusion or to predict when a given metadata type will be helpful. A systematic analysis relating metadata information content to training acceleration would make the claims much stronger.\n\n2. Over-smoothed attention analysis\n    - The attention analysis reports only average attention weights aggregated across all heads and layers. Such averaging may mask the presence of a few specialized heads that truly focus on useful metadata components (e.g., URL domain), while most others attend to superficial tokens like the prefix.\nA head-wise or layer-wise breakdown would clarify whether meaningful metadata utilization emerges in specific submodules rather than being uniformly weak across the network. As it stands, the conclusion that “prefix attention is a sink and unhelpful” may be an artifact of excessive averaging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ePjhNBqjF4", "forum": "fZ64NwiBpt", "replyto": "fZ64NwiBpt", "signatures": ["ICLR.cc/2026/Conference/Submission20539/Reviewer_9nY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20539/Reviewer_9nY6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796459030, "cdate": 1761796459030, "tmdate": 1762933958839, "mdate": 1762933958839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is a thorough study on the effects of conditioning *or* predicting metadata for pre-training documents. By means a number of controlled pre-training runs, the authors study the effects of different types of metadata and contrast conditioning and predicting. The main finding is that more granular data leads to more benefits. The paper proceeds to consider attention sinks and probing of internal states to understand the representational benefits of metadata learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper presents a comprehensive analysis around a relatively understudied type of pre-training technique, metadata conditioning.\n* The paper introduces a new technique, metadata prediction, and finds that it also provides benefits.\n* The pre-training scale of the paper (1.5B runs with up to 100B tokens) is quite extensive.\n* The paper takes a first step to build a more mechanistic understanding of the benefits of metadata conditioning by probing hidden representations."}, "weaknesses": {"value": "* While the paper adds more evidence to Observation 1 (need for fine-grained granularity), the hypothesis was already formed and supported by some ablations by Gao et al., Metadata Conditioning Accelerates Language Model Pre-training.\n* The paper should attempt to quantify the variance in the pre-training results and evaluations. I am little skeptical that the takeaways are all statistically significant. Specifically, the results in Figure 3 are worrying, since information-theoretically, prepending two types of information should yield similar benefits. (Unless it leads to substantially fewer \"predicted\" tokens during training)\n* The insights in Observations 2, 3, and 4 are interesting, but rather specific and anecdotal, such that the wider relevance and applicability of these insights is not clear to me. The paper also makes limited progress in my opinion towards a foundational explanation of why metadata conditioning leads to improved pre-training results."}, "questions": {"value": "Do you think the attention sink effect of metadata conditioning is an important benefit of the technique?\n\nCan metadata conditioning and metadata prediction be combined to yield complementary benefits (for two different types of metadata information)?\n\nWhat would be the recommendation of the paper with regards to best practices for metadata conditioned pre-training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DfqSwz0yQo", "forum": "fZ64NwiBpt", "replyto": "fZ64NwiBpt", "signatures": ["ICLR.cc/2026/Conference/Submission20539/Reviewer_1P2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20539/Reviewer_1P2e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965692920, "cdate": 1761965692920, "tmdate": 1762933958500, "mdate": 1762933958500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the prior work on using URLs to accelerate pre-training to add additional metadata information. The experiments are with a 1.5B LLaMA model on FineWeb-Edu (which I think the authors should amend the abstract to mention upfront). They also study whether it's better to append or prepend the metadata. Both provide acceleration but it seems like appending might be better for building a more rich representation space. They abstract away concrete metadata entirely by providing learnable meta-tokens that have no semantic meaning but encode quality-related structures in the attention patterns."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The space of data augmentation via metadata is underexplored and quite promising for accelerating pre-training with negligible extra computational cost. \n2. It is interesting to see more interpretability analyses on what metadata does in the model. The connection to attention sink is especially interesting.\n3. Experiments and ablations are run well and conducted thoroughly. The paper is written well and easy to understand."}, "weaknesses": {"value": "There is a lot of speculation around what the metadata does and it does not have clear grounding in empirical results. First, the optimization speedup is hard to understand and isn't described quantitatively. Figure 7 provides little insight into it -- I am especially unsure what to take away from the gradient norm, given the lack of other information (gradient moment estimates, update norms, etc). \n\nThere are also not enough hyperparameter ablations (I know they are expensive) to draw clean conclusions about the benefit of metadata wrt optimization. For example, Section 4.2 speculates about a soft regularization but it is hard to understand what that truly corresponds to -- for example, is it an optimization or generalization benefit? Both? The text is too vague on this point and I think there are not enough experiments to make claims of this type.\n\nThe interpretability studies are interesting but done too coarsely (eg averaged across all heads and layers). The authors may want to expand the appendix to detail more information for this to be a useful interpretability study to others."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7DvLMUyEXz", "forum": "fZ64NwiBpt", "replyto": "fZ64NwiBpt", "signatures": ["ICLR.cc/2026/Conference/Submission20539/Reviewer_QQyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20539/Reviewer_QQyK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201218142, "cdate": 1762201218142, "tmdate": 1762933958101, "mdate": 1762933958101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}