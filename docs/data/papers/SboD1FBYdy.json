{"id": "SboD1FBYdy", "number": 19197, "cdate": 1758294304943, "mdate": 1759897052589, "content": {"title": "InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning", "abstract": "Multimodal large language models (MLLMs) have shown remarkable potential in various domains, yet their application in the medical field is hindered by several challenges. General-purpose MLLMs often lack the specialized knowledge required for medical tasks, leading to uncertain or hallucinatory responses. Knowledge distillation from advanced models struggles to capture domain-specific expertise in radiology and pharmacology. Additionally, the computational cost of continual pretraining with large-scale medical data poses significant efficiency challenges. To address these issues, we propose InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs designed to deliver state-of-the-art performance in medical applications. We combined high-quality general-purpose and medical multimodal data and proposed a novel five-dimensional quality assessment framework to curate high-quality multimodal medical datasets. We employ low-to-high image resolution and multimodal sequence packing to enhance training efficiency, enabling the integration of extensive medical data. Furthermore, a three-stage supervised fine-tuning process ensures effective knowledge extraction for complex medical tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B and MedGemma-27B-IT, demonstrating superior performance in medical visual question answering and diagnostic tasks. By addressing key challenges in data quality, training efficiency, and domain-specific knowledge extraction, our work paves the way for more reliable and effective AI-driven solutions in healthcare.", "tldr": "InfiMed-Foundation models advance medical multimodal learning with compute-efficient pre-training and multi-stage fine-tuning, achieving superior performance in medical VQA and diagnostics.", "keywords": ["MLLM", "Medical AI", "Foundation Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c785ee50fe754c95bf1af55924d99701b5710880.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents InfiMed-Foundation, a family of multimodal LLMs in the medical domain with a multi-stage supervised fine-tuning pipeline and compute-efficient pre-training. Using a five-dimensional quality framework, the authors curate a sizable medical multimodal dataset and integrate low-to-high-resolution training and multimodal sequence packing. MedEvalKit experiments demonstrate competitive performance when compared to open-source medical MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. A reasonably comprehensive engineering pipeline integrating existing data curation and training strategies.\n2. Competitive results against some open-source baselines and modest compute-efficiency considerations.\n3. Clear exposition and organized presentation."}, "weaknesses": {"value": "1. The methodological novelty is limited. The proposed pipeline primarily recombines established techniques such as continual pretraining, staged SFT, etc. The overall contribution feels like a well-engineered extension of existing systems (e.g., LLaVA-Med, MedGemma, Lingshu, Open-Qwen2VL) rather than a conceptually new framework.\n2. Although the proposed models demonstrate competitive performance, they do not achieve state-of-the-art results across all benchmarks. For end users, especially in medical AI applications, the primary selection criterion is performance rather than model scale or training cost, since most practitioners deploy pre-trained medical MLLMs rather than retraining them. As a result, despite training efficiency, the practical impact may be limited if users continue to choose higher-performing models (e.g., GPT-5 or Lingshu-7B). In other words, being parameter-efficient does not compensate for not being the top performer, and the model’s advantage may not translate into real-world adoption.\n3. The performance gains on medical VQA tasks, while encouraging, do not clearly show superiority across diverse medical reasoning benchmarks. The paper does not include clinical report generation, temporal reasoning, radiology workflow tasks, medical conversation benchmarking, or structured knowledge grounding—limiting the claim of broad capability improvements.\n4. The model remains weak on challenging tasks like MedXVQA (21.9%), and the evaluation focuses largely on VQA, without broader clinical tasks such as report generation or real-world clinical validation."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c5To5vuUlT", "forum": "SboD1FBYdy", "replyto": "SboD1FBYdy", "signatures": ["ICLR.cc/2026/Conference/Submission19197/Reviewer_P1Qy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19197/Reviewer_P1Qy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802401281, "cdate": 1761802401281, "tmdate": 1762931194989, "mdate": 1762931194989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific multimodal large language models (MLLMs) designed to address challenges in applying general-purpose MLLMs to medical domains. The models are evaluated on the MedEvalKit framework, showing InfiMed-Foundation-4B outperforms comparable open-source models and approaches proprietary systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a five-dimensional quality assessment framework for curating high-quality medical datasets.\n2. The paper presents a compute-efficient pre-training strategy using multimodal sequence packing and reduced image patches.\n3. The InfiMed-Foundation-4B outperforms comparable open-source models and approaches proprietary systems."}, "weaknesses": {"value": "1. The proposed five-dimensional quality assessment framework lacks novelty and insight. Multiple prior works have already proposed multi-dimensional frameworks for evaluating data quality (e.g., data-centric AI literature, medical dataset curation papers). The authors fail to clearly articulate what is fundamentally new or superior about their specific five dimensions compared to existing frameworks. This contribution appears incremental rather than innovative.\n2. Using adaptive average-pooling to reduce token count is well-established and not novel. More critically, the authors provide no comparison with state-of-the-art token reduction methods such as: Q-Former (BLIP-2)，Perceiver Resampler (Flamingo).\n3. Table 2 shows InfiMed-Foundation's performance against open-source medical MLLMs is underwhelming: Significantly trails Lingshu-7B on multiple benchmarks (SLAKE: 77.7% vs 83.1%)"}, "questions": {"value": "1. The paper fails to establish a clear competitive advantage. Neither the technical innovations nor the model performance are sufficient for a high-impact publication at ICLR. The authors need to clearly articulate what fundamental contribution justifies acceptance at a top-tier venue.\n2. Mixing general-domain data with medical data is not novel. The three-stage training pipeline closely resembles existing medical MLLMs, particularly Lingshu, without demonstrating clear technical differentiators or superior performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7SJh07Tymz", "forum": "SboD1FBYdy", "replyto": "SboD1FBYdy", "signatures": ["ICLR.cc/2026/Conference/Submission19197/Reviewer_qjLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19197/Reviewer_qjLD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929422542, "cdate": 1761929422542, "tmdate": 1762931194566, "mdate": 1762931194566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce InfiMed-Foundation-4B and InfiMed-Foundation-1.7B, a family of multimodal LLMs trained for medical specialization. Starting from  The authors propose a multi-stage domain adaptation approach,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The InfiMed models resulting from their training approach generally show good downstream performance on various medical VQA datasets, often comparable to or better than the performances of larger models (i.e., more parameters).\n- The writing is generally well-written and easy to follow."}, "weaknesses": {"value": "- The proposed multi-stage pretraining and fine-tuning approach (i.e., the training recipe) is not really novel. Modulo relatively minor differences, the multi-stage training approach is analogous to those initially proposed in prior works on visual instruction tuning [1,2,3] and medical adaptation of VLMs [4,5,6,7] (which also include approaches that mix general-domain and medical SFT datasets to mitigate catastrophic forgetting). In fact, many of the general-domain and medical adaptation datasets appear to be coming from these papers. As such, the contributions to the broader ML and ML for health communities appear limited.\n- The paper frequently claims that the proposed approach is more computationally efficient, but there are no clear baselines to which their approach is being compared and no clear quantification or description of improvements in computational efficiency, and the results almost entirely focus on the downstream performance of the model. The most obvious efficiency gains appear to result from the sequence packing approach (to avoid wasting compute on padding tokens) and the the reduction in the number of image tokens, but neither approach are technically novel.\n- It is not obvious how much impact the data quality evaluation step (Section 3.2) has on downstream performance. Ideally, the authors should quantify how much downstream performance improvement actually resulted from this filtering process via dataset ablations, although I understand that this can be computationally challenging. Nonetheless, I think it should be noted a limitation.\n- Limitations of the study are not discussed.\n\nReferences:\n[1] Visual Instruction Tuning (Liu et al., 2023)\n[2] Improved Baselines with Visual Instruction Tuning (Liu et al., 2024)\n[3] InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Dai et al., 2023)\n[4] LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day (Li et al., 2023)\n[5] MediTron-70B: Scaling Medical Pretraining for Large Language Models (Chen et al., 2023)\n[6] Med42-v2: A Suite of Clinical LLMs (Christophe et al., 2024)\n[7] MedFlamingo: A Multimodal Few-shot Learner (Moor et al., 2023)"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EyvoMtd64G", "forum": "SboD1FBYdy", "replyto": "SboD1FBYdy", "signatures": ["ICLR.cc/2026/Conference/Submission19197/Reviewer_zgd5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19197/Reviewer_zgd5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940459047, "cdate": 1761940459047, "tmdate": 1762931194034, "mdate": 1762931194034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces two medical-focused Large Vision-Language Models (LVLMs), InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, with different model sizes. These models integrate high-quality open-source multimodal datasets from both general and medical domains, utilizing a five-dimensional quality assessment framework for data curation. To enhance training efficiency, the pretraining process adopts a progressive approach, starting with low-resolution images and increasing in resolution, along with multimodal sequence packing. The models are fine-tuned on three distinct data domains: general-purpose, medical-specific, and cross-domain data. Evaluations on the MedEvalKit framework show that InfiMed-Foundation-1.7B outperforms the Qwen2.5VL-3B model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively compiles and integrates available high-quality medical datasets, with the data selection process supported by a combination of LLM filtering and human evaluation, ensuring the quality of training data.\n\n2. The paper is well-written, with clear and easily understandable explanations of the methods."}, "weaknesses": {"value": "1. The paper lacks significant innovation; the main contributions are engineering-focused. The data preparation, CPT (Contrastive Pre-training), and SFT (Supervised Fine-Tuning) processes do not present new insights. Specifically:\na. Data: The data selection and filtering criteria follow common practices within the engineering community. The contribution mainly lies in the integration and filtering of existing open-source datasets.\nb. CPT: The use of resolution adjustments and other training strategies has been common in pretraining smaller models for both general and medical domains. For instance, methods like BLIP[1] and MISS[2], which predate this work, have used different resolutions at various stages of training.\nc. SFT: Stage-wise and mixed-data approaches have been commonly employed in recent research across various domains.\n\n2. Benchmarking and Metrics: Compared to general-domain models, the reported metrics do not show a clear and substantial advantage. Furthermore, several of the benchmark datasets used, such as VQA-RAD and SLAKE, are outdated. For example, LLaVA-Med[3], a model evaluated on SLAKE, is a 2023 work, and the reported experimental results differ from the original papers (e.g., LLaVA-Med reports SLAKE metrics above 83%).\n[1].Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[C]//International conference on machine learning. PMLR, 2022: 12888-12900.\n[2].Chen J, Yang D, Jiang Y, et al. MISS: A generative pre-training and fine-tuning approach for med-VQA[C]//International Conference on Artificial Neural Networks. Cham: Springer Nature Switzerland, 2024: 299-313.\n[3]. Li C, Wong C, Zhang S, et al. Llava-med: Training a large language-and-vision assistant for biomedicine in one day[J]. Advances in Neural Information Processing Systems, 2023, 36: 28541-28564."}, "questions": {"value": "1.  The main contributions are engineering-focused.\n2. Compared to general-domain models, the reported metrics do not show a clear and substantial advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YBBGF3rUTt", "forum": "SboD1FBYdy", "replyto": "SboD1FBYdy", "signatures": ["ICLR.cc/2026/Conference/Submission19197/Reviewer_r88U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19197/Reviewer_r88U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990826833, "cdate": 1761990826833, "tmdate": 1762931193611, "mdate": 1762931193611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}