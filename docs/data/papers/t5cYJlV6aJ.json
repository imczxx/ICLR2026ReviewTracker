{"id": "t5cYJlV6aJ", "number": 17133, "cdate": 1758272580613, "mdate": 1759897194686, "content": {"title": "ExpGuard: LLM Content Moderation in Specialized Domains", "abstract": "With the growing deployment of large language models (LLMs) in real-world applications, establishing robust safety guardrails to moderate their inputs and outputs has become essential to ensure adherence to safety policies. Current guardrail models predominantly address general human-LLM interactions, rendering LLMs vulnerable to harmful and adversarial content within domain-specific contexts, particularly those rich in technical jargon and specialized concepts. To address this limitation, we introduce ExpGuard, a robust and specialized guardrail model designed to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, we present ExpGuardMix, a meticulously curated dataset comprising 58,928 labeled prompts paired with corresponding refusal and compliant responses, from these specific sectors. This dataset is divided into two subsets: ExpGuardTrain, for model training, and ExpGuardTest, a high-quality test set annotated by domain experts to evaluate model robustness against technical and domain-specific content. Comprehensive evaluations conducted on ExpGuardTest and eight established public benchmarks reveal that ExpGuard delivers competitive performance across the board while demonstrating exceptional resilience to domain-specific adversarial attacks, surpassing state-of-the-art models such as WildGuard by up to 8.9% in prompt classification and 15.3% in response classification. To encourage further research and development, we open-source our code, data, and model, enabling adaptation to additional domains and supporting the creation of increasingly robust guardrail models.", "tldr": "We present ExpGuardMix, a multi-domain safety training and evaluation dataset, enabling the development and training of specialized guardrails ready for real-world applications.", "keywords": ["Safety", "Guardrails", "Moderation", "Domain Specialization", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fb806ad88eb887470adf8fb169b9b6464fa9409.pdf", "supplementary_material": "/attachment/fa767ebe5d79f4c429791375c6f3b4c4b90b0466.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces ExpGuard, a Guard model trained on ExpGuardMix, which consists of 58,928 prompts, including prompts from the specialized domains of Finance, Medicine, and Law. This dataset addresses the lack of domain-specific prompts for training guardrail systems. The Authors describe in high detail the creation of ExpGuardMix with a novel construction pipeline that utilized domain-specific terminology. In addition to specialized examples, ExpGuardMix incorporates real-world user-LLM conversations and human-written prompts, making ExpGuard models useful outside the specialized domains. ExpGuard is evaluated against other state-of-the-art guardrails, highlighting its advantage over other available solutions, both open-source and closed API models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The creation pipeline of ExpGuard is novel and addresses a crucial problem when trying to apply guardrails to domain-specific applications, such as medical. Especially, the usage of domain-specific terms extracted from Wikipedia ensures that generated prompts utilize language specific to the domains of interest. The dataset creation process is highly selective, ensuring that only high-quality data is able to pass the filtering pipeline. Finally, the Authors evaluate the usefulness of ExpGuardMix by training ExpGuard and comparing it to other state-of-the-art guardrails. These results show a significant advantage of ExpGuard over other models on specialized domains, as on average ExpGuard achieves an increase of ~10% over the next best guardrail. Another experiments show how ExpGuard performs on other well-known harmfulness datasets (e.g., ToxiChat, XSTest WildGuardMix). These results again validate that ExpGuard is a highly effective guardrail, as, on average is the best and second-best model on prompt and response classification tasks accordingly. I believe that this work is of high significance to the AI Safety community."}, "weaknesses": {"value": "* ExpGuardMix seems not to address jailbreak-like prompts, which could be a limitation for ExpGuard's robustness to domain-specific jailbreaks.\n* The Authors only utilized models with fewer than 8B parameters for training ExpGuard models. I would be highly interested in the scaling above 8B models.\n* For the generation of responses, Mistral-7B-Instruct-v0.1 was used, which is a model with low capabilities compared to newer and larger models. I believe that the usage of a larger model would be of high benefit for the quality of the responses in the dataset. My biggest concern is with the responses generated by this Mistral model being of low quality."}, "questions": {"value": "* Would it be beneficial to use a more capable model for the generation of compliant responses, as this early version of Mistral could hallucinate, especially for domain-specific prompts?\n* Have the Authors tested how samples would be classified if \"safe\" and \"unsafe\" were done by majority voting per category? That is, a sample would be deemed unsafe if the majority of models have returned at least one exactly the same class from 13 predefined harm categories.\n* Can the Authors provide rough numbers of how many prompts were discarded in each filtering phase?\n* Have the Authors added jailbreak-like examples into domain-specific harmful data? \n* Regarding Figure 4: Could the Authors also show the performance of those models on these prompts before adding jailbreaks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aIRPOJjvJ0", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_YTqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_YTqP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815640035, "cdate": 1761815640035, "tmdate": 1762927127066, "mdate": 1762927127066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EXPGUARD, a domain-specialized LLM guardrail model designed for content moderation in finance, healthcare, and law domains, where existing general-purpose safety models often fail to recognize harmful content expressed in technical jargon. The authors also present EXPGUARDMIX, a large-scale dataset of labelled samples and an expert-verified test subset, featuring harmful and benign prompts/responses crafted from domain-specific terminology. The dataset construction pipeline involves domain-term mining, prompt/response generation, and an ensemble labelling using multiple proprietary LLMs with chain-of-thought rationales. Experimental results demonstrate that EXPGUARD achieves state-of-the-art performance and maintains competitive results on general public safety datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe data construction pipeline for combining automated term mining, GPT-based generation, ensemble CoT-reasoning labelling, and expert verification is automatic and replicable for other domains.\n-\tThe experimental evaluation is comprehensive, spanning both specialized and public benchmarks, with clear ablation studies showing the effect of each dataset component.\n-\tThe paper is well-structured, with clear figures illustrating data composition, pipeline, and example prompts."}, "weaknesses": {"value": "-\tUnknown domain overlaps with existing datasets like WildGuardMix. Even though this paper focuses on special domains, it is hard to tell how different the proposed dataset is from existing datasets. It is observed that other guardrail models like WildGuard and LlamaGuard can already achieve over 70% or 80% F1 scores. This suggests that EXPGUARDMIX could only serve as a supplement to the existing data. Without the quantitative measurement of the overlapped data (both benign and harmful), it is hard to judge the value of the proposed dataset.\n\n-\tLimited evaluation. All reported evaluations are F1 scores. It is equally important to see the False Positive Rate (FPR), False Negative Rate (FNR), and prediction uncertainty to analyze what types of errors the guardrail models make, especially in high-stakes domains.  \n\n-\tFor both domain-specific and standard AutoDAN-Turbo jailbreaks in Figure 4, the ExpGuard trained with domain-specific data achieves little improvement on the unsafety detection rate compared to WildGuard, which makes me doubt its effectiveness when faced with other stronger jailbreak attacks."}, "questions": {"value": "-  Why fine-tune a new guardrail model from a base model? Why not just fine-tune an existing general guardrail model to keep both general and domain-specific capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPJg6LCL6r", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_52Br"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_52Br"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960119779, "cdate": 1761960119779, "tmdate": 1762927126644, "mdate": 1762927126644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes a dataset consisting moderation contents in specific domains such as finance, healthcare and law. Experiments on several benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the paper is well written and easy to follow\n2. the paper studies moderation problems in specific domains such as finance, healthcare and law which are not studied extensively by popular methods.\n3. the paper has attached its implementation and code which is good for reproducing the work\n4. the comparison is extensive consisting of both test composed by ExpGuard and public tests."}, "weaknesses": {"value": "1. the paper mainly focuses on terminology in specific domains. In order to tackle such problems, extensive efforts have to be made such as crawling wikipedia to collect all of them. The whole generation and training process seems very resourcing consuming."}, "questions": {"value": "1. The generation and then filtering pipeline works very similarly to the work [1]. Can the authors explain the advantage of the proposed work compared to the pipelines proposed in [1,2].\n2. Following weakness 1, the system cannot be updated promptly if new terms come out. Since LLMs are capable of dynamically retrieve the meaning of terminology, have the authors thought about bypassing the wikipedia collections process and directly incorporate the definition of such terminologies for final classification.\n3. GPT-4o should have built-in safety mechanisms that prevent users from constructing malicious prompts. How does the authors get around it to have GPT-4o construct all the prompts [3]?\n4. have the authors analyzed the difficulties of the proposed training and test set? Some prompts should be more easy to classify than others. Does the proposed pipeline possess the ability to generate training and test sets of different difficulty levels?\n\n\n[1] An, Bang, et al. \"Automatic pseudo-harmful prompt generation for evaluating false refusals in large language models.\" arXiv preprint arXiv:2409.00598 (2024).\n\n[2] Cui, Justin, et al. \"Or-bench: An over-refusal benchmark for large language models.\" arXiv preprint arXiv:2405.20947 (2024).\n\n[3] https://openai.com/index/introducing-the-model-spec/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "74Nr93ucXN", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_3d4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_3d4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981842133, "cdate": 1761981842133, "tmdate": 1762927126109, "mdate": 1762927126109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on LLM safety on harmful and adversarial content within domain-specific contexts, which include technical jargon and specialized concepts. The authors introduce a specialized guardrail model called EXPGUARD to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, they also present a human-curated dataset containing 58,928 labeled prompts paired with corresponding refusal and compliant responses, which has both training and test datasets. Comprehensive experiments on EXPGUARDTEST and eight public benchmarks show the competitive performance of EXPGUARD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dataset is of high quality, which can promote the future research on LLM safety in specific domains.\n2. Experimental results provide some insights into the task of LLM safety in specific domains.\n3. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The contribution of this paper mainly falls into the resource side. However, in my view, the technical contribution of the dataset construction method is somewhat limited. The authors spend a lot of room (i.e., Section 3.1 and 3.2) for the details about data construction, labeling, and filtering, whose technical design is comprehensive but not very novel.\n2. The multi-task training method of EXPGUARD (Section 3.3) seems direct and intuitive, which is similar to the models for LLM safety of general domains. The authors are suggested to further consider methodological design in addition to dataset construction."}, "questions": {"value": "I have included my questions in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MJ1YjKxnJD", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_MLL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_MLL9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762537336217, "cdate": 1762537336217, "tmdate": 1762927125340, "mdate": 1762927125340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}