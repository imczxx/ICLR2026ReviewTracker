{"id": "t5cYJlV6aJ", "number": 17133, "cdate": 1758272580613, "mdate": 1763652668462, "content": {"title": "ExpGuard: LLM Content Moderation in Specialized Domains", "abstract": "With the growing deployment of large language models (LLMs) in real-world applications, establishing robust safety guardrails to moderate their inputs and outputs has become essential to ensure adherence to safety policies. Current guardrail models predominantly address general human-LLM interactions, rendering LLMs vulnerable to harmful and adversarial content within domain-specific contexts, particularly those rich in technical jargon and specialized concepts. To address this limitation, we introduce ExpGuard, a robust and specialized guardrail model designed to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, we present ExpGuardMix, a meticulously curated dataset comprising 58,928 labeled prompts paired with corresponding refusal and compliant responses, from these specific sectors. This dataset is divided into two subsets: ExpGuardTrain, for model training, and ExpGuardTest, a high-quality test set annotated by domain experts to evaluate model robustness against technical and domain-specific content. Comprehensive evaluations conducted on ExpGuardTest and eight established public benchmarks reveal that ExpGuard delivers competitive performance across the board while demonstrating exceptional resilience to domain-specific adversarial attacks, surpassing state-of-the-art models such as WildGuard by up to 8.9% in prompt classification and 15.3% in response classification. To encourage further research and development, we open-source our code, data, and model, enabling adaptation to additional domains and supporting the creation of increasingly robust guardrail models.", "tldr": "We present ExpGuardMix, a multi-domain safety training and evaluation dataset, enabling the development and training of specialized guardrails ready for real-world applications.", "keywords": ["Safety", "Guardrails", "Moderation", "Domain Specialization", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b53f1e78e0bf8325ca73d98dc307ca27eaec61ec.pdf", "supplementary_material": "/attachment/fa767ebe5d79f4c429791375c6f3b4c4b90b0466.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces ExpGuard, a Guard model trained on ExpGuardMix, which consists of 58,928 prompts, including prompts from the specialized domains of Finance, Medicine, and Law. This dataset addresses the lack of domain-specific prompts for training guardrail systems. The Authors describe in high detail the creation of ExpGuardMix with a novel construction pipeline that utilized domain-specific terminology. In addition to specialized examples, ExpGuardMix incorporates real-world user-LLM conversations and human-written prompts, making ExpGuard models useful outside the specialized domains. ExpGuard is evaluated against other state-of-the-art guardrails, highlighting its advantage over other available solutions, both open-source and closed API models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The creation pipeline of ExpGuard is novel and addresses a crucial problem when trying to apply guardrails to domain-specific applications, such as medical. Especially, the usage of domain-specific terms extracted from Wikipedia ensures that generated prompts utilize language specific to the domains of interest. The dataset creation process is highly selective, ensuring that only high-quality data is able to pass the filtering pipeline. Finally, the Authors evaluate the usefulness of ExpGuardMix by training ExpGuard and comparing it to other state-of-the-art guardrails. These results show a significant advantage of ExpGuard over other models on specialized domains, as on average ExpGuard achieves an increase of ~10% over the next best guardrail. Another experiments show how ExpGuard performs on other well-known harmfulness datasets (e.g., ToxiChat, XSTest WildGuardMix). These results again validate that ExpGuard is a highly effective guardrail, as, on average is the best and second-best model on prompt and response classification tasks accordingly. I believe that this work is of high significance to the AI Safety community."}, "weaknesses": {"value": "* ExpGuardMix seems not to address jailbreak-like prompts, which could be a limitation for ExpGuard's robustness to domain-specific jailbreaks.\n* The Authors only utilized models with fewer than 8B parameters for training ExpGuard models. I would be highly interested in the scaling above 8B models.\n* For the generation of responses, Mistral-7B-Instruct-v0.1 was used, which is a model with low capabilities compared to newer and larger models. I believe that the usage of a larger model would be of high benefit for the quality of the responses in the dataset. My biggest concern is with the responses generated by this Mistral model being of low quality."}, "questions": {"value": "* Would it be beneficial to use a more capable model for the generation of compliant responses, as this early version of Mistral could hallucinate, especially for domain-specific prompts?\n* Have the Authors tested how samples would be classified if \"safe\" and \"unsafe\" were done by majority voting per category? That is, a sample would be deemed unsafe if the majority of models have returned at least one exactly the same class from 13 predefined harm categories.\n* Can the Authors provide rough numbers of how many prompts were discarded in each filtering phase?\n* Have the Authors added jailbreak-like examples into domain-specific harmful data? \n* Regarding Figure 4: Could the Authors also show the performance of those models on these prompts before adding jailbreaks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aIRPOJjvJ0", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_YTqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_YTqP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815640035, "cdate": 1761815640035, "tmdate": 1762927127066, "mdate": 1762927127066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EXPGUARD, a domain-specialized LLM guardrail model designed for content moderation in finance, healthcare, and law domains, where existing general-purpose safety models often fail to recognize harmful content expressed in technical jargon. The authors also present EXPGUARDMIX, a large-scale dataset of labelled samples and an expert-verified test subset, featuring harmful and benign prompts/responses crafted from domain-specific terminology. The dataset construction pipeline involves domain-term mining, prompt/response generation, and an ensemble labelling using multiple proprietary LLMs with chain-of-thought rationales. Experimental results demonstrate that EXPGUARD achieves state-of-the-art performance and maintains competitive results on general public safety datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe data construction pipeline for combining automated term mining, GPT-based generation, ensemble CoT-reasoning labelling, and expert verification is automatic and replicable for other domains.\n-\tThe experimental evaluation is comprehensive, spanning both specialized and public benchmarks, with clear ablation studies showing the effect of each dataset component.\n-\tThe paper is well-structured, with clear figures illustrating data composition, pipeline, and example prompts."}, "weaknesses": {"value": "-\tUnknown domain overlaps with existing datasets like WildGuardMix. Even though this paper focuses on special domains, it is hard to tell how different the proposed dataset is from existing datasets. It is observed that other guardrail models like WildGuard and LlamaGuard can already achieve over 70% or 80% F1 scores. This suggests that EXPGUARDMIX could only serve as a supplement to the existing data. Without the quantitative measurement of the overlapped data (both benign and harmful), it is hard to judge the value of the proposed dataset.\n\n-\tLimited evaluation. All reported evaluations are F1 scores. It is equally important to see the False Positive Rate (FPR), False Negative Rate (FNR), and prediction uncertainty to analyze what types of errors the guardrail models make, especially in high-stakes domains.  \n\n-\tFor both domain-specific and standard AutoDAN-Turbo jailbreaks in Figure 4, the ExpGuard trained with domain-specific data achieves little improvement on the unsafety detection rate compared to WildGuard, which makes me doubt its effectiveness when faced with other stronger jailbreak attacks."}, "questions": {"value": "-  Why fine-tune a new guardrail model from a base model? Why not just fine-tune an existing general guardrail model to keep both general and domain-specific capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPJg6LCL6r", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_52Br"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_52Br"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960119779, "cdate": 1761960119779, "tmdate": 1762927126644, "mdate": 1762927126644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes a dataset consisting moderation contents in specific domains such as finance, healthcare and law. Experiments on several benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the paper is well written and easy to follow\n2. the paper studies moderation problems in specific domains such as finance, healthcare and law which are not studied extensively by popular methods.\n3. the paper has attached its implementation and code which is good for reproducing the work\n4. the comparison is extensive consisting of both test composed by ExpGuard and public tests."}, "weaknesses": {"value": "1. the paper mainly focuses on terminology in specific domains. In order to tackle such problems, extensive efforts have to be made such as crawling wikipedia to collect all of them. The whole generation and training process seems very resourcing consuming."}, "questions": {"value": "1. The generation and then filtering pipeline works very similarly to the work [1]. Can the authors explain the advantage of the proposed work compared to the pipelines proposed in [1,2].\n2. Following weakness 1, the system cannot be updated promptly if new terms come out. Since LLMs are capable of dynamically retrieve the meaning of terminology, have the authors thought about bypassing the wikipedia collections process and directly incorporate the definition of such terminologies for final classification.\n3. GPT-4o should have built-in safety mechanisms that prevent users from constructing malicious prompts. How does the authors get around it to have GPT-4o construct all the prompts [3]?\n4. have the authors analyzed the difficulties of the proposed training and test set? Some prompts should be more easy to classify than others. Does the proposed pipeline possess the ability to generate training and test sets of different difficulty levels?\n\n\n[1] An, Bang, et al. \"Automatic pseudo-harmful prompt generation for evaluating false refusals in large language models.\" arXiv preprint arXiv:2409.00598 (2024).\n\n[2] Cui, Justin, et al. \"Or-bench: An over-refusal benchmark for large language models.\" arXiv preprint arXiv:2405.20947 (2024).\n\n[3] https://openai.com/index/introducing-the-model-spec/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "74Nr93ucXN", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_3d4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_3d4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981842133, "cdate": 1761981842133, "tmdate": 1762927126109, "mdate": 1762927126109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on LLM safety on harmful and adversarial content within domain-specific contexts, which include technical jargon and specialized concepts. The authors introduce a specialized guardrail model called EXPGUARD to protect against harmful prompts and responses across financial, medical, and legal domains. In addition, they also present a human-curated dataset containing 58,928 labeled prompts paired with corresponding refusal and compliant responses, which has both training and test datasets. Comprehensive experiments on EXPGUARDTEST and eight public benchmarks show the competitive performance of EXPGUARD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dataset is of high quality, which can promote the future research on LLM safety in specific domains.\n2. Experimental results provide some insights into the task of LLM safety in specific domains.\n3. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The contribution of this paper mainly falls into the resource side. However, in my view, the technical contribution of the dataset construction method is somewhat limited. The authors spend a lot of room (i.e., Section 3.1 and 3.2) for the details about data construction, labeling, and filtering, whose technical design is comprehensive but not very novel.\n2. The multi-task training method of EXPGUARD (Section 3.3) seems direct and intuitive, which is similar to the models for LLM safety of general domains. The authors are suggested to further consider methodological design in addition to dataset construction."}, "questions": {"value": "I have included my questions in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MJ1YjKxnJD", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Reviewer_MLL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Reviewer_MLL9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762537336217, "cdate": 1762537336217, "tmdate": 1762927125340, "mdate": 1762927125340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We thank the reviewers for their thoughtful and constructive feedback. We have carefully considered all comments and have revised our manuscript. Below is a summary of the key updates included in the revised manuscript (highlighted in **blue**), followed by a summary of the additional analyses provided in our rebuttal.\n\n### Part 1: Manuscript Revisions & Updates\n\n- Updated Figures\n    - **Figure 4 (Jailbreak Analysis):**\n        - Added performance results on prompts *before* jailbreak methods are applied (\"Original\") [**YTqP-Q5**].\n        - Added results for two additional state-of-the-art attacks: FlipAttack [1] and GASP [2] [**52Br-W3**].\n        - Introduced ExpGuard+, a variant of our model retrained with domain-specific adversarial examples, showing significantly improved robustness [**YTqP-W1/Q4**].\n- Updated Sections\n    - **Section 1:** Clarified the framing of our data construction pipeline as a resource-efficient solution, rather than resource-intensive [**3d4f-W1**].\n    - **Section 3.1.2:** Added details on the prefix-injection technique used to bypass GPT-4o safety refusals during data generation, adopted from [3] [**3d4f-Q3**].\n    - **Section 3.1.3:** Clarified the strict \"majority vote per category\" consensus protocol used for data filtering [**YTqP-Q2**].\n    - **Section 3.2:** Explicitly distinguished our approach from recent \"generate-then-filter\" frameworks [4,5] [**3d4f-Q1**].\n    - **Section 4.4:** Expanded the analysis of domain-specific jailbreak resilience [**52Br-W3**, **YTqP-W1**].\n- Updated Appendices & Tables\n    - **Appendix A.1:** Added computational details regarding the Wikipedia crawling process to demonstrate cost-efficiency [**3d4f-W1**].\n    - **[NEW] Appendix A.10 & Table 11:** Added a Semantic Overlap Analysis quantifying the distinctiveness of ExpGuardMix against WildGuardMix, SALAD-Bench, and BeaverTails [**52Br-W1**].\n    - **[NEW] Appendix A.11 & Tables 12–13:** Added detailed statistics on the Data Consensus and Filtering Pipeline, including discard rates at each phase [**YTqP-Q2/Q3**].\n    - **[NEW] Appendix A.12 & Table 14:** Added a Dataset Difficulty Analysis based on Item Response Theory principles to characterize the hardness distribution of our prompts [**3d4f-Q4**].\n    - **Appendix E.2:** Added results for ExpGuard (14B) to demonstrate performance scaling [**YTqP-W2**].\n    - **[NEW] Appendix E.4 & Table 18:** Added an analysis of the effect of the Response Generator Model (Mistral-7B vs. Mistral-Small-24B) on guardrail performance [**YTqP-W3/Q1**].\n    - **[NEW] Appendix E.5 & Tables 19–28:** Added Comprehensive Evaluation Metrics, reporting Recall@FPR=1%, Recall@FPR=5%, FPR, FNR, and ECE for all models [**52Br-W2**].\n\n### Part 2: Additional Analysis & Design Rationale\n\nIn addition to the manuscript changes, we have provided detailed discussions and experimental evidence in our individual responses to clarify specific design choices:\n\n- **Technical Novelty & Framework:** We clarified that our contribution is an end-to-end automated framework designed to solve the \"cold start\" problem for specialized safety, utilizing a novel multi-stage filtering and bias-mitigated labeling pipeline [**MLL9-W1**].\n- **Simplicity in Methodology:** We discussed why a simple multi-task objective was strategically chosen to emphasize data quality over architectural complexity, a hypothesis supported by our ablation studies showing that data composition is the primary performance driver [**MLL9-W2**].\n- **Training Strategy (Base Model vs. Fine-tuning):** We clarified that training from a base model versus fine-tuning an existing guardrail is a strategic design choice. We justified our decision to train from the base model to ensure distributional purity and to demonstrate dataset efficacy [**52Br-Q1**].\n- **Offline Curation vs. Dynamic Retrieval:** We detailed the specific constraints—latency, inference cost, and security risks (e.g., prompt injection)—that make our offline fine-tuning approach superior to dynamic RAG-based definition retrieval for production guardrail systems [**3d4f-Q2**].\n\nWe believe these revisions and clarifications significantly strengthen the paper's contribution and rigor. We thank the reviewers again for their time and valuable insights in helping us improve this work.\n\n---\n\n**References**\n\n[1] Liu et al. FlipAttack: Jailbreak LLMs via Flipping, ICML 2025.\n\n[2] Basani et al. GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs, NeurIPS 2025.\n\n[3] Lee et al. HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models, ICLR 2025.\n\n[4] An et al. Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models, COLM 2024.\n\n[5] Cui et al. OR-Bench: An Over-Refusal Benchmark for Large Language Models, ICML 2025."}}, "id": "CzbhDTLNWa", "forum": "t5cYJlV6aJ", "replyto": "t5cYJlV6aJ", "signatures": ["ICLR.cc/2026/Conference/Submission17133/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17133/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission17133/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653219119, "cdate": 1763653219119, "tmdate": 1763653219119, "mdate": 1763653219119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}