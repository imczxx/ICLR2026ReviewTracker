{"id": "vae4UOVbvA", "number": 4595, "cdate": 1757720217139, "mdate": 1763639261771, "content": {"title": "End-to-End Unified Dense 3D Geometry and Motion Perception", "abstract": "Predicting 3D geometry and motion from videos is crucial for various applications. Most existing methods adopt a two-stage reconstruct-then-tracking pipeline, which first perceives 3D geometry and then exploits this 3D information to track each pixel. They usually employ the conventional iterative tracking strategy and are thus inefficient, especially for dense motion estimation. Moreover, they fail to leverage the complementary motion information for better dynamic reconstruction. To address these limitations, we propose MotionVGGT, an end-to-end unified transformer architecture that simultaneously perceives dense 3D geometry, camera pose, and motion. We introduce a set of geometry, camera, and motion tokens to represent each frame and interact with each other through interleaved frame attention and global attention layers. We then employ multiple heads to decode point maps, camera poses, and 3D motions from the corresponding tokens. Specifically, we design a conditional dense prediction head and use the motion tokens as conditions to modulate the decoding process of geometry tokens to transform them into motions. Our model directly generates dense per-pixel 3D motion fields in a single forward pass without external trackers. By unifying geometry and motion modeling, MotionVGGT further equips visual geometry foundation models with motion awareness. Our MotionVGGT shows a strong generalization ability across diverse visual geometry perception tasks, establishing a practical and universal paradigm for more comprehensive scene understanding.", "tldr": "", "keywords": ["3D Reconstruction", "Motion Estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5ed824bfd4372b7ecca8166f5f49576f78d2e164.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MotionVGGT, a VGGT-based architecture that jointly predicts dense 3D geometry (cameras, depth, point maps) and dense 3D motion fields in world coordinates in a single forward pass. It augments a geometry-aware backbone with motion tokens and a conditional dense prediction head to decode per-pixel 3D motion; training proceeds in three stages (supervised with 2D tracks, self-supervised via reprojection, and joint finetuning). Experiments cover video depth on Sintel/Bonn/KITTI, dynamic reconstruction on TUM-Dynamics, optical flow on CVO, and dense 3D tracking on Kubric."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* **Joint geometry–motion formulation**. Unifying camera/depth/point-map prediction with dense 3D motion in a single forward pass targets an important and challenging problem with clear real-world relevance.\n\n* **Clear architecture**. The addition of motion tokens and a conditional head (AdaLN-conditioned DPT) is straightforward and easy to follow, making the design transparent."}, "weaknesses": {"value": "* **Limited technical novelty**. The pipeline closely mirrors St4rtrack (ICCV’25)—a motion head on top of an existing reconstruction model—here simply transplanted onto the stronger VGGT backbone with largely similar design, training, and objectives. The added motion tokens and AdaLN-DPT appear ineffective based on ablations (see below), offering little architectural innovation.\n\n* **Evaluation results**.\n    1. Data leakage: Foundational geometry models are typically evaluated zero-shot on Sintel and TUM-Dynamics, yet these datasets appear in the training phase here; consequently, results in Table 2 (first column), Table 3, and Table 7 are not comparable to prior zero-shot reports.\n\n     2. Weak motion quality: On Kubric3D and CVO, the method lags substantially behind baselines, indicating limited ability to capture dynamic motion.\n\n     3. Ablations with minimal effect: Ablations for the motion design (e.g., with/without motion tokens, reprojection loss) yield near-identical results, suggesting the proposed components do not support the gains.\n\n* **Generalizability claims overstated**: Given the weak performance on synthetic motion benchmarks, it is difficult to accept the broad claim of “strong generalization across diverse geometry perception tasks”"}, "questions": {"value": "Given the weak results on both 2D/3D motion benchmarks, have you explored that first trains the network to predict dense 2D motion (e.g., optical flow over short time), then lifts to 3D motion, similar to [1]? If so, please report settings and whether such pretraining improves performance on Kubric3D/CVO. If not, could you comment on why this path was not pursued and whether a zero-shot 2D motion pretraining (e.g., on Sintel/FlyingThings3D or large-scale in-the-wild flow, as in [1]) might help your 3D motion quality and generalization?\n\n\n**Conclusion**\n\nOverall, the paper reads as an incremental extension of St4rtrack, with the main change being a swap to the stronger VGGT backbone. The evaluation is insufficient: it omits comparisons to state-of-the-art dense 3D tracking, relies on datasets overlapping with training, and lacks diverse real-world tests. Thus I recommend reject.\n\n[1] Liang, Yiqing et al., “Zero-Shot Monocular Scene Flow Estimation in the Wild,” CVPR 2025, pp. 21031–21044."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mI0sMxRXR7", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Reviewer_nuhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4595/Reviewer_nuhf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448007054, "cdate": 1761448007054, "tmdate": 1762917461849, "mdate": 1762917461849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "0ZmjdrttrE", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763639261018, "cdate": 1763639261018, "tmdate": 1763639261018, "mdate": 1763639261018, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MotionVGGT, an end-to-end transformer that jointly predicts dense 3D geometry, camera pose, and per-pixel 3D motion in a single forward pass, eliminating the need for external trackers. \n\nEach video frame is encoded into geometry, camera, and motion tokens, which interact via interleaved frame-attention and global-attention layers. Dedicated heads then decode point-cloud maps, camera poses, and motion fields. A conditional dense-prediction head leverages motion tokens to modulate geometry tokens, coupling geometry and motion estimation. \n\nBy unifying these two aspects, MotionVGGT brings motion awareness to visual-geometry foundation models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "By discarding the prevailing reconstruct-then-track paradigm and its iterative trackers, the paper offers a unified formulation of dynamic-scene perception.\n\nThe method needs no optical flow, external tracker, or extra geometric cues, yet produces image-wide dense motion.\n\nBy equipping visual-geometry foundation models with motion awareness, MotionVGGT is a simple path from static to dynamic scene understanding."}, "weaknesses": {"value": "The interleaved frame-attention and global-attention design is inherited almost verbatim from VGGT; listing it as a new contribution is misleading. Please clarify what is genuinely new (e.g., the motion) and explicitly separate it from prior work.\n\nFig. 1 is the only visualization, and it shows barely perceptible motion against an almost static background. More diverse, high-motion scenes (e.g., human actions, street traffic) are needed to substantiate claims.\n\nFig. 2 mostly reiterates ideas already covered in TAPIP3D, it does not illustrate MotionVGGT’s unique elements. Consider replacing it with a more focused schematic."}, "questions": {"value": "Lack of the visualization of 4D results. Images and video results are crucial for evaluating 4D reconstruction. At present, none are provided, which makes the submission feel premature. \n\nThe paper does not discuss or compare against several closely related and contemporaneous methods:\n\n    St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World\n    TAPIP3D: Tracking Any Point in Persistent 3D Geometry\n    Easi3R: Estimating Disentangled Motion from DUSt3R Without Training\n    MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second\n    POMATO: Marrying Pointmap Matching with Temporal Motions for Dynamic 3D Reconstruction\n    SpatialTrackerV2: 3D Point Tracking Made Easy\n    TTT3R: 3D Reconstruction as Test-Time Training\n    Trace Anything Representing Any Video in 4D via Trajectory Fields\n    Depth Anything 3: Recovering the Visual Space from Any Views\n\nCould the authors clarify how MotionVGGT differs from, or improves upon, these approaches and, if possible, include quantitative/qualitative comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oVsLelpfVq", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Reviewer_fCRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4595/Reviewer_fCRw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902857161, "cdate": 1761902857161, "tmdate": 1762917461555, "mdate": 1762917461555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MotionVGGT, an end-to-end transformer that jointly predicts dense 3D geometry, camera pose, and per-pixel 3D motion in a single forward pass, eliminating the need for external trackers. \n\nEach video frame is encoded into geometry, camera, and motion tokens, which interact via interleaved frame-attention and global-attention layers. Dedicated heads then decode point-cloud maps, camera poses, and motion fields. A conditional dense-prediction head leverages motion tokens to modulate geometry tokens, coupling geometry and motion estimation. \n\nBy unifying these two aspects, MotionVGGT brings motion awareness to visual-geometry foundation models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "By discarding the prevailing reconstruct-then-track paradigm and its iterative trackers, the paper offers a unified formulation of dynamic-scene perception.\n\nThe method needs no optical flow, external tracker, or extra geometric cues, yet produces image-wide dense motion.\n\nBy equipping visual-geometry foundation models with motion awareness, MotionVGGT is a simple path from static to dynamic scene understanding."}, "weaknesses": {"value": "The interleaved frame-attention and global-attention design is inherited almost verbatim from VGGT, listing it as a new contribution is misleading. Please clarify what is genuinely new (e.g., the motion) and explicitly separate it from prior work.\n\nFig. 1 is the only visualization, and it shows barely perceptible motion against an almost static background. More diverse, high-motion scenes (e.g., human actions, street traffic) are needed to substantiate claims.\n\nFig. 2 mostly reiterates ideas already covered in TAPIP3D, it does not illustrate MotionVGGT’s unique elements. Consider replacing it with a more focused schematic."}, "questions": {"value": "Lack of the visualization of 4D results. Images and video results are crucial for evaluating 4D reconstruction. At present, none are provided, which makes the submission feel premature. \n\nThe paper does not discuss or compare against several closely related and contemporaneous methods:\n\n    St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World\n    TAPIP3D: Tracking Any Point in Persistent 3D Geometry\n    Easi3R: Estimating Disentangled Motion from DUSt3R Without Training\n    MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second\n    POMATO: Marrying Pointmap Matching with Temporal Motions for Dynamic 3D Reconstruction\n    SpatialTrackerV2: 3D Point Tracking Made Easy\n    TTT3R: 3D Reconstruction as Test-Time Training\n    Trace Anything Representing Any Video in 4D via Trajectory Fields\n    Depth Anything 3: Recovering the Visual Space from Any Views\n\nCould the authors clarify how MotionVGGT differs from, or improves upon, these approaches and, if possible, include quantitative/qualitative comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oVsLelpfVq", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Reviewer_fCRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4595/Reviewer_fCRw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902857161, "cdate": 1761902857161, "tmdate": 1763278744554, "mdate": 1763278744554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MotionVGGT, extending a pretrained geometry-aware transformer (VGGT) with motion tokens and a conditional DPT head to jointly predict camera parameters, depth/point maps, and dense world-coordinate 3D motion in a single forward pass. Trained in three stages (supervised 2D tracking, self-supervised reprojection, and joint fine-tuning), it offers a practical, unified approach to dynamic scene understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem by extending geometry foundation models into a unified, motion-aware framework. The design is elegant and practical, enabling dense motion prediction in a single forward pass without external trackers. The multi-source, multi-stage training scheme effectively combines supervised and self-supervised learning across diverse datasets."}, "weaknesses": {"value": "1) The core concern lies in the lack of methodological novelty. The proposed approach mainly adds a DPT head for motion prediction and conditional modulation on top of VGGT. This design is rather incremental and has been widely seen in recent vision transformer works.\n\n2) The experimental results conflict with the paper’s main claims. In Table 4 and Table 5, MotionVGGT performs significantly worse than its backbone (and other baselines), especially on motion-related benchmarks, which are supposed to be its core strength. This weakens the claimed advantage of \"unified motion perception.\" The paper repeatedly claims \"strong generalization,\" but the numbers clearly show otherwise. \n\n3) Although the authors briefly acknowledge performance degradation in the Limitations section, they do not analyze why it happens. The experimental discussion is superficial, and there are no qualitative visualizations to substantiate the claims.\n\n4) The writing is clear but contains noticeable redundancy, e.g., the Introduction and Method sections repeatedly emphasize \"unified\" and \"dense\" without adding substantive insight."}, "questions": {"value": "1) Section 4.1 states that the VGGT backbone is frozen and only the motion head is trained, while Section 3.3 mentions that Stage III involves joint fine-tuning of geometry and motion. This inconsistency should be clarified. \n\n2) Some key implementation details are missing, for example, how many motion tokens are used, and whether their number is fixed or adaptive across frames."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E0156izSfC", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Reviewer_ZN6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4595/Reviewer_ZN6p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951540111, "cdate": 1761951540111, "tmdate": 1762917461076, "mdate": 1762917461076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a joint 3D geometry and motion perception method. \nGiven a pretrained geometry foundation model, VGGT, the method attaches a motion head to predict 3D scene flow of each point.\nThe paper proposes multi-stage training techniques and achieves reasonable accuracy on both geometry and motion benchmarks.\n\n---\n\nThere are lots of concerns found in the paper: lack of novelty (incremental architectural improvement), no comprehensive ablation study (unclear if the proposed idea is effective), insufficient/invalid experiment setup (no qualitative results, test set is used during training). Thus the recommendation of the paper is **2. Reject**."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* **Good clarity**\n\n  The paper is written clearly. All the details are easy to follow. \n\n* **Good motivation for joint 3D geometry and 3D motion**\n\n  The introduction part nicely motivate the necessity of joint 3D geometry and 3D motion estimation, by comparing with previous methods of multi-stage pipelines (eg., estimation of geometry first, followed by motion estimation)."}, "weaknesses": {"value": "* **Novelty concern**\n\n  The architecture is a combination of VGGT and DynaDUSt3R [1]. The motion head from DynaDUSt3R is adopted in the VGGT architecture. It's difficult to find any new innovations for this problem of joint 3D geometry and 3D motion estimation. The newly proposed part is the multi-stage training in section 3.3 only, but the effectiveness of the multi-stage training is also questionable (continued in the other bullet points).\n\n  [1] Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos\n\n* **Multi-stage training and ablation study**\n\n  It is unclear if the multi-stage training is actually effective. In Table 6, the improvement is very marginal, 0.1 point, or the same. It's not so sure if this level of improvement can be considered as \"effective\". For better thoroughness, it is recommended to share the full ablation study. It can be:\n  * Stage1 only\n  * Stage2 only\n  * Stage1 + Stage2\n  * Stage1 + Stage3\n  * Stage2 + Stage3\n  * Stage1 + Stage2 + Stage3\n\n  and evaluate on both geometry and motion estimation tasks. Without it, it's hard to believe if this multi-stage training is actually needed. This also results in the lack of novelty concerns.\n\n\n* **Poor motion accuracy**\n\n  In Table 4 and 5, the 3D tracking accuracy heavily underperforms, comparing with other methods. 3D tracking of the method seems not properly working.\n\n* **TUM-dynamics dataset**\n\n  Table 3 reports the accuracy of the method on TUM-dynamics dataset. However, the dataset is actually included in the training sets (Table 1), which questions the validity of the results. \n\n\n* **No qualitative results**\n\n  There is no qualitative comparison attached in the paper. It is difficult to understand how the method behave."}, "questions": {"value": "* **In Figure 1**, what's the difference between images in the 2nd row and that in the 3rd row?\n\n* **In Equation 2**, the 3D tracking method can also recover the camera pose via post processing because it has the 3D correspondence. I am not so sure if this is truly a critical limitation of the 3D Tracking approach. \n\n* **In Equation 7**, when projecting points into image coordinate, it's likely that multiple points can be projected into a single pixel coordinate. In this case, how does the method handle depth ordering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tNLyrLdQh0", "forum": "vae4UOVbvA", "replyto": "vae4UOVbvA", "signatures": ["ICLR.cc/2026/Conference/Submission4595/Reviewer_evDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4595/Reviewer_evDg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966947455, "cdate": 1761966947455, "tmdate": 1762917460513, "mdate": 1762917460513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}