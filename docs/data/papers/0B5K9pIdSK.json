{"id": "0B5K9pIdSK", "number": 24843, "cdate": 1758360984163, "mdate": 1759896745854, "content": {"title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA", "abstract": "Large Language Models (LLMs) are widely applied in real world scenarios, but fine-tuning them comes with significant computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. One way to address this issue is through knowledge distillation, but its effectiveness inherently depends on training data. Recent work such as TransLoRA avoids this by generating synthetic data, but this adds complexity because it requires training an additional discriminator model. In this paper, we propose TiTok, a new framework that enables effective LoRA Transplantation through Token-level knowledge transfer. Specifically, TiTok captures task-relevant information through a contrastive excess between a source model with and without LoRA. This excess highlights informative tokens and enables selective filtering of synthetic data, all without additional models or overhead. Through experiments on three benchmarks across multiple transfer settings, our experiments show that the proposed method is consistently effective, achieving average performance gains of +4–8% compared to baselines overall.", "tldr": "We propose a new framework TiTok, which enables effective LoRA transplantation through token-level knowledge transfer", "keywords": ["Large Language Models", "Knowledge Transfer", "PEFT"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36dec6a7418f68bdb0d5b16bf31ccf444eadf348.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TITOK, a framework for transferring LoRA adapters between large language models through token-level contrastive knowledge transfer.\nUnlike existing methods such as TransLoRA, which rely on synthetic data filtered by an external discriminator, TITOK uses a self-contained contrastive excess mechanism that compares token-level likelihoods between a source model with and without LoRA. Tokens where the LoRA contributes the most (“excessive” tokens) are identified as carrying rich, task-specific knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Lightweight and scalable.\nTITOK requires no additional model training; it simply reuses the source model’s log-likelihoods. The proposed sample filtering and token selection can be implemented efficiently and applied to diverse architectures.\n\n2. Cross-tokenizer alignment.\nThe tokenizer-alignment algorithm (Fig. 4) is an important practical contribution that allows reliable transfer across families like Mistral→LLaMA, addressing a frequent but rarely discussed issue in adapter transplantation.\n\n3. Comprehensive evaluation.\nThe experiments cover both reasoning (BBH, MMLU) and personalization (LaMP) tasks, across intra- and inter-family transfers (same model, different family, size, and version). Ablations, sensitivity to token selection ratio, and external-data experiments are thorough and convincing.\n\n4. Strong empirical results.\nTITOK consistently improves over baselines (e.g., +7.11% average gain in cross-model transfer) and shows robustness even when transferring with unrelated data (Table 3)."}, "weaknesses": {"value": "1. Incremental relative to TransLoRA.\nWhile the contrastive excess concept is novel, much of the overall framework (synthetic data, adapter transplantation) follows TransLoRA. The improvement may be viewed as a simplification rather than a fundamental rethinking of LoRA transfer.\n\n2. Limited theoretical justification.\nThe paper could benefit from more formal analysis—e.g., why token-level contrastive loss correlates with adapter informativeness, or whether excess scores approximate information gain.\n\n3. Evaluation scope.\nAll experiments use instruction-tuned LLMs up to 8B parameters. It remains unclear whether TITOK scales to larger frontier models (≥70B) or non-text modalities.\n\n4. Minor clarity issues.\nThe term “contrastive excess” could be better linked to contrastive learning literature; at present it may be confusing since no contrastive pairs or negatives are explicitly optimized."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UQSkLVq5Pd", "forum": "0B5K9pIdSK", "replyto": "0B5K9pIdSK", "signatures": ["ICLR.cc/2026/Conference/Submission24843/Reviewer_g2oB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24843/Reviewer_g2oB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760546694973, "cdate": 1760546694973, "tmdate": 1762943217449, "mdate": 1762943217449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TITOK, a lightweight framework for transferring LoRA adapters across LLMs without original training data. It uses token-level contrastive excess scores from the difference between a base model and its LoRA version to filter synthetic data and focus training on informative tokens. TITOK avoids extra discriminators or full datasets, achieving consistent improvements over baselines like KD and TransLoRA across tasks and model types."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. TITOK avoids extra models or full datasets by leveraging only token-level contrastive excess scores, making it computationally efficient and easy to deploy.\n\n2. Demonstrates consistent performance gains across diverse transfer settings, including different model families, sizes, and tokenizers, highlighting robustness and broad applicability."}, "weaknesses": {"value": "- Only 7–8 B models; no evidence it scales to 70 B+ or harder generative tasks.\n\n- Source model generates both query and label; filter can’t catch wrong or biased synthetic data.\n\n- No formal justification that “contrastive excess” equals transferable knowledge."}, "questions": {"value": "Did the authors perform any human evaluation or external fact-checking to verify the quality of the filtered synthetic data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lynnvkiRbS", "forum": "0B5K9pIdSK", "replyto": "0B5K9pIdSK", "signatures": ["ICLR.cc/2026/Conference/Submission24843/Reviewer_YLn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24843/Reviewer_YLn5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909603326, "cdate": 1761909603326, "tmdate": 1762943217144, "mdate": 1762943217144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework called TITOK for transferring LoRA-based knowledge between large language models (LLMs). The authors note that existing parameter-efficient fine-tuning (PEFT) methods such as LoRA cannot be directly transplanted across different backbones, while knowledge distillation (KD) approaches depend heavily on access to original task data. TITOK introduces the concept of contrastive excess, computed between a source model with LoRA and its corresponding base model without LoRA, to identify task-informative tokens. Using these token-level signals, the method filters synthetic data and trains the target model’s new LoRA adapter only on the most informative tokens, avoiding the need for an additional discriminator or real training data. Experiments on BBH, MMLU, and LaMP benchmarks show that TITOK consistently outperforms KD and TransLoRA across multiple transfer settings, achieving average performance gains of 4–8%. Overall, the paper proposes a lightweight token-level transfer framework and demonstrates its effectiveness through empirical evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new perspective on LoRA knowledge transfer by focusing on token-level contrastive signals (“contrastive excess”) rather than sequence-level alignment. This provides a fine-grained and model-agnostic approach that does not require additional networks or data annotations.\n2. The authors conduct systematic experiments across multiple benchmarks (BBH, MMLU, and LaMP) and various transfer settings (intra-family, cross-family, and cross-version). The results consistently demonstrate that TITOK achieves stable improvements over established baselines such as KD and TransLoRA."}, "weaknesses": {"value": "1. The paper does not provide a solid theoretical justification for why the proposed contrastive excess signal effectively captures transferable task knowledge. The formulation remains largely heuristic, and no analysis is presented to support its generalization properties or convergence behavior.\n\n2. Although the experiments cover multiple datasets, the reported improvements (4–8%) are modest and sometimes within the margin of variance. The absence of standard deviations or statistical significance tests weakens the credibility of the claimed gains.\n\n3. Despite claiming to reduce data dependence, TITOK still relies heavily on synthetic data generated from the source model. The paper does not adequately evaluate how data quality, diversity, or prompt design affect transfer performance, making reproducibility and robustness questionable.\n\n4. The paper mainly compares against KD and TransLoRA but omits other recent or stronger PEFT transfer approaches. Without these comparisons, it is unclear whether TITOK’s benefits are meaningful beyond this narrow baseline set.\n\n5. Several key implementation details—such as the choice of hyperparameters (e.g., k% token ratio) and the impact of tokenizer alignment—are only described qualitatively. The algorithmic intuition is under-explained, and the overall presentation lacks ablation depth, which limits interpretability and reproducibility."}, "questions": {"value": "1. The paper lacks a clear explanation of why the contrastive excess between the LoRA-equipped and base models effectively captures transferable task knowledge. Could the authors provide theoretical reasoning or empirical evidence (e.g., gradient or information analysis) to support its validity?\n\n2. TITOK heavily depends on synthetic data generated by the source model. How robust is the method to variations in data quality, prompt design, or model strength? Would TITOK still perform well if the source model or synthetic data are suboptimal?\n\n3. The algorithm’s performance appears sensitive to the top-k% token selection and sample filtering. Can the authors provide quantitative sensitivity analyses or propose adaptive mechanisms to make TITOK more stable across tasks?\n\n4. The tokenizer alignment process is crucial but not quantitatively evaluated. How accurate is this alignment across model families with different tokenization schemes? Could small alignment errors lead to degraded performance?\n\n5. The comparisons only include KD and TransLoRA. To better position the contribution, could the authors include or discuss results against more recent PEFT transfer methods (e.g., KD-LoRA, AdapterFusion, DoRA)? This would clarify TITOK’s true advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hG139fdx1l", "forum": "0B5K9pIdSK", "replyto": "0B5K9pIdSK", "signatures": ["ICLR.cc/2026/Conference/Submission24843/Reviewer_LDW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24843/Reviewer_LDW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913819348, "cdate": 1761913819348, "tmdate": 1762943216712, "mdate": 1762943216712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TITOK (Transfer Token-level Knowledge), a framework for transplanting LoRA adapters across different LLM backbones by transferring token-level knowledge instead of sequence-level knowledge.\nUnlike previous approaches such as TransLoRA, which require a discriminator model to filter synthetic data, TITOK leverages a contrastive excess signal to identify informative tokens that encode task-specific knowledge. TITOK requires neither extra models nor additional training overhead. It comes with an effective mechanism to resolve tokenizer mismatches between source and target models, which enhances robustness and applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of contrastive excess as a proxy for task-specific token importance is elegant and avoids the need for auxiliary discriminators or extra training modules. It’s conceptually sound and efficiently leverages existing model behavior.\n- TITOK uses no extra models and operates with modest computational overhead compared to TransLoRA. This design aligns well with real-world deployment constraints.\n- The authors test across multiple model families (Mistral, Llama 2/3) and tasks (reasoning, personalization), presenting clear gains in each. The inclusion of ablations and sensitivity analyses (on token selection ratio, query generation source, etc.) adds strong empirical credibility.\n- The dual-pointer alignment and averaging rules are simple yet effective."}, "weaknesses": {"value": "- The proposed method still relies on generating a large pool of synthetic samples, which may introduce significant costs\n- While token-level filtering is the paper’s key innovation, there is no qualitative or visual analysis demonstrating which tokens are retained or why they contribute to effective transfer."}, "questions": {"value": "- How sensitive is TITOK to the quality of the source LoRA? Would a poorly tuned LoRA produce misleading excess scores?\n- Could contrastive excess be used iteratively (e.g., to refine the LoRA itself) rather than just for one-shot transfer?\n- How does TITOK behave when transferring between models with different architectures (e.g., dense vs. mixture-of-experts backbones)?\n- Can the authors clarify the computational overhead of computing per-token log-likelihood differences compared to TransLoRA’s discriminator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NQV3STp173", "forum": "0B5K9pIdSK", "replyto": "0B5K9pIdSK", "signatures": ["ICLR.cc/2026/Conference/Submission24843/Reviewer_AmyZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24843/Reviewer_AmyZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997917203, "cdate": 1761997917203, "tmdate": 1762943216501, "mdate": 1762943216501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}