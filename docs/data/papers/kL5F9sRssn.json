{"id": "kL5F9sRssn", "number": 16806, "cdate": 1758268869906, "mdate": 1759897218513, "content": {"title": "Landmark-Guided Policy Optimization for Multi-Objective Language Model Selection", "abstract": "Selecting a pretrained large language model (LLM) to fine-tune for a task-specific dataset can be time-consuming and costly. With several candidate models available to choose from, varying in size, architecture, and pretraining data, finding the best often involves extensive trial and error. In addition, the \"best\" model may not necessarily be the one with the lowest test loss, as practical considerations such as deployment costs, inference throughput, and limited search budgets might also play crucial roles. To address this, we introduce LAMPS (LAnguage Model Pareto Selection), a novel and open-source multi-objective AutoML framework that quickly identifies near-Pareto-optimal pretrained LLMs for a task-specific dataset. It is based on two key ideas: (1) landmark fine-tuning, which generates early performance indicators of the candidate models, and (2) meta-learning via reinforcement learning, which learns an effective selection policy from historical performance data (a meta-dataset). Our results show that, on held-out datasets, LAMPS reduces search time by an average of 71% compared to exhaustive search, while still covering more than 98% of the optimal target space hypervolume.", "tldr": "A novel and open-source multi-objective AutoML framework called LaMPS that quickly identifies and produces fine-tuned, near–Pareto-optimal pretrained language models for a task-specific dataset.", "keywords": ["AutoML", "metalearning", "multi-objective optimization", "model selection", "reinforcement learning", "large language models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9640e9516ac93c4a2b1279e7076b5d926ad2bc97.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LAMPS is a language model selection framework that aims to choose the best model for a particular task. The authors claim that the best model is not necessarily the one with the lowest test loss but could be influenced by many other factors (resources, deployment costs). Hence, in the context of AutoML, finding the best model is expensive. They formulate the problem as an RL problem (trained with PPO) with an objective function of finding a Pareto optimal set over the models, by minimizing the joint cost for all factors (model size, inference throughput, deployment cost) (defined as the hypervolume indicator)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has well formatted definitions and theorems\n- The figures are well made\n- The problem they choose to solve is interesting."}, "weaknesses": {"value": "- The related works section could be strengthened. From what I remember, there are works that try to automate neural architecture search, which seems like a similar problem setting. I understand that experiments would be too much to run, but at least including a discussion on whether these methods could be adapted to this task/setting would make the claims stronger.\n- It can be expensive to run all the configurations before deciding what model to select."}, "questions": {"value": "1. Is my understanding correct: the multi-objective RL comes in from the fact that each factor (test loss, model size, inference throughput, deployment cost) is a dimension in the hypervolume indicator ($i=0$ for test, $i=1$ for model size, etc.), and these factors are all aggregated with the Lebesgue measure?\n2. Is there any analysis on how the conflicting factors affect the performance of LAMPS? What if there were no conflicting factors? What if there were only conflicting factors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wl56KEMex3", "forum": "kL5F9sRssn", "replyto": "kL5F9sRssn", "signatures": ["ICLR.cc/2026/Conference/Submission16806/Reviewer_4aCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16806/Reviewer_4aCY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760985975668, "cdate": 1760985975668, "tmdate": 1762926838126, "mdate": 1762926838126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LAMPS, a novel multi-objective AutoML framework for selecting pretrained large language models (LLMs). The authors treat the practical challenge as a multi-objective problem, balancing the trade off between model performance, training cost, etd. LAMPS combines landmark fine-tuning, meta-learning via reinforcement learning, which trains a policy on historical model performance. In the experimental results, LAMPS reduces search time by 71% while maintaining coverage of over 98% of the optimal target-space hypervolume."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The paper Introduces a novel method, LAMPS within a multi-objective AutoML framework for efficiently selecting and fine-tuning models along a Pareto front. It combines multi-objective optimization with invalid action masking in RL, which is a novel way to improve exploration efficiency and reduce wasted computation. LAMPS consistently identifies near-Pareto-optimal models faster than baselines"}, "weaknesses": {"value": "The termination condition assumes the agent can detect when all Pareto-optimal models have been fully fine-tuned, which is not practical for the real scenario. \n\nThe RL agent training rely on fully fine tuning trajectories of 70 pretrained models, which limits its applicability to scenarios involving a large number of pretrained models.\n\nThe experiments use only nine datasets and two objectives, more experiments would strengthen the generality and demonstrate its performance on more objective trade-offs. It’s unclear how LAMPS scales when more objectives are added."}, "questions": {"value": "The paper does not clearly define what constitutes a *fully fine-tuned* mode, could the authors specify the exact convergence condition?  \n\nThe paper uses a sparse reward defined only at the end of each episode (t = T). Have the authors considered using intermediate rewards at each timestep t, for example, based on the hypervolume of the partially fine-tuned model’s current performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A9Mkt9hKBU", "forum": "kL5F9sRssn", "replyto": "kL5F9sRssn", "signatures": ["ICLR.cc/2026/Conference/Submission16806/Reviewer_8m2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16806/Reviewer_8m2N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794040210, "cdate": 1761794040210, "tmdate": 1762926837478, "mdate": 1762926837478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of selecting and fine-tuning a subset of pretrained LLMs to approximate the Pareto front across multiple, potentially conflicting objectives, such as test loss and fine-tuning time. The authors formulate this as a hypervolume maximization problem with a cardinality regularization term, and establish a sufficient condition under which the optimizer recovers the true Pareto set.\nTheir proposed method, LAMPS, uses early segments of model learning curves, termed \"landmarks,\" to predict final performance. These landmarks guide a reinforcement learning allocation policy, trained with Distral-regularized PPO, which decides which model to allocate further fine-tuning resources to. The policy receives a sparse reward that encourages efficient recovery of the full Pareto set, and incorporates mechanisms like invalid action masking to avoid unnecessary computation.\nThe deployment procedure begins with a zero-shot evaluation of all candidate models, followed by an iterative process of fine-tuning and evaluation under a budget constraint. The final output consists of non-dominated solutions. The method is meta-trained on a large dataset of full fine-tuning trajectories covering 70 models and nine classification datasets. Experiments show that LAMPS reaches 98 percent of the optimal hypervolume more quickly than baseline methods, achieving performance comparable to an oracle on most tasks while reducing wall-clock search time by 71 percent on average.\n\nUsing hypervolume to evaluate Pareto sets is standard in multi-objective optimization (MOO) and maximizing it aligns with Pareto optimality, but the paper’s contribution is to meta-learn a policy that allocates scarce fine-tuning budget across models to rapidly approach a good Pareto set in LLM selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel framing of model selection and fine-tuning as a policy learning problem that directly optimizes hypervolume. The sparse reward is well-designed and aligned with the set-level objective.\n\n- The formulation combining hypervolume and ℓ₀-style regularization is mathematically sound, with a clear condition on the regularization parameter. The reinforcement learning setup is thoughtful, using Distral for transfer, PPO for stability, and invalid-action masking to improve efficiency.\n\n- The experiments cover a large and realistic search space with 70 pretrained LLMs across nine datasets. Results are reported with respect to wall-clock time, showing a 71 percent average reduction in search time to reach 98 percent of optimal hypervolume. Comparative performance is clearly illustrated in Figure 3.\n\nDeployment details in Algorithm 1 and the inclusion of example commands and a policy checkpoint improve reproducibility and ease of adoption.\n\n- The focus on hypervolume as a set-level objective better reflects practical needs in multi-objective LLM selection and offers a promising path toward reducing compute costs in real-world settings."}, "weaknesses": {"value": "1.\tThe initial state and per-step evaluation use D_test, which risks peeking at the test set while adaptively selecting and training models, potentially biasing results. A pure validation set (or cross-validation) should drive decisions; the test set should be reserved for final evaluation only. \n2.\tBaselines are underspecified relative to the literature. The paper compares against Blind/ZigZag/Oracle but omits strong, well-known multi-fidelity HPO and early-stopping methods (e.g., Hyperband/ASHA, BOHB [7]) and learning-curve extrapolation (Domhan et al.) [2], which directly trade resource vs. accuracy and are relevant to the same compute-constrained setting. It also omits multi-objective HPO baselines (e.g., scalarization approaches like ParEGO [3], hypervolume-based MOBO, or MO-ASHA [4]). Without these, the magnitude of LAMPS’ advantage is harder to assess [1]. \n3.\tExperiments optimize only test cross-entropy and training time for classification tasks. Important deployment objectives (e.g., inference throughput/latency, VRAM, energy, dollar cost, fairness/robustness metrics, factuality metrics, or generation-quality metrics for seq2seq) are untested. The claim of objective-agnosticism would be stronger with such evaluations. \n4.\tAblations / design analyses are missing or light:\no\tEffect of landmark schedule (number and spacing) on prediction fidelity and policy quality.\no\tReward shaping alternatives and sensitivity to the terminal-only reward.\no\tRole of Distral vs. single-task policies.\no\tEffect of invalid-action masking removal.\no\tSensitivity to the reference point (r) for hypervolume. (The literature notes hypervolume can be reference-point sensitive.) [5]\n5.\tTraining uses 8× A100-40GB and advises ≥2 TB disk; this may be a barrier for many labs. Reporting total policy training cost and amortization analysis vs. per-dataset gains would help. \n6.\tRelation to hypervolume-maximizing selection. Since LAMPS ultimately maximizes hypervolume of a set, comparisons or discussion against hypervolume-driven selection algorithms (e.g., SMS-EMOA [6]) would contextualize design choices and computational efficiency.\n\nReferences:\n[1] https://arxiv.org/abs/1603.06560\n[2] https://www.ijcai.org/Proceedings/15/Papers/487.pdf\n[3] https://ieeexplore.ieee.org/document/1583627\n[4] https://arxiv.org/pdf/2106.12639\n[5] https://arxiv.org/pdf/2005.00515\n[6] https://www.sciencedirect.com/science/article/pii/S0377221706005443\n[7] https://arxiv.org/pdf/1807.01774"}, "questions": {"value": "1.\tCan you re-run the main results with decisions driven exclusively by a validation split (no access to test during policy rollouts) and report test-only results at the end? This would address potential test leakage. \n2.\tIt could be instructive to include:\no\tHyperband/ASHA (single-objective time-aware early-stopping) configured for either joint scalarization (e.g., weighted sum of objectives) or treating training time as the resource.\no\tBOHB (combines BO with Hyperband).\no\tLearning-curve extrapolation (Domhan et al.) to decide early continuation/termination.\no\tMulti-objective HPO like ParEGO (scalarization BO) or MO-ASHA. How does LAMPS compare in wall-clock to 98% hypervolume? \n3.\tCould you add experiments with (i) generation tasks (e.g., summarization), (ii) inference-time/VRAM/energy objectives, and (iii) robustness/fairness/factuality metrics to demonstrate objective-agnosticism?\n4.\tIt may be useful to provide: (a) landmark schedule sensitivity; (b) reward variants (e.g., per-step dense rewards); (c) with/without Distral; (d) invalid-action masking off; (e) reference-point (r) sensitivity.\n5.\tWhat reference point (r) is used and how chosen per dataset? Any normalization across objectives?\n6.\tWhat is the total compute to train the 45M-step policy, and how many target datasets does it take to amortize that cost compared to (say) BOHB?\n7.\tSince some Hugging Face models require license acceptance, how is this handled in automated runs (e.g., CI or cluster) to ensure compliance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vLdKtBeIQA", "forum": "kL5F9sRssn", "replyto": "kL5F9sRssn", "signatures": ["ICLR.cc/2026/Conference/Submission16806/Reviewer_jMZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16806/Reviewer_jMZd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816598617, "cdate": 1761816598617, "tmdate": 1762926837139, "mdate": 1762926837139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}