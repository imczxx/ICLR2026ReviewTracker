{"id": "zw0IMYPLQZ", "number": 870, "cdate": 1756821606585, "mdate": 1759898237956, "content": {"title": "SINGER: Leveraging Semantic Identifier Hierarchies for Generative Recommendation", "abstract": "Recent advances in large language models (LLMs) have sparked a new line of *generative recommendation*, in which the recommender **autoregressively** outputs a sequence of *Semantic IDs* (SIDs)—item identifiers that live in a structured SID space—rather than ranking a pre-selected candidate list of item titles in natural language.  \nAlthough the prevailing *supervised fine-tuning followed by reinforcement learning* (SFT-then-RL) pipeline improves performance, it still fails to model the SID space adequately:\n\n1. **Superficial SID understanding.**  \n   SFT often ends up memorising a closed SID vocabulary instead of learning its semantics.  \n2. **Coarse-grained rewards.**  \n   Rule-based RL treats all incorrect SIDs equally, ignoring the varying difficulty of different errors.\n\nTo address these limitations, we propose **SINGER** (*SID-Navigated GEnerative Recommender*), a framework that injects fine-grained SID knowledge into every training phase.\n\nSINGER consists of two key components:\n\n1. **Full-Process SID Alignment**  \n   Alignment objectives are embedded in both SFT and RL stages, deepening the model’s grasp of the SID space.  \n2. **SID-Navigated Reinforcement Learning**  \n- *SID-level rewards* grade each trajectory by the deepest correctly matched SID layer.  \n- *SID-prefix curriculum sampling* supplies partial prefixes as intermediate guidance for hard cases.\n\nExperiments on public benchmarks show that SINGER consistently outperforms strong sequential, generative, and recent LLM-based baselines across standard metrics, confirming the value of combining hierarchical SID signals with the world knowledge of pretrained LLMs.", "tldr": "", "keywords": ["LLM; Recommendation; Generative Recommendation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d77f55007fec0a5d72ff7e980af6612f504ae314.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper has focused on the generative recommendation, which has been a hotspot recently. The authors have found that existing generative recommendation models with only SFT cannot truly understand the SID learnt from RQ-VAE. Besides, rule-based RL mainly relies on coarse-grained rewards, which may lead to difficulties in training. To address these two problems, this paper proposes to embed alignment objectives into both the SFT and the RL process. The SID-level reward is designed for the RL stage. The experiments on two datasets have validated the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ S1. This paper is well-organized and well-written, making it easy to follow.\n+ S2. Many up-to-date generative recommendation models are compared in the experiments."}, "weaknesses": {"value": "- W1. The illustration of the preliminary experiment is unclear, which may lead to unreasonable motivation. The legend in Figure 1(a) demonstrates the results belong to different training patterns with SINGER, but the illustration in lines 72-75 demonstrates they are with GRPO instead of SINGER. I'd like to know what the RL type is in this figure, in fact.\n- W2. The motivation for limited SID understanding in alignment is not well verified. It is rough to only utilize a case in Figure 2 to validate the model only with SFT often fails to exploit SID histories.\n- W3. Only one public dataset is validated in the experiments.\n- W4. The code is not released, posing a challenge to reproduce this paper.\n- W5. The detailed illustration of the datasets is not seen in Appendix B."}, "questions": {"value": "All my questions have been included in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "glIbSMoPsf", "forum": "zw0IMYPLQZ", "replyto": "zw0IMYPLQZ", "signatures": ["ICLR.cc/2026/Conference/Submission870/Reviewer_NjZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission870/Reviewer_NjZ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575636702, "cdate": 1761575636702, "tmdate": 1762915632825, "mdate": 1762915632825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SINGER, a generative recommendation framework that addresses two key limitations of existing SFT-then-RL approaches: limited semantic understanding of SIDs and sparse RL rewards. SINGER incorporates full-process SID alignment by introducing auxiliary alignment tasks throughout training, which helps the LLM better capture the semantic structure of SIDs. In addition, it employs SID-guided reinforcement learning, combining prefix curriculum sampling with hierarchical SID-level rewards to provide richer, more informative learning signals, particularly for challenging samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents extensive experiments, evaluating the proposed method across multiple baselines and datasets.\n2. Applying reinforcement learning in the context of generative recommendation is an interesting and noteworthy exploration."}, "weaknesses": {"value": "1. While RL has been widely applied in recommender systems, it is usually motivated by objectives such as maximizing long-term user engagement. In this paper, the reward design appears to focus primarily on improving next-item prediction accuracy—a goal that could potentially be optimized effectively through SFT. It would be helpful if the authors could clarify why RL is particularly necessary or beneficial for this specific objective.\n2. The paper provides limited detail on the RL training procedure. For instance, in the “SFT-then-GRPO” experiment mentioned in the Introduction, it is unclear whether the model receives a single reward after generating the full SID sequence or computes a reward at each token step. Including a clear algorithmic flow of SINGER’s training stages could make the methodology easier to follow.\n3. The paper employs a SID-style hierarchical reward modeling approach, assigning different reward functions to samples of varying difficulty. While this is intended to encourage adaptive learning, it may raise concerns about potential inconsistencies in the optimization objective. It would be useful to clarify how the method ensures coherent learning across difficulty levels.\n4. To improve SID understanding, the paper designs alignment tasks. Prior work [1] proposes a similar approach, asking LLMs to predict the title of the next item given the historical SID sequence. It would help readers if the authors could explain how their alignment tasks differ from or extend these previous methods.\n5. The paper provides limited analysis of hyperparameters. It would be helpful if the authors could include an analysis of key hyperparameters related to RL training.\n\n[1]. Adapting large language models by integrating collaborative semantics for recommendation. ICDE 2024."}, "questions": {"value": "1. Could the authors further clarify why RL is necessary or particularly advantageous for optimizing next-item prediction accuracy compared to SFT alone?\n2. Could the authors provide evidence that using different reward functions for easy and hard samples does not introduce optimization inconsistencies, e.g., through training curves or gradient analyses?\n3. Could the authors elaborate on whether and how their proposed alignment tasks differ fundamentally from those used in LC-Rec [1] or related prior work?\n\n[1]. Adapting large language models by integrating collaborative semantics for recommendation. ICDE 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n0ZHCSIkRY", "forum": "zw0IMYPLQZ", "replyto": "zw0IMYPLQZ", "signatures": ["ICLR.cc/2026/Conference/Submission870/Reviewer_oLP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission870/Reviewer_oLP9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642872804, "cdate": 1761642872804, "tmdate": 1762915632710, "mdate": 1762915632710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to improve semantic ID-based recommendation models built on LLM backbones using reinforcement learning. The main idea is that the SFT-RL paradigm, which has proven effective for LLMs, may also benefit semantic ID-based recommendation models. The paper makes the following contributions:\n1. It verifies that the alignment tasks commonly used in the SFT stage can also be applied in the RL stage.\n2. It proposes two reward functions for the RL stage based on the hierarchical structure of semantic IDs."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a timely exploration of applying the RL training paradigm, which has been successful in LLMs, to semantic ID-based recommendation.\n2. By leveraging the hierarchical nature of semantic IDs, the paper proposes two well-motivated reward functions that improve both semantic ID generation and the alignment between semantic IDs and large language models.\n3. Extensive experiments are conducted on two public datasets."}, "weaknesses": {"value": "1. Experimental setting concerns. The current setup may produce false positives and inflated metrics.\n    1. According to Section 2.1, the authors use RQ-KMeans to produce three levels of semantic ID tokens for each item. In this way, different items may share the same semantic IDs, causing conflicts. Most existing methods add one extra non-semantic token per item to avoid such conflicts, as in TIGER (Rajput et al., 2024). However, this important treatment is not described in the paper. The provided prompts also suggest that the authors use only three tokens per item.\n    2. As multiple items can share the same semantic ID, generating a correct semantic ID does not necessarily mean predicting the correct item. If the evaluation counts predicting the ground-truth semantic ID as correct, the comparison with baselines (which predict the exact item) would be unfair.\n2. Missing references. The alignment objectives in Section 3.1 were first introduced in LC-Rec (Zheng et al., 2024), but this paper does not mention this fact when introducing these tasks.\n3. Lack of dataset details. The paper does not describe the benchmark processing procedure, including:\n    1. Dataset statistics after processing;\n    2. The method for splitting train/validation/test sets;\n    3. Whether any interactions or users were filtered during preprocessing.\n4. No available code. The code is not available during the review phase. Although the authors promise to release it later, this prevents verification of concerns such as semantic ID conflicts.\n5. The paper claims that \"RQ-KMeans is trained layer-wise for 1,000 steps per layer with a learning rate of 1×10^{-3}\" (Section B). However, RQ-KMeans is not a trainable model and does not require a learning rate, which makes this statement confusing."}, "questions": {"value": "1. How do the authors handle semantic ID conflicts, and how is the correctness of the predicted item evaluated?\n2. What is the detailed procedure for dataset processing?\n3. Could the authors clarify the statement about \"RQ-KMeans training\" mentioned in weakness point 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q1v7zOKHC3", "forum": "zw0IMYPLQZ", "replyto": "zw0IMYPLQZ", "signatures": ["ICLR.cc/2026/Conference/Submission870/Reviewer_rga4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission870/Reviewer_rga4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711345936, "cdate": 1761711345936, "tmdate": 1762915632576, "mdate": 1762915632576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses key limitations in the SFT-then-RL paradigm for generative recommendation: superficial Semantic ID (SID) understanding from SFT and ineffective, sparse rewards in RL.\nThe proposed SINGER framework tackles these issues by (1) integrating SID alignment objectives throughout the entire training process for deeper understanding, and (2) introducing a novel SID-Navigated Reinforcement Learning. This new RL method leverages the SID hierarchy to create fine-grained, level-based rewards and a curriculum sampling strategy for hard cases, effectively mitigating reward sparsity.\nExperiments show SINGER significantly outperforms strong baselines, validating its approach of deeply integrating hierarchical SID knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* By providing a profound analysis supported by quantitative data (Figure 1b) and qualitative cases (Figure 2), the paper establishes a clear and compelling motivation regarding the limitations of standard SFT-then-RL in recommendation.\n* The proposed SINGER framework, specifically the SIN-RL component, is novel and well-tailored to the problem. The SID-Prefix curriculum and hierarchical reward function ($R_{reason}$) elegantly address the issue of sparse rewards in generative recommendation.\n* The experiments are rigorous, showing consistent SOTA performance (Table 1), promising out-of-domain generalization (Table 2), and validating all core design choices through ablation studies (Figure 4).\n* The presentation is high-quality, and the detailed commitment to reproducibility (Section 5.2) regarding code and data release is commendable."}, "weaknesses": {"value": "* The technical implementation of \"Full-Process SID Alignment\" during the RL stage is unclear. It is described as \"jointly optimized\" (Lines 244-246), but the specific mechanism (e.g., auxiliary loss vs. data mixing) is not provided.\n* There is significant notational confusion with $\\beta$, which is used for both the GRPO KL penalty (Eq. 2) and the hierarchical reward decay (Eq. 8). Appendix B specifies \"$\\beta = 0.1$\" without clarifying which parameter this refers to, leaving the other unspecified."}, "questions": {"value": "* Could the authors define the reference policy $\\pi_{\\mathrm{ref}}$ used in Formula 2 (Line 190), as it is currently missing from the text?\n* In Section 4.1 (Lines 388-389), the text mentions \"three benchmark datasets—Industrial, Toys, and Books,\" yet the results only show \"Industrial\" and \"Office.\" Please correct this inconsistency.\n* Please clarify the naming for the OOD variant, which is inconsistently labeled as \"SINGER-w/ RL\" in the text (Line 403) and \"SINGER-w/o SFT\" in Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZBqwytKhq3", "forum": "zw0IMYPLQZ", "replyto": "zw0IMYPLQZ", "signatures": ["ICLR.cc/2026/Conference/Submission870/Reviewer_7cyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission870/Reviewer_7cyf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732383977, "cdate": 1761732383977, "tmdate": 1762936916796, "mdate": 1762936916796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}