{"id": "o29E01Q6bv", "number": 22452, "cdate": 1758331222454, "mdate": 1763703210388, "content": {"title": "LoongRL: Reinforcement Learning for Advanced Reasoning over Long Contexts", "abstract": "Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning  remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain,  a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question,   retrieve relevant facts and reason over them to answer correctly. \nRL training on KeyChain data induces an emergent plan–retrieve–reason–recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The  resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It  also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.", "tldr": "", "keywords": ["Long Context Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2e73b3443d3fa9e94287f4c6d929e3ce5be5c5f.pdf", "supplementary_material": "/attachment/346ea498c2098a93c3f95cd77dccf6ffa9bc8560.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LoongRL, a data-driven reinforcement learning (RL) method to improve long-context reasoning in LLMs, tackling the dual challenges of scarce high-difficulty data and high computational costs. The core contribution is KeyChain, a novel data synthesis technique that transforms short multi-hop QA into difficult long-context tasks by hiding the true question behind a multi-step UUID chain within a distractor-filled document. The authors find that training with RL on KeyChain data induces an emergent \"plan-retrieve-reason-recheck\" pattern that, critically, generalizes from 16K training contexts to 128K test contexts. This method yields substantial accuracy gains on Qwen2.5 models (+21-23%), allowing them to rival much larger frontier models while preserving short-context reasoning and retrieval abilities."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The KeyChain synthetic data is a clever and novel approach to enable long-context RL training. This work shows adding tracing and distractors in the multi-hop QA data can improve the reasoning of long-context tasks.\n2. The authors show models trained on 16K contexts can generalize to 128K, directly addressing the prohibitive computational cost of RL rollouts at full context length.\n3.  The authors designed their training data mix to prevent catastrophic forgetting of short-context and retrieval skills. The results in Table 2 (showing stable MMLU, MATH, and IFEval scores) and Figure 3 (perfect NIAH scores) are crucial for demonstrating that this specialization does not come at an unacceptable cost."}, "weaknesses": {"value": "1. The testing benchmark (LongBench and RULER) and training dataset are almost in the same domain (e.g., Wikipedia). We can see large improvement on these two benchmarks but relatively small improvement on LongBenchv2. It would be better to show more long-context benchmark improvements like HELMET, AA-LCR, MRCR, etc.\n2. The underlying reasoning data is derived entirely from multi-hop QA datasets (HotpotQA, MuSiQue, etc.). It is unclear if the emergent reasoning pattern would generalize to other long-context tasks, such as summarizing a novel or analyzing a large codebase."}, "questions": {"value": "1. Since you have included RULER QA, multi-key, multi-value NIAH in the training set, then can we still evaluate on RULER in Table 3? I think model may already learn the test data distribution in your RL training.\n2. Can we try training on HotPotQA keychain data but evaluate on other dataset like MuSiQue to see whether the training data can help to generalize in unseen dataset? Also, I am interested if your data can generalize your model to unseen domain more than Wikipedia. \n3. In Figure 4, you showed model learned to increase response length. Can you explain what includes in the response? Do model first learn to plan, then retrieve, reason and recheck? What skills do model learn progressively during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7zypHLvPQT", "forum": "o29E01Q6bv", "replyto": "o29E01Q6bv", "signatures": ["ICLR.cc/2026/Conference/Submission22452/Reviewer_ZCVK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22452/Reviewer_ZCVK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799074669, "cdate": 1761799074669, "tmdate": 1762942224071, "mdate": 1762942224071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Problem: Current RL-for-reasoning mostly targets short contexts; long-context scenarios require both retrieval from large inputs and multi-step reasoning. Direct long-context RL rollouts are costly; high-quality, verifiable, and challenging long-context data are scarce.\n- Key idea: A data-driven RL pipeline (LoongRL) centered on KeyChain, a synthesis method that transforms standard multi-hop QA into high-difficulty long-context tasks by inserting UUID key→value chains that hide the true question among distractors. Solving requires: follow chain → recover the real question → retrieve relevant evidence → reason → answer.\n- RL method: Train with GRPO on ~16K-token inputs to induce advanced patterns that purportedly generalize to 128K without full-length RL. Reward is outcome-only, via a rule-based “two-way substring exact match” applied to a boxed final answer to reduce reward hacking and tolerate formatting variants. A balanced data mix and multi-stage schedule preserve short-context capabilities.\n- Emergent pattern: Empirically, RL over KeyChain elicits a plan–retrieve–reason–recheck behavior that improves reliability and long-context generalization beyond training lengths.\n- Results: On Qwen2.5-7B/14B, LoongRL delivers large gains on long-context multi-hop QA (e.g., +23.5% and +21.1% absolute on LongBench v1), matches or approaches larger frontier models (o3-mini, DeepSeek-R1) at much smaller scales, improves Needle-in-a-Haystack and RULER (up to 128K), and preserves/mostly maintains general short-context abilities (MMLU, MATH, IFEval).\n- Ablations: (i) Replacing KeyChain with regular long-context QA yields notably lower gains; (ii) two-way substring match outperforms F1, LLM-as-judge, and strict EM as a training reward; (iii) multi-stage curriculum helps."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Clear, focused objective and thoughtful problem decomposition:\n  - Tackles a real gap: moving beyond retrieval to robust long-context reasoning.\n  - Designs data and training to be verifiable and compute-aware (shorter rollouts, longer generalization).\n- Novel and pragmatic data construction:\n  - KeyChain is a neat way to force chain tracing and disambiguation under heavy distractors, requiring both retrieval and reasoning.\n  - Uses real QA seeds (HotpotQA, MuSiQue, 2Wiki) to ground tasks in natural language rather than entirely synthetic QA.\n- RL methodology matched to constraints:\n  - Outcome-only rule-based reward (boxed answer + two-way substring) is a strong, reproducible alternative to LLM-as-judge, with an ablation showing its advantage for training.\n  - GRPO and small KL with a staged curriculum is standard yet well-justified to stabilize training.\n- Strong empirical results at favorable scale/cost:\n  - Large absolute gains vs. strong baselines on LongBench v1, competitive with much larger models.\n  - Convincing length generalization from 16K training to 128K evaluation across NarrativeQA, RULER, and Needle-in-a-Haystack.\n  - Maintains short-context and instruction-following capabilities, often outperforming R1-distilled counterparts that degrade on these.\n- Diagnostic evidence:\n  - Ablations disentangle the contribution of KeyChain vs. conventional long-context QA.\n  - Qualitative trajectories demonstrate the claimed plan–retrieve–reason–recheck behavior.\n- Reproducibility:\n  - Clear training details, datasets, prompts, and claim of releasing code and sample data."}, "weaknesses": {"value": "- Synthetic structure risk:\n  - The KeyChain format (UUID chains with a designated starting key) has a highly regular, explicit structure. There is a risk the model learns to exploit these patterns rather than developing general long-context reasoning skills. Although downstream improvements suggest transfer, additional tests against structurally varied chains would better establish robustness.\n- Reward and evaluation concerns:\n  - Two-way substring match is pragmatic but may still admit false positives (e.g., partial overlap on ambiguous entities) or false negatives (paraphrases). The ablation is helpful, but more analysis on reward precision/recall would increase confidence against reward hacking.\n  - Inference differences across baselines (temperature, sampling, pass@1 vs pass@k) and the use of YaRN to extend contexts for only some models can complicate fairness. While partially addressed, I recommend additional matched settings or a dedicated “strictly matched” comparison table.\n- Overlap and contamination:\n  - Training uses multi-hop QA seeds overlapping with evaluation domains (e.g., HotpotQA). The paper does not explicitly state safeguards for train/test separation or whether exact question/passages overlap with benchmark test items after augmentation. Without a rigorous de-duplication policy, there is a risk of leakage.\n- Limited statistical characterization:\n  - No variance across seeds or CI reported; improvements are large but stability under re-runs is not quantified.\n- Claims of emergent patterns:\n  - The plan–retrieve–reason–recheck behavior is shown qualitatively. A quantitative metric (e.g., automated tagging of plan/recheck segments; retrieval step correctness rates) would make the claim stronger and trackable across training stages and datasets.\n- Scope and breadth:\n  - Most evaluations are reading comprehension/multi-hop QA and retrieval. It would be informative to include tasks where the query is obscured in different ways (tables, code bases, non-UUID chains), or tool-augmented settings (e.g., retrieval tools outside the context), to demonstrate broader generalization."}, "questions": {"value": "- Data and leakage\n  - Please detail the exact splitting and de-duplication strategy to ensure that augmented KeyChain instances do not overlap (content-wise) with evaluation items in LongBench v1/v2. How do you prevent direct reuse of the same question/evidence pairs?\n  - When filling long contexts with distractors (including reusing documents from filtered tasks), how do you ensure no evaluation leakage?\n- Reward design\n  - Provide analyses of reward false positives/negatives: e.g., on a held-out set with human-verified correctness, report precision/recall for two-way substring vs EM/F1/LLM-as-judge. Include examples of where two-way substring fails and mitigation (e.g., normalization, entity linking).\n  - Clarify the reference policy used for KL (π_ref): is it the base instruct model frozen? Any mixing with SFT references?\n- Fairness and settings\n  - Add a “strictly matched inference” comparison where all models use the same temperature, pass@1, and no YaRN (or apply YaRN uniformly where possible) on a subset of tasks, to isolate the contribution of LoongRL vs. differences in decoding/context-extensions.\n  - Report sensitivity to decoding hyperparameters (temperature/top-p) on the main long-context benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "goZfBet2q1", "forum": "o29E01Q6bv", "replyto": "o29E01Q6bv", "signatures": ["ICLR.cc/2026/Conference/Submission22452/Reviewer_kHZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22452/Reviewer_kHZc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858127744, "cdate": 1761858127744, "tmdate": 1762942223817, "mdate": 1762942223817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's core contribution is an RL training task designed to elicit reasoning behaviours that are adapted to long-context problems. The authors augment existing multi-hop QA tasks by adding additional documents to the problem context and constructing key-value chains that need to be traversed for the appropriate question to be retrieved and then answered based on the documents' content. Experiments show that this task composition incentivises models to learn a structured plan, retrieve, reason, re-check reasoning pattern for long-context tasks. Two-way substring matching is used for verifiable rewards. A mixture of KeyChain, standard QA, retrieval and math reasoning tasks are used for training to improve long-context reasoning and maintain short-context reasoning performance. Results show that short-context reasoning is indeed preserved and 16k training contexts lead to long-context reasoning abilities that generalise to 128k evaluation contexts. Needle-in-a-Haystack evaluations are passed with 100% accuracy, demonstrating strong retrieval performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important open problem: reasoning over long contexts beyond basic retrieval. By enabling RL to target nontrivial but verifiable long-context reasoning problems, the KeyChain dataset overcomes a key bottleneck of long-context RL finetuning. The results indicate strong empirical performance gains on long-context benchmarks without regressing on the short-context reasoning benchmarks considered, however the latter is expected given the inclusion of short-context reasoning tasks in the training mixture."}, "weaknesses": {"value": "The KeyChain data construction is highly task-specific: synthetic multi-hop QA with UUID breadcrumbs. It is unclear from the current results if the learned reasoning behaviour generalises to other domains, such as open-ended dialogue, summarisation, or multi-document synthesis. There is no evaluation on long-context generation tasks, which has been artificially decoupled from reasoning over long, static input contexts. A major claim is the emergence of a general reasoning pattern for long-context problems, but it is unclear whether the structured reasoning pattern that is qualitatively demonstrated arises from task scaffolding (i.e., explicit chain following) rather than self-organised reasoning behaviour. Finally, the ablations are currently incomplete, particularly with regards to the multi-stage curriculum. Whilst accuracy is showing to rise across stages, there is no direct comparison between the full curriculum and: skipping the warm-up, doing a single RL stage on all data, or removing the Stage II difficulty filtering. The wam-up stage is even omitted for the 14B model, but the impact of this on results is never shown."}, "questions": {"value": "How did you detect or measure the plan, retrieve, reason, recheck reasoning pattern?\n\nCould you show how much each curriculum stage contributes independently?\n\nHave you tested LoongRL on non-QA long-context tasks (e.g., summarisation, code reasoning, document comparison)?\n\nHow do the training costs for LoongRL compare to training QwenLong-L1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kV34AVzyhc", "forum": "o29E01Q6bv", "replyto": "o29E01Q6bv", "signatures": ["ICLR.cc/2026/Conference/Submission22452/Reviewer_WxW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22452/Reviewer_WxW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932959824, "cdate": 1761932959824, "tmdate": 1762942223600, "mdate": 1762942223600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's core contribution is an RL training task designed to elicit reasoning behaviours that are adapted to long-context problems. The authors augment existing multi-hop QA tasks by adding additional documents to the problem context and constructing key-value chains that need to be traversed for the appropriate question to be retrieved and then answered based on the documents' content. Experiments show that this task composition incentivises models to learn a structured plan, retrieve, reason, re-check reasoning pattern for long-context tasks. Two-way substring matching is used for verifiable rewards. A mixture of KeyChain, standard QA, retrieval and math reasoning tasks are used for training to improve long-context reasoning and maintain short-context reasoning performance. Results show that short-context reasoning is indeed preserved and 16k training contexts lead to long-context reasoning abilities that generalise to 128k evaluation contexts. Needle-in-a-Haystack evaluations are passed with 100% accuracy, demonstrating strong retrieval performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important open problem: reasoning over long contexts beyond basic retrieval. By enabling RL to target nontrivial but verifiable long-context reasoning problems, the KeyChain dataset overcomes a key bottleneck of long-context RL finetuning. The results indicate strong empirical performance gains on long-context benchmarks without regressing on the short-context reasoning benchmarks considered, however the latter is expected given the inclusion of short-context reasoning tasks in the training mixture."}, "weaknesses": {"value": "The KeyChain data construction is highly task-specific: synthetic multi-hop QA with UUID breadcrumbs. It is unclear from the current results if the learned reasoning behaviour generalises to other domains, such as open-ended dialogue, summarisation, or multi-document synthesis. There is no evaluation on long-context generation tasks, which has been artificially decoupled from reasoning over long, static input contexts. A major claim is the emergence of a general reasoning pattern for long-context problems, but it is unclear whether the structured reasoning pattern that is qualitatively demonstrated arises from task scaffolding (i.e., explicit chain following) rather than self-organised reasoning behaviour. Finally, the ablations are currently incomplete, particularly with regards to the multi-stage curriculum. Whilst accuracy is showing to rise across stages, there is no direct comparison between the full curriculum and: skipping the warm-up, doing a single RL stage on all data, or removing the Stage II difficulty filtering. The wam-up stage is even omitted for the 14B model, but the impact of this on results is never shown."}, "questions": {"value": "How did you detect or measure the plan, retrieve, reason, recheck reasoning pattern?\n\nCould you show how much each curriculum stage contributes independently?\n\nHave you tested LoongRL on non-QA long-context tasks (e.g., summarisation, code reasoning, document comparison)?\n\nHow do the training costs for LoongRL compare to training QwenLong-L1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kV34AVzyhc", "forum": "o29E01Q6bv", "replyto": "o29E01Q6bv", "signatures": ["ICLR.cc/2026/Conference/Submission22452/Reviewer_WxW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22452/Reviewer_WxW6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932959824, "cdate": 1761932959824, "tmdate": 1763751378449, "mdate": 1763751378449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a synthetic training data construction method to improve long context reasoning for large language models. Basic idea is to insert irrelevant documents in addition to the relevant one as a challenging dataset together with key-value chains or arbitrary key-strings, e.g., UUIDs, so that the model has to traverse key-value pairs until reaching the correct question. Experiments are carried out on standard long-context reasoning benchmarks, e.g., HotpotQA, and general benchmarks, e.g., MMLU, showing gains when compared with other large language models with long reasoning capacity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work proposes an interesting approach of synthetic data construction for training a language model for long-context reasoning. Basic idea is to insert distractors both for contexts and questions so that a model has to pay attention not only the correct context, but correct question at the same time. It is interesting that a model trained only on \"shorter\" context, i.e., 16K, can scale to 128K contexts.\n- The idea of inserting key-value pairs is quite interesting so that a model has to traverse key and value pairs until reaching the correct question. This additional complexity might enforce a model to pay more attention to the question itself during the GRPO training.\n- Experiments are carried out systematically by comparing with diverse models and settings."}, "weaknesses": {"value": "- The motivation is not clear why UUIDs are used as keys. There exist alternatives, e.g., entity names, or other random strings, could be possible.\n- The detail settings are missing, e.g., the number of distractor contexts and questions, inserted to construct the synthetic dataset. It is also not clear whether the distractor questions are related to the irrelevant contexts already inserted in the long context filling step."}, "questions": {"value": "- The performance is evaluated on the standard dataset without distractor contexts or questions, but I'm curious of the performance when such distractors are inserted for LoongRL and other models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EIIg5MGt7o", "forum": "o29E01Q6bv", "replyto": "o29E01Q6bv", "signatures": ["ICLR.cc/2026/Conference/Submission22452/Reviewer_6KJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22452/Reviewer_6KJK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960177355, "cdate": 1761960177355, "tmdate": 1762942223324, "mdate": 1762942223324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}