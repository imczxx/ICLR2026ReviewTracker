{"id": "OswqOlTYR2", "number": 230, "cdate": 1756731933612, "mdate": 1759898270848, "content": {"title": "Consis-GCPO: Consistency-Preserving Group Causal Preference Optimization for  Vision Customization", "abstract": "Subject-driven generation faces a fundamental challenge: achieving high subject fidelity while maintaining semantic alignment with textual descriptions. While recent GRPO-based approaches have shown promise in aligning generative models with human preferences, they apply uniform optimization across all denoising timesteps, ignoring the temporal dynamics of how textual and visual conditions influence generation. We present Consis-GCPO, a causal reinforcement learning framework that reformulates multi-modal condition generation through discrete-time causal modeling. Our key insight is that different conditioning signals exert varying influence throughout the denoising process—text guides semantic structure in early steps while visual references anchor details in later stages. By introducing decoupled causal intervention trajectories, we quantify instantaneous causal effects at each timestep, transforming these measurements into temporally-weighted advantages for targeted optimization. This approach enables precise tracking of textual and visual contributions, ensuring accurate credit assignment for each conditioning modality. Extensive experiments demonstrate that Consis-GCPO significantly advances personalized generation, achieving superior subject consistency while preserving strong text-following capabilities, particularly excelling in complex multi-subject scenarios.", "tldr": "", "keywords": ["Multi-Subject Personalized Generation", "Diffusion Model", "Reinforcement Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e4a1d6be560aec49675bffc1082f8c00ae766d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new online reinforcement learning framework for personalized generation by performing causal modeling that provides temporally weighted advantages for optimization at each denoising timestep. Experiments on DreamBench and the newly proposed Dream-VBench demonstrate that the method achieves better performance than existing frameworks in both personalized image and video generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written and well-structured, with informative figures such as the challenge overview (Figure 2) and the training pipeline (Figure 3).\n* Introducing a causal framework for computing weighted advantages is both novel and insightful. The causal interpretation of multi-modal conditioning (text and visual reference) adds a principled perspective to reinforcement learning–based generation.\n* The experiments are extensive, including both image and video generation. The introduction of Dream-VBench further validates the generalizability of the method across modalities."}, "weaknesses": {"value": "* One key idea of the paper is to *entangle feedback* from text and image references. However, the proposed method still computes a single reward by summing the weighted advantages from both modalities. This seems to reduce the degree of entanglement. A more entangled approach might involve alternately fixing one modality and computing separate rewards for the other, then combining them to guide optimization.\n* The main method section could benefit from a deeper explanation of how the causal effect estimation directly contributes to solving the *temporal blindness* problem of previous GRPO methods. The framework is sound, but the \"why\" behind the causal weighting’s effectiveness would be clearer with an intuitive example or a brief theoretical justification.\n\n---\n\n**Overall**: The paper presents an interesting and timely idea, and the experimental results are strong, with comprehensive analysis and solid baselines. Although the method description could be clearer in connecting causal modeling to the core problem formulation, the contribution and potential impact are evident. Therefore, I lean toward accepting the paper."}, "questions": {"value": "The questions are mainly about the weaknesses:\n\n1. Could the authors explain in more detail how the proposed method resolves the entangled feedback issue, given that the final optimization still uses a single scalar reward?\n2. Why is the proposed causal weighting expected to provide a more reliable temporal signal for the reward, and how does it avoid the uniform timestep bias present in GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ksgiLgu72u", "forum": "OswqOlTYR2", "replyto": "OswqOlTYR2", "signatures": ["ICLR.cc/2026/Conference/Submission230/Reviewer_XhRE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission230/Reviewer_XhRE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761154211356, "cdate": 1761154211356, "tmdate": 1762915475989, "mdate": 1762915475989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work primarily addresses the limitations in the subject-driven generation domain, where current methods struggle to simultaneously ensure object fidelity and text alignment. The authors first identify the inherent issues of traditional GRPO algorithms and propose targeted solutions, namely decoupled causal intervention and temporally-weighted advantage computation mechanisms. These innovations culminate in the Consis-GCPO framework, which successfully resolves the existing challenges. The proposed approach demonstrates significant improvements over state-of-the-art personalized generation methods, achieving superior subject consistency while preserving strong text-following capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The inherent issues of directly transferring GRPO to the subject-driven generation domain have been identified. \n- Consis-GCPO demonstrates advantages over Flow-GRPO and Dance-GRPO in both qualitative and quantitative experiments."}, "weaknesses": {"value": "- The article is not well-written in certain areas. For instance, in line 275, the relationship between this sentence and Figure 3(c) is unclear. Additionally, the caption for Figure 3 does not correspond correctly with the subfigures, leading to confusion. In line 180, the sentence structure is convoluted, and the logical relationship is not clearly articulated, causing semantic confusion. Furthermore, there is a missing space between words in line 239.\n- The challenge of measuring the degree to which an action influences the final reward, especially in the long term, is a classic problem in reinforcement learning, and has led to many elegant solutions. In this paper, the use of $\\delta _{P/I_r}^{(g)}(t^`)$ to indicate the causal dependence on the conditioning signal at that timestep is a rather naive and imprecise estimate, as it only represents a single Monte Carlo sample.\n- The paper claims: \"Our key insight is that different conditioning signals exert varying influence throughout the denoising process—text guides semantic structure in early steps while visual references anchor details in later stages\", is there any theoretical or experimental evidence to support this claim?"}, "questions": {"value": "- How was the data preprocessed? \n- Why was the FFHQ dataset used for image tasks, considering it is focused on facial data?\n-  What dataset was used for video tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2FLPBbzUW", "forum": "OswqOlTYR2", "replyto": "OswqOlTYR2", "signatures": ["ICLR.cc/2026/Conference/Submission230/Reviewer_6q4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission230/Reviewer_6q4d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487238354, "cdate": 1761487238354, "tmdate": 1762915475251, "mdate": 1762915475251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Consis-GCPO**, a causal reinforcement learning framework for subject-driven image and video generation. The key innovation is reformulating multi-modal conditional generation through discrete-time causal modeling with **step-wise causal interventions**. \n\nUnlike existing GRPO methods (Flow-GRPO, DanceGRPO) that apply uniform optimization across all denoising timesteps, Consis-GCPO (1) introduces **decoupled causal intervention trajectories** that selectively **ablate text prompts or visual references** at specific timesteps, and (2) quantifies **instantaneous causal effects** to measure when textual vs. visual conditions are most critical. These effects are further converted into temporally-weighted advantages for targeted optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated problem**: a fundamental limitation in existing GRPO methods—\"temporal blindness\" to how text and visual conditioning vary in importance during denoising.\n\n2. **Strong empirical results**: The experimental results show consistent improvements across all metrics, particularly notable in multi-subject scenarios (*e.g.*, CLIP-I: 0.772 vs. 0.750 for Dance-GRPO).\n\n3. Table 3 effectively demonstrates the necessity of both prompt and reference interventions."}, "weaknesses": {"value": "1. I'm concerned about the causal identification assumptions. In my view, the text and image conditions are likely **not causally independent**, they interact through the model's attention mechanisms. In Equation (5), ablating $P$ or $I_r$ at timestep $t'$ is presented as a causal intervention $\\text{do}(C=∅, t')$, but what ensures this ablation isolates the causal effect rather than just correlation?"}, "questions": {"value": "1. How exactly is \"ablating\"  $P$ or $I_r$ implemented? Is it by setting it to an empty string/zero tensor? Or by using an unconditional model/learned zero embedding?\n\n2. **Sensitivity Analysis**: How sensitive is the method to the temperature parameter $\\tau$ in Eq. 11? And Can you provide error bars and statistical significance tests for all metrics in Tables 1-2?\n\n3. Can the authors provide some **failure cases** and discuss when the method fails or performs poorly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GFuHZgTTzs", "forum": "OswqOlTYR2", "replyto": "OswqOlTYR2", "signatures": ["ICLR.cc/2026/Conference/Submission230/Reviewer_AUFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission230/Reviewer_AUFx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906741296, "cdate": 1761906741296, "tmdate": 1762915475101, "mdate": 1762915475101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core conflict in personalized generation: balancing subject consistency (visual fidelity) with semantic alignment (following the text prompt). The authors' key insight is that this is a temporal problem: the text prompt matters most in the early denoising steps to set the scene, while the visual reference matters most in the late steps to lock in details."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Identifying the temporal nature of the text-vs-visuals conflict is a brilliant insight.\n\n- The \"causal intervention\" approach is a smart, non-hand-wavy way to measure the actual contribution of each modality at each step.\n\n- The results are fantastic. The qualitative images (like the \"standing\" bear in Fig 4) show it's really following the prompt while holding the subject's identity. It also wins quantitatively on both images and video (Tables 1, 2).\n\n- As a bonus, Figure 7 shows it converges 1.4x faster (in GPU hours) than the baseline."}, "weaknesses": {"value": "The main concern is computational cost. The method seems to require running multiple full-denoising rollouts for each training step to get the causal effects. The paper's claim of a 1.4x speedup (Fig 7) feels contradictory to this apparent 3x increase in work. This trade-off needs to be made much clearer."}, "questions": {"value": "- Your method seems to imply 3x the rollouts per step. Can you clarify the actual sampling overhead? How does that square with the 1.4x wall-clock speedup you show in Figure 7?\n\n- You measure the temporal importance of text vs. visuals. I'd love to see a graph of these weights ($\\omega_P(t)$ and $\\omega_{I_r}(t)$) over the denoising timesteps $t$. This would be the ultimate proof of your hypothesis!\n\n- How sensitive is the model to the balancing coefficients ($\\lambda_P$, $\\lambda_{I_r}$)? Was it hard to find a good balance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4u4psgzkTB", "forum": "OswqOlTYR2", "replyto": "OswqOlTYR2", "signatures": ["ICLR.cc/2026/Conference/Submission230/Reviewer_1Guz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission230/Reviewer_1Guz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840711109, "cdate": 1762840711109, "tmdate": 1762915474980, "mdate": 1762915474980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}