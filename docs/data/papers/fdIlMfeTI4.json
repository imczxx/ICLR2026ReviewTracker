{"id": "fdIlMfeTI4", "number": 15353, "cdate": 1758250492406, "mdate": 1759897312187, "content": {"title": "GEAR: A $\\textbf{G}$eneral $\\textbf{E}$valuation Framework for $\\textbf{A}$bductive $\\textbf{R}$easoning", "abstract": "Since the advent of Large Language Models (LLMs), research has primarily focused on improving their instruction-following and deductive reasoning abilities. Yet a central question remains: can these models truly discover new knowledge, and how can we evaluate this ability? In this work, we address this gap by studying abductive reasoning-the process of generating plausible hypotheses to explain observations.\nWe introduce **G**eneral **E**valuation for **A**bductive **R**easoning (GEAR), a new general-purpose, fully automated, transparent, and label-free evaluation paradigm that overcomes limitations of prior approaches. GEAR evaluates a set of hypotheses using three metrics: **consistency** (each hypothesis correctly explains the given observations), **generalizability** (consistent hypotheses make meaningful predictions on unseen inputs), and **diversity** (the set of hypotheses covers many distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers needed), reliable (transparent, deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new, plausible hypotheses, unlike existing static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four popular abduction benchmarks ($1{,}500$ problems), generating $50{,}340$ candidate hypotheses. GEAR reveals model differences and insights that are obscured by prior gold-answer-based or purely human evaluations.\nWe further propose a momentum-based curriculum training strategy that dynamically adjusts GEAR-derived training data by learning velocity: it begins with what the model learns faster and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives (e.g., instruction following and consistency). Without gold-label supervision, this strategy improves all three GEAR objectives—consistency, generalizability, and diversity—and these gains transfer to established abductive-reasoning benchmarks. Taken together, GEAR provides a principled framework that not only evaluates abduction but also supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses. We will release code and data upon acceptance.", "tldr": "", "keywords": ["Abductive reasoning", "Hypothesis generation", "LLM evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca0103978c934910d530ea3d60e54ed65e75809e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GEAR, an automated, label-free evaluation framework intended to assess abductive reasoning capabilities in large language models. Empirical results across nine models show that GEAR reveals finer-grained performance differences than traditional gold-label metrics  and that the momentum curriculum improves GEAR scores and modestly enhances downstream abductive benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper convincingly argues that existing abductive reasoning benchmarks relying on single \"gold\" labels or human judgments are inadequate for underdetermined reasoning problems.\n- The paper evaluates 9 diverse LLMs across 4 datasets and provides simulation studies demonstrating that abductive reasoning is defeasible"}, "weaknesses": {"value": "- Although the framework is titled General Evaluation for Abductive Reasoning, all experiments are performed on programmable and synthetic tasks (e.g., ARC, ACRE, LIST FUNCTIONS). \n\n- The β/γ-diversity measures are based purely on output pattern dissimilarity (set overlap/Jaccard distance) - may capture syntactic or behavioral heterogeneity, not genuine content diversity.\n\n- While the proposed adaptive weighting method is sensible, the results (Table 2, 4) show only marginal gains, and the analysis of why or how these improvements occur is minimal. The ablation studies offer little evidence of actual reasoning enhancement."}, "questions": {"value": "- The β/γ-diversity metrics are based on prediction overlap. How do you ensure these metrics capture conceptual rather than purely behavioral diversity?\n\n- Have you analyzed qualitative examples of generated hypotheses to verify that improvements in β/γ-diversity correspond to more meaningful explanations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a1AJXu0x6E", "forum": "fdIlMfeTI4", "replyto": "fdIlMfeTI4", "signatures": ["ICLR.cc/2026/Conference/Submission15353/Reviewer_brvY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15353/Reviewer_brvY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760783524524, "cdate": 1760783524524, "tmdate": 1762925640408, "mdate": 1762925640408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GEAR (General Evaluation for Abductive Reasoning), a new framework for evaluating the abductive reasoning capabilities of Large Language Models (LLMs). The authors argue that existing evaluations, which often rely on a single \"gold\" hypothesis, are insufficient for abduction. GEAR proposes a label-free, automated evaluation based on three metrics: (1) Consistency (the hypothesis must explain the given observations), (2) Generalizability (the hypothesis should make meaningful predictions on unseen inputs), and (3) Diversity (the model should generate a set of distinct and varied hypotheses). The paper evaluates several LLMs using GEAR on four abduction benchmarks. Additionally, it demonstrates that these metrics can be used as a preference signal to fine-tune LLMs, thereby improving their performance on these same metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clarity: The paper is well-written and easy to read.\n2. Sensible Criteria: The three proposed criteria for evaluation (Consistency, Generalizability, and Diversity) are sensible, logical, and well-grounded in the principles of scientific reasoning."}, "weaknesses": {"value": "1. Limited Novelty of the Criteria: The primary concern is the limited conceptual novelty of the framework's components. The core metrics proposed are largely a formalization of well-established principles:\n     + Consistency (matching observed data) is a fundamental requirement in almost any task.\n     + Generalizability (testing on unseen data) is analogous to standard test set evaluation.\n     + Diversity (generating distinct solutions) is a known metric in related fields like program synthesis.\nThe main contribution appears to be the combination of these ideas, rather than a fundamentally new evaluation paradigm.\n\n2. Constrained Domain and Overstated Generality: The framework's key advantages (e.g., being \"label-free\" and \"automated\") are heavily dependent on its constrained domain of application. The evaluation is exclusively demonstrated on benchmarks (like MINI-ARC, ACRE) where hypotheses are executable programs. This executable nature is what makes automated checks for Consistency and Generalizability possible; an execution oracle exists. This limits the \"general\" claim of the framework.\nThe paper does not solve the core, much harder problem of evaluating open-domain natural language abduction, where ambiguity, logical equivalence, and the lack of an execution oracle are the main challenges. \nFurthermore, given the executable setup, using these automated checks as preference signals is a straightforward application of reinforcement learning from feedback. While effective, it feels like an incremental contribution rather than a novel one, especially when the setup itself is so constrained."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yV8h89kYnr", "forum": "fdIlMfeTI4", "replyto": "fdIlMfeTI4", "signatures": ["ICLR.cc/2026/Conference/Submission15353/Reviewer_MKD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15353/Reviewer_MKD8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825573341, "cdate": 1761825573341, "tmdate": 1762925639463, "mdate": 1762925639463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights that many existing benchmarks for abductive reasoning rely on a single annotated “gold” hypothesis, which is limiting because multiple explanations may be equally valid. Rather than introducing new datasets, the framework described in this paper (GEAR) restructures several existing benchmarks — MINI-ARC, ARC-2025, LIST FUNCTIONS, and ACRE to evaluate the consistency, generalizability, and diversity of candidate hypotheses. While the framework is conceptually clear, it is difficult to follow precisely how the the restructuring is implemented. In particular, it appears that instances (or existing groups of tasks?) are grouped to test individual hypotheses across multiple observations, but the specifics are not fully described. Moreover, it is unclear how textual benchmarks such as ACRE are evaluated for alternative plausible hypotheses, leaving some ambiguity about the method’s applicability to natural-language domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Thoughtful reformulation of existing tasks to allow evaluation of multiple plausible hypotheses.\n- Evaluation across multiple large language models"}, "weaknesses": {"value": "- Several parts of the paper are unclear, particularly the construction of the evaluation datasets and the specifics of model evaluation.\n- The computation of metrics such as Instruction-Following Rate is not clearly defined."}, "questions": {"value": "1. Could you provide more details or examples of how the evaluation datasets were constructed for each benchmark, including how examples were grouped and how synthetic or additional test cases were generated? How does one ensure that the grouping is always feasible and sound?\n\n2. Could you clarify the training procedure — specifically, how the GEAR scores were used during training and how evaluation on unseen portions of the original tasks was performed?\n\n3. How exactly was the Instruction-Following Rate in Table 2 computed, and how does it interact with other metrics?\n\n4. Were metrics from the original benchmarks used in any way, or is evaluation entirely based on GEAR’s framework?\n\nOverall, I find the conceptual contribution promising, but the lack of clarity in critical implementation details makes it difficult to fully assess the experimental results. I look forward to clarifications during the author discussion period and will revise my review accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DkC113Onju", "forum": "fdIlMfeTI4", "replyto": "fdIlMfeTI4", "signatures": ["ICLR.cc/2026/Conference/Submission15353/Reviewer_9oVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15353/Reviewer_9oVs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874881036, "cdate": 1761874881036, "tmdate": 1762925638794, "mdate": 1762925638794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GEAR, a label-free framework for evaluating abductive reasoning in LLMs. It assesses sets of hypotheses using three interpretable criteria: (1) Consistency with observations, (2) Generalizability across a defined input space, and (3) Diversity among alternative explanations. The authors applied GEAR to four code-based abductive benchmarks and nine LLMs. It reveals that even large models struggle with consistency and produce limited diversity. The authors further use GEAR as a training signal through a momentum-based curriculum DPO approach, improving both diversity and downstream abductive accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The authors introduced a principled, label-free evaluation of abductive reasoning grounded in philosophy of science.\n\nSignificance: The paper addresses a fundamental gap in assessing LLMs’ capacity to generate plausible, falsifiable hypotheses.\n\nQuality: Experiments are extensive (9 LLMs, 50K hypotheses) and include insightful analyses, e.g., defeasibility and the correlation between GEAR scores and generalization.\n\nClarity: Well-written with precise definitions and clear visualisations. The momentum-based curriculum is simple yet effective."}, "weaknesses": {"value": "1. The framework depends on a predefined sample space and deterministic execution, limiting use beyond structured domains.\n\n2. Training evaluation omits comparisons to other prompting baselines.\n\n3. Heuristic design choices (e.g., 3-error stopping rule, sample-space construction) lack sensitivity analysis.\n\n4. The interpretability of diversity metrics (γ, β) could be improved with qualitative examples."}, "questions": {"value": "1. Why does model size not correlate with abductive diversity—does pretraining or alignment explain this?\n\n2. Could you elaborate on Line 415 (“some benefit from earlier emphasis on format/consistency, others from earlier diversity”)? And can you provide qualitative examples illustrating what “high-diversity” vs. “low-diversity” hypotheses look like?\n\n3. How could GEAR extend to natural-language abduction tasks beyond formal domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7jTJncJbqt", "forum": "fdIlMfeTI4", "replyto": "fdIlMfeTI4", "signatures": ["ICLR.cc/2026/Conference/Submission15353/Reviewer_5GLN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15353/Reviewer_5GLN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922436631, "cdate": 1761922436631, "tmdate": 1762925638371, "mdate": 1762925638371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all Reviewers"}, "comment": {"value": "Dear Reviewers,\n\n- We sincerely thank all of you for your time and for providing such thorough, high-quality reviews of our paper. Your thoughtful comments, challenging questions, and constructive criticisms have been invaluable to us. We appreciate the deep engagement with our work.\n\n- We have posted detailed, individual responses to each review (5GLN, 9oVs, MKD8, and brvY) to address every point raised, from clarifying our experimental setup and metrics to discussing the core conceptual framing of our contribution.\n\n- We hope these responses successfully address your concerns and clarify the novelty and significance of GEAR.  If any of our answers are unclear, or if new questions arise, we would be more than happy to dive into a more in-depth discussion.\n\nThank you once again for your feedback!"}}, "id": "iW0oYerBcC", "forum": "fdIlMfeTI4", "replyto": "fdIlMfeTI4", "signatures": ["ICLR.cc/2026/Conference/Submission15353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15353/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission15353/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763242655110, "cdate": 1763242655110, "tmdate": 1763242726872, "mdate": 1763242726872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}