{"id": "CFuNu8dK4s", "number": 12434, "cdate": 1758207785891, "mdate": 1759897510113, "content": {"title": "Vidar: Embodied Video Diffusion Model for Generalist Manipulation", "abstract": "Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and pixel-to-action VLA pipelines typically degenerate under background and viewpoint shifts. In this paper, we present Vidar, a prior-driven, low-shot adaptation paradigm that replaces most embodiment-specific data with transferable video priors. Vidar consists of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) adapter based on a key decoupling of the policy. The embodied diffusion model is pre-trained on Internet-scale videos and then domain-adapted to 750K multi-view trajectories from three real-world robot platforms using a unified observation space encoding robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. Crucially, the generative video prior models the distribution of plausible, temporally coherent interactions, implicitly capturing affordances, contact dynamics, and physical consistency from massive unlabeled video. This shifts the challenge from collecting large amounts of new robot data to efficiently aligning a rich prior with a new embodiment. With only \\~20 minutes of human demonstrations on an unseen robot (\\~1% of typical data), Vidar outperforms state-of-the-art VLA baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for ``one prior, many embodiments'': strong, inexpensive video priors + minimal on-robot alignment.", "tldr": "", "keywords": ["Robotics", "Video Diffusion Model", "Computer Vision"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/286c857060a08bba1526327d5d63ed678d495d4d.pdf", "supplementary_material": "/attachment/800ae62e2c85f1f1900121ca4f82206892dd63d0.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims to achieve generalist few-shot policy learner by leveraging embodiment agnostic priors such as video generative models. Specifically, the authors decompose the generalist robot policy as a composition of embodiment agnostic video predictive prior and embodiment specific adaptor. To improve the data efficiency and generalizability, a sparse mask-based regularization is proposed. Instead of naively conditioning the embodiment specific adaptor, the proposed method learns a parsimonious mask. As can be seen from the provided videos and figures, this enables the model to correctly filter out pixels that are irrelevant to predicting the grounded low-level action. Experimental results demonstrate this omission of information indeed helps downstream adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The qualitative visual results clearly show that the learned model can successfully point out the pixels that are relevant to predicting the low level policy. The methodology and claimed benefits are sound, and these are also well supported quantitatively through several experiments."}, "weaknesses": {"value": "* This kind of model falls into a broader category of learning unified action/observation representation. Comparing the proposed mask-based approach with other embodiment-agnostic action representation such as flow-based ones [1] would further strengthen the paper's argument. \n\n* Several works have already explored filtering out action-irrelevant information from visual inputs. For instance, [2] proposed more general information-theoretic approach that is not limited to pixel-aligned masking. \n\n[1] \"General Flow as Foundation Affordance for Scalable Robot Learning,\" Yuan et al., 2024\n\n[2] \"Temporal Predictive Coding For Model-Based Planning In Latent Space,\" Nguyen et al., 2021"}, "questions": {"value": "How sensitive is this method to different hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2I3AxRMKzb", "forum": "CFuNu8dK4s", "replyto": "CFuNu8dK4s", "signatures": ["ICLR.cc/2026/Conference/Submission12434/Reviewer_8PBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12434/Reviewer_8PBm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849868661, "cdate": 1761849868661, "tmdate": 1762923321959, "mdate": 1762923321959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a two-stage approach for behaviour cloning via video generation. In the first stage, a video diffusion model generates aa video of a rollout of a policy conditioned on the image of the scene and a task description. In the second stage an inverse dynamics model is used to control the robot given the video. The video model is pre-trained on a large dataset of robot demonstrations before fine-tuning on a few demonstrations from a target domain. Experiments demonstrate that this approach demonstrates strong results compared to end2end video+action generation models like VPP in a very low data regime in the real world. In simulation it performs on par with Pi0."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is relatively well written and easy to follow.\n\nThe proposed approach is sound. The masked inverse dynamics model has some non-trivial novelty.\n\nStrong results are reported in a real-world evaluation in a very low data regime.\n\nA minimal ablation study is reported."}, "weaknesses": {"value": "The positioning of the paper is inaccurate in several ways:\n\n1. The abstracts opens with this statement: “pixel-to-action VLA pipelines typically degenerate under background and view-point shifts”, which is simply incorrect for the latest pixel-to-action pipelines (as demonstrated later in the paper where the prosed approach performs worse than pixel-to-action Pi0 out of domain on the RoboTwin benchmark). \n\n2. While the authors do discuss existing works on video generation + robot control in related work, the introduction is written in such a way as if they are the first to propose this idea. \n\n3. In the related work section the authors mischaracterize the multi-view, 2-stage Dreamitate model as being end2end. \n\n4. Introduction claims that “Empirically, Vidar achieves state-of-the-art performance on the RoboTwin”, which is not true, it underperforms compared to Pi0 in an out-of-distribution evaluation.\n\nOverall, both the novelty and the contributions are over-claimed. The only (minimal) novelty I can see is in the design of the masked inverse dynamics model and the benefits of the proposed approach are not convincingly demonstrated. Real world evaluation is not reproducible and on RoboTwin the proposed approach performs on par with prior work (slightly better in-domain, slightly worse out-of-domain, which is supposed to be the key strength of the proposed approach). \n\nFor some reason the authors only use 40% of the demonstrations in RoboTwin. Results with 100% of the demonstrations need to be reported for reference. \n\nIt would be even better to report results on the more widely used RoboCasa or LIBERO benchmarks instead, using the standard protocol for these benchmarks. RoboCasa specifically targets out-of-domain generalization in a low data regime (50 demonstrations per task). \n\nIt's unclear why different video generation models are used for real and sim experiments. Results should be reported under a uniform model (or with both models in both setting for completeness).\n\nThe proposed approach is primarily tested on short-horizon pick-and-place and simple bimanual grasping tasks, while baselines such as VPP or UniPi were originally demonstrated on more complex or diverse scenarios."}, "questions": {"value": "Please provide a major revision of the positioning of the paper (a rewrite of the abstract and introduction) to address the positioning issues raised above.\n\nPlease provide more convincing evidence of the benefits of the propped approach compared to prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rT45LFMouk", "forum": "CFuNu8dK4s", "replyto": "CFuNu8dK4s", "signatures": ["ICLR.cc/2026/Conference/Submission12434/Reviewer_pjgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12434/Reviewer_pjgR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849905226, "cdate": 1761849905226, "tmdate": 1762923321664, "mdate": 1762923321664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a video-based pretraining framework for learning a generalizable action prior for manipulation tasks. The proposed framework consists of an embodied video diffusion model trained on multi-view trajectories from three robot platforms and a masked inverse dynamics model (MIDM) to learn action-relevant features from images. Experiments on the RoboTwin benchmark show the effectiveness of the proposed approach over existing baselines like VPP, UniPi. Real-robot results show rapid adaptation with only 20 minutes of human demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Pretraining on 750K multi-view trajectories from three robotic platforms is useful to learn a generalizable prior for manipulation tasks.\n- Using masks as an intermediate representation for predicting actions encourages the model to focus on embodiment-relevant features, as evident in Fig.3.\n- Experiments on the RoboTwin benchmark (Tab.1) show benefits over existing baselines like VPP and UniPi on both seen and unseen settings."}, "weaknesses": {"value": "- The text has several mentions about pretraining/learning priors from internet videos (L017-018, L060-061, L062-063, L140-141). However, Fig.1 and the training details in Sec.3.1.2 do not mention any details about pretraining on internet videos. L062-063 explicitly states: \"we propose a three-stage training pipeline, where Internet-scale videos are used for general pretraining\". However, pretraining details on internet videos are clear from the paper.\n- How does the unified observation space (L172) mitigate morphology gaps? The text describes aggregation of multi-view images and instructions related to robotic platforms, camera & task. However, this is done via concatenation and resizing/reshaping operations. It is not clear how this mitigates the morphology gaps and preserves the kinematic context.\n- The text mentions about generalization to unseen robotic platform/embodiments (L080-081, L155). However, this argument is not validated in the experiments (details are not provided about this result).\n- In experiments (Sec.3.2), there is mention about surpassing strong baselines like Pi0 (L293-294) with only 40% data usage relative to the leaderboard. However, I did not find any details about this result in Appendix C or the main paper.\n- L278-280 states that UniPi is not pretrained on the large robotics dataset used for the proposed approach. This makes a direct comparison difficult. Is this also the case for VPP? It is important to train all the baselines in the same setting so that the benefits are clear.\n- As noted in the related work section (L432-435), several VLA models train on millions of demonstrations across diverse embodiments. How is the pretrained strategy proposed in this work different from the pretraining for these VLA models?\n- At test-time, multiple video trajectories are generated from the diffusion model and ranked using GPT-4o to select the highest scoring trajectory. How does this help with physically plausible trajectories? Also, what does the \"scaling\" in test-time scaling refer to? The text simply describes generating multiple candidates and selecting the best one using a scoring function."}, "questions": {"value": "There are several concerns (more details in the weaknesses above):\n- Inconsistencies in the experiments: claims about surpassing baselines like Pi0 & generalization to new embodiments are not validated, baselines are not directly comparable\n-  Contributions are not clear: lack of details about pretraining on internet videos, mitigating morphology gaps, physically plausible trajectories\n- Differences with pretraining strategies used in existing VLAs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k7N4h0oeqj", "forum": "CFuNu8dK4s", "replyto": "CFuNu8dK4s", "signatures": ["ICLR.cc/2026/Conference/Submission12434/Reviewer_Pr6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12434/Reviewer_Pr6T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886010271, "cdate": 1761886010271, "tmdate": 1762923321386, "mdate": 1762923321386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Vidar, a generalist robotic manipulation framework that addresses data scarcity by decoupling the policy into a pre-trained Video Diffusion Mode for an embodiment-agnostic video prior based on the proposed unified observation space, and a lightweight Masked Inverse Dynamics Model (MIDM) adapter for action decoding. Vidar achieves state-of-the-art low-shot adaptation, successfully deploying on a real robot (Aloha) with only 20 minutes of target-domain demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors raise an important problem of data scarcity and cross-embodiment adaptation in generalist manipulation, and propose a highly effective prior-driven, low-shot adaptation paradigm to tackle it.\n\n- Moreover, the authors introduce the Masked Inverse Dynamics Model (MIDM), a lightweight and novel architectural component that implicitly learns spatial masks to focus on action-relevant regions, making the action decoding highly robust to visual distractors.\n\n- The proposed system demonstrates exceptional data efficiency, achieving competitive real-world performance with only ~20 minutes of target-platform demonstrations, which is a crucial step towards scalable robotics deployment.\n\n- The authors provide comprehensive experiments in the real world (Aloha robot) that verify the proposed idea, showing robust generalization to unseen tasks and challenging backgrounds."}, "weaknesses": {"value": "- The open-loop video generation process is currently computationally expensive and slow, requiring approximately 25 seconds on high-end GPUs, which is a significant barrier for real-time deployment.\n\n- The crucial performance improvement from Test-Time Scaling (TTS) relies on an external, large Vision-Language Model like GPT-4o for \"physics-aware reranking.\" This dependency introduces an opaque, expensive, and potentially brittle component into the core control system.\n\n- Confusion in Presentation/Conciseness: The presentation of this work can be more concise in certain sections, which would help improve the flow and focus for the reader.\n\n- Limited Target Embodiment Validation: While the pre-training dataset is vast, the critical low-shot adaptation is demonstrated only on a single target platform (the Aloha robot), which limits the verification of true \"generalist\" cross-embodiment transfer."}, "questions": {"value": "- Please elaborate on the implementation, cost, and latency of the GPT-4o \"physics-aware reranking.\" Can a more dedicated, robot-specific model be developed to replace this external dependency?\n\n- What is the feasibility and expected performance of distilling the video diffusion model to achieve latency suitable for real-time robot control?\n\n- Ablation on View Change: Since the target robot's camera view was adjusted, how much performance is lost if the new robot used one of the original pre-training views (where arms are partially occluded)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JmsRK8X2XU", "forum": "CFuNu8dK4s", "replyto": "CFuNu8dK4s", "signatures": ["ICLR.cc/2026/Conference/Submission12434/Reviewer_fXnT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12434/Reviewer_fXnT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101758554, "cdate": 1762101758554, "tmdate": 1762923321078, "mdate": 1762923321078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}