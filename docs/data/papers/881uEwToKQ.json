{"id": "881uEwToKQ", "number": 18054, "cdate": 1758283290012, "mdate": 1763709566994, "content": {"title": "MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE", "abstract": "Mixture-of-Experts (MoE) enables efficient scaling of large language models by activating only a subset of experts per input token.\nHowever, deploying MoE-based models incurs significant memory overhead due to the need to retain all experts in memory. \nWhile structured pruning is promising to reduce memory costs, existing methods often show suboptimal performance and unstable degradation in three dimensions: model architectures, calibration data sources, and calibration sample sizes.\nThis paper proposes \\textbf{M}ixture-\\textbf{o}f-\\textbf{N}ovices-and-\\textbf{E}xperts (\\textbf{MoNE}), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. \nMoNE evaluates expert redundancy based on two metrics: access frequency and output variance. \nExperts exhibiting low usage and stable outputs are pruned and replaced with lightweight novices—unbiased estimations of their original outputs—minimizing performance degradation. \nExtensive experiments demonstrate that MoNE consistently outperforms baseline methods with minimal accuracy degradation across the three dimensions, confirming its effectiveness and robustness. \nNotably, it outperforms baselines by up to 2.72 for the average zero shot accuracy across nine downstream tasks under 25\\% pruning ratio, with only 0.14 performance drop for Qwen2-57B-A14B. \nThe code is available at https://anonymous.4open.science/r/AnonymizedMoNE.", "tldr": "", "keywords": ["Model Compression", "Mixture-of-Experts", "Structured Pruning", "Expert Pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3c47592f7501300d34116aef7e0ee43b121b328.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MoNE, a pruning method for Mixture-of-Experts models. It identifies experts with low routing frequency and low output variance, and replaces them with constant vectors called novices. This reduces memory cost while keeping model accuracy stable. Experiments across multiple MoE architectures show that MoNE performs better than existing pruning baselines and remains stable across different calibration settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of replacing experts with constant novice vectors is simple, easy to implement, and maintains the computational benefits associated with pruning.\n\nThe redundancy score combining frequency and output variance is well motivated and avoids relying solely on routing frequency, which previous methods often do.\n\nThe experimental evaluation is broad, covering multiple MoE architectures, different pruning ratios, and variations in calibration data, showing consistent robustness."}, "weaknesses": {"value": "Replacing experts with constant vectors may reduce the model's expressive power, and test cases, due to their limitations, may not be able to cover the negative impacts of the evaluation.\n\nPruning strategies depend on the distribution of the calibration dataset. Different application scenarios may have different dependencies on experts, and differences between the distribution of the calibration dataset and the distribution of the real-world scenario may lead to the erroneous removal of important experts."}, "questions": {"value": "No further questions, see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LIc6nuZvI5", "forum": "881uEwToKQ", "replyto": "881uEwToKQ", "signatures": ["ICLR.cc/2026/Conference/Submission18054/Reviewer_tzmx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18054/Reviewer_tzmx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881490239, "cdate": 1761881490239, "tmdate": 1762927842650, "mdate": 1762927842650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research focus of this paper is expert pruning in Mixture-of-Experts (MoE) models. To address this issue, the paper proposes Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. Experiments demonstrate that MoNE consistently outperforms baseline methods across three dimensions—model architectures, calibration data sources, and calibration sample sizes—with minimal accuracy loss, validating its effectiveness and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper propose a novel expert pruning method named MoNE which replaces redundant experts with lightweight novices to compress MoE models with minimal performance loss\n2. This paper uses expert access frequency and output variance to measure redundancy, and unbiased output estimation to minimize post-pruning discrepancy, yielding effective and robust pruning."}, "weaknesses": {"value": "1. The combinatorial forms of frequency and variance adjacency matrices require ablation, such as weighted summation.\n2. Replacing experts with constant vectors may reduce expressiveness; could learnable vectors or biases be used instead?\n3. Could comparative experiments on pruning strategies (without finetuning) be provided to demonstrate the superiority of the proposed frequency- and variance-based pruning strategy?"}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q6FNyAaF4e", "forum": "881uEwToKQ", "replyto": "881uEwToKQ", "signatures": ["ICLR.cc/2026/Conference/Submission18054/Reviewer_cM6H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18054/Reviewer_cM6H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893264404, "cdate": 1761893264404, "tmdate": 1762927842040, "mdate": 1762927842040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MoNE proposes to prune MoE models by replacing selected experts with constant novices (per-expert mean outputs) instead of simply deleting or merging them. Redundancy is estimated by two metrics computed on a small calibration set—access frequency and output variance—and the novice for a pruned expert is its unbiased mean output. The method is evaluated across several MoE architectures (OLMoE, Moonlight, DeepSeek-V2-Lite, Qwen2-57B-A14B, Qwen3-30B-A3B), calibration sources/sizes, and pruning ratios, with ablations on the metrics and novice replacement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Simple, compute-friendly pruning primitive (constant novices) that retains router behavior and keeps overhead close to removal. \n\n* Consistent gains/robustness across models and calibration setups; headline numbers are competitive."}, "weaknesses": {"value": "* The ablation in Figure 4 is intersting, seems like the variance metric can bring improvement without the novice. It would be helpful to include more comprehensive ablation, i.e. more combination (e.g. only frequency and only variance) to show the gain from each part.\n\n\n* The novice is the unbiased mean output of a pruned expert (a constant vector), similar to FLAP’s use of averaged activations for compensation but at a different granularity. The paper should more explicitly discuss the relation with FALP and isolate the contribution.\n\n \n* The redundancy score is the product of variance and frequency. This hard-coded fusion may be scale-sensitive; it would be helpful to include normalized scores, log-sum, or a learned weight λ and report stability.\n\n\n* Some related works worth mentioning [1,2]\n\n[1] MOE-PRUNER: PRUNING MIXTURE-OF-EXPERTS LARGE LANGUAGE MODEL USING THE HINTS FROM ITS ROUTER\n[2] SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation"}, "questions": {"value": "* After pruning, do tokens get routed more often to the remaining real experts or to novices? Any trends per layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eSxblhrnfU", "forum": "881uEwToKQ", "replyto": "881uEwToKQ", "signatures": ["ICLR.cc/2026/Conference/Submission18054/Reviewer_14Ca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18054/Reviewer_14Ca"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972766513, "cdate": 1761972766513, "tmdate": 1762927841648, "mdate": 1762927841648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoNE, a structured pruning approach that replaces redundant experts with lightweight \"novices\" - essentially constant vectors representing averaged expert outputs. The key idea is identifying redundant experts using two metrics: access frequency (how often an expert is selected) and output variance (how stable an expert's outputs are). Experts with low frequency and low variance get replaced by their mean output. The authors test this on five MoE models (OLMoE, Moonlight, DeepSeek-V2-Lite, Qwen2-57B-A14B, Qwen3-30B-A3B) at 25% and 50% pruning ratios, showing better performance than existing methods like MC-SMoE, RS, Angular, and FLAP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea is intuitive, simple, and training-free. The fused metric (frequency + variance) is well-justified, and the \"novice\" replacement (the expert's mean output) is an effective closed-form solution to minimize output discrepancy.\n\n2. The experimental validation is a major strength. Testing on five different MoE architectures with varying sizes (7B to 57B parameters) demonstrates the method works across scales. The robustness evaluation across model architectures, calibration data sources (Zyda2 vs C4), and sample sizes (100, 500, 1000) is thorough."}, "weaknesses": {"value": "1. There is a lack of specialized tasks (e.g., coding, math) in evaluation. It's unclear if the redundancy metric, calibrated on general text, might inadvertently prune experts that are critical for these specialized capabilities.\n\n2. The paper doesn't explain or ablate the benefit of computing a dynamic, per-token gate for a static, constant \"novice\" vector. This appears computationally redundant."}, "questions": {"value": "1. How does MoNE perform on specialized benchmarks like Math or GSM8K? The current evaluation on general tasks may not be sufficient to prove that specialized experts are not being harmed.\n\n2. What is the justification for per-token routing to a static novice vector? Would a simpler approach, like adding the novice as a scaled, static bias, perform comparably while saving routing computation?\n\n3. Are novices trainable during continued pretraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "80QlE9PpyN", "forum": "881uEwToKQ", "replyto": "881uEwToKQ", "signatures": ["ICLR.cc/2026/Conference/Submission18054/Reviewer_qHPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18054/Reviewer_qHPd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144845988, "cdate": 1762144845988, "tmdate": 1762927841140, "mdate": 1762927841140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}