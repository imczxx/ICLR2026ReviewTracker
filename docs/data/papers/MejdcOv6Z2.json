{"id": "MejdcOv6Z2", "number": 25312, "cdate": 1758366598125, "mdate": 1758846997642, "content": {"title": "Efficient Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models", "abstract": "While Mixture-of-Experts (MoE) Large Language Models (LLMs) achieve higher accuracy with fewer active parameters, their pre-training remains challenging due to the enormous parameter sizes and low training efficiency caused by imbalanced expert routing. Unlike previous expert pruning methods that focus on the post-training phase, this paper proposes an efficient Expert Pruning Algorithm (EPA) for the pre-training of MoE LLMs. This algorithm enhances training efficiency while preserving model accuracy by pruning underutilized experts and rearranging experts within expert parallel groups based on token distribution. Extensive experimental results demonstrate that EPA can significantly reduce model size and improve training efficiency while maintaining nearly unchanged accuracy. Specifically, a 1010B parameter MoE LLM trained from scratch using EPA exhibits substantial improvements in training efficiency and delivers excellent performance across tasks in various domains. The code and the 1010B model will be made publicly available.", "tldr": "", "keywords": ["Expert Pruning", "MoE", "LLM", "Expert Loading balance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b6c3a9ca13190dbad993806797908ef10e7ee692.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}