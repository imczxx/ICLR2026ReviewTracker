{"id": "0oHaazjMUX", "number": 9649, "cdate": 1758132569495, "mdate": 1759897706685, "content": {"title": "LeSTD: LLM Compression via Learning-based Sparse Tensor Decomposition", "abstract": "Large Language Models (LLMs) achieve remarkable success, but their massive parameter counts present significant deployment challenges. Post-training tensor decomposition offers a promising, data-free compression strategy by exploiting structural redundancies within the model weights. However, existing tensor methods face a critical limitation: the dense core tensor bottleneck. While these methods find a shared low-rank basis, the resulting dense core tensor grows polynomially with the chosen ranks, becoming a new storage bottleneck and capping the maximum achievable compression. To overcome this fundamental barrier, we introduce LeSTD (\\textbf{Le}arning-based \\textbf{S}parse \\textbf{T}ensor \\textbf{D}ecomposition), a novel two-stage framework for the high-ratio compression of Multi-Head Attention (MHA) blocks. LeSTD first employs an iterative algorithm to identify a high-quality, and shared orthogonal basis that jointly represents all attention heads. Subsequently, it introduces a principled, importance-based pruning algorithm that learns an ultra-sparse core tensor by systematically removing the least salient elements and refitting the remaining ones to preserve model fidelity. By decoupling basis optimization from core sparsification, LeSTD breaks the compression ceiling imposed by the dense core, enabling significantly higher compression ratios than prior methods.", "tldr": "", "keywords": ["LLM Compression", "Post-training Compression", "Tucker Decomposition", "Sparsity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d915e59ba6cf683e5d9c2307560430b577dda5a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LeSTD (Learning-based Sparse Tensor Decomposition), a data-free, post-training compression framework for large language models. It addresses the dense core bottleneck in tensor decomposition methods by learning a shared basis across attention heads and then applying a theoretically grounded pruning mechanism to create an ultra-sparse core tensor. This yields higher compression ratios without accuracy loss. LeSTD performs inference directly in the compressed domain, providing throughput gains without custom kernels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. If I understand correctly, prior methods such as SVD-LLM and ASVD require calibration data, whereas LeSTD operates entirely data-free—yet still outperforms them. This is quite impressive.  \n2. The idea of tensorizing all weights into a unified 4D structure and applying Tucker decomposition** is both intuitive and novel, offering a principled way to exploit inter-layer correlations that matrix-based methods overlook."}, "weaknesses": {"value": "Check in questions part."}, "questions": {"value": "1. The authors should clarify the distinction between LeSTD and TensorLLM. If my understanding is correct, Section 3.1 is identical with TensorLLM, while Section 3.2 is the main difference. It would be helpful to make this relationship explicit.\n\n2. As I understand, LeSTD does not require calibration data, which is a key advantage. Would it be possible to incorporate activation information into this framework to further improve performance? I suspect this might be challenging under Tucker decomposition, but a discussion on its feasibility would be valuable.\n\n3. The paper could include comparisons with more advanced SVD-based pruning methods, such as Basis Sharing [1] and Pivoting Factorization [2]. The concept of Basis Sharing appears somewhat related, since Tucker decomposition also captures inter-weight similarity. \n\n4. It would strengthen the presentation to include an inference algorithm, similar to Algorithm 1 for pruning. Section 3.3 is currently somewhat difficult to follow, and a concise pseudocode description would improve clarity.\n\n5. Will the code be released? If I understand correctly, the original linear layer requires one matrix multiplication, low-rank layers require two, and the proposed Tucker-based structure requires four. This may introduce additional I/O overhead. The authors claim that the method achieves speedup without custom kernels—i.e., purely using PyTorch—which is an impressive claim but also raises concerns. Providing the implementation during the rebuttal period would greatly improve credibility and reproducibility.\n\n[1] Wang, Jingcun, et al. \"Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression.\" The Thirteenth International Conference on Learning Representations.  \n[2] Zhao, Jialin, Yingtao Zhang, and Carlo Vittorio Cannistraci. \"Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models.\" Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8YH2cKpFd", "forum": "0oHaazjMUX", "replyto": "0oHaazjMUX", "signatures": ["ICLR.cc/2026/Conference/Submission9649/Reviewer_W25T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9649/Reviewer_W25T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985401131, "cdate": 1761985401131, "tmdate": 1762921177159, "mdate": 1762921177159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage large language model compression framework based on Tucker decomposition. The first stage performs subspace decomposition to obtain low-rank latent representations, while the second stage compresses the core tensor via a closed-form sparse pruning method. The paper is well-organized, with sound theoretical analysis, detailed mathematical derivations, and extensive experimental validation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a concise and elegant closed-form sparsification method in the decomposed latent space.\n2.\tThe mathematical derivations are thorough and rigorous, providing solid theoretical support for amplitude pruning in the Tucker latent space.\n3.\tThe motivation is clearly articulated, and the background is well presented."}, "weaknesses": {"value": "1.\tThere are some formatting issues in the manuscript (e.g., line 100 references Figure 4 located at line 227).\n2.\tThe figures and their explanations are somewhat unclear. For example, in Figure 3, the components are scattered without clear annotation or explanation of what each parameter represents.\n3.\tThe introduction of the core tensor compression method—the paper’s key innovation—is somewhat disorganized. Although the mathematical derivations are complete, their presentation could be improved with more structured figures and explanations."}, "questions": {"value": "In the throughput analysis, the paper claims inference acceleration using standard PyTorch functions on sparse cores. However, it is uncertain whether PyTorch natively accelerates unstructured sparsity. Have the authors considered other possible factors that might contribute to the observed speedup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61SmGhfCrk", "forum": "0oHaazjMUX", "replyto": "0oHaazjMUX", "signatures": ["ICLR.cc/2026/Conference/Submission9649/Reviewer_ee46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9649/Reviewer_ee46"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992419198, "cdate": 1761992419198, "tmdate": 1762921176743, "mdate": 1762921176743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this submission a novel post-training compression framework for LLM called LeSTD is proposed. The approach is two-step: in the first step Tucker decomposition is applied to learn shared orthonormal factors across attention heads for each layer of the LLM. During the second step the ultra-sparse core tensor is created using importance-based iterative pruning. The importance score is derived from a reconstruction error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The two-step approach is well-motivated and intuitive: first we learn a basis and then we sparsify within that basis\n2.  Theoretical justification for magnitude-based pruning is solid.\n3.  Extensive experiments with different models and different datasets.\n4.  LeSTD shows improvement over the competing methods across different compression ratios.\n5.  Paper is well-written, the presentation of the method is good with neat illustration;\n6.  The limitations are acknowledged and discussed (unstructured sparsity and MHA-only compression)"}, "weaknesses": {"value": "0.  The overall novelty of the submission is limited: Stage I is a known combination of the Tucker decomposition and HOOI, Stage II reduces to magnitude pruning (which is known optimal for orthonormal bases). The contribution is primarily in combining these for LLMs compression rather than theoretical/methodological novelty\n \n1.  Other sparse tensor methods are not considered (CP decomposition, Tensor-Train Decomposition, structured/block-sparse Tucker variants, etc)\n2.  Experiments do not include statistical significance (which is crucial in such works): no error bars, confidence intervals, or multiple runs reported;\n \n3.  Limited ablation studies: only 6 rank configurations tested (Table 2), no ablation on pruning rate α, refitting frequency, or HOOI convergence criteria\n \n4.  For the throughput, LeSTD sometimes loses to SVD-LLM (e.g., for OPT-30B on MathQA at 0.8 compression rate), but analysis of when/why is insufficient"}, "questions": {"value": "1.  Can you provide error bars across multiple experimental runs to assess statistical significance of the improvements over other methods?\n2.  How sensitive is performance to the pruning rate α? Only α=0.1 is mentioned; was this tuned?\n3.  What is the actual sparse indexing overhead at different sparsity levels? How does this affect real compression ratios?\n4.  Why does LeSTD sometimes lose to SVD-LLM on throughput (e.g., OPT-30B)? Can you characterize when your method wins vs. loses?\n5.  Can you provide results extending LeSTD to FFN layers, even if preliminary?\n6.  How important is the refitting step (Eq. 6)? How does skipping this step affect the performance?\n7.  What are the wall-clock compression times for Stage I and Stage II compared to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vmZFFG2mdm", "forum": "0oHaazjMUX", "replyto": "0oHaazjMUX", "signatures": ["ICLR.cc/2026/Conference/Submission9649/Reviewer_y5qd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9649/Reviewer_y5qd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995371888, "cdate": 1761995371888, "tmdate": 1762921176402, "mdate": 1762921176402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LeSTD, a two-stage, post-training compression framework for LLMs. Stage I performs a shared-subspace Tucker decomposition of the tensorized multi-head attention (MHA) weights: all heads in a layer share three orthonormal factor matrices, while each head retains its own small core, estimated via HOOI to minimize reconstruction error. Stage II sparsifies the per-head Tucker cores using an importance score equal to the coefficient magnitude, followed by a closed-form refit of the remaining coefficients. The method supports inference directly in the compressed domain where it reuses the shared projection and contracts the (sparse) per-head cores—without reconstructing dense weights. Empirical results demonstrate the effectiveness and efficiency of the proposed method across various tasks and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written, with the methodology and experimental setup presented with detail in a organized way.\n* The pruning step is well justified.\n* Exploring tensor decomposition for post-training compression is an interesting and relatively underexplored direction in the domain."}, "weaknesses": {"value": "* Figure 1 is not strong enough to justify the paper's motivation. The “shared subspace across heads within the same layer” claim is not well supported given the low explained energy, and the intra-layer explained energy is only marginally higher than the inter-layer case.\n* As the paper laid out in the limitation section, the current method does not handle FFN layers which constitute a large fraction of LLM parameters. Additionally, because the pruning is unstructured, actual storage and speed benefits would depend on the chosen sparse format and kernel support, so the practical gains may be smaller than the reported parameter reduction."}, "questions": {"value": "* Can you report wall-clock for Stage I optimization?\n\n* Is it possible to consider structured pruning? Would this affect the performance greatly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N4Gj4WZh8x", "forum": "0oHaazjMUX", "replyto": "0oHaazjMUX", "signatures": ["ICLR.cc/2026/Conference/Submission9649/Reviewer_RLPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9649/Reviewer_RLPz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146963013, "cdate": 1762146963013, "tmdate": 1762921176052, "mdate": 1762921176052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}