{"id": "VmrJ8C3gxu", "number": 859, "cdate": 1756821158526, "mdate": 1759898238564, "content": {"title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering", "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision–language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image–text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.", "tldr": "", "keywords": ["MLLM Unlearning", "Test-time Unlearning", "Activation Steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68f1fda3731dd774f8d5b443e9618544a3e8fb5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "MLLMEraser introduces the first test-time unlearning method for multimodal large language models (MLLMs), eliminating the need for parameter updates. Instead of retraining, it uses activation steering to dynamically erase designated knowledge during inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Efficient Algorithm:** The work redefines unlearning as a reversible inference-time process rather than a retraining problem, marking an important conceptual shift. By steering activations instead of modifying weights, MLLMEraser sidesteps gradient conflicts between forget and retain sets that typically degrade model utility. \n- **Reversibility and plug-and-play design:** MLLMEraser does not modify parameters, it can be detached or re-applied instantly. This reversible property makes it suitable for real-world applications such as dynamic privacy control or content moderation, where models must flexibly enforce or lift restrictions. Few prior unlearning methods offer such operational simplicity."}, "weaknesses": {"value": "- The direction of determining function f(h) is modeled as a linear transformation, which assumes that forget and retain samples are linearly separable in activation space, but knowledge entanglement in MLLMs can be highly non-linear, especially across visual and textual dimensions. \n- MLLMU-Bench seems to have three forgetting setups (5%, 10%, 15%), and it seems like 15% case is missing across the entire paper. \n- MLLMEraser assumes that the underlying MLLM can already exhibit consistent refusal behavior, which is not always the case, especially for weaker or open-source base models without robust safety tuning. If the model cannot reliably produce refusal responses, the contrastive setup for constructing the erasure direction becomes ill-posed, potentially leading to ineffective or unstable unlearning.\n- The adversarial optimization used to simulate harmful visual recall (via PGD updates) introduces potential artifacts. While it successfully amplifies harmful responses, these perturbations may push the images outside the distribution of realistic inputs, leading to erasure directions that encode spurious noise rather than semantic differences. Hence, the model's sensitivity to the perturbation budget and step size should be discussed in the paper."}, "questions": {"value": "- The method applies activation injection at intermediate layers, but different layers represent different abstraction levels. Have you tested which layers yield the best trade-off between forgetting strength and utility retention?\n- Can the null-space projection fully eliminate retain interference when the two sets overlap semantically?\n- Just out of my curiosity, would a non-linear steering function further enhance selectivity?\n\nI am willing to adjust my score if the authors provide convincing explanations to my above questions and weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YugH146tbd", "forum": "VmrJ8C3gxu", "replyto": "VmrJ8C3gxu", "signatures": ["ICLR.cc/2026/Conference/Submission859/Reviewer_ssTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission859/Reviewer_ssTS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760491140254, "cdate": 1760491140254, "tmdate": 1762915629412, "mdate": 1762915629412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of machine unlearning for Multimodal Large Language Models. The authors propose MLLMEraser, a novel test-time unlearning approach that constructs erasure directions by combining textual and visual signals from forget samples. Unlike training-based methods that require costly parameter updates, MLLMEraser applies steered representations during inference, making it reversible and computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The test-time approach offers a practical alternative to expensive training-based methods, with the added benefit of reversibility。\n\n2. The experimental results show that the proposed method outperforms existing methods.\n\n3. This paper is well written and easy to understand."}, "weaknesses": {"value": "1. The core methodological idea feels like a modest variation on existing approaches rather than a fundamentally new contribution. While the adjustments may be useful in some contexts, I am not convinced they carry enough originality for a top-tier venue.\n\n2. Some competitive methods [1] that are well-known in the literature are missing from the comparison tables. Without such baselines, it’s difficult to assess whether the proposed approach actually offers a practical improvement. \n\n3. Some results are strange. Such as in Table 1, for Generation: Rouge Score the proposed method outperforms vanilla. Is it normal?\n\n\n\n## References ##\n\n[1] Liu, Zheyuan, et al. \"Modality-aware neuron pruning for unlearning in multimodal large language models.\" arXiv preprint arXiv:2502.15910 (2025)."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XfyTEODTyP", "forum": "VmrJ8C3gxu", "replyto": "VmrJ8C3gxu", "signatures": ["ICLR.cc/2026/Conference/Submission859/Reviewer_cAt7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission859/Reviewer_cAt7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760935667523, "cdate": 1760935667523, "tmdate": 1762915629253, "mdate": 1762915629253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MLLMEraser, an input-aware, training-free framework for test-time unlearning. The approach leverages activation steering to enable dynamic knowledge erasure without parameter updates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents MLLMEraser, an input-aware test-time unlearning framework for multimodal large language models.\n2. The method aims to enhance model trustworthiness by enabling efficient and reversible removal of designated information.\n3. The presentation and writing is well."}, "weaknesses": {"value": "1. What happens if the base model isn't very well-aligned? If the model doesn't reliably refuse to answer harmful prompts in the first place, it seems like your method would fail because you can't create the 'forgetting direction' it needs. Does this approach only work for models that are already highly aligned, or can it be applied to models with different safety levels?\n2. The method for making the model 'forget' seems to depend on getting it to refuse to answer, which you trigger with harmful prompts. But what about other forgetting tasks? How would this work for removing private information, correcting a factual error, or getting rid of copyrighted content? In those cases, the model doesn't refuse."}, "questions": {"value": "Please see the weakness, if you address the weakness, I will to improve my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lo6WSRr9ng", "forum": "VmrJ8C3gxu", "replyto": "VmrJ8C3gxu", "signatures": ["ICLR.cc/2026/Conference/Submission859/Reviewer_UrZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission859/Reviewer_UrZK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642397120, "cdate": 1761642397120, "tmdate": 1762915629124, "mdate": 1762915629124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MLLMEraser, a test-time unlearning framework for MLLMs that leverages activation steering to erase targeted knowledge without parameter updates. Specifically, it introduces a multimodal erasure direction constructed from contrastive image-text pairs and an input-aware steering mechanism to selectively apply interventions. Experiments on LLaVA-1.5 and Qwen-2.5-VL show superior forgetting performance and lower compute cost relative to prior training-based unlearning methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tThe proposed MLLMEraser introduces a novel activation steering vector that contrasts adversarial image-text pairs with their refusal-style counterparts, thereby overcoming the limitations of common text-only steering in MLLMs.\n\n(2)\tBy acting at inference rather than requiring re-training or parameter updates, it offers an efficient unlearning framework compared to traditional training-based unlearning approaches."}, "weaknesses": {"value": "(1) The meanings of $D^+$ and $D^-$ are not inconsistent throughout the main text, which significantly hampers clarity. Specifically, in Equation (3), the text states that '$D^+$  denotes the set of knowledge-recall samples and $D^-$  the corresponding knowledge-erasure samples'. In Equation (7), however, knowledge-recall pairs are assigned to the negative set $D^-$, whereas knowledge-erasure pairs are assigned to the positive set $D^+$. \n\n(2) The steering strength λ and regularization parameter γ are reported as empirical values tailored to LLaVA-1.5-7B and Qwen-2.5-VL-7B models, yet the tuning process is not adequately detailed in the main text. Thus, it remains unclear how to set these hyper-parameters on new datasets or architectures.\n\n(3) While Figures 4 provide qualitative insights into how activation distributions change before and after steering, the paper lacks a deeper quantitative analysis of these changes. Without such details, the robustness of the justification for the null-space projection constraint is not fully convincing.\n\n(4) I am curious about whether steering at different LLM layers or within the vision encoder could affect unlearning efficacy.\n\n(5) In Table 1, boldface data do not always represent the best results. In particular, for the Ret and Cele metrics, the results labeled 'Ours' are generally not better than the Vanilla method."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E2z1HfpbC3", "forum": "VmrJ8C3gxu", "replyto": "VmrJ8C3gxu", "signatures": ["ICLR.cc/2026/Conference/Submission859/Reviewer_vJ3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission859/Reviewer_vJ3Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813643633, "cdate": 1761813643633, "tmdate": 1762915628908, "mdate": 1762915628908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}