{"id": "cEyj6ewRFZ", "number": 15861, "cdate": 1758256261975, "mdate": 1759897277178, "content": {"title": "Nonparametric Teaching for Sequential Property Learners", "abstract": "Determining the properties of sequence-structured data, e.g., the sentiment of a text, fundamentally requires learning the implicit relationship that maps sequences to their corresponding properties. This learning process is often expensive for sequential property learners like Recurrent Neural Networks (RNNs). To tackle this, we introduce a paradigm called **Re**current **N**eural **T**eaching (ReNT), which reinterprets the learning process through a novel nonparametric teaching lens. Specifically, the latter provides a theoretical framework for teaching implicitly defined (i.e., nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of sequence-property pairs, with the ReNT teacher selecting a subset of them to facilitate faster convergence in RNN training. By analytically investigating the effect of sequence order on parameter-based gradient descent during training, and recasting the evolution of RNNs—driven by parameter updates—through functional gradient descent in nonparametric teaching, we reveal *for the first time* that teaching sequential property learners (i.e., RNNs) is consistent with teaching order-aware nonparametric learners. These new findings readily prompt ReNT to improve the learning efficiency of the sequential property learner, achieving substantial cuts in training time for sequence-level (-32.77% to -46.39%) and element-level (-36% to -39.17%) tasks, while still preserving its generalization performance.", "tldr": "", "keywords": ["Nonparametric Teaching", "Sequential Property Learning", "Functional Gradient Descent"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbc989bd6448a36942b9b8d5690e85c9a9ff7c98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ReNT, a nonparametric teaching-based paradigm that accelerates the training of RNNs on sequence-structured tasks. The central idea is to analyze how sequence order shapes parameter-space gradients in RNNs and to recast the resulting evolution in function space. Across sequence-level regression/classification tasks, ReNT reduces wall-clock training time by ~33–46% while maintaining comparable generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper gives a clear parameter-space derivation of order-aware gradients for RNNs (Eq. 10–12), showing how the power of recurrent weights encodes temporal dependencies and that the gradient shape is independent of sequence length (Eq. 12). This supports the claim that order impacts RNN updates in a principled way.\n\n2. Results show consistent wall-clock savings with comparable test MAE/ACC on six benchmarks.\n\n3. The work bridges nonparametric teaching with order-aware sequential learners and formalizes an RNTK→canonical-kernel connection for RNNs. \n\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The study focuses almost exclusively on vanilla RNNs. Modern sequence learners—state-space models (SSMs) and Transformer-based architectures—are not discussed or evaluated, leaving the practical impact and portability of the method to mainstream models unclear.\n\n2. Comparisons are primarily “with vs. without ReNT.” Strong, SOTA data selection/curriculum baselines are missing. The evaluated setups emphasize simple RNNs and relatively small/clean datasets; the scale performance of large and noisy datasets is unknown. The paper provides insufficient architectural detail for the RNNs, which hinders reproducibility and fair comparison.\n\n3. The paper lacks systematic ablations on key hyperparameter choices such as learning rate and epochs. Figure 4 is difficult to read (small fonts/markers); differences among methods are not clearly visible. Moreover, Figure 4 appears to show faster convergence in epochs for the baseline (better performance with fewer epochs). Please clarify whether compute/epoch budgets are matched, and how this affects the interpretation. This raises questions about generalization and stability on larger models/datasets."}, "questions": {"value": "1. Could you report results on at least one SSM and one Transformer variant to demonstrate portability? If compute is tight, small-scale ablations (same data, reduced depth/width) would still be informative.\n\n2. Please provide full RNN model specifications: number of layers, hidden sizes, activation functions, normalization, dropout, initialization, sequence lengths/windows, etc, for reproducibility.\n\n3. Could you include more results of SOTA baselines, large models, and datasets to further investigate the effectiveness and scaling performance of the method?\n\n4. Could you include metrics such as FLOPs and peak Memory usage for a more comprehensive efficiency evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B3Zic19ukA", "forum": "cEyj6ewRFZ", "replyto": "cEyj6ewRFZ", "signatures": ["ICLR.cc/2026/Conference/Submission15861/Reviewer_qhpD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15861/Reviewer_qhpD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766207211, "cdate": 1761766207211, "tmdate": 1762926081497, "mdate": 1762926081497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to improve the training efficiency of RNNs by adaptively selecting the mini-batch at each gradient step."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "The proposed method (Algorithm 1) is very simple, and the approach appears to be a minor extension of Zhang et al. (2023a;b; 2024; 2025) with modest adaptations to sequential data. The substantive contribution remains unclear, and the high computational cost of selecting teaching sequences may significantly limit both efficiency and applicability. The heavy terminology in the main text (\"non-parametric teaching\", \"RNTK\", \"sequential property learner\", etc.) is irrelevant and distracting.\n\nI recommend that the authors rewrite the paper: move Algorithm 1 into the main text, add more discussion on the algorithm itself, and reduce the heavy terminology to improve clarity. The derivations in Sections 4.1 and 4.2 are lengthy with little insight (Proposition 4 seems fairly standard) and could be shortened."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QupgtBVOsH", "forum": "cEyj6ewRFZ", "replyto": "cEyj6ewRFZ", "signatures": ["ICLR.cc/2026/Conference/Submission15861/Reviewer_gUyk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15861/Reviewer_gUyk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768147479, "cdate": 1761768147479, "tmdate": 1762926080884, "mdate": 1762926080884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReNT, a nonparametric training paradigm where the training data is sequenced according to the largest $m$ samples which maximizes $\\\\|f_{\\theta}-f^*\\\\|$. The proposed algorithm led to a reduce in time for training RNNs for about $-40\\\\%$ while keeping the test performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The ReNT algorithm managed to reduce the training time of RNN significantly while maintaining the genralization performance.\n2. The paper writing is good and comprehensive."}, "weaknesses": {"value": "1. The contribution compared with [1] seems limited. Theorem 3 and Proposition 4 is exactly the same as in [1], and the key idea of the ReNT algorithm, which is amplifying the steepest gradients by selecting the largest $m$ data to maximize $\\\\|f_{\\theta}-f^*\\\\|$, does not depend on the specific neural network architecture but rather the loss function of $\\mathcal{L}=\\frac{1}{2}(f_{\\theta} - f ^ *)^2$ . The implementation of ReNT algorithm therfore should not be much different from [1] besides changing the baseline neural network from an MLP to an RNN. The new part is perhaps sec 4.1 that examines the impact of sequence order on parameter-based gradient descent. But this is merely computing the gradient of RNN, which should not be a contribution of its own.\n\n\n\n\n[1] Zhang, C., Luo, S. T. S., Li, J. C. L., Wu, Y. C., & Wong, N. (2024). Nonparametric teaching of implicit neural representations. arXiv preprint arXiv:2405.10531."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pu4bj1Ou0P", "forum": "cEyj6ewRFZ", "replyto": "cEyj6ewRFZ", "signatures": ["ICLR.cc/2026/Conference/Submission15861/Reviewer_Uvqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15861/Reviewer_Uvqm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879247650, "cdate": 1761879247650, "tmdate": 1762926080198, "mdate": 1762926080198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper interprets gradient descent on parameters through a non-parametric lens, highlighting its similarity to functional gradient descent. This perspective goes beyond the standard Recurrent Neural Tangent Kernel (RNTK) analysis by being order-aware, explicitly taking the sequential nature of the data into account. Furthermore, by leveraging this equivalence, the authors propose a novel algorithm for learning RNNs. This algorithm strategically selects a subset of the data to ensure faster convergence. Empirical results demonstrate the algorithm's performance gains."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper goes beyond the  recurrent neural tangent kernel and shows that gradient descent  on parameter descent aligns with the evolution in functional gradient descent with the order-aware canonical kernel. This looks an interesting tool to study the dynamics of parameter-based gradient descent.  \n\n- Using this equivalence the paper develops an efficient learning algorithm of RNNs which is empirically validated which seems convincing."}, "weaknesses": {"value": "The paper appears not to be mathematically rigorous, see the questions below for detailed description."}, "questions": {"value": "i) The algorithm the paper suggests require access to f_*(S), what happens when $f_*$ cannot be computed ? \n\nii)  For the sufficient loss reduction lemma, what is the descent algorithm used here - is it parameter descent or functional descent ? \n\niii) In theorem 3, is the parameter convergence assumed ? In particular in the proof how $\\lim_{t\\to\\infty} \\left[ \\frac{\\partial L(f_{\\theta^t}(S), y}{\\partial f_{\\theta^t}(S)} \\right]  = 0$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "66azDRDQj0", "forum": "cEyj6ewRFZ", "replyto": "cEyj6ewRFZ", "signatures": ["ICLR.cc/2026/Conference/Submission15861/Reviewer_fLJ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15861/Reviewer_fLJ3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134418785, "cdate": 1762134418785, "tmdate": 1762926079596, "mdate": 1762926079596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}