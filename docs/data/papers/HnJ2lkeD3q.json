{"id": "HnJ2lkeD3q", "number": 20212, "cdate": 1758303734694, "mdate": 1759896990311, "content": {"title": "Embryology of a Language Model", "abstract": "Understanding how language models develop their internal computational structure is a central problem in the science of deep learning. While susceptibilities, drawn from statistical physics, offer a promising analytical tool, their full potential for visualizing network organization remains untapped. In this work, we introduce an embryological approach, applying UMAP to the susceptibility matrix to visualize the model's structural development over training. Our visualizations reveal the emergence of a clear \"body plan,\" charting the formation of known features like the induction circuit and discovering previously unknown structures, such as a \"spacing fin\" dedicated to counting space tokens. This work demonstrates that susceptibility analysis can move beyond validation to uncover novel mechanisms, providing a powerful, holistic lens for studying the developmental principles of complex neural networks.", "tldr": "We study the development of a small language model over training using a new interpretability technique called susceptibilities", "keywords": ["Interpretability", "Language Model", "Singular Learning Theory"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee42ec709e23d2a05ba53aceb23ffa1377c51d0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an \"embryological\" lens on the training dynamics of a 3M-parameter, 2-layer attention-only language model.\nThe key idea is to compute **per-token susceptibilities** for each attention head, stack them into vectors (\\eta_w(xy)), and then visualize these vectors with UMAP over the course of training.\nThe authors report (i) a long thin \"rainbow serpent\" manifold whose first two principal axes align with global expression/suppression (PC1) and a dorsal–ventral stratification tied to the induction circuit (PC2), and (ii) a newly described **\"spacing fin\"** associated with sequences of spacing tokens and their counts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality.**\n\n* Using **susceptibility vectors** (rather than activations) to visualize response is an interesting perspective that complements circuit-centric methods. The \"spacing fin\" is a surprising, concrete emergent structure that the authors unraveled with their visualization. \n* The biological metaphor (anterior–posterior / dorsal–ventral axes) is consistant through the manuscript and helps organize observations about stratification by token pattern.\n\n**Quality / Technical soundness.**\n\n* The susceptibility definition and its sign interpretation (expression vs. suppression) are clearly stated, with an explicit covariance-based definition (Def. 2.1) and discussion. \n* Cross-seed visualizations (Appendix F) support claims.\n\n**Clarity.**\nThe paper is easy to follow, generally well written, figures are plentiful and annotated."}, "weaknesses": {"value": "**The probabilistic setup and tractability of the quenched posterior need more transparency.**\n\n* Eq. (2) introduces the posterior ( $ p^{\\beta}\\_{n}(w) \\propto \\exp \\\\{-n \\beta L(w) \\\\} \\phi(w) $ ) with normalizer ( $ Z^{\\beta}\\_{n} $ ). The *practical* tractability of ( $ Z^{\\beta}\\_{n} $ ) and how its intractability propagates (or cancels) in ( $ \\chi $ ) estimates are not discussed. \n\n**Motivation for Def. 2.1 could be surfaced earlier.**\nThe definition of susceptibility (Def. 2.1) appears before an intuitive build-up of why *this* covariance captures \"expression/suppression.\" Consider moving the intuitive paragraphs (\"Negative susceptibility… Positive susceptibility…\") directly before the formal definition illustrating sign and magnitude. \n\n**Head labeling vs. permutation equivariance.**\nSection 3 states (based on prior work) which heads are previous-token/current-token/induction (e.g., 0:1, 0:4, 0:5, 1:6, 1:7), but attention heads are *a priori* permutation-equivariant under reindexing. Please add a sentence clarifying **how** heads are *identified and matched across runs/checkpoints*, and how this resolves the labeling issue across seeds.\n\n**Figures / Typo.**\n\n* Fig. 2’s subplots don’t share a y-axis within pattern groups, making comparisons harder. Please share y-axes across rows where meaningful or include small multiples with identical scales. Also label heads (l:h) more prominently. \n* Minor: page 9 line ~450 \"exhibition / exhibition\" → \"excitation / inhibition.\" (If \"exhibition/inhibition\" is deliberate, please justify the terminology.) \n\n**\"Universal body plan\" is stated strongly given one architecture/scale.**\nThe paper emphasizes universality across seeds of *one* tiny attention-only model with a specific tokenizer. Please temper claims or add more evidence: (i) a second tokenizer, or (ii) a small MLP-augmented transformer at the same scale, or (iii) at minimum, a dataset ablation showing how pattern frequencies (Fig. 3) modulate the observed geometry. Even a compact sensitivity table would help."}, "questions": {"value": "**Suggestions:**\n\n* Section 4.1 uses \"Serpent\" metaphors; we believe the term \"Eel\" (which has fins) could be better suited.\n* On p. 6 you refer to \"PC1/PC2\" without first saying you did PCA; add a forward pointer to Appendix C. \n* On p. 9, line 450, typo \"exhibition / exhibition\" $\\rightarrow$ \"exhibition / inhibition\".\n\nIn addition to the concerns raised in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f4ZA4Chhz6", "forum": "HnJ2lkeD3q", "replyto": "HnJ2lkeD3q", "signatures": ["ICLR.cc/2026/Conference/Submission20212/Reviewer_6eSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20212/Reviewer_6eSy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968214787, "cdate": 1761968214787, "tmdate": 1762933710387, "mdate": 1762933710387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an “embryology” lens for LM training by projecting per-token susceptibility vectors into 2D with UMAP, then tracking how the geometry evolves over training. This is meant to shed light on how language models develop their internal computational structure. The authors find the projection produces a striking rainbow serpent structure whose axes point towards the emergence of an induction circuit."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach is unlike most views i've seen in terms of interpreting LLMs and is creative/novel.\n- The paper presents a fairly holistic view of interpretability in LLMs. Instead of focusing on single circuits, the method reveals global organization and complementary expression/suppression roles across heads.\n- Joint use of UMAP snapshots and per-pattern susceptibility trajectories might point to plausible temporal causal structure emergence."}, "weaknesses": {"value": "- The results presented in the paper are entirely based on  a 3M, 2-layer attention-only model. It’s unclear whether the serpent structure and spacing fin persist or change in mid/large LMs with MLPs and modern tokenizers (e.g., non-whitespace-heavy merges).\n- Although partly addressed, the method still relies on a nonlinear, stochastic embedding with known global-distance distortions; the work would benefit from corroboration via isometry-aware metrics in the original space.\n- The approach may be sensitive to confounding, since the prominence of spacing tokens may be an artifact of the truncated GPT-2 vocab and dataset composition; more direct controls or alternate tokenizers would help.\n- Overall, apart from the PC2 thickening statistic for induction, much of the case rests on visuals. More numerical results in the form of for instance statistical tests (e.g., separability indices, cluster stability, supervised recovery of pattern labels from η) would strengthen claims.\n\nSuggestions for improvements:\n- Replicate results on other architectures (with ablations) on a small-MLP transformer and a ~100–300M LM with modern BPE or unigram LM tokenizers;\n- Provide chain-to-chain variance, R-hat-style diagnostics, and sensitivity to SGLD hyperparameters; test subsampling stability of η-space geometry."}, "questions": {"value": "1. What exactly does susceptibility represent causally? Is χ interpretable as a local sensitivity to intervention on a head, or more akin to covariance with a loss gradient?\n\n2. Why is the “embryology” metaphor meaningful beyond aesthetics? What else does it add in this context that we would not be able to infer otherwise?\n\n3. Do we expect a similar 2D structure if we used t-SNE, Isomap, or diffusion maps? Is there theoretical reason (e.g., low intrinsic dimensionality of susceptibility space) to expect such a compact geometry?\n\n4. How stable are susceptibility estimates? Did you try random seeds, sampling noise, SGLD parameters (β, ε, γ)? Is there a confidence measure per χ that could be visualized (e.g., variance across samples)?\n\n5. Could susceptibility vectors serve as features for causal discovery (e.g., learning directed edges between heads)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rIKCt56KiZ", "forum": "HnJ2lkeD3q", "replyto": "HnJ2lkeD3q", "signatures": ["ICLR.cc/2026/Conference/Submission20212/Reviewer_eDxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20212/Reviewer_eDxW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029687700, "cdate": 1762029687700, "tmdate": 1762933710058, "mdate": 1762933710058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an “embryological” lens on visualization of training small language models: compute a per‑token susceptibility vector (one coordinate per attention head) that measures how changing weights in a component covaries with the loss on a specific continuation, then apply dimension reduction techniques to many such vectors over training. The resulting 2D plots form a characteristic “rainbow serpent” whose long axis (PC1) reflects global expression vs. suppression across heads and whose short axis (PC2) thickens as an induction circuit emerges. The authors also report a previously unremarked structure, a “spacing fin”, consisting of spacing tokens, especially those preceded by long runs of spaces/newlines, which they hypothesize reflects counting of spacing tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I believe the authors have studied an interesting question and created various intriguing visualizations that could be of interest to the community. The paper presentation was good, and the figures are very nicely presented. In particular, \n\n1. The \"embryological\" analogy, framing model training as a developmental process, is conceptually intuitive yet powerful. Applying UMAP to susceptibility vectors, rather than just model activations, is a novel approach. It provides a global visualization of how the entire set of model components (attention heads) collectively organizes to handle different token patterns.\n\n2. The theory was validated (at least in small scales) nicely by the experiments showing the emergence of induction circuits, which were intensively studied in literature, and placing it into their own contexts, offering their interpretation of the underlying mechanisms of the model."}, "weaknesses": {"value": "The major limitations are already straightforwardly discussed in the paper. Here I rephrase the two most important ones in my opinion:\n\n1. The experiments are pretty severely limited by scale, as they were only visualized with a tiny 3M model with two layers of attention-only modules. It is a pretty significant leap from even tiny-scaled language models by today's standards. This is (in my opinion) the most significant weakness of the present paper, and it is not clear what the limitation is for the authors not to report more extensive experiments.\n\n2. Some key claims (e.g. the spacing fin’s separation) depend critically on UMAP. The paper also notes PCA fails to reveal the fin in the first three PCs and suggests it lives in higher PCs. However, as the authors have pointed out, UMAP is a non-linear visualization technique that can distort global geometric structures. This raises the question of the robustness as well as reliability of their findings when migrating to different architectures, as well as making interpreting the results harder.\n\nA minor point is that a lot of biology references are made, which may make it hard for someone without knowledge of biological sciences to capture the analogies. Despite the above, I still have questions for the paper (see below) and will consider raising the score if the authors provide satisfying responses to the weaknesses/questions."}, "questions": {"value": "1.  Following up with the scalability concern, how much computational overhead is there to compute the necessary statistics for the proposed visualization? Is training a larger base model actually the biggest computational burden, or are the tools developed for visualization demanding nontrivial computational resources beyond training the model?\n\n2. The authors discussed the experiment results, \"likely influenced by the tokenizer, and different tokenization strategies could lead to different learned structures.\" Can you provide more empirical evidence or insights into why this might be the case, or better, provide experiments on how tokenization changes the results?\n\n3. While the figures are interesting, I didn't find much discussion on the impact for practitioners who are training a model or understanding a trained model. What qualitative or quantitative features of the proposed framework (e.g., emergence of induction circuits) are expected to transfer when training a (possibly much different) model or inspecting a trained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LrU9Eo9QIx", "forum": "HnJ2lkeD3q", "replyto": "HnJ2lkeD3q", "signatures": ["ICLR.cc/2026/Conference/Submission20212/Reviewer_Esoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20212/Reviewer_Esoa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329904838, "cdate": 1762329904838, "tmdate": 1762933709685, "mdate": 1762933709685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an \"embryological\" approach for studying the development of structure during the training of a small language model. The authors use UMAP on per-token susceptibility vectors to visualize how structural organization emerges through training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**Originality: poor** \nApplication of UMAP to high-dimensional susceptibility vectors for visualization is a straightforward combination of existing methodologies rather than a substantial advance. The framing of “embryology” and “body plan” in neural networks reads more as metaphorical novelty than genuine technical originality.\n\n**Quality: poor**\nThe overall quality of the work is limited by a lack of rigorous experimental validation and an overreliance on qualitative visualization. Key claims about the “universality” and interpretability of observed structures are not robustly substantiated, and methodological choices (model size, tokenizer, architecture) are unjustified and insufficiently explored. Findings appear sensitive to experimental setup and dimensionality reduction parameters. \n\n**Clarity: good**\nThe paper is generally clear and well-organized, with visualizations that are appealing and easy to follow. Explanations of the experimental setup and the visualization process are accessible, and the writing is coherent.\n\n**Significance: poor** \nThe significance is limited, as the claims do not meaningfully advance our theoretical or practical understanding of neural network internals. The scientific insights derived from the visualizations are superficial. The potential for broader impact is unclear given the narrow experimental scope."}, "weaknesses": {"value": "**Unsound/superficial application of UMAP.** \n- The paper relies heavily on UMAP for what amount to no more than qualitative visualizations. The limitations of UMAP for interpretability are identified in the appendix, and others are well-documented in various literatures. As a consequence, \"anatomical\" claims lack quantitative rigor to support the \"serpent\" as a stable, robust feature rather than a visualization artifact. The authors acknowledge that the geometry of the \"serpent\" should be \"nterpreted cautiously.\" Given that the paper's claims rely almost exclusively on this geometry, all results should be interpreted *at least* as cautiously. \n- The paper's claim that UMAP \"faithfully represented\" aspects of the underlying high-dimensional distribution is not justified. There is little to no effort to quantify the claims of reliability or interpretability of the UMAP projections. \n- The authors remark that UMAP parameters were varied and patterns that did not persist were dismissed. However, there is no quantitative stability result to support that the observed structures are not induced by specific hyper-parameter choices. \n\n**Experimental limitations and poor generalization claims.** \n- Critical aspects like the \"spacing fin\" are demonstrated to be contingent on the tokenizer and possibly the dataset (?). But the paper does not explore how changing tokenization, data distribution, or model size / complexity alter these findings. Again, the paper's claims to have identified a persistent structure but makes no effort to rigorously test or quantify this persistence across variations. Claims to \"universality\" of the body plan are called into question. \n- Findings are only supported by UMAP *visualizations* and not quantitative measurements. \n- The \"spacing fin\" is presented as a newly discovered structure, but its mechanistic significance is left at best conjectural."}, "questions": {"value": "- Quantitative support for persistence or robustness of the \"body plan\" and \"spacing fin\"? \n- Any sort of rigorous quantitative evidence that the observed structure are not artifacts of UMAP? \n- Given the dependence on tokenizer, do similar features emerge with alternative tokenization schemes and different data distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xHT0fvlyfd", "forum": "HnJ2lkeD3q", "replyto": "HnJ2lkeD3q", "signatures": ["ICLR.cc/2026/Conference/Submission20212/Reviewer_7Ksj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20212/Reviewer_7Ksj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20212/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964585879, "cdate": 1762964585879, "tmdate": 1762964585879, "mdate": 1762964585879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}