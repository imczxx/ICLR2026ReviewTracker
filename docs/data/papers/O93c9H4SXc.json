{"id": "O93c9H4SXc", "number": 18709, "cdate": 1758290311406, "mdate": 1763733050826, "content": {"title": "Beyond Softmax and Entropy: $f$-Regularized Policy Gradients with Coupled Parametrizations", "abstract": "We introduce $\\texttt{f-PG}$, a new class of stochastic policy gradient methods regularized by a family of $f$-divergences, including entropy and Tsallis divergences. For each divergence, we employed a $\\textit{coupled}$ parameterization, defined by $f$-softargmax, which allows us to establish the first explicit, non-asymptotic, last-iterate convergence rates for stochastic policy gradient.\nTo derive our analysis, we prove that the $f$-regularized value function is smooth and satisfies a Polyak-Łojasiewicz inequality as a function of $f$-softargmax parameters. To establish the latter, we introduce a general policy improvement operator that restricts optimization to a well-defined policy space that excludes ill-behaved policies. In the case of softmax, this allows to escape the \"gravitational pull\" and yields the first $\\textit{explicit}$ convergence guarantees for this parameterization, closing a gap in the literature.\nFinally, we leverage these rates to derive sample complexity bounds for the unregularized problem and show that $\\texttt{f-PG}$ with Tsallis divergences provides a provably better sample complexity/regularization bias trade-off compared to softmax-based policy gradient with entropy regularization.", "tldr": "We propose an $f$-divergence–regularized policy gradient method with coupled parameterization, providing explicit global last-iterate convergence rates in the stochastic setting.", "keywords": ["policy gradient methods", "reinforcement learning theory", "f-divergence", "Tsallis entropy", "Shannon entropy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8b26f039dd71284d2ac5bcffc0d847eb9067f1c.pdf", "supplementary_material": "/attachment/aba84d9d0ba1d7d79be09555240e9d424b75b101.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a general f-regularized policy gradient method with coupled parametrization, which generalizes the popular KL regularization with softmax parametrization. Theoretically, they demonstrate global convergence and show an improved trade-off between sample complexity and regularization bias compared to the classical entropy-softmax combination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a general framework for studying policy gradient methods in f-divergence-regularized RL with coupled parametrization, providing strong theoretical guarantees."}, "weaknesses": {"value": "- The experiments are conducted on a 5×5 GridWorld, which limits the generalizability of the proposed policy gradient method.\n- Apart from the theoretical contributions, which I do not feel qualified to fully assess, it is unclear to me what practical advantage the proposed algorithm offers compared to using KL regularization with softmax parametrization."}, "questions": {"value": "- What are the main factors limiting the proposed algorithm from being evaluated on a broader set of environments, such as Atari?\n- How does this approach compare to prior work [1] showing that Mirror Descent, with different choices of mirror map and optimization space (e.g., logits or policy), can lead to novel regularized policy gradient objectives?\n\n[1] Vaswani, S., Bachem, O., Totaro, S., Müller, R., Garg, S., Geist, M., Machado, M.C., Castro, P.S. and Roux, N.L., 2021. A general class of surrogate functions for stable and efficient reinforcement learning. arXiv preprint arXiv:2108.05828."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "24yxMJvnX9", "forum": "O93c9H4SXc", "replyto": "O93c9H4SXc", "signatures": ["ICLR.cc/2026/Conference/Submission18709/Reviewer_ri98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18709/Reviewer_ri98"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761351679200, "cdate": 1761351679200, "tmdate": 1762928415242, "mdate": 1762928415242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes generalization of entropy regularized policy gradient methods using f-divergences, as well as corresponding parameterizations.\n\nThe authors shows global convergence results the proposed methods, as well as experimental results to verify the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Generalizing the entropy regularizer to f-divergences is a reasonable idea.\n2. The projection operator to avoid deterministic policies is useful."}, "weaknesses": {"value": "1. The work seems to be generalizing and recovering existing work (entropy, Tsallis). It seems this work is not suggesting some new regularization which is better performing than existing ones."}, "questions": {"value": "1. I am wondering how the methods perform when the interest is to obtain the unregularized optimal value/policy. I can imagine the operator to avoid small probability could fail (since it is unavoidable to get close to deterministic policies in that case). Does your method provide better iteration/sample complexity?\n\n2. The authors mentioned that entropy and Tsallis, which are two important existing regularization, can be recovered by the f-divergences. Are there any new regularization which can perform better than existing ones (or the other way those two are the best possible)?\n\n3. It is claimed that for the PL inequality, \"our proof, based on the properties of Fenchel-Legendre conjugation, is much simpler\". After checking it seems to me the key ideas of smoothness and PL inequality proofs are largely similar to existing proofs in entropy regularized proofs, especially the lower bounding of policy gradient using Eq. (52) and $H(w_\\theta)$, and upper bounding the suboptimality gap by Eq. (51). Could you elaborate how Fenchel-Legendre conjugation makes your proofs much simpler (my understanding is that this helps proving upper bounding suboptimality gap in general f-divergences)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UkvLFum5y9", "forum": "O93c9H4SXc", "replyto": "O93c9H4SXc", "signatures": ["ICLR.cc/2026/Conference/Submission18709/Reviewer_zseX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18709/Reviewer_zseX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077609333, "cdate": 1762077609333, "tmdate": 1762928414070, "mdate": 1762928414070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the entropy regularized softmax PG in Mei et al. 2020b to the a general regularization based on the coupled parameterization. Convergence guarantees are also established."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Extension of entropy regularized softmax PG."}, "weaknesses": {"value": "* Presentation can be improved. In particular, more discussion of the theoretical results will be helpful for the reader to get a better understanding since there are quite a lot of parameters involved. \n* More numerical experiments can be conducted, even though the authors argue that the goal is to verify the effectiveness of Tsallis divergence. It is known that (entropy regularized) softmax PG is highly inefficient compared to (entropy regularized) NPG due to that the appearance of the policy in the exponential term may cause a mislead. I am wondering whether the new algorithm based on Tsallis divergence will be efficient than NPG or not. At least, tests for the exact setting can be conducted. \n* References are missing. For example in \"Elementary analysis of policy gradient methods\" by Liu et al 2024, it is shown that softmax PG (without regularization) can achieve sublinear convergence for ANY constant step size though the problem dependent constant still exists."}, "questions": {"value": "* In Mei et al. 2020b, there are exists sublinear convergence result for the non-regularized softmax PG. As far as I can see, the authors only extend the regularized counterpart. Is it right? Even though the sample complexity can be obtained for the non-reguarlized case by choosing the parameter carefully, the sublinear convergence result in the exact setting for the non-regularized case cannot be obtained from the linear result for the regularized case. Isn't it?\n* A major theoretical contribution claimed by the authors is that there is no problem dependent hidden in the convergence rate. Is it fully due to the projection operator or also relies the the particular divergence? Will it also be helpful for removing the constant in the sublinear convergence of the non-regularized softmax PG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B2TF3wqP1d", "forum": "O93c9H4SXc", "replyto": "O93c9H4SXc", "signatures": ["ICLR.cc/2026/Conference/Submission18709/Reviewer_s4Jk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18709/Reviewer_s4Jk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155513011, "cdate": 1762155513011, "tmdate": 1762928413078, "mdate": 1762928413078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission derives fast rates (i.e., $\\tilde{O}(\\epsilon^{-1})$) for learning against the $f$-divergence regularized objective in discounted MDPs, which is achieved in a batch-online manner. In the algorithm design, the *log-linear policy with a reference model $\\pi^\\text{ref}$* under KL-regularization is extended to the so-called \"coupled parametrization\" in this submission. Finally, the submission claims a separation result between Tsallis-entropy regularization and entropy regularization by comparing *two upper bounds*."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This is the first work deriving fast rates $\\tilde{O}(\\epsilon^{-1})$ for learning w.r.t. divergence-regularized objectives in the discounted settting.\n- The proofs are correct."}, "weaknesses": {"value": "> **To AC: the reviewer is willing and ready to further discuss about this submission with AC or even SAC if needed.**\n\n- Every time $[f']^{-1}$ appears in this submission, it is **mathematically wrong**: because, for example, chi-square divergence is already nice enough, but the $a \\mapsto \\max\\{0, a\\}$ (also appears as ReLU in the lierature) in the Lemma G.2 of [10] already certificates that it is not possible to simply subsume the solution to this constrained optimization problem using $[f']^{-1}(\\cdot)$ and the complementary slackness condition might need to be tackled in a case-by-case way.\n\n- The smoothness property (i.e., the Hessian spectrum upper bound) is trivial and does not even deserve a theorem, i.e., Theorem 4.3, because it is well known that the Fenchel conjugate of a $\\alpha$-strongly-convex function is $\\alpha^{-1}$-smooth (under very mild qualitative regularity conditions); which is nearly exactly the case for the regularized objective with equation (9) as the parametrization\n- The artificial assumptions from Line 201 to Line 210 are far from enlighting and even **known to be** highly unnecessary for important cases like reverse-KL regularization and chi-square divergence regularization: \n  1. to be concrete, the reviewer **strongly disagrees** with Line 215-216 (the sentence around \"which prevents\") because the analysis of learning w.r.t. the reverse-KL-regularized objectives have become sharp (in terms of the dependency on $\\epsilon$, i.e., $\\tilde{O}(\\epsilon^{-1})$) for contextual bandits and episodic MDPs in the hybrid setting [6, 1], for contextual bandits in the offline setting [9,7], and for both contextual bandits and episodic MDPs in the online setting [8].\n  2. Even without Assumption P, and even in the pure offline setting without exploration, the analysis of learning against many divergence regularized objectives is doable, as manifested in the [7]\n- The term \"coupled parametrization\" is confusing because it is just an extension of the \"log-linear policy *with a reference policy*\" to the general $f$-divergence setting, see, e.g., Definition 1.1 in [1]\n- The PL condition, i.e., the \"essentially strongly concave\" property of the regularized objective **should not appear as a brandly new contribution** in this submission because it has been presented in a more minimalist setting in the offline contextual bandits setting in [7]\n\n\n- If the authors do plan to claim the separaion between Tsallis regularization and vanlla entropy regularization for learning against the unregularized objective at the end of Section 5, they should not compare two upper bounds.\n- The study of learning w.r.t. divergence-regularized value functions and objectives dates back to a long line a previous effors **no later than** [2, 3], at least in the episodic MDP setting. And **divergence-regularized performance difference lemma** appeared **no latter than** Section 5 in [8] and Lemma 3 in [5]\n  - If the authors do consider their analysis is totally unrelated to the divergence-regularized performance difference lemma (or the so-called soft peformance difference lemma) in the literature, they should justify it **technically instead of secretly**.\n  - Also, the **divergence-regularied Bellman operator** has been illustrated in detail in both [3] and [5], which is certainly not a \"newly introduced\" concept in this submission.\n- The reviewer does not want to claim that the authors are **plagiarizing or rephrasing previous works in a more involved way**, but the authors should respect the previous efforts in the theory community in a decent way.\n  - If the authors do consider the discounted setting in this submission is fundamentally different from the finite-horizon episodic MDP settings or the contextual bandit settings considered in [1-10] in the literature and does not plan to discuss the relation between this submission and any of [1-10], they should justify it **technically instead of secretly**.\n  - **To AC: the reviewer is willing to discuss about this point if necessary.**\n\n\nReferences\n\n[1] Foster, Dylan J., Zakaria Mhammedi, and Dhruv Rohatgi. \"Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration.\" arXiv preprint arXiv:2503.07453 (2025).\n\n\n[2] Xiong, Wei, et al. \"Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint.\" arXiv preprint arXiv:2312.11456 (2023).\n\n[3] Xie, Tengyang, et al. \"Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf.\" arXiv preprint arXiv:2405.21046 (2024).\n\n[4] Huang, Jiawei, et al. \"Can rlhf be more efficient with imperfect reward models? a policy coverage perspective.\" arXiv preprint arXiv:2502.19255 (2025).\n\n[5] Yuan, Yurun, et al. \"Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning.\" arXiv preprint arXiv:2505.15311 (2025).\n\n\n[6] Zhao, Heyang, et al. \"Sharp analysis for kl-regularized contextual bandits and rlhf.\" arXiv preprint arXiv:2411.04625 (2024).\n\n[7] Zhao, Qingyue, et al. \"Towards a Sharp Analysis of Offline Policy Learning for $ f $-Divergence-Regularized Contextual Bandits.\" arXiv preprint arXiv:2502.06051 (2025).\n\n[8] Zhao, Heyang, et al. \"Logarithmic regret for online kl-regularized reinforcement learning.\" arXiv preprint arXiv:2502.07460 (2025).\n\n\n[9] Aminian, Gholamali, et al. \"Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models.\" arXiv preprint arXiv:2502.01203 (2025).\n\n\n[10] Huang, Audrey, et al. \"Is best-of-n the best of them? coverage, scaling, and optimality in inference-time alignment.\" arXiv preprint arXiv:2503.21878 (2025)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The authors are plagiarizing or rephrasing previous works in a more involved way, which has been detailed by the reviewer in the **Weaknesses** section."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MFsshywhv6", "forum": "O93c9H4SXc", "replyto": "O93c9H4SXc", "signatures": ["ICLR.cc/2026/Conference/Submission18709/Reviewer_q88P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18709/Reviewer_q88P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762607469909, "cdate": 1762607469909, "tmdate": 1762928412161, "mdate": 1762928412161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}