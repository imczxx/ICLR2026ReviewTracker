{"id": "QehxrR7e8F", "number": 10465, "cdate": 1758172486625, "mdate": 1759897649264, "content": {"title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark", "abstract": "Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical LiDAR and a hybrid semi-solid-state LiDAR, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas.\nThen, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by 63.2% and 40.2%, respectively, and improves the registration recall (RR) by 5.4%, which validates its effectiveness in achieving accurate cross-source point cloud registration.", "tldr": "", "keywords": ["real-world cross-source point cloud registration dataset;  cross-source point cloud registration"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8ebb09df0d4db512dce022a482e33b9433bdd8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the more specific and challenging problem of cross-source point cloud registration, and proposes a real-world dataset named Cross 3DReg for outdoor scenes, which can be used for training and to some extent fills the gap in the field. At the same time, a cross-source registration framework based on cross-registration is also proposed. It uses unaligned images to predict the overlapping areas of the source point cloud and the target point cloud, effectively filtering out redundant points in the irrelevant areas and significantly reducing the interference of noise in the non-overlapping areas. Finally, cross-source point cloud registration is achieved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. In the previous research field, either the cross-source datasets were too small to support training, or they were synthetic datasets. The real-world cross-source datasets of this paper have to some extent filled the gap in this field.\n2. The article is well-written and easy to understand.\n3. This paper uses image features to enhance point cloud features to solve the data problems between cross-source point clouds, which is an effective idea."}, "weaknesses": {"value": "1. Although cross-source point cloud registration is novel, the dataset you provided, Cross 3DReg, consists of rotating mechanical lidar and mixed semi-solid lidar from different sources. I believe this is essentially radar scanning, and the distribution differences between the data of the two are not significant. The research significance and value are not great, and it is questionable.\n2. The capitalization of the names of the baselines needs to be standardized, such as RoITr.\n3. The superscripts in formulas and symbols (point clouds and point cloud features) are somewhat chaotic and not conducive to reading. It is recommended to unify and standardize them.\n4. The method in this paper is an improvement based on the Geotrans framework, adding two modules, OMP and VGAM. The innovation at the method level is not significant.\n5. The ablation experiment setting is unreasonable. A set of VGAM w/o OMP experiments should be added in Table 3. Only from (a) and (d), it is not clear whether it is the influence of OMP or VGAM. The effect of OMP is not clear."}, "questions": {"value": "1. Based on experience, it is necessary to re-examine whether the OMP module is necessary. Firstly, in some cases, the overlapping estimation of images and point clouds may introduce incorrect masks. Secondly, the overlapping area between the point clouds selected by the two image overlapping masks is smaller than that between the two original point clouds. This reduction in overlap will definitely affect the registration effect. Thirdly, if the data volume is sufficient, I think directly using visual features to enhance the complete point cloud features instead of applying masks to the point clouds may yield better results.\n2. You mentioned in 139-141 that the recent methods Zhang et al. (2022a); Xu et al. (2024; 2025) have introduced other modal information, which may fail in cross-source point cloud registration tasks. However, in your method, you also introduced images (containing color, semantic, and texture information), but you only compared it with the point cloud input methods (such as GeoTrans) in the experiments, which is somewhat unfair. Please add comparative experiments with methods that incorporate multimodal input."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PPSZ1osOQa", "forum": "QehxrR7e8F", "replyto": "QehxrR7e8F", "signatures": ["ICLR.cc/2026/Conference/Submission10465/Reviewer_dtjm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10465/Reviewer_dtjm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761108640011, "cdate": 1761108640011, "tmdate": 1762921763247, "mdate": 1762921763247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of cross-source point cloud registration, which aims to align 3D data collected from different types of sensors. Compared with same-source registration, this task is more challenging due to the lack of large-scale real-world datasets and the inherent differences in sensor characteristics. To address these issues, the authors construct Cross3DReg, a large-scale real-world multi-modal dataset collected using a rotating mechanical LiDAR and a hybrid semi-solid-state LiDAR. They further propose an overlap-based registration framework that predicts overlapping regions between unaligned images and point clouds to filter out irrelevant points, and introduce a visual-geometric attention module to enhance feature consistency across modalities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses the problem of cross-source point cloud registration, which is technically challenging. \n\nThe construction of a cross-source registration dataset represents a contribution to the community. \n\nThe manuscript is overall well written and easy to follow."}, "weaknesses": {"value": "1. The core concern lies in the introduction of additional image information during the point cloud registration process. This raises questions about the fairness of the comparison for the baselines do not use the images, and Table 3 shows that the performance improvement primarily stems from the inclusion of image data.\n\n3. Although the authors claim that the images and point clouds are not geometrically aligned, they still assume a significant spatial overlap between the image and the two point clouds. Moreover, the registration process is conducted only within these “image-pc overlapping regions”, which seems difficult to achieve in practice.\n\n3. Furthermore, first determining the overlapping regions between the image and point clouds appears somewhat counter-intuitive, since the modality gap between images and point clouds is typically larger than that between cross-source point clouds."}, "questions": {"value": "Please refer to the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Esbve6uO7x", "forum": "QehxrR7e8F", "replyto": "QehxrR7e8F", "signatures": ["ICLR.cc/2026/Conference/Submission10465/Reviewer_5DW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10465/Reviewer_5DW7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481415107, "cdate": 1761481415107, "tmdate": 1762921762813, "mdate": 1762921762813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-source point cloud registration method that aligns point cloud data from different sensors. The authors construct a large-scale multi-modal dataset for cross-source point cloud registration. They introduce an overlap-based registration framework that uses unaligned images to predict overlapping regions between source and target point clouds. Additionally, they propose a visual-geometric attention-guided matching module that enhances cross-source point cloud feature consistency by fusing image and geometric information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A large scale cross source point cloud registration dataset is constructed.\n- An overlap-based cross-source point cloud registration method is proposed."}, "weaknesses": {"value": "- The dataset construction uses a hybrid semi-solid-state LiDAR to capture source point clouds and a 64-line rotating mechanical LiDAR for target point clouds. However, the rationale for this choice is unclear. Is this a common practice in real-world applications?\n- One key to the proposed method's success is predicting the overlap region between source and target point clouds, as well as between point clouds and images. More experimental results demonstrating this prediction should be added.\n- The experiments are insufficient. It would better to also evaluate the proposed method on other cross-source point cloud registration datasets. In addition, more state-of-the-art cross-source point cloud registration methods discussed in related work should be compared."}, "questions": {"value": "- What is the rationale for constructing the cross-source point cloud registration dataset in such a way?\n- How does the proposed method perform on other existing cross-source point cloud registration datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rgl27LI2HC", "forum": "QehxrR7e8F", "replyto": "QehxrR7e8F", "signatures": ["ICLR.cc/2026/Conference/Submission10465/Reviewer_XRz2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10465/Reviewer_XRz2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907037462, "cdate": 1761907037462, "tmdate": 1762921762146, "mdate": 1762921762146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method for cross-source point cloud registration. The key insight is leveraging images to detect overlap regions between point clouds captured by different sensors, thereby addressing registration failures caused by sensor discrepancies. To facilitate research in this area, the authors introduce Cross3DReg, a new large-scale real-world dataset for cross-source point cloud registration, which was previously lacking in the field. Experimental results demonstrate that the proposed method significantly outperforms existing point cloud registration approaches, achieving substantial improvements across evaluation metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear presentation**: The paper is well-written and easy to follow, providing sufficient context to understand both the field and the proposed method's contributions.\n- **Valuable dataset contribution**: The proposed Cross3DReg dataset addresses a critical gap in the community, providing the large-scale real-world benchmark that researchers have been seeking for cross-source point cloud registration.\n- **Strong experimental validation**: The proposed method demonstrates substantial performance gains over existing baselines. The ablation study effectively validates the contribution of each component in the proposed approach."}, "weaknesses": {"value": "- **Concerns regarding evaluation fairness**: While leveraging auxiliary image data to identify overlap regions and guide point matching through the visual-geometric attention module is a distinctive feature of the proposed method, this additional modality was not available to baseline methods. This asymmetry in input data raises concerns about fair comparison—the performance gains may partially stem from access to richer input rather than solely from algorithmic improvements."}, "questions": {"value": "- **Performance of VRHCF on Cross3DReg**: VRHCF is the only dedicated cross-source point cloud registration method included in the comparison, yet it appears to fail completely on the Cross3DReg benchmark. Could the authors provide insight into why this occurs? Is this due to specific characteristics of the Cross3DReg dataset (e.g., scale, sensor types, scene complexity), limitations in VRHCF's architecture, or implementation issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wObUeEOJ84", "forum": "QehxrR7e8F", "replyto": "QehxrR7e8F", "signatures": ["ICLR.cc/2026/Conference/Submission10465/Reviewer_3obY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10465/Reviewer_3obY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968188946, "cdate": 1761968188946, "tmdate": 1762921761445, "mdate": 1762921761445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}