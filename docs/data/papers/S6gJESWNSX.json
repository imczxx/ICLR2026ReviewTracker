{"id": "S6gJESWNSX", "number": 2230, "cdate": 1757037763573, "mdate": 1763608144983, "content": {"title": "Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation", "abstract": "We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper.\nWe build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: \n(i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. \nIt features 18 tasks in multiple-choice and open-ended formats.\nSpecifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement.\nUsing this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations.\nTo improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning.\nWe start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records.\nThis produces 140,000 candidate pairs.\nWe then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references.\nThis process yields a final set of 639 new pairs.\nFinally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and Gemma-27B.\nFine-tuning leads to consistent performance improvements across Sci2Pol-Bench.\nNotably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B).\nThese demonstrate the effectiveness of our corpus in bridging the gap between science and policy.", "tldr": "The first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper.", "keywords": ["Benchmark", "Dataset", "Policy", "Science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cb1e54f4929a647e5a1c7ff718d79a1bc5fbd3d.pdf", "supplementary_material": "/attachment/2c23faab835453f513b6e8b2823767b048b8234d.zip"}, "replies": [{"content": {"summary": {"value": "The aim of the paper is to explore how LLMs can enhance by themselves their expertise in technology evaluation tasks.\nThe paper introduces a benchmarks and  a self-questioning and retrieval framework in which an LLM generates questions about patents, retrieves related information, and re-evaluates its own judgments to improve accuracy. That's the core idea. \nExperiments are conducted on a large dataset of ICT patents, using LLaMA as the main model, and investigate how question–answer pairs generated by different LLMs affect performance in pairwise similarity judgments between patents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem, i.e., enabling LLMs to refine their own reasoning and knowledge without additional supervised data by assuming that knowledge is in but not easy to extract\n- A large patent dataset for pairwise comparisons.\n- The study of cross-LLM question transfer is novel (as far as I know as reviewer) and provides interesting observations.\n- The analysis reveals patterns of when the LLM has knowledge but it's not easy to be explored"}, "weaknesses": {"value": "The main limitation of the work concerns its restricted generalizability. Although the authors claim that the framework is broadly applicable, all experiments focus on ICT patent abstracts. The absence of tests in other domains—such as biomedical, legal, or scientific text—makes it difficult to assess how well the method transfers beyond this narrow context.\n\nA second limitation lies in the dependence on the chosen model. Most experiments use LLaMAs even if appendix mentions results with Qwen, Mixtral, and GPT-4o-mini. \n\nThe construction of the patent dataset and the definition of similarity might require a clearer exposition. Pairs are selected according to cosine similarity in a PATCER embedding space, but the paper does not seem to  specify the similarity thresholds, how duplicates were filtered, or how balanced the dataset is. Reporting these details—and testing sensitivity to the similarity cutoff—would make the empirical section more reproducible.\n\n\nThe use of the term “active learning” is potentially misleading. The proposed method involves self-questioning and retrieval, but not the core features of traditional active learning such as query selection with human feedback. Clarifying the terminology—or providing a formal mapping to the active learning paradigm—would prevent conceptual ambiguity.\n\nThe complexity of generated questions remains limited. Although the authors distinguish between surface-level and deeper questions, the generation process could be extended to produce more structured, compositional, or multi-hop queries that probe higher-order reasoning.\n\nRegarding labeling, the paper should explain more clearly how the binary yes/no decisions are obtained in the pairwise task. Specifically, it would be useful to state how the label mapping (e.g., 1 = same, 0 = different) and the confidence aggregation were handled, and whether equivalent formulations (“same?” vs. “different?”) produced consistent results.\n\nHow can authors be sure that used testset has not been used during the trining because many patent texts and scientific documents may already be part of pretraining corpora for large models. Is it possible that the evaluation data overlap with training data?\n\nIs it possibile to extend and generalize what here presented to other scenario (not just patents)?"}, "questions": {"value": "See previous section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eynEQz6i6D", "forum": "S6gJESWNSX", "replyto": "S6gJESWNSX", "signatures": ["ICLR.cc/2026/Conference/Submission2230/Reviewer_6pgv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2230/Reviewer_6pgv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835653061, "cdate": 1761835653061, "tmdate": 1762916153650, "mdate": 1762916153650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sci2Pol-Bench and Sci2Pol-Corpus for evaluating and fine-tuning LLMs on policy brief generation from scientific papers. Sci2Pol-Bench contains 18 tasks across a five-stage taxonomy (Autocompletion, Understanding, Summarization, Generation, Verification) built on 85 expert-written paper-brief pairs. The authors evaluate 13 LLMs and reveal significant limitations. Sci2Pol-Corpus contains 639 paper-brief pairs curated from 5.6M policy documents. Fine-tuning three models on this corpus yields improvements, with Gemma-27B surpassing GPT-4o and DeepSeek-V3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses the gap between scientific research and policy communication through a well-designed five-stage taxonomy that mirrors real brief-writing workflows.\n\n2. Uses 85 expert-written paper-brief pairs from prestigious venues, ensuring reliable ground truth. The 18 tasks enable fine-grained evaluation across multiple capabilities.\n\n3. Three-stage pipeline (retrieval from 5.6M documents, LLM filtering, in-context polishing) with validation studies addressing information leakage concerns."}, "weaknesses": {"value": "1. Only 85 test pairs and 639 training pairs, concentrated in climate/energy and health sociology. No evidence of generalization to other scientific domains (CS, math, humanities, economics).\n\n2. Tasks 7-15 use Gemini-2.5-Pro as judge, while GPT-o3 generates data for Tasks 5,11,13,16,18 and polishes all corpus documents. Models in GPT/Gemini families may benefit from evaluation alignment with their own generation patterns.\n\n3. Table 3 shows Gemma-27B-SFT outperforms GPT-4o, but lacks analysis of what was learned (factual grounding vs. style vs. pattern memorization). No qualitative comparison of base vs. fine-tuned outputs."}, "questions": {"value": "1. Can you validate LLM judges with human experts? Provide inter-rater agreement between Gemini-2.5-Pro and multiple political science experts for Tasks 11-15.\n\n2. How does performance transfer across domains? Test fine-tuned models on policy briefs from economics, technology policy, or other areas beyond Nature/JHSB domains.\n\n3. What are common failure modes? Categorize errors (factual/structural/stylistic/logical) for top models on each task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SgTQNzdewh", "forum": "S6gJESWNSX", "replyto": "S6gJESWNSX", "signatures": ["ICLR.cc/2026/Conference/Submission2230/Reviewer_F3Nb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2230/Reviewer_F3Nb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921433511, "cdate": 1761921433511, "tmdate": 1762916153468, "mdate": 1762916153468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors contribute an evaluation benchmark and corpus for the translation of scientific research papers to policy briefs: Sci2Pol-Bench and Sci2Pol-Corpus. For Sci2Pol-Bench, they detail a rigorous construction process of 18 tasks drawn from 85 expert-written paper-brief pairs, e.g., multiple-choice probes and open-ended generation tasks, along with associated metrics (e.g., F1, LLM-as-judge). For Sci2Pol-Corpus, they mine high-quality paper-brief pairs using a large-scale web dataset and LLM-as-judge filtering, obtaining 639 pairs. Lastly, they evaluate several frontier models and find finetuning on Sci2Pol-Corpus enables smaller models (e.g., Gemma-3-27B) to beat near-frontier models (DeepSeek-V3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Important setting and strong motivation: the conversion of scientific knowledge into content helpful to policymakers is increasingly important as AI matures and its real-world impact increases. Many simpler benchmarks are saturated by frontier models, and therefore contributing challenging expert-driven, real-world tasks is an important direction for the field.\n- The paper is clear and well-written.\n- The pipeline for evaluation and data curation is very meticulous. These details are clearly presented and will be very useful, such as to those tackling settings such as constructing RL environments for other domains."}, "weaknesses": {"value": "The major challenge with new benchmarks is whether they are already saturated or likely to be saturated soon.\n* Could you provide a human baseline on Sci2Pol-Bench (e.g., with a policy expert)? Without it, it is hard to judge whether the benchmark is really unsaturated, or if there is just very high label noise or ambiguity.\n* Could you provide an analysis of the saturation of Sci2Pol-Bench with respect to some measure of model capability, such as pretraining FLOPs or release date (e.g., similar to the analysis of Epoch AI [1])? This would provide some idea of how soon this benchmark might be saturated.\n* [Less critical] If possible, could you report the agreement between expert human graders and the LLM grader function, e.g., for the Reference-Free Score in Tasks 7-10 or Reference-based Score in Tasks 11-15?\n\n[1] Kwa et al., Measuring AI Ability to Complete Long Tasks, 2025. https://arxiv.org/abs/2503.14499"}, "questions": {"value": "Please see Weaknesses above for my concrete suggestions to improve the paper. These are more minor clarifications:\n- How did you source your experts, e.g., the political experts mentioned at line 202?\n- Do you use experts in all of your task constructions involving selection, e.g., Tasks 7-10 where you find summarizing paragraphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l3bBZ5VwFg", "forum": "S6gJESWNSX", "replyto": "S6gJESWNSX", "signatures": ["ICLR.cc/2026/Conference/Submission2230/Reviewer_RJb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2230/Reviewer_RJb4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953427805, "cdate": 1761953427805, "tmdate": 1762916153264, "mdate": 1762916153264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Sci2Pol-Bench, a five-stage benchmark (Autocompletion, Understanding, Summarization, Generation, Verification) for turning scientific papers into policy briefs, and Sci2Pol-Corpus, a curated training set to fine-tune LLMs for this task. The benchmark draws on 85 expert-written paper–brief pairs from venues such as Nature Energy and Nature Climate Change and spans 18 tasks (mixed MCQ and open-ended). For generation tasks, the authors argue ROUGE/BERTScore are inadequate and instead use an LLM-as-judge rubric aligned to paper-grounded criteria. They evaluate 13 models, then fine-tune three open models on a 639-pair corpus created via citation linking over 5.6M policy documents, LLM-based filtering, and in-context polishing. They report consistent gains and claim a fine-tuned Gemma-27B can outperform GPT-4o/DeepSeek-V3 on their benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear problem framing & timely task: Scientific-to-policy translation is impactful and under-served in current evaluation suites; the five-stage taxonomy mirrors a realistic writing workflow and is easy to reason about.\n\n* Well-structured benchmark: 18 tasks with transparent construction and metrics per stage; the dataset composition and task table are clearly presented and grounded in real expert-authored briefs.\n\n* Thoughtful metric discussion: The paper diagnoses why ROUGE/BERTScore miss quality signals in policy writing and proposes rubric-driven LLM judging tailored to sections (problem, findings, methods, implications) and full briefs.\n\n* Empirical breadth: the authors evaluate13 diverse models, and results are summarized across taxonomy stages."}, "weaknesses": {"value": "* Evaluation circularity & judge dependence: Many key results rely on proprietary LLM judges (Gemini-2.5-Pro for reference-free and reference-based scoring; o3 for filtering/polishing). This creates potential vendor/prompt-specific bias, and the headline claim “Gemma-27B-SFT surpasses GPT-4o/DeepSeek-V3” is only as strong as the judge’s alignment to human experts. Human evaluation (even on a subset) or judge triangulation (≥2 independent judges + agreement analysis) would strengthen credibility.\n\n* Risk of construction-time leakage/style imprinting: The in-context polishing step conditions on three expert exemplars from the 85-pair set; while Appendix H.6 reportedly checks leakage, the paper still asks the reader to trust that polishing didn’t imprint benchmark-specific style cues that advantage similar models/decoding. A stronger defense would include ablations that: (i) polish with held-out exemplars from outside the 85 pairs; (ii) evaluate on a disjoint external set of expert briefs.\n\n* Heuristic retrieval bias: Keeping only policy documents citing ≤3 papers biases toward single-paper briefs and may not reflect real policy synthesis, which often aggregates many sources. An analysis of how this heuristic shapes style/length and whether models trained on such pairs generalize to multi-source briefs is missing.\n\n* Limited human oversight in label generation: Several tasks use o3 to generate items/labels with “manual review” mentioned briefly (e.g., “correct three issues” in Lines 267-268). More details on reviewer counts, agreement, and correction rates would calibrate trust in classification ground truth.\n\n* Ethics & release clarity: The Ethics section is brief; given the policy domain, more detail is warranted on licensing, redistribution boundaries, bias auditing of the judge, and safeguards to prevent models from emitting over-confident policy prescriptions not supported by evidence. The paper mentions anonymous links and fair-use reasoning but lacks concrete release protocols and impact/risk assessments specific to policy advice."}, "questions": {"value": "Please address all issues raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RjIsdp7njI", "forum": "S6gJESWNSX", "replyto": "S6gJESWNSX", "signatures": ["ICLR.cc/2026/Conference/Submission2230/Reviewer_UZs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2230/Reviewer_UZs4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030286908, "cdate": 1762030286908, "tmdate": 1762916153090, "mdate": 1762916153090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}