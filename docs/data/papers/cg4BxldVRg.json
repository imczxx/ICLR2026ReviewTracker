{"id": "cg4BxldVRg", "number": 9215, "cdate": 1758115423077, "mdate": 1759897737314, "content": {"title": "RLTime: Reinforcement Learning-Based Feature Attribution for Interpretable Time Series Models", "abstract": "Deep time-series models are widely used in healthcare and finance, where interpretability is essential. Explaining these models is challenging due to temporal dependencies, nonadditive feature interactions, and high-dimensional inputs. Recent approaches learn continuous masks under sparsity constraints to generate attribution maps. While effective, this method has two key limitations: it explores the combinatorial space of feature subsets myopically, often missing synergistic features, and suffers from a soft-to-hard gap, where soft masks used during training misalign with the discrete selections needed at inference. We introduce RLTime, a framework that learns discrete attributions through sequential information acquisition. A masked reconstruction network recovers the latent representation of the reference model from partially observed inputs, such that the change in the reconstructed latent after revealing a feature can be leveraged to quantify its marginal value. This signal defines rewards for a distributional reinforcement learning agent that iteratively unmasks features, balancing exploration and exploitation while operating directly in the discrete action space. The agent’s value function scores the utility of revealing each feature, enabling a clear ranking of features and a non-myopic acquisition policy. Experiments on synthetic and real-world datasets demonstrate that RLTime significantly improves attribution quality, exploration-exploitation balancing, and interpretability.", "tldr": "This paper utilize RL to replace the discrete optimization techniques in explainable ai", "keywords": ["time series classification", "explainable ai", "reinforecement-learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86f7c5b4225edf4c4977b0b9e1f4ae0da0ad3ccc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper leverages reinforcement learning to study and explain the feature importance of time series data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work leverages reinforcement learning to study and explain the feature importance of time series data, to bypass the limitations of continuous relaxation. \n- Comprehensive experiments show the strong performance of the proposed method."}, "weaknesses": {"value": "- Computational training: masked reconstruction model, reference model, and agent require training. Any one model without careful training would drop the performance.\n- Figure 1 and Figure 2 lack clear explanations. It is good to provide key components and processes within the captions.\n- No source code."}, "questions": {"value": "- Is it possible to design the hybrid reward functions by combining input space and embedding space?\n- Did you consider the out-of-distribution (OOD) issue for reconstruction model and reference model? If so, how did you handle it?\n- You are focusing time series classification tasks in this work, is it possible to generalize to regression tasks?\n- lines 83-85: what do you mean by \"We first pretrain a masked reconstruction network that predicts the latent representation of a reference model.\"? I thought masked reconstruction network is only responsible for recovering time series, and has nothing to do with \"predicts the latent representation\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5UR0tgwK6I", "forum": "cg4BxldVRg", "replyto": "cg4BxldVRg", "signatures": ["ICLR.cc/2026/Conference/Submission9215/Reviewer_c1g8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9215/Reviewer_c1g8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772646815, "cdate": 1761772646815, "tmdate": 1762920877448, "mdate": 1762920877448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper (RLTime) presents a novel approach to time series feature attribution by formulating it as a sequential information acquisition task, effectively addressing the \"soft-to-hard gap\" in continuous relaxation interpretation methods. The proposed reward signal is based on local latent shifts in a pretrained reconstruction model, which is theoretically motivated and shows strong empirical performance. However, the framework introduces significant complexity through the two-stage training process, and the computational overhead during training is not reported or analyzed. Furthermore, the improved exploration appears to be trade-off against precision in several cases (Table 3)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. RLTime works in the discrete action space (selecting features to reveal) during both training and evaluation. This design directly circumvents the \"soft-to-hard gap\" and \"fractional leakage\" issues associated with continuous relaxations (e.g., Gumbel-Softmax).\n\n2. The use of reinforcement learning aims to learn a non-myopic acquisition policy that balances exploration and exploitation, potentially capture features that gradient-based methods might miss due to myopic exploration.\n\n3. The reward is defined as the local latent shift , measuring the marginal impact of revealing a feature on the reconstructed latent representation of the reference model.Proposition.1 provides theorem by linking the expected latent shift to the expected drop in latent MSE (assuming a calibrated reconstructor).\n\n4. The paper is well-motivated, well-structured and easy to follow."}, "weaknesses": {"value": "1. (Major) Unreported Computational Overhead and Training Complexity:\nThe methodology requires a complex two-stage training process: pretraining the masked reconstruction network and then training the RL agent. The RL training phase involves generating trajectories and requires forward pass through the reconstruction model and reference encoder per-step to compute rewards, which seems to be computationally intensive, especially with a large action space ($T \\times V$). While inference times are reported in Table 3, the paper did not cover any discussion of training times or a comparison of the overall computational cost against the baselines.\n\n2. (Major) Gray-Box Requirement:\nThe entire framework depends on accessing the intermediate latent representation $z$ produced by the reference model's encoder. The framework also assumes the reference model can be clearly separated into an Encoder ($G$) and a Predictor ($F$) with a distinct latent bottleneck. It cannot be applied if the reference model’s internal activations are hidden, or if the architecture is monolithic.\n\n\n3. (Major) Clarity of the Agent:\nThe MDP state is defined as $s_{t}=(x,m_{t})$, which includes the full input $x$. It is unclear what the RL agent actually observes. If the agent sees the full $x$, it is unclear how the mask $m_t$ effectively restricts the information available to the agent itself, rather than just restricting the input to the reconstruction model used for reward calculation. Also it is unclear if the policy is global across instances or trained per-instance?\n\n4. (Minor) Performance Indicating Trade-offs:\nDespite strong performance in recall (AUR), RLTime shows regressions in Area Under Precision (AUP) on synthetic datasets. It ranks last on SeqCombUV for AUP (0.7609 vs 0.9411 for the best baseline) in Table.1. This suggests that the improved exploration (higher AUR) achieved by the non-myopic RL approach may come at the cost of precision compared to the myopic baseline, a trade-off that is not discussed in the paper.\n\nClarifications are welcome and I will reconsider and raise the score if the questions are addressed."}, "questions": {"value": "1. The method assumes the reference model has learned a well-structured and meaningful latent space. If the latent space is entangled or poorly regularized, will the explanations still faithfully reflect this internal state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fyM8dKZWl", "forum": "cg4BxldVRg", "replyto": "cg4BxldVRg", "signatures": ["ICLR.cc/2026/Conference/Submission9215/Reviewer_2HDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9215/Reviewer_2HDc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870814644, "cdate": 1761870814644, "tmdate": 1762920877116, "mdate": 1762920877116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies interpretable time series models and highlights two key limitations of existing approaches, namely myopic exploration and the soft-to-hard gap. To address these limitations, the authors present RLTime, a framework that formulates attribution as a sequential information acquisition task and directly operates in a discrete action space. The framework integrates a masked reconstruction network and a distributional reinforcement learning agent with a well-defined reward. Finally, RLTime is evaluated on both synthetic and real-world datasets, demonstrating improved interpretability and predictive performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The paper addresses a practically important problem, aiming to improve explainability in deep time series modeling while maintaining predictive accuracy. Enhancing transparency in such models is valuable for building trust and supporting accountable decision processes.\n\n**S2.** The RLTime framework re-formulates attribution as sequential information acquisition and uses a reinforcement learning agent with a clearly defined reward, a masked reconstruction network, and a reference model encoder. From a technical perspective, the design is coherent and the components are integrated in a logical manner.\n\n**S3.** The experimental evaluation compares RLTime with representative baselines on both synthetic and real-world datasets. The study includes occlusion analyses, search space investigation, ablations, and visualization experiments, providing a relatively comprehensive empirical assessment."}, "weaknesses": {"value": "**W1.** While the paper discusses the two challenges in existing models, namely myopic exploration and the soft-to-hard gap, the motivation could be further strengthened. In particular, including empirical observations or case studies that demonstrate these issues in practice would help clarify their impact and better motivate the core design of the proposed framework.\n\n**W2.** In the experimental evaluation, it would be helpful to analyze the “failure cases” in greater depth. For instance, RLTime shows lower performance than TimeX++ under small thresholds in Figures 4 (b), (f), and (h). In addition, RLTime does not surpass the baseline models on the Boiler dataset in Table 2. A discussion of these cases, including possible reasons and insights, would strengthen the understanding of the framework’s behavior and limitations.\n\n**W3.** In Table 3, RLTime appears to consistently underperform Naïve Search on the AUP metric. This raises the question of whether different methods are making different trade-offs between AUP and AUR, with RLTime potentially favoring AUR at the cost of AUP. It would be useful to clarify which metrics are considered primary for model comparison, and to provide further discussion on this trade-off to help readers interpret the results.\n\n**W4.** The dataset selection across different experiments in both the main paper and the appendices appears inconsistent. Without a clear justification, this variation makes it more difficult to interpret the results in a unified manner and may affect the perceived coherence of the evaluation. To strengthen the credibility of the experimental findings, it would be helpful either to adopt consistent dataset choices across analyses or to clearly explain the rationale behind the selection for each experiment.\n\n**W5.** It would be beneficial to include an analysis of key hyperparameters to demonstrate the robustness of RLTime under different settings. Such results would provide a clearer picture of the framework’s sensitivity and stability."}, "questions": {"value": "Beyond W1-W5, I have the following questions for clarification:\n\n**Q1.** In Section 3.3, the paper explains the benefits of the proposed local difference formulation when defining the reward, compared to a global formulation. It would be helpful to present a comparison with the global alternative, i.e., the residual gap to the fully unmasked embedding, to provide empirical evidence supporting this design choice.\n\n**Q2.** In Section 3.4, the initial state assumes that all entries are masked. Further explanation on how training is initiated from this state would improve clarity, especially regarding how the reinforcement learning agent begins meaningful exploration under full masking.\n\n**Q3.** In the discussion of explainable reinforcement learning in Section 5, the connection between RLTime and existing methods in this category should be made more explicit. This would help position RLTime within the broader literature and clarify its conceptual differences or similarities.\n\n**Q4.** Appendix C suggests that RLTime and Naïve Search are both proposed in this work, with RLTime being more explorative and Naïve Search being more exploitative. If this interpretation is correct, a clearer explanation of their relationship would be valuable. In addition, practical guidance on when each approach would be preferable would support adoption by practitioners."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IzkM0PCxY4", "forum": "cg4BxldVRg", "replyto": "cg4BxldVRg", "signatures": ["ICLR.cc/2026/Conference/Submission9215/Reviewer_Yzyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9215/Reviewer_Yzyd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929400168, "cdate": 1761929400168, "tmdate": 1762920876811, "mdate": 1762920876811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}