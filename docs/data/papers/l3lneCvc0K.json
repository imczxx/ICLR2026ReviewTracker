{"id": "l3lneCvc0K", "number": 11775, "cdate": 1758203720298, "mdate": 1763649296703, "content": {"title": "Sequential Emergence of Phonemic, Syntactic, and Semantic Representations in Neural Networks", "abstract": "During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning.\nWhile the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations.\nHere, we investigate whether and when phonemic, lexical and syntactic representations emerge in the activations of artificial neural networks during their training.\nOur results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure.\nWhile this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require 2 to 4 orders of magnitude more data for these neural representations to emerge.\nTogether, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.", "tldr": "We propose that self-supervised models instantiate phoneme, semantic and syntactic structure, and measure the emergence of these 3 linguistic codes.", "keywords": ["structure", "linguistic", "representations", "decoding", "self-supervised", "speech"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c09024a30fe0354401fccd5fe11432cd39a92bae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the emergence of phonemic, lexical, and syntactic structure in LLMs over the course of training, and compare the resulting dynamics to patterns of acquisition in children language learning. It finds that speech and text models exhibit a staged trajectory through these types of knowledge, first learning phonemic, next lexical, and finally syntactic structure. It further shows that while these features qualitatively are similar to children, quantitatively there is a ~3 order of magnitude difference in number of samples required."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper conducts a very difficult measurement with admirable rigor and clarity. \n\nThe resulting message is potentially quite significant, suggesting that LLMs recapitulate qualitative features of language acquisition, and therefore understanding their acquisition dynamics may contribute to understanding human language learning (of course recognising the considerable quantitive difference which remains). \n\nThe paper is clearly written and easy to follow, with figures and experiments of a high standard."}, "weaknesses": {"value": "The paper could be strengthened by disaggregating the studied phenomena into subcategories (specific syntactic categories, etc), but this is reasonable to leave for future work.\n\nThe distinctions between phonemics, syntax and semantics rely on particular definitions that come from linguistics and word net, etc. It is well-demonstrated that these distinctions are represented in the network. But one could ask whether a different partitioning of inputs would yield an even clearer developmental trajectory with sharper distinctions. That is, to what extent should we trust these categories (particularly the specific syntactic categories posited)? Again this is reasonable to leave for future work."}, "questions": {"value": "How do learning dynamics and layer scores interact? Do all layers learn at the same time or at different times?\n\nThe paper links to toy models of hierarchical learning, which can have a coarse to fine dynamic. However, it is known that basic level categories can emerge first, perhaps due to frequency effects ('dog' is seen far more than carnivore). This effect was also modelled in Saxe et al 2019. Is there evidence for this aspect of human language development?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bnkd6NqDZM", "forum": "l3lneCvc0K", "replyto": "l3lneCvc0K", "signatures": ["ICLR.cc/2026/Conference/Submission11775/Reviewer_syiK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11775/Reviewer_syiK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938586485, "cdate": 1761938586485, "tmdate": 1762922799590, "mdate": 1762922799590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work analyzes text and audio language models in terms of the emergence of phonetic, syntactic and semantic representations using structural probes. Basic idea of the structural probe is to train a small model to quantify the distance of two elements, e.g., syntactic distance of two tokens quantified by the number of edges, and evaluate the prediction by the gold distance created by linguists.\nAnalyses show that:\n\n- Phonetic distance measured by the difference of articulation features is encoded in the audio models in earlier training stage.\n- Semantic distance measured by the number of edges in WordNet is also encoded in text models, but not in audio models.\n- Syntactic distance measured by the number of edges in Universal Dependency is encoded in both models.\n\nIn addition, the more training and the more parameters imply the better capacity for those linguistic features, and such emergence appear in the earlier stage of training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting analysis of text models, i.e., LLama2 and Pythia, and audio models, i.e., Wav2Vec, in terms of the linguistic features, i.e., phonetic, lexical semantics and syntactic features. The findings sound very natural to me.\n- Analyses are carried out systematically using carefully designed linguistic probes of employing distances defined over phonemes, syntax, lexical semantics."}, "weaknesses": {"value": "- No phoneme level analysis is performed by text models, e.g., Pythia. Given that the analyses for the lexical semantics and syntax are carried out on audio models, it is not clear why the similar analysis is not performed for text models.\n- Analyses are performed for three features, syntax, lexical semantics and phoneme, but their relations are not clear, i.e., whether they are correlated or not, although there exist some studies in Figure 5. For example, it is possible to analyze the relationship in terms of Pareto optimality [1]. It is also interesting to figure out what neurons are primarily triggered for particular features by analyzing the learned matrices, B, to find the relations of those linguistic features.\n\n[1] Jiannan Xiang, Huayang Li, Defu Lian, Guoping Huang, Taro Watanabe, Lemao Liu. Visualizing the Relationship Between Encoded Linguistic Information and Task Performance. EMNLP 2022 Findings."}, "questions": {"value": "- Some details are missing:\n  * Phonemic representations are immediately clear in section 2.2, but is it using the IPA sequences given the use of Montreal Forced Aligner?\n  * What is meant by 2-dimensional or 2D probes mentioned elsewhere? Do they mean the dimension of B, i.e., p?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cy86gAUpsR", "forum": "l3lneCvc0K", "replyto": "l3lneCvc0K", "signatures": ["ICLR.cc/2026/Conference/Submission11775/Reviewer_VF6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11775/Reviewer_VF6d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969856868, "cdate": 1761969856868, "tmdate": 1762922798807, "mdate": 1762922798807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has three main aims according to my reading: (1) applying the structural probe of Hewitt and Manning to linguistic structures outside of syntactic dependency parse trees, specifically to distances between English vowels based on their articulatory features and distances between English nouns based on WordNet graph; (2) analyzing whether these distances (including syntactic distance) are represented in language models trained on text or speech; and (3) if they are, what their developmental trajectories look like.\n\nThe main findings as claimed by the paper are as follows. Signatures of three levels of linguistic representation (phonemic, syntactic, lexical semantic) can be found in text and speech models. The signal strength for lexical semantic representation is weaker in speech models, and the signal present seems to derive mostly from phonemic subsequence overlap. When analyzing the training trajectory of the speech models, the transitional phase of metric score increases occurs in the order of phonemic - lexical semantic - syntactic scores.\n\nThere are definitely interesting sets of raw results in this work. But I think the framing and interpretation of the results are questionable, and sometimes even misleading, as I will discuss in the individual sections."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "There are definitely new empirical results that people in computational linguistics and cognitive science would be interested in. The research question itself is interesting, and so is looking at the learning trajectories (especially of speech models, which is done less frequently). The implementation of the experiments seemed overall reasonable."}, "weaknesses": {"value": "Overall, how the experiments were set up seemed fair to me as mentioned in the Strengths section (the quantitative findings reported are probably real findings), minus a few questionable parts that I will comment on below. However, the major soundness issue I see in this work is how the experimental results are interpreted and used to support the core claim made. The presentation and exposition also seemed weak, which I will also discuss below separately.\n\n**Soundness** \n- Specifically, I take issue with the interpretation that audio models \"partially instantiate\" lexical semantic structure - to me, to partially instantiate it, it must learn something interesting about it. The numeric difference between control and audio models as shown in Figure 3D seems to be quite small, and the paper itself seems to point out that audio models don't actually seem to be learning anything meaningful about lexical semantics other than the fact that certain labels for semantic categories share phonological subsequences (citing from the paper, the scores of the control models \"were in the same range as the semantic scores of models exposed to English speech\"). Then, I'm not sure how the current results can be interpreted as \"partial instantiation\" (L311: \"Together, these results demonstrate a clear and partial instantiation of lexical semantic structure by text and audio models, respectively.\") unless there were some qualitative evidence and statistical tests showing that there are interesting gains over control models. If there is nothing much substantive here, I don't know how the whole analysis and interpretation in Section 3.4 (\"Order of Acquisition\"), and very broad claims like \"Linguistic structures are acquired in sequential order\" are justifiable at all.\n- I think the trajectory result still can have a sensible interpretation; maybe having some grasp of phonology is a precondition to being able to realize that there are shared phonological subsequences between expressions. But this is certainly different from saying that lexical semantics emerges in a network after phonology and before syntax, which is one of the core claims of the paper (it's in the title!). I acknowledge that some flavor of this comment exists already in draft too (L420-), but my opinion is that even with this hedging, the current framing is an overinterpretation. And even more problematic to make this interpretation the title of the paper.\n- In general, the framing connecting the results to child language acquisition also seems to be misleading. For instance, if we take the paper's words that \"the developmental ordering between syntax and lexical semantics, a central question in cognitive science (Gleitman, 1990; Pinker, 1994), remains unresolved.\", then how can one justify statements like \"Like children, audio models appear to spontaneously build linguistic features, and this emergence follows a stereotypical sequence of phonemic, lexical and syntactic representations.\"? I think there are actual stakes in making uncareful analogies between human and LM results because it promotes proliferation of unqualified claims/beliefs like \"LLMs learn like human children do\" - I could totally see this paper being cited in this way when the actual picture painted by the experiments a lot more nuanced.  \n- Additionally, I see a few representativeness issues in the selection of the domain and method. These aren't problematic per se (the scope of things that one paper can achieve is limited), but becomes problematic when trying to use the results to claim something much more broad.\n\t- Representativeness of the selected evaluation domains: \"phonemic structure\" is only limited to articulatory features of vowels, and \"lexical semantic structure\" is only limited to nouns.\n\t- Maybe more major but this perhaps reflects my own bias - representativeness of reducing linguistic structural representations to distance. This obviously is true of the original method of H&M too, but I feel like the framing taken in this work about reducing structure to distance is celebratory, rather than viewing it as a limitation. Taking from the paper: \"Each level representation can be formalized as a metric system, which indicates e.g. whether different phonemes are more or less similar to one-another or whether different words in a sentence are more or less syntactically related. Formally, this prediction implies that language representations reduce down to a distance matrix.\" -> Two comments here: (1) is about the perhaps subtle distinction between representation being formalized as distance vs. formalization of linguistic representation in a way that lets you allow distance metrics to be defined between them. I am sympathetic to well thought out ways to achieve the latter but not the former. (2) is about the actual ways in which the latter idea is realized, which can be assessed in terms of their adequacy of capturing linguistic information that we care about. I think the paper's proposed representations and distance metrics miss out a lot, connecting back to representatitiveness issue I mentioned above.\n\n**(Minor) Methodological comment**\n- L199: \"Only the French large and base models outperform the tiny English model, which is expected given the model’s limited size. These controls prove that the instantiation of phoneme structure results from the emergence of speech-specific processes.\" I'm not sure if I am convinced. There should be a tiny French model in order to make this claim based on the current results.\n\n**Presentation**\n\nI found the exposition, discussion, and interpretations offered often confusing. Attribution to existing work could also be improved.\n\nExamples of confusing/missing exposition:\n- L257: \"This hierarchical organization across layers highlights that these semantic embeddings are not present in the audio or text tokens’ embedding used as input to these transformer models. Instead, several computational steps are performed to merge and position these tokens’ embedding together into a semantic subspace.\" This really needs a lot of unpacking. Why does the (inverse) U-shape indicate \"hierarchical organization\"? And by semantic embeddings, I'm guessing what is intended is (purported) lexical semantic information? Does the \"Instead...\" sentence offer anything more than just saying that there is layer-wise processing that contextualizes information with respect to other tokens in the input in a transformer? This is the type of discussion that popped up in many places where the logical flow doesn't sound quite right or the justification for the claim is not quite there.  \n- L316: \"Language models are known to code syntactic trees through distances (Hewitt & Manning, 2019), while only correlates of syntactic code have been found in speech models (Pasad et al., 2024b).\" I think I understand what is intended here but I find this kind of phrasing a bit bizarre because to me distances also seem like correlates of syntactic code. Although to be honest I'm not sure my understanding of the term \"syntactic code\" aligns with how it's used.\n- What is the method with which the trees are reconstructed from the distance metric? (e.g., in Fig 4) Minimum spanning tree as in Hewitt and Manning? I'm also curious why UUAS was dropped as a metric for syntactic trees if the tree reconstruction was done.\n\nExamples of missing attribution:\n- L206: \"This is considerably more than the 10^7 words that children experience in their first year on average.\" -> What is the source of this claim?\n- L214: \"Nevertheless, no analysis has shown the existence of a theoretically constructed semantic graph in a subspace of the model activity.\" -> First of all I'm not sure if I understand what exactly is in the set of \"a theoretically constructed semantic graph\". But there's plenty of representational studies based on information in semantic graphs and plenty on info from WordNet specifically (although they might not be looking at exactly the formulation of the problem looked at in this paper):\n\t- https://aclanthology.org/2023.findings-eacl.139\n\t- https://arxiv.org/abs/2402.16837\n\t- Maybe more importantly, I think Park et al. (2025) is currently cited after \"the activation patterns of self-supervised models represent simple semantic relationships\" but their analysis of hierarchical structure of words in WordNet in terms of shortest distance matrix between nouns seem exactly aligned in spirit with the lexical semantics part of this work. Surely there are differences but maybe this analysis at least merits a discussion?\n\nNitpicky stylistic comments:\n- I suggest using proper opening quotation marks in latex, all of them are currently closing quotation marks. \n- Figure 2D model identifiers are hard to map onto textual descriptions of the models for a reader who is not familiar with the identifiers. (e.g., I had to backtrace citations to figure out FMA is a music dataset, maybe relabeling them to match the descriptions in the text might help the readers)\n- L092: \"Each level representation\" -> \"Each level of representation\"\n- L118: made of text different text datasets -> made of different text datasets\n- L159: \"score models with known strong cognitive abilities\" seems like an unqualified claim. what does \"strong cognitive abilities\" mean? \n- L204: \"Fig. 2D shows a gradual rise\" -> 2E?\n- L256: \"For this 200-D probe, the semantic scores across model layers follow a U-shape in all models\" - Do you mean inverse U?"}, "questions": {"value": "- If there are any responses to the soundness issues I raised, I'd be happy to engage! The main claim didn't feel fully justifiable to me but I'd be happy to update my beliefs if there is good justification.\n- Is it problematic that the phonemic scores for the models trained with nonlinguistic data like music is very high? It being high for French models is understandable, since some vowel distinctions in English will be shared in French vowels, but it feels like a Spearman correlation of >0.5 is almost excessively high for a model trained on music or environmental sounds.\n- Given the discussion about linear trees throwing off the metrics on p7 (which I appreciated), maybe it would be good to put the correlation with linear tree directly on Fig 4D as another baseline, eg as a horizontal dotted line on the graph."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X1e5tUjXt1", "forum": "l3lneCvc0K", "replyto": "l3lneCvc0K", "signatures": ["ICLR.cc/2026/Conference/Submission11775/Reviewer_RCBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11775/Reviewer_RCBG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977896534, "cdate": 1761977896534, "tmdate": 1762922798303, "mdate": 1762922798303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper investigates whether semantic categories emerge in transformer-based text and audio models during training and make comparisons with WordNet structured semantic graph. Among other things, the authors evaluate when different semantic categories (e.g., \"mammal\", \"dog\") emerge in the models by probing representations at different training checkpoints. Authors do not observe a sequential emergance of semantic categories, but observe a sequential emergence across inguistic structures. The analysis is done across various models (Pythia, Llama) and includes both text and audio models.\n- The paper is well-structured but some parts are dense. \n- The assumption that linguistic structures are encoded in a geometric code via linear subspaces is a simplification, but it is supported by prior work showing that linear probes can extract meaningful syntactic and semantic structures. While the brain and neural networks likely use more complex mechanisms, this approach provides a tractable and interpretable way to study the emergence of linguistic representations during training. Thus, the methodology is sound but not new."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The papers is well furnished in terms of references to the litterature.\n- Construction of a paired text-audio benchmarks with a great amount of data is welcome.\n- Use of various models (Pythia, Llama2) and audio models (Wave2vec2)."}, "weaknesses": {"value": "- Assuming linear and distance matrices for language representations can be considered as too simple way nowadays to explore representations in NN.\n- The method uses knn classification to score how well categories are separated. The use of knn is simple and may miss fine details. \n- It is not clear if retrieving the geometry of phonemix articulation is an impressive result, given that 20 years old models (e.g. Oudeyer) where already able to represent such geometry with Self-Organising Maps.\n- The paper is well-structured but some parts are dense. Authors also make some shortcuts that don't always seemed justified. For instance, the \"double bump\" in fig 2C is rather flat in-between 0.5 and 0.8 relative depth, which is know to be the most expressive depth for various analysis such as brain encoding. Thus, this sentence don't seem justified: \"This demonstrates that instantiating\nthe phoneme structure is not performed trivially with random acoustic processing.\"\n- Only small random models where explored (<= 10^8 params), e.g. Fig 3D. Why? As they don't need training, it should be rather quick to test to see if some effect emerge at 10^10 parameters."}, "questions": {"value": "- I understand that authors want to find clear linear relations, but why other methods, even unsupervised method already estabished since 5 years (e.g. UMAP), where not used to explore relative distances in projected space?\n- Only small random models where explored (<= 10^8 params), e.g. Fig 3D. Why?\n- How do these results relate to in-context learning or prompting? Could such \"linear analyses\" performed in such context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OvK0g7gwnv", "forum": "l3lneCvc0K", "replyto": "l3lneCvc0K", "signatures": ["ICLR.cc/2026/Conference/Submission11775/Reviewer_4wAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11775/Reviewer_4wAq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027079268, "cdate": 1762027079268, "tmdate": 1762922797712, "mdate": 1762922797712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}