{"id": "TU6Zowg9TG", "number": 2765, "cdate": 1757242233111, "mdate": 1763360597878, "content": {"title": "TimeFK: Towards Time Series Forecasting via Treating LLMs as Fuzzy Key", "abstract": "Time series forecasting (TSF) aims to predict future values based on historical data. Recent advancements in large language models (LLMs), which integrate cross-modal information (time series data and textual prompts), have demonstrated remarkable performance in TSF tasks. However, significant gaps remain between LLM-based methods and deep learning approaches due to their inherent differences. To bridge this gap, we propose TimeFK, an innovative TSF framework that uses LLMs as ``fuzzy keys'' to activate forecasting capabilities. Specifically, we introduce a tri-branch multi-modal encoding scheme that combines numerical and linguistic representations: (1) a time series encoder generates precise but weak embeddings, (2) a statistical encoder captures robust yet entangled features, and (3) a background encoder learns dataset-related information that remains disentangled. The fusion of precise, robust, and disentangled representations improves prediction accuracy. To further mitigate noise from language prompts, we introduce a Gaussian fuzzy mapping mechanism that maps hidden representations from LLMs into a fuzzy set space, preserving semantic richness while reducing irrelevant noise. Additionally, we prevent entanglement by using fused cross-modal representations as keys and time series embeddings as values in a fuzzy-aware attention decoder, enabling query-based interactions for forecasting. Extensive experiments on seven real-world benchmark datasets demonstrate that TimeFK outperforms state-of-the-art methods, highlighting the effectiveness of integrating fuzzy reasoning with multi-modal time series analysis.", "tldr": "", "keywords": ["Time series forecasting", "large language models", "multi-modal", "Gaussian fuzzy mapping", "fuzzy-aware attention decoder"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/212cfcb34c7143e4c4e985a79f07647e4b6affdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper works on multivariate time series forecasting (TSF), aiming to utilize the well-learned knowledge of LLMs to improve the performance of TSF in a multi-modal manner. Specifically, this paper presents TimeFK, a TSF framework that leverages LLM as \"fuzzy keys\" to enhance the prediction capabilities. The main technical components comprise: (1) a tri-branch multi-modal encoder (time series, statistical, and background encodings), (2) a Gaussian fuzzy mapping mechanism to reduce prompt noise, and (3) a fuzzy-aware attention decoder to avoid representation entanglement. Extensive experiments have been conducted and the reported performance is good."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Extensive experiments.\n2. The reported performance is good."}, "weaknesses": {"value": "1. This paper appears to be an incremental method built upon TimeCMA, it's necessary to provide a detailed comparisons between TimeFK and TimeCMA, particularly their differences.\n2. The motivation of introducing Background Encoding Branch is unclear, as well as its role played in Cross-Modality Fusing module.\n3. In L053, authors claim TimeCMA-like method has an efficiency bottleneck, however, (1) TimeCMA only has 2 branches, while TimeFK has 3 branches; (2) there is no efficiency comparisons (but not memory comparisons provided in appendix) to demonstrate that TimeFK is efficient; (3) why not put Background Input in front of Statistical Input to be a single prompt, the former part is short and fixed, and we can also use KV Cache to avoid replicated calculations.\n4. The main motivation of treating LLMs as \"fuzzy keys\" is underdeveloped and lacks rigorous theoretical grounding, (1) the paper fails to clearly define what constitutes a \"fuzzy key\" in the context of TSF, i.e., how LLMs inherently align with fuzzy set theory remains unclear; (2) there are no clues to validate the claim that this component can not only reduce noise but also perform better cross-modality fusion that previous methods (like TimeCMA).\n5. The Fuzzy-aware Attention Decoder looks like a standard Transformer Decoder Block, without anyother design. In general, the novelty of this paper is quite limited.\n6. Regarding baselines, (1) TimeCMA is not the current state-of-the-arts, there are some stronger models like TimeXer; (2) in quantitative results, the reported values of TimeCMA is much worse than those reported in their paper, i.e., the comparison is unfair and invalid."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EIdKrHzs51", "forum": "TU6Zowg9TG", "replyto": "TU6Zowg9TG", "signatures": ["ICLR.cc/2026/Conference/Submission2765/Reviewer_4HqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2765/Reviewer_4HqS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295199666, "cdate": 1761295199666, "tmdate": 1762916365291, "mdate": 1762916365291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "T2IwAwT3su", "forum": "TU6Zowg9TG", "replyto": "TU6Zowg9TG", "signatures": ["ICLR.cc/2026/Conference/Submission2765/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2765/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763360596992, "cdate": 1763360596992, "tmdate": 1763360596992, "mdate": 1763360596992, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an important problem of time series forecasting based on large language models (LLMs). The authors design a tri-branch multi-modal encoding scheme with numerical and linguistic representations. In addition, a Gaussian fuzzy mapping mechanism is proposed to alleviate the noise of LLMs. Experiments show the effectiveness of the proposed method to some extent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. A tri-branch multi-modal encoder is proposed to learn comprehensive representations of time series.\n3. Experiments are conducted to show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Although the baseline comparison shows that TimeFK has better performance than existing methods. However, it is strange that the prediction of TimeFK does not follow the ground truth in Figure 8. Especially, DLinear, FreTS, and FEDformer show that their prediction can more accurately trace the ground truth. This raises a concern about whether TimeFK really works for time series forecasting.\n2. More recent baselines are required as follows.\n\n[1]. UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting, WWW 2024.\n\n[2]. One fits all: Power general time series analysis by pretrained LM, NeurIPS 2023.\n\n[3]. Sundial: A Family of Highly Capable Time Series Foundation Models, ICML 2025.\n\n[4]. Timer-XL: Long-Context Transformers for Unified Time Series Forecasting, ICLR 2025.\n\n[5]. DUET: Dual Clustering Enhanced Multivariate Time Series Forecasting, SIGKDD 2025.\n\n3. After carefully checking the baselines, the proposed TimeFK seems to be an incremental version of TimeCMA with an additional background encoding branch. Further, the prompt templates shown in Figure 3 are almost the same as those in TimeCMA. This implies insufficient technical improvement over existing papers.\n\n4. It would be better to include a case study to intuitively show the effectiveness of the proposed Gaussian fuzzy mapping mechanism as this is the major argument.\n\n5. The authors argue that existing LLM-based methods may have low inference speed. However, there are no experiments regarding efficiency (e.g., inference time and FLOPs) in the major component of the paper. Only the memory comparison in Table 15 is not enough to show the efficiency of the proposed method. It is also suggested to include the training time comparison. The efficiency experiments are suggested to be conducted on multiple datasets with different prediction lengths."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XfpIFX6H0l", "forum": "TU6Zowg9TG", "replyto": "TU6Zowg9TG", "signatures": ["ICLR.cc/2026/Conference/Submission2765/Reviewer_LV4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2765/Reviewer_LV4R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617419299, "cdate": 1761617419299, "tmdate": 1762916364987, "mdate": 1762916364987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To bridge the gap between LLM-based methods and deep learning approaches due to their inherent differences, this work propose TimeFK, an innovative TSF framework that uses LLMs as “fuzzy keys” to activate forecasting capabilities. A wealth of empirical evidence demonstrates the effectiveness of the method proposed in this work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research motivation is relatively clear. The understanding of the current existing research is profound.\n\n2. The proposed architecture is very interesting and can bring new insights to the field.\n\n3. The discussion of the experiment is sufficient and reasonable"}, "weaknesses": {"value": "1. Can other LLMs except GPT2 be selected as the control for the experiment?\n\n2. Whether the viewpoints presented in this work are transferable, such as being expanded in the prediction of spatio-temporal data?\n\n3. The discussion on training efficiency and prediction efficiency should be analyzed.\n\n4. Sec 2 should be organized more logically to demonstrate the differences between this paper and the existing works."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OwFUTHLQb0", "forum": "TU6Zowg9TG", "replyto": "TU6Zowg9TG", "signatures": ["ICLR.cc/2026/Conference/Submission2765/Reviewer_15sJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2765/Reviewer_15sJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802810625, "cdate": 1761802810625, "tmdate": 1762916364778, "mdate": 1762916364778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a new multimodal method for time series forecasting and evaluated it on 4 ETTs, Exchange, ILI, and Weather  benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed pipeline is easy to understand."}, "weaknesses": {"value": "### **Baselines and Related Work**\nFor the multimodal time series forecasting method, I suggest the author add a discussion and comparison with [1-4].\n\n\n[1] Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting\n\n[3] Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting\n\n[3] Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives\n\n\n### **Dataset**\nThe author only evaluated their method on ETT, Exchange, ILI, and Weather.  There is no short-term forecasting dataset, e.g., M4 and PEMS, and other commonly used datasets need to be evaluated.\n\n\n### **Wriring**\n\nThere are lots of paragraphs with GPT-style words."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Some sections are heavily GPT-style words. Additionally, I attempted to use a third-party tool to verify it, which indicated that it was 100% AI-generated writing. Although the tool is not fully conclusive, it serves as a reference.\n\nFor example,\n\n> 2 RELATED WORK\n> FEDformer (Zhou et al., 2022) incorporated a mixture-of-experts design to enhance trend and seasonal decomposition and\nproposed a sparse attention mechanism in the frequency domain to balance efficiency and accuracy.\nTimesNet (Wu et al., 2023) decomposed time series into periodic segments and modeled intra- and inter-period interactions by Inception blocks, facilitating generalized time series modeling. Crossformer (Zhang & Yan, 2023) captured dependencies across both temporal and variable dimensions using an attention mechanism. PatchTST (Nie et al., 2023) patched time series into subsequences, preserving local semantics and enabling long-range temporal modeling. DLinear (Zeng et al., 2023) introduced a simple one-layer linear model that achieved high accuracy through direct multistep prediction. TSMixer (Chen et al., 2023) extracted temporal and feature-wise information by stacking multilayer perceptrons (MLPs) across mixed time-feature dimensions. To address the limitations of pointwise mapping and the information bottleneck of MLP-based approaches, FreTS (Yi et al., 2023) applied MLPs in the frequency domain to improve global dependency modeling. iTransformer (Liu et al., 2024b) leveraged dimension inversion to enhance long-sequence handling, mitigating performance degradation and reducing computational overhead.\n\n> Tab. 1\nI also read Tab.1, which is very similar to GPT-style."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dmegX4Tya7", "forum": "TU6Zowg9TG", "replyto": "TU6Zowg9TG", "signatures": ["ICLR.cc/2026/Conference/Submission2765/Reviewer_c9WE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2765/Reviewer_c9WE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024283389, "cdate": 1762024283389, "tmdate": 1762916364626, "mdate": 1762916364626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}