{"id": "ReGfabQufK", "number": 15735, "cdate": 1758254619543, "mdate": 1759897285495, "content": {"title": "A New Perspective on Large Language Model Safety: From Alignment to Information Control", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nacross a wide range of domains, yet their increasing deployment in sensitive\nand high-stakes environments exposes profound safety risks—most notably, the\nuncontrolled generation of inappropriate content and the inadvertent leakage of\nconfidential information. Traditionally, such risks have been approached through\nthe lens of alignment, focusing narrowly on ensuring outputs conform to general\nnotions of helpfulness, honesty, and harmlessness. In this work, we argue that\nsuch alignment-centric perspectives are fundamentally limited: information itself\nis not inherently harmful, but its appropriateness is deeply context-dependent.\nWe therefore propose a paradigm shift in LLM safety—from alignment to information control. Rather than merely shaping model behavior through the existing\npractice of alignment, we advocate for the principled regulation of who can access\nwhat information under which circumstances. We introduce a novel framework\nfor context-sensitive information governance in LLMs, grounded in classical secu-\nrity principles such as authentication, role-based access control, and contextual\nauthorization. Our approach leverages both the internal knowledge represen-\ntations of LLMs and external identity infrastructure to enable fine-grained,\ndynamic control over information exposure.\n\nWe systematically evaluate our framework using recent models and a suite of\nbenchmark datasets spanning multiple application domains. Our results demon-\nstrate the feasibility and effectiveness of information-centric control in mitigating\ninappropriate disclosure, providing a robust foundation for safer and more\naccountable language model deployment. This work opens a new frontier in LLM safety, one rooted not in abstract alignment ideals, but in enforceable,\ncontext-aware control of information flow.", "tldr": "Using Information Control Framework to Secure LLM applications", "keywords": ["Information Control", "Information Security", "AI Governance", "LLM Application", "LLM safety", "LLM alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d334584120cc6df744e0c316ec7e411798f0760.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed an information control framework for large language models, shifting from alignment-based safety to context-aware information flow management. To enhance security, the framework integrates user identification, policy alignment, role-based access control, and post-processing modules. A benchmark dataset was constructed based on real enterprise scenarios, and experiments demonstrated that the proposed method significantly improves defense against unauthorized information leakage while maintaining high correctness and acceptable latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Unlike existing approaches that primarily focus on content filtering or adversarial training, this research reframes LLM security at the architectural level by proposing \"information control\" as a new paradigm that offers superior engineering feasibility and organizational adaptability. The technical solution demonstrates rigorous design with clear module segmentation and logical consistency. The inclusion of Shapley value analysis and module combination testing further strengthens the credibility of the conclusions. As LLMs become increasingly integrated into core enterprise workflows, the need for fine-grained, controllable access to sensitive information has become increasingly urgent. This solution that offers significant reference value for both subsequent research and real-world deployment."}, "weaknesses": {"value": "1. Compared to classical rule-based access control systems, the semantic approach of the PA module may introduce new uncertainties and attack surfaces.\n\n2. While the PP module helps prevent information leakage, it may lead to erroneously removing non-sensitive but semantically similar content,  compromising output usability. Although the paper mentions \"configurable sensitivity,\" it does not quantify the usability-security trade-off under different configurations."}, "questions": {"value": "1. The PA module is described as using \"LLM-based reasoning to semantically interpret prompts.\" Is this module itself also an LLM? How does it ensure robustness against the same jailbreaking attacks it aims to prevent? Is its decision-making process auditable?\n\n2. When the PP module incorrectly redacts critical business terminology, how can users obtain actionable feedback?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0odAd0kXIH", "forum": "ReGfabQufK", "replyto": "ReGfabQufK", "signatures": ["ICLR.cc/2026/Conference/Submission15735/Reviewer_axdv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15735/Reviewer_axdv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878783723, "cdate": 1761878783723, "tmdate": 1762925973717, "mdate": 1762925973717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a framework that shifts the paradigm of LLM safety mechanisms from reshaping alignment to information/access control. Rather than merely shaping model behavior through the existing practice of alignment, they examined the traditional access control-based technique in LLMs to prevent information leakage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a clear and practical framework that controls who can access what information in an LLM, making it much safer for real-world use in companies handling sensitive data.\n2.\tThe proposed system improves protection against data leaks and harmful outputs while still keeping good accuracy and only a small delay in response time."}, "weaknesses": {"value": "1.\tThe description of the modules of Figure 1 lacks a complete picture of the methods on how each component works. For example, the user identification module checks if the user is authenticated to have such information. How does this module perform this operation? A detailed methodology is highly required to make it clear. \n2.\tHow would the authors justify the use of GPT-4o as a judge since the same model is used as the target model for evaluation?\n3.\tWhy did the authors only evaluate on those specific three models in the paper, where numerous open-sourced and closed-sourced models are available? What justifies the selection of the specifically 2 OpenAI models and one Google model?\n4.\tAlso, what's the performance of the proposed defense on reasoning-focused models and mixture-of-expert models?\n5.\tThe paper should also test the defense performance against SOTA attacks to understand the proposed defense method’s utility and compare the performance with the other defense techniques under the same attacks.\n6.\tWhat are potential failure cases of the proposed method?\n7.\tThis paper severely lacks a performance comparison with the baseline techniques for LLM defense."}, "questions": {"value": "Please follow the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UWkdNSNhOr", "forum": "ReGfabQufK", "replyto": "ReGfabQufK", "signatures": ["ICLR.cc/2026/Conference/Submission15735/Reviewer_M585"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15735/Reviewer_M585"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939882941, "cdate": 1761939882941, "tmdate": 1762925973247, "mdate": 1762925973247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that traditional alignment-based safety frameworks (e.g., RLHF, Constitutional AI) are insufficient because they treat all content generation as equally harmful or safe, ignoring who requests the information and in what context.\nThe authors propose a paradigm shift from alignment to information control, introducing a modular framework inspired by classical access control principles (authentication, RBAC, and contextual authorization).\nThe framework integrates four modules (user identification, policy alignment, role checking,  and post processing) to regulate LLM outputs based on identity, role, and context.\nExperiments on GPT-4o and open-source models show that the combined modules can reach 97.7% defense rate while maintaining high correctness and acceptable latency, outperforming baseline alignment-only and static-control setups."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides an original and well-argued conceptual shift: viewing LLM safety as information governance rather than behavioral alignment. This perspective could influence future safety frameworks.\n2. The empirical results include comparisons across different module combinations, ablation studies, latency analysis, and both closed- and open-source models."}, "weaknesses": {"value": "1. Although framed as a new paradigm, many components (policy filtering, role-based rules, post-filtering) resemble structured prompt-engineering pipelines rather than a fundamentally new safety algorithm, which raises concerns about how different this really is from prompt conditioning or retrieval gating.\n2. While latency and correctness are measured, the paper does not analyze usability degradation, false denials, or context misclassification, which are crucial for deployability.\n3. The comparison with alignment-based safety methods is shallow. The paper positions itself as a paradigm shift but doesn't empirically demonstrate where alignment fails and information control succeeds. More comprehensive comparison should be made (such as comparison with other alignment techniques, and representative jailbreak defense methods)."}, "questions": {"value": "1. Can this framework be combined with alignment methods to get better results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ARLFYemiAy", "forum": "ReGfabQufK", "replyto": "ReGfabQufK", "signatures": ["ICLR.cc/2026/Conference/Submission15735/Reviewer_FuvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15735/Reviewer_FuvH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971035775, "cdate": 1761971035775, "tmdate": 1762925972827, "mdate": 1762925972827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}