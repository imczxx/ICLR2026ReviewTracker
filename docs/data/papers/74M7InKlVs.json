{"id": "74M7InKlVs", "number": 103, "cdate": 1756728784779, "mdate": 1759898275688, "content": {"title": "C$^3$-Bench: Evaluating and Achieving Controllable Code Completion in Code LLM", "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion\\textemdash a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, \\textbf{\\underline{C}}ontrollable \\textbf{\\underline{C}}ode \\textbf{\\underline{C}}ompletion Benchmark (C$^3$-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C$^3$-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C$^3$, achieves state-of-the-art performance on C$^3$-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "tldr": "We created C³-Bench, a new benchmark for code LLMs that tests both code correctness and instruction following, revealing gaps in current models and developed a better-performing solution through automated training data generation.", "keywords": ["Large Language Models", "Code Language Models", "Code Completion", "Instruction Following"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2365463e7c923ffa6529d3000c4c06c547b44ea5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces C$^{3}$-Bench, a benchmark designed to evaluate the instruction-following capabilities of Large Language Models in code completion tasks. C$^{3}$-Bench encompasses a diverse range of code completion scenarios and instructions. The evaluation results reveal a substantial gap in instruction-following abilities between open-source and advanced proprietary models. Furthermore, the authors propose a data synthesis method to generate data aimed at enhancing model performance on C$^{3}$-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated. It aims to evaluate the instruction-following capabilities of LLMs in code completion tasks, which is a valuable and timely research topic.\n\n2. The paper is well-structured and comprehensive. The overall workflow, which includes problem definition, benchmark construction, and a proposed solution, is complete and presented with clarity.\n\n3. The consistency observed between the results from C$^{3}$-Bench and exsiting benchmarks like Copilot Arena is impressive."}, "weaknesses": {"value": "1. The paper's core contribution lies in the evaluation of instruction-following capabilities. However, a primary concern is that the evaluation uses a single model (Claude 3.5). While the authors have validated Claude's judgments against human annotations for consistency, the use of an LLM-as-a-judge for such a subjective task can inevitably introduce bias [1], particularly for those ambiguous model-generated responses. Is it possible to incorporate more controllable, quantitative metrics for the evaluation? Alternatively, at a minimum, could the authors provide results using other LLMs as judges to demonstrate the consistency and robustness of the evaluation results?\n\n2. Following up on the previous point, the performance of the fine-tuned Qwen2.5-Coder-C$^{3}$ on the ICC task is understandably constrained by the capabilities of its base model, making the modest improvement in pass@1 seem reasonable. However, the remarkably significant increase in the IF score raises a critical question: Does this suggest that the IF score can be easily somehow hacked? Specifically, could using the same model that generated the benchmark data as the judge introduce a systemic bias, potentially rewarding models that mimic the judge's stylistic preferences?\n\n3. The scope of C$^{3}$-Bench is currently limited to in-file tasks in Python. The impact of this work could be significantly enhanced by extending the benchmark to support repository-level contexts, and to encompass multiple programming languages.\n\n[1] Wei H, He S, Xia T, et al. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates[J]. arXiv preprint arXiv:2408.13006, 2024."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SjAQCZMkQO", "forum": "74M7InKlVs", "replyto": "74M7InKlVs", "signatures": ["ICLR.cc/2026/Conference/Submission103/Reviewer_LELU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission103/Reviewer_LELU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958154868, "cdate": 1760958154868, "tmdate": 1762915451691, "mdate": 1762915451691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The premise of this paper is that well known benchmarks for programming tasks\ndo not adequately test the \"instruction following\" capabilities of LLMs, where\n\"instruction following\" in this paper means testing non-functional requirements\nin the prompt. Non-functional requirements are anything that cannot be\ntested using simple test cases of the form \"assert pred(f(x))\" where f is the\nsynthesized program, x is a test input, and pred some predicate of the output.\n\nThe paper presents a synthetic dataset of programming tasks, derived from\nHumanEval and SAFIM, that test models' non-functional requirements.\nIn addition, the paper uses the same benchmark generation pipeline to generate\na SFT dataset. The paper decontaminates the SFT dataset and uses it to fine-tune\na Qwen Coder model, which performs very well on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Important problem with a large evaluation."}, "weaknesses": {"value": "At this point, there are probably hundreds of benchmarks that evaluate the\ncoding abilities of LLMs in different ways. Although the exact prompt format\nthat this paper uses may be new (though just a prompt format is not a\ncontribution), there are plenty of other papers that test the ability of models\nto complete code in context with instructions that have non-functional requirements.\nFrom my memory, here are a few:\n\n- NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness\n   https://openreview.net/forum?id=h5umhm6mzj\n\n  NoFunEval is HumanEval-derived, similar to some of the benchmarks in\n  the current submission.\n\n-  Can Large Language Models Write Parallel Code?\n   https://dl.acm.org/doi/10.1145/3625549.3658689\n\n   ParEval goes beyond functional correctness, which matters for parallel code\n   I recall it has some other metrics that go beyond pass@1. There is a lot\n   of follow up work from the ParEval authors.\n\n- D3: https://openreview.net/forum?id=Ksq7fgagId\n\n  This is a dataset, perhaps not a benchmark. But, the contribution overlaps.\n\nOverall, I think the contribution of this paper is very small. This is particularly\nthe case since the benchmark is partially synthetic, with portions of prompts\ngenerated by Claude 3.5 Sonnet.\n\nSome further notes on the writing:\n\n- I wish the main body of the paper gave the reader some examples of the\n  kinds of problems in the benchmark. I think the only example is in the appendix\n  (Figure 7). Moreover, I don't find that example very compelling. \n\n- L160: \"Multi-line completion mandates the generation of a predetermined number of\n  complete code lines\" It seems peculiar to require a solution in X lines of\n  code.\n\n- L202: \"The original HumanEval [..] datasets primarily contain single-line implementations\n  of ground truth middle code.\" Is this really true of HumanEval? Just\n  scanning the canonical solution column here:\n\n  https://huggingface.co/datasets/openai/openai_humaneval\n\n  There are a number of solutions that begin with \"return ...\", but most of\n  them begin with loops, variable definitions, if statements, etc.\n\nFinally, the prompt format that the paper uses seems peculiar. It is possible\nthat scores will go up significantly by picking a format that is more in\ndistribution. E.g., just having a single code block with \"### INSERT CODE HERE ###\"\nto fill in."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TyJG7ljWlB", "forum": "74M7InKlVs", "replyto": "74M7InKlVs", "signatures": ["ICLR.cc/2026/Conference/Submission103/Reviewer_R1yQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission103/Reviewer_R1yQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506054150, "cdate": 1761506054150, "tmdate": 1762915451542, "mdate": 1762915451542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes C3-Bench, a new benchmark for evaluating large language models on controllable code completion, focusing on their ability to follow detailed implementation instructions—an aspect overlooked by existing benchmarks like HumanEval and CrossCodeEval.\nC3-Bench defines two tasks: Implementation-Control Completion (ICC) and Scale-Control Completion (SCC), covering functional, structural, and size constraints. The benchmark contains 2,195 Python tasks, measuring both functional correctness and instruction adherence.\nThe authors also introduce a data synthesis pipeline using Qwen2.5-Coder, and fine-tune it to create Qwen2.5-Coder-C3, achieving state-of-the-art results on C3-Bench. Experiments on over 40 LLMs (open and commercial) show that current benchmarks overestimate model capability, and C3-Bench correlates better with real-world developer evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies instruction-following as a crucial yet underexplored aspect of real-world code completion. \n\nThe division into ICC and SCC tasks provides comprehensive coverage of different types of control in code generation, going beyond traditional correctness-based metrics.\n\nThe authors detail a hybrid process (AST extraction, variant generation, instruction synthesis, filtering) that ensures data diversity and precision.\n\nEvaluation across 40+ models and multiple benchmarks provides a rich empirical analysis, showing that C3-Bench aligns well with practical code-assistance performance."}, "weaknesses": {"value": "The benchmark focuses solely on Python and single-file completions, which underrepresents real-world multi-language, multi-file development complexity.\n\nQwen2.5-Coder-C3’s gain mainly comes from instruction-tuning rather than architectural advances; ICC performance still saturates at base-model limits.\n\nEven with consistency checks, using an LLM as the primary instruction-following evaluator may introduce subtle bias or inconsistency compared to human judgment."}, "questions": {"value": "Do the authors plan to extend C3-Bench to multi-language or multi-file tasks (e.g., C++, JavaScript)?\n\nHow is prompt sensitivity of the LLM judge controlled?\nHave the authors evaluated inter-judge consistency (e.g., Claude vs. GPT-based judges)?\n\nAre there notable differences between ICC and SCC results across models (e.g., algorithmic control vs. scale control)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bpaSktTCat", "forum": "74M7InKlVs", "replyto": "74M7InKlVs", "signatures": ["ICLR.cc/2026/Conference/Submission103/Reviewer_eyK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission103/Reviewer_eyK1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917560343, "cdate": 1761917560343, "tmdate": 1762915451427, "mdate": 1762915451427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel benchmark to evaluate the instruction-following capabilities of LLMs in code completion tasks. Specifically, the benchmark adds specific requirements to the prompts of code competition tasks, and evaluates how well LLMs follow these instructions using LLM-as-a-Judge and rule-based methods, beyond functional correctness tests. Furthermore, this paper proposes a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper explores an important and interesting research direction.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- Extracting the middle code by selecting nodes from the AST tree is not novel. SAFIM has explored syntax-aware completion within code’s AST including algorithmic blocks that targets at multi-line completion and serve as the major part of SAFIM, which however fails to get acknowledged in Section 2.3.1.\n- The benchmark focuses on function-level code completion, which however is different from real-world scenarios where code completion always happens in the large repositories with cross-file context. It’s also important to study whether the instruction-following capabilities of LLMs will change in repository-level code completion tasks."}, "questions": {"value": "None beyond the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WtOaYEy8uB", "forum": "74M7InKlVs", "replyto": "74M7InKlVs", "signatures": ["ICLR.cc/2026/Conference/Submission103/Reviewer_bRxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission103/Reviewer_bRxr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987078578, "cdate": 1761987078578, "tmdate": 1762915451272, "mdate": 1762915451272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce $C^3$, an instruction-guide code completion dataset, that is designed to address a fundamental limitation of existing  code completion benchmarks which only look for functional correctness. The paper also discusses results across 40 LLMs on this benchmark exposing gaps in instruction following capabilities of closed vs open models. The authors curated high quality SFT data which is used to train Qwen2.5-Coder-$C^3$ model, resulting in SoTA results on $C^3$ dataset. Interestingly, it is also shown that the performance on this dataset, correlates with results from Copilot arena, demonstrating its practical significance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- LLM assisted coding in real world applications need to adhere to user instructions. However, most of the publicly available datasets just account for functional correctness and do not measure the instruction following (IF) ability. This paper introduced $C^3$ dataset, which is a novel and important contribution to the field. \n- The authors conducted extensive studies across different models, sizes, exposing gaps in the IF abilities of these models, thereby providing guidance to the open source community to work on improving this capability.\n- Through SFT data curation and training  Qwen2.5-Coder-$C^3$, they achieved SoTA on $C^3$ bench, demonstrating the importance of high quality and relevant dataset to improve IF capability."}, "weaknesses": {"value": "- For Semantic Validation for ICC, the authors used LLM-based judging system with Claude3.5-Sonnet. They mentioned that this has 98% agreement with senior Python developers across 10 independent assessment rounds. However, the detail on whether this has been done across all the examples in ICC or a subset of them is unspecified. \n- As acknowledged by the authors, the dataset only comprises of in-file python tasks, limiting the scope of usage for this dataset."}, "questions": {"value": "- Can you share more details on how the agreement of 98% between LLM judge and human developers is achieved?\n- Why is this paper (https://www.arxiv.org/pdf/2507.22462) not referenced anywhere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rFxFyL2fy5", "forum": "74M7InKlVs", "replyto": "74M7InKlVs", "signatures": ["ICLR.cc/2026/Conference/Submission103/Reviewer_9mu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission103/Reviewer_9mu6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027292643, "cdate": 1762027292643, "tmdate": 1762915451059, "mdate": 1762915451059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}