{"id": "61OBn2aI04", "number": 7993, "cdate": 1758049848293, "mdate": 1759897816699, "content": {"title": "Implicit State Estimation via Video Replanning", "abstract": "Video-based representations have gained prominence in planning and decision-making due to their ability to encode rich spatiotemporal dynamics and geometric relationships. These representations enable flexible and generalizable solutions for complex tasks such as object manipulation and navigation. However, existing video planning frameworks often struggle to adapt to failures at interaction time due to their inability to reason about uncertainties in partially observed environments. To overcome these limitations, we introduce a novel framework that integrates interaction-time data into the planning process. Our approach updates model parameters online and filters out previously failed plans during generation. This enables implicit state estimation, allowing the system to adapt dynamically without explicitly modeling unknown state variables. We evaluate our framework through extensive experiments on a new simulated manipulation benchmark, demonstrating its ability to improve replanning performance and advance the field of video-based decision-making.", "tldr": "A video-based planning framework that adapts online using interaction feedback, updates parameters, and rejects failed plans—enabling implicit state estimation and improved replanning in uncertain manipulation tasks.", "keywords": ["Video Planning", "State Estimation", "Diffusion Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0aa268d240851936b5a17dc5ccacb2d60a89193b.pdf", "supplementary_material": "/attachment/31d9982a1d69272a198f526ef4bbaa1ab04d7481.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel method for trajectory planning via video generation. The idea is to use a diffusion video generation model to generate a video of successful task completion given an image of a state and a goal. In difference to prior methods, the key innovation of this paper is to introduce a replanning stage that updates a state latent based on prior experiences and an offline data buffer. The results show that replanning steps can be reduced by using these buffers, as they enable to select those generated trajectories that are unlikely to fail. The evaluation compares to some BC and RL baselines as well as a prior video prediction method which seems to have inspired this work."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- I like the brick sliding task. It seems like its a bit more challenging than the other tasks. I wonder how the demonstrations shape the performance on this task.\n- The evaluation is sufficient in terms of number of baselines and tasks I think. On top, the work performs real-world experiments which is always nice to see.\n- The introduction is well written."}, "weaknesses": {"value": "# Method\n- The method uses video as an action representation, similar to other recent works that use pixel-based representations. However, in the presented form those are limited to quasi-static manipulations if I look at the experiments (apart from brick sliding maybe?). I would mention that this representation is not without limitations despite its merits.\n- I think claiming that you introduce a novel benchmark is a slight overstatement, because you essentially seem to modify some tasks from metaworld. You can just say you evaluate your method.\n\n# Wording:\n- The writing is pretty sloppy in places. I think individual typos are not an issue which is why I listed them under minor, but in their sum, there are quite a lot of them. Especially the related work section has quite a few typos and mistakes that must be fixed.\n\n# Evaluation\n- How do you collect the offline interactions? How robust is the system to the choice of demos here?\n- It seems like the method gets access to an offline dataset, which could be seen as access to additional trials. How would the performance compare to the baselines if you only provide access to the actual previously failed interactions until M=14?\n- I am not sure how meaningful the results are in Table 3. Shouldnt the plans be semantically similar to the GT plans and not visually? Especially SSIM can be quite prone to some minor details here. What about clip embedding similarities or something like that? I like the general idea of the experiment, but I am not sure if visual similarity is the right metric.\n- I think having some other baselines that test video retrieval methods might be interesting, but it is not a big weakness.\n\n# Clarity\n- You distinguish between plans and interactions without rigorously defining the two concepts. To me it is not completely clear what sets them apart. Can you explain this further? This would improve the clarity for the reader in my eyes.\n- In the abstract and title you state that your method is \"implicit\" as it does not directly model the unknown state variables. Yet you state in Section 4 that you encode \"a hypothesis of the hidden parameters\" and that you \"estimate[...] hidden parameters from failed interactions\". I think I follow you in saying that your method does not model hidden parameters in a way as Memmel et al (2024) for instance, but then I would reformulate the above sentences in Section 4 to improve clarity on this. \n- I am not 100% sure if I understand the refinement procedure of the method \"Ours (Refine)\". What do you mean by refining from scratch? This part of the paper should be clarified slightly in wording I think.\n- The related work is a bit off in my eyes. Even if you compare to offline RL, I think your method really does not fall into that category. Instead I think you might be better off at discussing other recent works that use video retrieval for few-shot adaptation, eg [1, 2].\n- Table 5 us difficult to understand and should be improved. The same holds for Table 6.\n\n# Minor\n- \"people have explored using video to train dynamics model\" in line ~84. --> should be plural: \"models\", same for reward models.\n- \" to guide downstream policy\" in line 88 --> \" to guide a downstream policy\"\n- \" Fisher Information Matrix\" in line ~92 should be \" Fisher information matrix\"\n- \" The paradigm of offline RL has existed for dacades\" in line ~111 should be \"decades\".\n- \"In addition, with the development of large language model and transformer, there are also some research about using transformer for reinforcement learning\" in line 114. This sentence needs reformulation. Should be LLMs and transformers in plural and there \"is\" research.\n- \"our framework do not assume\" in line 116 --> \"does not assume\"\n- \"RL methods\" in line ~76 --> introduce abbreviation even if its clear in this context.\n- \"We consider a setting where certain system parameters θ, such as mass, friction, or utility\" in line ~120. Utility is not a system but rather a task parameter I would say.\n- \" We complement our focus on plan evaluation by using a fixed action prediction module to convert videos into actions for execution.\" I am not sure if \"complement\" is the right word here?\n- \" you often don’t know its exact weight\" --> \" you often do not know its exact weight\"\n- Sometimes you italicize the dataset variable D and sometimes you use \\mathcal{D}. Be consistent here.\n- \" where the Policy takes in state and state embedding directly\" in line 246. Dont capitalize policy here\n- Line 422 has a double dot at the end.\n- Line 391 \"Aciton\" -> \"Action\"\n\n---\n# Sources\n[1] Haldar, Siddhant, et al. \"Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations.\" _Robotics: Science and Systems_. 2023.\n[2] Memmel, Marius, et al. \"STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning.\" _The Thirteenth International Conference on Learning Representations_. 2024."}, "questions": {"value": "- Could the method also work using only its own failed attempts and no extra offline dataset?\n- How would your selection mechanism compare to selecting the action thats closest to the best succeding demo instead of picking the one furthest away from failure?\n- Does you method suffer from compounding errors under long action sequences? How long can you make those sequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2LKElImfAG", "forum": "61OBn2aI04", "replyto": "61OBn2aI04", "signatures": ["ICLR.cc/2026/Conference/Submission7993/Reviewer_1Eqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7993/Reviewer_1Eqz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761062719442, "cdate": 1761062719442, "tmdate": 1762919998058, "mdate": 1762919998058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a novel video planning framework that enables robots to adapt to uncertain environments by learning from past failures. Instead of explicit system identification, the proposed method performs implicit state estimation by integrating interaction-time data to update model parameters and filter out unsuccessful strategies.\n\nThe proposed system uses a retrieval mechanism to refine its understanding of hidden parameters (e.g., mass, friction) and a rejection strategy to avoid repeating failed plans. This allows for rapid online adaptation without prior knowledge of the environment's dynamics.\n\nThe proposed framework has been validated only on two scenarios: a simulated benchmark and a real-world door-opening task. The results show that in the two considered scenarios, it significantly reduces replanning attempts and outperforms state-of-the-art video planning and reinforcement learning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the interesting problem of the inability to adapt to failures and reason about uncertainty during interaction. It provides a framework that leverages past interaction videos and a simple rejection mechanism to guide future plans, mimicking human trial-and-error, while other approaches use complex explicit belief models.\n\nThe paper provides a thorough experimental evaluation, but only in two scenarios: their new simulation benchmark (Meta-World System Identification) and the real problem of opening a closed door. The experiments show clear quantitative improvements over strong baselines (AVDC, BC, CQL) in these two scenarios in reducing replans."}, "weaknesses": {"value": "The framework attributes all failures to planning errors, assuming a near-perfect action module that can always execute the generated video plan. This omits real-world execution noise and control uncertainties, which could cause failures regardless of plan quality.\n\nThe method relies solely on vision. It may struggle with tasks where the crucial state parameter is visually ambiguous (e.g., two objects with identical appearance but different masses) or requires other sensory modalities like touch or force-feedback.\n\nWhile still practical, the full method (with retrieval and generating multiple candidate plans) is slower than the baseline AVDC (~5 sec vs ~2.3 sec per replan). The refinement process adds significant latency (~16.5 sec), making it less suitable for real-time tasks.\n\nThe video generator can produce overly deterministic plans conditioned heavily on the initial observation, especially with limited data. While mitigated with noise injection, it remains a limitation that needs further analysis.\n\nThe retrieval-based state estimation requires storing and accessing the entire experience dataset at test time, which may not be scalable or efficient for long-term deployment compared to a fully learned belief state. No proper discussion on this problem has been conducted and analysed.\n\nLast, the notion of planning used in the paper is not the classical one used, for instance, in the AI Planning community, and it seems more related to motion planning. The authors shall better characterize their notion of planning, and the relation that there may exist with POMDP planning (i.e. synthesize a policy to achieve a goal in a partially observable Markov decision problem).\n\nThe additional material, consisting of just a link to another web page, is not informative enough to enable an individual to reproduce the paper's results."}, "questions": {"value": "The questions are to comment on the 7 weak points identified in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qT1fUTxK8i", "forum": "61OBn2aI04", "replyto": "61OBn2aI04", "signatures": ["ICLR.cc/2026/Conference/Submission7993/Reviewer_DZm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7993/Reviewer_DZm1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206186195, "cdate": 1761206186195, "tmdate": 1762919997626, "mdate": 1762919997626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Implicit State Estimation via Video Replanning, a framework that enables robots to adapt plans during execution by leveraging failed interactions without explicitly modeling system parameters. The system maintains buffers of failed plans and interactions, updates latent state embeddings online, and uses a video diffusion model to generate improved future plans. A novel Meta-World System Identification Benchmark is introduced to evaluate adaptation across tasks with hidden physical parameters. Experiments in simulation and real-world setups demonstrate improved replanning efficiency and robustness compared to video planning, imitation learning, and offline RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "strengths \n\n1: Novel implicit adaptation mechanism that integrates past failures into video-based planning without requiring explicit parameter estimation or belief models.\n\n2: Comprehensive evaluation, including a new benchmark, ablations, and real-world experiments, showing strong performance and practical feasibility.\n\n3: Modular and general framework that enhances existing video planning pipelines and leverages diffusion models for flexible plan generation."}, "weaknesses": {"value": "Weaknesses\n\n1: Computational overhead: Online embedding refinement and multiple video plan generations increase inference cost, limiting scalability in real-time or resource-constrained settings.\n\n2: Limited theoretical analysis: While effective empirically, the method lacks formal guarantees on convergence, stability, or conditions under which implicit state estimation succeeds.\n\n3: Dependence on visual similarity and pixel-space distances for plan rejection may struggle in cluttered scenes or visually ambiguous scenarios."}, "questions": {"value": "1: Scalability of Implicit State Embedding:\nHow does the proposed framework scale when applied to more complex environments with higher-dimensional hidden state variables or long-horizon tasks? Is there any evidence that implicit state embeddings remain reliable as task complexity increases?\n\n2: Generalizability Across Objects and Tasks:\nThe method relies on retrieval from past interaction data grouped by object ID, how well does the approach generalize to entirely new objects or tasks without prior failure data? Would the system fail or fall back to vanilla video planning in such cases?\n\n3: Efficiency vs. Computational Cost:\nGiven that the framework involves repeated video generation, embedding refinement, and rejection-based search, what is the real-time inference cost? Is the method practical for deployment in time-sensitive robotic applications, and how does the latency compare to conventional planners or belief-state methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CxRl53SQOt", "forum": "61OBn2aI04", "replyto": "61OBn2aI04", "signatures": ["ICLR.cc/2026/Conference/Submission7993/Reviewer_1k5g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7993/Reviewer_1k5g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531913442, "cdate": 1761531913442, "tmdate": 1762919997277, "mdate": 1762919997277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary\n\nThe paper introduces a new framework for adaptive video-based decision-making in robotics. The system uses an adaptive video planning framework, wherein a video generation model attempts to generate candidate future rollouts conditioned on the current scene and a hypothesized latent state embedding, which are then converted into actions.  The key innovation is that the planner can adapt its behavior base on previous failed trials.  When a robot fails during task execution (e.g., pushing instead of pulling a door), the failure itself contains valuable information about hidden environment parameters—like friction, mass, or interaction mode.  The planner uses two forms of adaptation:  \n- implicit state estimation (ISE): The system refines its internal latent “state embedding” by minimizing the discrepancy between its generated rollouts and the observed interaction.  This allows it to make more realistic video generations for the next trial.\n- Rejection-based replanning: at each planning round, the Video Plan Generator generates N candidate plans, covering a range of possible outcomes.  To prevent the agent from repeatedly attempting failed plans, the plan that has the maximum L2 pixel distance from previous failed plans is selected."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem formulation is clear and well-motivated.  The problem is important to the field.  I especially liked the example given in the introduction of opening a door without knowing whether it should be pushed or pulled.  This made it really clear what the conceptual challenge was.\n- The approach is interesting.\n- The experimental domains seem reasonably interesting/challenging.\n- The paper proposes a new benchmark (Meta-World System Identification Benchmark) with 5 manipulation-style tasks where key parameters (e.g. center of mass, friction, push vs pull mode) are hidden and must be inferred through interaction, which feels like a good testbed for this setting.\n- Empirically, the method outperforms AVDC, BC, and offline RL (CQL) in terms of ‘replans needed until success,’ suggesting that leveraging failed attempts is actually useful in practice."}, "weaknesses": {"value": "There were many parts of the paper that I found lacked sufficient detail.  \n- In Sec. 4.1, in the Representing state embeddings section, it is stated that “we preprocess the dataset by grouping entries by object ID and selecting a successful trial as the canonical embedding e^o_j”.  I don’t understand what these “canonical embeddings” are or how they are used, as the paper never explicitly revisits how these canonical embeddings are stored, updated, or surfaced at inference time, which made it hard to connect the preprocessing step to the online retrieval procedure in Sec. 4.1.\n- In Sec. 4.1, in the Retrieval and refinement section, it is unclear what is meant by the “current video”.  If we haven’t yet performed an interaction, how do we have a current video?  Is this an initial frame, or some initial sequence of frames?  \n- What are state embeddings?  Is this a vector that we get from a video, a vector for each frame, or something else?  This is central and it should really be formalized.\n- Figure 3 does not illustrate refinement, as far as I can tell, which is a shame because I found the written text explanation very vague.\n- The approach could really benefit from more mathematical descriptions of what’s going on.  The text descriptions tend to be vague and hard to follow.\n- Dataset preprocessing: what is object ID?  Is this just the object being manipulated, or is it also the “scenario” as well?  I.e., in the faucet example, are trials where the faucet moves in the same way grouped together?\n- I don’t see it clearly stated what action space is used for the baselines.\n- The ablation labels in Fig. 5 (AVDC+Retrieval, AVDC+Rejection, Ours) aren’t described in enough procedural detail to be reproducible. For example, what exactly is being fed to the planner in AVDC+Retrieval vs AVDC? Is AVDC+Retrieval simply AVDC but conditioned on a single retrieved embedding instead of no embedding? This should be clarified.\n\n\nThere are also some conceptual weaknesses:\n- The problem setting is not novel, as far as I can tell it is the same one studied in Hidden-Parameter MDPs [1].  Many approaches exist for rapidly adapting in environments with the same type of uncertainty studied here [2,3,4].  The refinement method used in this paper seems to be a fairly heuristic solution, approximating what some of these other papers do in a more principled manner.  From a formulation standpoint, this is extremely close to Hidden-Parameter MDPs and latent-context meta-RL, except the latent context is represented by a vision encoder + diffusion refinement, and the policy is instantiated via video plan + tracking instead of a standard parametric policy. The paper does not compare against prior latent-context adaptation methods, so I cannot tell if this buy-in to video planning actually provides an advantage beyond aesthetics.\n- an 8-frame video plan seems extremely short.  Does this really mean that the approach can only plan 7 timesteps into the future?  What is the control frequency? Are these 7 steps high-level waypoints that get tracked over longer horizons via the Action Module, or is this genuinely planning only ~7 control steps ahead?\n- With regard to the rejection strategy, L2 distance in pixel space seems like it would be a poor distance metric, as it doesn’t really capture any semantic detail about what’s actually happening in the plan, and is sensitive to things like subtle shifts in camera view. The authors do provide an ablation (Table 4 / Fig. 6) showing that pixel distance actually outperforms DINOv2 feature distance on their benchmark, which partially mitigates this concern, but I still worry about generalization to more realistic visual variability. \n- Using a set of heuristic policies for the action module seems somewhat limited and hard-coded.   \n- Because I don’t know what action space is being used for the baselines, I can’t tell if a fair comparison is being done.\n- Is there no ablation that keeps only the refinement component but removes the rejection component?  This seems like a major oversight as refinement is one of the main proposed methods of adaptation.\n\n\n[1] Doshi-Velez, Finale, and George Konidaris. \"Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations.\" IJCAI: proceedings of the conference. Vol. 2016. 2016.\n[2] Rakelly, K., Zhou, A., Finn, C., Levine, S. &amp; Quillen, D.. (2019). Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. <i>Proceedings of the 36th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 97:5331-5340 Available from https://proceedings.mlr.press/v97/rakelly19a.html\n[3] Sæmundsson, Steindór, Katja Hofmann, and Marc Peter Deisenroth. \"Meta reinforcement learning with latent variable gaussian processes.\" arXiv preprint arXiv:1803.07551 (2018).\n[4] Killian, Taylor W., et al. \"Robust and efficient transfer learning with hidden parameter markov decision processes.\" Advances in neural information processing systems 30 (2017)."}, "questions": {"value": "Please clarify the questions I raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bIw0wxNPCQ", "forum": "61OBn2aI04", "replyto": "61OBn2aI04", "signatures": ["ICLR.cc/2026/Conference/Submission7993/Reviewer_u999"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7993/Reviewer_u999"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668213204, "cdate": 1761668213204, "tmdate": 1762919996879, "mdate": 1762919996879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}