{"id": "oJ2Art4XTY", "number": 8300, "cdate": 1758077935016, "mdate": 1759897793519, "content": {"title": "Excavating Consistency Across Editing Steps for Effective Multi-Step Image Editing", "abstract": "Multi-step image editing with diffusion models typically requires repeatedly executing the inversion–denoising paradigm, which leads to severe challenges in both image quality and computational efficiency. Repeated inversion introduces errors that accumulate across editing steps, degrading image quality, while regeneration of unchanged background regions incurs substantial computational overhead. In this paper, we present ExCave, a training-free multi-step editing framework that improves both image quality and computational efficiency by excavating consistency across editing steps. ExCave introduces an inversion sharing mechanism that performs inversion once and reuses its consistent features across subsequent edits, thereby significantly reducing errors. To eliminate redundant computation, we propose the CacheDiff method that regenerates only the edited regions while reusing consistent features from unchanged background regions. Finally, we design GPU-oriented optimizations to translate theoretical gains into practical reductions in end-to-end latency. Extensive experiments demonstrate that ExCave achieves superior image quality and dramatically reduces inference latency, establishing a new paradigm for accurate and efficient multi-step editing.", "tldr": "", "keywords": ["region consistency;multi-step image editing;diffusion;acceleration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8d004b2adf4ee7d83da4faedf2ef24f4fbd72ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenges of inversion-based multi-turn image editing, where repeated inversions cause error accumulation and redundant computation. To mitigate these issues, the authors propose performing inversion only once and reusing intermediate features across multiple editing steps to preserve consistency and reduce degradation. To further enhance efficiency, they introduce CacheDiff, a caching mechanism that stores inversion results (from the input image to the noise space) for reuse in subsequent edits. CacheDiff selectively regenerates edited regions using localized cross-attention masks while keeping unedited background areas intact. Additionally, the authors implement GPU-oriented optimizations to accelerate their ExCave framework for practical, real-time multi-turn editing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall assessment: The proposed method is straightforward and well-motivated.\n2. Strength: The key advantage of the ExCave framework lies in its ability to selectively reuse unchanged features during multi-turn editing based on different editing prompts, thereby reducing redundant inversions and mitigating error accumulation across editing steps."}, "weaknesses": {"value": "1. Ambiguity in terminology and writing: The paper’s writing is sometimes ambiguous. The term “Multi-Step Editing” may be misleading, as it could be interpreted as multiple denoising steps during diffusion inference. The authors are encouraged to adopt the term “multi-turn”, consistent with prior work such as “Multi-turn Consistent Image Editing” [1]. Additionally, the exposition in Section 4.1 is difficult to follow—particularly Equation (5), where the meanings of i and j are unclear.\n2. Insufficient description of CacheDiff: The proposed CacheDiff method in Section 4.2 requires more detailed explanation, especially regarding the VS Fusion design in Algorithm 1 and the sparse attention module in Algorithm 2. This section also appears to lack sufficient experimental evidence to substantiate the claimed efficiency and accuracy improvements.\n3. Potential limitation in generality: The proposed CacheDiff approach may be constrained to object-centric editing, as it depends on masks derived from VS Fusion (based on cross-attention maps). In more complex scenarios such as style or appearance editing, where both content and texture undergo substantial changes, reusing cached key–value features may not yield effective results.\n\n[1] Zhou Z, Deng Y, He X, Dong W, Tang F. Multi-turn Consistent Image Editing. arXiv preprint arXiv:2505.04320. 2025 May 7.\n[2] Zhang K, Mo L, Chen W, Sun H, Su Y. Magicbrush: A manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems. 2023 Dec 15;36:31428-49."}, "questions": {"value": "1. Lack of discussion on related work: The Related Work section omits prior studies on multi-turn editing, despite referencing “Multi-turn Consistent Image Editing” [1] in the experiments, which tackles the same problem as this paper. The authors should include a discussion of this and other related approaches to better contextualize their contributions.\n2. Overly complex averaging operation in Algorithm 1: The averaging step in Algorithm 1 (Section 4.2.1, lines 1–4) appears unnecessarily complicated and could likely be simplified. Moreover, in line 7, the variable k (“turn k”) seems redundant, as the algorithm is executed within a single-turn editing process. Simplifying these formulations would improve clarity and readability.\n3. Lack of motivation and citations for proposed methods: In Section 4, all the described components appear to be the authors’ own proposals, yet no references or motivations from prior work are provided. If these methods are indeed novel, additional experimental evidence or ablation studies are needed to substantiate their effectiveness.\n4. Use of inappropriate evaluation benchmark: The experiments use the PIE dataset, which is not a benchmark for multi-turn editing. A more suitable dataset, such as MagicBrush [2], supports multi-turn evaluation and would provide a more convincing validation. Additionally, results should be reported for each editing turn to demonstrate how error accumulation evolves and how the proposed method compares against baselines across turns.\n5. Fairness of comparison in Table 1: The strong visual quality in Table 1 may partly stem from using FLUX as the underlying diffusion backbone. Since the proposed method is designed to work across multiple diffusion models (e.g., SD and FLUX), fair comparisons should be performed under the same backbone used by competing methods.\n6. Choice of attention maps in VS Fusion: In VS Fusion, why were cross-attention maps selected for mask extraction? Have the authors explored alternatives such as self-attention maps? An ablation comparing these choices would clarify the design rationale.\n7. Motivation for preserving background via KV features: In Section 4.2.2, the paper states that KV features are preserved to maintain background information. What motivated this specific design choice? Could other representations (e.g., latent feature blending) be equally effective?\n8. Clarification of “sparse attention” and “sparse MLP” modules: The “sparse attention” module in Algorithm 2 appears to be standard full attention with modified K and V matrices that mix edited and background tokens. The authors should clarify how this differs from standard attention and provide details on the “sparse MLP” component, ideally with diagrams or pseudocode.\n9. GPU optimization clarity: In Section 4.3, the GPU-oriented optimizations are described at a high level. Including pseudocode or an illustrative figure would greatly enhance clarity and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BlAe8xO3Uc", "forum": "oJ2Art4XTY", "replyto": "oJ2Art4XTY", "signatures": ["ICLR.cc/2026/Conference/Submission8300/Reviewer_t2qe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8300/Reviewer_t2qe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742281726, "cdate": 1761742281726, "tmdate": 1762920229409, "mdate": 1762920229409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ExCave, a training-free framework for efficient and accurate multi-step image editing with diffusion models. Traditional approaches repeatedly perform inversion and denoising at each editing step, which causes accumulated errors and high computational cost. ExCave addresses these problems through two main ideas: inversion sharing, which performs inversion only once and reuses consistent features across edits, and CacheDiff, which regenerates only the modified regions while reusing unchanged background features. The authors also introduce GPU optimizations to further reduce latency. Experiments show that ExCave improves image quality and significantly speeds up inference, establishing a new paradigm for multi-step diffusion-based image editing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. The framework reuses inversion features across editing steps, which reduces error accumulation and improves image consistency.\n2. It regenerates only the edited regions while caching unchanged areas, which significantly enhances computational efficiency.\n3. It works without additional model training and includes GPU-oriented optimizations, making it practical for real-world multi-step editing."}, "weaknesses": {"value": "Weaknesses\n1. The method relies on the assumption that shared inversion features remain valid across large semantic edits, which may fail for major content changes.\n2. The CacheDiff mechanism depends on accurate region masks, and errors at boundaries can cause visual artifacts or inconsistencies.\n3. The paper does not compare against recent strong directions such as inversion-free editing methods and prior cache-based or region-reuse acceleration approaches, so generality and relative gains are unclear."}, "questions": {"value": "1. How robust is the inversion sharing mechanism when the edits involve large semantic or structural changes? Would a partial re-inversion strategy help maintain consistency in such cases?\n2. How sensitive is the method to inaccuracies in region masks or gradual region drift over multiple edits?\n3. Could the authors provide quantitative or qualitative comparisons with inversion-free and other cache-based acceleration methods to better show where ExCave stands relative to these recent baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "meGazVcMOh", "forum": "oJ2Art4XTY", "replyto": "oJ2Art4XTY", "signatures": ["ICLR.cc/2026/Conference/Submission8300/Reviewer_HAzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8300/Reviewer_HAzD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878442775, "cdate": 1761878442775, "tmdate": 1762920228919, "mdate": 1762920228919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ExCave, a training-free framework for multi-step image editing with diffusion models that enhances both image quality and computational efficiency. ExCave introduces two main innovations: (1) Inversion Sharing Mechanism (ISM): Performs inversion once and reuses consistent latent features across subsequent editing steps, reducing error propagation. (2) CacheDiff: A feature caching method that regenerates only edited regions while reusing background features, significantly reducing latency. Additionally, the authors introduce GPU-oriented optimizations to convert theoretical gains into real speedups. Experiments on an extended PIE-Bench dataset demonstrate consistent improvements in background consistency, text–image alignment, and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel observation: Recognizing region-level consistency across editing steps and leveraging it effectively.\n- Practical implementation: GPU optimizations demonstrate attention to real-world deployment."}, "weaknesses": {"value": "- No user study: While objective metrics are solid, perceptual or human preference evaluations would strengthen claims about “editing quality.”\n- Comparison to state-of-the-art editing methods: The paper lacks comparisons to state-of-the-art image editing methods such as GPT-4o, Gemini2.5 (nano-banana)."}, "questions": {"value": "- How sensitive is ExCave to inaccuracies in the VS Fusion localization? Would misidentified edited regions harm image coherence?\n- Can ExCave be combined with fine-tuned or training-based acceleration methods (e.g., ControlNet, LoRA) without retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zPANz2dMyj", "forum": "oJ2Art4XTY", "replyto": "oJ2Art4XTY", "signatures": ["ICLR.cc/2026/Conference/Submission8300/Reviewer_jyn3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8300/Reviewer_jyn3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900020533, "cdate": 1761900020533, "tmdate": 1762920228445, "mdate": 1762920228445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ExCave, a training-free framework for multi-step diffusion-based image editing that boosts both quality and efficiency. It targets two issues in iterative pipelines: error accumulation from repeated inversion and redundant computation on unchanged backgrounds. ExCave combines (1) an Inversion Sharing Mechanism (ISM) that performs inversion once and reuses consistent features across edits, and (2) CacheDiff, which sparsely recomputes only edited regions while reusing cached background features. With GPU optimizations (memory pre-allocation, multi-stream parallelism, and prefetching with delayed write-back), ExCave delivers up to 65.8% lower latency on extended PIE-Bench while maintaining or improving image quality and text–image alignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and underexplored challenge in multi-step image editing, clearly diagnosing error accumulation from repeated inversion and redundant background regeneration, and proposes an edit-only keep-the-rest principle through inversion sharing and region-aware caching in a training-free manner.\n\n2. The motivation-to-method logic is clear, moving from region-consistency observations to a concrete design that suppresses inversion errors and enables sparse recomputation, with sufficient technical detail and illustrative figures and pseudocode that make the pipeline easy to follow.\n\n3. The GPU-oriented optimizations, including memory pre-allocation, multi-stream parallelism, and prefetching with delayed write-back, show strong attention to deployment by bridging algorithmic ideas with system-level efficiency and yielding substantial end-to-end latency reductions without sacrificing quality."}, "weaknesses": {"value": "1. The paper states that edited regions are localized by comparing the denoising input with the corresponding inversion image, which “directly reflects visual changes,” but it is unclear precisely where in the pipeline this comparison is applied and how it interacts with the cache-consistency assumption that backgrounds remain reusable.\n\n2. The VS Fusion locator depends on semantic and visual thresholds (Ps and Pv), yet the paper provides no systematic analysis of their sensitivity to prompt phrasing, illumination changes, or scale variations, nor an assessment of how false positives and false negatives propagate to image quality and end-to-end latency.\n\n3. The GPU optimization section, while technically sound, appears tightly coupled to a specific implementation stack, raising concerns about portability and reproducibility.\n\n4. The experimental setup emphasizes a single sampling configuration and solver choice on a limited set of backbones, which constrains the external validity of the conclusions. Additional experiments spanning step counts, solvers, and diverse backbones would better establish robustness and generalization."}, "questions": {"value": "1. How robust is the VS Fusion–based localization to imprecise or compositional prompts, and what failure modes (if any) produce visible artifacts or degrade efficiency in multi-step editing?\n\n2. Can the authors provide a systematic sensitivity study of the semantic and visual thresholds (Ps, Pv), including how false positives and false negatives affect image quality and end-to-end latency?\n\n3. Have the authors evaluated the trade-off between cache size and latency gains under different GPU memory configurations, and how do these settings influence portability across hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "86nREJXO09", "forum": "oJ2Art4XTY", "replyto": "oJ2Art4XTY", "signatures": ["ICLR.cc/2026/Conference/Submission8300/Reviewer_15y7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8300/Reviewer_15y7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925765820, "cdate": 1761925765820, "tmdate": 1762920228163, "mdate": 1762920228163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}