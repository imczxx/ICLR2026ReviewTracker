{"id": "LmGSD6CedJ", "number": 11388, "cdate": 1758197958786, "mdate": 1759897578393, "content": {"title": "RL$^2$eak: Reinforcement Learning Enhanced Prompt Leakage Attack in Multi-tenant Large Language Model Services", "abstract": "Large Language Models (LLMs) have become a transformative technology in both academia and industry. In practice, LLM services are typically deployed using multi-tenant serving frameworks. Popular inference frameworks such as vLLM and SGLang both employ Key-Value cache sharing among users to enhance computational efficiency. However, this shared caching mechanism may lead to potential leakage of the private user prompts. Previous works have demonstrated the impact of this information. Nevertheless, these works mainly focus on expanding the attack surface brought by the cache sharing mechanism, rather than optimizing the attack performance. This prevents users from accurately assessing the leakage's impact, thus hindering the timely leakage mitigation.\nTo investigate the bounds of the cache-based side channel attack, we propose RL$^2$eak, a reinforcement learning enhanced prompt leakage attack framework. We show that the adversary requires far fewer active prompt guesses with RL$^2$eak than reported by previous works. To validate the effectiveness of our RL$^2$eak, we apply RL$^2$eak to two real-world scenarios, i.e., medical and finance, achieving a maximum 12.48$\\times$ reduction in average requests needed to guess one token. This study highlights the necessity for enhanced leakage transparency and careful management of cache-based information sharing, providing critical insights and references for future security countermeasures.", "tldr": "", "keywords": ["Prompt leakage attack", "reinforcement learning", "LLM service"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d6f7788bf8c1f0aeb4632fbe5200be34022cf77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies an active side-channel attack against shared LLM services, where the attacker tries to guess a victim user's query. This involves an attacker model, which predicts the possible next token in the victim prompt. The TTFT gap between generated queries provides an indicator of whether the target token is hit. To better enhance the prediction efficiency, this work proposes additionally training the attacker model on domain-specifc datasets. This leads to better effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper discusses an interesting side channel attack about prompt leakage.\n\n- It is intriguing to introduce the concept of RL to this attack scenario for a new opportunity."}, "weaknesses": {"value": "- The discussion about the potential mitigation strategies is too simple.\n    \n    - An LLM inference engine keeping confidentiality in mind can easily mitigate this threat by only allowing the reuse of the KV cache in a user-isolated way.\n        \n    - What's more, the attack only targets the SGLang, and the method in Section 4.2 exploits the Longest Prefix Match scheduling policy specific to SGLang. This is an inappropriate setting. What about other scheduling policies, and what about other inference engines? It is necessary to discuss the design or setting in detail.\n        \n- Missing baseline comparison. As discussed in Related Works, there are many existing active side channel attacks. This work does not compare the proposed RL$^2$eak method to them, but claims to optimize the probing query generation process. This is unsubstantiated.\n    \n- Writing concerns\n    \n    - The motivation for introducing SFT and DPO to better align with the domain-specific dataset is unclear. I guess the intuition is that the target victim query that will be asked is similar to other queries. In this case, enabling the adversarial model to predict in a similar way will be helpful. If so, it is necessary to do decontamination between the training and the test set to avoid hacking issues.\n        \n    - What's more, why this attack is called an RL-enhanced attack is also unclear. Just because it operates in a trial-and-error way?"}, "questions": {"value": "- An algorithm presentation for this attack may facilitate understanding how the attack operates.\n    \n- It is unclear why introducing $m$ dummy queries is necessary (Section 4.2).\n    \n- What about extending to other inference engines, e.g., vLLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8481ETqLG", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_RQv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_RQv1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760674016655, "cdate": 1760674016655, "tmdate": 1762922510981, "mdate": 1762922510981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RL^2eak, a framework to improve the efficiency of prompt leakage attacks that exploit shared KV-caches in multi-tenant LLMs. The method uses a two-stage fine-tuning process (SFT followed by DPO) and features a novel automated annotation technique for DPO that generates preference pairs from token generation difficulty. Experiments show that RL^2eak significantly reduces the number of queries required to reconstruct a victim's prompt."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting application of reinforcement learning techniques (SFT + DPO) to optimize cache-based side-channel attacks in LLM services. \n2. The assumptions about adversary capabilities (domain knowledge but no specific prompt knowledge) are reasonable for real-world scenarios."}, "weaknesses": {"value": "1. The paper's motivation is weak, as it fails to convincingly argue why reducing the number of attack requests is a significant problem.\n2. Evaluations are limited to only two domain-specific datasets and exclusively use the Qwen-2.5 model family, without testing other mainstream open-source LLMs such as Llama, Deepseek, or GLM."}, "questions": {"value": "1. The paper's central weakness is the lack of justification for why optimizing an already-feasible attack matters. Could the authors please elaborate on the significance of improving attack efficiency? In what specific, practical threat models is the number of queries the primary bottleneck for an adversary, to the extent that a 10x improvement moves the attack from being impractical to practical? \n2. Experimental evaluations were conducted using only two domain datasets and the Qwen-2.5 model. How does the proposed method generalize to other knowledge-intensive domains beyond medical and financial contexts, such as legal documents, scientific literature, or software code? More importantly, does RL^2eak demonstrate consistent effectiveness when applied to other prominent open-source LLM families like Llama, Deepseek, or GLM models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eAOgn1rmrM", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_C8wA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_C8wA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534923227, "cdate": 1761534923227, "tmdate": 1762922510483, "mdate": 1762922510483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the vulnerability of Key-Value cache sharing among users of multi-tenant serving frameworks. Building on prior works which have expanded attack surface of cache sharing mechanism, the authors propose RL2eak which involves domain knowledge of user queries by SFT and RL, in order to improve attack performance, mainly on efficiency. RL2eak first SFT base local model with domain-specific data, then leverage this SFT model to annotate training data for DPO, identifying \"hard tokens\" that are challenging to generate. Experiments demonstrate RL2eak improve attack performance across medical and finance scenarios, achieving a maximum 12.48\\times reduction in average requests needed to guess one token."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Good topic**: The authors choose a practical scenario of deployed LLMs where multi-tenant serving framework is used and Key-Value cache sharing mechanism is employed. And they explore the prompt leakage threat from shared caching mechanism, taking Time to Fist Token (TTFT) as side channel signal.\n2. **Good intuition of main method**: the method is build on the intuition that the cached key-value pairs can be reused when multiple users submit prompts with identical prefixs (line 156), so an observable timing side-channel can effectively indicate the correspondence of dummy query and victim query. Moreover, involving more domain knowlegde to generate dummy queries is realistic and practical."}, "weaknesses": {"value": "1. The main concern is the strong assumption that the attacker's queries and the victim's queries are on the same GPU node (line 196). But this is not always achievable because the scheduling mechanism of server is not accessible. Even if they are on the same GPU node, the GPU can process multiple batches of queries (not only the ones from victims and attackers), which means the response time can be influenced by other irrelative queries.\n2. The authors indicate that only-SFT leads to poor attack performance primarily due to overfitting and mode collapse (line 248), but there is no demonstration of such limitation.\n3. The test set and the train set are highly correlated since they are sampled from the same dataset (line 319-323), contributing to the good performance of RL2eak. However, in real-world scenarios, the victim queries and dummy queries can be less correlated. It is recommended to train RL2eak on dataset 1 and test on dataset 2 (dataset 1 and 2 are from the same domain but have different distribution) to observe whether the attack is practical.   \n4. The proposed method has limited contribution. The main difference to prior work is involving domain knowledge via RL when generating dummy queries. Moreover, in Table 1, RL2eak has not much improvement compared to SFT."}, "questions": {"value": "1. What does the consistent prompt prefix \"Help me to guess the input:\" (line 356) mean? Is it the target prefix to steal from victim queries?\n2. In Table 1, I don't see a 18.1% reduction of RL2eak in APR compared to SFT (line 375). Is that a mistake?\n3. Although the authors use APR to quantify efficiency, what is the actual time cost to steal a prompt? And is it acceptable? The victim query may be terminated before all dummy queries are processed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wE6uO4G8p", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_sS1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_sS1o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645866714, "cdate": 1761645866714, "tmdate": 1762922510057, "mdate": 1762922510057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study an important safety problem when client uses LLM, where attacker might exploit KV-cache reuse in multi-tenant LLM services to induce prompt leakage. They propose RL@EAK, an attacker-model training pipeline using SFT then DPO that automatically constructs preference pairs by identifying \"hard tokens\", then optimizing a probe-generation policies to save probing queries. Experiments show large reduction in average request per token and improvement in attack success rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is an important and novel topic for LLM safety, very worth studying. \n2. The experiments are solid, evaluating realistic domains with meaningful metrics. \n3. The attack optimization is solid and practical, author did provide ablation studies for each component of the pipeline."}, "weaknesses": {"value": "1. It is assumed that the attacker knows the victim domain so they can construct the domain auxiliary dataset. This maybe optimistic in real deployments -- even the LLM is deployed on certain domain, the difference on subdomain between user's prompt and attackers' auxiliary dataset might harm the current red-teaming method. \n\n2. Commercial LLM hosting stacks vary. It is assumed that the attacker knew the particular caching/scheduling behavior of LLMs. An analysis of robustness of the attack across different caching design might strengthen the paper."}, "questions": {"value": "Please address the suggestions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pwgVh2fPez", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_zDf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_zDf7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866918230, "cdate": 1761866918230, "tmdate": 1762922509569, "mdate": 1762922509569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}