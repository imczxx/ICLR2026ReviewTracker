{"id": "LmGSD6CedJ", "number": 11388, "cdate": 1758197958786, "mdate": 1763720949281, "content": {"title": "RL$^2$eak: Reinforcement Learning Enhanced Prompt Leakage Attack in Multi-tenant Large Language Model Services", "abstract": "Large Language Models (LLMs) have become a transformative technology in both academia and industry. In practice, LLM services are typically deployed using multi-tenant serving frameworks. Popular inference frameworks such as vLLM and SGLang both employ Key-Value cache sharing among users to enhance computational efficiency. However, this shared caching mechanism may lead to potential leakage of the private user prompts. Previous works have demonstrated the impact of this information. Nevertheless, these works mainly focus on expanding the attack surface brought by the cache sharing mechanism, rather than optimizing the attack performance. This prevents users from accurately assessing the leakage's impact, thus hindering the timely leakage mitigation.\nTo investigate the bounds of the cache-based side channel attack, we propose RL$^2$eak, a reinforcement learning enhanced prompt leakage attack framework. We show that the adversary requires far fewer active prompt guesses with RL$^2$eak than reported by previous works. To validate the effectiveness of our RL$^2$eak, we apply RL$^2$eak to two real-world scenarios, i.e., medical and finance, achieving a maximum 12.48$\\times$ reduction in average requests needed to guess one token. This study highlights the necessity for enhanced leakage transparency and careful management of cache-based information sharing, providing critical insights and references for future security countermeasures.", "tldr": "", "keywords": ["Prompt leakage attack", "reinforcement learning", "LLM service"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d0ea56eb30ba6c33d69971ee2f0dfb8a8d63745.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies an active side-channel attack against shared LLM services, where the attacker tries to guess a victim user's query. This involves an attacker model, which predicts the possible next token in the victim prompt. The TTFT gap between generated queries provides an indicator of whether the target token is hit. To better enhance the prediction efficiency, this work proposes additionally training the attacker model on domain-specifc datasets. This leads to better effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper discusses an interesting side channel attack about prompt leakage.\n\n- It is intriguing to introduce the concept of RL to this attack scenario for a new opportunity."}, "weaknesses": {"value": "- The discussion about the potential mitigation strategies is too simple.\n    \n    - An LLM inference engine keeping confidentiality in mind can easily mitigate this threat by only allowing the reuse of the KV cache in a user-isolated way.\n        \n    - What's more, the attack only targets the SGLang, and the method in Section 4.2 exploits the Longest Prefix Match scheduling policy specific to SGLang. This is an inappropriate setting. What about other scheduling policies, and what about other inference engines? It is necessary to discuss the design or setting in detail.\n        \n- Missing baseline comparison. As discussed in Related Works, there are many existing active side channel attacks. This work does not compare the proposed RL$^2$eak method to them, but claims to optimize the probing query generation process. This is unsubstantiated.\n    \n- Writing concerns\n    \n    - The motivation for introducing SFT and DPO to better align with the domain-specific dataset is unclear. I guess the intuition is that the target victim query that will be asked is similar to other queries. In this case, enabling the adversarial model to predict in a similar way will be helpful. If so, it is necessary to do decontamination between the training and the test set to avoid hacking issues.\n        \n    - What's more, why this attack is called an RL-enhanced attack is also unclear. Just because it operates in a trial-and-error way?"}, "questions": {"value": "- An algorithm presentation for this attack may facilitate understanding how the attack operates.\n    \n- It is unclear why introducing $m$ dummy queries is necessary (Section 4.2).\n    \n- What about extending to other inference engines, e.g., vLLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8481ETqLG", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_RQv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_RQv1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760674016655, "cdate": 1760674016655, "tmdate": 1762922510981, "mdate": 1762922510981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of rebuttal"}, "comment": {"value": "Dear PCs, SACs, ACs, Reviewers,\n\nWe sincerely thank the reviewers for their insightful and constructive feedback, which has helped us significantly improve the quality and clarity of our manuscript. We have carefully considered all comments and revised the paper accordingly. We have also submitted the revised manuscript, in which all modifications are highlighted in **blue**. We hope the following summary will assist the AC and future readers in better understanding our work and the rebuttal. We believe these revisions have strengthened the paper, and we provide our detailed point-by-point responses below. \n\n**Addressing the Concerns Raised by Reviewer zDf7:**\n\n- **Assumption on high-level domain knowledge:** We have added experiments in Section 5.4 to evaluate robustness under varying degrees of adversarial advantage: (1) No prior knowledge, (2) High-level intra-domain knowledge, (3) Precise intra-domain knowledge, and (4) Misaligned knowledge. The results demonstrate that $\\text{RL}^2\\text{eak}$ remains robust in the high-level intra-domain setting (reducing APR by 27.9%), while misaligned knowledge (e.g., training on Finance, testing on Medical) hampers performance, confirming our hypothesis on domain-specific bias.\n- **Robustness against caching design:** We updated Section 6 to clarify that $\\text{RL}^2\\text{eak}$ decouples query guessing from verification. Consequently, our method is compatible with various scheduling policies (e.g., FCFS, priority-based) by leveraging timing-based side channels (Time-To-First-Token) alongside the Longest Prefix Match (LPM) mechanism.\n\n**Addressing the Concerns Raised by Reviewer sS1o:**\n\n- **Assumption on the KV cache pool:** We clarified in Section 3.3 that our assumption relies on the victim and attacker sharing a server node with a KV cache pool. We referenced real-world implementations, which utilize KV-cache pooling for persistence, validating the practicality of our threat model.\n- **Demonstration of SFT overfitting:** We highlighted our ablation study (Section 5.3) which demonstrates that SFT is prone to model collapse as training steps increase, necessitating the DPO approach. We have updated the description in Section 4.1 to emphasize this finding.\n- **Evaluation on less correlated datasets:** We conducted cross-dataset experiments (e.g., training on **PubMedQA**, testing on **MedQA**) to prove that $\\text{RL}^2\\text{eak}$ is effective even when training and testing data exhibit distribution disparities within the same general domain.\n- **Time cost:** We clarify that our empirical experiments show that for a query of approximately 100 tokens, $\\text{RL}^2\\text{eak}$ requires around 5 minutes to recover the whole query. Notably, we clarify that the attack window is determined by **KV cache eviction** rather than the victim's query termination. Given that modern systems employ pooling techniques that extend cache persistence, supporting APR as our primary metric.\n\n**Addressing the Concerns Raised by Reviewer** **C8wA****:**\n\n- **Motivation for efficiency:** We updated Section 1 to emphasize that prior KV cache side-channel attacks were often impractical due to the prohibitive number of requests required, as documented in prior works. $\\text{RL}^2\\text{eak}$ bridges the gap between theoretical feasibility and practical exploitation.\n- **Generalization on domains and models:** We expanded our evaluation to include the **PubMedQA** dataset and the **Llama-3.1** model series. Furthermore, we have conducted cross-dataset experiments to test $\\text{RL}^2\\text{eak}$â€™s generalizability under different attacker assumptions. The results confirm that $\\text{RL}^2\\text{eak}$ generalizes effectively across different attacker assumptions and model architectures.\n\n**Addressing the Concerns Raised by Reviewer** **RQv1****:**\n\n- **Mitigation and Target Cache Policy:** We discussed that while user isolation is a mitigation, it significantly reduces cache sharing efficiency. We also clarified that our attack is compatible with other engines, such as **vLLM** (which supports token-level matching), by adapting the verification mechanism.\n- **Choice of Baselines:** We justified our use of order-based side channels as the baseline due to their stability compared to TTFT-based methods, while emphasizing that our main contribution is the efficient *generation* of queries, which is independent of the specific side channel used for verification.\n- **SFT/DPO Motivation and Decontamination:** We clarify that we adhere to a strict train-test split. Furthermore, our new cross-dataset experiments demonstrate robustness to distributional shifts without data leakage.\n\nOnce again, we sincerely appreciate your time and valuable feedback. We hope that the revisions and clarifications we have provided effectively address the concerns and contribute to strengthening our submission.\n\nBest,\n\nThe Authors"}}, "id": "nLqQrh1sxf", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718598087, "cdate": 1763718598087, "tmdate": 1763718598087, "mdate": 1763718598087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RL^2eak, a framework to improve the efficiency of prompt leakage attacks that exploit shared KV-caches in multi-tenant LLMs. The method uses a two-stage fine-tuning process (SFT followed by DPO) and features a novel automated annotation technique for DPO that generates preference pairs from token generation difficulty. Experiments show that RL^2eak significantly reduces the number of queries required to reconstruct a victim's prompt."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting application of reinforcement learning techniques (SFT + DPO) to optimize cache-based side-channel attacks in LLM services. \n2. The assumptions about adversary capabilities (domain knowledge but no specific prompt knowledge) are reasonable for real-world scenarios."}, "weaknesses": {"value": "1. The paper's motivation is weak, as it fails to convincingly argue why reducing the number of attack requests is a significant problem.\n2. Evaluations are limited to only two domain-specific datasets and exclusively use the Qwen-2.5 model family, without testing other mainstream open-source LLMs such as Llama, Deepseek, or GLM."}, "questions": {"value": "1. The paper's central weakness is the lack of justification for why optimizing an already-feasible attack matters. Could the authors please elaborate on the significance of improving attack efficiency? In what specific, practical threat models is the number of queries the primary bottleneck for an adversary, to the extent that a 10x improvement moves the attack from being impractical to practical? \n2. Experimental evaluations were conducted using only two domain datasets and the Qwen-2.5 model. How does the proposed method generalize to other knowledge-intensive domains beyond medical and financial contexts, such as legal documents, scientific literature, or software code? More importantly, does RL^2eak demonstrate consistent effectiveness when applied to other prominent open-source LLM families like Llama, Deepseek, or GLM models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eAOgn1rmrM", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_C8wA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_C8wA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534923227, "cdate": 1761534923227, "tmdate": 1762922510483, "mdate": 1762922510483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the vulnerability of Key-Value cache sharing among users of multi-tenant serving frameworks. Building on prior works which have expanded attack surface of cache sharing mechanism, the authors propose RL2eak which involves domain knowledge of user queries by SFT and RL, in order to improve attack performance, mainly on efficiency. RL2eak first SFT base local model with domain-specific data, then leverage this SFT model to annotate training data for DPO, identifying \"hard tokens\" that are challenging to generate. Experiments demonstrate RL2eak improve attack performance across medical and finance scenarios, achieving a maximum 12.48\\times reduction in average requests needed to guess one token."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Good topic**: The authors choose a practical scenario of deployed LLMs where multi-tenant serving framework is used and Key-Value cache sharing mechanism is employed. And they explore the prompt leakage threat from shared caching mechanism, taking Time to Fist Token (TTFT) as side channel signal.\n2. **Good intuition of main method**: the method is build on the intuition that the cached key-value pairs can be reused when multiple users submit prompts with identical prefixs (line 156), so an observable timing side-channel can effectively indicate the correspondence of dummy query and victim query. Moreover, involving more domain knowlegde to generate dummy queries is realistic and practical."}, "weaknesses": {"value": "1. The main concern is the strong assumption that the attacker's queries and the victim's queries are on the same GPU node (line 196). But this is not always achievable because the scheduling mechanism of server is not accessible. Even if they are on the same GPU node, the GPU can process multiple batches of queries (not only the ones from victims and attackers), which means the response time can be influenced by other irrelative queries.\n2. The authors indicate that only-SFT leads to poor attack performance primarily due to overfitting and mode collapse (line 248), but there is no demonstration of such limitation.\n3. The test set and the train set are highly correlated since they are sampled from the same dataset (line 319-323), contributing to the good performance of RL2eak. However, in real-world scenarios, the victim queries and dummy queries can be less correlated. It is recommended to train RL2eak on dataset 1 and test on dataset 2 (dataset 1 and 2 are from the same domain but have different distribution) to observe whether the attack is practical.   \n4. The proposed method has limited contribution. The main difference to prior work is involving domain knowledge via RL when generating dummy queries. Moreover, in Table 1, RL2eak has not much improvement compared to SFT."}, "questions": {"value": "1. What does the consistent prompt prefix \"Help me to guess the input:\" (line 356) mean? Is it the target prefix to steal from victim queries?\n2. In Table 1, I don't see a 18.1% reduction of RL2eak in APR compared to SFT (line 375). Is that a mistake?\n3. Although the authors use APR to quantify efficiency, what is the actual time cost to steal a prompt? And is it acceptable? The victim query may be terminated before all dummy queries are processed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wE6uO4G8p", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_sS1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_sS1o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645866714, "cdate": 1761645866714, "tmdate": 1762922510057, "mdate": 1762922510057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study an important safety problem when client uses LLM, where attacker might exploit KV-cache reuse in multi-tenant LLM services to induce prompt leakage. They propose RL@EAK, an attacker-model training pipeline using SFT then DPO that automatically constructs preference pairs by identifying \"hard tokens\", then optimizing a probe-generation policies to save probing queries. Experiments show large reduction in average request per token and improvement in attack success rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is an important and novel topic for LLM safety, very worth studying. \n2. The experiments are solid, evaluating realistic domains with meaningful metrics. \n3. The attack optimization is solid and practical, author did provide ablation studies for each component of the pipeline."}, "weaknesses": {"value": "1. It is assumed that the attacker knows the victim domain so they can construct the domain auxiliary dataset. This maybe optimistic in real deployments -- even the LLM is deployed on certain domain, the difference on subdomain between user's prompt and attackers' auxiliary dataset might harm the current red-teaming method. \n\n2. Commercial LLM hosting stacks vary. It is assumed that the attacker knew the particular caching/scheduling behavior of LLMs. An analysis of robustness of the attack across different caching design might strengthen the paper."}, "questions": {"value": "Please address the suggestions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pwgVh2fPez", "forum": "LmGSD6CedJ", "replyto": "LmGSD6CedJ", "signatures": ["ICLR.cc/2026/Conference/Submission11388/Reviewer_zDf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11388/Reviewer_zDf7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866918230, "cdate": 1761866918230, "tmdate": 1762922509569, "mdate": 1762922509569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}