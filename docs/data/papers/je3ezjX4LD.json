{"id": "je3ezjX4LD", "number": 18815, "cdate": 1758291067386, "mdate": 1759897079873, "content": {"title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "abstract": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines a insertion-based Edit Flows for discrete text tokens and Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.", "tldr": "diffusion-based multimodal model for variable-length, concurrent, interleaved text-image generation", "keywords": ["multimodal", "flow matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/817379cd06ca1a03d8617c87b9afb02dfc25a07d.pdf", "supplementary_material": "/attachment/1645bf5a892d3c0ba4fb7a7470fadbc736100307.zip"}, "replies": [{"content": {"summary": {"value": "OneFlow is a non-autoregressive multimodal generation framework that jointly handles variable-length text and an arbitrary number of images by combining (i) an insertion-based Edit Flows procedure for discrete tokens and (ii) Flow Matching for image latents. A novel interleaved time schedule couples per-image generation times to the text insertion process so images can be inserted and denoised concurrently with text rather than waiting until an image is completed."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel Integration: The work integrates Edit Flows (insertion operations) with continuous Flow Matching for image latents in a single backbone so both modalities are predicted by the same model and can be denoised jointly.\n\nRigorous Evaluation: The paper conducts rigorous experiments using a diverse set of benchmarks, metrics, and baselines to thoroughly evaluate the effectiveness of its proposed method. Also the appendix is really rich.\n\nEmergent Reasoning: The observation that OneFlow develops implicit reasoning chains (Figure 5, 19) is a fascinating finding from my point of view.\n\nWriting: The paper is well-written, and the methodology is explained with remarkable clarity and it was so easy for me to follow."}, "weaknesses": {"value": "Ablation clarity for interleaved schedule: The interleaved schedule is central. I would like to see ablations for it. The paper describes κ(t)=t (linear) and claims it works well, but sensitivity analysis is missing.\n\nPotential mode of failure for complex interleavings: Examples show 2 images interleaved with text. It is unclear how the model behaves when the number of inserted images is large, or when images must be heavily conditioned on earlier generated text.\nLimited operation diversity in Edit Flows: The model only supports insertion operations, omitting deletion and substitution — both fundamental to edit-based generative modeling. (they are even present in the Original Edit Flow Paper)"}, "questions": {"value": "1- It would be insightful to further evaluate the model’s ability to handle compositional generation and compositional visual question answering, and to benchmark its performance against comparable multimodal baselines in these challenging settings. (e.g. attribute binding, missing entities etc.)\n\n2- What modifications would be needed to extend OneFlow to other modalities like audio or video, given its reliance on Edit Flows for discrete elements and Flow Matching for continuous ones?\n\n3- Beyond the linear κ_t scheduler, what alternatives were tested, and why did linear perform best? Could adaptive schedules further reduce FLOPs while maintaining performance?\n\n4- The paper shows that CFG for text increases detail but also the chance of hallucinations (Figure 11). A more systematic analysis of this trade-off (e.g., a plot of detail vs. hallucination rate across CFG scales) would be beneficial.\n\n5- Please include qualitative examples of failed generations (misplaced images, inconsistent text-image references, incoherent insertions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQ0Dzg1t2p", "forum": "je3ezjX4LD", "replyto": "je3ezjX4LD", "signatures": ["ICLR.cc/2026/Conference/Submission18815/Reviewer_wDyu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18815/Reviewer_wDyu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801016493, "cdate": 1761801016493, "tmdate": 1762999984570, "mdate": 1762999984570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their constructive feedback on inference speed, hallucination analysis, and scheduler ablations.\n\n## KV Cache\nThe lack of KV caching is inherent to all bidirectional attention models, not specific to OneFlow. We explicitly acknowledge this limitation, whereas many papers omit this detail. As noted in Muddit (Section 4.5) and the survey on discrete diffusion models (Section VI.C), bidirectional attention prevents theoretically lossless caching since key-value pairs must be recomputed when attending to updated tokens.\n\n**Computational Complexity Analysis:**\nAs shown by Muddit's inference complexity analysis:\n- AR with KV Cache: O(L²D)\n- AR without KV Cache: O(L³D) total FLOPs\n- OneFlow and Discrete Diffusion Models: O(TL²D), where T ≪ L (typically 10-50 vs 512+ tokens)\n\nWhere L is the sequence length, D is the model dimension, and T is the number of sampling steps.\n\n**Performance Trade-offs:**\nWhile bidirectional attention prevents KV caching (a fundamental limitation shared by all discrete diffusion models), OneFlow compensates through substantially fewer sampling steps. We compare against AR with KV cache to present the strongest baseline. Our results show OneFlow achieves AR-level performance using only 6 sampling steps (Figure 16), demonstrating competitive wall-clock time despite the caching limitation. Against AR without KV cache or other non-autoregressive models, OneFlow would be significantly faster. Developing efficient caching mechanisms for bidirectional diffusion models remains an active research area and important future direction.\n\n## Hallucination Analysis\nCFG scaling creates a controllable precision-detail tradeoff. At lower CFG values (0.0-0.8), OneFlow maintains lower hallucination rates than AR (CHAIRs: 2.5-2.9 vs 3.0) while producing shorter captions (Rating: 5.37-5.51 vs 5.60). At CFG=1.0, OneFlow matches AR's caption detail (Rating: 5.61) with comparable hallucination rates (CHAIRs: 3.1 vs 3.0, CHAIRi: 2.4 vs 2.3). Higher CFG values (1.4-2.0) generate more detailed captions but increase hallucinations, providing users explicit control over the precision-detail tradeoff based on application requirements.\n\n|  | CHAIRs (lower) | CHAIRi (lower) | Rating (higher) |\n| :---- | :---- | :---- | :---- |\n| AR | 3.0 | 2.3 | 5.60 |\n| **OneFlow** |  |  |  |\n| CFG 0.0 | 2.5 | 1.7 | 5.37 |\n| CFG 0.4 | 2.7 | 1.8 | 5.48 |\n| CFG 0.8 | 2.9 | 2.1 | 5.51 |\n| CFG 1.0 | 3.1 | 2.4 | 5.61 |\n| CFG 1.4 | 5.6 | 3.9 | 5.53 |\n| CFG 2.0 | 7.5 | 5.1 | 5.44 |\n\n## K Scheduler Ablations\nWe ablated over different k-scheduler variants as described in DFM (Gat et al., 2024): linear, quadratic, and cubic. Results demonstrate that the linear scheduler (degree 1.0) achieves the best performance across both CIDEr (129) and ROUGE_L (0.59) metrics. Performance degrades monotonically with higher polynomial degrees, with cubic scheduling showing substantial drops (CIDEr: 111, ROUGE_L: 0.55). This suggests that uniform token insertion rates are more effective than accelerated scheduling for our multimodal generation task. Full ablations will be included in the camera-ready version.\n\n| K Scheduler Degree | CIDEr | ROUGE_L |\n| :---- | :---- | :---- |\n| 1.0 | 129.25 | 0.5934 |\n| 2.0 | 124.23 | 0.5821 |\n| 3.0 | 111.73 | 0.5588 |\n\n## Paper Focus\nOur paper's primary contribution is demonstrating that non-autoregressive generation can match autoregressive performance in multimodal settings through rigorous controlled experiments. We train our own AR baseline with identical data, architecture, and training procedures to enable fair apples-to-apples comparison, rather than comparing against external models with different training conditions. This controlled setup allows us to isolate the impact of the generation paradigm itself.\n\n## References\n- Shi et al., 2025: \"Muddit: Liberating generation beyond text-to-image with a unified discrete diffusion model\" (Section 4.5)\n- Discrete Diffusion Survey: \"Discrete Diffusion in Large Language and Multimodal Models: A Survey\" (Section VI.C)\n- Gat et al., 2024: \"Discrete flow matching\""}}, "id": "IP05BrqM7h", "forum": "je3ezjX4LD", "replyto": "je3ezjX4LD", "signatures": ["ICLR.cc/2026/Conference/Submission18815/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18815/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18815/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763509285675, "cdate": 1763509285675, "tmdate": 1763509335896, "mdate": 1763509335896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OneFlow, the first non-autoregressive (NAR) multimodal model designed for variable-length and concurrent mixed-modal (text-image) generation. It addresses key limitations of existing autoregressive (AR) and diffusion-based multimodal models: AR enforces rigid sequential generation (preventing cross-modal refinement), while diffusion models only support fixed-length, pre-specified text-image pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. OneFlow fills a critical gap in multimodal generation by pioneering an NAR framework for concurrent, variable-length interleaved text-image generation. The integration of Edit Flow and Flow Matching into a unified transformer backbone is non-trivial: it avoids modality-specific silos and enables cross-modal dependency modeling during generation.\n\n2. The model’s training efficiency is a standout advantage: by using a linear deletion scheduler that retains only 50% of tokens during training, OneFlow reduces training FLOPs by up to 50% compared to AR baselines.\n\n3. OneFlow enables practical, previously unachievable use cases: Unlike AR models that append images to the end of text, OneFlow inserts images dynamically within text (via <|image|> tokens) and refines them simultaneously; Without CoT prompting or RL post-training, OneFlow generates reasoning chains for visual questions (e.g., object counting, visual search), demonstrating that NAR architectures can support complex reasoning."}, "weaknesses": {"value": "1. High Inference Cost: OneFlow lacks key-value caching (due to bidirectional attention), leading to higher inference latency and memory usage than AR models. This limits its applicability to low-latency scenarios (e.g., real-time multimodal chatbots). No preliminary optimizations (e.g., semi-autoregressive decoding, sparse attention) are proposed to mitigate this.\n\n2. Incomplete Comparisons to SOTA Models: While the paper compares OneFlow to VQA baselines like Show-O (1.3B), Janus-Pro (1.5B/7B), and MMaDA (8B), it omits critical recent SOTA models that have set new benchmarks in visual question answering. Specifically, there is no comparison to Show-O2, Mogao, or BAGEL; For image generation (Table 1), the paper evaluates OneFlow against AR models (e.g., Transfusion, Janus-Flow) and diffusion models (e.g., MMaDA, FUDOKI) but lacks comparisons to some SOTA works: Show-O and DreamLLM."}, "questions": {"value": "1. For the VQA task, you omit comparisons to recent SOTA models like Show-O2, Mogao, and BAGEL. Do you have preliminary results comparing OneFlow to these models on key VQA benchmarks (e.g., VQAv2, GQA, DocVQA)? \n\n2. You note that OneFlow’s lack of KV caching increases latency. Have you explored lightweight optimizations (e.g., semi-autoregressive block decoding, sparse bidirectional attention, or model distillation) to reduce inference cost?\n\n3. You finetune on 512×512 images, but many SOTA models (e.g., SD3) support higher resolutions (1024×1024). Has OneFlow been tested on higher-resolution image generation, and if so, how does performance (e.g., FID, detail) scale with resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ubXzTdGWWy", "forum": "je3ezjX4LD", "replyto": "je3ezjX4LD", "signatures": ["ICLR.cc/2026/Conference/Submission18815/Reviewer_qxm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18815/Reviewer_qxm9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837330228, "cdate": 1761837330228, "tmdate": 1762999984191, "mdate": 1762999984191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OneFlow, the first non-autoregressive multimodal model that enables concurrent and variable-length text-image generation by combining an insertion-based \"Edit Flow\" for text with \"Flow Matching\" for images, which outperforms autoregressive baselines with greater efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This is a very solid work from both a technical and an engineering perspective. It skillfully combines the recent, popular diffusion text generation techniques with the consistently effective diffusion image generation methods within a single Transformer. I believe this is a strong contribution to both the DLMs and the unified model communities.\n\n- The paper presents comprehensive and rigorous experiments. This includes scaling experiments from 1B to 8B parameters, extensive comparisons against baseline models (for both multimodal understanding and generation), and supplementary ablation studies in Appendix F. This significantly increases the credibility of the paper."}, "weaknesses": {"value": "This paper frequently mentions two terms: \"interleaved mixed-modal generation\" and \"concurrent mixed-modal generation.\" My concerns are primarily centered on these two points.\n\n- Regarding Interleaved Generation: In the introduction, the authors highlight OneFlow's ability to perform interleaved generation with a variable number of outputs as a major contribution. However, in the experiments, the paper only evaluates OneFlow's performance on multimodal understanding and image generation. It notably lacks evaluations on interleaved generation benchmarks, such as OpenING. I believe this experimental omission fails to substantiate the claims made in the introduction.\n\n- Regarding Concurrent Generation: Similarly, the introduction categorizes concurrent mixed-modal generation as a novel capability for unified models. However, after reading the paper, I still do not understand why we need concurrent generation. Is it simply because it leads to better performance (as suggested in Figure 3)? Is there a deeper insight or analysis behind this capability that I missed?"}, "questions": {"value": "- Will the OneFlow model weights and modeling files be open-sourced? If subsequent researchers cannot build upon this work, it would be detrimental to the unified model community.\n- It appears the authors used closed-source fine-tuning data (Line 267). This could potentially impact the reproducibility of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nan"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "857Lys5ibG", "forum": "je3ezjX4LD", "replyto": "je3ezjX4LD", "signatures": ["ICLR.cc/2026/Conference/Submission18815/Reviewer_rS7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18815/Reviewer_rS7v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917447190, "cdate": 1761917447190, "tmdate": 1762930288057, "mdate": 1762930288057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OneFlow, a novel non-autoregressive multimodal model that unifies text and image generation within a single framework and overcomes key limitations of existing autoregressive and diffusion models by combining Edit Flows for variable-length discrete text generation and Flow Matching for continuous image generation. This allows for the concurrent and interleaved generation of variable-length text and a variable number of images, enabling capabilities like simultaneous cross-modal refinement."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed OneFlow model requires fewer training FLOPs than compared to AR models, thanks to its non-autoregressive, insertion-based training which only predicts missing tokens.\n- The paper provides extensive experiments showing that OneFlow is competitive with or superior to state-of-the-art AR and diffusion models across a diverse range of benchmarks for both image generation and understanding.\n- The paper convincingly demonstrates new emergent capabilities, such as implicit reasoning chains without Chain-of-Thought prompting, the application of classifier-free guidance to improve text detail, and the dynamic insertion and denoising of images within a text sequence."}, "weaknesses": {"value": "- While the integration of Edit Flows with Flow Matching is innovative and effective, the core architectural contribution is the combination of these existing techniques for a new multimodal task. The paper's impressive results are therefore heavily reliant on the pre-established Edit Flows framework, and the fundamental methodological novelties introduced beyond this combination are more incremental.\n- As stated by the authors, the bidirectional attention required for non-autoregressive generation prevents the use of key-value caching, making OneFlow's inference slower and more memory-intensive than cached AR sampling, despite requiring fewer steps.\n- The 20% mixed-generation probability is used without justification. The performance sensitivity to this key hyperparameter is also unknown, making it hard to determine the optimal data mixture.\n- Certain details about hyperparameters are omitted and could use ablations, see Q1 (Ablation on Scheduler) and Q2 (t-Independent Parameterization).\n\nFormatting Concerns:\n- Table captions must precede the tables.\n- Although clear from the context, the abbreviation VQA is not explicitly clarified."}, "questions": {"value": "- Can you elaborate more on the different candidates for κt (line 112) and provide more information?\n- The decision to use a t-independent model is noted to work better in practice despite the theoretical justification for t-dependence. Could you provide an ablation quantifying the performance gap between these two parameterizations? Were there specific tasks where the t-dependent model performed better?\n- As shown in Figure 11 and discussed in Sections 3.5 and 4, higher CFG weights consistently lead to longer, more detailed captions but also to an increased chance of hallucinations. Can you discuss this trade-off in more depth?\n- The emergent reasoning chains (line 367) are a fascinating finding. Can you elaborate on the conditions or training data that you believe led to this behavior? Is it a general property of the Edit Flow text generation, or is it specific to the multimodal pretraining mixture used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OSZl3yEK3I", "forum": "je3ezjX4LD", "replyto": "je3ezjX4LD", "signatures": ["ICLR.cc/2026/Conference/Submission18815/Reviewer_SeK2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18815/Reviewer_SeK2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997581975, "cdate": 1761997581975, "tmdate": 1762930245137, "mdate": 1762930245137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}