{"id": "HVShyTzRgy", "number": 19958, "cdate": 1758300967647, "mdate": 1759897010381, "content": {"title": "Circuit Distillation", "abstract": "Model distillation typically focuses on behavioral mimicry, where a student model is trained to replicate a teacher's output while treating its internal computations as a black box. In this work we propose an alternative approach: Distilling the underlying computational mechanisms implemented by a teacher model. Specifically, we propose circuit distillation, which  introduces an objective to align internal representations between  analogous circuit components in teacher and student models. We propose a method to match ``functionally correspondent'' circuit components and introduce a loss reflecting similarities between the representations that these induce.  We evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks using models from the Llama3 family. Our results demonstrate that circuit distillation outperforms standard  distillation, successfully transferring algorithmic capabilities by adjusting only a small, targeted subset of  student model parameters. This work establishes the feasibility of transferring mechanisms, which may in turn allow for efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms.", "tldr": "Moving beyond behavioral mimicry to distill computational mechanisms in language models", "keywords": ["model distillation", "knowledge distillation", "interpretability", "entity tracking", "retrieval"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b369e97c1c3fecf8da75c21de78776d620d98637.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Circuit Distillation, a novel approach to model distillation that transfers mechanisms rather than behaviors. Instead of aligning full-model outputs, the method aligns specific task-related circuits between teacher and student networks. Using CKA-based similarity and ablation-derived functional mapping, the paper demonstrates that aligning these circuit heads improves transfer efficiency on two tasks: entity tracking and theory of mind."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of transferring mechanisms, rather than just outputs, is innovative yet simple\n\n- The ablation-based mapping of student and teacher heads is simple\n\n- Demonstrations on two nontrivial tasks (entity tracking and ToM) using Llama models go beyond toy examples and toy models; the ToM case is especially challenging.\n\n- The approach makes a clear connection between interpretability research and practical distillation, hinting at possible efficiency and control benefits."}, "weaknesses": {"value": "- Maun concern: The method distills task-specific circuits, not general capabilities. This limits its practical impact compared to standard knowledge distillation, which aims to transfer the global behavior of the teacher model.\n\n- Since only single-task circuits are transferred, it’s unclear how multiple mechanisms would interact or accumulate—this is the most interesting setting.\n\n- Initialization assumption: The author assumes the student already has a comparable circuit structure and is not randomly initialized, which is sometimes not the case in distilation. \n\n- In all the experiments, the teacher–student gap is relatively small, making it hard to evaluate the results. Potentially, as a result, the gap between full distillation and circuit distilation is very small and not very impressive. unclear if the initial larger gap will make a better separation, or if both methods will still perform similarly.\n\n- The paper does not test how distilling one circuit interferes with other capabilities of the student.\n\n- Ablation-based mapping may not guarantee true functional correspondence. Heads with different functionalities can still have a similar effect on the performance degradation of the output when ablated."}, "questions": {"value": "- Can multiple circuits be distilled jointly, or would they interfere?\n\n- How does circuit distillation affect performance on unrelated tasks?\n\n- Could this framework be applied in settings where the student is trained from scratch, without pre-existing circuits?\n\n- Section 2.2 leaves ambiguity about whether the “performance” metric refers to final accuracy or the probability vector; clearer definitions would help.\n\n- Minor comments:\n\nFigure 2 caption: missing “to” in the phrase “to do” (second line)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEsULNQ0nQ", "forum": "HVShyTzRgy", "replyto": "HVShyTzRgy", "signatures": ["ICLR.cc/2026/Conference/Submission19958/Reviewer_8ybk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19958/Reviewer_8ybk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882674193, "cdate": 1761882674193, "tmdate": 1763000000139, "mdate": 1763000000139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Circuit Distillation, a method that aims to transfer not only the output behavior of a large language model but also its internal reasoning mechanisms to a smaller student model. The approach first identifies functional “circuits” in the teacher model (i.e., specific groups of attention heads responsible for tasks like entity tracking or theory-of-mind reasoning), and then finds corresponding components in the student through ablation impact similarity and CKA-based representation alignment. During training, the student jointly optimizes standard cross-entropy loss and a circuit-alignment loss, encouraging it to reproduce the teacher’s internal computational patterns. Experiments on reasoning benchmarks show that Circuit Distillation outperforms traditional model distillation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes circuit distillation, which goes beyond traditional behavioral distillation by aligning internal circuit-level computations between teacher and student models\n2. The formulation of the method (ablation-based mapping + CKA-based alignment) is well-defined\n3. Results on entity tracking and theory of mind convincingly demonstrate that aligning internal circuits yields better transfer of algorithmic capabilities than standard distillation"}, "weaknesses": {"value": "1. The paper is quite difficult to follow. Key concepts such as “circuit mapping”, “ablation impact similarity”, and “CKA alignment” are introduced with minimal intuition or motivating examples\n2. The ablation-based mapping between teacher and student heads is heuristic and potentially unstable\n3. Computing ablation impacts for all heads across teacher and student is computationally heavy, and the paper does not report cost, runtime, or scalability to larger architectures\n4. Experiments are confined to small-scale Llama 1B–8B models and narrow cognitive tasks"}, "questions": {"value": "1. What is the cost of computing ablations in circuit distillation?\n2. Could the method scale to bigger sizes and different architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pBD2jQcWVu", "forum": "HVShyTzRgy", "replyto": "HVShyTzRgy", "signatures": ["ICLR.cc/2026/Conference/Submission19958/Reviewer_Mq9k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19958/Reviewer_Mq9k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957466413, "cdate": 1761957466413, "tmdate": 1763000000502, "mdate": 1763000000502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to improve distillation of neural models by attempting to align the internal mechanisms (“circuits”) of the student and teacher models, rather than merely aligning outputs without the benefit of information about internal mechanisms. Specifically, the authors propose methods for “circuit distillation” which aim to change the student learning objective from output-level mimicry to direct alignment of internal circuits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The intuition behind circuit distillation is sensible and compelling.\n\nThe authors examine the performance of various system components (e.g., CE, Align CKA, Random CKA) alone and in combination, to identify what components are necessary for that performance."}, "weaknesses": {"value": "The introduction is somewhat difficult to follow, and does not immediately make a strong case for model distillation. The paper would be improved by a clearer and more compelling case for when model distillation is useful.\n\nThe testing is limited to only Entity Tracking and ToM. This is an extremely limited form of testing. The authors know this, and explain why, but it is still overly very limited evidence for effectiveness of the proposed methods.\n\nAs the authors note, the method assumes “the prior identification of a well-defined circuit in the teacher model” and “finding such circuits remains challenging and is an ongoing topic of research in mechanistic interpretability.” However, the basic approach outlined in the paper will only become more useful as (presumably) better methods for identifying circuits are developed."}, "questions": {"value": "(none)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Hz5czZHSO9", "forum": "HVShyTzRgy", "replyto": "HVShyTzRgy", "signatures": ["ICLR.cc/2026/Conference/Submission19958/Reviewer_QV1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19958/Reviewer_QV1G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762391456428, "cdate": 1762391456428, "tmdate": 1763000000583, "mdate": 1763000000583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"circuit distillation\" as an extension of traditional distillation method. For a fixed task, circuit distillation modifies the student's learning objective to both mimic the behavior of the teacher and the mechanisms used by the teacher to solve the task. A new metric is proposed to measure the distance between mechanisms across two models. A host of distillation experiments are performed using the Llama family of models (with sizes ranging from 1B to 8B) on two tasks: entity tracking and theory of mind. The paper finds that circuit distillation outperforms naive behavior distillation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Overall, I found the paper easy to follow; the presentation is clear, and the treatment is relatively concise. Given existing literature on \"mechanism\" transfer across different tasks [Merullo et al. 2024](https://arxiv.org/abs/2310.08744), the motivation of the problem to me is very clear. The ablations explored make a convincing argument that the proposed circuit distillation is effective at transferring capabilities from the teacher to the student. Concretely, \n- [Lines 40-45]: The motivation for the method is clearly and convincingly stated. \n- [Table 1, 2]: Thorough ablations with the addition of the similarity metric are performed which demonstrate that the addition of circuit distillation unlocks more performance and alignment between the student and teacher. \n- [Sections 4.1, 4.2]: Two different tasks that vary in both complexity and skill set are explored (entity tracking and theory of mind). This demonstrates that the method could be scaled to tasks that are even more complex."}, "weaknesses": {"value": "## Major Issues\n\nThere are several issues with the paper in its current form.\n\nMy main qualm with the paper is its novelty. Two main contributions the authors claim are (1) \"circuit *distillation*...modifies objective...to direct alignment of internal circuits\" [Line 53]; and, (2) \"a functional circuit mapping strategy\" [Lines 70-72]. Both of these ideas have been extensively discussed in the mechanistic interpretability community and I believe there may be a lapse in the authors' literature review. More specifically, \n- *Causal Abstraction* is essentially a mapping between fine-granularity components in one model (in the author's case, the teacher model) and coarse-granularity components (student model) that preserves the underlying algorithm. [Geiger et al. 2021](https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf); [Geiger et al. 2025](https://www.jmlr.org/papers/volume26/23-0058/23-0058.pdf) formulate this mathematically and works such as [Geiger et al. 2023](https://arxiv.org/abs/2303.02536), [Gupta et al. 2024](https://arxiv.org/abs/2407.14494), [Sun et al. 2025](https://arxiv.org/abs/2503.10894) devise practical algorithms to learn these mappings. \n- The mapping $\\mathcal{M}$ that the authors propose in [Lines 187-199] is simply a special case of causal abstraction. Also, that this mapping needs to obey transformation invariances specified in [Lines 103-107] (orthogonal transformation, isotropic scaling) has also been considered by [Geiger et al. 2023](https://arxiv.org/abs/2303.02536) (see Definition 3). \n- *Distilling circuits from one model to another.* The work of [Gupta et al. 2024](https://openreview.net/pdf?id=R9gR9MPuD5), I believe is another important omission. The authors take a model with a known circuit and train another model (of potentially different architecture) from scratch such that the circuits across both models are equivalent. Although the setup is slightly different, the ideas of causal abstraction are used to locate and enforce an adaptive alignment map. See [Gupta et al. 2024](https://openreview.net/pdf?id=R9gR9MPuD5) (see Algorithm 1), the authors also break the loss into two distinct parts: behavioral alignment which they call $L_{\\mathrm{behavior}}$ , and circuit alignment which they call $L_{\\mathrm{SIIT}}$ and $L_{\\mathrm{IIT}}$.\n\n\nThe setup assumes that the student model is \"warm\" in that it can perform the task reasonably well before distillation. This assumption is not reasonable because often we are trying to distill capabilities from the teacher to the student that the student may not have learned before. \n- The circuit on the student model needs to be identified [Lines 196-197]. But, a non-vacuous, unique circuit on the student model only exists if the student has some non-trivial performance on the target task; otherwise $\\Delta P_s(h_s)$ could be constant for all $h_s$ and $d_{\\mathrm{abl}}$ would be not informative ([Heimersheim and Nanda 2024](https://arxiv.org/pdf/2404.15255)).  \n- Further, the ablation studies the authors perform support this claim, as they demonstrate that it is necessary for the student circuit to be identified and paired properly with the teacher circuit for circuit distillation to achieve high performance (Table 1, 2, rows CE + Rand CKA). \n## Minor Issues/Nitpicks\n\n- The problem setup assumes that the teacher model must be whitebox since we need to identify its circuits and grab its internal representations during training [Lines 235-240]. This is much stronger than only requiring logits from the teacher model. Thus, it is unclear to me that knowledge distillation is a fair baseline comparison. \n- Why define Equation 3, 4? They are never reference again in the main text and Equation 5 can be defined equivalently as $$d_{\\mathrm{abl}}(h_s , h_t) = | P_{s\\_\\mathrm{abl}}(h_s) - P_{t\\_\\mathrm{abl}}(h_t) |.$$ \n- The authors claim that \"our findings indicate that it is feasible to distill not just knowledge, but also the algorithms that produce it\" [Lines 79-80]. I believe that this is an over-claim. The method only aligns individual components (where \"align\" is defined as aligning their representations). It's unclear\n\t- The circuits are even the same (i.e. ablating $h_s$ in the student model and $\\mathcal{M}(h_s)$ in the teacher model result in the same drop in performance **after** circuit distillation).\n\t- Even if the circuits were the same, it seems quite a leap to go from individual circuit component equivalences to algorithmic equivalence (see Section 2.4 in [Heimersheim and Nanda 2024](https://arxiv.org/pdf/2404.15255))."}, "questions": {"value": "* How would circuit distillation generalize to much more complex tasks like next-token prediction? In such a case, averaged over all next tokens the teacher circuit could be the entire model. How would $M$ nontrivially change to account for this?\n* To clarify, on [Lines 233-234] when the authors say that \"$y$ denoting the teacher labels,\" they mean that $y$ is the logits from the teacher model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JYRl0IenME", "forum": "HVShyTzRgy", "replyto": "HVShyTzRgy", "signatures": ["ICLR.cc/2026/Conference/Submission19958/Reviewer_Dpyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19958/Reviewer_Dpyn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762636729367, "cdate": 1762636729367, "tmdate": 1763000000574, "mdate": 1763000000574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}