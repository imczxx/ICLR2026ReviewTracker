{"id": "6tx4BGjwJP", "number": 3948, "cdate": 1757572039858, "mdate": 1763723279453, "content": {"title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "abstract": "In this paper, we propose UniLIP, a unified framework that adapts CLIP for multimodal understanding, generation and editing. Although CLIP excels at understanding, it lacks reconstruction abilities required to be a unified visual encoder. However, previous CLIP-based unified methods fail to balance understanding and reconstruction, leading to semantic degradation or inconsistent reconstructions. In contrast, we introduce a novel two-stage training scheme with a self-distillation strategy that progressively endows CLIP with high-fidelity reconstruction abilities while preserving its original comprehension performance. For enhanced reasoning and consistency in generation and editing, we further develop a dual-condition architecture built upon the MetaQuery framework. Our architecture jointly utilizes multimodal hidden states for rich contextual details and learnable query embeddings to harness the powerful reasoning abilities of Multimodal Large Language Models (MLLMs). Leveraging advanced image representation and architectural design, UniLIP demonstrates superior instruction following and edit fidelity. With only 1B and 3B parameters, UniLIP can outperform larger unified models such as BAGEL (7B) and Uniworld-V1 (12B), achieving state-of-the-art performance of **0.90** on GenEval, **0.63** on WISE, and **3.94** on ImgEdit. These results demonstrate that UniLIP successfully expands the application of CLIP, establishing its continuous features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks. Code and models will be publicly available.", "tldr": "", "keywords": ["Unified Multimodal Understanding and Generation", "Image Editing", "CLIP"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2faa220068cbaced6bb60aa9a9ae9a5aa2b2d93e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UniLIP, a unified framework that extends CLIP to support multimodal tasks, including understanding, generation, and editing. UniLIP incorporates a two-phase training methodology featuring self-distillation, which enhances CLIP's capabilities for high-fidelity reconstruction without compromising its baseline comprehension performance. To improve reasoning and ensure consistency in generation and editing tasks, the authors have innovatively implemented a dual-condition architecture within the MetaQuery framework. Despite its relatively smaller size with 1B and 3B parameters, UniLIP demonstrates superior performance over larger models such as BAGEL (7B) and Uniworld-V1 (12B), achieving benchmark scores of 0.90 on GenEval, 0.63 on WISE, and 3.94 on ImgEdit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper effectively addresses the limitations in CLIP's reconstruction capabilities through a two-stage training process, transforming it into a unified visual encoder.\n2. This proposed dual-condition architecture utilizes the multimodal hidden states and query embeddings as the input, which can effectively avoid information loss while leveraging the reasoning capabilities of LLMs through query embeddings. \n3. This paper conducts extensive experiments on understanding, generation, and image editing. With only 1B and 3B parameters, UniLIP outperforms larger unified models such as BAGEL (7B) and Uniworld-V1 (12B), thereby demonstrating the efficacy of the proposed method."}, "weaknesses": {"value": "1. Given that the dual-condition architecture is based on MetaQuery, the novelty appears limited.\n2. In Tab.6, the performance improvement in understanding appears to be primarily due to self-distillation. Additionally, \"Lr Decay\" seems to have a more significant impact compared to \"Two Stage.\"\n3. The ablation studies in Table 6 are not thorough. For instance, they fail to convincingly demonstrate the effectiveness of each stage in the two-stage reconstruction training."}, "questions": {"value": "1. After undergoing two-stage reconstruction training, how does UniLIP perform on downstream tasks such as linear probe or attentive probe within the CLIP framework?\n2. Has the study explored any self-distillation losses other than MSE loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yygxyG1AH0", "forum": "6tx4BGjwJP", "replyto": "6tx4BGjwJP", "signatures": ["ICLR.cc/2026/Conference/Submission3948/Reviewer_E7dY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3948/Reviewer_E7dY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761112678316, "cdate": 1761112678316, "tmdate": 1762917108259, "mdate": 1762917108259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniLip, a novel two-stage training scheme for CLIP to capture both semantic information and pixel details, enabling unified multimodal understanding and generation. The scheme finetunes the CLIP model to optimize reconstruction quality while preserving the features in the original CLIP. Moreover, this paper proposes a dual-condition architecture for editing tasks, which supplements hidden states of MLLMs as conditions to provide pixel details."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a training approach for CLIP that effectively balances semantic understanding and pixel-level detail, achieving SOTA performance in various multimodal tasks.\nThe presentation is clear and well-structured, with comprehensive experiments and ablation studies demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "The description of related works for unified multimodal models could be more detailed to better position the contributions of this work.\nThe method requires finetuning for the pixel decoder in both stages; the reason for this design choice is not well explained."}, "questions": {"value": "1. What is the benefit of unified CLIP for both generation and editing tasks, compared to training two separate models for these two tasks respectively to achieve similar performance?\n2. Comparison with BLIP3-o: What is the essential reason for BLIP3-o applying a complex pipeline (described in lines 147-151)?\n3. Why the stage 1, which solely trains the pixel decoder is necessary? Can we directly train both the CLIP and decoder module and skip stage 1? (The benefit of the two-stage is not significant in Table 6, maybe we can just increase the training time in one stage.)\n4. The generation and editing require a different training process; why not describe the training in the method section?\n5. How about other CLIP methods (e.g., BLIP3-o mentioned in the paper) perform on the reconstruction tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FZbJnGZ2IP", "forum": "6tx4BGjwJP", "replyto": "6tx4BGjwJP", "signatures": ["ICLR.cc/2026/Conference/Submission3948/Reviewer_6SkM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3948/Reviewer_6SkM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716899054, "cdate": 1761716899054, "tmdate": 1762917107889, "mdate": 1762917107889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified MLLM for understanding and generation. Through self-distilling, the inherent strong understanding ability of CLIP enoder is well-preserved with boost in reconstruction ability. Leveraging the proposed dual-condition framework that combines query embeddings and MLLM latents, the reasoning ability of MLLMs can be transfered to downstream generation and editing tasks, enhancing instruction following and complex semantics comprehension. Meanwhile, pixel details can be preserved thanks to the reserved MLLM latents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed pipeline extends several previous works and is simple yet effective. Finetuning CLIP through self-distillation makes sense and proves effective, while direct optimization for reconstruction frustrates understanding. The dual-condition of mulitimodal hidden states and query embeddings makes downstream diffusion transformer performs better than using either. Comparisons among state-of-the-art models are conducted and analyses are convincing.\n1. Rigorous Ablation of the Two-Stage Training Strategy: The authors provide extensive ablation studies on the two-stage training approach and the self-distillation strategy. These experiments are crucial, demonstrating that directly fine-tuning CLIP for reconstruction causes catastrophic forgetting of comprehension capabilities. The ablation results (Table 6) confirm that the self-distillation loss is essential, preventing the understanding score (MMBench) from plummeting by 54.2 points. This work rigorously solves the key challenge of adapting CLIP for reconstruction without compromising its original semantic power.\n2. State-of-the-Art Performance Across Unified Benchmarks: UniLIP demonstrates superior performance across understanding, generation, and editing tasks, showcasing that it effectively integrates high-level reasoning and low-level detail."}, "weaknesses": {"value": "1. Insufficient Literature Review on Query Embeddings: The dual-condition architecture relies on query embeddings to connect the MLLM and the diffusion transformer, following precedents like MetaQuery (Pan et al., 2025) and BLIP3-o (Chen et al., 2025a). However, the earliest proposal was made by DreamLLM (Dong et al., ICLR 2024 Spotlight).\n2. Lack of Explicit Architectural Comparison to Dual-Encoder Baselines: UniLIP successfully combines high-level semantics and low-level pixel details by adapting CLIP for reconstruction. This approach yields competitive results, surpassing Janus-Pro (7B) in certain understanding benchmarks (MMBench 80.7 vs 79.2). However, the manuscript does not explicitly compare the necessity of the unified CLIP adaptation (UniLIP) versus a conceptually simpler dual-encoder system (one encoder optimized for high-level tasks like LLaVA setting/understanding, and another specialized encoder for low-level detail/reconstruction). The paper's ablation only proves that the original CLIP is inadequate for reference image encoding in editing, but a dedicated comparison against a generalized dual-encoder setup as a structural baseline would strengthen the argument for UniLIP's architectural unification."}, "questions": {"value": "I am more concerned about comparing and illustrating the importance of the unified encoder with dual encoder baselines like Janus pro."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zmgEmjt5Iv", "forum": "6tx4BGjwJP", "replyto": "6tx4BGjwJP", "signatures": ["ICLR.cc/2026/Conference/Submission3948/Reviewer_eMyq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3948/Reviewer_eMyq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834012791, "cdate": 1761834012791, "tmdate": 1762917107652, "mdate": 1762917107652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UniLIP, a unified multimodal framework that adapts CLIP into a reconstructable and generative visual representation while preserving its strong semantic understanding. UniLIP aims to enhance reasoning and edit consistency by combining multimodal hidden states with learnable queries, and reports state-of-the-art results on image understanding, editing, and generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The two-stage training plus self-distillation scheme for CLIP is well-motivated and appears to strike a good balance between semantic preservation and detail fidelity.\n2. UniLIP demonstrates strong empirical performance across popular benchmarks in image understanding, editing, and generation."}, "weaknesses": {"value": "1. UniLIP's training objective are heavily reconstruction-centric and largely pixel-level, while self-distillation predominantly constrains the representation not to deviate from the original CLIP distribution. This setup may limit the modelâ€™s ability to discover a better feature distribution for both understanding and generation. Why the self-distillation is sufficient for preserving its understanding-centered semantics, will any proxy task such as image classification better than distillation? \n\n2. The ablation of its dual conditioning is not convincing. Were results obtained by removing one of the two conditionings at inference from the same trained UniLIP model, or by training separate model variants that use only one conditioning pathway (multimodal hidden states or learnable queries)?\n\n3. The multimodal hidden states are modeled by the LLM in a casual way, thereby containing reasoning knowledge, why is UniLIP have to use learnable query to further enhance its reasoning knowledge? In what cases do multimodal hidden states alone fail to provide sufficient reasoning, and how do learnable queries remedy this?\n\nThe author could provide an ablation study on different learnable query length, to discover the effectiveness of the learnable query, and how the learnable queries complement the multimodal hidden states, and quantify their complementarity.\n\nThis paper could benefit from a more comprehensive image editing benchmarketing, such as KRIS and RISE"}, "questions": {"value": "Refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hn7qzN75rE", "forum": "6tx4BGjwJP", "replyto": "6tx4BGjwJP", "signatures": ["ICLR.cc/2026/Conference/Submission3948/Reviewer_6pTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3948/Reviewer_6pTW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877688014, "cdate": 1761877688014, "tmdate": 1762917107334, "mdate": 1762917107334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}