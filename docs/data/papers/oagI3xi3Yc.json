{"id": "oagI3xi3Yc", "number": 17973, "cdate": 1758282551095, "mdate": 1759897141710, "content": {"title": "Evo-PI: Scaling Medical Reasoning via Evolving Principle-Guided Reinforcement Learning", "abstract": "Effective reasoning over complex visual data and medical knowledge is critical for medical Visual Question Answering (VQA). While multimodal large language models (MLLMs) show promise, their reasoning capabilities remain fundamentally capped by the static nature of current training paradigms. Existing reinforcement learning (RL) methods act as fixed tutors, providing unchanging guidance that often optimizes output format without explicit medical expertise, leading to performance plateaus and reward hacking. Drawing inspiration from how human experts continuously refine clinical principles, we introduce \\textbf{Evo-PI}, a framework that operationalizes a synergistic loop of evolving principle-guided learning. Evo-PI generates, applies, and iteratively refines abstract medical principles, which serve as dynamic rewards. This co-evolution of the reasoning model and its guiding principles enables MLLMs to develop more robust and clinically aligned reasoning. Across eight medical VQA benchmarks, Evo-PI consistently improves performance over diverse backbones and RL algorithms, achieving up to 24.6\\% accuracy gains. Our results establish evolving principle scaling as a scalable and generalizable paradigm for aligning MLLMs with expert-like reasoning, advancing the path toward trustworthy medical AI.", "tldr": "", "keywords": ["Medical Visual Question Answering", "Medical Reasoning", "Principle-Based Iterative Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6b1e862e37cf7c321ff870a3a41c633bc309536.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1.  **Principle Bank Initialization:** A knowledgeable Large Language Model (LLM) generates an initial set of medical principles based on a few examples and category names (e.g., CT, X-ray).\n2.  **Guided Reinforcement Learning:** The target MLLM is trained using RL (specifically, GRPO or GSPO are mentioned). A separate, frozen \"judge\" LLM evaluates the MLLM's reasoning trace against the current set of principles, generating a \"principle reward\" and a \"thinking point reward\". These are combined with standard RL rewards (accuracy, format) to update the MLLM.\n3.  **Principle Evolution:** Based on the MLLM's performance and training dynamics (monitored via policy entropy), the knowledgeable LLM refines, expands, or prunes the principle set. This updated set is used in the next iteration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core concept of co-evolving principles and the reasoning model is highly novel and contrasts sharply with existing RLHF/RLAIF methods that use fixed reward models or heuristics. The idea of using abstract, evolving principles as reward signals is a significant departure."}, "weaknesses": {"value": "1.  **Complexity and Reliance on External LLMs:** The framework involves a complex interplay between three models (knowledgeable LLM, judge LLM, MLLM backbone). The overall performance is heavily dependent on the quality and reliability of the knowledgeable LLM (for generating/evolving principles) and the judge LLM (for evaluating adherence). The paper uses powerful models (GPT-4O-mini, Qwen2.5-7B) but provides limited analysis on how sensitive the results are to the choice of these models or potential failure modes (e.g., the judge misinterpreting principle adherence, the knowledgeable LLM generating flawed principles).\n2.  **Lack of Direct Principle Validation:** The \"principles\" are abstract and generated by an LLM. There is no external validation (e.g., by human medical experts) to confirm their clinical accuracy or relevance throughout the evolution process. The framework assumes the principles evolved by the LLM are inherently beneficial and correct.\n3.  **Evaluation Limited to Accuracy:** While the motivation includes enhancing reasoning quality and clinical alignment, the primary evaluation metric is accuracy on VQA benchmarks. There is limited qualitative or quantitative analysis of the *reasoning traces* themselves to demonstrate improved clinical validity or reduced hallucination, beyond anecdotal case studies. How does Evo-PI compare on metrics specifically designed to evaluate reasoning faithfulness or factual consistency?\n4.  **Potential for Principle Overfitting:** The principles evolve based on the MLLM's performance within the training loop. This creates a risk that the principles might overfit to the specific biases or failure modes of the MLLM or the training data distribution, rather than converging towards genuinely robust clinical heuristics.\n5.  **Text-Only Judge:** The judge LLM evaluates principle adherence based only on the textual reasoning trace provided by the MLLM. It does not perform its own visual analysis. This means the rewards might reinforce textually plausible reasoning that is not actually grounded in the image evidence.\n6.  **Lack of Benchmarks:** Why there is only OmniMedVQA used?"}, "questions": {"value": "1.  How robust is the framework to the choice of the knowledgeable LLM and the judge LLM? Have you experimented with different models (e.g., smaller open models, models with different medical domain expertise) for these roles, and how does it impact performance and the evolved principles?\n2.  Could you provide more insight into the principle evolution process? How does the knowledgeable LLM decide *how* to refine principles based on \"training dynamics\" or \"good/bad cases\"? Is there a risk of generating incorrect or clinically misleading principles during this automated evolution?\n3.  Beyond accuracy, have you evaluated the quality of the reasoning traces generated by the Evo-PI trained models? For instance, using metrics for factual consistency, hallucination rates, or even expert clinical review of sample reasoning paths?\n4.  The judge LLM is text-only. How does the framework ensure that the reasoning steps rewarded for adhering to principles are actually grounded in the visual evidence, rather than just being textually plausible? Could a multimodal judge improve performance or reliability?\n5.  The paper mentions mitigating reward hacking (Sections 1, 3.4, 3.5). Could you elaborate on the specific mechanisms within Evo-PI designed to prevent the MLLM from simply generating text that mentions principle keywords without demonstrating true understanding, especially given the judge is text-only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YHkuO5E4Ol", "forum": "oagI3xi3Yc", "replyto": "oagI3xi3Yc", "signatures": ["ICLR.cc/2026/Conference/Submission17973/Reviewer_tyP6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17973/Reviewer_tyP6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578619669, "cdate": 1761578619669, "tmdate": 1762927767171, "mdate": 1762927767171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Evo-PI — a framework that teaches a medical VLM to reason through principle evolution. Instead of relying on a static reward model or handcrafted heuristics, Evo-PI keeps a bank of medical “principles” (abstract reasoning rules such as “recognize modality signatures,” “integrate clinical context,” etc.) and iteratively refines them through a knowledgeable LLM (GPT-4o-mini) after every RL phase.\nOverall, I enjoy this idea having a evaluating knowledge database."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well written and clearly organized, making a complex reinforcement-learning framework easy to follow.\n\n- I particularly appreciate the idea of iteratively expanding the knowledge base (principle bank) — this dynamic evolution of domain principles is both intuitive and elegant, and it mirrors how clinicians refine expertise over time.\n\n- The method demonstrates strong and consistent performance improvements across multiple medical VQA benchmarks, showing that the evolving-principle strategy leads to tangible reasoning gains."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "- My main concern is that the framework generates its knowledge base independently for each dataset and modality, resulting in separate sets of principles. While the model’s reported performance improves, it is unclear whether the underlying model itself becomes more capable, or if the gains simply reflect dataset-specific tailoring.\n\n- From my understanding, the authors train and evolve principles per dataset modality. Would it not make more sense to train a single unified model using a combined dataset and a shared evolving principle bank, especially since the baseline comparisons (e.g., Med-R1, HuatuoGPT-Vision) are based on universal backbones?\n\n- Code and model weights are not available. For a method this intricate—combining multiple LLMs, dynamic rewards, and iterative evolution—open-sourcing is essential for verification and reproducibility.\n\n- I suspect the model could generate repetitive or redundant principles as a byproduct of its reward mechanism (“reward hacking”). While you may not need to empirically prove this, a verbal explanation or qualitative analysis of how Evo-PI prevents principle duplication or collapse would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8JTmwuv9Hn", "forum": "oagI3xi3Yc", "replyto": "oagI3xi3Yc", "signatures": ["ICLR.cc/2026/Conference/Submission17973/Reviewer_SsZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17973/Reviewer_SsZW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971163570, "cdate": 1761971163570, "tmdate": 1762927766832, "mdate": 1762927766832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Evo‑PI, a three‑stage pipeline for finetuning medical LLMs on medical VQA consisting of: (1) a Knowledgeable LLM distills “principles” from a few cases to initialize a principle bank; (2) a frozen judge LLM scores each rollout’s <think>…</think> reasoning trace against these principles to yield Principle Reward and Thinking‑Point Reward; (3) the principle set is iteratively refined and the process repeats. Training is done with RLVR (e.g., GRPO/GSPO). Core details: a frozen judge evaluates principle adherence and verifies step‑by‑step support for the final answer; rewards are formalized (Eqs. 3–4) and integrated into a PPO‑style objective (Eqs. 5–7); the rollout prompt enforces <think> and <answer> tags. The evaluation uses OmniMedVQA across eight modalities with Accuracy as the task metric; GPT‑4o‑mini is the Knowledgeable LLM and Qwen2.5‑7B‑Instruct is the judge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and sound algorithm formalization. The rewards are explicitly defined (Principle Reward / Thinking‑Point Reward) and combined with RLVR (GRPO/GSPO) in a standard clipped‑ratio objective with normalization.\n2. Interpretable reasoning representation. The paper enforces <think>…</think> reasoning traces and uses them to compute verifiable‑format rewards and judge checks.\n3. Consistent accuracy gains. On OmniMedVQA across eight modalities and two backbones (HuatuoGPT‑Vision, Med‑R1), the authors report sizable and consistent accuracy improvements (Table 2, §4.2).\n4. Artifacts. Anonymized repo link and detailed prompts are provided in the appendix."}, "weaknesses": {"value": "1. Fair Algorithmic vs. Poor Scientific Soundness\n\na) Algorithmic soundness (internal coherence). The optimization objective is well‑defined; reward clipping/normalization is specified; RLVR training with GRPO/GSPO is standard; iteration control via token‑level entropy is described. On these grounds, the method functions as designed.\n\nb) Scientific soundness (external validity). The paper evaluates “reasoning quality” only via a frozen judge LLM (Qwen2.5‑7B‑Instruct) that compares traces against textual principles and “verifies, step by step” support for the answer; there is no clinician or human validation, no correlation of judge scores with expert ratings or clinical correctness, and no ground‑truth explanations. Thus, process‑quality claims remain self‑referential.\n\n2. Evaluation scope is narrow for a medical‑reasoning claim\na) The paper reports accuracy as the sole task metric; it does not report process‑quality metrics against human judgement (faithfulness, plausibility, possible harm) or expert audits of traces/principles.\n\n3. Unvalidated measurement instrument\na) The judge is frozen and unvalidated for clinical verification; there is no robustness or agreement study (e.g., alternative judges, judge–human correlations), which makes the central measurement instrument insufficiently trustworthy for claims about reasoning quality.\n\n4. Clarity gaps:\n\na) Terminology confusion with Med PI vs Evo-PI. Also, there are many models in this process that don't have distinguishable names, this harms the clarity of Figure 2 discussed below.\n\nb) Figure 2 (the three‑phase loop) is visually crowded, and component names overlap semantically; it’s hard to discern which boxes are fixed vs. learned. A simplified, story‑first schematic would help. (Figure appears with §3.2 discussion.)\n\nc) Compute reporting lists hardware (4×H100 for training; 1×H100 for the judge) but omits GPU‑hours.\n\nd) Table 1 lacks captions.\n\n5. Mismatch between contribution and experiment design.\n\na) This work proposes a paradigmatic contribution, but the experiments don't measure the contribution of the new paradigm against the old (e.g., SFT). Instead, the result deltas only show that Evo-PI can finetune a previously finetuned model to excel on these benchmarks. However, by itself, this is hardly a surprising finding - if a model is finetuned on a benchmark, it is expected to improve its performance.\n\nb) More appropriate experiments could include 1) starting from the baseline models' baseline models and comparing the finetuning impact under the baselines' paradigm vs Evo-PI, or 2) perhaps more easily, finetuning the baseline models under their respective previous paradigms can allow you to compare your existing results against their paradigms.\n\nF. Fair Novelty.\n\na) The paper’s novelty framing (\"Co-Evolution\") is slightly exaggerated. Instead, the contribution is largely representational instead of paradigmatic as claimed—textual, evolving reward specification.\n\nb) Relative to reward modeling (RLHF/RLAIF), Evo‑PI swaps a fixed neural reward regressor for an iteratively refined natural‑language principle set that a frozen judge LLM interprets. This improves transparency/interpretability but preserves the core policy–reward coupling; the novelty lies in representation and authoring of rewards, not in the RL optimizer itself. (This is an evaluative comparison; the paper does not present a RAG or RM baseline.)"}, "questions": {"value": "1. External validation: Can you run a small clinician audit (≈200 items, 2–3 raters) of reasoning traces (faithfulness, evidence‑grounding, plausibility, potential harm) and report inter‑rater agreement? (No human eval is currently reported.)\n\n2. Judge calibration: Do judge scores correlate with human ratings and/or task Accuracy? Please report those correlations on a held‑out slice. (The judge is currently frozen and unvalidated.)\n\n3. Robustness: How sensitive are results to the choice of judge (different families/sizes) and of the Knowledgeable LLM (GPT‑4o‑mini vs. open models)?\n\n4. Causal use of steps: Can you add ablate‑a‑step and counterfactual occlusion tests to show the rewards drop when key evidence/steps are removed, demonstrating you’re incentivizing useful steps rather than verbosity?\n\n5. Why principles vs. RAG? Please include a RAG baseline (textbook/diagnostic guideline retrieval) and optionally RAG+Evo‑PI to show whether evolving principles outperform or complement retrieval.\n\n6. Compute: Please report GPU‑hours and wall‑clock per iteration (and #iterations)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YUq0SfrvpF", "forum": "oagI3xi3Yc", "replyto": "oagI3xi3Yc", "signatures": ["ICLR.cc/2026/Conference/Submission17973/Reviewer_xQku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17973/Reviewer_xQku"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762543634518, "cdate": 1762543634518, "tmdate": 1762927766337, "mdate": 1762927766337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an Evolving Principle–guided reinforcement learning framework (Evo-PI) for medical visual question answering. Evo-PI externalizes domain knowledge into an editable principle bank, uses a frozen judge LLM to convert principle adherence and “thinking-point” checks into verifiable rewards, and iteratively scales/refines the principles across training rounds. Experiments are conducted on the OmniMedVQA benchmark across eight imaging modalities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The target problem is valuable.\n2. The experiments on a benchmark show that the proposal achieves consistent improvements on eight imaging modalities."}, "weaknesses": {"value": "**Major:**\n\n1. The claim of evaluation on “eight medical VQA benchmarks” misleads readers. In fact, the paper only performs experiments on a single benchmark dataset OmniMedVQA (Section 4.1).\n\n2. The paper should extend the experiments to more widely used benchmark datasets, such as: (1) general medical VQA datasets VQA-RAD, SLAKE, PathVQA, and PMC-VQA; (2) medical reasoning benchmarks MMMU (H&M) (Yue et al., 2024) and MedXpertQA (Zuo et al., 2025).\n\n3. The evaluation metric is limited. The paper claims that the proposed method can generate trustworthy, scalable, generalizable, robust, clinically aligned, and expert-like reasoning. However, the paper only reports accuracy for evaluation. Moreover, the usefulness of “principle-guided” explanations in practice should also be assessed. Robustness and OOD stress-testing should be provided as well [Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models, arXiv 2025].\n\n4. More analysis should be provided. The paper reports average gains but does not vary: (i) the number, length, or abstraction level of principles; (ii) iteration count; (iii) judge model size/type; (iv) knowledgeable LLM choice; or (v) masking of the <think> channel. These factors are crucial to cost-effectiveness.\n\n**Minor:**\n\n5. Limited backbone diversity. Although the paper claims the approach is general, both backbones are Qwen-family VL models (7B and 2B variants), and the judge is Qwen2.5-7B-Instruct.\n\n6. Statistical significance tests should be provided, considering the limited evaluation data."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PKb4POL9e8", "forum": "oagI3xi3Yc", "replyto": "oagI3xi3Yc", "signatures": ["ICLR.cc/2026/Conference/Submission17973/Reviewer_YAUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17973/Reviewer_YAUF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762766517115, "cdate": 1762766517115, "tmdate": 1762927765955, "mdate": 1762927765955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}