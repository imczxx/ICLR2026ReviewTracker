{"id": "FXxva35wtF", "number": 12194, "cdate": 1758206256874, "mdate": 1759897526018, "content": {"title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning", "abstract": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning foundation models.  However, federated fine-tuning using LoRA is challenging due to suboptimal updates arising from traditional federated averaging of individual adapters. Existing solutions either incur prohibitively high communication cost that scales linearly with the number of clients or suffer from performance degradation due to limited expressivity. We introduce **Federated Silver Bullet (Fed-SB)**, a novel approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix ($R$) between adapters $B$ and $A$, keeping other components fixed. Direct averaging of $R$ guarantees exact updates, substantially reducing communication cost, which remains independent of the number of clients, and enables scalability. Fed-SB achieves **state-of-the-art performance** across commonsense reasoning, arithmetic reasoning, and language inference tasks while reducing communication costs by up to **230x**. In private settings, Fed-SB further improves performance by (1) reducing trainable parameters, thereby lowering the noise required for differential privacy and (2) avoiding noise amplification introduced by other methods. Overall, Fed-SB offers a state-of-the-art, efficient, and scalable solution for both private and non-private federated fine-tuning. Our code is available anonymously at: https://anonymous.4open.science/r/fed-sb-anonymous-6F3D.", "tldr": "", "keywords": ["LoRA", "Low-rank adaptation", "Fine-tuning", "Federated fine-tuning", "Foundation Models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe25e96e7d3a50011fc7cada728fb1b699cf83de.pdf", "supplementary_material": "/attachment/761c00fb57343376342b8ac17b8b3b28592c4646.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method, Fed-SB, to achieve exact aggregation in federated LoRA fine-tuning without the high communication costs of prior methods. Fed-SB initializes LoRA adapters (B and A) using an SVD-based approximation of the first full fine-tuning update step and then freezes them, training and communicating only a very small $r \\times r$ matrix R between two adaptors. This allows for mathematically exact aggregation through simple averaging of the R matrices, drastically reducing communication cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The core idea is intuitive and easy to follow. \nS2: The method demonstrates strong empirical performance across various benchmarks, while using drastically fewer communication parameters per round."}, "weaknesses": {"value": "W1: The paper lacks clarity on the initialization phase's implementation details. \nW2: Model performance appears highly sensitive to the initial subspace quality, risking a severe performance cap if the approximation phase uses limited or unrepresentative data.\nW3: The constrained update space (only matrix R is trained) raises overfitting concerns to the initial subspace, potentially limiting generalization."}, "questions": {"value": "Q1: Is communication required during initialization? If so, what is its parameter scale, and was this one-time cost included in the communication efficiency analysis?\nQ2: How does initialization robustness perfom with more clients and higher data heterogeneity? In large-scale or highly heterogeneous scenarios, does the method require significantly more data samples or resource overhead to obtain a sufficiently representative initial subspace?\nQ3: How do alternative SVD-based initializations (e.g., LoRA-GA[1]) for the fixed A and B matrices compare against the proposed method within the same B-R-A architecture?\n\n[1] Wang, S., Yu, L., and Li, J. LoRA-GA: Low-Rank Adaptation with Gradient Approximation. In Advances in Neural Information Processing Systems 38 (NeurIPS 2024), 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "le72IeLsM3", "forum": "FXxva35wtF", "replyto": "FXxva35wtF", "signatures": ["ICLR.cc/2026/Conference/Submission12194/Reviewer_H7f5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12194/Reviewer_H7f5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396481690, "cdate": 1761396481690, "tmdate": 1762923142709, "mdate": 1762923142709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Fed-SB, a federated adaptation of LoRA-SB. Fed-SB freezes local low-rank adapters and trains an $r \\times r$ matrix $R_i$ for each client. The server aggregates by averaging only $R_i$. This makes aggregation algebraically exact and keeps per-round communication low. The approach is also compatible with Differential Privacy. The experiments show consistent gain across multiple tasks and multiple LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple and effective.\n2. The experiments show consistent gains across multiple tasks and multiple LLMs."}, "weaknesses": {"value": "1. Despite strong experimental results, the submission lacks the core justification (theoretical or isolating experiments) that would explain why the method works and under what conditions it should be expected to work.\n2. Lack of key ablation studies:\n (1) extreme low-rank regimes (rank-1, rank-2, â€¦), to identify the expressivity threshold below which Fed-SB ceases to be effective; (2) different initializations of LoRA-B and LoRA-A, since LoRA-A, LoRA-B are frozen, initialization may be the dominant factor; (3) experiments involving more clients, such as 30 or more. \n3. Table 2-6 should also report standard deviation."}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V2IFmuHC5k", "forum": "FXxva35wtF", "replyto": "FXxva35wtF", "signatures": ["ICLR.cc/2026/Conference/Submission12194/Reviewer_iqz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12194/Reviewer_iqz4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508240297, "cdate": 1761508240297, "tmdate": 1762923142081, "mdate": 1762923142081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Federated Silver Bullet (Fed-SB), an approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix (R) between adapters B and A, while keeping other components fixed. Fed-SB directly averages R while keeping other components fixed (including A and B), which guarantees exact updates, substantially reduces communication costs, remains independent of the number of clients, and enables scalability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to understand.\n2. The introduced method achieves exact updates for FL with LoRA fine-tuning."}, "weaknesses": {"value": "1. The novelty of this work is limited. The proposed Fed-SB is built upon LoRA-SB [1], which is not an original contribution of this study. Moreover, such strategies have been applied in [2].\n2. The initialization of A and B can significantly impact performance, yet this paper lacks such an analysis, as it directly initializes A and B as orthonormal matrices. Exploring different initializations, such as random initialization or performing SVD decomposition on W_0 and using the decomposed values as the initialization for A and B, is necessary. Similar investigations into how different initializations affect model performance are needed.\n3. Some advanced works are missing and should be included for comparison, such as FlexLoRA [3], FedSA-LoRA [4], FRLoRA [5], and others.\n\n\n[1] Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, and Praneeth Vepakomma. Initialization using update approximation is a silver bullet for extremely efficient low-rank fine-tuning. arXiv preprint arXiv:2411.19557, 2024. \\\n[2] Guo, Wei, Siyuan Lu, Yiqi Tong, Zhaojun Hu, Fuzhen Zhuang, Xiao Zhang, Tao Fan, and Jin Dong. \"H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity.\" arXiv preprint arXiv:2507.22633. \\\n[3] Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. Federated fine-tuning of large language models under heterogeneous tasks and client resources. In Proceedings of the 38th International Conference on Neural Information Processing Systems, 2024. \\\n[4] Pengxin Guo, Shuang Zeng, Yanran Wang, Huijie Fan, Feifei Wang, and Liangqiong Qu. Selective aggregation for low-rank adaptation in federated learning. In The Thirteenth International Conference on Learning Representations, 2025. \\\n[5] Yunlu Yan, Chun-Mei Feng, Wangmeng Zuo, Rick Siow Mong Goh, Yong Liu, and Lei Zhu. Federated residual low-rank adaptation of large language models. In The Thirteenth International Conference on Learning Representations, 2025."}, "questions": {"value": "In Line 312, \"Fed-SB: Pushing the Pareto Frontier,\" what is the meaning of 'Pareto Frontier'? To my understanding, the Pareto Frontier is a concept in multi-objective optimization. How is it applied here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1fcjccr3Ra", "forum": "FXxva35wtF", "replyto": "FXxva35wtF", "signatures": ["ICLR.cc/2026/Conference/Submission12194/Reviewer_JFJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12194/Reviewer_JFJQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533216494, "cdate": 1761533216494, "tmdate": 1762923141462, "mdate": 1762923141462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Fed-SB, a communication efficient federated LLM fine tuning approach that directly adopts a prior LoRA-SB method. The key point of Fed-SB is that it only requires aggregating small rank by rank matrices among participating clients in the federated learning process. Experimental results have been provided to justify the effectiveness of Fed-SB."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and well motivated in general.  \n- Seeking to improve the communication efficiency of federated LLM fine tuning seems to be an interesting research direction.  \n- The proposed Fed-SB method is intuitive and easy to follow.  \n- The experimental results of the Fed-SB method seem to be promising."}, "weaknesses": {"value": "- The contribution of the proposed Fed-SB method seems to be marginal. I find it hard to identify significant algorithmic innovation, except for extending the LoRA-SB method to a federated setting.  \n- The base model used in the experimental study seems to be somewhat outdated.  \n- I am not sure if federated LLM fine tuning is a practical scenario, as centralized fine tuning appears to be dominating. And I failed to find any real-world adopting for federated LLM fine-tuning."}, "questions": {"value": "- What is the major motivation for federated fine tuning? If it is for obtaining higher quality SFT data, what would be the most reasonable tasks to work on? And why has it not been adopted in real world LLM fine tuning practices (what are the major barriers)?  \n- How would Fed-SB perform, for example, on Qwen 3?  \n- Can Fed-SB be extended to support RL workflows for reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "exVma7bIEE", "forum": "FXxva35wtF", "replyto": "FXxva35wtF", "signatures": ["ICLR.cc/2026/Conference/Submission12194/Reviewer_WNPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12194/Reviewer_WNPY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12194/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762968265188, "cdate": 1762968265188, "tmdate": 1762968265188, "mdate": 1762968265188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}