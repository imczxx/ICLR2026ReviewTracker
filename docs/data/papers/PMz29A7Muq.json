{"id": "PMz29A7Muq", "number": 5456, "cdate": 1757911686734, "mdate": 1759897973686, "content": {"title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory", "abstract": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update episodic and semantic memories, gradually accumulating world knowledge. Its memory is organized in an entity-centric, multimodal manner, enabling deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn reasoning and retrieves relevant memories to complete tasks. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a long-video question answering benchmark comprising 100 newly recorded robot-perspective videos (M3-Bench-robot) and 920 diverse web-sourced videos (M3-Bench-web). We annotate QA pairs designed to test capabilities essential for agent applications, such as person understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances multimodal agents toward more human-like long-term memory and provides insights for their practical design.", "tldr": "", "keywords": ["multimodal agent", "long-term memory"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c3b412555187084e3bccb2e52d0e7c2f8f2f4b2.pdf", "supplementary_material": "/attachment/e12dbbe5c20b05671eb82f2092c23dec052821ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents M3-Agent, a multimodal agent framework that integrates long-term memory with reinforcement fine-tuning. The system processes continuous visual and auditory inputs to form episodic and semantic memories, which are stored in an entity-centric multimodal graph.\n\nTo evaluate the effectiveness of memory effectiveness and memory-based reasoning in multimodal agents, the authors also build M3-Bench, a new benchmark for long-video question answering. It consists of two parts: M3-Bench-robot, containing 100 egocentric videos recorded from a robot’s viewpoint, and M3-Bench-web, including 920 web videos across diverse scenarios. Experiments show that M3-Agent surpasses baselines including GPT-4o and Gemini-1.5-pro–based prompting agents on M3-Bench and VideoMME-long."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposed a challenging and realistic video understanding benchmark M3-Bench that goes beyond existing works. Instead of relying solely on visual perception,  it further requires the agent to leverage higher-level cognitive abilities and integrate world knowledge to solve complex tasks that involve long-horizon dependencies, temporal reasoning, and cross-modal understanding. Moreover, M3-Bench fills an important gap in evaluating memory-based long-term multimodal reasoning, especially in scenarios with robot-perspective data.\n\n2. This paper proposed a complete and well-engineered system M3-Agent that integrates video/audio perception, memory construction, and multi-step retrieval control. The overall framework is coherent and clear: episodic memory captures fine-grained observations, semantic memory provides additional cues for retrieval, and the entity-centric memory graph maintains information consistency across long contexts."}, "weaknesses": {"value": "1. Lack of real methodological novelty. The overall pipeline resembles VideoAgent [1]: the agent first perceives multimodal inputs, then stores textualized or embedding-based observations into an external memory, performs RAG during query time, and finally conducts multi-step reasoning over the retrieved memory to produce answers. The entity graph is not novel, similar identity linking appears in StoryTeller [2]. The RL training also follows a conventional approach, similar to Search-R1 [3], making the proposed framework appear more like an incremental engineering integration rather than a fundamentally novel paradigm.\n\n2. Memory quality is central to paper claims yet barely evaluated. The entity-based memory graph is central to the contribution, but its behavior over long horizons is not analyzed. There is no evaluation of memory corruption, contradiction resolution, or scalability as memory grows. Conflict handling is briefly mentioned via weighted voting (Sec. 4.1) but is under-specified and not measured experimentally.\n\n3. The claim of being an \"online agent\" is unsubstantiated. Despite repeatedly claiming “online” and “streaming” capability, the paper does not report any latency, throughput, or memory scaling measurement, nor demonstrates real-time processing. The entire pipeline is evaluated offline on fixed-length videos, and no evidence is provided that M3-Agent is suitable for online deployment. \n\n4. Insufficient Evaluation of “World Knowledge Construction”. The paper rightly points out that prior methods focus too much on low-level visual details and neglect high-level world knowledge (e.g., character identity, attributes). However, the evaluation on M3-Bench (with five QA types) does not sufficiently probe the system’s ability to generalize and transfer knowledge. For instance, if the agent observes in multiple videos that “green bins are for recycling,” can it apply this rule to a new household scene?\n\n5. Failure Case Analysis Could Be More In-Depth. Appendix J includes a case study showing successes and “hard cases” but lacks quantitative error categorization. While the paper identifies types of failures (e.g., spatial reasoning), it doesn’t quantify how prevalent each error type is. For instance, what percentage of errors on M3-Bench-robot stem from spatial misunderstandings?  Without this, it’s hard to gauge the main bottlenecks.\n\n[1] VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding\n\n[2] StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification\n\n[3] Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"}, "questions": {"value": "1. What if one doesn’t have access to in-house data? Is your training data sourced from the same distribution as your benchmark? If so, could this create an unfair advantage for your method and negatively impact fair comparison in future work?\n2. During RL training, you use GPT-4o for scoring. Does this reliance on API calls significantly increase training time or introduce practical bottlenecks?\n3. What is the typical length of the input history? When retrieving memories, are both images and text included directly as inputs to the model? Is there any further compression or processing applied to the retrieved multimodal content?\n4. The original Video-MME Long paper [1] reports a Gemini-1.5-Pro result of 73.6, which is notably higher than the numbers reported in your paper. Additionally, the baselines you compare against appear outdated. Have you evaluated your method against more recent state-of-the-art models?\n\n[1] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hbEqhLsh1G", "forum": "PMz29A7Muq", "replyto": "PMz29A7Muq", "signatures": ["ICLR.cc/2026/Conference/Submission5456/Reviewer_BEzR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5456/Reviewer_BEzR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447282219, "cdate": 1761447282219, "tmdate": 1762918073355, "mdate": 1762918073355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M3-Agent, a multimodal agent architecture with long-term, entity-centric memory. The system runs two concurrent loops: (i) memorization that parses streaming video/audio into episodic and semantic memories; and (ii) control, which performs multi-turn retrieval and reasoning over this memory to answer questions. To evaluate memory-based reasoning beyond low-level perception, this paper introduces M3-Bench, a long-video QA benchmark with 100 robot-egocentric videos and 920 web videos, annotated with question types targeting multi-evidence, multi-hop, cross-modal, and general knowledge extraction. Across M3-Bench and VideoMME-long benchmark, M3-Agent outperforms baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. M3-Bench is a novel evaluation benchmark designed to assess memory-based reasoning beyond low-level perception. This dataset comprises 100 robot-egocentric videos and 920 web videos, offering a good option for agent evaluation. The data processing and evaluation reflect the author's significant efforts.\n\n2. This paper provides a detailed account of the development process and implementation specifics for constructing M3-bench and M3-agent, supported by comprehensive experiments. Furthermore, the paper features a well-structured layout, good presentation, and is easy to follow."}, "weaknesses": {"value": "1. Automatic grading uses GPT-4o to judge correctness; although this paper report 96% agreement with human majority on a small set, relying on a single proprietary LLM as an oracle can introduce bias and inflate or depress certain methods’ scores.\n\n2. GPT-4o was employed for data generation, evaluation, and baseline comparison. On one hand, this diminishes the paper's contribution, making it more akin to prompt engineering using GPT-4o. On the other hand, constructing data and conducting evaluations with the same model introduces preference bias.\n\n3. The proposed M3 agent lacks technological innovation. From data processing to model training (DAPO), it relies entirely on existing methods, while its memory construction primarily depends on MLLMs."}, "questions": {"value": "1. Beyond the 100-sample spot-check, did you run a larger human evaluation or cross-evaluator check (e.g., Claude/LLama-judge) to ensure conclusions are not tied to GPT-4o’s preferences?\n\n2. How does index size, retrieval time, and accuracy scale as video hours and the number of identities grow?\n\n3. The ablations show big drops without semantic memory and identity equivalence. Could you further decompose semantic memory into subtypes (attributes vs. relations vs. rules) to identify which contributes most by question type?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSC9BdguvH", "forum": "PMz29A7Muq", "replyto": "PMz29A7Muq", "signatures": ["ICLR.cc/2026/Conference/Submission5456/Reviewer_urmt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5456/Reviewer_urmt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876709940, "cdate": 1761876709940, "tmdate": 1762918073074, "mdate": 1762918073074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an agent (M3-agent) to process streams of video and audio input. In addition, the paper introduces a dataset and bechmark to evaluate long-horizon video understanding (M3-Bench).\n\nThe agent contains two sub-agents; one to build a semantic and episodic memory from long video streams, and another to query the streams to answer questions. The memory forming policy is trained behavior cloning/SFT, and the control policy is trained using RL (GRPO or DAPO).\n\nThe results across M3-Bench and Video-MME-Long are compelling; outperforming (1) modular systems that use proprietary models from Google and OpenAI with RAG, (2) off-the-shelf video understanding models, (3) Socratic models\n\nThe benchmark contains two components: one scraped from youtube and another of ecocentric videos collected by humans. The benchmark is designed so that the questions require information from video; not only audio. Many of the details about collection are provided in the paper and supplementary."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper provides both a practical recipe for a long-term memory, as well as a dataset and benchmark. In my opinion, it is an excellent contribution. \n\nThe paper is well-written and easy to follow; despite there being quite a lot of work being described.\n\nThe memory agent is interesting and flexible;  involves tool use -- e.g. to do facial recognition and associate memories with the speaker. \n\nThe proposed agent serves as an excellent baseline on the proposed benchmark. The fact that DAPO on the control policy improves over modular baselines helps demonstrate a potential path for improvement."}, "weaknesses": {"value": "Overall I don't see major weaknesses; some minor ones about the memory (which I see as a baseline on the new benchmark).\n\n* The control policy is trained using RL, but memory uses only SFT. Though it may be tricky to implement from an engineering perspective, RL seems especially useful also for the memory policy, too.\n* Due to the method of constructing of entities in the memory dictionary, it seems speaker faces must be visible"}, "questions": {"value": "* Is the memory agent able to merge or edit memories? \n* Is the agent able to call tools on its own, or is all tool use scripted beforehand?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I don't anticipate issues; just noting the submission involves these elements and that I don't have the background to review the practices used for data collection:\n* scraping youtube\n* maintaining privacy of the people in the youtube videos\n* human subject doing video collection; these or other people may appear in videos"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DLJkUtbjlI", "forum": "PMz29A7Muq", "replyto": "PMz29A7Muq", "signatures": ["ICLR.cc/2026/Conference/Submission5456/Reviewer_2Awb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5456/Reviewer_2Awb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958324507, "cdate": 1761958324507, "tmdate": 1762918072845, "mdate": 1762918072845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}