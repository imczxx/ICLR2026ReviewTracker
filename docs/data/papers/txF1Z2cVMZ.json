{"id": "txF1Z2cVMZ", "number": 16231, "cdate": 1758262096445, "mdate": 1759897253190, "content": {"title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs", "abstract": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (think-on) and when to respond directly (think-off). We construct a cross-domain, logically rich dataset using a hybrid multi-agent construction pipeline that provides explicit supervision for reasoning-mode selection. Then, building on this data, we introduce a hybrid reinforcement learning (RL) reward system that integrates mode-specific rewards with global bonuses to align reasoning quality with efficiency. Experiments across mathematics, coding, and general knowledge benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy. Further analysis shows that HiPO learns fine-grained, context-sensitive reasoning behavior, activating CoT primarily on reasoning-intensive tasks and suppressing it when unnecessary.", "tldr": "", "keywords": ["Language models", "Natural language processing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c45c03b66c213bdb426b6ba012b85c22256a5d1c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose Hybrid Policy Optimization, which could adaptively choose to use or not to use the “thinking mode” during reasoning. Specifically, HiPO first builds hybrid training data containing both Think-on and Think-off samples, with DeepSeek-V3 generated explicit explanations to justify the mode selections. Next, the Hybrid reinforcement learning reward system begins to learn via the Judge and Answer segments, with two advantages considering both mode-level and instance-level benefits. The experimental results on Qwen series have verified the effectiveness and efficiency of the proposed HiPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThis work introduces a straightforward and effective pipeline for building an adaptive reasoning model. The experiments show satisfactory results.\n-\tThe authors have conducted extensive analyses on the detailed model settings in Section 4.3 and 4.4, which makes the conclusion more solid.\n-\tOverall, the writing is clear."}, "weaknesses": {"value": "-\tIn the Related work part, the authors have claimed that “Despite progress, …, limited adaptation to hard cases due to monotonic shortening”. However, the existing adaptive reasoning methods (at least the adaptive baselines used in experiments) should be discussed to highlight the differences, technical novelty, and advantages of HiPO. It is not that clear which technical part is novel in this work.\n-\tIn experiments, is the cold-start stage of HiPO the same as those of baselines (GRPO, AdaptThink, and AutoThink)? Did AdaptThink and AutoThink adopt the exact same data as HiPO’s data in both stages? These questions come from the weird results: in most cases, AdaptThink and AutoThink perform worse than the Cold-start (on or all) baselines. Fair comparisons are required.\n-\tHiPO has an additional segment of “Judge_analysis” that may bring in new costs for both Think-on and Think-off modes. Does the “Length” metric in Table 2 indicate the average total inference length (including all judge/think/answer/other segments)?\n-\tThe experiments in Table 4 should include other adaptive baselines to provide a more convincing proof.\n-\tTypo: Page 7, Table 5 -> Figure 5."}, "questions": {"value": "-\tWill the data used in this work be open-sourced?\n-\tIn Section 3.1.2, how to set the predefined threshold for different tasks with various hardness levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TUZkeiG9iF", "forum": "txF1Z2cVMZ", "replyto": "txF1Z2cVMZ", "signatures": ["ICLR.cc/2026/Conference/Submission16231/Reviewer_PRES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16231/Reviewer_PRES"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131344324, "cdate": 1761131344324, "tmdate": 1762926389199, "mdate": 1762926389199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiPO (Hybrid Policy Optimization), a training framework that enables Large Language Models to dynamically decide when to perform explicit reasoning (Think-on) or to directly answer (Think-off). The method combines a hybrid data pipeline that pairs concise and detailed responses with mode-justification annotations, and a hybrid RL reward balancing accuracy, token efficiency, and mode-selection quality. Experiments on math and code benchmarks show that HiPO reduces average output length while improving accuracy, effectively mitigating overthinking without hurting performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes a hybrid RL framework that learns when to reason (Think-on/off) to substantially reduce token length while maintaining or improving accuracy.\n\n2. Extensive experiments on math/code benchmarks show consistent token reduction and accuracy gains.\n\n3. Problem formulation, data pipeline, and two-stage training are described precisely."}, "weaknesses": {"value": "1. Auto-generated judge quality unchecked: \n\n   The pipeline uses DeepSeek-V3 to produce “why this mode” justifications that directly enter the RL loss. No ablation measures how often these rationales are wrong or self-contradictory; noisy judge labels could mislead policy updates and inflate gains.\n\n2. Binary gating only:\n \n   Think-on/off is a single-bit decision, so the model cannot partially unroll a chain or perform staged reasoning (plan → verify → summarize). The paper does not show how to extend HiPO to finer-grained or depth-adaptive gating, limiting utility on problems of medium complexity.\n\n3. Statistical significance unclear:\n \n   Main results come from one rollout on small benchmarks (e.g. AIME 24/25) without multiple runs. Without detailed testing procedures in the main text, it is hard to tell whether the observed improvements are real or just random fluctuation."}, "questions": {"value": "1. What is the \"reasoning mode-difficulty\" relationship (correlation between reasoning mode with performance) after HiPO training?\n\n2. Have you considered about how to extend HiPO to finer-grained or depth-adaptive gating? If so, what architectural or reward changes are needed, and do results on medium-difficulty tasks show further token savings without accuracy loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dTzFLANTUF", "forum": "txF1Z2cVMZ", "replyto": "txF1Z2cVMZ", "signatures": ["ICLR.cc/2026/Conference/Submission16231/Reviewer_puqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16231/Reviewer_puqf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929240011, "cdate": 1761929240011, "tmdate": 1762926388314, "mdate": 1762926388314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HiPO, a framework for adaptive reasoning in large language models (LLMs) that aims to dynamically decide when to engage in detailed chain-of-thought reasoning (\"Think-on\") and when to respond directly (\"Think-off\"). It introduces (1) a hybrid data construction pipeline generating paired Think-on/Think-off examples with mode justifications, and (2) a hybrid reinforcement learning (RL) reward system combining accuracy and efficiency rewards. Experiments on mathematical and coding benchmarks suggest HiPO reduces token usage while maintaining accuracy"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear. \n- Addresses a relevant problem in LLM reasoning efficiency.\n- Includes ablation studies on bias terms and normalization factors."}, "weaknesses": {"value": "- There already exists a substantial body of research on adaptive reasoning and dynamic CoT control in large language models. However, the authors reduce all prior methods to a single dismissive sentence “these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.” This characterization is overly simplistic and fails to accurately represent the diversity and sophistication of existing approaches. Many prior works have proposed different strategies for trade-offs, such length penalties, etc. Moreover, the authors do not convincingly explain why their proposed HiPO framework is inherently better suited to address these issues. Without a detailed comparative analysis and a clearer articulation of the unique mechanisms through which HiPO overcomes these prior limitations, the novelty of the paper remains questionable.\n\n- It is unclear whether the authors used the Qwen3-8B base model or the Qwen3-8B instruct model during experimentation. The paper states: “Since the Qwen3 model can freely switch between inference modes, we chose it for our experiment.” However, this capability is specific to the instruct variant of Qwen3. If the authors indeed used the instruct model, their reported results should be compared to the Qwen3 technical report, particularly Table 17, which shows a MATH score of 97.4, while this paper reports only 93.6, even after applying the proposed method. This discrepancy raises concerns about experimental reproducibility and baseline validity. The authors need to clearly specify which version of the model was used, under what inference configuration, and explain why their results are significantly lower than the officially reported benchmarks.\n\n- The paper mentions a cold-start stage before reinforcement learning. Regarding the cold-start stage, the paper mentions generating hybrid data using an external model, and Figure 1 strongly suggests that the DeepSeek model was used to construct the cold-start dataset. If that is the case, the process resembles knowledge distillation, where the target model learns from a stronger teacher rather than discovering effective strategies on its own. It would be valuable to test whether the system could train directly with RL without cold-starting the policy. Such an experiment could help determine whether the cold-start stage provides essential stability or merely serves as a redundant initialization step. If HiPO’s reward structure is indeed well-designed, a cold-start-free RL variant should, in principle, converge similarly.\n\n- It would be highly informative to explore whether the ratio of Think-on to Think-off instances correlates with task difficulty. The reviewer suggests dividing the MATH-500 dataset according to its five officially defined difficulty levels and analyzing how HiPO adapts across them. \n\n- The reviewer is concerned that the model might be implicitly relocating reasoning content into the Answer segment, effectively performing internal reasoning within Think-off mode, as suggested by examples like Figure 1. If such behavior occurs, it could represent a form of reward hacking, since the current reward formulation does not explicitly penalize the length or reasoning structure of responses. As a result, the model might learn to disguise reasoning tokens within the Answer segment to maximize rewards without genuine efficiency improvement. The authors should therefore report the token distributions of Think-on and Think-off outputs separately, and analyze whether reasoning leakage occurs."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CtMg69qoQQ", "forum": "txF1Z2cVMZ", "replyto": "txF1Z2cVMZ", "signatures": ["ICLR.cc/2026/Conference/Submission16231/Reviewer_cZp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16231/Reviewer_cZp4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988683277, "cdate": 1761988683277, "tmdate": 1762926387818, "mdate": 1762926387818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HiPO, a framework for adaptive reasoning control that lets an LLM decide when to Think-on (emit CoT) vs. Think-off (answer directly). HiPO has two parts: (i) a hybrid data construction pipeline that pairs Think-on/Think-off responses per query (preferring the shortest correct sample and adding an explanation justifying the chosen mode), and (ii) a hybrid RL reward with a bias adjustment to avoid over-reliance on verbose Think-on and mode-aware advantages that allocate signal to <judge> (mode decision) vs. <answer> tokens. On Qwen3 (1.7B/8B/32B) across math and coding benchmarks (AIME24/25, MATH-500, HumanEval, LiveCodeBench, MBPP, GPQA), HiPO reports reduced token length and lower Think-on ratio while maintaining or improving accuracy over baselines (Cold-Start±GRPO, AdaptThink, AutoThink)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem focus (overthinking) with an intuitive mode-gating mechanism and token-level training signal split across <judge> and <answer>. \n\n- Consistent efficiency gains (shorter outputs, lower Think-on ratio) with comparable or better accuracy on multiple benchmarks and model sizes. \n\n- Various ablations. Removing global advantage or local normalization degrades accuracy/efficiency, and sensitivity to γ/ω/N is explored."}, "weaknesses": {"value": "1. Limited statistical rigor. Tables show point improvements but lack variance/CI across seeds and tasks, making deltas (often modest) hard to assess. \n\n2. Heuristic bias adjustment. The ω-scaled boost to Think-off when close to Think-on can tilt optimization without a principled guarantee (risk of mode-collapse oscillations); analysis is empirical only. \n\n3. Data pipeline dependence. Selecting the shortest correct response may bias style and harms robustness if “shortest” correlates with template artifacts; the policy could overfit to training distributions. (Ablations help but remain narrow...) Any other variants can improve the quality for this part.\n\n4. Generalization claims. While Qwen3 1.7B/32B are included, domains remain math/code; transfer to tool-use, multilingual tasks, or safety-critical reasoning is not shown"}, "questions": {"value": "1. Can you include Statistical results over seed and different temperatures?\n\n2. Bias adjustment safety. Can you provide a convergence/monotonicity argument, or at least diagnostics showing no oscillatory gating? What happens for ω→0 and ω→0.05 across datasets? (Fig. 5 hints at trade-offs.) \n\n3. Failure modes. When Think-off is chosen incorrectly (hard question), does HiPO recover (e.g., via fallback) or lock into shallow answers? Any error typology?\n\n4. Beyond math/code. Have you tried tool-augmented QA or long-context tasks where mode selection interacts with retrieval?\n\n5. Ablations on explanation. If you remove the mode-explanation supervision, how much do gating quality and accuracy drop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1v0BBFs7jJ", "forum": "txF1Z2cVMZ", "replyto": "txF1Z2cVMZ", "signatures": ["ICLR.cc/2026/Conference/Submission16231/Reviewer_aTj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16231/Reviewer_aTj3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093008486, "cdate": 1762093008486, "tmdate": 1762926387039, "mdate": 1762926387039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}