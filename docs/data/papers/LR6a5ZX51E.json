{"id": "LR6a5ZX51E", "number": 20350, "cdate": 1758305005684, "mdate": 1759896982324, "content": {"title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models", "abstract": "How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.", "tldr": "", "keywords": ["Reasoning", "compositional generalization", "interpretability", "steering", "large language models", "function vectors", "representational geometry", "finetuning", "topic modeling", "foundation models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73f61a43182f75e2abe89d779c4d403b75e4127e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for identifying and steering algorithmic primitives in LLMs by combining clustering of latent activations with residual-stream vector interventions. Using tasks such as TSP, 3SAT, AIME, and graph navigation on models like Phi-4, Phi-4-Reasoning, and Llama-3-8B, the study shows that these primitives compose algebraically and transfer across tasks, revealing a compositional geometry of reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel and thought-provoking framework for understanding the internal behaviors of LLMs.\n\nThe experiments cover multiple models and datasets, which provides credible support for the conclusions.\n\nThe analysis of reasoning-step transfer and compositional primitives is particularly interesting and offers new insights into LLM reasoning mechanisms."}, "weaknesses": {"value": "The presentation and methodological details could be improved. For example, the paper lacks clarity regarding clustering hyperparameters and the statistical evaluation metrics, as noted in the questions below.\n\nIn addition, since the paper introduces several new concepts and definitions, it would be helpful to include a summary table that clearly defines all key terms included examples for better readability.\n\nIf these concerns are addressed, I would consider raising my score."}, "questions": {"value": "In Section 3, what does the variable x specifically refer to?\n\nHow are candidate vectors selected for clustering? Please provide more detail on hyperparameter settings.\nWhat are the quantitative results of clustering? Have you evaluated the quality using metrics such as ARI or NMI?\n\nDid you observe differences in clustering behavior across layers? If so, how do these relate to the model’s reasoning hierarchy?\n\nIn Section 5, how is the “higher reward node” defined? What is the reference or baseline used for the comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TJhdcvlkIm", "forum": "LR6a5ZX51E", "replyto": "LR6a5ZX51E", "signatures": ["ICLR.cc/2026/Conference/Submission20350/Reviewer_7Ty5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20350/Reviewer_7Ty5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506828174, "cdate": 1761506828174, "tmdate": 1762933808991, "mdate": 1762933808991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes “Primitive Vectors” (PVs): directions in the residual stream extracted from attention head activity on self-generated reasoning traces, which can be injected at a target layer to steer behavior. The authors claim PVs correspond to reusable “algorithmic primitives” and that linear combinations compose these primitives."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. **Mechanistic angle on reasoning.** Treats skills as additive residual directions with a tunable strength (α), connecting to activation patching / feature-vector literature.\n\n\n2. **Compositional tests.** Evaluates whether sums/differences of PVs yield composed behaviors rather than single-feature nudges."}, "weaknesses": {"value": "1. **Presentation gaps impede assessment.** Plots frequently lack axis labels and captions. Most importantly, there is no clear, self-contained equation in the main text for how PVs are computed. Line 191 points to §4.2; §4.2 (line 238) punts to the appendix; the appendix has no concrete definition. This is a blocker for reproducibility and for understanding what is actually averaged/normalized where.\n\n\n2. **Heuristic choices are under-justified.** Why K-Means for potentially high-dimensional representations? Why (k=50)? Why layer 17? No sensitivity or ablations are provided to justify these design choices.\n\n\n3. **Ad-hoc functionality assignment.** PV identities are hand-picked from 6 of 50 clusters after manual inspection. That feels brittle and hard to reproduce without objective criteria.\n\n\n4. **Alternative explanation (bag-of-words steering).** Line 218 suggests K-Means is run over (h_17) for tokens in self-generated reasoning traces. If so, clusters may mostly track lexical/discourse cues (“alternatively”, “thus”, parentheses/digits) rather than procedure. The resulting average vector could simply boost a bag of tokens, which in turn nudges the next steps of a “reasoning” template. This also explains linear add/subtract as logit mass addition/suppression—not necessarily algorithmic composition. Suggest to add controlled experiments (e.g., unembedding alignment) to show this reflects a latent procedure rather than BOW steering.\n\nThe core idea is interesting, but the paper is hard to follow and omits key methodological details. With a precise PV definition and controls that rule out BOW steering, I could see this moving to 4 or better."}, "questions": {"value": "1. Please provide a precise mathematical definition of PV construction (token set, head set, centering/normalization, layer choice, averaging domain, and scaling).\n\n\n2. How sensitive are results to (k), distance metric, layer index, and α? Any reason to expect stability across model sizes?\n\n\n3. For TSP, do the 6 selected clusters share any statistics that suggest an automated criterion (e.g., size, radius, density)?\n\n\nTypos/Minor:\n\n1. Line 784: broken appendix reference/link.\n\n2. Please add axis labels and full captions to all figures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Frii2307eO", "forum": "LR6a5ZX51E", "replyto": "LR6a5ZX51E", "signatures": ["ICLR.cc/2026/Conference/Submission20350/Reviewer_NEzR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20350/Reviewer_NEzR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852721688, "cdate": 1761852721688, "tmdate": 1762933808530, "mdate": 1762933808530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper  proposes a framework for finding algorithmic primitive vectors in LLMs. Primitive vectors are vectors that are found by averaging activations in LLMs that are associated with a unique algorithm building block. Authors use clustering methods to identify tokens with similar activation patterns, average activations within a cluster and then in a bottom-up approach associate each primitive with an algorithmic behavior. Once found, these vectors can be used to steer LLMs towards relying more on those algortithmic building blocks. In the case of this paper, authors study algorithmic primitives in the Phi4 and Phi4 reasoning models and focus on reasoning tasks. Findings include the observation that reasoning fine-tuning leads to more general resolution algorithmic building block. Also: \"We find that algorithmic primitives exhibit geometric regularities through compositional operations like addition, subtraction, multiplication,and scalar modulation. The layer and magnitude of injection shape the output expression of the primitive (Fig. 2). We show cross-task transfer and generalizability of primitive vectors: spatial reasoning and verification primitives extracted from AIME successfully transfer to TSP. This transferability hints at potentially universal algorithmic building blocks underlying diverse reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Form: paper is well-written, easy to follow, polished\n- Relevance: as pointed out by the authors, the question investigated by the authors participates to the general conversation about whether LLMs reason or memorize. Through the proposed framework and findings, this work suggests that general algorithmic building blocks are used by LLMs to solve tasks requiring reasoning and further highlight the geometric structure in the representation space of LLMs. The topic is hence relevant to the community.\n- Simplicity of the proposed method: the framework proposed by authors is straightforward, driven by first principles, easy to translate (task and model agnostic), hence its application to other settings seems straightforward"}, "weaknesses": {"value": "- Missing ablations to support robustness of the proposed method: the authors propose a framework that can be used to understand pre-trained models. While straightforward, the method relies on the application of a clustering method which itself requires a choice of a number of clusters and a layer. How are those chosen? How sensitive is the analysis to these choices? how robust are these hyperparameters across models? There are limited explanations regarding those choices which seem important to be able to sell a transferable framework. How many of those clusters were not found to be linked with a clear algorithmic trace?\n- Scope of empirical evidence: the findings are insightful notably the observations that Phi4 reasoning relies on primitive associated with general and reusable algorithmic building blocks. However, I think the scope of the experiments should be extend to more models in order for the analysis to be impactful.\n- Questions about Impact: While the findings are relevant to the community and the proposed method is straightforward, I have concerns about the overall impact of the work—particularly regarding how well the approach aligns with the actual internal computations of LLMs. The method relies on clustering to extract primitive vectors associated with algorithmic traces. However, the results in Table 1 indicate that steering the model using a single primitive affects multiple algorithmic components, suggesting that these behaviors are not disentangled. Consequently, I question the significance of the proposed approach and its findings, as the primitives lack clear independence, and the paper does not provide a comprehensive analysis of their dependencies or a causal model linking them."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hkrRpL70W3", "forum": "LR6a5ZX51E", "replyto": "LR6a5ZX51E", "signatures": ["ICLR.cc/2026/Conference/Submission20350/Reviewer_EaF2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20350/Reviewer_EaF2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994368615, "cdate": 1761994368615, "tmdate": 1762933808039, "mdate": 1762933808039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an “algorithmic primitives” framework to trace and steer multi-step reasoning in LLMs. It clusters internal activations to discover recurring primitives (e.g., generate_path, nearest_neighbor, compare_or_verify), maps them to reasoning traces, then derives primitive vectors (via averaged top attention-head outputs) that can be injected into residual streams to induce, suppress, and compose reasoning behaviors. Across TSP, 3-SAT, AIME, and graph navigation and across Phi-4, Phi-4-Reasoning, and Llama-3-8B. the authors show primitives exhibit a compositional geometry (addition, subtraction, scalar scaling), transfer across tasks, and differ systematically between base vs. reasoning-finetuned models (e.g., more verification/planning in Phi-4-Reasoning)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem this paper is exploring is interesting to me. Vector injection and subtraction produce clear, selective changes in reasoning hallmarks (e.g., verification spikes).\n\n2. The expeirments demonstrate additive composition that induces a composite skill and cross-task transfer, suggesting shareable, reusable building blocks.\n\n3. By mapping token traces, activation clusters, primitive vectors and layering in the temporal/meta-cluster dynamics, the work delivers a genuinely end-to-end interpretability account."}, "weaknesses": {"value": "1. The use of layer 17, k=50, and small response batches risks making the identified primitives dependent on those choices and vulnerable to sampling noise.\n\n2. The mapping from clusters to primitive labels is hand-interpreted from local text, and some clusters (e.g., compare_or_verify) appear context-dependent and multi-purpose."}, "questions": {"value": "1. Can we have time complexity for this methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ckYERkUzkq", "forum": "LR6a5ZX51E", "replyto": "LR6a5ZX51E", "signatures": ["ICLR.cc/2026/Conference/Submission20350/Reviewer_Tj9J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20350/Reviewer_Tj9J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283636247, "cdate": 1762283636247, "tmdate": 1762933807637, "mdate": 1762933807637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}