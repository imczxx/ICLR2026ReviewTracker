{"id": "h3D4q2y97Z", "number": 17208, "cdate": 1758273435417, "mdate": 1759897190770, "content": {"title": "From Symbolic Perception to Logical Deduction: A Framework for Guiding Language Models in Geometric Reasoning", "abstract": "Plane geometry is a long-standing challenge in AI, requiring the integration of visual perception and mathematical reasoning. \nLarge Multimodal Models (LMMs) such as Gemini 2.5 Pro handles visuo-linguistic inputs but are resource-intensive. \nWe show that a pure Large Language Model, when equipped with specialized modules, can rival state-of-the-art LMMs on complex geometry problems. \nOur framework integrates a Geometric Vision Parser, which translates diagrams into symbolic form, with a Symbolic Solver that performs formal deductions on angular relations, mitigating hallucinations and promoting interpretable reasoning. \nTo enable rigorous evaluation, we curate a benchmark of difficult problems from the 2025 Chinese Zhongkao examinations, ensuring novelty and testing deeper deductive skills. \nExperiments demonstrate that our approach achieves performance comparable to Gemini 2.5 Pro while delivering clearer, human-like solutions.", "tldr": "", "keywords": ["AI for Math", "Geometric Reasoning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b118fdb8147d774adfd83775f7c4affe6969733b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors focus on solving geometry problems with both images and text descriptions. They propose a pipeline that first uses a diagram parser to convert the images into structured representations, and then a symbolic solver trying to solve the problems and then LLM to generate the final answer. They also provide a new dataset of geometry problems from ZhongKao exams with images. Experiments show that their method outperforms some state-of-the-art LLMs on this dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a practical pipeline to convert the geometry problems with images into a format that LLMs can better understand and solve. \n2. Their methods can be easily combined with any LLMs without changing the model structure and requirement to re-train the model.\n3. They provide a new dataset of geometry problems from ZhongKao exams with images."}, "weaknesses": {"value": "1. The authors claim their ZhongKao problems are more difficult than the existing geometry problems. However, there are many new datasets which are significantly more difficult than ZhongKao, such as geometry problems in math olympiad (e.g., [IMO-geometry](https://huggingface.co/datasets/theblackcat102/IMO-geometry), the geometry parts in [OMNI-math](https://huggingface.co/datasets/KbsdJames/Omni-MATH) and [GAOKAO](https://github.com/OpenLMLab/GAOKAO-Bench)). The scale of the Zhongkao is also not larger than these datasets.\n2. The experiments do not show significant improvement over the existing methods. Considering the method is based on DeepSeek-R1, the improvement is quite limited (from 88.39% to 92.13%). It is also not fair to use Major@3 to compare with other methods' Major@1 results. It is commonly found that Major@3 results are significantly higher than Major@1 results for most methods including the LLMs themselves."}, "questions": {"value": "1. Although a common believe is that iamges are important for geometric reasoning, I wonder whether it is true. In fact, there is no additional information can only be obtained from images but not from text. As my knowledge, the IMO even does not provide images for geometry problems. And the AlphaGeometry also does not require images input. So I wonder whether we need to consider the input images. Is your diagram parsing important for geometric reasoning? Can you provide some statistics about how many problems in some geometric dataset cannot deduce a unique image from only the text description? Can you show some examples to support the claim in lines 165-167: \"Unlike algebraic or purely text-based tasks, geometric problem solving often depends on recognizing implicit constraints, such as collinearity, angle properties, that are visually encoded in diagrams rather than explicitly stated in text.\" In my experience of math olympiad, the implicit constraints do not meet the questioning standards and should be avoided very carefully.\n2. It seems that the proposed method share very similar idea to the AlphaGeometry and the letter one is even introduced more interesting ideas like heuristic auxiliary lines proposed by LLMs and an iterative solving procedure. Can you compare your method with AlphaGeometry?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I33DOz23AI", "forum": "h3D4q2y97Z", "replyto": "h3D4q2y97Z", "signatures": ["ICLR.cc/2026/Conference/Submission17208/Reviewer_ag9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17208/Reviewer_ag9d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761193132541, "cdate": 1761193132541, "tmdate": 1762927177766, "mdate": 1762927177766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles plane geometry problem solving via  hybrid neuro-symbolic framework enabling a pure LLM (DeepSeek-R1) to perform on par with SOTA LMMs such as Gemini 2.5 Pro."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  clear design of three-stage system for plane geometry\n\n- new dataset that emphasizes multi-step and multi-objective questions\n\n- ompetitive empirical results on Gemini 2.5 Pro on L2/L3 (especially with Major@3)."}, "weaknesses": {"value": "- No formal basis of the proposed Literal Expansion, Fact Deduction and Fact Filtering. What is the sysntac, semantics and detuctive reasoning solver used for this. What are the theorical guarentees? What are the guarentee od soundness and completeness of the derived facts with respect to a declared axiom?\n\n- The fact-deduction engine relies on ordering constraints and geometric priors to prune a combinatorial search. Does the pruning preserves completeness for the targeted theorems?\n\n- The system “translates into FormalGeo,” while Appendix tables list predicates, there is no full semantics. \n\n- The solver chooses theorems via search + filters. It is not clear how the the matching criterion, unification and substitutions work. \n\n-Evaluation design is somehow weak. Baselines are used with single run making Major@3 not fair for comparaison? Cost/latency should be reported."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b7nAyFkLTk", "forum": "h3D4q2y97Z", "replyto": "h3D4q2y97Z", "signatures": ["ICLR.cc/2026/Conference/Submission17208/Reviewer_Vqsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17208/Reviewer_Vqsy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835860546, "cdate": 1761835860546, "tmdate": 1762927177421, "mdate": 1762927177421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework of to solve plane geometry problems: first, use a geometric visual parser to convert the points/lines/circles/angles in the image into structured symbol representations, and align them with the question text. \nThen use a symbolic reasoning tool to match and prune key properties such as angles, generating high confidence geometric facts. Finally, feed the obtained information as an \"enhanced context\" to a general LLM to generate readable solutions. The author also proposed the \"ZhongkaoGeo\" three-layer benchmark (L1/L2/L3), emphasizing the use of exam questions in the near future to reduce training pollution, and reported comparable or better results with Gemini 2.5 and other LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper propose a framework by firstly formalize the problem and further conduct theorem reasoning to facilitate LLMs on the problem solving.\n2.\tA new benchmark ZhongkaoGeo was proposed, to challeng that current benchmarks facing the data leaking issue to the training of LLMs, and also has different layers to classify the difficulty of the problems."}, "weaknesses": {"value": "1.\t**Limited novelty and contribution.** The framework mainly integrates existing components (YOLO detection, rule-based symbolic solver from FormalGeo, and prompt-based LLM reasoning) without introducing a new learning paradigm or optimization objective. And the theorem reasoning module leveraged FormalGeo. It remains unclear how these modules interact under a unified theoretical framework or whether the system optimizes any well-defined objective. As a result, the contribution appears more engineering-driven than learning-oriented, which weakens its theoretical novelty for an ICLR submission.\n2.\tThe authors claim that providing verified symbolic facts can “effectively constrain the reasoning space and mitigate hallucination.” (Line 283, 284) However, the paper lacks a theoretical mechanism explaining this claim. Author does not define what the “reasoning space” is, nor describe how it is constrained by symbolic facts, and provides no formal framework (e.g., probabilistic, information-theoretic, or search-based) to support the notion of such space reduction.\n3.\t**Limited parsing ability and query of the propose dataset.** The current parser seems has no ability to understand the annotations in the diagram, like the degree of the angle, length of line. It is crucial for solving PGPs, if the problems in ZhongkaoGeo do not require the annotation understanding, that means the dataset is too simple, which not satisfy the current research requirements on PGP area. The previous work like PGDP [1] already tackled the parsing task with very high accuracy, the parsing module in this work has no contribution to the study of plane geometry diagram parsing.\n4.\tThe experiment is not fair. The author used major@3 to get performance increase, under this case, other baselines should also use the majority vote in the experiments. Comparing with the performance without majority vote, the proposed method has lower performance then Gemini2.5-Pro in ZhongKaoGeo-L1.\n5.\t**Experiments are not sufficient.** It is necessary to conduct experiments on popular benchmarks such as Math-Verse, Math-Vista, the current experiments only conduct on the proposed dataset with very limited LLMs. It is not related to the data leaking issue, as the framework is universal to different models, I just wondering the effectiveness of the framework. Also, the proposed method is a framework, it is necessary to apply this framework with different LLM backbones, not only limited to DeepSeek-R1, like assemble your framework with Gemini, GPT, to show whether the performance will increase based on this framework.\n6.\tThe problems in ZhongkaoGeo has several types (angle, length, area …). As a benchmark, these statistics of the experiments should be introduced, the current table is too simple, only has accuracy. No deeper analyses were provided from the experiments.\n7.\tDespite the author claim “Large Multimodal Models (LMMs) such as Gemini 2.5 Pro handles visuo-linguistic inputs but are resource-intensive.”, experiments should also cover the experiments of LMMs with smaller parameters, like many plane geometry problem solving models, such as R-CoT, G-LLaVA, GeoX. Comparing to models like Gemini, these models use lower computing resources. What is the performance of these models on ZhongkaoGeo? If they has better performance, the value of this framework will be challenged.\n8.\t**Losing details.** How did you train the YOLO to do the parsing task, what dataset used to train this module?\nAbove all, the current version of this paper is not satisfy the bar of ICLR.\n\nAbove all, the current version of this paper is not satisfy the bar of ICLR.\n\nRef:\n\n[1] Plane Geometry Diagram Parsing, in IJCAI 2022"}, "questions": {"value": "1．\tLine 049, “This paper challenges the prevailing assumption that end-to-end LMMs are the definitive solution for complex geometric reasoning. “ Is there any source to support this claim? As far as I know, now a popular research direction in solving PGPs is not end-to-end, like a lot of works are using neural-symbolic framework.\n\n2．\tThere are L1, L2 and L3 in ZhongkaoGeo, why the statistic of L3 not be introduced in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZodlZUM6bV", "forum": "h3D4q2y97Z", "replyto": "h3D4q2y97Z", "signatures": ["ICLR.cc/2026/Conference/Submission17208/Reviewer_ws1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17208/Reviewer_ws1b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008593712, "cdate": 1762008593712, "tmdate": 1762927177133, "mdate": 1762927177133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a hybrid framework to enhance the geometric reasoning ability for LLMs.\nEssentially, the framework first identifies geometric elements in a given image by combining detection models and heuristic preprocessing.\nThen, the framework uses a rule-based deduction engine to infer some conclusions.\nSince these conclusions may not lead to the final answer of a given query, LLM is used to reason over the given problem (image + query) and the conclusions inferred by the deduction engine.\n\nTo conduct experiments, authors propose a new benchmark according to the 2025 Chinese Zhongkao examinations, ensuring complexity and avoiding data contamination.\nOn the proposed benchmark, results show that the proposed framework can achieve performance comparable to large multimodal models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and well-organized.\n2. The proposed framework and benchmark are well-motivated."}, "weaknesses": {"value": "1. Experiments are not enough. Authors should compare the proposed framework with other symbolic or hybrid methods. Although authors claim that involving symbolic systems has many issues (Line 35-41), the proposed framework also involves symbolic conversion, symbolic deduction, and heuristic processing. Therefore, the proposed framework also suffers from many of those issues and should be compared with other symbolic or hybrid methods.\n2. Figure 3 is not good. For example, the left subfigure is too big with just a little information. The fonts in the subfigures also looks non-academic.\n3. The presentation is not clear enough. See *Questions*."}, "questions": {"value": "1. Line 45-48: You claim that LMMs are computationally expensive, resource-intensive, and their reasoning can be opaque, and that their performance on specialized domains like geometry is often constrained by distributional shifts between their generalist training data and the specific symbolic logic of geometric diagrams. Are not these also limitations of LLMs?\n2. Line 46: What is resource-intensive? What is the difference between resource-intensive and computationally expensive?\n3. Line 76-77: Is there evidence justifying that the deduction on angular relationships is a frequent source of LLM hallucination? The description given in Line 243-249 is not convincing. Real examples that can justify the description in Line 243-249 will be more convincing.\n4. Table 5: Is the degree of angle annotated in the original image from the dataset? Is so, recognizing the degree from the images can show that Gemini-2.5-Pro has strong image understanding ability, instead of data contamination.\n5. Line 172-173: Why LLMs can produce more coherent solutions than symbolic solver?\n6. Is the proposed framework only capable of addressing angular relation problem?\n7. How does the difficulty of the benchmark you proposed compare to existing benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nev99ECLHM", "forum": "h3D4q2y97Z", "replyto": "h3D4q2y97Z", "signatures": ["ICLR.cc/2026/Conference/Submission17208/Reviewer_i5qT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17208/Reviewer_i5qT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762756624553, "cdate": 1762756624553, "tmdate": 1762927176920, "mdate": 1762927176920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}