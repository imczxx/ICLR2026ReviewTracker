{"id": "Y2JL8JXwmf", "number": 4556, "cdate": 1757706246352, "mdate": 1759898026750, "content": {"title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "abstract": "Evaluating large language models (LLMs) on question answering often relies on static benchmarks that reward memorization and understate the role of retrieval, failing to capture the dynamic nature of world knowledge. We present LiveSearchBench, an automated pipeline for constructing retrieval-dependent benchmarks from recent knowledge updates. Our method computes deltas between successive Wikidata snapshots, filters candidate triples for quality, and synthesizes natural-language questions at three levels of reasoning difficulty, each guaranteed to admit a unique, verifiable answer through SPARQL validation. The pipeline is fully automated, scalable across time, and minimizes human intervention, enabling continual regeneration of temporally grounded benchmarks. Experiments show a pronounced performance drop when models confront facts that post-date pretraining, with the gap most salient on multi-hop queries. Retrieval augmented methods and larger, instruction-tuned models provide partial gains but fail to close this recency gap. By design, LiveSearchBench shifts evaluation from static memorization toward tasks that require up-to-date retrieval and reasoning, offering a foundation for systematic, long-term assessment of LLMs under evolving knowledge. Data and code are available at [LiveSearchBench](https://anonymous.4open.science/r/LiveSearchbench-43E2).", "tldr": "An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "keywords": ["Large Language Models (LLMs)", "Benchmarks", "Retrieval-Augmented Generation (RAG)", "Search agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/132b3d25fa60b920f192c0b375c34d0e8cfc1688.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes LiveSearchBench, an automated pipeline for constructing a dataset that provides a clean testbed for evaluating retrieval-augmented generation. The dataset is created by taking the difference between two successive Wikidata snapshots; low-quality tuples are discarded before converting to QA pairs. The resulting dataset includes three difficulty levels. Evaluation is conducted on two model families (Llama and Owen) with two dataset versions (2021 and 2025). Results show that retrieval yields larger gains on fresher knowledge that is unlikely to be stored in model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important problem: once a benchmark is published, models may train on it. An ever-evolving dataset with the latest knowledge is crucial for correctly evaluating RAG systems.\n* The benchmark creation process is rigorous with solid quality control. Low-quality triples are filtered in the Candidate Filtering step, improving reliability, and the Finalization and Validation step further improves question quality."}, "weaknesses": {"value": "* Missing discussion of closely related work. The idea appears very similar to AntiLeakBench [1], especially in methodology: both use Wikipedia as the knowledge base, leverage edit history across snapshots to construct non-contaminated data, and use fully automated QA generation pipelines. The differences between this paper and AntiLeakBench should be clarified.\n* Limited model families and sizes. It would be better to include closed-source models and more open-source families.\n* The “fully automated” claim may be overstated. In §4.4 the authors note: “To guarantee quality, five PhD researchers reviewed the synthesized triples and reasoning paths behind each question,” indicating a human evaluation component.\n\n\n[1] Wu, X., Pan, L., Xie, Y., Zhou, R., Zhao, S., Ma, Y., Du, M., Mao, R., Luu, A. T., & Wang, W. Y. (2025). AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge, arXiv:2412.13670."}, "questions": {"value": "Writing note:\n* Define SPARQL at first mention; adding a brief description would improve clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hGDSfkoNDR", "forum": "Y2JL8JXwmf", "replyto": "Y2JL8JXwmf", "signatures": ["ICLR.cc/2026/Conference/Submission4556/Reviewer_wUJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4556/Reviewer_wUJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780214759, "cdate": 1761780214759, "tmdate": 1762917438761, "mdate": 1762917438761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveSearchBench, an automated pipeline for constructing dynamic knowledge benchmarks in the QA format to test LLM on new knowledge. It builds on the difference of Wikipedia graph snapshots at different times, and constructs QA pairs of different difficulties based on number of hops and entity scopes. Experiments on two samples of the benchmarks show LLM memorization on older Wikipedia data and RAG helps with new information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clear and easy to read.\n\n- The benchmark construction based on Wikipedia graph updates makes sense to reduce LLM memorization of the knowledge being tested.\n\n- Results are well presented, and agree with expectations."}, "weaknesses": {"value": "- The novelty of the paper is limited. Dynamic benchmarking is an important direction, but there has been many works such as AntiLeakBench [1], Daily Oracle [2], among others [3, 4]. Among them, one of the most relevant work, AntiLeakBench [1], also works with the Wikipedia updates for constructing knowledge testing samples that are dynamic and not memorized. Compared to the prior research, the proposed work lacks both scale and comprehensiveness in both benchmark construction and evaluation. Plus, none of the relevant prior works have been mentioned at all in the paper.\n\n- The discussion of the dynamic QA benchmarks is also lacking. For example, previous dynamic benchmarks such as Realtime QA [5], Streaming QA [6], Fresh QA [7], etc. are not mentioned and compared. All of the prior efforts aim to construct dynamic knowledge benchmarks in the QA format, from different sources of information. The proposed benchmark adds little to what we had considering the previous research, and its effort and contribution is limited for a full ICLR paper publication.\n\n- The paper is fairly clear, and the presentation is good. In terms of quality, it feels the analysis, benchmarking and evaluation are a bit contrived and toy, which makes it hard to believe this benchmark is going to make a good impact. For example, Figure 1 does not seem to be very helpful with the vast majority of more relevant research work (as mentioned above) not discussed, and it does not add to the point that the paper is making. With the fast pace of NLP/LLM research, the discussion is out of context of the most recent research. The analysis in Section 3 also does not add much to the paper - it is well expected that many of the old datasets are memorized by the LLMs, and the direction of constantly building dynamic benchmarks is an important one without this contrived analysis.\n\n- The message sent by the paper is not clear. It talks about dynamic benchmarking based on Wikipedia updates, and also conducts experiments with retrieval-augmented methods. It builds an 2021 benchmark and a 2025 benchmark, where the 2021 benchmark is memorized by the model. So the 2021 benchmark saw a higher accuracy from LLMs, and retrieval helps more on the newer 2025 benchmark since the knowledge is not in the model parameters. These are all expected results, and the main contribution remains unclear in terms of what it brings to the community. It feels like the paper pieces a few different things together without a clear message, especially given the prior efforts in this space, among which similar ideas have been well executed.\n\n- The benchmark is based on defined relation types and exact match for evaluation, limiting its practical usage for language variations and knowledge complexity. The size of the benchmarks is also small, 300 examples for each diff of Wikipedia graph. Significance and scalability are not demonstrated in the paper, again limiting its practical usefulness.\n\n- The discussion of search and retrieval in Related Work may lack clear logic. RAG is listed with search agents, and RL for search and reasoning, which is hard to parse. RAG is a general approach to leverage external knowledge, and the search results are also passed to LLMs typically for generation, so it is still RAG. It may be that specific approaches use different ways to formulate search queries, search different databases, etc. But they are all RAG for knowledge augmentation. I think the positions of search, RAG, search QA, etc. are not well narrated, rendering the paper a bit less rigorous in its focus and discussions.\n\n\n> [1] (2024 Dec) AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge\n\n> [2] (2024 Nov) ​​Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\n\n> [3] (2024 June) Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models\n\n> [4] (2024 June) Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models\n\n> [5] RealTime QA: What's the Answer Right Now?\n\n> [6] StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models\n\n> [7] FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation"}, "questions": {"value": "1. For the question generation based on the SRO triplets, the natural language is produced with pre-defined templates? Was there any LLM involved?\n\n2. How many Wikipedia updates are there typically from month to month, or from day to day? And out of those, how many can we effectively use for constructing the knowledge testing cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vEB4gZOWkF", "forum": "Y2JL8JXwmf", "replyto": "Y2JL8JXwmf", "signatures": ["ICLR.cc/2026/Conference/Submission4556/Reviewer_sRfe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4556/Reviewer_sRfe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965121003, "cdate": 1761965121003, "tmdate": 1762917437683, "mdate": 1762917437683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LiveSearchBench, an automatically constructed benchmark for evaluating LLMs’ retrieval and reasoning abilities under dynamic knowledge conditions. The core idea is to leverage temporal deltas between consecutive Wikidata snapshots to automatically synthesize QA instances whose answers depend on newly added or updated facts.\nThe pipeline consists of four stages: 1) differential knowledge extraction, 2) candidate filtering, 3) hierarchical question synthesis, and 4) SPARQL-based validation. These four stages ensure that every question has a unique, verifiable answer. Three difficulty tiers (L1 to L3) correspond to factual recall, compositional reasoning, and multi-hop reasoning with attribute fuzzing.\nExperiments on 600 synthesized QA instances (LiveSearchBench-2021 and LiveSearchBench-2025) show that all tested LLMs experience a significant accuracy drop when confronted with post-training knowledge, especially in multi-hop cases. Retrieval-augmented and RL-based models (e.g., Search-R1, SSRL) mitigate but do not close this \"recency gap\", highlighting the limits of static benchmarks and the need for temporally grounded evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] Important task with clear motivation. The paper directly addresses a specific underexplored task in current LLM evaluation: overreliance on static benchmarks that reward memorization. By curating questions in post-dated Wikidata changes, LiveSearchBench offers a principled way to isolate retrieval and reasoning over evolving knowledge. \n\n[S2] Methodology is sound and fully automated. The four-stage pipeline (delta extraction -> filtering -> synthesis -> SPARQL verification) is conceptually clean and sound. The strict COUNT=1 uniqueness constraint and final SPARQL re-validation ensure factual correctness without human annotation noise.. The hierarchical difficulty design (L1–L3) also makes the benchmark interpretable and extensible.\n\n[S3] Solid empirical results. Results are systematic and revealing: performance drops 50–60% on the 2025 batch, confirming that current models struggle with unseen, temporally novel information. Retrieval methods like RAG and Search-R1 yield clear gains (+20 absolute EM), and the observed relative improvement (>300%) over direct prompting underscores the framework’s discriminative power."}, "weaknesses": {"value": "[W1] Scale and manual review. Despite claiming full automation, the released set contains only 600 questions with some degree of human verification by five reviewers. This is sufficient for a proof-of-concept but small compared to existing QA benchmarks, potentially limiting statistical reliability and diversity of reasoning types.\n\n[W2] Evaluating more models will make the conclusions more convincing. The experiments focus on a small set of model families (Llama3.2, Qwen2.5) and retrieval-based methods. While results are informative, the scope could be extended to more varied models (e.g., closed-source models) and retrieval infrastructures (web vs. local). Moreover, metrics are limited to Exact Match; additional human or process-level evaluation would clarify reasoning quality versus retrieval precision."}, "questions": {"value": "[Q1] How stable is the SPARQL validation under frequent Wikidata schema changes? Are constraints (e.g., `COUNT=1`) consistently preserved across versions? Are there any nondeterminism in results observed (e.g., the results varying across queries)?\n\n[Q2] Is there a plan to include non-Wikidata deltas (e.g., Wikipedia or Common Crawl) to capture natural-language phrasing and reasoning beyond triples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q4nNwVacjA", "forum": "Y2JL8JXwmf", "replyto": "Y2JL8JXwmf", "signatures": ["ICLR.cc/2026/Conference/Submission4556/Reviewer_cErv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4556/Reviewer_cErv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972220858, "cdate": 1761972220858, "tmdate": 1762917437410, "mdate": 1762917437410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an automated pipeline to generated factual question answering (QA) benchmarks at any time, based on relations extracted from Wikidata updates.  The goal is to evaluate LLMs' ability to answer questions about facts that may have evolved after their training, by using retrieval augmentation, as well as to use reasoning to combine information from multiple sources. The benchmark is generated by extracting the relevant relations from Wikidata and prompting an LLM to generate QA pairs based on the extracted data. The questions are organized into 3 levels, ones involving a single simple lookup, ones combining information from multiple sources, and ones that combine multiple sources and also require some inference (for example, Ronaldo played for Al Nassr ==> Ronaldo played for a Saudian Arabian club).  \n\nThe motivation for a frequently updated, automatically generated benchmark is clear, and the general approach appears sound. However, I believe this paper is far from ready for publication (see weaknesses listed below) and so am recommending rejection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses a real need for benchmarks that evolve to stay ahead of LLMs' parametric knowledge.\n\n2. The approach of automatic benchmark generation via updated Wikidata snapshots is sensible (but see a minor concern about Wikidata under \"weaknesses\").\n\n3. The inclusion of the three question levels is helpful, as it makes it possible to separately evaluate search and reasoning abilities."}, "weaknesses": {"value": "1. There is insufficient analysis of the results.  Even with retrieval, the best performance on the simplest questions (level 1) is in the low 70s%.  I would expect the paper to include analysis of whether the errors come from search errors (this could be done by including an \"oracle\" search that provides a known correct document) or another source.  In the absence of such details, I am wondering how trustworthy the results are.\n\n2. There is insufficient detail provided about the human validation of the generated questions.  It is stated only that \"five PhD researchers reviewed the synthesized triples and reasoning paths\".  More information is needed (see questions below).\n\n3. It is hard to judge the quality of the generated QA pairs.  In the absence of convincing human evaluation results, I looked for example QA pairs.  No examples are given beyond the three QA pairs in Fig. 3, and the code base at the provided link also does not appear to include QA pairs.  Also, the code base is incomplete, with multiple missing files (as of this writing, the readme, the code for generation of level 2 and 3 questions, and multiple other files).  Note:  I don't consider the code base to be necessary for acceptance of the paper, but I mention it only because I was hoping to be able to extract QA examples from it.\n\n4. The writing is fairly poor.  While the general pipeline and results using the generated benchmarks are clear, there are some unclear or missing details and vague terms, while the rest of the text is quite repetitive.  For example, the terms \"Pass@k_{no_search}\", \"Pass@1_{search}\", \"retrieval@1\", \"Pass@N\", \"Pass@k\" are not defined.  Vague terms include \"headline performance\", \"real-world editing streams\", \"contemporaneous labels and templates\", \"reasoning paths\" -- all terms for which I can imagine some meaning, but they are not quite clear even in context.  Repetitive text examples:  the first two paragraphs appear to say essentially the same thing, the \"three key insights\" in the last paragraph of the intro also appear essentially the same, and the observations below Fig. 2 appear to be stated multiple times in different paragraphs.\n\n5. Minor: I believe relations in Wikidata tend to be fairly slow-varying, changing no more frequently than every several months (and indeed the experiments in the paper are based on pairs of snapshots 3 months apart).  Therefore, before a newly trained model can be tested, it may take a few months before a new benchmark version will be able to test it.\n\n6. Minor:  The paper does not mention some of the related prior efforts to build frequently updated factual QA benchmarks, such as FreshQA (https://arxiv.org/abs/2310.03214) and RealTimeQA (https://arxiv.org/abs/2207.13332).  StreamingQA would also be worth mentioning (although it was not, to my knowledge, updated after the paper publication).  I don't think any of these invalidate the contribution of the benchmark, since none of them are fully automatic.  But they deserve mention and comparison in the related work section.\n\n7. Minor: The QA generation relies on ChatGPT (the paper just mentions \"an LLM\", as far as I can tell, but the codebase uses GPT-5), a proprietary model whose details are unknown and can change at any time."}, "questions": {"value": "Regarding the human evaluation:  Who are the researchers who served as human validators (are any of them the authors or their colleagues?)?  How was the validation done?  Did the validation include only the triples and reasoning paths, or also the final questions?  What were the quantitative results of the evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vgTKZ0pEvT", "forum": "Y2JL8JXwmf", "replyto": "Y2JL8JXwmf", "signatures": ["ICLR.cc/2026/Conference/Submission4556/Reviewer_8yJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4556/Reviewer_8yJx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127794735, "cdate": 1762127794735, "tmdate": 1762917437206, "mdate": 1762917437206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}