{"id": "lDz41YfJVy", "number": 11356, "cdate": 1758197294041, "mdate": 1762924815590, "content": {"title": "VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?", "abstract": "Recent works demonstrated that long-chain reasoning paradigms can enhance capabilities of multimodal large language models (MLLMs) to solve complex problems. However, the precise reasons for the effectiveness of such paradigms remain unclear and difficult to probe. Specifically, it is challenging to analyze with quantitative results how much the model's extraction of visual cues and reasoning during the long-chain inference process contribute to its performance improvements. Therefore, evaluating the faithfulness of MLLMs' reasoning to visual information is crucial. To address this issue, we first present a cue-driven automatic and instruction-following image editing pipeline with GPT-Image-1. Furthermore, we introduce VFaith-Bench, the first benchmark to our knowledge to evaluate MLLMs' visual faithfulness when generating long reasoning process. Using the designed pipeline, we constructed comparative question-answer pairs by editing the visual cues in images that are crucial for solving the original reasoning problem, thereby changing the question's answer to another option. By testing similar questions with images that have different details, the average accuracy reflects the model's visual reasoning ability, while the difference in accuracy before and after editing the test set images effectively reveals the model's faithfulness of reasoning to visual cues. We developed a filtering mechanism based on multi-model detection to identify error reason and self-contradictory within images. This approach, combined with manual verification, effectively eliminates image quality degradation. We conducted in-depth testing and analysis of existing mainstream flagship models and prominent open-source model series/reasoning models on VFaith-Bench, further investigating the underlying factors of their reasoning capabilities. Our code and data will be open-sourced after review period.", "tldr": "", "keywords": ["Large Multimodal Models", "Multimodal Reasoning", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71a9db96a6e88438b99a3a84f07937b859eae2b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VFaith-Bench, a benchmark for evaluating the faithfulness of multimodal large language models (MLLMs) in incorporating visual cues during reasoning for visual question-answering tasks. The authors draw data from the M3CoT and MegaBench datasets, extract key visual cues from images, modify them so that the correct answer changes, and generate new images based on the updated cues using GPT-Image-1. This setup enables testing whether models genuinely rely on visual information rather than recalling memorized solutions. The authors evaluate a broad range of open and proprietary models on VFaith-Bench, providing insights into the degree of visual reasoning faithfulness across different MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and timely problem: assessing whether MLLMs can faithfully incorporate visual cues into their reasoning process is crucial for advancing multimodal reasoning research. This topic is of clear interest to the ICLR community.\n2. VFaith-Bench provides a well-targeted diagnostic benchmark for systematically studying visual faithfulness in MLLMs.\n3. The benchmark includes 755 samples spanning six task categories, and the semi-automated image-editing pipeline could be potentially applied to generate additional data in the future.\n4. The evaluation covers a diverse set of open and proprietary models, with analysis conducted from multiple perspectives, including performance on original versus edited images and auxiliary metrics capturing memorization and perceptual grounding.\n5. The paper is relatively clearly written and easy to follow."}, "weaknesses": {"value": "1. The benchmark’s ability to reveal memorization effects is inherently limited. It can only expose potential memorization if the evaluated models were trained on data overlapping with M3CoT and MegaBench, which together contain around 20k instances. The authors do not verify whether these datasets were part of the training corpora of the evaluated open-source models. If they were not, the benchmark’s capacity to probe faithfulness issues related to memorization would be significantly reduced.\n2. The authors acknowledge limitations of the GPT-Image-1–based editing pipeline in Section 3.2.1 and attempt to mitigate them through manual review. However, the analysis of “Reason 3” errors in Section 4.3 reveals that roughly 10% of the dataset still contains low-quality images that were missed in human filtering. The authors then remove these samples and recompute results (the “Refined” columns in Tables 2-3). This finding undermines confidence in the overall data quality and suggests that many reported results may be affected by this issue. Moreover, this crucial limitation appears only late in Section 4.4, disrupting the narrative flow and potentially misleading readers about dataset reliability.\n3. The design of the Perception task raises questions. It is framed as a multi-choice task rather than as a set of binary classification subtasks, each dedicated to a specific visual cue. To me, the latter formulation would arguably be more well-defined and interpretable, allowing models to focus on isolated factors.\n4. The Repeat Ratio metric is an interesting idea but conceptually underdeveloped and insufficiently explained. As currently defined, it measures—among cases where the model was initially correct—the proportion of incorrect answers (after image editing) that repeat the original correct answer. The metric only considers examples where the model was correct before editing, which biases comparisons across models with different base accuracies. It also ignores cases where models consistently repeat wrong answers, which could equally indicate reliance on memorization.\n5. In the Evaluation Pipeline, the authors state that direct output matching was applied only for certain multiple-choice questions, while in other cases, evaluation was conducted using Claude-3.5-Sonnet. However, the paper does not specify which dataset categories use which evaluation protocol. Moreover, crucial details about the LLM-as-a-judge setup are missing, making it difficult to assess the reliability of the reported results. For instance, were the model-based evaluations correlated with human judgments to confirm their validity? Without such evidence, the faithfulness of the results obtained through automated grading remains questionable.\n6. The qualitative analysis section provides limited insight. The authors do not discuss performance trends across dataset categories. For example, models occasionally perform better on edited than on original images in certain tasks (e.g., CRE and TIF, as with Gemini-2.5-Pro on TIF), yet this outcome is not analyzed or explained. A category-level breakdown or error analysis would have improved interpretability.\n7. The experimental methodology in Section 4.3 raises significant concerns. Model responses are graded by Gemini-2.5-Pro and subsequently verified using Claude-3.5-Sonnet. However, Gemini-2.5-Pro itself performs imperfectly on VFaith-Bench (see Table 2), which undermines its reliability as an evaluator of other models’ reasoning. Additionally, Claude-3.5-Sonnet is never evaluated on the benchmark, leaving its assessment accuracy entirely untested. To ensure validity, the authors should manually review a subset of responses, categorize error types, and quantify agreement between human and model-based judgments."}, "questions": {"value": "1. M3CoT contains over 11K samples and MegaBench over 8K, yet VFaith-Bench includes fewer than 1K samples. Could the dataset be expanded further using your semi-automated pipeline?\n2. How many human annotators were involved in the Image Quality Checking Pipeline? How were disagreements handled, and what was the inter-annotator agreement (e.g., Cohen’s κ)?\n3. Did the authors verify whether M3CoT or MegaBench were part of the training data of the evaluated open-source models? If not, how do the authors ensure that observed memorization or faithfulness issues are not attributable simply to unseen data?\n4. Which dataset categories were evaluated via direct output matching versus Claude-3.5-Sonnet judgment?\n5. Why was the Perception task formulated as a multi-choice question instead of several binary classification tasks, each isolating a single visual cue?\n6. Since Gemini-2.5-Pro and Claude-3.5-Sonnet were used for model grading, did the authors perform any human–LLM agreement analysis to confirm that this evaluation pipeline is reliable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R8DqlsXZjU", "forum": "lDz41YfJVy", "replyto": "lDz41YfJVy", "signatures": ["ICLR.cc/2026/Conference/Submission11356/Reviewer_sYoe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11356/Reviewer_sYoe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598891162, "cdate": 1761598891162, "tmdate": 1762922489132, "mdate": 1762922489132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gjH0YFHWfZ", "forum": "lDz41YfJVy", "replyto": "lDz41YfJVy", "signatures": ["ICLR.cc/2026/Conference/Submission11356/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11356/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924813522, "cdate": 1762924813522, "tmdate": 1762924813522, "mdate": 1762924813522, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current MLLMs often achieve high performance on complex reasoning tasks, but it's unclear if this success comes from genuine visual reasoning or from exploiting brittle patterns and memorized data. Existing benchmarks lack the ability to quantitatively assess the visual fidelity of MLLMs' reasoning. This paper introduces VFaith-Bench, a new benchmark designed to evaluate how MLLMs reason based on visual information, rather than simply relying on memorized patterns from training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a unique cue-driven editing pipeline for generating multimodal benchmark data to induce hallucinations and probe reasoning chains. The evaluations revealed deficiencies in visual cue perception and adherence, as well as potential data leakage, providing insights for developing more reliable MLLMs.\n\nUnlike prior multimodal reasoning benchmarks (e.g., HallusionBench), this work goes beyond assessing raw reasoning ability — it probes how much of the reasoning is truly grounded in visual cues rather than memorized patterns or prior context.\n\nThe experiments reveal consistent insights, e.g., performance degradation after cue modification, discrepancies between perception and reasoning, and evidence of data leakage or memorization. All of which are empirically meaningful and reproducible findings that advance understanding of multimodal reasoning mechanisms."}, "weaknesses": {"value": "1. The paper does not specify how many visual cues are extracted for each image, which is crucial because the number and granularity of cues directly affect the subsequent image editing process.\n\n2. After large models extract vision cues, there appears to be no human validation or quality control to verify whether these cues are accurate or relevant. Similarly, in Section 4.3, the identification of error reasons (Reason 1–3) is entirely model-driven, without human cross-checking. This lack of human validation could compromise the credibility of the experimental results.\n\n3. Although the paper reports the overall dataset size (755 samples) and the distribution across task types, it does not provide detailed benchmark statistics.\n\n4. Table 2 presents detailed metrics for each subset, while Table 3 (for smaller reasoning models) only reports overall accuracy and ∆, without per-subset breakdowns. It would be helpful to include fine-grained results to show whether smaller models exhibit consistent performance degradation across all categories.\n\n5. The paper is written in a rush, so there are many typos: \n\n in the abstract, \"long-chain inference process contribute to its performance improvements\" contribute --> contributes\n \nline 204: we propose a MLLM reasoning output paradigm --> an MLLM\n\nline 468: after refine--> after refinement\n\nline 447: The proportion of errors answers --> error answers"}, "questions": {"value": "It is unclear why Section 4.4 (“Image Editing Quality Control and Refine”) is introduced as a separate stage. If low-quality edited images were detected, why were they not filtered out earlier during the human validation phase? The authors could have incorporated a rubric for image quality into the initial human validation process instead of conducting post-hoc filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LDKKFSOcl6", "forum": "lDz41YfJVy", "replyto": "lDz41YfJVy", "signatures": ["ICLR.cc/2026/Conference/Submission11356/Reviewer_1ojn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11356/Reviewer_1ojn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629931071, "cdate": 1761629931071, "tmdate": 1762922488677, "mdate": 1762922488677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the unclear link between MLLMs’ visual input and reasoning, and lack of metrics for visual faithfulness. It proposes: 1) A cue-driven auto-editing pipeline with GPT-Image-1 to modify key visual cues, building comparative QA pairs. 2) VFaith-Bench (755 entries, 6 subsets including a perception task) to evaluate MLLMs’ visual faithfulness in long reasoning. 3) A multi-model filtering mechanism plus manual checks to ensure image quality. Evaluations on closed-source and open-source models show all models drop in accuracy post-editing, with hallucinations and over-reliance on training patterns."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The major strengths are as follows:\n\nS1. This paper is well written and is easy-to-follow.\n\nS2. The studied task is interesting and important in the research field.\n\nS3. The authors propose a good baseline to make data."}, "weaknesses": {"value": "The weaknesses are clear from my point of view.\n\nW1. From the methodology side. As a machine-generated benchmark, its quality and diversity are influenced by the machine. No new knowledge will be created.\n\nW2. From the statistics side, some core details are also missed. For example, the distribution of the question, answers, comparisons with existing related benchmarks.\n\nW3. The difficulty of the benchmark is limited. According to Tab. 2, the Gemini-2.5-pro can correctly answer 84.78% questions.\n\nW4. Lack of a technical baseline to advance model performance."}, "questions": {"value": "See weaknesses. In addition,\n\nQ1. How will you distribute the benchmark to avoid data leakage?\n\nQ2. How about using your data engine for model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kmQGEZozNy", "forum": "lDz41YfJVy", "replyto": "lDz41YfJVy", "signatures": ["ICLR.cc/2026/Conference/Submission11356/Reviewer_5C65"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11356/Reviewer_5C65"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964532072, "cdate": 1761964532072, "tmdate": 1762922488261, "mdate": 1762922488261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed VFaith-Bench, a benchmark to evaluate the visual faithfulness of Multimodal Large Language Models (MLLMs) when generating long reasoning process. Multiple experiments with several MLLMs show that the produced VFaith-Bench provide good challenges for both the open-source and proprietary models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Both the motivation of evaluating MLLMs' faithfulness and the proposed VFaith-Bench make sense and are technically sound to me. Without a full ablation based benchmark like the one proposed in the draft, it is difficult to figure out the faithfulness of the responses from MLLMs.\n2. The authors conducted extensive experiments on both open-source and proprietary models and show that they all suffer from the lack of visual faithfulness.\n3. Writing is good and easy to follow."}, "weaknesses": {"value": "1. My main concern is the heavy dependence on MLLMs themself in the curation of the benchmark. While it help automate the pipeline and make it more scalable, it is a bottleneck and capped by the capability of the models used. \n2. The benchmark is limited to multiple-choice questions which is only a small portion of the whole spectrum of evaluations."}, "questions": {"value": "Please refer to the paper weakness section for more details and provide more justification on the proposed benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EPVdXCZrDf", "forum": "lDz41YfJVy", "replyto": "lDz41YfJVy", "signatures": ["ICLR.cc/2026/Conference/Submission11356/Reviewer_dL7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11356/Reviewer_dL7h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979631896, "cdate": 1761979631896, "tmdate": 1762922487827, "mdate": 1762922487827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}