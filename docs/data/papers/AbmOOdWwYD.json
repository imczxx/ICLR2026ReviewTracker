{"id": "AbmOOdWwYD", "number": 21222, "cdate": 1758315112405, "mdate": 1759896933669, "content": {"title": "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment", "abstract": "We introduce Perceptual-Initialization (PI), a paradigm shift in visual representation learning that incorporates human perceptual structure during the initialization phase rather than as a downstream fine-tuning step. By integrating human-derived triplet embeddings from the NIGHTS dataset to initialize a CLIP vision encoder, followed by self-supervised learning on YFCC15M, our approach demonstrates significant zero-shot performance improvements without any task-specific fine-tuning across 29 zero shot classification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains emerge after approximately 15 epochs of pretraining. Benefits are observed across datasets of various scales, with improvements manifesting at different stages of the pretraining process depending on dataset characteristics. Our approach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and retrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks, without requiring any adaptation to target domains. These findings challenge the conventional wisdom of using human-perceptual data primarily for fine-tuning and demonstrate that embedding human perceptual structure during early representation learning yields more capable and vision-language aligned systems that generalize immediately to unseen tasks. Our work shows that \"beginning with you\", starting with human perception, provides a stronger foundation for general-purpose vision-language intelligence.", "tldr": "Perceptual-Initialization (PI) yields faster, stronger zero-shot performance", "keywords": ["Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/241518ca9596b446310c4acee13ba15f7965cc96.pdf", "supplementary_material": "/attachment/3fd709e511d0bc31bd790f9997d53871cb64d1c6.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Perceptual-Initialization (PI), which initializes the CLIP vision encoder using human perceptual similarity data (NIGHTS triplets) before standard large-scale image-text contrastive training on YFCC15M.\nCompared with random initialization and with post-hoc perceptual fine-tuning, the proposed method yields consistent zero-shot gains across 29 classification and 2 retrieval benchmarks. The authors argue that embedding human perceptual priors at the start of training leads to faster convergence and more human-aligned representations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel use of human perceptual priors as initialization rather than alignment fine-tuning.\n2. Comprehensive evaluation over diverse datasets shows consistent positive gains.\n3. Very low additional compute cost.\n4. Clear comparison showing that late perceptual fine-tuning disrupts alignment and opens new direction for human or brain aligned pretraining."}, "weaknesses": {"value": "1. No experiments using random or pseudo perceptual triplets to isolate the contribution of human perceptual structure. \n2. The approach is validated only on NIGHTS; applicability to richer datasets remains untested. \n3. No probing or visualization is provided to show how perceptual initialization changes internal feature space or similarity structure compared to the baseline."}, "questions": {"value": "1. Could the authors analyze which visual attributes benefit most from perceptual initialization (e.g., texture vs. shape bias)?\n2. Does PI primarily affect the early layers or propagate to higher-level semantics during contrastive training?\n3. How much perceptual data is necessaryâ€”does performance saturate after a certain fraction of NIGHTS triplets?\n4. Could PI be combined with supervised or robust-CLIP initializations, or would they interfere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LRsUlpfXhK", "forum": "AbmOOdWwYD", "replyto": "AbmOOdWwYD", "signatures": ["ICLR.cc/2026/Conference/Submission21222/Reviewer_rYhZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21222/Reviewer_rYhZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391239357, "cdate": 1761391239357, "tmdate": 1762941631903, "mdate": 1762941631903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two stage training paradigm  for VLMs like CLIP arguing the benefits of perceptual initialization (PI) over random initialization. Further it argues that incorporating PI in initialization phase more advantageous than post-hoc finetuning. The main contribution is demonstrating that this early-stage alignment provides a stronger foundation for general-purpose VLM intelligence. PI models show significant zero-shot performance improvements, without any task-specific fine-tuning, across a comprehensive suite of 29 classification and two retrieval benchmarks. PI approach consistently outperforms a randomly-initialized baseline, and a direct comparison shows that post-hoc perceptual fine-tuning is catastrophic to V-L alignment."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**originality**: Lveraging supervised human behavioural data as a foundational inductive bias in the model intialization is a novel idea that opens a new research direction. The works provided a provides a structured solution that converts often ignored variance of random initialiation into a principled prior.\n\n**Significance**:  PI paradigm is the core strength of the paper. It uses the supervised human perceptual data to initialize a VLM parameter prior to large scale pretaining, provide a potent to human aligned inductive bias right from time t=0.\n\n**Quality**: The provided results empirically validate the PI hypothesis, having  consistent performance gains, outperforming 23/29 classification tasks. Further, it shows how post-hoc finetuninig leads to catastrophic forgetting.\n\n**writing**: The argument for PI is presented logically, starting from the known \"path-dependency\" of deep networks and the variance of random seeds, making the motivation for a principled initialization intuitive."}, "weaknesses": {"value": "**Limited scope of the prior**: Only the vision encoder is initialized with PI and the text encoder is still randomly initialized and trained from scratch. What is the reason for this choice for the experiments? \nCLIP like model operates on the shared latent space of vision and text modalities. The paper could be strenthened by exploring complementary intialization of text encoder, to see if such complete model with PI initialization provides synergistic benefits.\n\n**Perceptual Loss**:  The core of the PI benefits lays in the perceptual loss function which is derived from a previous works. There's no/lack of evidence/interpretation (apart from the final results) provided on how does this loss function work/fail in the assumed context:  pretraining vs post-hoc finetuning.\n\n**Mechanistic Analysis**: While efficacy id proven, the paper does not delve into why the inductive bias remains so effective after 32 epochs, where the post-hoc finetuning fails.  This theoretical insight is critical to see the compatibility of leveraging this idea to different models or scenarios. Many of the questions in **Questions** section could not answered from the given content of the paper.\n\n**Limited evaluation**: current training uses 15M image-text pairs, while this is substantial, SOTA VLMs often trained on hunderds of millions or biilions of pairs. Will the proportional gains from PI would persist, diminsh or grow continuously (Though limited scling law provided in the paper). In failure cases, how PI should be addressed?"}, "questions": {"value": "- Do the PI weights remain closer to the perceptiual optimum throughout training?\n- How does the learned Image-Text alignment module interact differently with the PI-derived features versus the baseline features? How does the shared representation space differ?\n- Have any preliminary experiments been conducted to determine the minimal amount of human perceptual data required in Stage 1 to achieve a statistically significant positive gain?\n- Why/How does this loss function work?\n- Can the authors analyze the evolution of the logit scaling parameter ($\\tau$) in Stage 2?\n- For failure cases, should the perceptual prior be \"re-anchored\" at intermediate stages, or perhaps weakened by introducing a temperature parameter to the perceptual loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gpv46yhu4g", "forum": "AbmOOdWwYD", "replyto": "AbmOOdWwYD", "signatures": ["ICLR.cc/2026/Conference/Submission21222/Reviewer_NrFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21222/Reviewer_NrFX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040310614, "cdate": 1762040310614, "tmdate": 1762941631649, "mdate": 1762941631649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new visual representation learning scheme called Perceptual-Initialization, which trains the visual encoder to match human preference before the contrastive learning. Specifically, the human preference alignment is achieved using a triplet contrastive loss on the NIGHT dataset and the resultant model weights are used as the initialization of the formal contrastive learning. PI achieves zero-shot performance improvements on a variety of image classification and retrieval benchmarks compared to the baseline CLIP."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed method is novel, simple yet effective. The promising results of the paper can encourage following researches exploring other initialization strategies.\n- Results in zero-shot image classification and retrieval tasks demonstrates that PI scales as the data volume increases, indicating the method's potential in large-scale training.\n- The paper is well organized and nicely presented. The ending section points out remaining challenges faithfully and offers valuable insights, strengthening its contribution to the field."}, "weaknesses": {"value": "- The proposed method limits its scope for the initialization of CLIP type model, despite that the human preference alignment is independent to the text encoder. The author could add experiments on other visual backbones such as vanilla ViTs to fully explore the potential of the method."}, "questions": {"value": "- As mentioned in the weakness part, I'm wondering if PI could also benefit other types of visual pretraining?\n- Additionally, does the model trained using PI demonstrates stronger transferability compared to normal training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "POruyhI5u2", "forum": "AbmOOdWwYD", "replyto": "AbmOOdWwYD", "signatures": ["ICLR.cc/2026/Conference/Submission21222/Reviewer_uwto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21222/Reviewer_uwto"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051576804, "cdate": 1762051576804, "tmdate": 1762941631389, "mdate": 1762941631389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}