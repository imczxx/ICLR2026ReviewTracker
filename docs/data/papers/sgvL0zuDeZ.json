{"id": "sgvL0zuDeZ", "number": 6561, "cdate": 1757988965569, "mdate": 1759897907863, "content": {"title": "OpenAVS: Training-Free Open-Set Audio Visual Segmentation with Foundational Models", "abstract": "Audio-visual segmentation (AVS) aims to separate sounding objects from videos by predicting pixel-level masks based on audio signals. Existing methods primarily concentrate on closed-set scenarios and direct audio-visual alignment, which limits their capability to generalize to new, unseen situations. In this paper, we propose OpenAVS, a novel training-free language-based approach that, for the first time, effectively aligns audio and visual via text proxy for open-vocabulary AVS. Equipped with multimedia foundation models, OpenAVS directly infers masks through 1) audio-to-text description generation, 2) visual-to-text description generation, 3) LLM-guided prompt translation, and 4) text-to-visual sounding object segmentation. The objective of OpenAVS is to establish a simple yet flexible architecture that harnesses the strengths of appropriate foundation models, thereby maximizing their potential for effective knowledge transfer to downstream AVS tasks. Moreover, we present a model-agnostic framework OpenAVS-ST that enables the integration of OpenAVS with any advanced supervised AVS model via pseudo-label based self-training. This approach enhances performance by effectively utilizing large-scale unlabeled data when available.\nComprehensive experiments on four benchmark datasets demonstrate the superior performance of OpenAVS. It surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a significant margin, achieving absolute performance gains of 3.9% ~ 6.7% and 2.2% ~ 4.9% in mIoU and F-score, respectively, in challenging scenarios.", "tldr": "", "keywords": ["Open-Vocabulary Audio-Visual Segmentation", "Multimedia Foundation Models", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fb5827ea3387f5b4230d7eced4f84ad3572f65e.pdf", "supplementary_material": "/attachment/fc7cc44189a7b509176d9a9b5bbf7a259c64b251.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose OpenAVS, a novel training-free language-based approach that, for the first time, effectively aligns audio and visual via a text proxy for open-vocabulary AVS. But there is still work to do."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method seems feasible.\n2. The writing is clear and easy to read.\n3. Enough visualization is provided."}, "weaknesses": {"value": "1. The citation style appears inconsistent with ICLR guidelines. Mixing \\citet{} and \\citep{} hinders readability and should be standardized.\n\n2. Integrating multiple mature systems introduces clear drawbacks. Please include a runtime comparison for each model in Table 3, and analyze potential error accumulation across the pipeline.\n\n3. The paper evaluates on S4, MS3, AVSS, and V3 from AVSBench, which are typically used in closed-set settings. Even though the method is training-free, comparisons against closed-set trained baselines are still important. Moreover, evaluation on an open-vocabulary AVS dataset would be more appropriate, given that LLMs operate in an open-vocabulary regime.\n\n4. Evaluation details are missing. Please report the inference resolution and the average input/output token counts for the LLM.\n\n5. You state “GroundingDINO 1.0 for detection (box threshold = 0.25).” How critical is the box threshold? Provide an ablation or sensitivity analysis to justify the chosen value.\n\n6. In Figure 3, AVSS is a classification task where every mask should be assigned a color. Why do the GT and other model outputs lack color assignments? Please clarify the visualization protocol or correct the figure.\n\nExtra: I think this training-free method based on LLMs is naturally suitable for Ref-AVS [2] tasks. Why not test on Ref-AVS tasks\n\n[1] Guo, R., Qu, L., Niu, D., Qi, Y., Yue, W., Shi, J., ... & Ying, X. (2024, October). Open-vocabulary audio-visual semantic segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia (pp. 7533-7541).\n\n[2] Wang, Y., Sun, P., Zhou, D., Li, G., Zhang, H., & Hu, D. (2024, September). Ref-avs: Refer and segment objects in audio-visual scenes. In European Conference on Computer Vision (pp. 196-213). Cham: Springer Nature Switzerland."}, "questions": {"value": "My main concern is the evaluation and analysis of the overall system. I will finalize my score after the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XQh8icnEQC", "forum": "sgvL0zuDeZ", "replyto": "sgvL0zuDeZ", "signatures": ["ICLR.cc/2026/Conference/Submission6561/Reviewer_2d29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6561/Reviewer_2d29"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760605462358, "cdate": 1760605462358, "tmdate": 1762918901530, "mdate": 1762918901530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a training-free, open-vocabulary audio-visual segmentation (AVS) framework, OpenAVS, which uses language as a mediator to alleviate cross-modal misalignment in multi-source and temporally drifting scenarios. The method first converts audio and visual frames into semantic text via audio language models (ALMs) and vision-language models (VLMs), then employs a large language model (LLM) to translate and consolidate prompts with model/prompt/frame consistency constraints. The refined noun-centric prompts guide visual foundation models (VFMs) (e.g., Grounded-SAM/SAM2) to produce pixel-level masks frame-by-frame. TThe method attains competitive results on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1). The paper is generally well written and easy to follow, with a clear problem setup and pipeline description.\n\n2). The method shows reasonable performance without task-specific training in the reported settings.\n\n3). The pipeline is modular and can be instantiated with alternative ALMs/VLMs or VFMs with minimal changes.\n\n4). By operating in text space, the approach can accommodate unseen categories to a limited extent, offering broader semantic coverage than fixed-class models."}, "weaknesses": {"value": "1). Incremental contribution. The framework largely assembles off-the-shelf components, and similar workflow-style pipelines have appeared in prior work [1–3], including AVSS/AVVS systems that convert audio into textual/context cues to guide frame selection [1].\n\n2). Limited novelty of ALM usage. The headline contribution—introducing an ALM to obtain text from audio—seems straightforward. Related LLM/agent literature [4, 5] already invokes specialized modules coordinate tasks, which makes the methodological advance here appear modest.\n\n3). Multi-source disambiguation is insufficiently analyzed, particularly for overlapping or co-occurring sound sources and attribution when two similar sources are active.\n\n4). The approach may be sensitive to prompt templates and LLM choices, and the paper provides limited ablations on prompt variants and threshold settings.\n\n5). Latency concerns. The reported inference time (≈5.13–6.71 s for the best settings) suggests the method is far from practical deployment. For typical videos at 25–35 FPS, real-time operation would require ~28–40 ms per frame; the current latency is orders of magnitude higher unless substantial optimization is shown.\n\n6). Questionable cost–effectiveness. The best configuration (e.g., OpenAVS-Large + GPT-4o-mini + GDINO+SAM2) attains mIoU 0.684 / F 0.769 at ~$0.00163 per sample, yet still trails task-specific trained baselines. It is unclear why one should pay per-inference costs for comparatively lower accuracy. Conversely, lighter variants (e.g., OpenAVS-Lite + GPT-2 XL + GDINO+SAM at mIoU 0.431 / F 0.561) fall below thresholds that would be usable in practice.\n\n7). Metric concerns: the reviewer highlights that Jaccard and IoU differ in segmentation evaluation. I refer to the SAM2 paper to emphasize this distinction and note that previous methods were computed using Jaccard.\n\n\n[1] Unleashing the temporal-spatial reasoning capacity of gpt for training-free audio and language referenced video object segmentation.\n\n[2] Open-Vocabulary Audio-Visual Semantic Segmentation \n\n[3] Retrieval-Augmented Generation for AI-Generated Content: A Survey\n\n[4] Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools\n\n[5] Mind2web: Towards a generalist agent for the web"}, "questions": {"value": "nil."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "29auPsmvbT", "forum": "sgvL0zuDeZ", "replyto": "sgvL0zuDeZ", "signatures": ["ICLR.cc/2026/Conference/Submission6561/Reviewer_CRVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6561/Reviewer_CRVn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831138327, "cdate": 1761831138327, "tmdate": 1762918901183, "mdate": 1762918901183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a novel training-free language-based approach for open-set audio visual segmentation. \nThe language-based method achieves more robust audio-visual alignment than existing methods.\nThis is a flexible and cost-efficient framework, and achieves state-of-the-art performance on four benchmark and training-free, few-shot, and zero-shot AVS tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This is  a flexible, model-agnostic and cost-efficient framework.\n* Experiments on 4 benchmarks show the good performance of OpenAVS.\n* It surpasses existing unsupervised, zero-shot, and few-shot AVS methods."}, "weaknesses": {"value": "* I believe this method is likely to work effectively, as none of the steps appear problematic. However, it seems more focused on the engineering aspect rather than offering new insights to the community.\n\n* Are there any specific challenges or difficulties in applying this pipeline to solve the task?\n\n* Given the large number of models used, is it reasonable to combine so many for this task, especially in comparison to other approaches?\n\n* I am unclear about how few-shot AVS is applied in the proposed method. The pipeline appears primarily focused on generating masks from audio signals. Where are the few-shot examples integrated into this process?\n\n* More details on the self-training process would be helpful. For example, which specific parts of the model are being trained during this phase?\n\n* Could you clarify the role of the V2T component? It might also be beneficial to conduct ablation studies to better understand the contributions of both the A2T and V2T components.\n\nI would be happy to revise my score if the author addresses these points."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1tblRjyfF6", "forum": "sgvL0zuDeZ", "replyto": "sgvL0zuDeZ", "signatures": ["ICLR.cc/2026/Conference/Submission6561/Reviewer_sshA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6561/Reviewer_sshA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894200352, "cdate": 1761894200352, "tmdate": 1762918900665, "mdate": 1762918900665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on audio-visual segmentation, i.e., separating sounding objects from videos by predicting pixel-level masks of audio signals. Instead of direct audio-text alignment, this paper use one training-free idea, namely OpenAVS, to aligns audio and visual via text proxy. And the pipeline is divided into audio-to-text description generation, visual-to-text description generation, LLM-guided prompt translation, and text-to-visual sounding object segmentation. Experiments are carried out on four benchmarks, across unsupervised, zero-shot, and few-shot AVS settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[+] The manuscript is well written, with clear logics and sufficient formulations.\n\n[+] Exploring the omni-modal alignment of image-text-audio is one promising direction.\n\n[+] Experiments are conducted across unsupervised, zero-shot, and few-shot AVS settings."}, "weaknesses": {"value": "[-] The bottleneck of information compression. Text is a highly compressed form of audio/images, which means a lot of information is lost when converting audio/image to text. Many textual descriptions are coarse-grained, for example, audio-to-text usually cannot capture differences between dog breeds. If a video contains two different breeds of dogs, OpenAVS may fail after converting audio/image to text.\n\n[-] The noise is gradually amplified. This paper’s idea is severely limited by the performance of the audio/image to text pre-trained models. What should be done if there is no commonality in the text converted from images/audio? Usually, the pre-trained models can only convert simple objects/noiseless timbres, which means that the performance of this paper is limited in practical scenarios.\n\n[-] How to solve complex scenarios. Across all datasets in this paper, the scenes are simple and quite detached from reality. How does the performance of  OpenAVS work, especially when dealing with mixed-sound, multi-entity and off-screen scenes?  \n\nWhat’s Making That Sound Right Now? Video-centric Audio-Visual Localization. ICCV2025"}, "questions": {"value": "[-] In OpenAVS, multiple pre-trained inferences are concatenated, which results in low inference efficiency. What is the RT of the overall process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5oMkNsk00D", "forum": "sgvL0zuDeZ", "replyto": "sgvL0zuDeZ", "signatures": ["ICLR.cc/2026/Conference/Submission6561/Reviewer_k19a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6561/Reviewer_k19a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903466537, "cdate": 1761903466537, "tmdate": 1762918899854, "mdate": 1762918899854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}