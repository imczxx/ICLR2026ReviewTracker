{"id": "WOIf5MGJXB", "number": 22878, "cdate": 1758336588527, "mdate": 1759896841519, "content": {"title": "Efficient Inference with Large Reasoning Models", "abstract": "Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR) that detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about x3.7 (from 4799 to 1290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs", "tldr": "", "keywords": ["Thinking Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e9d23b8bf9c820a13c8a3ccbb12edc5704732aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ESTAR (Early-Stopping for Token-Aware Reasoning) to address the reasoning redundancy problem in Large Reasoning Models (LRMs). The main idea is to detect and reduce this redundancy using a LightGBM classifier. The tuning process includes SFT on a dataset with the <stop> token inserted, and the finetuned model is further trained using GRPO with a compute-aware reward function. ESTAR can largely reduce output tokens and maintain accuracy, providing a better accuracy-efficiency trade-off than other methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper tackles a practical problem. Redundant reasoning is a major source of inefficiency in LRMs, and the goal of stopping early without sacrificing accuracy is a valuable insight.\n- The multi-stage design of ESTAR is a strong point. Combining a lightweight classifier with a model trained to propose its own stop tokens is a clever way.\n- The reported results are good. A 3.7x token reduction while maintaining 98.9% of the original accuracy would be a strong contribution."}, "weaknesses": {"value": "- This paper mainly claims efficiency, but the experiments simply translate this to output tokens, which ignores the newly involved overhead. When using highly optimized inference engines like vllm or sglang, this method may break the workload, so I would like to see if there is end-to-end testing.\n- The consistency and accuracy definitions may have problems and even bring contradiction in training. The LITE classifier and SFT focus on consistency, but the RL step is using an accuracy signal, which may conflict when the model's answer is wrong.\n- The robustness of both training and evaluation is a huge concern. You mentioned temperature and top-p, but it seems only sampled once for both training and evaluation. Sampling can have a huge impact on reasoning output and even the final answer.\n- As your method evolves through its 3 stages, there is a lack of ablations on how each component helps with the final result.\n- The paper lacks details, especially for baseline methods. The repository you provided is also empty (as of 10/31/2025)."}, "questions": {"value": "- How does this method deal with batch scenarios?\n- How do you deal with wrong answers, specifically in the LITE classifier step and the SFT step?\n- Have you tried random sampling for the training data? How will different samples from the same question influence the result?\n- Will the classifier be updated in the SFT and RL steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V6MpWplBKW", "forum": "WOIf5MGJXB", "replyto": "WOIf5MGJXB", "signatures": ["ICLR.cc/2026/Conference/Submission22878/Reviewer_eejB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22878/Reviewer_eejB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896596358, "cdate": 1761896596358, "tmdate": 1762942422575, "mdate": 1762942422575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel three-stage approach called Early-Stopping for Token-Aware Reasoning (ESTAR) that detects and reduces reasoning redundancy without sacrificing accuracy for LRMS. The approach consists of: (1) Using a lightweight classifier (LightGBM) to identify optimal early-stopping positions in reasoning trajectories; (2) Training the LRM via Supervised Fine-Tuning (SFT) to autonomously generate `<stop>` signals based on these positions; and (3) Employing Reinforcement Learning (RL) to further refine the model's self-generation of these `<stop>` signals. Experimental results across four benchmarks indicate that ESTAR achieves a significant reduction in reasoning length (approximately 3.8x) while maintaining task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear and the topic is well presented.\n\n2. The proposed ESATR method is described in sufficient detail and appears technically sound.\n\n3. The experimental results are strong, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "### Major Concerns:\n\n1. Overly General Title: The paper's title is too broad. \"Efficient inference with LRMs\" can be achieved through many different approaches (e.g., architectural changes). The current title does not accurately reflect the paper's contribution.\n\n2. Empty Code Repository: The provided anonymous repository for code is currently empty.\n\n3. Presentation and Formatting Issues: The core methodology sections (Preliminaries and Methods) suffer from presentation issues. The text appears \"messy\" and contains inconsistent formatting. Specific issues include:\n\n- Awkward or unusual line breaks in the Preliminaries section.\n- The use of non-standard markers, such as \"(RQ1)\", within the body of the Methods section, which seem out of place in a formal paper.\n- Inconsistent formatting of mathematical equations between the Preliminaries and Methods sections.\n\n4. Unclear Statements: Several statements in the paper are confusing or incomplete and require clarification:\n- L123, ”... update ESTAR-LITE to stay aligned with the new trajectories.“\n- L195, \"... the tabular classifier also confirms the consistency of the model’s earlystop answer.\"\n- L251, \"Then we apply regular teacher ...\"\n\n### Minor Issues:\n\n1. L121, ESTAR-FT is used before its formal definition.\n\n2. L203, inconsistent notations vs. Eq. (1)."}, "questions": {"value": "The core methodology and the reported results are promising, and this work appears to be a valuable contribution to the community. However, the paper's current writing is a significant concern. In order to raise my rating, I would like the authors to address the different points in the major concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CYzD1zuieM", "forum": "WOIf5MGJXB", "replyto": "WOIf5MGJXB", "signatures": ["ICLR.cc/2026/Conference/Submission22878/Reviewer_qWyZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22878/Reviewer_qWyZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920040348, "cdate": 1761920040348, "tmdate": 1762942422371, "mdate": 1762942422371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work combines truncation-base and fine-tuning-base approaches for efficient reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Efficient reasoning is one of the more important fields for LRMs.\n- Testing consistency + accuracy is a good point. I am convinced."}, "weaknesses": {"value": "The experiment execution seems to be on the weaker side.\n- Only evaluated on two Qwen3 models.\n- Baselines are mainly only featured in Table 2, but not all tasks and models. \n- Questionable decoding budget as indicated in Table 1—the vanilla decoding is often <5k.\n- No end-to-end latency/throughput efficiency report.\n\nAlso, there should be more discussion and comparison of other efficient reasoning methods. Without digging too much, this work is already missing some key comparisons.\n- o1-pruner is a well-recognized fine-tuning-based method for efficient reasoning.\n- FlashThink, which is cited in the paper, is in fact not a binary (think or no think) method but a truncation one. Similarly, https://arxiv.org/abs/2506.02536 is another one. There are definitely many more early stopping methods applied to CoT, and they should be properly discussed and compared.\n- AutoL2S also utilized the idea of incorporating special token for efficient reasoning, which should also be compared and discussed."}, "questions": {"value": "- Which model is Table 2 testing?\n- How many run per each question?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4cnxwbbJ2p", "forum": "WOIf5MGJXB", "replyto": "WOIf5MGJXB", "signatures": ["ICLR.cc/2026/Conference/Submission22878/Reviewer_oRZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22878/Reviewer_oRZx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009131510, "cdate": 1762009131510, "tmdate": 1762942422143, "mdate": 1762942422143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on improving the reasoning efficiency of large reasoning models (LRMs). First, ESTAR-LITE trains a classifier to predict whether reasoning should stop or continue. Then ESTAR-FT fine-tunes LRMs on curated CoT to enable them to determine their own stopping points. Finally, ESTAR adapts GRPO to reward correct \"stop\" emissions, resulting in a more efficient reasoning model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper is well-written and easy to follow.\n-\tThe three research questions (when to truncate reasoning, how to let LRMs decide stopping points, and how to leverage self-generated stop signals with reinforcement learning) are well articulated and explored.\n-\tExperimental results are promising, demonstrating substantial reductions in tokens while maintaining original accuracy levels."}, "weaknesses": {"value": "- Limited evaluations. Experiments are conducted only on Qwen3-8B and Qwen3-14B. Evaluations on additional LRMs would strengthen the conclusions. The results in Tables 1 and 2 appear to be repeated. Moreover, Table 2 seems to include only the results of Qwen3-8B. It would be helpful to also compare different methods with Qwen3-14B and other models (e.g., DeepSeek-R1).\n- Limited discussion of baseline methods (AdaptThink and Length-Penalty). It would be helpful to clarify whether these are state-of-the-art approaches and to justify their selection as baselines. It would also be beneficial to compare with additional baseline methods, such as [1, 2, 3].\n- Insufficient evaluation of reasoning quality. While efficiency has been improved, it would be useful to assess whether shortened reasoning remains coherent and logically sound.\n- Minor: Figure 3 is somewhat difficult to read.\n\n[1] Yang, C., Si, Q., Duan, Y., Zhu, Z., Zhu, C., Li, Q., ... & Wang, W. (2025). Dynamic Early Exit in Reasoning Models. arXiv preprint arXiv:2504.15895.\n\n[2] Chen, R., Zhang, Z., Hong, J., Kundu, S., & Wang, Z. (2025). Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986.\n\n[3] Wang, C., Feng, Y., Chen, D., Chu, Z., Krishna, R., & Zhou, T. (2025). Wait, We Don't Need to\" Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency. arXiv preprint arXiv:2506.08343."}, "questions": {"value": "-\tIn Table 2, why are comparisons across different methods on the GPQA dataset missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8egkWpUueR", "forum": "WOIf5MGJXB", "replyto": "WOIf5MGJXB", "signatures": ["ICLR.cc/2026/Conference/Submission22878/Reviewer_KAMF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22878/Reviewer_KAMF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327704827, "cdate": 1762327704827, "tmdate": 1762942421911, "mdate": 1762942421911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}