{"id": "Srcsec50YK", "number": 6933, "cdate": 1758002536852, "mdate": 1763607093868, "content": {"title": "Latent Sketchpad:  Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "abstract": "While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. \nInspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce **Latent Sketchpad**, a framework that equips MLLMs with an internal *visual scratchpad*.\nThe internal visual representations of MLLMs have traditionally been confined to perceptual understanding.\nWe repurpose them to support generative visual thought without compromising reasoning ability.\nBuilding on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process.\nIt allows the model to interleave textual reasoning with the generation of visual latents. \nThese latents guide the internal thought process and can be translated into sketch images for interpretability. \nTo realize this, we introduce two components: a Context-aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images.\nWe evaluate the framework on our new dataset MazePlanning. \nExperiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL.\nBy extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human–computer interaction and broader applications.", "tldr": "", "keywords": ["Multimodal Reasoning; MLLM; LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1c3e1fb666b1bebeb0f27cf7b1ed68555d7df89.pdf", "supplementary_material": "/attachment/beb963611b369d5ca96dc25fad467964dd363c22.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Latent Sketchpad, a modular framework designed to enable Multimodal Large Language Models (MLLMs) to perform visual reasoning by integrating visual latent generation into their native autoregressive process. The approach comprises a Context-Aware Vision Head for generating visual latents based on both local and global context, and a Sketch Decoder that converts these internal states into human-interpretable sketches. The framework augments existing MLLMs, such as Gemma3 and Qwen2.5-VL, without altering their core parameters. Evaluation on the newly constructed MAZEPLANNING dataset demonstrates that Latent Sketchpad preserves, and in some cases slightly improves, reasoning performance, while producing visual traces that support interpretability and facilitate multimodal reasoning. The method is validated through extensive analysis, ablations, and comparison to relevant baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a significant gap in the current MLLM landscape by moving beyond pixel-level rendering and static perception, offering a pathway for internal visual thought processes within stepwise multimodal reasoning—an aspect not explicitly handled by existing MLLMs or unified autoregressive models.\n2. Latent Sketchpad is implemented in a way that does not require end-to-end retraining or architectural changes to the main MLLM. This plug-and-play modularity, evidenced in experiments with both Gemma3 and Qwen2.5-VL, is practical for broad adoption.\n3. The use of internal sketches, visualized via the Sketch Decoder, provides a human-understandable window into model “thought,” which is valuable for both debugging and user-facing applications. Figures such as Figure 4 (qualitative visual examples) and Figure 7 (quantitative/qualitative validation of Sketch Decoder) showcase these strengths."}, "weaknesses": {"value": "1. This work may lack innovation. While the integration of a visual sketchpad is interesting, a number of recent efforts (e.g., Visual Sketchpad (Hu et al., 2024), Interactive Sketchpad (Chen et al., 2025), Visual-ARFT (Liu et al., 2025)) are closely aligned. The paper’s distinction lies more in implementation detail than breakthrough conceptual separation, and this is not sufficiently articulated in the text. The positioning versus directly relevant prior work is still somewhat superficial; e.g., Section 5 makes mention of “sketchpad” approaches, but does not substantively contrast with these closest competitors in either methods or results. The core innovation is incremental rather than transformative.\n2. All empirical evidence is centered on the synthetically generated MAZEPLANNING dataset. While this allows controlled analysis, the scope is narrow—visual chain-of-thought for maze navigation is a particular reasoning task, and the broader applicability (e.g., VQA, real-world visual planning, or naturalistic tasks) remains unproven. This undermines the paper’s claims of “broad applicability.”\n3. The quantitative improvements from Latent Sketchpad are modest. For instance, Table 1 and Table 3 report absolute gains that are typically <3% in success or progress rate when augmenting baselines. For Qwen2.5-VL in Table 1 the gains are even smaller (less than half a percent in most settings), which is unlikely to move the needle in practical terms. The interpretability benefits are not clearly benchmarked against user or task-driven utility.\n4. While the paper is generally carefully written, some aspects of the latent regression formulation and training are underspecified. For instance, Equation for $\\mathcal{L}_{reg}$ on Page 4 is kept generic (“various similarity or distance measures”), but in ablations it is revealed L1 is best—this could be systematized. The causal attention mechanisms, as shown in Figure 2, are described at a high level but without clear formal specification (e.g., masking details, variable dependencies). The mapping from Vision Head latent space through the AlignerNet to VAE latent codes is abstracted, yet non-trivial for reproducibility or for adapting to different base models.\n5. While Table 4 provides some insight into ablation by showing effects of connector adaptation and loss, the influence of the pretrained Sketch Decoder on reasoning performance, compared to using simpler visualizer modules, is not deeply examined. Also, how much the interpretability/visualization adds to actual reasoning success or end-user value is not directly assessed."}, "questions": {"value": "1. What fundamental advances does Latent Sketchpad offer over Visual Sketchpad (Hu et al., 2024), Interactive Sketchpad (Chen et al., 2025), and Visual-ARFT (Liu et al., 2025)? Please detail the algorithmic or empirical distinctions—ideally supported by direct head-to-head comparison or ablation against these methods.\n2. Can the authors clarify the specific masking and sequencing logic for the cross/self-attention in Vision Head (Figure 2)? Are the attention masking strategies and context construction robust to variable sequence lengths and complex, multi-turn tasks?\n3. What are the technical or empirical bottlenecks preventing Latent Sketchpad from yielding larger gains on the Qwen2.5-VL backbone, especially on OOD test sets as per Table 3 and Figure 5?\n4. Are there any plans to test Latent Sketchpad on broader real-world datasets or tasks (e.g., VQA, robotic planning, dialog-based spatial reasoning) to substantiate claims of “broad applicability”? What, if any, are the barriers to such generalization?\n5. How sensitive are the results to different choices of the latent regression loss (e.g., cosine similarity, MSE)? Is the Vision Head prone to collapse or instability depending on training details?\n6. Can the authors provide clearer insight into the limits of using SSIM for structural assessment (Figure 3), since this metric may not correlate with reasoning-relevant spatial fidelity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "guwJ1Vp1Ws", "forum": "Srcsec50YK", "replyto": "Srcsec50YK", "signatures": ["ICLR.cc/2026/Conference/Submission6933/Reviewer_QkdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6933/Reviewer_QkdV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274713599, "cdate": 1761274713599, "tmdate": 1762926305496, "mdate": 1762926305496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "## Clarification on Core Contribution, Feasibility, and Vision\n \nOur primary contribution lies in **repurposing pretrained semantic features from frontier vision-language models (VLMs)** to enable *native visual thought generation*, using sketches as a tractable and interpretable proxy.\n\n\nA key advantage of this approach is its reliance on **uniform visual representations**—unlike methods that depend on VAE-based generative features, our framework is **architecture-agnostic**, making it more flexible and compatible with diverse model families.\n \nConceptually, our approach parallels the recent direction introduced by **Representation Autoencoders (RAEs)** [1].\n\n\nRAE replaces the traditional VAE used for image generation with pretrained representation encoders such as **DINO** or **SigLIP**, achieving semantically meaningful latent spaces for diffusion transformers.\n\n\nSimilarly, our framework leverages pretrained semantic features to **enable reasoning-oriented latent representations** within existing VLMs.\n \nA significant part of our contribution is a **comprehensive examination of architectural feasibility**. We extensively validated this approach across:\n \n* **Diverse VLMs:** (e.g., Qwen2.5-VL, Gemma3), by integrating a vision head and training the model to support native visual thought during decoding—originally a text-only process.\n\n\n* **Various Semantic Feature Sets:** (e.g., OpenCLIP, SigLIP), providing a series of pretrained sketch decoders that demonstrates strong generalization and robust reconstruction ability.\n \nThis foundational work demonstrates the **architectural viability** of our method and, we believe, **paves the way for future exploration** by the community. We position this direction alongside promising approaches like RAE and emphasize the potential of semantic features for **unified visual understanding and generative reasoning**.\n \n---\n \n## Distinction from Interleaved Reasoning and Tool-Use\n \nWe emphasize that our main contribution **is not a new interleaved reasoning paradigm** or a novel tool-use VLM.\n \nInstead, our framework **enables advanced reasoning capabilities natively within existing VLMs**.\n\n\nThe critical distinction is that our method fosters internal visual reasoning **without relying on external, pixel-level image generation modules or auxiliary tools**.\n \nReferences:\n\n\n[1] Zheng, Boyang, et al. \"Diffusion Transformers with Representation Autoencoders.\" arXiv preprint arXiv:2510.11690 (2025)."}}, "id": "cpeOGF6UdR", "forum": "Srcsec50YK", "replyto": "Srcsec50YK", "signatures": ["ICLR.cc/2026/Conference/Submission6933/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6933/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6933/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763604236343, "cdate": 1763604236343, "tmdate": 1763604236343, "mdate": 1763604236343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Latent Sketchpad, a framework that equips Multimodal Large Language Models with an internal visual reasoning process inspired by human sketching. The approach integrates a Context-Aware Vision Head to generate visual latents during autoregressive reasoning and a Sketch Decoder to render these latents into interpretable sketches. The idea is creative and relevant to advancing interpretable multimodal reasoning, though several technical and experimental aspects could benefit from clarification and broader validation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely and meaningful problem: enhancing MLLMs with visual imagination and interpretable visual reasoning, similar to human mental sketching.\n\nThe modular architecture, i.e., the Context-Aware Vision Head and Sketch Decoder, is potentially applicable across various MLLMs (e.g., Gemma3, Qwen2.5-VL).\n\nKeeping the visual reasoning within the latent space, rather than decoding full images during reasoning can balances interpretability and computational efficiency."}, "weaknesses": {"value": "1. Ambiguity in the description of visual latent training (Section 2.2). The explanation of the Auto-regressive Visual Latent Generation and its associated loss is somewhat confusing. It is unclear how the “target latent obtained from pretrained visual features of the vision encoder” is defined. The paper should clarify whether these visual features come from the initial input image or from intermediate reasoning steps. If the latter, the authors should consider adding a brief preliminary section what the inputs and outputs are in this training setup, to help readers grasp the data flow more intuitively.\n\n\n2. Limited scope of experiments and evaluation.\n\n    2.1 The experiments are conducted only on the MAZEPLANNING dataset. While this dataset is useful, it alone is insufficient to demonstrate generalization. It would strengthen the paper to include results on other visual reasoning tasks such as Sokoban or Sudoku.\n\n    2.2 Additionally, the paper should explore how a model trained with visual latent reasoning performs on more general multimodal reasoning benchmarks, such as MathVista or MMMU. A discussion about potential transferability to these tasks would make the contribution more convincing.\n\n3. Limited performance improvement on specific backbones. According to Table 1, the gain on Qwen2.5-VL is marginal (less than 0.5). This small improvement weakens the claim that the method consistently enhances reasoning ability."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "imwrlTLi2Y", "forum": "Srcsec50YK", "replyto": "Srcsec50YK", "signatures": ["ICLR.cc/2026/Conference/Submission6933/Reviewer_4jS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6933/Reviewer_4jS7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508814777, "cdate": 1761508814777, "tmdate": 1762919166309, "mdate": 1762919166309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces latent Sketchpad, specifically on visual maze problems. They introduce two components: a Context-aware Vision Head that produces visual representations, and a pretrained Sketch Decoder that renders these into human-interpretable images. They create a new dataset MazePlanning and evaluate their method on this new dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. provides a method to conduct latent sketchpad by designing a Context-aware Vision Head, and a Pretrained Sketch Decoder.\n2. Provides a 47.8K training data for maze puzzle solving.\n3. Experiments upon Gemma3-12B and Qwen2.5-VL-7B, shows improvement (+0.39-+2.2%) on the proposed dataset."}, "weaknesses": {"value": "The biggest weakness is that the experiment is not clear and some citations are missing. \n1. For the experiments, the evaluations are done on generated text action sequences (Sec 3.1). For the result in Table 1, it shows text-only output and interleaved text-image output. What does it mean? Does it mean still only the text is evaluated the interleaved output setting? Also, it is unclear what training data (text only data v.s. multimodal data) is used for training for these two outputs. Do you use text-only data for text-only output? \n2. While the settings are unclear, it seems most improvement hover around 0.5%. It doesn't seem significant. Does this result consider generate multiple outputs and than evaluate? If not, what would be the new scores after generating multiple outputs than eval?\n3. Baseline missing. It seems unified model should be added to the baselines e.g. MetaMorph, Bagel, Janus Pro. Also Gemini 2.5 Pro and GPT 5. Also, should include [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought] as a baseline because you're solving same tasks.\n4. Evaluation Benchmark too few, only evaluating on your proposed benchmark cannot prove the efficacy of the method. Should consider also evaluate on other benchmarks, e.g. [VISUALPUZZLES: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge], [VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models]... etc.\n5. Missing citations. Especially, (a) should be cited because that's usually where \"sketchpad\" comes from.  a. [Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models] from NeurIPS 2024. b. [Perception Tokens Enhance Visual Reasoning in Multimodal Language Models] from CVPR 2025. c. [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens] d. [VISUALPUZZLES: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge]"}, "questions": {"value": "Please see weakness. Happy to raise score if these are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "inS5mBcimx", "forum": "Srcsec50YK", "replyto": "Srcsec50YK", "signatures": ["ICLR.cc/2026/Conference/Submission6933/Reviewer_FKhg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6933/Reviewer_FKhg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964275468, "cdate": 1761964275468, "tmdate": 1762919165741, "mdate": 1762919165741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that enables Multimodal Large Language Models (MLLMs) to “think visually” during reasoning. Inspired by human mental sketching, the method adds a Context-Aware Vision Head that generates visual latents interleaved with textual reasoning, and a pretrained Sketch Decoder that translates these latents into interpretable sketches. The authors introduce a new MAZEPLANNING dataset designed to test multimodal reasoning involving both text and spatial planning. Experiments on models such as Gemma3 and Qwen2.5-VL show that integrating the Latent Sketchpad slightly improves reasoning accuracy while producing interpretable visual traces. Overall, the work contributes a plug-and-play method to integrate visual imagination into MLLMs without retraining the backbone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper’s originality lies in its attempt to simulate a “visual thinking” process within MLLMs by introducing a latent sketching mechanism—a creative and human-inspired idea that connects internal representation learning with interpretable visual reasoning. \n2. The quality of the work is solid, with a clear architectural design combining a context-aware vision head and a pretrained sketch decoder, along with empirical validation on the proposed MazePlanning benchmark and existing reasoning tasks. \n3. In terms of clarity, the paper is well-structured and communicates its motivation and framework intuitively, aided by clear figures illustrating how sketches emerge during reasoning. \n4. Regarding significance, the work offers a promising step toward more interpretable multimodal reasoning and introduces a framework that can be easily adapted across models without retraining, making it an interesting and practical contribution to the MLLM research community."}, "weaknesses": {"value": "1. While the idea of a “latent sketchpad” is creative, the paper’s novelty is somewhat limited, as related works such as Visual Chain-of-Thought (Zhou et al., 2023), MM-ReAct (Yao et al., 2023), and Sketch-Guided CoT (Luo et al., 2024) have also explored visual reasoning traces or intermediate visualizations.\n2. The experiments are relatively narrow—focused mainly on MazePlanning and a few reasoning benchmarks, the OOD performance also degrades significantly. Maybe the type of dataset can be extended to broader domains.\n3. The observed performance gains are modest, suggesting that the sketching component currently serves more as a visualization tool than a strong reasoning enhancement."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vawqERcPcd", "forum": "Srcsec50YK", "replyto": "Srcsec50YK", "signatures": ["ICLR.cc/2026/Conference/Submission6933/Reviewer_PDJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6933/Reviewer_PDJW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044446390, "cdate": 1762044446390, "tmdate": 1762919165071, "mdate": 1762919165071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}