{"id": "6xB2mKOGqx", "number": 2021, "cdate": 1756977203256, "mdate": 1759898173640, "content": {"title": "Routing-Deconstructed LoRA in Federated Fine-Tuning", "abstract": "The integration of Large Language Models (LLMs) with Federated Learning (FL) offers a promising approach to privacy-preserving Parameter-Efficient Fine-Tuning (PEFT). However, resource and data heterogeneity in FL cause differences in local knowledge distribution across clients. As a representative PEFT approach, LoRA still faces three key challenges in such settings: aggregation noise, knowledge contamination, and aggregation distortion. To address these issues, we propose Routing-Deconstructed LoRA (RD-LoRA). Building on an alternating freezing strategy to mitigate aggregation noise and concurrently reduces communication cost, RD-LoRA further introduces two novel components. For knowledge contamination, we design a Server-Client Routing Deconstructor (SCRD) that separates shared semantics from local biases, retaining fine-grained knowledge with semantic consistency. To address aggregation distortion, we propose a Poly-Consensus Aggregation (PCA) mechanism that uses adaptive weighted averaging to align global LoRA parameters with heterogeneous client distributions, thus correcting the global update direction. Extensive experiments demonstrate that RD-LoRA is effective and robust in both homogeneous and heterogeneous settings.", "tldr": "", "keywords": ["LLMs", "Parameter-Efficient Fine-Tuning", "Federated Learning", "LoRA", "Resource Heterogeneous"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55caa0a02e87fcfd6b5bd24948623fbc04cc01a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes RD-LoRA for federated LoRA fine-tuning under client/data heterogeneity. It builds on alternating freezing (only A or B is updated per round) to lower communication and aggregation noise, and introduces a routing matrix split to decouple shared semantics from client-specific signals and reduce knowledge contamination. On the server, a PCA applies position-wise adaptive weights with historical regularization to mitigate aggregation distortion when fusing A/B/R. The method handles heterogeneous ranks via zero-padding/truncation before aggregation and redistribution. Experiments on Llama-2 and TinyLlama report consistent gains over baselines in both homogeneous and heterogeneous settings, with communication similar to alternating-freeze methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly pinpoints three key challenges in federated LoRA fine-tuning: aggregation noise, knowledge contamination, and aggregation distortion. The solution proposed focus on the specific challenges and has clear motivation.\n\nEmpirical results are strong and consistent across Llama-2 and TinyLlama in both homogeneous and heterogeneous rank settings, with low communication comparable to alternating-freeze methods and ablations showing that removing SCRD or PCA hurts performance.\n\nThe paper clearly states the strength and weakness of previous works, exploring the accuracy, communication, and heterogeneity."}, "weaknesses": {"value": "The pipeline is overly complex for practical LLM fine-tuning, hinging on alternating A/B rounds, an extra routing matrix, and server-side PCA; moreover, the experiments use very heavy schedules (e.g., 200 communication rounds with 10 local epochs per round), which may not be necessary in many real deployments and can confound the contribution of the routing/PCA design.\n\nThe treatment of heterogeneous ranks lacks a stronger theoretical underpinning: cross-rank alignment ultimately relies on zero-padding or truncation, which the paper itself acknowledges as a limitation and potential source of degradation, this seems partly at odds with the broader claim of mitigating aggregation noise.\n\nSome figures and expressions are not sufficiently precise; for instance, the RD-LoRA overview (Figure 2) is visually crowded with many modules/arrows, making the flow difficult to understand quickly."}, "questions": {"value": "Could you report the test accuracy trajectory over all 200 communication rounds, including variance across seeds, and clarify at which round RD-LoRA first surpasses baselines?\n\nWhat is the base model’s zero-shot (or supervised) accuracy on each benchmark before any federated fine-tuning, so we can quantify absolute gains?\n\nHow does the method handle client selection or partial participation per round (e.g., random subset of clients)? Please specify any changes to routing updates, PCA aggregation, and convergence behavior under varying participation rates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4azTv5veOO", "forum": "6xB2mKOGqx", "replyto": "6xB2mKOGqx", "signatures": ["ICLR.cc/2026/Conference/Submission2021/Reviewer_wjiw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2021/Reviewer_wjiw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846817130, "cdate": 1761846817130, "tmdate": 1762915993363, "mdate": 1762915993363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Routing-Deconstructed LoRA (RD-LoRA), a federated fine-tuning framework for large language models that introduces two main components: the Server–Client Routing Deconstructor (SCRD) to separate global and local knowledge, and the Poly-Consensus Aggregation (PCA) mechanism, which employs adaptive weighted averaging for aggregation under data heterogeneity. The work aims to alleviate aggregation noise, knowledge contamination, and aggregation distortion in federated LoRA fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method demonstrates clear empirical effectiveness, especially under non-IID settings, and achieves superior performance over several strong baselines.\n- The experimental section is comprehensive in terms of datasets, models, and metrics, providing a convincing empirical comparison."}, "weaknesses": {"value": "- Equation (2) is imprecise and potentially misleading. $\\frac{1}{k}\\sum \\Delta W_i$ is not directly equal to $\\frac{1}{k}\\sum \\Delta B_i \\times \\frac{1}{k}\\sum \\Delta A_i$ but somehow approximated by it.\n- The discussion of bias from averaging B and A (Equation 3) lacks depth. The paper shows that the left-hand side differs from the right-hand side but does not analyze whether this necessarily constitutes a biased or sub-optimal estimate. More intuition or references to prior analyses of LoRA aggregation bias would strengthen the argument.\n- The reasoning of theorem 3.1 of “ effectiveness of SCRD arising from reduced gradient scale” is unclear. The theoretical analysis focuses solely on local updates, while in federated settings $R_{\\text{global}}$ is produced through server-side aggregation. It is not evident how the local analysis guarantees separation between local and global knowledge in practice.\n- A key baseline, FedIT (FedAvg + LoRA), is missing. Given that RD-LoRA claims novelty on aggregation scheme, direct comparison with FedIT is essential as a comparison with the most naive method is necessary.\n- Only two clients out of ten are sampled per communication round, which may not adequately represent real-world FL participation patterns and can bias results.\n- No indication is given that experiments were repeated with multiple random seeds. Reporting variance would improve reliability."}, "questions": {"value": "- **OSIM implementation:** The paper does not describe how the per-branch logits $s_i[u,v]$ in the Omni-Scale Integration Module are computed.\n- **PCA specificity:** The proposed Poly-Consensus Aggregation seems generally applicable to any matrix aggregation, not specifically tailored to LoRA. Can author share more insights on how LoRA can be beneficial on PCA?\n- **Assumption in Theorem 3.1:** Is it realistic to assume the symmetric part of $R_{\\text{global}}^\\top R_{\\text{client}}$ is positive definite?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8sxzxcXQc", "forum": "6xB2mKOGqx", "replyto": "6xB2mKOGqx", "signatures": ["ICLR.cc/2026/Conference/Submission2021/Reviewer_8fzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2021/Reviewer_8fzm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923140621, "cdate": 1761923140621, "tmdate": 1762915993193, "mdate": 1762915993193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose RD-LoRA, a communication-efficient federated fine-tuning framework for large language models using parameter-efficient LoRA adapters, designed for realistic non-IID settings where clients have heterogeneous data distributions and even different LoRA ranks.  It introduces SCRD—a routing mechanism separating global and client-specific adaptation to prevent knowledge contamination—and PCA, an adaptive aggregation method that selects dominant client updates per parameter and stabilizes training with historical regularization.  The authors conduct experiments on Llama2-7B and TinyLlama show RD-LoRA outperforms existing methods on MMLU and MT-Bench."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide a practical problem formulation, and identify three concrete bottlenecks in scalable federated LoRA (aggregation noise, knowledge contamination, distortion) .\n\n2.The authors introduce a routing matrix split into global (frozen) and client-specific (trainable) components, cleanly separating shared knowledge from local bias, with theoretical support for improved update stability.\n\n3. The authors propose fine-grained aggregation via pca, which learns position-wise reliability weights and uses historical alignment to stabilize updates.\n\n4. The paper measures and reports per round upload and download cost for each baseline."}, "weaknesses": {"value": "1. Regarding experiments:\n\n a. the authors only adopt Llama2 7B and TinyLlama 1.1B as the backbone model, how about other models such as Qwen series model?\n\nb. The training loop alternates B round and A round. But we only see two clients per round in experiments. How does that scale to a larger pool of clients across two hundred rounds.\n\nc. Limited ablations on PCA internals:\n   PCA has several moving parts. Poly Fusion Gate to create a contextual prior. Omni Scale Integration Module to learn per position weights. Historical steady alignment with a lambda term. The paper provides a single ablation that swaps PCA with FedAvg style averaging, and then discusses cosine similarity and singular value retention. That shows PCA helps, but it is still hard to tease apart which sub part of PCA is doing the heavy lifting. For example, is the historical regularizer alone already enough to stabilize training, or is the per position attention absolutely necessary. More granular ablations would make PCA easier to adopt by others."}, "questions": {"value": "see weakness 1.a, 1.b, 1.c"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sUI6RjAvhv", "forum": "6xB2mKOGqx", "replyto": "6xB2mKOGqx", "signatures": ["ICLR.cc/2026/Conference/Submission2021/Reviewer_zbqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2021/Reviewer_zbqX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142417220, "cdate": 1762142417220, "tmdate": 1762915993025, "mdate": 1762915993025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method named RD-LoRA, which aims to improve federated LLM fine-tuning using LoRA-based methods. RD-LoRA seeks to address the limitations and challenges of existing PEFT methods for federated tuning of LLMs, namely aggregation noise, knowledge contamination, and aggregation distortion. The authors claim that RD-LoRA can address all the aforementioned challenges simultaneously. The key aggregation method used by RD-LoRA is the Poly-Consensus Aggregation approach, which adaptively aligns global model weights with local model weights. Experimental results have been presented to demonstrate the effectiveness of RD-LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and well motivated.  \n- Improving the performance and efficiency of federated PEFT over LLMs seems to be a reasonable research direction to pursue.  \n- The proposed RD-LoRA method is easy to follow and intuitive.  \n- The experimental results of the paper look promising."}, "weaknesses": {"value": "- The authors proposed a relatively complex method, but the accuracy improvement appears to be marginal (if not very marginal) based on the main results shown in Table 1 and Table 2, especially compared to FlexLoRA.  \n- The experiments were conducted on somewhat outdated models, for example, Llama2. We already have Llama4 and even stronger open source models, so why still fine-tuning on Llama2?  \n- The fine-tuning tasks also seem a bit outdated; it would be more interesting to show results on more challenging benchmarks such as AIME 2025.  \n- It is not clear what the implementation and hyperparameter tuning overheads are when using RD-LoRA compared to other simpler methods."}, "questions": {"value": "Please focus on addressing the concerns in Weaknesses.\n\n- If the performance gain is marginal, why using RD-LoRA?\n- What does the performance of RD-LoRA look like on say Qwen3 finetuning and for harder tasks, e.g., some reasoning benchmarks.\n- What are the overheads introduced by RD-LoRA ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQmKvUD0En", "forum": "6xB2mKOGqx", "replyto": "6xB2mKOGqx", "signatures": ["ICLR.cc/2026/Conference/Submission2021/Reviewer_RtDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2021/Reviewer_RtDT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762729717005, "cdate": 1762729717005, "tmdate": 1762915992892, "mdate": 1762915992892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}