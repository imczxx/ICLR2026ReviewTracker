{"id": "jNbxjdc745", "number": 12717, "cdate": 1758209670628, "mdate": 1759897491703, "content": {"title": "Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning", "abstract": "Using a nearly-frozen pretrained model, the continual representation learning paradigm reframes parameter updates as a similarity-matching problem to mitigate catastrophic forgetting. However, directly leveraging pretrained features for downstream tasks often suffers from multicollinearity in the similarity-matching stage, and more advanced methods can be computationally prohibitive for real-time, low-latency applications. Inspired by the fly olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with a wide range of pretrained backbones. Fly-CL substantially reduces training time while achieving performance comparable to or exceeding that of current state-of-the-art methods. We theoretically show how Fly-CL progressively resolves multicollinearity, enabling more effective similarity matching with low time complexity. Extensive simulation experiments across diverse network architectures and data regimes validate Fly-CL’s effectiveness in addressing this challenge through a biologically inspired design.", "tldr": "", "keywords": ["continual learning", "fly olfactory circuit", "class incremental learning", "decorrelation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34ed58492f6817572445d2232a5e26e43a3ad59f.pdf", "supplementary_material": "/attachment/41d82463ad55f46c33951b426ff7857b8b5ff9b0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes FlyCL, which addresses multicollinearity in continual learning. They use a mixture of sparse random projection, top-k sparsity and streaming ridge classification. They demonstrate impressive computational speedups (90%), while maintaining comparable performance. \n\nThese are real and practical speedups, and this is definitely good engineering. The bio-inspiration however does not add much substance beyond motivation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Very strong practical results, the paper shows very significant speedups with barely any loss in accuracy.\n2. The method is clearly general enough to adapt to different architectures and datasets, in a plug-and-play manner.\n3. The work has solid experimental evidence, good ablations, and statistical reporting."}, "weaknesses": {"value": "1. The fly brain parallel is mostly surface-level (it looks like), and only inspires the sparse projection component.\n2. Some hyperparameters seem to require architecture-specific tuning, although that does not necessarily negate the proposed generality.\n3. Would be useful to examine if this scales to tasks at the scale of modern foundational models."}, "questions": {"value": "1. How sensitive is the performance to m, p and k? The defaults chosen seem somewhat arbitrary.\n2. When 10k dimensions are not sufficient, how does this scale to larger models?\n3. Is there a benefit to keeping the projection matrix random as opposed to learning it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhZfnqE7vL", "forum": "jNbxjdc745", "replyto": "jNbxjdc745", "signatures": ["ICLR.cc/2026/Conference/Submission12717/Reviewer_awjr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12717/Reviewer_awjr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804454981, "cdate": 1761804454981, "tmdate": 1762923542919, "mdate": 1762923542919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Fly-CL, an efficient framework for continual learning with frozen pre-trained encoders. It introduces two key components: (1) a sparse random projection with top-k activation sparsification to decorrelate features and improve prototype separability, and (2) a streaming ridge regression classifier with adaptive regularization via generalized cross-validation for stability and low computational cost. Experiments across ViT-B/16 and ResNet-50 backbones show that Fly-CL achieves comparable or higher accuracy than prior representation-based methods (e.g., RanPAC, F-OAL) while reducing post-extraction training time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clear motivated and formulated, easy-to-follow\n2. High efficiency with strong accuracy trade-off\n3. Simple and generalizable design, broadly applicable to backbones and datasets."}, "weaknesses": {"value": "1. The proposed framework shares conceptual similarities with earlier representation-based approaches such as RanPAC and F-OAL, both of which employ random projections and analytic updates. While Fly-CL introduces additional sparsification and adaptive regularization, the methodological advancement over these predecessors appears incremental rather than fundamentally novel.\n\n2. This paper does not empirically demonstrate the effect of reduced prototype correlation.\n3. The experiments primarily report average accuracy across tasks, but omit standard CL metrics such as the final-task accuracy ($\\mathbf{A}_T$) and forgetting measure. \n4. The study focuses on representation-based and prompt-based baselines but do not compared with recent lora/adapter-efficient continual tuning methods such as InfLoRA [A], SEMA [B], and MoE-Adapters [C].\n5. Discuss why Fly-CL slightly underperforms RanPAC on CIFAR-100 despite achieving substantial gains on other datasets. \n6. It is not specified whether the ViT-B/16 model is initialized from ImageNet-21K or ImageNet-1K pre-trained weights. \n\n[A] Liang, Y. S., & Li, W. J. Inflora: Interference-free low-rank adaptation for continual learning. CVPR2024.\n\n[B] Wang, H., et al. Self-expansion of pre-trained models with mixture of adapters for continual learning. CVPR2025\n\n[C] Yu, J., et al. Boosting continual learning of vision-language models via mixture-of-experts adapters. CVPR2024."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mQH5zkGvaT", "forum": "jNbxjdc745", "replyto": "jNbxjdc745", "signatures": ["ICLR.cc/2026/Conference/Submission12717/Reviewer_uEtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12717/Reviewer_uEtz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877635280, "cdate": 1761877635280, "tmdate": 1762923542553, "mdate": 1762923542553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Fly-CL, a continuous learning framework inspired by the Drosophila olfactory circuit, designed to address multicollinearity issues in representation learning based on pre-trained models while reducing training time. Fly-CL achieves feature decoupling and efficient classification through mechanisms including sparse random projection, Top-k activation filtering, and streaming ridge classification. Experiments demonstrate that this method achieves or surpasses state-of-the-art performance across multiple datasets and backbone networks while significantly reducing training time, exhibiting particularly strong advantages during post-feature extraction processing stages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the issue of “persistent feature decoupling in pre-trained models.” While the constituent components (random projection, Top-k, ridge regression) are established techniques, their creative combination and application constitute the contribution.\n    \n2. The paper provides a solid theoretical foundation, demonstrating the information retention capability of sparse projections. The experimental design encompasses multiple architectures (ViT, ResNet), datasets, and evaluation metrics.\n    \n3. This work provides a solution for efficient continuous learning in resource-constrained scenarios."}, "weaknesses": {"value": "1. I believe the core mechanism of the paper—“random projection + Top-k sparse activation”—shares striking similarities with the fundamental concept of Kanerva's Sparse Distributed Memory (SDM). SDM is similarly inspired by neuroscience and employs high-dimensional sparse representations and similarity matching to address memory and learning challenges. The paper omits discussion with this classic approach.\n\n2. Despite significant efficiency gains, projecting dimensions of m=10,000 may still impose memory constraints on extreme edge devices."}, "questions": {"value": "1. Could the author establish a simple baseline by using only Fly-CL's projection and Top-k layers, followed by a straightforward linear classifier or k-nearest neighbors classifier, to demonstrate the necessity of streaming ridge regression in your problem setting?\n\n2. Although Fly-CL claims to mitigate forgetting, how does it affect old tasks at the feature space level? When learning new tasks, do the class prototypes of old tasks drift or distort in the high-dimensional space after Fly-CL processing?\n\n3. Theorem B.1 aims to prove that sparse projection matrices W are almost certainly full-rank, which is considered an argument for their ability to preserve information. However, in machine learning, the fact that a random matrix is full-rank does not directly equate to it being a “good” feature mapper. More importantly, the matrix's isometric property or distance-preserving characteristic is guaranteed by the Johnson-Lindenstrauss (JL) lemma. Can the authors provide evidence that your sparse matrix W indeed preserves pairwise distances between feature vectors with high probability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WcUVZH8q0F", "forum": "jNbxjdc745", "replyto": "jNbxjdc745", "signatures": ["ICLR.cc/2026/Conference/Submission12717/Reviewer_gFjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12717/Reviewer_gFjn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980176865, "cdate": 1761980176865, "tmdate": 1762923542172, "mdate": 1762923542172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Taking inspiration from the fly's olfactory system, the authors introduce FLY-CL, a way to extract and use features of a pretrained model for class incremental learning.\n\nThe features of a pretrained model are expanded with a fixed random projection, in a similar way to the a layer from \"projection neurons\" to \"Keynyon cells\" in the olfactory circuit of a fly. A top-k activation simulates lateral inhibition. A learned similarity matching down projection emulates the projection to \"mushroom body output neurons\". \n\nThey show how this method reduces catastrophic forgetting when using pretrained models with a set of image benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The high-dimensional sparse layer does seem to effectively prevent catastrophic forgetting. \n\nNo task identity is required at inference. \n\nworks with unmodified pre-trained models \n\nablation studies do show the need for each part of the model (random projection / ridge regression / normalisation)\n\nFigure 5 shows how important the high dimension layer is, with performance saturating at m>10k. This is crucial since I believe this is the main novel contribution."}, "weaknesses": {"value": "There can be high memory costs associated with the large sparse layer (the authors do discuss this).\n\nThey do need the model to receive task boundaries and to store \"class prototypes\" for use during inference. \n\nNo adaptation to the backbone so it is dependent on a good pretrained model (a weakness or a strength depending on the circumstance).  \n\nThe paper would benefit from a more detailed analysis of how the key hyperparameters (m, p, and k) scale with task complexity, the number of tasks/classes, and different pretrained backbones.\n\nminor typo:\n037 \"generalization in downstream tasks for downstream tasks\""}, "questions": {"value": "you show the performance saturates beyond 10k. Do you expect that saturation point to remain stable as the number of tasks/classes grows much larger?\n\nHave you run experiments that study how the required projection dimensionality scales with increased task complexity or number of tasks/classes? Or if it changes if you use a different pretrained backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rj6wAOmpin", "forum": "jNbxjdc745", "replyto": "jNbxjdc745", "signatures": ["ICLR.cc/2026/Conference/Submission12717/Reviewer_4tXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12717/Reviewer_4tXN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762265321621, "cdate": 1762265321621, "tmdate": 1762923541848, "mdate": 1762923541848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}