{"id": "B19MBDrvlM", "number": 22207, "cdate": 1758327762603, "mdate": 1759896880266, "content": {"title": "On Fairness of Task Arithmetic: The Role of Task Vectors", "abstract": "Model editing techniques, particularly task arithmetic with task vectors, offer an efficient alternative to full fine-tuning by enabling direct parameter updates through simple arithmetic operations. While this approach promises substantial computational savings, its impact on fairness has remained largely unexplored—despite growing concern over biased outcomes in high-stakes applications such as hate speech detection. In this work, we present the first systematic study of fairness in task arithmetic, benchmarking it against full fine-tuning (FFT) and Low-Rank Adaptation (LoRA). We evaluate across multiple language models and datasets using standard group fairness metrics, including Demographic Parity and Equalized Odds. Our analysis shows that task vectors can be tuned to achieve competitive accuracy while reducing disparities, and that merging subgroup-specific task vectors provides a practical mechanism for steering fairness outcomes. We further provide a theoretical bound linking task-vector scaling to fairness metrics, offering insight into the observed trade-offs. Together, these findings establish task arithmetic not only as a cost-efficient editing method but also as a fairness-aware alternative to existing adaptation techniques, laying the groundwork for responsible deployment of large language models. Our code is available at: https://anonymous.4open.science/status/fairness_task_vector-4F2F", "tldr": "We analyze fairness implications of task arithmetic in model editing to guide responsible practices.", "keywords": ["Fairness", "Model Editing", "Task Arithmetic"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8aa3a121a25fd3a85ed8ecc72d15902e8042466.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the fairness implications of task arithmetic–based model editing, where task vectors are manipulated through simple arithmetic operations as a lightweight alternative to full fine-tuning. The authors benchmark task vectors against Full Fine-Tuning (FFT) and LoRA across multiple language models and datasets, using standard group fairness metrics such as demographic parity and equalized odds. Experimental results show that task vectors can maintain competitive accuracy while reducing group disparities, and combining subgroup-specific task vectors enables controllable fairness adjustments. The paper further provides a theoretical bound linking task-vector scaling to fairness metrics. Overall, the work positions task arithmetic as a cost-efficient yet fairness-aware model adaptation strategy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Analyze the impact of task vector on fairness is interesting and seems novel.\n2. Well-structured paper and easy to understand"}, "weaknesses": {"value": "1. The model used in this paper (Llama2-7b, DistilBERT, Qwen2.5-0.5B) seems out of date\n2. The informal theory is a little hard to understand. \n3. The observation in 5.3 and 5.4 seems to be useful, but we need a more general or interesting conclusion in 5.3, which is a section called empirical results overview.\n4. As a benchmark, it should be more comprehensive, such as include more baseline, dataset, base model."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IxHuG9uIZq", "forum": "B19MBDrvlM", "replyto": "B19MBDrvlM", "signatures": ["ICLR.cc/2026/Conference/Submission22207/Reviewer_RrTq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22207/Reviewer_RrTq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891275997, "cdate": 1761891275997, "tmdate": 1762942113902, "mdate": 1762942113902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the systematic investigation of fairness in task arithmetic, offering a comprehensive comparison between task-vector editing, full fine-tuning (FFT), and Low-Rank Adaptation (LoRA), while further examining whether the integration of subgroup-specific task vectors into an FFT model can enhance fairness control. By evaluating multiple language models and datasets through standard group fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD), the study demonstrates that adjusting task-vector scaling coefficients can substantially improve fairness outcomes without compromising predictive accuracy. Furthermore, the merging of task vectors derived from underrepresented subgroups enables targeted fairness adjustments with minimal performance degradation. The authors also derive a theoretical upper bound linking task-vector scaling to demographic parity difference, thereby providing a principled explanation for the observed fairness–accuracy trade-offs. Collectively, this analysis positions task arithmetic as a cost-efficient, interpretable, and fairness-aware alternative to existing model adaptation techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tIt presents a comprehensive evaluation comparing full fine-tuning (FFT), Low-Rank Adaptation (LoRA), task-vector editing, and a hybrid approach that injects task vectors into FFT, systematically analyzing their effects on fairness metrics and predictive performance.\n2.\tIt demonstrates that fairness can be achieved through task-vector scaling, showing that adjusting scaling coefficients effectively improves fairness while maintaining model accuracy.\n3.\tIt integrates task vectors from underrepresented subgroups, facilitating targeted fairness adjustments while maintaining minimal degradation in model performance.\n4.\tIt provides an analytical  upper bound which links task-vector scaling to demographic parity difference, offering a principled explanation for the trade-off between fairness and accuracy."}, "weaknesses": {"value": "1.  The experimental scope is limited to binary classification on two related tasks (hate speech/toxicity detection) with specific demographic annotations, raising questions about generalizability to other NLP tasks.\n2. The theoretical bound in Proposition 1 only addresses DPD, leaving EOD, which is equally emphasized empirically, without theoretical grounding.\n3. The related work section does not discuss several recent and relevant fairness studies."}, "questions": {"value": "1. How well would the proposed approach generalize beyond binary classification tasks such as hate speech and toxicity detection?\n2. Why does the theoretical analysis focus solely on DPD while omitting a formal justification for EOD, which is equally emphasized in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eoeK8pz09q", "forum": "B19MBDrvlM", "replyto": "B19MBDrvlM", "signatures": ["ICLR.cc/2026/Conference/Submission22207/Reviewer_cnud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22207/Reviewer_cnud"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923155779, "cdate": 1761923155779, "tmdate": 1762942113689, "mdate": 1762942113689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of the fairness implications of task arithmetic—a parameter-efficient model editing technique that applies task vectors (parameter differences between fine-tuned and base models) via arithmetic operations. The authors compare task arithmetic against Full Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA) across multiple models and datasets, using group fairness metrics such as Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD). Key contributions include: (1) demonstrating that task arithmetic can achieve competitive accuracy while reducing fairness disparities, especially when scaling coefficients $\\lambda$ are tuned; (2) showing that merging subgroup-specific task vectors enables targeted fairness control; and (3) providing a theoretical bound linking $\\lambda$ to fairness metrics. The work establishes task arithmetic as a fairness-aware and efficient alternative to existing adaptation methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the first systematic comparative analysis of fairness in task arithmetic, filling a critical gap in understanding the societal implications of this efficient model editing paradigm.\n\n2.The scaling coefficient technique is demonstrated to be theoretically grounded and empirically viable."}, "weaknesses": {"value": "1. The core methodological approach relies on manually tuning a unified scalar coefficient, to balance accuracy and fairness. However, for complex real-world scenarios involving multidimensional fairness constraints—such as simultaneous optimization across gender, race, and age—this single-scalar control mechanism appears overly simplistic and lacks scalability. Manually identifying the optimal $\\lambda$ configuration for every possible subgroup combination is inefficient and impractical.\n\n2. The proposed method is presented as a general approach for improving fairness. However, its empirical validation is confined exclusively to binary classification tasks. The scope of algorithmic fairness extends far beyond this, encompassing more complex tasks such as text generation, dialogue, and translation. The present experimental design does not demonstrate the method's effectiveness for these broader and often more relevant fairness challenges.\n\n3. Performance validation was conducted only on models with a maximum of 7B parameters. Given the significant differences in emergent capabilities observed across model scales, these experiments are insufficient to establish the applicability and effectiveness of the proposed method for larger-scale models."}, "questions": {"value": "1. The discussion of the scalar coefficient $\\lambda$ appears limited, particularly when complex tasks require optimization across multiple coefficients.\n\n2. Please clearly define the scope of the research. If the study is focused solely on the fairness of binary text classification tasks, then using the broad research category of \"fairness\" seems overly expansive. If the claim is a general investigation into fairness, experimental results on other types of tasks need to be supplemented.\n\n3. What is the advantage of your method-edited 7B model, in terms of its overall performance (considering both accuracy and fairness), compared to a larger-scale base model that relies solely on well-crafted prompts for zero-shot/few-shot inference? If a significant advantage cannot be demonstrated, the practical significance of the proposed method would be considerably diminished."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wPXYD16vYI", "forum": "B19MBDrvlM", "replyto": "B19MBDrvlM", "signatures": ["ICLR.cc/2026/Conference/Submission22207/Reviewer_8Kur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22207/Reviewer_8Kur"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979509675, "cdate": 1761979509675, "tmdate": 1762942113447, "mdate": 1762942113447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}