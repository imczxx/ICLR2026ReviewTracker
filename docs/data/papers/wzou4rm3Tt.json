{"id": "wzou4rm3Tt", "number": 16703, "cdate": 1758267826865, "mdate": 1759897223988, "content": {"title": "MEGA: A Large-Scale Molecular Editing Dataset for Guided-Action Optimization", "abstract": "Large language models show strong potential for molecular editing, but progress has been constrained by the limited scale and quality of available training data. To address this, we introduce MEGA, a family of large-scale datasets comprising 31M molecule pairs, each representing a single property-improving chemical edit annotated with an explicit action: Replace, Insert, or Delete. We demonstrate MEGA's utility in a controlled supervised fine-tuning (SFT) setting, where a model trained on MEGA outperforms models trained on existing datasets by up to +21.47 percentage points in hit ratio. Furthermore, we show that Group Relative Policy Optimization (GRPO) post-training with a similarity-aware reward achieves state-of-the-art performance and a remarkable $\\sim36\\times$ improvement in data efficiency, while also preserving edit locality. We release MEGA in open access to the community to enable data-centric benchmarks and accelerate progress in molecular editing with generative models.", "tldr": "We release MEGA, a 31M-pair dataset for molecular editing and show through extensive benchmarking that large language models trained on MEGA with similarity-aware GRPO post-training achieve state-of-the-art performance and unmatched data efficiency.", "keywords": ["Open Source Datasets", "Molecular Editing", "Large Language Models", "Reinforcement Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/095f4b2557e4cbee3745bc9d34c251346a12dd81.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors release MEGA, a molecular editing dataset of 31.4M parent–child SMILES pairs (MEGA‑Large) with single action annotations (Insert/Delete/Replace) across 28 tasks; a 522k MEGA subset mirrors the action distribution for resource-constrained use. The construction uses common slicing rules to identify edit sites and validates property improvements deterministically via RDKit under MoleculeSTM-style thresholds. The dataset also provides 41M “near-miss” negatives to support contrastive and RL training. Using a fixed Llama‑3 8B with LoRA, the authors show SFT on MEGA improves hit ratios over other corpora, and GRPO post-training with a similarity-aware reward yields SOTA performance. The dataset is intended for open access."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Scale & annotation: Largest-to-date positive edit corpus with Insert/Delete/Replace labels; supports per‑action supervision and diagnostics.\n* Evidence of usefulness: Consistent SFT gains over other datasets on shared tasks, and strong GRPO results with similarity-aware rewards.\n* Design for locality: Reward shaping explicitly balances property improvement and local edits, aligning with medicinal-chemistry practice."}, "weaknesses": {"value": "* Proxy-based labels: All property “hits” are deterministic RDKit thresholds; consider releasing parallel subsets with more physically grounded or experimental endpoints where feasible. \n* Scaffold drift: Construction “does not constrain scaffold preservation”; the dataset might favor edits that break similarity. Consider providing scaffold-retaining slices or tags to enable stratified training. \n* Coverage analysis: Missing statistics on duplicate parents/children, class imbalance per action/task, and measures of chemical diversity across tasks.\n* Licensing & reproducibility: Ensure licensing for derived data is explicit. Will scripts to regenerate thresholds and task splits be included?"}, "questions": {"value": "* Can you release per-action difficulty metrics (e.g., hit ratio vs. action, per-task) to encourage targeted benchmark ablations? \n* How often do edits decrease other key properties (e.g., SA score) while optimizing the target? Consider multi-objective annotations for trade-off analysis.\n* Will you publish a canonical evaluation server with fixed parents per task and verifiable calculators to avoid drift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yWPT9CKOC5", "forum": "wzou4rm3Tt", "replyto": "wzou4rm3Tt", "signatures": ["ICLR.cc/2026/Conference/Submission16703/Reviewer_gQsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16703/Reviewer_gQsF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599992099, "cdate": 1761599992099, "tmdate": 1762926755560, "mdate": 1762926755560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEGA, a new family of large-scale datasets for molecular editing, comprising up to 31.4 million property-improving molecule pairs. A key feature of MEGA is that each pair is annotated with an explicit edit action: Replace, Insert, or Delete. The authors show that a model fine-tuned on MEGA (SFT) outperforms models trained on existing datasets. They additionally use GRPO with a similarity-aware reward for post-training, achieving new SOTA performance on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors systematically validate the dataset's effectiveness through a well-designed two-stage process involving SFT and GRPO. The results convincingly demonstrate that models trained on MEGA achieve significant performance gains.\n2. The paper innovatively reframes the complex task of molecular property optimization into \"edit actions\" that are more interpretable and easier for LLMs. The superior performance of models trained on MEGA compared to those trained on other datasets effectively validates the efficacy of this novel paradigm.\n3. The authors release MEGA in open access to the community, which provides a solid foundation for reproducibility and future research."}, "weaknesses": {"value": "1. While the MEGA dataset is a core contribution, the molecular properties it covers are narrowly focused on physicochemical attributes (e.g., LogP, TPSA, QED). These properties represent only a small fraction of the optimization objectives in drug discovery and can often be predicted with high accuracy using classical computational methods or rule-based models. The true value of AI in molecular optimization lies in addressing properties that are expensive to acquire experimentally, such as biological activity and ADMET profiles. To enhance the dataset's practical impact, it is strongly recommended that the authors consider extending it to include more critical endpoints like binding affinity and ADMET properties, which are abundantly available in public databases like ChEMBL.\n2. Although the paper presents a comparison against DrugAssist and Gemini 2.5 Pro on the DrugAssist benchmark, it lacks a broader evaluation against other leading commercial and open-source LLMs and domain-specific models. Several public benchmarks for molecular optimization have already emerged (PMO, ChemCoTBench). Benchmarking the MEGA-trained models on these third-party platforms would be highly beneficial."}, "questions": {"value": "1. The paper defines three coarse edit actions. While effective, this taxonomy might not capture more complex edits like scaffold hopping or ring system modifications. Have the authors considered these limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tvKIbFWTfE", "forum": "wzou4rm3Tt", "replyto": "wzou4rm3Tt", "signatures": ["ICLR.cc/2026/Conference/Submission16703/Reviewer_wdBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16703/Reviewer_wdBR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622823491, "cdate": 1761622823491, "tmdate": 1762926754748, "mdate": 1762926754748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEGA, a large-scale dataset for molecular editing and property-guided optimization. It contains over 31 million parent–child molecular pairs, each annotated with explicit edit actions (Insert, Replace, Delete) and property-based improvement labels. Using MEGA, the authors train a Llama-3 8B model via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) with a similarity-aware reward. The MEGA-trained models significantly outperform prior datasets (MolEdit-Instruct, MolOpt-Instructions, DrugAssist) by up to +21.47pp in hit ratio and achieve a 36× gain in data efficiency under GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized, clearly written, and easy to follow. Figures and tables support the narrative, and the methodology is presented with good clarity.\n2. The experimental design is solid and thorough. The comparisons, ablations, and qualitative results convincingly demonstrate MEGA’s effectiveness and the robustness of the findings.\n3. The MEGA dataset is large and diverse."}, "weaknesses": {"value": "1. Because MEGA relies on predefined fragmentation rules (BRICS, HR, RECAP) and applies only one edit per molecule, it may overrepresent easy-to-fragment scaffolds and common functional groups while under-sampling complex chemistries such as macrocycles or multi-center transformations. The strong skew toward Replace and Insert actions (≈97%) could also bias models toward minimal, local edits and limit generalization to broader molecular modifications.\n\n2. Property improvements are defined solely by RDKit-calculated proxies (LogP, QED, TPSA, etc.) and discrete thresholds. This could cause models to overfit to these proxy metrics rather than true pharmacological quality, and to exploit threshold boundaries. Similarly, the Tanimoto-based reward in RL favors small structural changes, which, while chemically valid, may discourage more creative or synthetically diverse optimizations."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "H8H7aKe62F", "forum": "wzou4rm3Tt", "replyto": "wzou4rm3Tt", "signatures": ["ICLR.cc/2026/Conference/Submission16703/Reviewer_13Xh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16703/Reviewer_13Xh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925030001, "cdate": 1761925030001, "tmdate": 1762926753960, "mdate": 1762926753960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper outlines a new chemical edit dataset for use with large language models. The goal is to improve language models' understanding of consequences of chemical modifications such as replacing functional groups etc. Each dataset record includes a pair of smiles, edit operation used, as well as resulting property changes (whether they reach pre-defined thresholds). 28 tasks/properties included are all simple immediately calculable properties that RdKit can provide (e.g., aqueous solubility, QED, etc). The dataset is offered in two versions, small and large. The authors experiment with fine-tuning language models on their new dataset, including GRPO RL post-training, in comparison to other, smaller datasets, and showing clear gains in their dataset-aligned evaluation tasks. GRPO with a reward function adjusted with a thresholded Tanimoto similarity is shown to offer clear gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Distilling into language models some ability to understand consequences of chemical edits can be very helpful. The authors take a step in this direction by building a reasonably sized dataset (Mega-Large, 31M edit-pairs), derived from combinatorial compound library ZINC. The evaluation tasks are aligned with the dataset, including two-property combinations, showing (expectedly due to close alignment) gains in these tasks after fine-tuning with/without RL. The evaluation in this sense seems careful and comprehensive. The authors' choice of including some constructed \"negative\" pairs is a good addition to the dataset."}, "weaknesses": {"value": "My main concern is that the properties in the dataset are RdKit calculable properties and thresholded with pre-set boundaries. These are relatively simple and straightforwardly calculable properties. If this is the focus, why not instead provide a virtual dataset (of any size) with calls to RdKit for properties rather than fix compounds, thresholds etc? \n\nAs far as I can see there are no measured quantities, no reaction information included in the proposed dataset. Reactions would seem like a more helpful fine-tuning dataset than edits. I.e., what products (including yield) would result from using a particular catalyst, reaction conditions, etc. Commercial databases such as Reaxys do include such information and thus may be more helpful for compound optimization, resolving retrosythetic pathways, or learning to hypothesize alternative, easily synthesizable products that have similar, desirable property profiles. \n\nGiven that edits, properties and thresholding are relatively simple, my concern is also that the language model improvements based on the dataset do not offer much practical utility, do not generalize beyond these edits. Some demonstration that the edit dataset helps generalize LLM capabilities beyond RdKit properties would be particularly helpful."}, "questions": {"value": "Can you show any gains in tasks that are not directly aligned with tasks in the dataset? \nAre allowable edit locations/types of edits also explicated in the dataset records?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QnHNt4PW6r", "forum": "wzou4rm3Tt", "replyto": "wzou4rm3Tt", "signatures": ["ICLR.cc/2026/Conference/Submission16703/Reviewer_BLFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16703/Reviewer_BLFN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097768711, "cdate": 1762097768711, "tmdate": 1762926753536, "mdate": 1762926753536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}