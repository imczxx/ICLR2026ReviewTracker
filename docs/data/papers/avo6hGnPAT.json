{"id": "avo6hGnPAT", "number": 14048, "cdate": 1758227576399, "mdate": 1759897393667, "content": {"title": "SkipSR: Faster Super-Resolution with Token Skipping", "abstract": "Diffusion-based super-resolution (SR) is a key component in video generation and video restoration, but is slow and expensive, limiting scalability to higher resolutions and longer videos.\nOur key insight is that many regions in video are inherently low-detail and gain little from refinement, yet current methods process all pixels uniformly. To take advantage of this, we propose SkipSR, a simple framework for accelerating video SR by identifying low-detail regions directly from low-resolution input, then skipping computation on them entirely, only super-resolving the areas that require refinement. \nThis simple yet effective strategy preserves perceptual quality in both standard and one-step diffusion SR models while significantly reducing computation. In standard SR benchmarks, our method achieves up to 60% faster end-to-end latency than prior models on 720p videos with no perceptible loss in quality. Video demos are available at our anonymous project page.", "tldr": "We accelerate video super-resolution by identifying complex regions and only applying sparse attention to them instead of uniformly processing the entire input.", "keywords": ["video generation", "efficient transformers", "super-resolution", "diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c53b3e27d4d94ae659be130fca12d59ec683f99.pdf", "supplementary_material": "/attachment/559f9586943b07f5da14097e131322708784a214.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes SkipSR to focus on refining complex regions only and skip simple regions by directly upsampling in pixel space. To determine the simple regions, it determines a threshold offline and  trains a light-weight predictor module. Detailed experiments in multi-step and one-step setting demonstrate the efficiency of the proposed method in cascaded video super-resolution scenario."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The authors observed an unnoticed problem: smooth regions and complex regions were allocated the same amount of computational power. They proposed a simple and effective method to address it.\n* To implement their method to open-source framework, they introduced several modifications to adapt to a swin-based DiT backbone.\n* Detailed experiments, including qualitative, quantitative, user study and video results, demonstrate its effectiveness."}, "weaknesses": {"value": "* ● My primary concern lies in the simple supervision of this predictor. In real-world VSR, the input videos often contain high-frequency degradation patterns, which conflicts the assumption in this paper. Also, no real-world results are shown in the video demo. The authors should explain why the current objective remains effective in real-world scenarios. Otherwise, the authors should consider to restrict the scope of SkipSR to cascaded video super-resolution (for AIGC videos only).\n* SkipSR is implemented on a window-based DiT backbone and the results are mixed with upsampled input by complexity mask. Thus, I am concerned about the seam problem and color difference between adjacent areas in the generated results. The authors should indicate the potential risks associated with this method or explain why such issues do not exist.\n* The motivation of skip-aware RoPE is unclear. By using original RoPE setting, each unskipped token can access other tokens in the 3D attention module. By adapting with the flash attention, the proposed method can seamlessly integrate into its DiT backbone. Thus, modifying the RoPE is confusing and unnecessary."}, "questions": {"value": "* More details about the mask predictor should be added, including the selection of the training videos, the convergence speed and its robustness against diverse testing scenarios.\n* A formula is missed in line 262, and the part of skip-aware RoPE is unclear now.\n* The authors are recommended to provide more uncompressed video results, including real-world and aigc scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "81DeH7YqQk", "forum": "avo6hGnPAT", "replyto": "avo6hGnPAT", "signatures": ["ICLR.cc/2026/Conference/Submission14048/Reviewer_BvQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14048/Reviewer_BvQa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948034926, "cdate": 1760948034926, "tmdate": 1762924534252, "mdate": 1762924534252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SkipSR is a framework that accelerates Video Super-Resolution (VSR) by skipping visually simple tokens for the diffusion loop and just using their interpolation (simple tokens are defined by undercutting a threshold for the reconstruction error of the VAE and interpolation) instead. By doing so, they achieve substantial speedups (if the videos are overall not heavily degraded), which is an interesting idea and important research direction for the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Although the selection of visually important areas to apply SR models to is not entirely novel, this paper goes a step further than previous work by completely bypassing transformer computation for certain regions to achieve substantial runtime improvement. \n- The technical quality of the work is strong and thorough. The authors motivate their design through empirical analysis, showing that large portions of typical videos consist of low-frequency, simple regions that can be reconstructed with minimal loss using cheap upsampling. \n- The paper is clearly structured and easy to follow."}, "weaknesses": {"value": "- The authors highlighted that themselves: SkipSR time improvement is limited to clean inputs with skippable patches.\n- Dependence on a heuristic threshold and empirical tuning. The threshold sounds a bit arbitrary and should be analyzed further. Also, an analysis on how good the mask predictor is, when does it fail and so on, would be appreciated."}, "questions": {"value": "- The paper uses a relatively lightweight 3D convolutional predictor to identify skippable patches. Could the authors clarify how sensitive SkipSR’s performance is to the predictor design?\n- Have you experimented with 2D predictors (spatial only) or different temporal receptive fields to assess whether temporal context meaningfully improves mask accuracy? \n- How stable are the predicted masks across frames in dynamic scenes? If the mask varies rapidly between adjacent frames, could this introduce subtle flicker or temporal inconsistencies in the reconstructed video?\n- The skip threshold τ = 0.0002 appears empirically tuned. Do the authors foresee a way to make this parameter adaptive (e.g., through training the mask predictor on the fly and using the mask sparsity as additional regularization)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jX86hFeQrZ", "forum": "avo6hGnPAT", "replyto": "avo6hGnPAT", "signatures": ["ICLR.cc/2026/Conference/Submission14048/Reviewer_Hc6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14048/Reviewer_Hc6Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761170155501, "cdate": 1761170155501, "tmdate": 1762924533825, "mdate": 1762924533825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SkipSR – a method to accelerate video SR by skipping transformer processing in “simple” regions like sky or simple background. These skipped regions are upsampled via bilinear interpolation. The authors claim the method achieves up to 60% faster end-to-end latency on 720p videos with no perceptible quality loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is original in its application to modern video diffusion transformers.\n2. Experiments are broad, covering multiple datasets, resolutions, and model types.\n3. The speedups are substantial and significant for practical contribution for real-world applications."}, "weaknesses": {"value": "1. The analysis in Table 1 shows that on heavily corrupted videos, the skippable patch percentage drops to 0.9%, resulting in no speedup. This suggests the method's utility is highly dataset-dependent and may not generalize well to challenging restoration tasks.\n2. The mask predictor is crucial, yet its design is too simple. There is no ablation study on its architecture, nor a comparison to potentially more sophisticated segmentation networks\n3. The practical overhead of the masking operation itself is not profiled in detail. The claim on Page 7 that the mask predictor adds \"negligible overhead\" is stated but not quantitatively broken down, which is important for a method whose value is entirely in net speedup.\n4. The paper does not deeply explore the trade-off between mask accuracy and computational overhead in more dynamic or complex scenes."}, "questions": {"value": "1. Could the masking strategy be adapted dynamically per frame or scene to improve robustness?\n2. Have you considered combining SkipSR with other efficiency methods like distillation or quantization for further speedup?\n3. Given the significant performance drop on corrupted videos (YouHQ-40 corrupted), what is a realistic estimate of the fraction of real-world video content where SkipSR would provide a visible speedup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YVqJAbk5fD", "forum": "avo6hGnPAT", "replyto": "avo6hGnPAT", "signatures": ["ICLR.cc/2026/Conference/Submission14048/Reviewer_YAoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14048/Reviewer_YAoC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927488585, "cdate": 1761927488585, "tmdate": 1762924533350, "mdate": 1762924533350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}