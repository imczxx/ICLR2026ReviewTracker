{"id": "POLPQHcuYY", "number": 19364, "cdate": 1758295704522, "mdate": 1763559872153, "content": {"title": "Topology of Attention Detects Hallucinations in Code LLMs", "abstract": "While the AI-code assistant tools become widespread, automatic assessment of the correctness of the generated code becomes a significant challenge. Code LLMs are prone to hallucinations, which may lead to code that does not solve a required problem, or even to code with severe security vulnerabilities. \nIn this paper, we propose a new approach to assessment of code correctness. Our solution is based on topological data analysis (TDA) of attention maps of code LLMs.\nWe carry out experiments with two benchmarks - HumanEval, MBPP and 5 code LLMs: StarCoder2-7B,  CodeLlama-7B, DeepSeek-Coder-6.7B, Qwen2.5-Coder-7B, Magicoder-S-DS-6.7B.\nThe experimental results show that the proposed method is better than several baselines. Moreover, the trained classifiers are transferable between coding benchmarks.", "tldr": "", "keywords": ["Code Models", "Robustness", "Attention Matrices"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f14fa243fb3f4c0113e26d62cba2a5ba9785a150.pdf", "supplementary_material": "/attachment/8fb12c1dd6d1f6184a0a04022688760583687f55.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel approach to detect hallucinations in code generated by Large Language Models using topological data analysis of attention maps. The authors transform attention patterns into weighted graphs by applying Manifold Topology Divergence (MTD) on the weighted graphs derived from the attention maps. These attention features are used to detect the hallucinations using a XGBoost algorithm. The paper claims that the proposed method is better than several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper shows an innovative application of topological data analysis to code hallucination detection\n- The paper evaluated the experiments on multiple code based LLMs, code benchmarks, and multiple languages"}, "weaknesses": {"value": "- \"To enable diversity of generated solutions, a sampling with non-zero temperature\nof was done.\" This sentence is unclear.\n- The presentation in the paper can be better, there are a few grammatical errors, and the paper references a number of results from Appendix, even before explaining the results in the main body. Whenever the main body results are referenced, there is a lame reference saying the results are in so and so table, without actually explaining what the insights are from the observations from the experiments.\n- The paper poses to use XGBoost as a classifier (I feel more as a meta classifier) on the attention features extracted from the transformer. There are number of clarifications sorted here:\n  - Which attention layer are these coming from there is more than one attention layer in any given coding LLM used in the paper?\n  - Why XGBoost as a classifier for hallucination detection?\n  - Why XGBoost was compared against a Random classifier baseline? No details are offered about that Random baseline.\n  - Why a random baseline, since the extracted features are already in the graph format, whynot use a simple light-weight Graphical Neural Net (GNN) based classifier as a baseline or even as the main classifier?\n- What is the definition of the mean log probabilities of the generated tokens? Is the log applied on the softmax probabilities of the generated tokens?\n- The paper does not talk about the computational overhead incurred in terms of the topological feature extraction."}, "questions": {"value": "See the weaknesses section above for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Nf4oPyliu9", "forum": "POLPQHcuYY", "replyto": "POLPQHcuYY", "signatures": ["ICLR.cc/2026/Conference/Submission19364/Reviewer_6RBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19364/Reviewer_6RBV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600019374, "cdate": 1761600019374, "tmdate": 1762931299270, "mdate": 1762931299270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the automatic assessment of code correctness without using test cases. The proposed approach uses the attention maps of a code LLM to extract topological features, which in turn are used to train an xgboost classifier. The topological features come from the prior work of Barannikov et al. (2021) and Chazal & Michel (2017). The authors show that using these features outperforms many well-thought-out baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses the correctness assessment of code generated by LLMs. The addressed problem is important because correctness testing may be costly in time, and traditional methods require curation of test cases, where good coverage is challenging to achieve. Moreover, LLM-generated code may contain security vulnerabilities, so a prediction before code execution is desirable.\n\nThe authors have done a very comprehensive evaluation of the proposed method. The experiments include not just comparison with well-thought-out baselines but also feature analysis, different classification/ranking setups, evaluations on various languages, transferability, and an ablation study. The empirical evaluation is solid."}, "weaknesses": {"value": "It is a mystery why attention maps and the topological features are effective for code correctness prediction. There are a few cited references (see the Introduction and Related Work sections of the paper) showing the usefulness of attention maps, but the evidence is just empirical. Moreover, there lack of justification on why the specific feature, Manifold Topology Divergence (MTD), is a good choice (except that it happens to apply to the current problem).\n\nThe ablation study (see Table 9 in the appendix) suggests that the MTD feature is very much comparable to the diagonal attention features, and combining the two does not yield significant improvement. As a consequence, one can be convinced that attention maps are helpful for the problem at hand, but one does not necessarily need to use the MTD features; the diagonal features are equally effective.\n\nFigure 4: It is unclear why the shown distributions suggest that the corresponding features are discriminative. In fact, the distributions have significant overlaps."}, "questions": {"value": "See the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0qVoqKgQCu", "forum": "POLPQHcuYY", "replyto": "POLPQHcuYY", "signatures": ["ICLR.cc/2026/Conference/Submission19364/Reviewer_CScZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19364/Reviewer_CScZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855195697, "cdate": 1761855195697, "tmdate": 1762931298743, "mdate": 1762931298743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new hallucination detection method for code generation. Their approach analyzes the attention maps using topological data analysis. The attention map is represented as a weighted graph, and its structure is quantified using Manifold Topology Divergence (MTD) metrics between the prompt and the generated code. These topological features are used to train an XGBoost classifier to predict whether generated code is correct without executing it. Experiments on HumanEval and MBPP across five code LLMs demonstrate higher ROC-AUC and F1-scores than other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method uses the topology method to interpret and access the LLM attention map, which is innovative.\n2. The method solves a meaningful task. It detects the correctness of generated code without executing it.\n3. The experiment on five code LLMs and two benchmarks demonstrates strong performance, outperforming other baselines."}, "weaknesses": {"value": "1. While this paper introduces a topology analysis method for hallucination detection, it lacks a formal argument or theoretical proof of the relationship between topological divergence and hallucination.\n2. This paper provides insufficient details about how the training data is collected for the classifier. If the training data are derived from the test benchmarks, there is a potential risk of data leakage.\n3. The derivation of the symmetrized weight w{ij} = 1 - \\max(a{ij}, a_{ji}) (Line 193) is heuristic. Justification for this equation or alternative design is not discussed.\n4. Some baselines (e.g., Self-Eval) are adapted from general-purpose LLMs. More advanced methods like CODEJUDGE (EMNLP 2024) are not discussed. \n5. Since this paper is to determine the correctness of generated code, it would be valuable to compare with traditional program analysis methods (e.g., control flow graph analysis), which may be more lightweight."}, "questions": {"value": "1. Though section 5.6 provide the classification fo error types, including failure case study would help strengthen the paper.\n2. What is the advantage of this method compare traditional static analysis methods, such as control flow graph analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bnHtTTdWiV", "forum": "POLPQHcuYY", "replyto": "POLPQHcuYY", "signatures": ["ICLR.cc/2026/Conference/Submission19364/Reviewer_V2ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19364/Reviewer_V2ph"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957817750, "cdate": 1761957817750, "tmdate": 1762931298355, "mdate": 1762931298355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to detect \"hallucinations\" in code LLMs before executing the code. The key idea is to treat the attention matrices between prompts and generated code as weighted complete graphs, then compute topological features like Cross-Barcode/MTD based on Topological Data Analysis (TDA), and use a lightweight classifier to predict correctness. The method outperforms various baselines on HumanEval and MBPP across multiple models (StarCoder2-7B, CodeLlama-7B, DeepSeek-Coder-6.7B, Qwen2.5-Coder-7B, Magicoder-S-DS-6.7B), and the trained classifiers can transfer between the two benchmarks. They symmetrize the attention matrix ($A=[a_{ij}]$) into edge weights ($w_{ij}=1-max(a_{ij},a_{ji}) for i≠j$), extract barcodes from the filtration as thresholds increase, and aggregate them into vertex-set-normalized features (MTD₀, MTD₁) plus diagonal attention sums. These features are computed layer-by-layer and head-by-head, then fed into an XGBoost binary classifier. The full pipeline goes: \"concatenate prompt + generation, extract attention, build graph, compute Cross-Barcode, train classifier\".\nExperimental results show that these topological features effectively discriminate between hallucinated and correct code distributions. Interestingly, only a small subset of heads/layers is needed to approach full performance, suggesting the relevant information is concentrated in a few attention heads. The authors note that future work will focus on pinpointing the specific buggy locations in the code."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "# Strengths\n\nThis paper proposes a novel approach to detecting hallucinations in code LLMs by applying Topological Data Analysis (TDA) to attention maps. The core innovation lies in representing attention maps as weighted graphs and computing Manifold Topology Divergence (MTD) features to quantify the dissimilarity between prompt and generated code token interactions, providing an interpretable geometric perspective on model failures. The technical execution is rigorous, incorporating thoughtful methodological choices: symmetrizing attention maps into undirected graphs, extracting H₀ (connected components) and H₁ (cycles) topological features, and including diagonal attention values for completeness. The authors also find that only approximately 5% of attention features are needed to achieve high performance, suggesting that specific attention heads can serve as reliable indicators of code correctness. The visualization design (Figures 1-2 illustrating MTD intuition, Figures 11-14 showing cross-barcodes and attention patterns) effectively helps readers understand the topological approach, and the paper is well-structured with comprehensive appendices supporting reproducibility.\n\nThe experimental evaluation is thorough and demonstrates strong empirical results across multiple dimensions. The authors tested 5 different code LLMs (with parameters ranging from 6.7B to 7B), two benchmarks (HumanEval and MBPP), and multiple programming languages (Python, Java, Go, Rust, Lua), achieving consistent ROC-AUC improvements of 10-20% over baseline methods. The experimental design employs proper 5-fold stratified group cross-validation to prevent data leakage between training and testing sets, and includes valuable transferability analysis (Table 4), feature importance analysis (Section 5.5), multi-class error detection (Table 5), and practical ranking applications (Table 3 showing 10-15% pass@1 improvements)."}, "weaknesses": {"value": "1. While the authors provide an intuitive explanation that \"attention drifting away from the prompt\" causes hallucinations, this only explains one failure mode. Section 5.6 shows that the method successfully detects multiple error types (SyntaxError, ZeroDivisionError, NameError, etc.), yet the paper fails to explain how these different failures manifest in attention topology.\n\n2. The symmetrization formula $w_{i,j} = 1 - max(a_{i,j}, a_{j,i})$ lacks theoretical justification, and there are no ablation experiments comparing alternative graph construction methods (thresholding, asymmetric graphs, different aggregation functions).\n\n3. More critically, there is a lack of causal analysis: do incorrect codes inherently possess different attention topology, or is this correlation driven by confounding factors (such as code length or complexity)?\n\n4. The transferability results (Table 4) reveal concerning performance degradation. For example, CodeLlama's ROC-AUC drops from 85.6 to 69.5 when transferring from HE→MBPP, and the best transfer performance requires combining attention features with mean log probability, indicating that attention features alone are insufficient. The paper does not analyze the distribution shift factors causing transfer failures (prompt structure? code style? problem difficulty?), includes no cross-model transferability experiments (can a classifier trained on StarCoder2 be applied to CodeLlama?), and only tests a single temperature parameter (T=0.8), whereas production systems typically use greedy decoding.\n\n5. The experimental design and baseline comparisons have significant limitations that restrict the scope of conclusions. The baseline methods do not include any static analysis tools (CodeQL, Pylint, basic syntax checkers) that can capture many syntax and some logic errors, nor do they include execution-based methods (such as running partial test suites).\n\n6. Practical deployment faces challenges: computing MTD for all heads and layers incurs non-trivial overhead (no timing analysis is provided), the method requires storing attention maps (memory issues are acknowledged but not addressed), and it needs labeled training data, which is impractical for new domains without existing functional tests."}, "questions": {"value": "See the previous section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zu99ULXwK5", "forum": "POLPQHcuYY", "replyto": "POLPQHcuYY", "signatures": ["ICLR.cc/2026/Conference/Submission19364/Reviewer_FxPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19364/Reviewer_FxPf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979445725, "cdate": 1761979445725, "tmdate": 1762931297811, "mdate": 1762931297811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}