{"id": "VFMXO3yrBk", "number": 3919, "cdate": 1757567656613, "mdate": 1759898062784, "content": {"title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets", "abstract": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using cryptocurrency markets as a natural laboratory, where \\$30 billion was lost to exploits in 2024, we evaluate 17 leading models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.\n\nOur results reveal a fundamental capability gap: without tools, even frontier models achieve only 12-28\\% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4\\% (GPT-5) versus 80\\% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search (55.5\\% of invocations) over authoritative blockchain data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps.\n\nThe implications extend beyond cryptocurrency to any domain where adversaries actively exploit AI weaknesses, e.g. cybersecurity, content moderation, etc. Our finding that Pass@k metrics mask dangerous trial-and-error behavior challenges fundamental assumptions about autonomous deployment. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.", "tldr": "We introduce CAIA, the first AI agent evaluation framework in hostile financial environments where every decision faces active opposition. Our results expose a fundamental limitation of LLMs on environments where data integrity cannot be assumed.", "keywords": ["benchmark; agent; tool use; finance; application; adversarial; LLM limitations;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40e3038c094d8a47547bf849abd59c4acf87bc6e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new benchmark that measures agentic capabilities in cryptocurrency markets.  They examine the fallibility of the pass@k metric, the cost-efficiency of open source models over closed source, and the general lack of competence displayed by language models on this task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper clearly discovers a set of tasks where language models could be, but are not, economically valuable.  This should serve as a warning to companies that are building or relying on language models to inform financial decisions.  \n\nThe paper is in general clear, well-motivated, and easy to follow."}, "weaknesses": {"value": "Too many details are missing from this paper for me to assess it properly.  To name just a few:\n\n1.  How are the models prompted and how much prompt optimization did you do?  Is failure on the crypto benchmark really indicative of poor capabilities or did you not spend enough time optimizing the prompts?\n2.  There are a lot of agentic benchmarks and evaluations out there (some of which are not cited, e.g. [1, 2]), especially since you mention model vulnerability to false information.  I don't see what sets this one apart.  Yes, I read your justification for why crypto markets are a great setting to study agents in high stakes settings, but it seems like there are many others.  Why is this particular setting more worth studying than others?\n3.  There are too many missing details in the paper that make it difficult to assess.  What exactly do these tasks look like?  Can you provide some specific examples rather than just high-level details?  I found it very hard to assess this paper because the descriptions were too high-level.\n4.  The insights feel a bit shallow.  The arguments about pass@k being inaccurate feel fairly obvious, and I would likewise have guessed that open-source models would be cheaper than closed-source ones by at least 1-2 orders of magnitude.  \n5.  The agents underperform on simple tasks, but can you provide some guidance or an actionable plan to improve the agents?  \n6.  There are also no error bars in your study.  You could at least provide bootstrapped error bars given that your dataset is not that huge.\n\n[1] https://arxiv.org/abs/2406.12045\n[2] https://openreview.net/forum?id=RwoMf7YSfD"}, "questions": {"value": "See the above.  \n\nI would also have liked to see some analysis on where humans are still far outperforming language models versus where their capabilities are close.  You collected human data, but you don't provide very much analysis of it."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper included a small-scale study involving human participants.  Not enough details were given on how these human participants were compensated or treated throughout the study."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d0NM8IUPHM", "forum": "VFMXO3yrBk", "replyto": "VFMXO3yrBk", "signatures": ["ICLR.cc/2026/Conference/Submission3919/Reviewer_VCGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3919/Reviewer_VCGp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593336646, "cdate": 1761593336646, "tmdate": 1762917097602, "mdate": 1762917097602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CAIA, a benchmark to evaluate state-of-the-art LLMs, with or without additional tools, on blockchains/cryptocurrencies related tasks. Following the recent standards in benchmark evaluations, the authors claim the benchmark is time-sensitive to mitigate contamination. The evaluation reveals a large performance increase when tools are used, but the performance remain under that of human experts, which is likely explained by the over-reliance on less reliable tools, such as web search, showcasing failure in more realistic, adversarial prone environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Benchmarking AI agents is a hot and interesting research topic. Narrow domain benchmarks are necessary to shed light on the limitations of the current AI systems.\n- 17 state-of-the-art LLMs were tested, with coherent conclusions across the board.\n- The authors provide a codebase that seems easy to use for reproducibility"}, "weaknesses": {"value": "- The adversarial attack model is not well explained in the paper. The authors repeatedly refer to adversarial and manipulated environments without precisely describing that. It seems like the authors just assume that results from web search are inherently prone to attacks without further details.\n- Related to the previous point, in the introduction, the authors list a few deception tactics, can they explain why these “adversarial conditions require agents to genuinely distinguish truth from manipulation” ? Do we expect these models to provide some security guarantees not already provided by blockchains ?\n- The paper lacks many detail about the agentic framework, such as a full description of how the tool calling is orchestrated.\n- Overall, most of the conclusions of the article are not surprising. For instance, It is very typical to have pass@5 much bigger than pass@1. The performance with-tools is expected to be much greater than the performance without tools.\n- Real examples of benchmark questions should be included in the paper for easier readability. No appendix is provided with the paper.\n- The claim that the “Web2 data” the current AI systems are trained on are trustworthy is blatantly wrong.\n- The number of tasks seems quite limited.\n- Hallucinations, while mentioned in the title, are never mentioned in the paper which seems a bit misleading.\n\n**Some minor weaknesses**\n\n- 421 - It seems to me that tokenomics and project discovery show a larger gap than on-chain and trend analysis\n- 415 - This sentence contradicts the previous one, and it is not clearly justified : “On the other hand, the improvement by more tool calls is not apparent “\n- 430 - The fact that these improvements are largely driven by generic web searches may point to a weakness of this benchmark. Some tasks may be easily solvable via web search.\n- 167 - “(Etherscan, CoinGecko, DefiLlama) eth; coi; def ” This is not a standard way to refer to bibliographical works."}, "questions": {"value": "- How does the benchmark ensure time-sensitivity ? The authors claim the benchmark will be updated, but will the new added tasks require the curation procedure (including the expert reviews) ? \n\n- Can the tool selection catastrophe phenomenon raised in the paper be a simple artifact of the prompt that is used ? Would including something that suggest to the model that blockchain tools are more authoritative increase their usage percentage (and perhaps also enhance the performance if these tools are objectively better) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cfdd3B0jTM", "forum": "VFMXO3yrBk", "replyto": "VFMXO3yrBk", "signatures": ["ICLR.cc/2026/Conference/Submission3919/Reviewer_tuNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3919/Reviewer_tuNg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756845903, "cdate": 1761756845903, "tmdate": 1762917097445, "mdate": 1762917097445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes benchmarking the agentic capabilities of large language models (LLMs) in a real-world setting by using a series of questions (queries) related to cryptocurrency. The benchmark consists of 178 questions (or tasks) designed to measure the tool-calling capabilities of LLMs (e.g., Google and Twitter search). The paper argues that cryptocurrency presents an \"adversarial information landscape,\" requiring LLMs to critically assess the information found online. It further posits that this domain represents a high-stakes environment where incorrect answers can lead to significant financial losses. The experiments benchmark 17 state-of-the-art (SOTA) LLMs on the 178 questions, both with and without the provided tools, against baseline human performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The benchmark contains 178 questions relevant to cryptocurrency markets with objectively verifiable ground truth answers. This is crucial for evaluating the robustness of LLMs in critical scenarios. The significance of cryptocurrency as an application area is underscored by the observed gap between human performance (80%) and the best-performing LLMs (~67%).\n\n-The dataset required crowd-sourcing, comprising 10,000 queries collected from 3,000 users."}, "weaknesses": {"value": "-The paper could enhance clarity by providing examples from the dataset in the section titled \"BENCHMARK CURATION,\" including sample questions and their corresponding ground truth answers.\n\n-It is mentioned that \"Our community-driven curation process, involving over 3,000 contributors including protocol developers, quantitative researchers, and venture capital investors, ensures ecological validity\" However, it would be beneficial to specify which community is referenced and what services the contributors use as active users.\n\n-The paper lists \"Financial Reality Grounding\" and \"Adversarial-First Evaluation\" as novelties but lacks concrete examples that illustrate these challenges. Providing specific tasks (out of 178) that could lead to irreversible financial consequences would strengthen the argument. In particular, experimental evidence showing instances where an LLM was misled by manipulative online content would be compelling.\n\n-The discussion of relevant work is scattered throughout the paper and could be more detailed.\n\nMinor comments:\n-Fig. 2, \"The dashed line indicates 80% human baseline performance \" dashed line is not rendered"}, "questions": {"value": "-It is stated that \"Crucially, our data curation process in ensures that correct answers are always accessible through appropriate tool use, and thus the challenge lies not in information availability but in tool selection and synthesis.\"\nAre the ground truth tool part of the dataset?\n\n-The accuracy reported in Tables 2 and 3 reflects overall performance across all tasks. Providing accuracy metrics for each of the six defined categories would enhance the paper.\n\n-Clarification on the terms \"query\" and \"task\" would be useful, as they seem to be used interchangeably. For instance, the sentence in the abstract \"CAIA evaluates 17 state-of-the-art models through 178 rigorously curated, time-anchored tasks sourced from 10k+ real world queries,\" might imply that there are 178 tasks with a total of 10,000 queries. Additionally, in section 2.3, the 10,000 queries are later referred to as 1,000 tasks after stage 1: automated filtering.\n\n-On pg. 4, it is stated that \"CAIA addresses weaknesses noted in prior evaluations: ...single-metric reporting that masks capability gaps Liang et al. (2022)\". Can you clarify how the paper specifically addresses the issue of single-metric reporting?\n\n-It would be useful to briefly discuss the prompt strategies used to evaluate the models. For example, are there prompt strategies that can help the LLM become more aware of malicious information sources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NjrFxmy83L", "forum": "VFMXO3yrBk", "replyto": "VFMXO3yrBk", "signatures": ["ICLR.cc/2026/Conference/Submission3919/Reviewer_KduE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3919/Reviewer_KduE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827936848, "cdate": 1761827936848, "tmdate": 1762917097280, "mdate": 1762917097280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper curates a dataset of 178 cryptocurrency-related questions and offers a custom framework to evaluate LLMs on it. It reveals issues in models making suboptimal tool choices, for instance choosing simple Google Search over Specialized blockchain tools for blockchai-related questions. Several LLMs are used both open and closed source."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper provides 178 cryptocurrency-related questions that may useful as a domain-specific eval.\n- Interesting issues are raised such as agents acquiring outdated or misinform data via tool-calls, or making suboptimal tool choices when better tools are available for the task."}, "weaknesses": {"value": "- No Related Works or Background section\n- The dataset shared in the paper only contains 178 questions-answer pairs.\n- The paper is written in a very sensationalistic style with little specificity and reads like it has been written with LLMs. (the authors do disclose this).\n- Going through the dataset shared, I do not believe these question-answer pairs adhere to the authors' proposed opinion shift of \"evaluations should test robust survival in adversarial, high-stakes environments\". For instance, answering the following questions on their own do not seem related at all to 'high-stakes environments' or 'robust survival': \"Retrieve the block timestamp for Ethereum block 19,560,000\", \"Calculate the total gas consumed by CYBER L2 on 2025-05-01 UTC.\".\n- Overall the paper seems very weak in its methodolog and literature review (if any).\n- The paper is not rigurous in showing how the findings could \"extend to any adversarial domain where information is weaponized for profit\"."}, "questions": {"value": "- What are the 23 tools available in the 'with-tools' set up? Add this to the appendix.\n- How do your findings extend to any adversarial domain where information is weaponized for profit? Most questions are specific to cryptocurrencies, and so does your framework. You do not provide enough detail in the methodology to assess this, and some of the tools are blockchain specific.\n- Is your claim that the dataset questions are mostly adversarial? Is e.g. \"Retrieve the block timestamp for Ethereum block 19,560,000\" adversarial?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ew3Ctp5Q9h", "forum": "VFMXO3yrBk", "replyto": "VFMXO3yrBk", "signatures": ["ICLR.cc/2026/Conference/Submission3919/Reviewer_VpY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3919/Reviewer_VpY7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982164489, "cdate": 1761982164489, "tmdate": 1762917096872, "mdate": 1762917096872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark of AI agents in high-stakes environments based on crypto financial markets. The benchmark is built on 178 carefully selected tasks, which cover multiple aspects for agents benchmarking.\nA crucial and novel feature of the benchmark is that it proposes an adversarial, hostile environment, where taking the wrong action can have harsh consequences: the goal here is to evaluate \"resilience\" rather than \"competence\" only.\nThis benchmark offers another perspectives on AI agents, evaluating models along multiple metrics, notably their one-shot performance rather than averaged over multiple runs.\nA meaningful human baseline is derived, based on decisions taken by a set of entry-level human analysts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The benchmark proposed in this paper offers a novel perspective on AI agents evaluation, allowing evaluation from multiple perspectives rather than merely accuracy in fixed tasks and controlled environment. This goes beyond classical assumptions that tools work as expected, information is trustworthy, and other agents cooperate.\n2. Although limited to crypto markets, many tasks are included, allowing for evaluation of a larger range of abilities than usual. Moreover, the tasks are based on real tasks, not handcrafted ones.\n3. A crucial feature of the proposed study is the idea of \"deception\", measured here in terms of money loss, due to hostility of other agents or unreliability of used information. This seems to be a key feature towards developing agents that are better-behaved on real tasks.\n4. This benchmark takes a step towards more complete and fine-grained evaluation metrics, beyond the usual \"accuracy\" which may be to easy to overfit for without improving behavior on real problems.\n5. Many state of the art, proprietary and open source, models are evaluated, which gives a strong confirmation that the proposed benchmark gives useful assessment of AI agents performance."}, "weaknesses": {"value": "1. One important claim is that current benchmark do not translate into real-world readiness: while this seems to take a step towards this direction, there is no guarantees yet that the proposed benchmark improves over existing ones in this regard.\n2. The claim of \"universality\" seems a bit too strong given that the benchmark is solely based on crypto markets. Despite the fact that this takes a significant step towards more sensible evaluation of AI agents, this claim appears as an over-statement, as there are not tangible reasons for other problems to exhibit similar properties as crypto markets.\n3. The categorization of the tasks, given in Table 1, is too limited; it is difficult to understand what are the abilities evaluated by each task, and what types of task are included in each category."}, "questions": {"value": "1. The proposed tasks were selected based on proposition from 3000 users: who were these users? Where they recruited specifically for this study?\n2. A human baseline is provided, which is a novel contribution in itself. However, it consists in entry-level analysts: why this restriction? How expensive would it be to produce a similar baseline including experts of the domain?\n3. Authors claim that aggregating perfomance over multiple runs may lead to improved performance because agents luckily guess through trial and error rather than reasoning: can this be assessed for more rigorously?\n4. It is claimed (line 415) that based on Figure 4's result \"the improvement by more tool calls is not apparent\": this seems contradictory with the fact that models that call more tools on average are more performant. Can the authors elaborate on this statement?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "This paper proposes a novel benchmark, that allows for novel criteria to evaluate AI agents based on observations made in crypto markets.\n\nWhile this is a significant shift of perspective in evaluating AI agents, this may have harmful consequences in multiple regards:\n- making this benchmark available can significantly help to produce AI agents that are much more capable in adversarial contexts, allowing for models that are better at falsifying evidence, creating misinformation, or manipulating markets;\n- the metrics and insights derived from this benchmark hold for financial crypto markets, but the authors claim that this goes beyond this scope: this is a dangerous statement, as there are many other aspects that are not accounted for in the benchmark since they are not relevant to financial markets (e.g., discriminations, privacy issues).\n\nIn my opinion, these ethical concerns do not mean that the benchmark should not be published, by I believe that (i) it should be clearly mentioned that the proposed benchmark is not intended for building AI agents that purposefully manipulate markets (e.g., releasing it under a license that prohibits such usage maybe?), and (ii) the claims that the conclusions drawn from crypto markets extend to other tasks should be seriously toned down."}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Px2YY5ZtAx", "forum": "VFMXO3yrBk", "replyto": "VFMXO3yrBk", "signatures": ["ICLR.cc/2026/Conference/Submission3919/Reviewer_VfNr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3919/Reviewer_VfNr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762366594475, "cdate": 1762366594475, "tmdate": 1762917096667, "mdate": 1762917096667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}