{"id": "If8O8CdCbi", "number": 19906, "cdate": 1758300451471, "mdate": 1759897012860, "content": {"title": "Keep the Beam on Track: Stabilizing Reward Trajectories in Guided Decoding", "abstract": "Decoding algorithms play a central role in enhancing the performance of large language models (LLMs) on complex reasoning tasks. \nA common approach incorporates Process Reward Models (PRMs), which estimate the quality of intermediate reasoning paths and guide the selection of possible continuations. \nIn this setting, our analysis reveals two notable phenomena: reward estimates tend to decline as reasoning progresses, and the reasoning paths exhibit distinct volatility patterns across decoding steps\ndepending on whether the paths lead to correct or incorrect final answers. \nIn particular, correct reasoning tends to be associated with stable reward trajectories, while incorrect reasoning often shows high volatility. \nMotivated by this observation, we propose Volatility-Scaled Guided Decoding (VSGD), a decoding algorithm that prioritizes candidate paths with lower volatility by jointly considering the magnitude of PRM-estimated rewards and the volatility of these rewards across decoding steps. \nExperiments on datasets including GSM8K and MATH500 indicate that VSGD reduces the volatility of selected reward trajectories and improves the accuracy of the final answer. \nThese findings suggest that considering the temporal dynamics of reward values, in addition to their magnitude, provides a potential direction for enhancing guided decoding in LLMs.", "tldr": "", "keywords": ["Large Language Model", "inference", "Decoding algorithm"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea8286fe2e1ea46a33d5fbb50ffc35ef6fd7c614.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Volatility-Scaled Guided Decoding (VSGD), a decoding algorithm for large language models that stabilizes Process Reward Model (PRM) guidance by accounting for the volatility of reward trajectories during reasoning. The authors observe that correct reasoning paths exhibit lower reward volatility than incorrect ones and design VSGD to prioritize more stable trajectories. Evaluations on GSM8K, MATH500, and an MMLU subset demonstrate consistent accuracy improvements and reduced incomplete reasoning, showing that modeling temporal reward dynamics enhances guided decoding efficiency and reliability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method itself is coherent and easy to follow. The authors conduct experiments on several benchmarks to validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1.\tOverall, the contribution of this paper, while meaningful, is somewhat incremental relative to prior PRM-guided decoding research. The method reuses standard components such as beam search and PRM scoring, with limited theoretical advancement beyond empirical validation.\n2.\tAcross datasets, performance gains over the strongest baseline are relatively small (about 1–1.5% on average). Such improvements may not justify introducing an additional volatility computation layer in practical systems. Moreover, the improvements vary across domains, with some categories (e.g., Virology and GSM8K) showing minimal or even negative differences, indicating limited robustness.\n3.\tThe authors employ only one LLM backbone in their experiments. To strengthen the evaluation, recently proposed LLMs, such as Mistral and Qwen should be included as the backbone models for comparation.\n4.\tThe paper does not investigate how sensitive VSGD is to its hyperparameters, such as the stability constant ϵ or the aggregation function used in ranking candidates. It also lacks ablation experiments isolating the effects of volatility scaling from other algorithmic factors."}, "questions": {"value": "1.\tHow sensitive is VSGD to the choice of the stability constant ϵ and the aggregation function Agg?\n2.\tWould volatility normalization still help when PRM rewards are poorly calibrated or highly correlated with token length?\n3.\tCould volatility be exploited during PRM training rather than only inference?\n4.\tHave the authors tested whether volatility correlates with interpretability or logical consistency of reasoning paths beyond correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ftJGyMOU9g", "forum": "If8O8CdCbi", "replyto": "If8O8CdCbi", "signatures": ["ICLR.cc/2026/Conference/Submission19906/Reviewer_ZvW3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19906/Reviewer_ZvW3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619499397, "cdate": 1761619499397, "tmdate": 1762932063500, "mdate": 1762932063500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the instability of reward signals in Process Reward Model (PRM)–guided decoding for large language models. The authors observe that correct reasoning paths exhibit stable (low-volatility) reward trajectories, while incorrect ones fluctuate significantly. To exploit this, they propose Volatility-Scaled Guided Decoding (VSGD), which adjusts reward values based on their temporal volatility to favor stable reasoning paths. Experiments on GSM8K, MATH500, and MMLU subsets show that VSGD improves reasoning accuracy and reduces incomplete reasoning, demonstrating that incorporating reward stability enhances guided decoding performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strength lies in introducing Volatility-Scaled Guided Decoding (VSGD), which leverages reward stability to guide search. By prioritizing reasoning paths with low reward volatility, it offers a simple yet effective way to stabilize decoding and improve reasoning accuracy."}, "weaknesses": {"value": "1. The paper only observes the fluctuation of PRM rewards empirically without providing a theoretical explanation.\nIt remains unclear why reward volatility naturally emerges or correlates with reasoning correctness.\n\n2. All experiments rely solely on LLaMA-3.1-8B and VersaPRM, making the results model-specific.\nIt is uncertain whether stronger models like GPT-5 would show the same volatility behavior or performance gains.\n\n3. The study omits stronger decoding and test-time scaling baselines such as MCTS or Q-function-based methods. Moreover, the reported accuracy improvement is small (around +1.1–1.4), limiting the practical significance.\n\n4. The paper introduces several hyperparameters but does not analyze their sensitivity or robustness. Without such evaluation, it is difficult to assess the stability and reproducibility of the proposed method."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lDdG1jKik6", "forum": "If8O8CdCbi", "replyto": "If8O8CdCbi", "signatures": ["ICLR.cc/2026/Conference/Submission19906/Reviewer_m9QW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19906/Reviewer_m9QW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738502736, "cdate": 1761738502736, "tmdate": 1762932063023, "mdate": 1762932063023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new decoding algorithm for Large Language Models (LLMs) called Volatility-Scaled Guided Decoding (VSGD), which prioritizes candidate paths with lower volatility by jointly considering the magnitude of PRM-estimated rewards and the volatility of these rewards across decoding step. The technique is derived from the observation that correct reasoning paths usually exhibit high rewards but low volatility across reasoning steps. Thus, instead of selecting candidates with maximum reward in each step, the authors select the ones with the maximum ratio of reward and standard deviation. Experimental results demonstrate the merits of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- In terms of clarity, I think the authors did a great job. From pilot study to algorithm, everything is crystal clear.\n- I think exploiting the dynamics of reward for better decoding is interesting and promising. The use of std as a damping factor avoids myopic emphasis on high-reward steps.\n- The experiments are comprehensive."}, "weaknesses": {"value": "- The technical depth is limited. The major contribution is two folds: (i) the empirical observation of PRM dynamics of both correct/incorrect reasoning paths; (ii) replacing the selection criterion in beam search to incoprate variability with no theoretical result, which seem to be straightfoward. Also, it is unclear to me that whether the proposed method can be generalized to other decoding algorithms like MCTS.\n- The experimental results are not strong and convincing enough. The authors only compared their methods with vanilla beam search which is obviously not the SotA methods for test-time scaling. The datasets used are also a bit old. I suggest the authors considering popular benchmarks like AIME or AMC. Finally, the improvement is marginal, which sheds some doubts on the significance of the proposed method."}, "questions": {"value": "See Weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rTqIOB60Xn", "forum": "If8O8CdCbi", "replyto": "If8O8CdCbi", "signatures": ["ICLR.cc/2026/Conference/Submission19906/Reviewer_EJpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19906/Reviewer_EJpL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986463447, "cdate": 1761986463447, "tmdate": 1762932062256, "mdate": 1762932062256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focues on guided decoding with a process reward model. Firstly, the authors observe that the reward distribution for different steps has a close correlation with correctness. I.e. a declining reward across steps and a high volatility patten of the rewards normally hint a final false prediction. Inspired by this observation, the authors propose Volatility-Scaled Guided Decoding (VSGD), which prioritizes candidate paths with lower volatility by jointly considering the magnitude of PRM-estimated rewards and the volatility of these rewards across decoding steps. \n\nExperiments on three benchmarks (GSM8K, MATH500 and MMLU) with a process reward model show that VSGD outperforms other baselines. Extensive ablation also justify the design choice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation is interesting. I.e. reward stability and volatility distribution is correlated with the prediction correctness.\n2. The proposed method is closely related to the observation, making the paper coherent.\n3. Extensive experimemts and ablation show the benefits from VSGD."}, "weaknesses": {"value": "1. Lack of process reward models and decoding models. Only one process reward model (VersaPRM) and one decoding model (Llama-8B) are verified here. It cast doubts on the generalization of the observation and results.\n2. Limited improvement. From Table 1, we can see that the improvement from VSGD is very limited compared, +0.6 or +0.7 on MATH500 and -0.2 or +0.1 on GSM8K. The most improvement comes form MMLU, with +1.1 or +1.4. This shows the limitation of the proposed method on various domains."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KpQuTfavIS", "forum": "If8O8CdCbi", "replyto": "If8O8CdCbi", "signatures": ["ICLR.cc/2026/Conference/Submission19906/Reviewer_kJFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19906/Reviewer_kJFH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997854681, "cdate": 1761997854681, "tmdate": 1762932060728, "mdate": 1762932060728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}