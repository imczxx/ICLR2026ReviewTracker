{"id": "B6t4qcdmM0", "number": 16101, "cdate": 1758260041908, "mdate": 1759897261751, "content": {"title": "CAPL: Graph Few-Shot Class-Incremental Learning Via Class-Adaptive Prototype Learning", "abstract": "Few-shot class-incremental learning has always been a challenging problem due\nto catastrophic forgetting, insufficient labels and class imbalance. Graph few-shot\nclass-incremental learning(GFSCIL), with the presence of edges between nodes\nand complex relationships between classes, further increases the difficulty of the\nlearning process. Current researches in this field mainly employ meta-learning and\nmetric-learning approaches. However, these methods do not consider the relation-\nships between classes and treat all classes equally, which does not conform to the\nreal-world applications. To address these limitations, we propose a class-adaptive\nprototype learning (CAPL) method that adaptively processes each class based on\nthe relationships between classes, thereby alleviating spatial confusion between\nnew and old classes as well as the catastrophic forgetting problem. Specifically,\nwe first adopt a class-adaptive spatial reservation module to allocate larger spaces\nfor the arrival of new classes, preventing confusion between new and old classes.\nWe then utilize a class-adaptive prototype alignment module for knowledge distil-\nlation. By considering the positional relationship between new and old classes in\nthe feature space, we provide greater flexibility to classes closely related to new\nclasses while retaining classification information of old classes, thus adapting to\nthe arrival of new classes. Experiment results demonstrate the superiority of the\nproposed method.", "tldr": "", "keywords": ["Few-shot class-incremental learning", "Graph few shot learning", "Graph class-incremental learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7851ae9fcdb8d8d33e4030495bf6d43ec1c419c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenges of Graph Few-Shot Class-Incremental Learning (GFSCIL), including catastrophic forgetting and inter-class confusion. The authors propose a Class-Adaptive Prototype Learning (CAPL) framework that adaptively adjusts class prototypes based on inter-class relationships. It consists of a spatial reservation module to allocate space for new classes and a prototype alignment module to balance knowledge retention and adaptation. Experiments show that CAPL effectively mitigates forgetting and outperforms existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a clear and effective framework for Graph Few-Shot Class-Incremental Learning (GFSCIL). The Class-Adaptive Prototype Learning (CAPL) method adaptively handles different classes, reducing confusion between new and old classes and alleviating forgetting. The design is intuitive, well-motivated, and achieves strong performance improvements over existing methods."}, "weaknesses": {"value": "The paper lacks clarity on whether base data are accessible during incremental learning. Comparisons with related GNN-based methods are missing. Graph structures are not fully utilized beyond prototype computation, and no t-SNE visualizations are provided to show feature space evolution."}, "questions": {"value": "1. In the incremental stage, is the data from the base stage still accessible? If it is available, will forgetting of the base classes still occur? In that case, does it still make sense to suppress the forgetting of base classes?\n\n2. Currently, there already exist several methods that employ Graph Neural Networks (GNNs) for Few-Shot Class-Incremental Learning (FSCIL), such as CEC [1], as well as approaches that conduct experiments on graph-structured datasets, such as SMILE [2]. Therefore, this paper should include comparative experiments with these types of methods.\n\n3. There are still some spelling errors in this paper. For example, in Figure 1, the word “Attention” in the base stage section is misspelled as “Atention.”\n\n4. In this paper, the graph properties are only utilized when computing prototypes. In the subsequent process, however, the edge-weight relationships between nodes in the graph are not exploited, and the connections among instances are not fully explored. Therefore, introducing a graph structure into FSCIL appears to be non-essential in this context.\n\n5. The proposed method mainly focuses on adjusting and optimizing the feature space. However, the experimental section does not include any t-SNE visualizations of the feature space. It is recommended to add t-SNE plots that illustrate the changes in the feature distribution and the positions of class prototypes.\n\n[1] Zhang C, Song N, Lin G, et al. Few-shot incremental learning with continually evolved classifiers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12455-12464.\n\n[2] Liu Y, Li M, Giunchiglia F, et al. Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks. In Proceedings of the ACM on Web Conference 2025. 2025: 2646-2656."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OToDWx0kZe", "forum": "B6t4qcdmM0", "replyto": "B6t4qcdmM0", "signatures": ["ICLR.cc/2026/Conference/Submission16101/Reviewer_SHiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16101/Reviewer_SHiP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470612693, "cdate": 1761470612693, "tmdate": 1762926278861, "mdate": 1762926278861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes CAPL, a class-adaptive prototype learning framework for graph few-shot class-incremental learning (GFSCIL). It tackles catastrophic forgetting, overfitting, and class confusion by introducing two key modules: Class-Adaptive Prototype Alignment (CAPA), which performs flexible knowledge distillation, and Class-Adaptive Prototype Clustering (CAPC), which reserves feature space adaptively for different classes. Experiments on Cora-Full, Amazon-Electronics, and Amazon-Clothing show that CAPL outperforms prior methods by over 3% on average accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work tries to address an interesting problem—few-shot class-incremental learning on graphs—where data scarcity and structural dependencies make standard FSCIL methods ineffective.\n2. The proposed class-adaptive framework is technically sound, and ablation studies verify the contribution of its main components."}, "weaknesses": {"value": "1. Lack of theoretical analysis or interpretability case for CAPL. While the model is decomposed into three modules (prototype learning, CAPC, CAPA), their claimed effects (e.g., “CAPC further improves intra-class compactness and reserves space for new classes”) are not convincingly supported by theoretical justification or targeted empirical analysis. The paper mainly relies on overall accuracy gains rather than directly validating the intended mechanisms.\n2. Incremental novelty. The main ideas—prototype-based classification, clustering regularization, and class-aware distillation—are largely adapted from existing FSCIL and GNN literature. The“class-adaptive” design appears incremental rather than conceptually transformative.\n3. Writing and clarity. While the paper presents valuable ideas, some sections, particularly the introduction and methods, could benefit from more concise and clearer phrasing. A more streamlined presentation and a more explicit motivation for each component would enhance the readability and overall clarity of the manuscript."}, "questions": {"value": "1. The paper lists four challenges of graph few-shot class-incremental learning, but it is unclear which specific challenge CAPL primarily aims to solve. The authors should clarify whether the method mainly targets catastrophic forgetting, feature overlap, or another issue, rather than broadly claiming to address all four.\n\n2. How exactly does the CAPC module influence the geometry of the feature space? Could the authors provide quantitative or visual evidence (e.g., t-SNE, intra-/inter-class distances) to show that it indeed improves intra-class compactness and inter-class separability?\n\n3. The definitions and roles of the base stage and incremental learning stage are not clearly explained. A clearer description of their differences, objectives, and how they interact within CAPL would help readers follow the training process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bquZtZTaLi", "forum": "B6t4qcdmM0", "replyto": "B6t4qcdmM0", "signatures": ["ICLR.cc/2026/Conference/Submission16101/Reviewer_KYGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16101/Reviewer_KYGh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701738203, "cdate": 1761701738203, "tmdate": 1762926278528, "mdate": 1762926278528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Graph Few-Shot Class-Incremental Learning (GFSCIL) by proposing a Class-Adaptive Prototype Learning (CAPL) method. The authors argue that existing approaches treat all classes equally and do not consider inter-class relationships.  The proposed method is evaluated on three graph datasets and shows improvements over baselines in terms of average accuracy and forgetting metrics."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental evaluation includes multiple datasets and detailed ablation studies that demonstrate the contribution of different components.\n2. The paper provides motivation for why class-adaptive approaches are needed for graph data, emphasizing the non-i.i.d. nature of graph nodes and complex inter-class relationships."}, "weaknesses": {"value": "1 The core ideas presented are largely combinations or incremental modifications of well-established techniques in the (FS)CIL domain. The method is built on a foundation of standard prototype learning [1] and uses prototype alignment via knowledge distillation, which is a common technique for mitigating catastrophic forgetting in continual learning [2, 3, 4]. Similarly, the \"class-adaptive\" knowledge distillation (CAPA) is a distance-based weighting scheme for a standard KD loss.\n\n2 The proposed method also appears to lack component integration, feeling more like a collection of disparate loss terms rather than a unified, principled framework. There is no clear theoretical or practical explanation of how the \"space reservation\" achieved by CAPC interacts with or benefits the \"prototype flexibility\" provided by CAPA.\n\n3 In Equation 7 ,while borrowed from clustering, its application here is not well-justified. The DB index assumes symmetric relationships between clusters, but the paper emphasizes directed, asymmetric relationships in graphs. The max operation focuses on one neighbouring class, but graphs often have multiple important neighbours.\n\n4 The product in Equation 10 can make weights extremely small when many new classes exist, potentially causing numerical instability.\n\n5 More experiments should be conducted on commonly used FSCIL datasets (e.g. CUB, Mini-ImageNet, CIFAR100) to validate the effectiveness of the method.\n\n6 No computational cost analysis is provided. Class-adaptive processing likely increases complexity significantly\n\n\n[1] Few-Shot Incremental Learning with Continually Evolved Classifiers\n\n[2] Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning\n\n[3] SSFE-Net: Self-Supervised Feature Enhancement for Ultra-Fine-Grained Few-Shot Class Incremental Learning\n\n[4] Few-Shot Class-Incremental Learning via Relation Knowledge Distillation"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cm9NrFRToT", "forum": "B6t4qcdmM0", "replyto": "B6t4qcdmM0", "signatures": ["ICLR.cc/2026/Conference/Submission16101/Reviewer_Ce4A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16101/Reviewer_Ce4A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888503482, "cdate": 1761888503482, "tmdate": 1762926277938, "mdate": 1762926277938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles graph few-shot class-incremental learning (FSCIL) problem and proposes class-adaptive prototype learning.\nThe authors propose several loss functions including $\\mathcal{L}_c$, $\\mathcal{L}_n$, $\\mathcal{L}inter$, $\\mathcal{L}db$, and $\\mathcal{L}kd$ that encourage learning representation and class-prototypes that exhibit compact intra-class clusters and discriminative inter-class prototypes.\nThe experimental results show that the proposed loss functions contribute to the performance improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The experimental results show that the proposed method outperforms the existing FSCIL methods."}, "weaknesses": {"value": "$\\textbf{1. Lack of Novelty}$\n\nThe motivation to form compact and discriminative class clusters to reserve feature space for new classes has already been addressed by many previous works in FSCIL [1,2,3,4].\nThe only difference appears to be in the loss function design, but the reviewer finds that the difference is minor and the proposed loss functions seem to be hand-crafted and less insightful.\n\n$\\textbf{2. Lack of comparison}$\n\nSome previous [5,6] works show that reducing intra-class distance and maximizing the inter-class margin is not beneficial in the context of FSCIL, because this might hurt the transferability of learned representation during the base session.\nTo make this paper more appealing, it appears to be necessary to present more discussion and analysis of the proposed method compared to the previous representation learning based FSCIL methods [1,2,3,4,5,6].\n\n$\\textbf{3. Inaccurate notations or mathematical expressions}$\n\nIn L111, The equation is incomplete: $\\Delta \\mathcal{C}_i \\cap \\Delta \\mathcal{C}_j = $.\nIn L152, the equation for $p_i$ is inaccurate: $z_i$ is not defined and the subscript should be $j$ not $i$.\nIn L156, the equation for $\\boldsymbol{Z}_i$ is confusing. What does $||$ indicate? Does it mean $\\text{Concatenate}( \\[\\boldsymbol{z}_j | \\boldsymbol{v}_j \\in S_t^j\\])$?\nIn Equation 3, $C$ should be $\\mathcal{C}$.\nThese inaccurate expressions make the paper difficult to follow and unprofessional.\n\n[1] Hersche etal, \"Constrained few-shot class-incremental learning\", in CVPR 2022\n\n[2] Song etal, \"Learning with fantasy: semantic-aware virtual contrastive constraint for few-shot class-incremental learning\" in CVPR 2023\n\n[3] Yang etal, \"Neural collapse inspired feature-classifier alignment for few-shot class incremental learning\", in ICLR 2023\n\n[4] Zhou etal, \"Forward compatible few-shot class-incremental learning\" in CVPR 2022\n\n[5] Zou etal, \"Margin-Based Few-Shot Class-Incremental Learning with Class-Level Overfitting Mitigation\", in NeurIPS 2022\n\n[6] Oh etal, \"CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning\", in ECCV 2024"}, "questions": {"value": "1. During the incremental stage, if the model parameters are updated, the prototypes for old classes might become outdated. Although the authors propose adjusting prototypes for old classes, it is not guaranteed that the adjusted prototypes effectively cover the actual feature shift for old classes.\n\n2. It seems that the proposed method can be applied to the general FSCIL problem like image classification. Since many FSCIL works evaluate their methods on image classification task, it would be better to compare the proposed method with them in the commonly used setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D7ZX9MYknl", "forum": "B6t4qcdmM0", "replyto": "B6t4qcdmM0", "signatures": ["ICLR.cc/2026/Conference/Submission16101/Reviewer_UPv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16101/Reviewer_UPv6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925027539, "cdate": 1761925027539, "tmdate": 1762926277575, "mdate": 1762926277575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}