{"id": "ahcb5auRmy", "number": 21284, "cdate": 1758315849622, "mdate": 1759896930867, "content": {"title": "Revive and Recouple: Mitigating Plasticity Loss in Transformer Architectures", "abstract": "A key trait of general intelligence is the ability to continuously adapt and learn in non-stationary environments.\nNeural networks progressively lose their ability to learn in such settings, this phenomenon is known as plasticity loss.\nExisting mitigation techniques for plasticity loss either require extensive additional memory, compute, or suffer from loss of crucial task information, causing a performance drop.\nCrucially, while this phenomenon is well-studied in traditional MLPs, there is a significant lack of insight regarding loss of plasticity in transformer architectures.\nIn this work, we find that plasticity loss also occurs in transformer architectures, both in dense layers and layer norm parameters.\nTo address this, we present a novel two-step framework Revive and Recouple (RnR) designed to mitigate plasticity loss while preserving crucial knowledge, thereby avoiding performance drops.\nOur experiments show that RnR significantly outperforms current approaches on transformer architectures in Continual Learning (CL) scenarios.", "tldr": "", "keywords": ["transformers", "plasticity loss", "continual learning", "dormant neuron"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/078753d47b1ce88e3ac5cda57a8930c22e44c1ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies loss of plasticity in transformer architectures. The results show a substantial loss of plasticity in certain Vision Transformers. The analysis shows that layer normalization also contributes to loss of plasticity in these architectures, even though LN has been shown to reduce loss of plasticity in feed-forward networks. The final step is to propose a new strategy, Revive and Recouple (RnR), that reduces disruption in the network output when a large number of units are reset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The clear demonstration that layer normalization is contributing to loss of plasticity (in Figure 3) is novel and useful. The recouple strategy mitigates a significant issue with algorithms like ReDo and Regrama, making unit-resetting algorithms more practical."}, "weaknesses": {"value": "Although the paper presents some useful ideas, it is poorly written and presented, a lot of relevant literature is missing, the results are not statistically significant, and some baselines are missing.\n\n* Presentation issues. There are several presentation issues in the paper. First, the colours in Figure 2 are all just different shades of beige/brown. It is not legible. One of the labels is \"reset linear\", but it is never explained what it refers to or how it differs from \"reset MLP\". When discussing Figure 2 in the first paragraph of section 4.1, it says \"resetting all the fully connected layers ...\" What exactly is \"resetting all the fully connected layers\"? Is this the \"reset linear\" label? If so, how is it different from \"reset MLP\"? Similar issues are present in Figure 3. What is the difference between \"reset\" and \"newly-initialized network\"? When exactly is the quantity plotted in Figure 3 measured, and for which layers of the network? Similar issues are present in the text. Desiderate 3.1 on line 159 is called \"scalability\", but what is described is \"it can be applied repeatedly without practical limitations.\" I don't think this is what scalability means; it seems a better phrase for description is \"ease of use\".\n\n* Missing prior work. The paper claims that plasticity loss in transformers is largely unexplored. This is somewhat true, as recent papers have explored plasticity loss in transformers. Specifically, Farias and Jozefiak (2024) studied plasticity loss in transformers, and Hernandez-Garcia et al. (2025) studied plasticity loss in vision transforms, very similar to the architectures presented in this paper. These recent papers should be discussed in detail in this paper.\n\n* Missing baselines: The paper claims that unit resetting algorithms can dramatically change the output of the network. That is true for algorithms like ReDo and ReGrama, but not for continual backpropagation, as it typically only resets one unit per layer at a time. However, there is no comparison presented, and this difference is not even acknowledged in the paper.\n\n* Statistical significance. The experiments only show the results for three seeds and plot the standard deviation. Standard deviation is not a measure of statistical significance. Standard error or bootstrapped confidence interval should be plotted. Additionally, all the experiments should include 30 runs for each algorithm. Hernandez-Garcia et al. (2025) showed that Vision transformers using ReDo or continual backpropagation can collapse in class-incremental problems. However, this only happens once every 20 to 30 runs. Conducting just 3, or even 10 runs, strongly reduces the chances of seeing behaviours like this. Does RnR mitigate this collapse behaviour?\n\nFarias and Jozefiak, SELF-NORMALIZED RESETS FOR PLASTICITY IN\nCONTINUAL LEARNING, ICLR 2025.\nHernandez-Garcia et al. REINITIALIZING WEIGHTS VS UNITS FOR MAINTAINING PLASTICITY IN NEURAL NETWORKS, CoLLAs 2025."}, "questions": {"value": "1. What hyperparameters were tried for Redo and SnP? Please include that detail in the appendix and perform an appropriately wide parameter sweep for all algorithms.\n2. What is the added computational cost of the recouple step? Please provide the additional wall clock time for one of the experiments.\n3. Have you tried RnR on feed-forward networks? Does it improve ReDo or ReGrama? It would be a good idea to conduct a toy experiment on a problem like random label MNIST."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fmxUcUli1z", "forum": "ahcb5auRmy", "replyto": "ahcb5auRmy", "signatures": ["ICLR.cc/2026/Conference/Submission21284/Reviewer_q5mA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21284/Reviewer_q5mA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933009101, "cdate": 1761933009101, "tmdate": 1762941669799, "mdate": 1762941669799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Revive and Recouple (RnR), a two-step framework to mitigate plasticity loss in transformer architectures. RnR first “revives” dormant neurons by resetting their weights, then “recouples” downstream layers through a closed-form correction to preserve the network’s output distribution. Experiments on ViT-Tiny and ViT-Small show RnR improves adaptability over existing reset-based methods while retaining prior knowledge."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important and under-explored issue — plasticity loss in transformer architectures.\n\n- The proposed RnR framework is a neat idea. The first step (Revive) reinitializes dormant neurons, and the second (Recouple) adjusts downstream weights in a closed-form way to preserve the model’s output distribution. \n\n- The paper is written clearly overall, and was easy to follow."}, "weaknesses": {"value": "- I think the main weakness is the narrow empirical scope. All experiments are done on ViT-Tiny and ViT-Small using CIFAR-100 and Tiny-ImageNet — both small-scale. This makes it hard to know if RnR scales to realistic transformer workloads (like ViT-Base or LLMs). The method is motivated as a general solution, but the validation is quite limited.\n\n- Missing comparisons. The authors cite recent works (such as Continual Backprop, Hare–Tortoise) but don’t actually compare against them which is relevant baselines. \n\n- Hyperparameter and robustness analysis is missing. It remains unclear how sensitive RnR is to choices like reset interval, threshold τ, or batch size. Even parameters like learning rate and weight decay can drastically affect plasticity, but there’s no mention of tuning or ablation. \n\n- The pseudo-inverse step can be expensive, especially with large representative batches. A proper runtime or memory analysis is missing.\n\n- Maybe I misunderstood soemthing here - the authors claimed that RnR preserves network outputs while restoring plasticity. However, Figure 4 and Section 4.2 show the opposite—RnR does not preserve outputs as well as Shrink & Perturb or ReDo. In practice, it trades some output stability for faster recovery and higher final performance. \n\n- The authors stated that prior methods were only studied on dense networks, but several cited works have also been evaluated on ViTs or transformer-based architectures. Additionally, testing (or at least discussing) RnR on MLPs or CNNs could help demonstrate that the approach generalizes beyond ViTs.\n\n- I also think the claim that *LayerNorm layers themselves suffer from plasticity loss* is a bit overstated. The results show reduced gradients, but that doesn’t necessarily prove causation. Prior work (e.g., Lyle et al., 2023; 2024) showed LayerNorm tends to reduce plasticity loss in dense networks, not that it eliminates it."}, "questions": {"value": "Please refer to the comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBbUfmcBE3", "forum": "ahcb5auRmy", "replyto": "ahcb5auRmy", "signatures": ["ICLR.cc/2026/Conference/Submission21284/Reviewer_Mfid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21284/Reviewer_Mfid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952955341, "cdate": 1761952955341, "tmdate": 1762941669321, "mdate": 1762941669321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles plasticity loss specifically in Transformer architectures. The authors show that Transformers suffer from plasticity loss not only in the MLP layers but also in the LayerNorm parameters. To address this issue, the authors propose Revive and Recouple (RnR), a two-step framework that restores network plasticity while preserving previously learned knowledge, thereby avoiding the performance collapse that is typical in existing approaches."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed method, RnR, outperforms existing approaches while exhibiting a smaller performance drop than full resetting.\n* The paper introduces a novel “recouple” mechanism that minimizes performance drop after reset.\n* The paper is well written and easy to follow."}, "weaknesses": {"value": "The paper lacks novelty.\n* It has been shown that methods designed to mitigate trainability loss (such as ReDo) are ineffective in mitigating generalizability loss [1]. Thus, it is not surprising that ReDo shows poor performance in the setting presented in Figure 4.\n* Previous works have already demonstrated that plasticity loss exists in Transformer architectures [1, 2, 3].\n* While the aim of this paper is to tackle the loss of plasticity, the proposed method employs existing approaches from ReDo and ReGraMa to mitigate plasticity loss.\n\nThe paper does not provide appropriate and sufficient evidence to support its claims.\n* The authors argue that the LayerNorm layer suffers from plasticity loss by showing that resetting only fully connected layers cannot match the performance of full resets. However, this indicates that the learnable parameters in LayerNorm contribute to plasticity loss, not the normalization itself. To prove that LayerNorm suffers from plasticity loss, the authors should perform the same comparison without learnable parameters in LayerNorm.\n* While the authors mention that ReGraMa shows significant performance drops, the corresponding result is not presented in Figure 4.\n* Why not simply perform additional gradient updates on the representative batch to recover knowledge? The authors should provide justification for performing iterative correction instead of directly applying additional gradient updates on the representative batch (e.g., superiority in computational cost or effectiveness). Without such comparative evaluation, it is impossible to judge the method’s effectiveness and practicality.\n* The authors present desiderata for their method, but the supporting evidence for the scalability, training efficiency, and sample efficiency of RnR is not presented in the experimental section.\n* The experimental settings are insufficient. The authors should consider more diverse settings, such as warm-start [1] or class-incremental settings [4, 5].\n\nThe results in the paper contradict previous work.\n* According to the results from [1], Shrink & Perturb mitigates plasticity loss in Transformer architectures (see Figure 6, ViT-Tiny with CIFAR-100). The authors mentioned that they used the same setting as [1], but their results contradict these findings.\n\n[1] Lee, Hojoon, et al. \"Slow and steady wins the race: Maintaining plasticity with hare and tortoise networks.\" *arXiv preprint arXiv:2406.02596* (2024).\n[2] Lyle, Clare, et al. \"Understanding plasticity in neural networks.\" *International Conference on Machine Learning*. PMLR, 2023.\n[3] Lyle, Clare, et al. \"What Can Grokking Teach Us About Learning Under Nonstationarity?.\" *arXiv preprint arXiv:2507.20057* (2025).\n[4] Lewandowski, Alex, et al. \"Learning continually by spectral regularization.\" *arXiv preprint arXiv:2406.06811* (2024).\n[5] Park, Sangyeon, et al. \"Activation by interval-wise dropout: A simple way to prevent neural networks from plasticity loss.\" *arXiv preprint arXiv:2502.01342* (2025)."}, "questions": {"value": "How is the size of the representative batch chosen? How sensitive is RnR to the size of the representative batch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kD51QGVShL", "forum": "ahcb5auRmy", "replyto": "ahcb5auRmy", "signatures": ["ICLR.cc/2026/Conference/Submission21284/Reviewer_RVo2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21284/Reviewer_RVo2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980729725, "cdate": 1761980729725, "tmdate": 1762941668910, "mdate": 1762941668910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores a reset based plasticity loss mitigation technique for Transformer based architectures. Specifically, they propose a slightly different criterion for selecting neurons to reset and they adjust the weights of the layer after the reset layer to achieve the same output distribution of activations as pre-reset. They evaluate their approach on a continual supervised learning setup where the data is broken into 10 chunks and a chunk is added (with a decreasing amount of label noise for each chunk) for each task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The idea that layer norm parameters are what are causing issues in reset based plasticity methods is interesting and novel.\n- The method suggested is intuitive and the story for it makes sense, although I think more work needs to be done for empirical justification."}, "weaknesses": {"value": "- A big part of the story of the paper is that resetting weights with current approaches creates some kind of distribution shift in the activations that needs to be corrected. I think this needs to be more thoroughly shown. First, the existence of the shift, and how severe it is. Second, an exploration of how it causes issues.\n- I think you need additional evaluations. More of the settings presented in Lee et al. (2024) such as the warm start settings, but also settings such as RL should be added.\n- I think another ablation could be added exploring whether applying RnR to the whole network is necessary, or whether just applying it to certain parts of the network is enough. A similar experiment to Figure 2.\n- I expand on this in the Questions section, but I am not convinced that all the results are correct, given there's no difference between ReDo, Base, and SnP in the main experiments."}, "questions": {"value": "- Could you clarify what the difference between MLP-QKV and Reset Linear is? Also is the difference between Reset Linear and Reset the additional reset of the Layernorm parameters?\n- When training with your method, is there any notion of convergence? Am I correct in thinking that your method will never actually converge, since you always reset some proportion of the network?\n- For resetting the linear modules, isn’t the solution to maintain the output distribution simply to zero out the weights connected to the reset neurons? If so, how is that different than ReDo?\n- In the original Lee et al. (2024) where the setting was introduced, SnP was a very strong baseline, achieving close to the best performance on the setting that you are using (over 50% accuracy at the end of continual training on CIFAR-100 with ViT-Tiny, which matches your baseline). Here, it’s a bit unclear, but it seems like SnP’s performance is essentially the same as base (40%). There doesn’t seem to be a performance drop with SnP, but there was in the Lee et al paper. In general, there doesn’t seem to be a difference at all between Base, ReDo, and SnP, which seems a bit odd. Could you explain the difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S55biU42fu", "forum": "ahcb5auRmy", "replyto": "ahcb5auRmy", "signatures": ["ICLR.cc/2026/Conference/Submission21284/Reviewer_Tsdk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21284/Reviewer_Tsdk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762382272653, "cdate": 1762382272653, "tmdate": 1762941668627, "mdate": 1762941668627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}