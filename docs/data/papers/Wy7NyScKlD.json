{"id": "Wy7NyScKlD", "number": 6392, "cdate": 1757978579487, "mdate": 1759897917451, "content": {"title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts", "abstract": "Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.", "tldr": "Retrieval-of-Thought (RoT) improves LLM reasoning efficiency by reusing prior reasoning steps as dynamic templates, cutting tokens, cost, and latency while preserving accuracy.", "keywords": ["Efficient Reasoning", "Large Reasoning Models", "Retrieval Augmented Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3507a91c17c502db2269cba768e67a452600cf5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Retrieval-of-Thought (RoT), a framework that reuses previously generated reasoning steps as composable “thought” units to guide reasoning on new problems. By replacing text generation—which is typically slow and computationally intensive—with retrieval from a vector database, RoT improves inference efficiency while maintaining reasoning quality. \n\nThe key insight is that in the chain-of-thought process, individual reasoning segments can be represented as graph-like nodes, enabling retrieval and recomposition into new reasoning traces.\n\nExperimental results demonstrate that RoT+TI substantially reduces output tokens, per-sample cost, and end-to-end latency, while incurring only minimal accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a retrieval-based chain-of-thought framework that efficiently reuses previously generated reasoning blocks, offering a novel and highly effective approach to improving inference efficiency.\n2. The work introduces a well-defined methodology for Thought Graph Construction and Retrieval, providing a conceptually elegant and technically sophisticated mechanism for organizing and reusing reasoning processes."}, "weaknesses": {"value": "1. The experimental evaluation is limited to the Qwen3 family of models (0.6B, 1.7B, 4B, 8B, and 14B), all of which are relatively small and share the same architecture. This restricts the assessment of the proposed method’s scalability and generalizability across different model families.\n2. The evaluation relies solely on the AIME and AMC datasets, which together include only about 130 problems and primarily target high-difficulty mathematical reasoning. This narrow dataset scope limits the assessment of RoT’s generalization across varying difficulty levels and task domains.\n3. The paper does not evaluate RoT’s robustness to confounding or near-duplicate problems. For example, when two questions share similar structures but differ in numerical details, it remains unclear whether RoT might retrieve a reasoning block with correct logic but incorrect values, potentially misleading the model’s final prediction."}, "questions": {"value": "1. Does RoT exhibit robustness against confounding or near-duplicate inputs? Specifically, when two problems share identical structures but differ only in numerical values, could the system retrieve reasoning blocks with correct logic but incorrect numerical content, thereby misleading the model’s final output?\n2. To what extent does the proposed Thought Graph Construction method generalize beyond mathematical reasoning tasks? Would the same approach remain effective for domains involving non-numerical or open-ended reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "orhZ806jIK", "forum": "Wy7NyScKlD", "replyto": "Wy7NyScKlD", "signatures": ["ICLR.cc/2026/Conference/Submission6392/Reviewer_VNeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6392/Reviewer_VNeJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641437144, "cdate": 1761641437144, "tmdate": 1762918677287, "mdate": 1762918677287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Retrieval-of-Thought (RoT), a framework to improve the inference efficiency of Large Reasoning Models (LRMs) by reusing prior reasoning steps. RoT organizes individual reasoning steps (“thoughts”) into a thought graph with sequential edges (preserving template flow) and semantic edges (connecting analogous steps across templates). At inference, RoT retrieves query-relevant graph nodes, uses reward-guided traversal to assemble dynamic, problem-specific templates, and integrates these templates into the prompt (via `` tags for adherence). Experiments on mathematical reasoning benchmarks (AIME 2023–2025, AMC 2023) with Qwen3 models (0.6B–14B) show RoT reduces output tokens by up to 40%, latency by 82%, and cost by 59% while maintaining or improving accuracy—outperforming baselines like CoT, RAG, and BoT. Key contributions include: (1) a dynamic template construction paradigm (vs. static templates in prior work), (2) the thought graph with reward-guided traversal, and (3) empirical validation of efficiency gains across model scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* RoT addresses a critical limitation of prior retrieval-based reasoning (e.g., BoT, RAG)—static templates—by introducing dynamic, query-adaptive template assembly via a structured thought graph. This mimics human “connecting the dots” (Gick & Holyoak, 1980) and enables flexible reuse of granular reasoning steps, a novel departure from fixed scaffolds.\n* The work is methodologically rigorous: it formalizes core components (templates, thought graph) with mathematical definitions, uses a curated template dataset (ReasonFlux) to avoid benchmark contamination, and conducts comprehensive evaluations (accuracy, tokens, latency, cost, path switching). Ablations (e.g., thought graph scalability, retrieval overhead) further validate robustness.\n* The paper is well-structured: Figure 1 clearly contrasts RoT with CoT, Algorithm 1 distills the inference workflow, and appendices (e.g., parameter selection, additional results) provide necessary details. Technical concepts (semantic edges, reward functions) are explained accessibly, balancing depth and readability.\n* RoT directly tackles the “efficiency-accuracy tradeoff” of LRMs—where longer reasoning traces improve accuracy but inflate latency/cost. By showing substantial efficiency gains (especially for small/medium models) without performance loss, it provides a scalable solution for real-world LRM deployment, where cost and latency are critical constraints."}, "weaknesses": {"value": "* The evaluation is limited to mathematical reasoning (AIME/AMC). While the authors note RoT is domain-agnostic, testing on non-mathematical reasoning tasks (e.g., logic puzzles, code debugging, scientific problem-solving) would strengthen claims of generalizability—especially to domains with less standardized reasoning patterns.\n* The reward function parameters (e.g., α=0.8 for initial node selection, τ=0.85 for semantic edges) are justified via sensitivity analysis in Appendix C, but the analysis is limited to the used dataset/model. Testing how these parameters perform across different template sizes or reasoning domains would improve confidence in their general applicability."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5X9FdsANAB", "forum": "Wy7NyScKlD", "replyto": "Wy7NyScKlD", "signatures": ["ICLR.cc/2026/Conference/Submission6392/Reviewer_ZE4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6392/Reviewer_ZE4Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862556867, "cdate": 1761862556867, "tmdate": 1762918676458, "mdate": 1762918676458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a Retrieval-of-Thought (RoT), a “thought graph” that stores prior reasoning steps (thoughts) from solved problems. When a new query arrives, RoT first retrieves relevant thoughts from this graph based on semantic similarity and metadata, and then uses reward-guided traversal to assemble these into a problem-specific reasoning template. Templates are then used to guide inference. This allows the model to reuse and recombine prior reasoning patterns dynamically—reducing redundant exploration typical in Chain-of-Thought (CoT) reasoning.\nAuthors perform experiments on math reasoning benchmarks (AIME 2023–2025, AMC 2023) and Qwen3 models (0.6B–14B), and show that RoT reduces the amount of output tokens and inference latency, while maintaining accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel dynamic retrieval-based reasoning framework that reuses fine-grained reasoning steps via a thought graph. RoT's ability to assemble context-specific templates on the fly using reward-guided traversal is novel. It brings retrieval closer to compositional memory and human-like analogical reasoning.\n2. Results show consistent improvements across performed experiments"}, "weaknesses": {"value": "1. All experiments are conducted on mathematical reasoning tasks (AIME, AMC), which have highly structured, formulaic reasoning patterns and strong inter-problem overlap. This domain bias may overstate the general efficiency of template reuse, since the retrieval of semantically similar reasoning steps is easier in math than in open-domain or multimodal reasoning.\n2. The system currently relies on manual tagging of template types and knowledge domains (e.g., algebraic, geometric) to filter candidate nodes. This introduces a degree of human supervision and limits the method’s autonomy and scalability. Especially if we concider going beyond mathematical domain."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ifFC8Hsizd", "forum": "Wy7NyScKlD", "replyto": "Wy7NyScKlD", "signatures": ["ICLR.cc/2026/Conference/Submission6392/Reviewer_UvKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6392/Reviewer_UvKG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959653281, "cdate": 1761959653281, "tmdate": 1762918675585, "mdate": 1762918675585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}