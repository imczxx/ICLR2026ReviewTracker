{"id": "4fm4Rc5fUK", "number": 18020, "cdate": 1758282961452, "mdate": 1759897138917, "content": {"title": "Autoformalizer with Tool Feedback", "abstract": "Autoformalization addresses the scarcity of data for Automated Theorem Proving (ATP) by translating mathematical problems from natural language into formal statements. \nEfforts in recent work shift from directly prompting large language models to training an end-to-end formalizer model from scratch, achieving remarkable advancements. \nHowever, existing formalizer still struggles to consistently generate valid statements that meet syntactic validity and semantic consistency. \nTo address this issue, we propose the Autoformalizer with Tool Feedback (ATF), a novel approach that incorporates syntactic and consistency information as tools into the formalization process. \nBy integrating Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge approach for consistency validation, the model is able to adaptively refine generated statements according to the tool feedback, enhancing both syntactic validity and semantic consistency. \nThe training of ATF involves a cold-start phase on synthetic tool-calling data, an expert iteration phase to improve formalization capabilities, and Direct Preference Optimization to reduce ineffective revisions. \nExperimental results show that ATF markedly outperforms a range of baseline formalizer models, with its superior performance further validated by human evaluations.\nSubsequent analysis reveals that ATF demonstrates excellent inference scaling properties.\nMoreover, we open-source Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate advancements in autoformalization and ATP research.", "tldr": "", "keywords": ["Autoformalization using large language models; Tool-integrated reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/234cefd014180e94d1db4f5c9efe3d3fb9f81251.pdf", "supplementary_material": "/attachment/416646ab21098d8d86fd6d02bff692ef87ef5441.zip"}, "replies": [{"content": {"summary": {"value": "ATF is a system that turns natural-language math problems into formal Lean 4 statements using feedback tools. It combines **syntax checks** from the Lean compiler and **semantic checks** from multiple LLM judges to iteratively refine results. Trained in three stages, ATF greatly improves both accuracy and consistency over previous models and releases a 750K-sample dataset (**Numina-ATF**) to support further research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework, ATF, is clearly structured and the experimental results are reported in a generally comprehensible way.  \n- The topic itself is timely, and the authors make an effort to connect their work to recent trends in LLM-based reasoning and formal verification."}, "weaknesses": {"value": "- I want to know what other tool calls, besides the **Syntax Check Tool**, can enhance autoformalization.\nI doubt that there are many tools capable of surpassing **Lean** in terms of checking ability, so the paper should explore **Lean’s potential as a tool** more deeply.\n\n- Lean is not good at performing numerical calculations, but I didn’t see you invoke any **calculator-related tools** in your framework.\n\n- Please provide experiments on **benchmarks that require extensive numerical computation**."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yhyrIQ93B9", "forum": "4fm4Rc5fUK", "replyto": "4fm4Rc5fUK", "signatures": ["ICLR.cc/2026/Conference/Submission18020/Reviewer_ZNEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18020/Reviewer_ZNEc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816950945, "cdate": 1761816950945, "tmdate": 1762927811852, "mdate": 1762927811852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to All Reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank all reviewers for their thorough and constructive reviews. We greatly appreciate their recognition of the significance and the novelty of this work, our contribution to the ATP community.\n\nWe have carefully considered all concerns raised and hope we have adequately addressed each reviewer's questions in the individual responses below.\n\nRegarding the common suggestion raised by all reviewers to expand our evaluation to additional benchmarks, we provide a unified response here:\n\nWe initially selected FormalMath-Lite, ProverBench, and CombiBench to follow the experimental setup in StepFun-formalizer, ensuring fair and direct comparison among the most recent fromalizers. To better support the conclusions in this paper, we have conducted additional experiments on **four widely-used automated formal proving benchmarks**: MiniF2F, ProofNet, MathOlympiadBench, and PutnamBench, keeping all experimental parameters consistent with those reported in the paper. The results are presented in the table below:\n\n| **Model** | **MiniF2F** || **ProofNet** || **MathOlympiadBench** || **PutnamBench** ||\n|----------------------------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n| | SC | CC | SC | CC | SC | CC | SC | CC |\n| _Pass@1_ |||||||||\n| Kimina-7B | 93.36| 75.37 | 52.26 | 30.70 | 26.58 | 10.47 | 73.68 | 31.72 |\n| StepFun-7B | 93.07 | 80.61 | 44.30 | 30.32 | 74.56 | 45.69 | 57.73 | 27.16 |\n| Goedel-V2-8B | 97.46 | 87.50 | 69.46 | 57.15 | 84.42 | 55.58 | 79.47 | 49.10 |\n| StepFun-32B | 96.11 | 84.55 | 54.25 | 38.71 | 79.94 | 49.86 | 62.33 | 35.73 |\n| Goedel-V2-32B | 97.17 | 89.96 | 68.01 | 58.55 | 84.89 | 58.17 | 80.39 | 53.59 |\n| ATF-32B (Ours) | 99.15 | 97.31 | 69.56 | 62.16 | 91.44 | 81.70 | 88.76 | 76.14 |\n| _Pass@8_ |||||||||\n| Kimina-7B | 99.43 | 94.39 | 78.82 | 56.72 | 62.78 | 34.61 | 94.05 | 57.92 |\n| StepFun-7B | 99.02 | 97.01 | 67.26 | 54.41 | 91.19 | 71.4 | 91.57 | 65.56 |\n| Goedel-V2-8B | 99.34 | 98.52 | 83.12 | 76.83 | 96.28 | 83.56 | 95.53 | 80.90 |\n| StepFun-32B | 99.63 | 97.99 | 74.95 | 66.02 | 92.72 | 76.33 | 82.59 | 63.48 |\n| Goedel-V2-32B | 99.59 | 99.10 | 85.54 | 77.20 | 96.36 | 86.92 | 95.39 | 86.06 |\n| ATF-32B (Ours) | 99.80 | 99.39 | 91.76 | 85.52 | 99.26 | 96.15 | 98.58 | 95.57 |\n| _Pass@16_ |||||||||\n| Kimina-7B | 99.59 | 96.72 | 82.80 | 61.83 | 70.00 | 43.06 | 96.12 | 64.60 |\n| StepFun-7B | 99.18 | 97.95 | 70.97 | 58.60 | 93.61 | 76.11 | 94.41 | 75.16 |\n| Goedel-V2-8B | 99.59 | 98.77 | 86.56 | 79.03 | 97.50 | 87.78 | 97.20 | 87.11 |\n| StepFun-32B | 100.00 | 98.77 | 78.49 | 70.43 | 94.44 | 80.56 | 85.56 | 70.19 |\n| Goedel-V2-32B | 99.59 | 99.59 | 88.17 | 80.65 | 97.22 | 91.39 | 97.05 | 70.19 |\n| ATF-32B (Ours) | 100.00 | 99.59 | 94.62 | 89.25 | 99.72 | 97.78 | 99.22 | 97.52 |\n\nThe results show that ATF-32B still achieves the best results on every dataset. Notably, on MathOlympiadBench and PutnamBench, ATF achieves significant improvements, demonstrating the effectiveness of revisions based on feedback."}}, "id": "ALYKYKghZG", "forum": "4fm4Rc5fUK", "replyto": "4fm4Rc5fUK", "signatures": ["ICLR.cc/2026/Conference/Submission18020/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18020/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18020/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763519932011, "cdate": 1763519932011, "tmdate": 1763519932011, "mdate": 1763519932011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ATF is a system that turns natural-language math problems into formal Lean 4 statements using feedback tools. It combines **syntax checks** from the Lean compiler and **semantic checks** from multiple LLM judges to iteratively refine results. Trained in three stages, ATF greatly improves both accuracy and consistency over previous models and releases a 750K-sample dataset (**Numina-ATF**) to support further research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework, ATF, is clearly structured and the experimental results are reported in a generally comprehensible way.  \n- The topic itself is timely, and the authors make an effort to connect their work to recent trends in LLM-based reasoning and formal verification."}, "weaknesses": {"value": "- I want to know what other tool calls, besides the **Syntax Check Tool**, can enhance autoformalization.\nI doubt that there are many tools capable of surpassing **Lean** in terms of checking ability, so the paper should explore **Lean’s potential as a tool** more deeply.\n\n- Lean is not good at performing numerical calculations, but I didn’t see you invoke any **calculator-related tools** in your framework.\n\n- Please provide experiments on **benchmarks that require extensive numerical computation**."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yhyrIQ93B9", "forum": "4fm4Rc5fUK", "replyto": "4fm4Rc5fUK", "signatures": ["ICLR.cc/2026/Conference/Submission18020/Reviewer_ZNEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18020/Reviewer_ZNEc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816950945, "cdate": 1761816950945, "tmdate": 1763641551268, "mdate": 1763641551268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model called ATF (Autoformalizer with Tool Feedback), which is designed to translate mathematical problems from natural language into formal statements. To this end, the authors design two types of tool feedback mechanisms: the first uses the Lean 4 compiler to check and correct the syntax of the generated formal statements, ensuring syntactic validity; the second adopts a multi-model voting approach to evaluate the semantic consistency of the generated results. When errors occur in the formalization results, the model can iteratively revise its outputs based on the tool feedback. To train ATF, the authors propose a three-stage training process: first, a “cold start” phase on synthetic data to teach the model how to use tools for correction; then an “expert iteration” phase to further improve the model’s capability through simulated expert feedback; and finally, a Direct Preference Optimization (DPO) phase to reduce ineffective modifications. In the experiments, ATF is evaluated on three mainstream benchmark datasets (FormalMath-Lite, ProverBench, and CombiBench), and the results show that ATF significantly outperforms the current best baseline, Goedel-V2-Formalizer-32B, in both syntactic validity and semantic consistency. The authors also release a synthetic dataset containing 750,000 formal statements (Numina-ATF) and conduct detailed human evaluations and ablation studies to verify the effectiveness of each component. Overall, this work demonstrates a new approach to significantly improve automatic mathematical formalization through tool feedback and provides new data resources."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- High innovation: For the first time, the paper introduces the use of Lean compiler outputs as a syntax verification tool and multi-model collective judgment as a semantic verification tool in the automatic formalization task, effectively combining the strengths of formal systems and large model reasoning.\n- Significant experimental results: The proposed method substantially surpasses the current best approach (Goedel-V2-Formalizer-32B) on multiple popular benchmarks, including FormalMath-Lite, ProverBench, and CombiBench, with particularly notable improvements in semantic consistency metrics, demonstrating the effectiveness of the approach.\n- Well-designed training process: The proposed three-stage training strategy—cold start, expert iteration, and DPO—progressively optimizes the model for different needs, enabling it to learn how to invoke tools and make reasonable corrections based on feedback, showing a thoughtful and well-structured design.\n- Resource contribution: The authors release the Numina-ATF synthetic formalization dataset with 750,000 samples, providing the community with valuable resources for training and evaluation, which holds high practical value.\n- Detailed analysis: The paper includes human evaluations and ablation studies, offering in-depth analysis of the roles of each component and the model’s scalability (such as extension effects in the inference stage), enhancing the credibility of the work. The writing is clear, and the figures are easy to read, making the contributions easy to grasp."}, "weaknesses": {"value": "- Concerns about the reliability of the multi-model consistency tool: The paper relies on multiple large language models as “judges” to determine whether the generated statements are semantically consistent with the problems. However, the judgments made by LLMs may be unstable or biased, especially when it comes to subtle logical errors. Although the authors conducted human evaluations, the error rate and potential blind spots of the consistency checking tool remain unclear. It is recommended to further quantify or add verification mechanisms to ensure the accuracy of consistency feedback.\n- Limited generalization ability and scope: The current tool feedback is based on the Lean 4 compiler. If mathematical problems need to be formalized in other languages (such as Isabelle or Coq) or in different versions of Lean, the current approach may not be directly applicable. The authors mention the differences between Lean versions, but there is insufficient study on the adaptability of the method. Future work could explore the method’s transferability across different formal systems or introduce language-agnostic tool interfaces.\n- Training and inference overhead: The three-stage training and multi-round feedback mechanisms of ATF increase computational complexity. In particular, during inference, the repeated invocation of the compiler and multi-model judgments may lead to slower inference speed and higher resource consumption. The paper does not discuss efficiency in detail. In practical applications, fast response is also important, and it would be helpful for the authors to specify the model’s inference cost and latency, as well as its performance under limited computational resources.\n- Interpretability and failure analysis: Although the paper provides overall performance improvement data, it lacks an in-depth analysis of failure cases. For example, it remains unclear what types of problems ATF still struggles to formalize, or when tool feedback fails to correct the output. A detailed analysis of failure cases would help reveal the limitations of the approach and potential directions for improvement."}, "questions": {"value": "- The paper mentions using multiple large language models (LLMs) as “judges” for semantic consistency verification. However, I only observed the use of **QWQ-32B** and **Qwen3-32B** in the main text. Could the authors clarify whether these are the only LLMs employed in the consistency check, or if other models were also used but not explicitly mentioned in the paper?\n- In Table 1, the results do not appear to show a clear advantage of the *Ensemble Vote* method compared to using a single LLM as the judge. I would recommend adding a new evaluation metric — **Accuracy** — to the table, which would provide a more intuitive comparison and make it easier to quantify the improvement brought by the ensemble voting method.\n- Since ATF requires multiple rounds of tool calls for iterative correction during inference, does this lead to a significant computational overhead? How does the actual inference speed compare to conventional one-shot formalization models? Moreover, have the authors considered strategies such as reducing the number of iterations or parallelizing the process to enhance scalability for large-scale mathematical libraries?\n- In Section 5.2 (“Tool Analysis”), the paper states: “As shown in Figure 5, the number of tool calls varies by dataset; CombiBench requires the highest average number of tool invocations (8.35) due to its combinatorial complexity, while FormalMath-Lite requires fewer attempts (3.19).”However, I was unable to locate the corresponding values (8.35 and 3.19) in Figure 5. Similarly, the sentence *“ProverBench is an exception where consistency checking (66.34%) outperforms syntax checking (61.65%)”* cites values that also do not appear in the figure. Could the authors verify whether these numbers are accurate or possibly correspond to an earlier version of the figure?\n- In Table 4 (ablation study), the improvement brought by adding the DPO training stage over the *Expert Iteration* stage alone appears rather marginal (around 1% increase). Could the authors elaborate on whether the DPO stage provides additional benefits beyond accuracy improvement, such as better stability, generalization, or robustness in handling ambiguous formalization cases?\n- The experiments report strong results on **FormalMath-Lite**, **ProverBench**, and **CombiBench**. However, other widely used benchmarks for formalization tasks include **MiniF2F** and **ProofNet**. Could the authors share any results or observations on these datasets, or discuss potential challenges in applying ATF to them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZFYXQLnzHY", "forum": "4fm4Rc5fUK", "replyto": "4fm4Rc5fUK", "signatures": ["ICLR.cc/2026/Conference/Submission18020/Reviewer_7cvV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18020/Reviewer_7cvV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897276090, "cdate": 1761897276090, "tmdate": 1762927811449, "mdate": 1762927811449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Autoformalizer with Tool Feedback (ATF), a framework that integrates Lean 4 compiler feedback and multi-LLM semantic evaluation to improve mathematical autoformalization. Through three-stage training—cold start, expert iteration, and DPO—ATF learns effective tool usage and revision strategies. Experiments on FormalMath-Lite, ProverBench, and CombiBench show significant gains over prior systems like Goedel-V2 and StepFun-Formalizer, particularly in semantic consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies two key bottlenecks in current autoformalization models—syntactic errors and semantic drift—and systematically addresses both through tool feedback.\n2. The experiments show substantial improvements in syntactic validity and semantic consistency, further validated by human evaluation, demonstrating a strong correlation with human judgment.\n3. The system is thoughtfully designed, featuring grouped execution and expert iteration mechanisms that enable efficient syntax checking and progressive tool learning.\n4. The paper is well-written and comprehensive, with detailed appendices and clear figures that effectively illustrate the model’s iterative reasoning and tool interaction process."}, "weaknesses": {"value": "1. The paper introduces a meaningful but moderately novel approach by systematizing tool feedback specifically for autoformalization\n2. Training and evaluation datasets are all derived from the Numina ecosystem; although similarity-based decontamination (cosine < 0.8) is performed, stronger guarantees against overlap would make the results more convincing. Include one external dataset (e.g., MiniF2F or PutnamBench) or a stricter decontamination threshold."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ez5oCAOxIe", "forum": "4fm4Rc5fUK", "replyto": "4fm4Rc5fUK", "signatures": ["ICLR.cc/2026/Conference/Submission18020/Reviewer_A4wN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18020/Reviewer_A4wN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926704677, "cdate": 1761926704677, "tmdate": 1762927810916, "mdate": 1762927810916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}