{"id": "VnrUsa2K6I", "number": 17344, "cdate": 1758274875580, "mdate": 1763076858456, "content": {"title": "The Devil is in the Tokens: Token-Level Structural Analysis Uncovers Hallucinations in LVLMs", "abstract": "Large vision-language models (LVLMs) achieve strong results in visual reasoning and question answering but remain vulnerable to object hallucination. To explain the causes, prior studies examine global statistics and image-level attention, overlooking the local, and token-level dynamics that underlie hallucinations. We present a fine-grained analysis of hallucination at the patch-token level across layers. Our study reveals two core findings: (i) hallucinated tokens exhibit diffuse, non-localized attention, while faithful tokens show compact attention on relevant patches; and (ii) hallucinated text tokens are not aligned with any object regions. Leveraging these insights, we introduce a lightweight, explainable hallucination detector based on patch-level statistics and hidden features, achieving up to 90% accuracy in token-level hallucination detection. These results demonstrate the value of structural, token-level analysis for understanding and mitigating hallucinations in LVLMs.", "tldr": "", "keywords": ["Large Vision Language Models", "Object Hallucination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/56cd8de3cdfac73a341ef100cfab1170dac057eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a detailed token-level analysis of hallucination behaviors in Large Vision-Language Models (LVLMs). Unlike prior work that focuses on global or image-level statistics, the authors investigate hallucination phenomena at the patch- and token-level through cross-modal attention and feature alignment analysis. The study identifies two distinctive characteristics of hallucinated tokens: diffuse and spatially scattered attention distributions, and weak semantic alignment with any visual region. To quantify these findings, the authors introduce two novel measures—Patch-wise Attention Spatial Entropy and Patch-wise Cross-Modal Feature Similarity—and integrate them into a lightweight, interpretable hallucination detector. Experimental evaluation across multiple LVLM architectures demonstrates that the proposed approach achieves up to 90% accuracy in distinguishing hallucinated from grounded tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents an interpretable analysis of hallucinations in multimodal models from the perspective of token-level representations. The authors’ introduction of spatial entropy and feature alignment as objective metrics is both intuitive and empirically validated, revealing consistent patterns between hallucinated and real tokens across architectures and layers. The proposed detector is computationally lightweight, explainable, and demonstrably effective, achieving state-of-the-art accuracy without relying on external supervision or large-scale retraining."}, "weaknesses": {"value": "1. Does the training of a classifier heavily depend on the data distribution of the training set? If the training data differ significantly from the real-world data, will it cause the model to fail?\n\n2. Although lightweight in computation compared to full model retraining, the approach still requires fine-grained token-level computation across layers and modes, which could be computationally intensive for large-scale deployment or streaming applications.\n\n3. Lack of comparison with other state-of-the-art methods on public datasets, such as POPE[1] and MME[2].\n\n4. Both this method and PATCH[3] adopt a visual patch. Please explain the main differences and the distinct starting points between the method proposed in this paper and that method.\n\n[1] Evaluating Object Hallucination in Large Vision-Language Models\n\n[2] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n\n[3] From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models"}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RrMa0K0oL5", "forum": "VnrUsa2K6I", "replyto": "VnrUsa2K6I", "signatures": ["ICLR.cc/2026/Conference/Submission17344/Reviewer_36ng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17344/Reviewer_36ng"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818014482, "cdate": 1761818014482, "tmdate": 1762927266700, "mdate": 1762927266700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We appreciate the reviewers' and ACs' efforts in reviewing our work. We will incorporate the feedback and revise the manuscript to clarify and better highlight our technical novelty.\n\nBest regards, \nOn behalf of the authors"}}, "id": "sfnveDdFrZ", "forum": "VnrUsa2K6I", "replyto": "VnrUsa2K6I", "signatures": ["ICLR.cc/2026/Conference/Submission17344/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17344/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763076857700, "cdate": 1763076857700, "tmdate": 1763076857700, "mdate": 1763076857700, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper under review proposes two metrics for token wise hallucination detection for vision language models (VLM). First, the authors propose to exploit spatially distributed cross-attention of visual token patches and measure its entropy (after discarding some classes of lesser entropy) to obtain a heatmap. The authors observe that for non-hallucinated object tokens this entropy is focused in the visual region occupied by the object corresponding to the token, resulting in low entropy, while for hallucinate tokens the attention is diffuse and thus entropy is high. The second metric the authors design is a feature similarity of the language features with visual features over tokens. Here the authors extract k tokens with the highest (similarity?) score and average over these to obtain a second metric. Both metrics are tested for their capability to separate correct from hallucinated object tokens and achieve AUROC values around 80%. The tests are conducted on a self curated data set from 4000 COCO images  with ChatGPT 4.0 as arbitrary for correct vs hallucinated. Both metrics are thereafter combined into a single score using Random Forests, a FCNN or an XGBoost trained on sample data. The authors conduct comparisons over various VLM and compare themselves to recent hallucination detection baselines, finding considerable improvement over the competing models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper deals with an important and timely topic\n+ The proposed metrics seem to be new in the given context and show promising performance\n+ Comparison with baselines shows considerable improvement over recent hallucination detection methods\n+ The attention-entropy method results in images that can to some degree be interpreted by humans\n+ The figures are instructive and the paper is well written and easy to follow"}, "weaknesses": {"value": "- The main weakness I see is the evaluation on a self-curated data set consisting out of 4000 images from COCO and ChatGPT as arbitrary. It has not become clear to me, why standard metrics like CHAIR are not evaluated. This  would help the comparison with existing methods a lot. Like this, it is even not described in detail how this subset of COCO is curated.\n- In the same direction, only a single benchmark is evaluated. Others, like POPE, are just left our even though this in the meantime has become a standard benchmark to evaluate.\n- The spatial attention entropy metric is not completely new, the authors might like to cite [1] for usage in a different, but related context.\n- Details on the training of the XGBoost, RF and FCNN are missing - e.g. what was the data split? Was it separate from the test data? \n- Sometimes, the paper has a tendency toward 'colorful language' where the precise scientific content is difficult to grasp, starting with \"The devil wears tokens\", \"interactions between patch tokens\", \"fine grained statistics on the patch level\", \"structural dynamics within the model\" etc. \n- Ablation studies, except for the different classifiers, are largely missing, e.g. with respect to the cut off parameter in the attention masks or the patch features. \n- A study on the obtainable effects from hallucination detection, e.g. via nucleus sampling or contrastive decoding is missing. \n\nMy main concern is the insufficient evaluation. If this can be improved and the findings are confirmed throughout, I would find this an interesting paper.\n\n[1] Krzysztof Lis, Matthias Rottmann, Annika Mütze, Sina Honari, Pascal Fua, Mathieu Salzmann, AttEntropy: [....], BMVC 2024"}, "questions": {"value": "* I expect more diffuse attention entropy maps for larger objects - has a study of the efficiency of the attention entropy metric as depending on the object size, e.g. available via the COCO segmentation masks, been conducted?\n* Is your data curation over COCO randomized or have other criteria played a role. Does your subset e.g. lean towards smaller objects?\n* Which are the data sets and the parameter settings for the classifier models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sCVKzbS2sc", "forum": "VnrUsa2K6I", "replyto": "VnrUsa2K6I", "signatures": ["ICLR.cc/2026/Conference/Submission17344/Reviewer_xSqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17344/Reviewer_xSqC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839925575, "cdate": 1761839925575, "tmdate": 1762927265756, "mdate": 1762927265756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the object hallucination issue in Large Vision-Language Models (LVLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Innovative Fine-Grained Analysis Perspective: The paper breaks away from the limitations of traditional global-level hallucination analysis and pioneers a patch-token level investigation."}, "weaknesses": {"value": "1. Limited Generalization to Non-Object Hallucinations: The study exclusively focuses on object hallucination in LVLMs, ignoring other types of hallucinations such as attribute hallucinations or relational hallucinations. This limits the general applicability of the proposed method to broader hallucination scenarios in LVLMs.\n\n2. Potential Sensitivity to Hyperparameters: The detector relies on several hyperparameters, such as the top-x% of patch responses (x=10) for attention entropy calculation and the number of top-k layers (k=10) for feature aggregation. The paper only mentions that these hyperparameters are empirically determined but lacks a systematic analysis of their impact on detection performance. This makes it unclear how the detector would perform when hyperparameters are adjusted or applied to different LVLMs with varying architectures.\n\n3. Lack of Exploration into Hallucination Mitigation: While the paper effectively detects hallucinations, it does not extend its insights to hallucination mitigation. \n\n4. Dependence on GPT-4o for Dataset Labeling: The classification of true and hallucinated object tokens in the dataset relies on GPT-4o, which may introduce annotation biases. The paper does not validate the accuracy of GPT-4o's labels (e.g., through human annotation verification) or discuss potential errors in label assignment, which could affect the reliability of the experimental results and the training of the hallucination detector.\n\n5. Lack of some Related Works: How does this paper differ from DHCP, and how do their performances compare? They all utilize the attention of LVLMs for hallucination detection.\n\nDHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models, uploaded to Arxiv in 2024."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SrKTOWqbOM", "forum": "VnrUsa2K6I", "replyto": "VnrUsa2K6I", "signatures": ["ICLR.cc/2026/Conference/Submission17344/Reviewer_T71p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17344/Reviewer_T71p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949085285, "cdate": 1761949085285, "tmdate": 1762927265299, "mdate": 1762927265299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates object hallucination in large vision–language models at the token level and argues that hallucinated object words exhibit two consistent structural patterns: their visual attention is spatially diffuse with higher entropy, and their cross-modal similarity to any image patch is weak. Building on these observations, the authors construct a lightweight detector that aggregates layerwise attention-entropy and patch-similarity features and show that it outperforms prior token-level hallucination detectors (e.g., MetaToken, HalLoc, SVAR) across several LVLM architectures, suggesting that these two signals capture model-agnostic properties of ungrounded generations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes the empirical observation that real-object tokens localize attention in a few contiguous patch components, whereas hallucinated tokens spread attention and often hit attention sinks.\n2. Results on LLaVA, InternVL, MiniGPT-4, and GPT-4V suggest the patterns are not tied to a single architecture"}, "weaknesses": {"value": "1. The paper proposes a hallucination detector, yet all quantitative comparisons are conducted on the authors’ own COCO–GPT-4o curated token-level dataset. This makes it difficult to assess how much of the reported gain over other methods would persist on established hallucination benchmarks (e.g. CHAIR, POPE, ROPE) or on publicly available multi-image/object settings.\n2. The analysis and experiments are conducted around an image-description style interaction, and the paper does not show whether the proposed token-level signals remain equally discriminative in more diverse settings.\n3. Although the paper describes the detector as simple and lightweight, the actual feature pipeline requires extracting attention maps and visual patch embeddings from all transformer layers, running per-layer component filtering and attention-sink suppression, and computing patch-level similarities. This is considerably more expensive than baselines such as MetaToken or SVAR, and may limit applicability in online or long-form generation settings. A complexity / runtime report would help quantify this overhead."}, "questions": {"value": "1. Can you report results when the detector is plugged into existing hallucination benchmark pipelines so we can see whether the relative gains over MetaToken / HalLoc / SVAR still hold in a standard setting?\n2. How sensitive is the detector to decoding changes? Since these will change token trajectories, it would be useful to know whether your features remain stable.\n3. Is there a way to utilize your analysis into mitigation methods that could reduce hallucination on standard benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WgegAcGtYb", "forum": "VnrUsa2K6I", "replyto": "VnrUsa2K6I", "signatures": ["ICLR.cc/2026/Conference/Submission17344/Reviewer_nPEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17344/Reviewer_nPEp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977884317, "cdate": 1761977884317, "tmdate": 1762927264884, "mdate": 1762927264884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}