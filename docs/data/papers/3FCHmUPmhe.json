{"id": "3FCHmUPmhe", "number": 2675, "cdate": 1757192850395, "mdate": 1759898133770, "content": {"title": "Frozen Priors, Fluid Forecasts: Prequential Uncertainty for Low-Data Deployment with Pretrained Generative Models", "abstract": "Deploying ML systems with only a few real samples makes operational metrics (such as alert rates or mean scores) highly unstable. Existing uncertainty quantification (UQ) methods fail here: frequentist intervals ignore the deployed predictive rule, Bayesian posteriors assume continual refitting, and conformal methods offer per-example rather than long-run guarantees. We introduce a forecast-first UQ framework that blends the empirical distribution with a frozen pretrained generator using a unique Dirichlet schedule, ensuring time-consistent forecasts. Uncertainty is quantified via martingale posteriors: a lightweight, likelihood-free resampling method that simulates future forecasts under the deployed rule, yielding sharp, well-calibrated intervals for both current and long-run metrics without retraining or density evaluation. A single hyperparameter, set by a small-$n$ minimax criterion, balances sampling variance and model--data mismatch; for bounded scores, we provide finite-time drift guarantees. We also show how this framework informs optimal retraining decisions. Applicable off-the-shelf to frozen generators (flows, diffusion, autoregressive models, GANs) and linear metrics (means, tails, NLL), it outperforms bootstrap baselines across vision and language benchmarks (WikiText-2, CIFAR-10, and SVHN datasets); e.g., it achieves $\\sim$90\\% coverage on GPT-2 with 20 samples vs.\\ 37\\% for bootstrap. Importantly, our uncertainty estimates are operational under the deployed forecasting rule agnostic of the population parameters, affording practicable estimators for deployment in real world settings.", "tldr": "Provably coherent prequential UQ for frozen generative models: a decaying-weight blend with a GPU-parallel martingale posterior that yields calibrated, small-n intervals for long-run operational rates.", "keywords": ["predictive uncertainty quantification", "prequential inference", "measure-valued martingales", "frozen generative models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71078ce729f6cb1a5885e09fa97b08dfa6d49f48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new forecasting method that quantifies prequential uncertainty via Martingle posteriors with a pretrained generator and a Dirichlet schedule. The proposed method requires a small-$n$ minimax criterion hyperparameter to balance sampling variance and model-data mismatch. Experimental results show the proposed method outperforms bootstrap baselines in vision and language datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is generally well-written and is clear to understand the important aspects.\n- The Prequential Blend method is solid with theoretical guarantees about how to adjust the blend and predict horizon.\n- The proposed framework is also useful in a generative model use case where we need to decide when to retrain the pre-trained model."}, "weaknesses": {"value": "- Sampling the martingale posteriors requires multiple forward passes, raising a concern about the computational efficiency of the algorithm in long-horizon.\n- The proposed method requires predefining a hyperparameter $n$, which may not be trivial to select in practice.\n- Experimental results may need to be improved with a larger scale setting, such as with modern architectures and a higher-dimensional dataset (e.g., ImageNet, etc.)."}, "questions": {"value": "1. How computationally efficient of the DWS compared to other baselines? In the case of non-GPU-parallel, do the authors have an idea to improve the sampling efficiency?\n2. In Theorem 1, the generative model $Q_\\phi$ is $\\mathcal{F}_n$-measureable. How does this assumption hold in practice with modern models (e.g., Diffusion, GAN, VAE)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hN13jbVRzc", "forum": "3FCHmUPmhe", "replyto": "3FCHmUPmhe", "signatures": ["ICLR.cc/2026/Conference/Submission2675/Reviewer_iCc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2675/Reviewer_iCc2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851823005, "cdate": 1761851823005, "tmdate": 1762916328820, "mdate": 1762916328820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"Prequential Blend,\" a forecast-first framework for quantifying uncertainty in machine learning models when only a small amount of real-world data is available. It is validated under language (GPT2 on WikiText), vision (CIFAR10) and toy scenarios, demonstrating significantly better performance than traditional bootstrap methods in few-sample settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and nicely presented. \n- The proofs and implementation details are provided in detail."}, "weaknesses": {"value": "Please refer to questions"}, "questions": {"value": "- Are the settings in the experiments practical usage and worth considering? Take the language modelling case for example, the proposed method is advantageous when n < 100. This seems like an insignificant number considering today's LLMs' huge throughput.\n- Could you please clarify the definition of the \"coverage@90%\" metric used in the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "nUJMY38xgM", "forum": "3FCHmUPmhe", "replyto": "3FCHmUPmhe", "signatures": ["ICLR.cc/2026/Conference/Submission2675/Reviewer_CDPE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2675/Reviewer_CDPE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870880569, "cdate": 1761870880569, "tmdate": 1762916328613, "mdate": 1762916328613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has introduced an uncertainty quantification framework that \"blends\" the empirical distribution with a frozen distribution (e.g. a distribution from a pre-trained generator), using a Dirichlet schedule. The main algorithm, referred to as \"Martingale posterior sampling\", is illustrated in Algorithm 1 in Section 5. Rigorous theoretical motivation and justification for this algorithm have been provided in Section 2, 3, and 4. In Section 6 and 7, this paper also discusses when to stop resampling in the algorithm and when to retrain the frozen generator. Preliminary experiment results are demonstrated in Section 8."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper seems to provide an alternative uncertainty quantification approach different from existing frequentist, Bayesian, and conformal approaches. I am not familiar with the existing literature on this approach (e.g. Fong et al. 2023), so it is a little bit hard for me to judge the novelty of this paper. However, to the best of my knowledge, the proposed approach seems novel for the machine learning/AI community.\n\n- The mathematical framework, derivation, and analysis in this paper seem to be rigorous.\n\n- The proposed algorithm (Algorithm 1), is simple and elegant."}, "weaknesses": {"value": "- I very much like the mathematical rigor of this paper. However, I do feel that some sections of this paper are a little bit hard to read for machine learning/AI practitioners who are less mathematical. Thus, I recommend the authors to further polish Section 2-7 to make them easier to follow while keeping the mathematical rigor. One possible approach is to add more plain English explanations of the core ideas.\n\n- In the experiment section, the only metric this paper has considered is the predictive coverage. Can we demonstrate experiment results under some other metrics? For instance, in simple Bayesian settings where the posterior can be exactly computed, might the authors also demonstrate the KL-divergence (or other probability space metric) between the predictive distribution under each method (i.e. NPB, BB, JK, DWS, and MP) and the predictive distribution under the true posterior? I think such results will further strengthen the paper.\n\n- Section 7 (decide when to retrain) is very interesting. Might the authors also provide some experiment results (even preliminary ones) to support the theory in this section?\n\n- [Minor] in Theorem 2, where is $R(\\lambda, F^*)$ rigorously defined?"}, "questions": {"value": "- Please better explain the novelty of this paper relative to recent literature, especially [Fong et al. 2023]\n\n- Please try to address the weaknesses listed above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u0vfdyYz4M", "forum": "3FCHmUPmhe", "replyto": "3FCHmUPmhe", "signatures": ["ICLR.cc/2026/Conference/Submission2675/Reviewer_UbgZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2675/Reviewer_UbgZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885629144, "cdate": 1761885629144, "tmdate": 1762916328326, "mdate": 1762916328326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of uncertainty quantification (UQ) for long-run operational metrics, e.g., mean scores, alert rates, when a system is deployed with very few initial samples (n). The authors argue that standard UQ methods fail in this \"low-data, frozen-model\" setting: bootstrap is unstable, Bayesian posteriors assume refitting, and conformal prediction targets per-example guarantees, not long-run rates.\n\n\nThe authors propose a forecast-first UQ framework with three core contributions:\n\n(1) a prequential blend: a forecasting rule, $P_i$, that blends the sparse empirical data ($\\hat{F}\\_i$) with a frozen pretrained generator ($Q_{\\phi}$). The paper proves that a specific Dirichlet schedule ($\\lambda_i = \\alpha / (i + \\alpha)$) is the only affine blend that ensures time-consistent (martingale) forecasts.\n\n(2) a minimax hyperparameter: a data-driven method to set the blend strength $\\alpha$. It is derived from a minimax criterion (Theorem 2) that optimally balances sampling variance (trusting the noisy data) versus model-data mismatch (trusting the potentially biased model).\n\n(3) a novel UQ method: uncertainty is quantified via Martingale Posteriors (MP), a likelihood-free resampling method (Algorithm 1) that simulates the long-run value of the metric ($\\theta_{\\infty}$) under the deployed blend rule.\n\n\nThe authors validated the framwework on a range of experiments on language (GPT-2 on WikiText-2) and vision benchmarks (image generation on CIFAR-10 and SVHN datasets) Experiments on GPT-2 (for NLL) and OOD vision tasks (for CLIP rarity) show the method achieves its target 90% coverage, with as little as n=20 samples for GPT-2, where other standard methods such as bootstrap fails (37% coverage)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Significance**\n- this work addresses the critical and practical problem of UQ for long-run metrics in the \"low-data\" initial deployment phase\n\n**Originality**\n- the prequential (forecast-first) framing seems to me a novel way to approach UQ for frozen models.\n- to me the paper provides a few strong contributions: proving the Dirichlet blend is uniquely coherent (Theorem 1), deriving the minimax criterion for $\\alpha$ (Theorem 2) towards setting the blend strength, applying the martingale posterior in a generative context.\n\n**Clarity**\n- I find that the motivation of the paper is clear, arguing why standard UQ methods (bayesian, boostratp, conformal) are not suitable for this problem of a frozen model and long-run metric.\n\n\n**Quality**\n- The experimental results show that the proposed approach does well on language and vision tasks in the low-sample regime in comparison with multiple standard baselines.\n- I find the bits useful the bit on deciding when to stop resampling and on when to decide to retrain. They are practical."}, "weaknesses": {"value": "**Stability of hyperparameter estimation**\n- for choosing $\\alpha$ the $\\sigma^{2}$ and $\\Delta$ are estimated from the few available real samples. \n- I'm wondering how stable would these be for n=5 or n=10 and how doest this impact behavior and performance of the framework given that $\\alpha$ might be derived from unstable statistics? How does the method remain so well calibrated?\n\n**OOD experiments**\n- the considered experiment using CIFAR-10 as in-distribution and SVHN as out-of-distribution is interesting but quite extreme as the two distributions are far away in terms of semantics (very different classes) and overall content (objects vs. street numbers). This makes it easier to distinguish the two distributions are they come from different domains.\n- it would be interesting to see if such a model could capture semantic drifts as well. For instance the generator can be trained on classes 0-4 from CIFAR-10 and the remaining classes 5-9 could be the out-of-distribution ones.\n\n**The UQ target (surrogate vs. true)**\n- here the UQ is for the surrogate metric $\\theta_{\\infty}$ (the long-run value of the blend), not the true population metric $\\theta(F^{*})$ \n- if the frozen model $Q_{\\phi}$ is bad, the surrogate $\\theta_{\\infty}$ could be far from the truth $\\theta(F^{*})$. While  the UQ is \"operationally\" correct, it could potentially be \"factually\" wrong, which could be misleading.\n\n\n\n**Missing implementation details**\n- there are no details about the generator used for the vision tasks. Is it an autoencoder, VAE, GAN?\n- the considered CLIP rarity score is not described or referenced. Presumably it's [b]\n\n\n**[Minor] Writing and format**\n- I find that the paper is nicely written, yet in a very compact manner. Even like this, the experiments and related work are in the appendix.\n- A longer form version of this in a journal format might be easier to adopt to given the content of the work\n\n\n**References:**\n\n[a] Ahmed & Courville, Detecting semantic anomalies, AAAI 2020\n\n[b] Han et al., Rarity Score: A New Metric to Evaluate the Uncommonness of Synthesized Images, ICLR 2025"}, "questions": {"value": "This paper takes an interesting direction of study: the problem of UQ for long-run operational metrics when a system is deployed with very few initial samples. I find that the paper is strong and theoretically dense.\nThe reported results in with few samples are impressive. My main questions concern the stability of the estimators and the experiments on the vision domain.\n\nHere are a few questions and suggestions that could be potentially addressed in the rebuttal or in future versions of this work (please note that suggested experiments are not necessarily expected to be conducted for the rebuttal):\n\n1. The key hyperparameter $\\hat{\\alpha}$ is calculated from statistics computed from a tiny number of samples (e.g., 5, 10, 20, ...) with potential high-variance. How does the method remain so well-calibrated (Fig 1a) when its core parameter is based on such an unstable estimate?\n\n2. Please add additional implementation details on the vision side. What type of generator was used? How does the performance of DWS vary depending on the type of generator.\n\n3. Extension of vision OOD experiments towards more semantic drifts (e.g., use clases 0-4 as in-distribution and classes 5-9 as OOD) than domain drifts.\n\n\n4. Regarding the surrogate target, if the frozen model $Q_{\\phi}$ is bad, the surrogate $\\theta_{\\infty}$ could be far from the truth $\\theta(F^{*})$. What can be done in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nUgnKZ8zrT", "forum": "3FCHmUPmhe", "replyto": "3FCHmUPmhe", "signatures": ["ICLR.cc/2026/Conference/Submission2675/Reviewer_GPLc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2675/Reviewer_GPLc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990159604, "cdate": 1761990159604, "tmdate": 1762916327943, "mdate": 1762916327943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}