{"id": "l852J55j9n", "number": 22637, "cdate": 1758333813476, "mdate": 1759896855629, "content": {"title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "abstract": "Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks.\n\nTo address these gaps, we introduce **SciReas**, a diverse suite of existing benchmarks for scientific reasoning tasks, and **SciReas-Pro**, a selective subset that requires more complex reasoning.\nOur holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone.\nWe then propose **KRUX**, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks.\n\nCombining the two, we conduct an in-depth analysis that yields several key findings: \n\n(1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning;\n\n(2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement;\n\n(3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge.", "tldr": "We conduct a systematic study to examine reasoning and knowledge synergy in scientific problem-solving. We show that reasoning LLMs can be bottlenecked by domain knowledge, while reasoning-fine-tuning can help models surface relevant knowledge.", "keywords": ["reasoning", "knowledge tracing/discovering/inducing", "applications"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d46baa7b36577dabb0f1b4c36b9bd128ebb6e9e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SCIREAS, a unified evaluation suite that integrates 10 existing scientific benchmarks, spanning physics, chemistry, biology, medicine, materials, mathematics, computer science, and engineering. It supports diverse question types including multiple-choice, fill-in-the-blank, structured, and protocol/procedural tasks. Additionally, the authors construct SCIREAS-PRO, a compact subset designed to more precisely assess complex reasoning capabilities. To disentangle the roles of knowledge and reasoning, the authors propose the KRUX probing framework: “knowledge ingredients” (KIs) are extracted from the reasoning traces of one model (the knowledge source) and injected as context into another model (the target model), thereby controlling knowledge input and isolating reasoning ability. Using this framework, the authors conduct three core analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work is the first to construct a unified, standardized, and reasoning-intensive evaluation suite for scientific reasoning, achieving high-quality filtering via a subtask-level exclusion protocol.\n2. The KRUX framework is innovative: by fixing knowledge input (KIs) and varying the target model, it achieves controlled separation of knowledge and reasoning, avoiding spurious correlations. The KIs are extracted from real reasoning traces, ensuring ecological validity.\n3. The experiments are thorough and well-executed, providing vLLM + mainstream API batch inference scripts for scalability and reproducibility."}, "weaknesses": {"value": "The design of the KRUX framework and the phrasing of its experimental conclusions are somewhat ambiguous. The claim that “even when the KIs are already known by the base model, reasoning fine-tuning still improves performance” (lines 93–97) seems to lack supporting evidence of zero-shot recall of those KIs by the base model."}, "questions": {"value": "1. Regarding lines 88–97:\n\n- What is meant by “suggesting that internalizing and retrieving the right knowledge”—how is this defined operationally?\n- How is “even when the KIs are already known by the base model” proven or verified?\n- What is the precise definition of knowledge recall in this context?\n\n\n2. KIs are extracted by DeepSeek-R1 from reasoning traces—how is the quality of extraction ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TLTK5HHUSd", "forum": "l852J55j9n", "replyto": "l852J55j9n", "signatures": ["ICLR.cc/2026/Conference/Submission22637/Reviewer_euDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22637/Reviewer_euDn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745678568, "cdate": 1761745678568, "tmdate": 1762942314351, "mdate": 1762942314351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a central ambiguity in scientific problem solving with LLMs: when a model fails on a science task, is it because it doesn’t know the requisite facts or because it can’t reason with them? The paper introduces SCIREAS, a unified suite of ten public, science-focused benchmarks wrapped in a standardized harness for reproducible evaluation. And SCIREAS-PRO, a reasoning-intensive subset, and KRUX, a probing framework that injects compact, answer-agnostic Knowledge Ingredients — atomic facts extracted from another model’s CoT—to hold knowledge constant while measuring how target models use it.\n\nThis paper found 1) base models augmented with high-quality in-context KIs often exceed the performance of their reasoning counterparts on science tasks; 2) Reasoning models still benefit from external knowledge beyond their reasoning gains; 3) KIs extracted from a math-reasoning-tuned model help the base model more than KIs from the base model itself, even when both “know” the facts—implicating better recall/selection, not just more facts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper extracts knowledge from the reasoning track of a reasoning model and performs fine-grained processing. This approach is simple yet effective.\n\n2. The significance is that when external knowledge is supplied (KIs), even non-reasoning models exhibit a remarkably large improvement (≥10%). Nevertheless, reasoning-enhanced models still outperform them when both are given the same KIs, indicating additive and complementary benefits. This highlights an important insight for system design — combining retrieval/memory with reasoning leads to stronger overall performance.\n\n3. The writing is good and easy to follow."}, "weaknesses": {"value": "1. The selection process of SCIREAS-PRO relies on proprietary models and may implicitly bundle other behaviors. How capable are open-source models in this aspect? Is this filtering method model-idiosyncratic?\n\n2. How consistent are the KIs extracted across different models, and how diverse are they?"}, "questions": {"value": "1. When extracting KIs, it is important to ensure that performance does not change significantly (lines 359–360). Why, then, do both the base model and the reasoning model show substantial improvements when KIs are added during the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8bC9v2TyNE", "forum": "l852J55j9n", "replyto": "l852J55j9n", "signatures": ["ICLR.cc/2026/Conference/Submission22637/Reviewer_sPGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22637/Reviewer_sPGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876352173, "cdate": 1761876352173, "tmdate": 1762942314133, "mdate": 1762942314133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This works introduces a evaluation framework in distinguishing the contribution from knowledge recall and reasoning in several domains of reasoning problems. The authors select a SCIREA dataset for this task and a SCIREA-Pro subset which can be solved with models but only with high reasoning efforts. The results show that the performance gap is amplified with such model selection. The authors further trace back the reasoning improvement of reasoning. The authors finetuned models with different sources (Math only, STEM only and both), and employing a model-based knowledge extraction module to observe whether direct knowledge integration is beneficial for reasoning. The results show that knowledge extracted from the original model only bring marginal impact on the reasoning performance, while knowledge extracted from much stronger model brings significant reasoning gains (even through in-context learning). The same findings can extend to reasoning-based models, which demonstrates the critical role of verbalizing knowledge in reasoning problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The study of knowledge recall vs other reasoning effects is an interesting direction that reveals the direction for building models with more reliable reasoning ability.\n- The result shows the effectiveness of directly reinforcing the knowledge ingredient through verbalization."}, "weaknesses": {"value": "- The separation of datasets is unclear, the stem split also contains math problems, which blurs the improvements from models.\n- The knowledge ingredients introduced from DeepSeek-R1 serves as a distillation-like role.  Although there are supporting experiments that shows directly applying those ingredients do not lead reasoning gains. More evidence would be helpful to show how these information are utilized and take into effect.\n- Although models tuned with math data demonstrates performance gains on benchmark of more diverse subjects, such benchmark also includes math problems, which would undermine the claims made in the result."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j820yKn7oP", "forum": "l852J55j9n", "replyto": "l852J55j9n", "signatures": ["ICLR.cc/2026/Conference/Submission22637/Reviewer_gwoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22637/Reviewer_gwoU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969877809, "cdate": 1761969877809, "tmdate": 1762942313852, "mdate": 1762942313852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to study the distinct roles of domain knowledge and reasoning ability in LLMs when solving scientific problems. To facilitate this, the authors introduce SCIREAS, a comprehensive benchmark suite created by unifying and filtering ten existing scientific reasoning datasets, and SCIREAS-PRO, a more challenging subset focused on complex reasoning. Next, this paper proposes KRUX, a novel analytical framework that disentangles knowledge from reasoning by extracting atomic \"knowledge ingredients\" (KIs) from one model's reasoning trace and providing them in-context to another. The experiments lead to three key findings: (1) knowledge retrieval from model parameters is a major bottleneck for scientific reasoning; (2) providing external knowledge consistently improves the performance of reasoning-enhanced models; and (3) fine-tuning on chain-of-thought (CoT) data improves a model's ability to surface relevant knowledge it already possesses."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Quality: The experimental design in Section 4 is rigorous. The use of controlled supervised fine-tuning (SFT) on Math, STEM, and BOTH data subsets allows the authors to make more robust claims about the impact of different training data. It also reports the performance of two different model families (Qwen and Llama). The analysis in RQ3 also reliably distinguish between acquiring new knowledge versus better surfacing of existing knowledge (Tables 4 & 5). \n\n- Originality: While the goal of disentangling knowledge and reasoning is not new, the proposed KRUX framework offers a novel and pragmatic methodology. The specific technique of extracting \"knowledge ingredients\" from one model's reasoning trace to probe another is an original and effective way to create a controlled experimental setup.\n\n- Significance: The proposed SCIREAS evaluation suite offers a holistic benchmark for scientific reasoning and standardizes the evaluation across diverse datasets. The KRUX framework provides an effective tool for researchers to probe and understand the interplay between parametric knowledge and reasoning. The findings, particularly that knowledge retrieval is a key bottleneck, have direct implications for future research in scientific reasoning. \n\n- Clarity: This paper is overall well-written and easy to follow. The core idea of the proposed KRUX framework is well illustrated in Figure 1. The motivations of the proposed SCIREAS benchmark and KRUX framework are clear. The conclusion of the three research questions studied in this paper is also clearly written."}, "weaknesses": {"value": "- The definition of \"Knowledge Ingredients\" (KIs) is operational and lack of formal definition. While the paper describes KIs as \"essential atomic knowledge units\" (line 346), this definition remains vague. In practice, a KI is simply the output of an extractor model (DeepSeek-R1) given a specific prompt (Figure 12). This makes the central concept of the KRUX framework dependent on the specific extractor model and prompt used, which could affect reproducibility and the generalizability of the findings. The authors should provide a more formal or at least a more deeply reasoned definition of what constitutes a \"knowledge ingredient, as it is an essential component of all the empirical analysis of in the proposed KRUX evaluation framework. \n\n- RQ2, ”DO REASONING-ENHANCED MODELS BENEFIT FROM EXTERNAL KNOWLEDGE?”, seems to address a question with an obvious answer. It is quite obvious that feeding external knowledge to reasoning-enhanced model can boost their performance, it is better to phrase it into “To what extend does reasoning-enhanced model benefit from external knowledge”, It will make the research question more interesting, and the experiment in Table 3 does provide data for these more interesting research question.\n\n- The paper states that subtasks were filtered using a \"subtask-level exclusion protocol\" based on \"the authors' graduate-school-level expertise\" (footnote 3 In P.4). This process is critical to the construction of the benchmark, yet it is described too briefly. Key details are missing, such as: How many authors/annotators were involved in this manual inspection? If multiple annotators are involved, does it use majority vote process to reach the final conclusion? The authors can include these details in the paper.\n\n- This paper does not cite and discuss the CURIE benchmark [1], which is also a comprehensive scientific reasoning benchmark that covers disciplines including materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins. This paper should cite and discuss the differences between the proposed benchmark and the CURIE benchmark.\n\n[1] CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning. Cui et al. 2025."}, "questions": {"value": "- How many annotators were involved in the subtask-level exclusion process?\n\n- Since larger LLMs contain more parameters to store factual knowledge, do you think the knowledge bottleneck will become a less significant issue for larger-scale LLMs in scientific reasoning problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xSMinthfs1", "forum": "l852J55j9n", "replyto": "l852J55j9n", "signatures": ["ICLR.cc/2026/Conference/Submission22637/Reviewer_w48u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22637/Reviewer_w48u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977289459, "cdate": 1761977289459, "tmdate": 1762942313450, "mdate": 1762942313450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}