{"id": "AHpexliCTM", "number": 2849, "cdate": 1757280557049, "mdate": 1759898123269, "content": {"title": "Cooperative Sheaf Neural Networks", "abstract": "Sheaf neural networks (SNNs) leverage cellular sheaves to induce flexible diffusion processes on graphs, generalizing the diffusion mechanism of classical graph neural networks. While SNNs have been shown to cope well with heterophilic tasks and alleviate oversmoothing, we argue that there is further room for improving sheaf diffusion. More specifically, we show that SNNs do not allow nodes to independently choose how they cooperate with their neighbors, i.e., whether they convey and/or gather information to/from their neighbors. To address this issue, we first introduce the notion of cellular sheaves over directed graphs and characterize their in- and out-degree Laplacians. We then leverage our construction to propose Cooperative Sheaf Neural Network (CSNN). Additionally, we formally characterize its receptive field and prove that it allows nodes to selectively attend (listen) to arbitrarily far nodes while ignoring all others in their path, which is key to alleviating oversquashing. Our results on synthetic data empirically substantiate our claims, showing that CSNN can handle long-range interactions without succumbing to oversquashing. We also show that CSNN performs strongly in heterophilic node classification and long-range graph classification benchmarks.", "tldr": "", "keywords": ["sheaves", "message-passing", "graphs", "neural networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed656ba9ad1d05cd36f2f7e3c6d6b72d2d19fa66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that standard Sheaf Neural Networks (SNNs) on undirected graphs cannot realize node-level cooperative behavior (separating LISTEN vs PROPAGATE), because silencing incoming messages at a node also suppresses its outgoing influence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Clear motivation\n* The limitation of undirected-sheaf diffusion for cooperative behavior is crisply identified and formalized. Proposition 3.1 explicitly shows PROPAGATE -> LISTEN coupling in classical SNNs; the directed-sheaf construction is a natural remedy\n\n### Empirical breadth on heterophily\n* Across 11 node-classification benchmarks (including the cleaned Squirrel/Chameleon), CSNN often outperforms both cooperative GNNs and prior SNNs\n\n### Implementation details\n* Parametrization for orthogonal maps, per-node conformal scaling, and a complexity discussion point to a careful engineering effort"}, "weaknesses": {"value": "### Novelty and Positioning\n* Treating undirected edges as two directed edges plus per-node source/target maps is a natural extension but arguably an incremental one within the sheaf literature. The paper acknowledges related notions (e.g., quiver Laplacians), but the novelty boundary versus prior directed/sheaf constructions (and vector bundle variants on directed graphs) is not fully pinned down\n\n### Theoretical Analysis\n* Proposition 4.3 is an existence proof: there exist restriction maps for clean long-range, path-selective propagation. The paper does not analyze whether gradient-based training reliably finds such configurations in realistic, noisy data\n* The paper notes their directed-sheaf Laplacians can have complex eigenvalues with negative real parts (unlike PSD Laplacians). Yet the stability of the discrete diffusionis not analyzed\n* The model still performs local diffusion; the 2-hop per layer can accelerate reach but may also accelerate oversmoothing. There’s no theorem bounding oversmoothing for CSNN or showing improved expressivity beyond MPNN limitations\n\n### Writing Clarifications\n* A derivation/intuition for why this specific normalization (scalar multiples of identity under conformal maps) is preferred (vs. other block scalings) would help\n* Regarding Proposition 4.3 \n  * The constructive path-only propagation is compelling but assumes the ability to zero out many maps across layers. In practice, with shared parameters and noisy optimization, how often does training approximate this regime?"}, "questions": {"value": "Please see the above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7kZsU1uTP", "forum": "AHpexliCTM", "replyto": "AHpexliCTM", "signatures": ["ICLR.cc/2026/Conference/Submission2849/Reviewer_ZAJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2849/Reviewer_ZAJv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760517817106, "cdate": 1760517817106, "tmdate": 1762916408439, "mdate": 1762916408439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends Sheaf Neural Networks, a class of GNNs that use cellular sheaves to generalize message passing, to support cooperative communication between nodes. Classical SNNs effectively address heterophily and oversmoothing, but they lack the flexibility for nodes to independently choose how to exchange information (i.e., whether to propagate, listen, or isolate).\nTo overcome this, the authors propose the Cooperative Sheaf Neural Network (CSNN), which introduces directed cellular sheaves with separate in- and out-degree Laplacians, enabling asymmetric and selective communication."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear theoretical guarantees linked to long-range neighbors and over-squashing.\n\nEmpirical performance is compelling"}, "weaknesses": {"value": "Scalability of sheaf-based models on large-scale graphs remains to be tested."}, "questions": {"value": "1. This paper seems to be an A+B work, with a combination of sheaf GNN + cooperative GNN, and some extension on directed graphs. And it is not well motivated or does not have some special designs for heterophily and over-squashing.\n\n2. Why does the sheaf neural network have to achieve cooperative behavior?\n\n3. \"our model has the ability to reach longer distances.\" It is no surprise when you multiple your in-degree and out-degree sheaf Laplacian together. However, it is found that long-range information is harmful in many heterophilic datasets [1].\n\n4. Missing comparison with some baseline models, e.g. ACMGCN [2], FSGNN [3], GloGNN [4]. More tests on malignant and ambiguous heterophilic datasets listed in [5]. Experiments on large scale datasets used in [4].\n\n\n\n\n[1] Less is More: on the Over-Globalizing Problem in Graph Transformers. In Forty-first International Conference on Machine Learning 2024.\n\n[2] Revisiting heterophily for graph neural networks. Advances in neural information processing systems. 2022 Dec 6;35:1362-75.\n\n[3] Simplifying approach to node classification in graph neural networks. Journal of Computational Science, 62, 101695.\n\n[4] Finding global homophily in graph neural networks when meeting heterophily. In International conference on machine learning (pp. 13242-13256). PMLR.\n\n[5] The heterophilic graph learning handbook: Benchmarks, models, theoretical analysis, applications and challenges. arXiv preprint arXiv:2407.09618. 2024 Jul 12."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vbD4CP73wR", "forum": "AHpexliCTM", "replyto": "AHpexliCTM", "signatures": ["ICLR.cc/2026/Conference/Submission2849/Reviewer_o6mM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2849/Reviewer_o6mM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686566523, "cdate": 1761686566523, "tmdate": 1762916407832, "mdate": 1762916407832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a new Sheaf Neural Network model, called Cooperative Sheaf Neural Network (CSNN), with the goal of combining the benefits of Sheaf Neural Networks in tackling oversmoothing in GNNs and handling heterophilic data, with the property of performing selective communication between nodes, which is expected to lead to cooperative behaviour and avoid oversquashing problems. The model and its benefits are validated through synthetic and real-world experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation, goal, and methodology are well formulated.\n- The paper reads well, it has a nice structure, and the necessary background knowledge is well presented."}, "weaknesses": {"value": "There are strong claims regarding (1) consistently outperforming SNNs and cooperative GNNs (both in introduction and in the experiments, see Results in Section 6.2), and (2) about not soccumbing to oversquashing (abstract). Looking at the results, it doesn't appear to improve substantially with respect to the competitors, so I would advise to reconsider the strength of the claims, which may lead to high expectations in the experiments.\n\nAdditionally, the introduction of a sheaf structure generally adds computational and runtime overhead compared to a GNN model, so its inclusion should be well justified in two respects: (1) whether the additional cost is outweighed by the gain in prediction accuracy, and (2) whether there is a real practical need for a sheaf-based model. Regarding (1), a discussion ideally comparing the model with non-sheaf baselines, is missing in the main part of the paper. Regarding (2), I would expect a convincing discussion of the advantages over other SNN methods in preventing oversquashing (due to the additional cooperative component), and over cooperative GNNs in mitigating oversmoothing and handling heterophilic datasets (thanks to the additional sheaf structure). These advantages are not evident from the experiments, as the relevant comparisons are either missing or the results/discussions are not sufficiently convincing."}, "questions": {"value": "**General concerns** \n- In your introduction, you mention that selective communication is a desirable property for SNN in order to tackle oversquashing. Recently, Nonlinear Sheaf Neural Networks have shown a similar behavior, selectively exploiting information of neighbors in complex node interactions [1]. Do you think there may be a connection between your method and the employment of a nonlinear Laplacian? \n- After definition 2.3: \"...however, when they publicly discuss this topic, they may prefer to not manifest their true opinion. \". In my understanding, since the edge stalks may be different from the node stalks, the individuals don't necessarily need to discuss the topics of the private opinion spaces. The topics in the public agreement space may also be different.\n- It is not clear how your result in Proposition 4.3 relates to your discussion on the oversquashing behavior relying on the definitions in [2] and [3], which you reference after Example 4.4. These works formalize oversquashing as a bound on the Jacobian, as you also state, and this bound is strongly influenced by the presence of $A_{i,j}^{(t)}$. In Example 4.4, although you show that at each layer one node is influenced by only a single other node, when computing the Jacobian the update matrices $T$ and $S$ will still accumulate in the product across multiple layers. So, there is still a product of $O(t)$ matrices in the bound for the Jacobian. Wouldn’t this have the same effect as including the term  $A_{i,j}^{(t)}$? Could you please elaborate on this point, perhaps by explicitly showing how your method improves this bound compared to a sheaf model that does not perform selective communication?\n\n**Experiments**\n- It would be useful to have in Table 1 and 2 an homophily measure for each dataset, to intuitively understand the setting.\n- I would recommend to rephrase the claims regarding the results. For example, in the \"Results\" of section 6.2, you state \"We note CSNN often outperforms both NSD and CO-GNN by a significant margin\", although looking at the results of Table 1 and 2, and considering the +/- confidence bound, the improvements are not as significant as stated. \n- As mentioned in the weaknesses, I believe the experiments would benefit from a direct comparison between the proposed method and its non-sheaf (CO-GNN) and non-cooperative (NSD/BuNN) counterparts - for example, by including these models in the synthetic experiments of Section 6.1 and adding CO-GNN in Table 2 to provide a more complete comparison and a clearer demonstration of the claims.\n\n**Writing/Typos**\n- In related works, in the paragraph related to Cooperative GNNs: \"... that chooses the cooperation the action each node takes\". There seems to be an error in the structure of this sentence.\n- In Table 2, last column, the second-best result is not highlighted with grey color.\n\n\n\n[1] Zaghen et al., Sheaf Diffusion Goes Nonlinear: Enhancing GNNs with Adaptive Sheaf Laplacians (2024)\n\n[2] Di Giovanni et al., On over-squashing in message passing neural networks: The impact of width, depth, and topology. (2023)\n\n[3] Topping et al., Understanding over-squashing and bottlenecks on graphs via curvature (2022)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jlLcArpU1c", "forum": "AHpexliCTM", "replyto": "AHpexliCTM", "signatures": ["ICLR.cc/2026/Conference/Submission2849/Reviewer_rWHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2849/Reviewer_rWHL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903664906, "cdate": 1761903664906, "tmdate": 1762916407418, "mdate": 1762916407418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an approach for learning cooperative behavour between nodes, by treating undirected edges between nodes as pairs of directed edges and introducing directed sheaf neural networks to effectively deal with them."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The work is well motivated, addressing a clear limitation of sheaf neural networks in modeling cooperation patterns between nodes.\n\nThe proposed solution has solid theretical grounds.\n\nSynthetic results clearly confirm the ability to mitigate oversquashing, and an extensive evaluation on real-world datasets shows competitive performance wrt the state of the art."}, "weaknesses": {"value": "Experimental results on the classical datasets for heterophilic analysis (Table 2) show very marginal improvements (considering the high variance, most likely none of these is significant). This is not a novelty, and questions the appropriateness of these datasets as benchmarks (as Platonov et al already pointed out). I encourage the authors to briefly discuss this aspect, so as to direct further research towards more appropriate evaluation benchmarks."}, "questions": {"value": "Is it possible to have a high-level illustration clarifying the problem with plain SNN and the advantage of CSNN? This would help interested readers not familiar with the math behind SNN to gather the intuition behind the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qLFGWFK0DG", "forum": "AHpexliCTM", "replyto": "AHpexliCTM", "signatures": ["ICLR.cc/2026/Conference/Submission2849/Reviewer_4F1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2849/Reviewer_4F1X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081897937, "cdate": 1762081897937, "tmdate": 1762916406266, "mdate": 1762916406266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}