{"id": "rWzdEvakRi", "number": 23040, "cdate": 1758338645470, "mdate": 1759896834741, "content": {"title": "Learning Randomized Reductions", "abstract": "A self-corrector for a function $f$ takes a black-box oracle computing $f$ that is correct on most inputs and turns it into one that is correct on every input with high probability. Self-correctors exist for any function that is randomly self-reducible (RSR), where the value $f$ at a given point $x$ can be recovered by computing $f$ on random correlated points. While RSRs enable powerful self-correction capabilities and have applications in complexity theory and cryptography, their discovery has traditionally required manual derivation by experts. We present Bitween, a method and tool for automated learning of randomized self-reductions for mathematical functions. We make two key contributions: First, we demonstrate that our learning framework based on linear regression outperforms sophisticated methods including genetic algorithms, symbolic regression, and mixed-integer linear programming for discovering RSRs from correlated samples. Second, we introduce Agentic Bitween, a neuro-symbolic approach where large language models dynamically discover novel query functions for RSR property discovery, leveraging vanilla Bitween as a tool for inference and verification, moving beyond the fixed query functions ($x+r$, $x-r$, $x \\cdot r$, $x$, $r$) previously used in the literature. On RSR-Bench, our benchmark suite of 80 scientific and machine learning functions, vanilla Bitween surpasses existing symbolic methods, while Agentic Bitween discovers new RSR properties using frontier models to uncover query functions.", "tldr": "We automate the discovery of randomized self-reductions—mathematical properties that enable self-correcting programs—using both efficient regression and LLM-guided exploration.", "keywords": ["Randomized Self-Reductions", "Neuro-Symbolic Learning", "Self-Correctness", "Mathematical Function Learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fc2cee48ca6f803921fb74359a6bf423ebd6475.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes Bitween, an automated framework for BSRs from mathematical functions. Specifically, the authors claim that the vanilla Bitween can outperform traditional symbolic regression methods under fixed query functions. Moreover, agentic Bitween can leverage LLMs to dynamically discover novel query functions. To justify the effectiveness of the proposed method, the experiments are conducted on RSR-Bench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is in general well written and the method is easy to understand.\n\n2.The combination of linear regression and symbolic verification is practical and well-motivated."}, "weaknesses": {"value": "1.The query function generation process of agentic Bitween is opaque, and the convergence behavior is not guaranteed.\n\n2.The experimental design focused on quantity and lacked in-depth analysis. For example, there is a lack of in-depth discussion on whether the designed RSRs have mathematical meaning.\n\n3.Authors argue that the proposed method outperforms sophisticated methods like GP and SR. The interpretability is the merit of GP and SR, the author should analyze this point.\n\n4.The study mainly focuses on the performance in comparisons. However, the expression size and the search cost also need consideration, especially in practical terms.\n\n5.More up-to-date peer competitors should be compared, such as RAG-SR [1], ParFam [2], and Metasymnet [3].\n\n6.The authors argue that BSRs are important for complexity theory and cryptography. Thus, are there any downstream applications or case studies demonstrating practical value of the proposed method?\n\n*References:*\n\n[1] Zhang H, Chen Q, Banzhaf W, et al. RAG-SR: Retrieval-augmented generation for neural symbolic regression[C]//The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Scholl P, Bieker K, Hauger H, et al. ParFam--(Neural Guided) Symbolic Regression via Continuous Global Optimization[C]//The Thirteenth International Conference on Learning Representations. 2025.\n\n[3] Li Y, Li W, Yu L, et al. Metasymnet: A tree-like symbol network with adaptive architecture and activation functions[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(25): 27081-27089."}, "questions": {"value": "Please see the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZLqpAE75P", "forum": "rWzdEvakRi", "replyto": "rWzdEvakRi", "signatures": ["ICLR.cc/2026/Conference/Submission23040/Reviewer_u98M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23040/Reviewer_u98M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761187047443, "cdate": 1761187047443, "tmdate": 1762942488094, "mdate": 1762942488094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bitween, an algorithm for learning randomized self-reductions (RSRs). The authors evaluate their method on a suite of 80 scientific and machine learning functions, demonstrating that Bitween can successfully learn RSRs for a significant portion of these functions. The authors also introduce Agentic Bitween, which integrates large language models (LLMs) into the Bitween framework to enhance RSR discovery. Experimental results indicate that Agentic Bitween outperforms all shown baselines and solves some cases previously believed intractable."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well-presented, well-motivated, and establishes a valuable connection to the well-known field of PAC learning. \n2. The authors dedicate significant effort to building a theoretical foundation, stating assumptions and providing formal proofs. \n3. The core problem of learning RSRs is interesting and holds potential for broader applications."}, "weaknesses": {"value": "1. The theoretical foundation is riddled with numerous formatting and referencing errors. These mistakes significantly detract from the readability and undermine the mathematical rigor, which is a key claimed contribution. Examples include citation errors (e.g., L983), vague self-references (e.g., Section 2 referring to itself in L88), and, most critically, incorrect cross-references within the theoretical sections. The text refers to \"Theorem 5\" when it means \"Definition 5\", and similar errors occur on L766 (confusing Theorem and Claim) and L775 (confusing Theorem and Remark). Such mistakes make the formalisms difficult to follow. While a diligent reader might eventually decipher the intended meaning, the burden of clarity is on the authors.\n\n2. It is seriously confusing what you refer to as \"V-Bitween\" versus baselines like PySR, GPLearn, and MILP. At one point, such as L071 (in the contributions), you treat them as separate methods, while in the experiments section, you say that V-Bitween uses these baselines as backends and Figure 2 names them with the \"V-Bitween-\" prefix. Is Vanilla Bitween the same thing as V-Bitween? This needs to be clarified. \n\n3. Some of the results are badly reported/presented. Examples include:\n* Section 6 reports results in text form (e.g., the contribution 1 paragraph) that would be much better suited for tables or plots.\n* Figure 2 (left) reports \"Average Verified RSRs by Function Category\": I assume that means you average the number of RSRs found and verified for each function over the respective category. This is a bit strange to me. Why do you attempt multiple RSRs per function? Shouldn't one be enough? Wouldn't it make more sense to report the fraction of functions for which at least one RSR was found and verified? \n* Why are the RSR Rate and Verification Rate reported separately in Figure 2 (right)? Shouldn't they be the same, since only verified RSRs should count? \n* What is Function Coverage in Figure 2 (right)? This is not clearly defined in the text. How do you measure it? Is it coverage over individual functions or function categories?\n* In Section 6, you say \"Figure 3 demonstrates that this breakthrough stems from Agentic Bitween’s intensive use of verification and inference tools across all function categories.\" Looking at Figure 3, I see no evidence for this claim. It merely plots a heat map of \"tool calls\" between different A-Bitween variants (different LLMs). It does not correlate tool usage with performance at all.\n\n4. Table 1 lists \"Novel Query **Functions** Discovered by Agentic Bitween,\" but the entries appear to be equations rather than functions. Maybe I'm missing something, but this is rather confusing. \n\n5. The paper claims Agentic-Bitween as its breakthrough contribution, yet it is hardly described in the main text other than saying that it uses an LLM to generate RSRs then verifies them symbolically. At the very least, an outline diagram of the architecture or a more detailed description of its operation should be provided in the main paper.\n\n6. L406-408: \"Larger models exhibit increased reasoning depth, with more sophisticated analysis leading to higher token usage but correspondingly better RSR **discovery quality**.\" What does \"RSR discovery quality\" mean here? Are some RSRs of higher quality than others? If so, how is this measured? This is not discussed anywhere in the paper.\n\n7. L411-413: \"The tool-based reasoning approach, while more expensive, enables systematic exploration of novel mathematical relationships **impossible** with pure neural reasoning.\" This is a very strong claim that requires proper backing. Why is it impossible with pure neural reasoning?\n\n## Minor Issues \n1. Why are all citations rendered as text citations (e.g., \\citet{})? This appears to be a formatting error.\n\n2. Vague references like \"in Section 2\" (e.g., L88 & L103) are not very helpful. Please refer to the specific labels for equations, figures, and subsections.\n\n3. The citation in L344 for Claude 4 points to a paper about Claude 3.\n\n4. Broken citations/references to F* and VeriFast (L983)."}, "questions": {"value": "I already raised several questions in the weaknesses section. Below are some additional ones: \n1. Is the Bitween algorithm (Algorithm 1) sound? I cannot find a clear failure case in the algorithm definition. Does it simply return the same function as the input if no RSR is found? \n2. Could you please explain, in one place, a proper outline of Agentic-Bitween? How is it structured? My current understanding is that it uses queries to generate potential query functions for the main algorithm. Is this correct? What symbolic reasoning tools does the LLM have access to, and how are they integrated?\n3. I see the fixed query function set $\\\\{x + r, x − r, x · r, x, r\\\\}$ includes $x$, doesn't this defeat the purpose of the reduction since it is meant to derive $f(x)$ without directly evaluating $f$ at $x$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n2Wf9jDhAm", "forum": "rWzdEvakRi", "replyto": "rWzdEvakRi", "signatures": ["ICLR.cc/2026/Conference/Submission23040/Reviewer_FY2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23040/Reviewer_FY2m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957151378, "cdate": 1761957151378, "tmdate": 1762942487826, "mdate": 1762942487826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes a method for learning RSR for mathematical functions. It introduces Vanillan Bitween, a linear-regression–based learner and A-Bitween, an agentic LLM-driven variant that can discover new query functions beyond the classic ones. An RSR-Bench with 80 functions is proposed and used for evaluation of the method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces an interesting framework for RSRs, which potentially could be highly impactful.  The proposed Bitween combines sparse linear regression with formal verification, and is further extended to the neuro-symbolic A-Bitween.  The concept and the methodology are novel, and appear effective as argued in the paper.  \n\nThe method's theoretical foundation is sound, with a clear connection to PAC learning. The proposed RSR-Bench could be a valueable contribution to the field.  Using 80 benchmark functions, the study validates performance against baselines PySR, GPLearn, and MILP.  Overall, the work reflects a substantial and well-motivated research effort."}, "weaknesses": {"value": "The empirical comparison is not clearly presented.  It is written that \"Vanilla Bitween surpasses traditional symbolic methods within the fixed query function paradigm, discovering 76 total verified RSRs compared to PySR’s 54, GP-Learn’s 47, and MILP’s 64\".   The details should be provided.  The comparisons focus on PySR, GPLearn, and MILP, while other methods, e.g. AI Feynman and DSR are mentioned but not used.  Justification should be provided for that.  In addition, the detailed settings of other methods are not provided.  For example, GP is known to be a strong symbolic regressor, but its performance varies significantly with factors such as population size, number of generations, function set, and several other parameters. Without specifying these details, the comparision won't be convincing.  \n\nThe proposed benchmark of 80 functions need to be better described and justifed.  For broader adoption, the coverage should be as comrehensive as possible, e.g. involving piecewise, discontinuous, or domain-restricted functions. Are these functions part of the consideration? Argubly, sigmoid functions are kind of toy problems.  Would it possible to add some end-to-end large functions/systems?\n\nIt’s unclear how LR's performance would scale with input dimension, e.g. number of queries. The scaling behavior/limits are not quantified. Also the study assumes uniformly samplable domains and relies on uniform draws. Would that limit its performance and robustness over more realistic and non-uniform distributions?  \n\nThe introduction of the agentic component may dilute the focus. The paper could benefit from first establishing a more comprehensive empirical foundation, addressing the concerns mentioned above, e.g. scalability, sampling, and benchmark coverage.  Once a strong foundation is estalished, agentic-bitween can then be pursued as future work.\n\nIn Algorithm 1, maximum denominator constraint is not specified. What is the upper bound?"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TgFiOYMJAA", "forum": "rWzdEvakRi", "replyto": "rWzdEvakRi", "signatures": ["ICLR.cc/2026/Conference/Submission23040/Reviewer_gpS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23040/Reviewer_gpS3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982220276, "cdate": 1761982220276, "tmdate": 1762942486782, "mdate": 1762942486782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}