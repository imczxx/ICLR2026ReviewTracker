{"id": "tmAK9FJr5X", "number": 10180, "cdate": 1758163238276, "mdate": 1762924529817, "content": {"title": "Proactive Cost Generation for Offline Safe Reinforcement Learning Without Unsafe Data", "abstract": "Learning constraint-satisfying policies from offline data without risky online interaction is essential for safety-critical decision making. \nConventional approaches typically learn cost value functions from large numbers of unsafe samples in order to delineate the safety boundary and penalize potentially violating actions.\nIn many high-stakes settings, however, risky trial-and-error is unacceptable, resulting in offline data that contains few, if any, unsafe samples.\nUnder this data limitation, existing approaches tend to treat all samples as uniformly safe, neglecting the substantial presence of safe-but-infeasible states—states that are currently constraint-satisfying but inevitably violate safety constraints within a few future steps—thereby resulting in deployment failures.\nTo overcome this challenge, we present PROCO, a proactive offline safe RL framework tailored to datasets largely devoid of violations. PROCO first learns a dynamics model from the offline data to capture environment information. It then constructs a conservative cost signal by grounding natural-language descriptions of unsafe states in large language models (LLMs), yielding risk assessments even when violations are unobserved. Finally, PROCO performs model-based rollouts under this cost to synthesize diverse and informative counterfactual unsafe samples, which in turn enable reliable feasibility identification and support feasibility-guided policy learning.\nAcross diverse Safety-Gymnasium tasks, PROCO consistently reduces constraint violations and improves safety relative to conventional offline safe RL and behavior cloning baselines when training data contains only safe or minimally risky samples.", "tldr": "We propose PROCO, an offline safe RL algorithm capable of identifying infeasible states and learning safe policies from datasets with few or no unsafe samples.", "keywords": ["Reinforcement Learning", "Offline Reinforcement Learning", "Safe Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/deff3fab4d9195270c552cca2552a0e4881f4f23.pdf", "supplementary_material": "/attachment/b242f408c4783edbbad25376f900dcf84d45aca1.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of learning a safe policy from an offline dataset that has few or no unsafe samples. The proposed algorithm, called PROCO, uses a learned dynamics model and a cost function generated by LLM to perform rollouts, synthesizing more unsafe samples. The algorithm is tested on Safety-Gymnasium benchmark against several offline safe RL baselines, and results show that PROCO is effective in improving safety."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithm is effective in improving safety.\n2. The writing of the paper is clear."}, "weaknesses": {"value": "1. Generating the cost function requires natural language specification, including task information and cost description. Such specifications relies on detailed knowledge of the environment and task-by-task design.\n2. The dynamics model used for generating unsafe samples is learned on a dataset with few unsafe samples in the first place. This means that the generated unsafe samples are likely unreliable because they come from extrapolating the dataset."}, "questions": {"value": "1. Is a natural language specification of a task always available? Does the construction of such specifications generalizable to different tasks? \n2. From Appendix D.2, it seems that writing the task descriptions requires even more knowledge and labor than constructing the cost functions. Why is the LLM necessary? Why not just handcraft the cost function?\n3. How accurate is the dynamics model on unsafe samples that are not in the dataset? Why can we believe these samples since they come from extrapolation, which we aim to avoid in offline RL?\n4. From the experimental results, the rewards of PROCO are much lower than the baselines. Is this because PROCO is very conservative or this is already a pretty high reward for a safe policy?\n5. How to choose the proportion of safe samples to be labeled as unsafe? How does this proportion affect reward and cost performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IxEa1ksiM2", "forum": "tmAK9FJr5X", "replyto": "tmAK9FJr5X", "signatures": ["ICLR.cc/2026/Conference/Submission10180/Reviewer_oxxF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10180/Reviewer_oxxF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602944943, "cdate": 1760602944943, "tmdate": 1762921548720, "mdate": 1762921548720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JZHsuTrz8a", "forum": "tmAK9FJr5X", "replyto": "tmAK9FJr5X", "signatures": ["ICLR.cc/2026/Conference/Submission10180/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10180/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924527686, "cdate": 1762924527686, "tmdate": 1762924527686, "mdate": 1762924527686, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PROCO, a proactive offline safe RL framework for datasets with few or no unsafe samples. It combines a learned dynamics model with LLM-based cost estimation to generate counterfactual unsafe samples for feasibility-guided policy learning. Experiments show that PROCO improves safety and reduces violations compared to existing offline safe RL methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is clear, and the authors provide some theoretical justification for the proposed conservative feasible Bellman operator."}, "weaknesses": {"value": "1. The assumption that the learned dynamics model has bounded error is strict and unlikely to hold when the policy encounters out-of-distribution states, particularly in settings with only safe or limited unsafe data. In addition, the paper does not specify the assumptions on the learned reward function.\n\n2. The usage of LLM seems unnecessary, given that a clear description of the task’s safety constraint is available (lines 130-133) and the cost-related features can already be derived from the agent’s observations (Appendix G).\n\n3. The policy learning component appears incremental compared to FISOR, as the main difference lies only in the addition of an extra term in Eq. (12) when computing $\\mathcal{L}_{Q_h}$. This limits the novelty and technical contribution of the proposed method."}, "questions": {"value": "1. The authors mention several times that the dataset consists of no more than 100 unsafe transitions (Section 3, 5.1, and Appendix D.2). Why was this specific number chosen?\n\n2. Regarding Assumption 4.7 on the learned dynamics, since the dataset mostly contains safe data with very limited unsafe samples, how can the model accurately capture or generate unsafe transitions that lie outside the dataset’s support?\n\n3. The paper introduces LLM-based cost generation, but Appendix D.2 indicates that human experts provide feedback to the LLM outputs. Given that task safety constraints are already described and experts are involved, why not manually design the cost function instead of relying on the LLM?\n\n4. The results in Table 1 indicate that the proposed method achieves very low normalized rewards (e.g., 0.14) while maintaining safety. How do the authors ensure that this conservativeness does not simply result from trivial behavior, such as the agent remaining stationary to avoid violations?\n\n5. What is the ratio of generated unsafe data to the original safe data, and how does varying the number of generated unsafe samples influence the learned policy’s performance and safety–reward trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XzHyx5AVPF", "forum": "tmAK9FJr5X", "replyto": "tmAK9FJr5X", "signatures": ["ICLR.cc/2026/Conference/Submission10180/Reviewer_1Chq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10180/Reviewer_1Chq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439178010, "cdate": 1761439178010, "tmdate": 1762921548056, "mdate": 1762921548056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses offline safe reinforcement learning when training data contains few or no unsafe samples. The authors propose **PROCO**, a framework that: (1) learns a dynamics model from offline data, (2) leverages large language models (LLMs) to generate conservative cost functions from natural language safety constraint descriptions, and (3) performs model-based rollouts to synthesize counterfactual unsafe samples. The key insight is identifying \"safe-but-infeasible\" states—states currently satisfying constraints but inevitably leading to violations within a few steps. PROCO demonstrates 5× improvement in safety over baselines across 17 Safety-Gymnasium tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a practical and underexplored scenario where collecting unsafe data is expensive or prohibited in safety-critical applications\n-  Innovative application of LLMs to generate conservative cost functions from natural language, with a validation-and-feedback mechanism to ensure reliability\n- Provides formal analysis (Theorems 4.6, 4.9) showing how conservative cost functions reduce rollout horizon and model error impact on feasibility identification"}, "weaknesses": {"value": "- Despite check-and-feedback, still susceptible to LLM hallucinations; some tasks (Walker2dVelocity) failed to achieve 100% accuracy on unsafe samples due to observation-reality misalignment\n\n- Model learning, LLM queries, and iterative validation add overhead not thoroughly discussed\n\n- Requires careful tuning of conservativeness range [p_min, p_max] and multiple LLM queries (up to 10) may still fail to find satisfactory cost functions"}, "questions": {"value": "1. How does PROCO perform when the cost function structure is highly complex or requires multi-step reasoning? Can the LLM-generated cost functions generalize to scenarios not covered in the prompt examples?\n\n2. How does the proportion of safe-but-infeasible states in the dataset affect performance? Is there an analysis showing when PROCO provides the most benefit over baselines?\n\n3. Figure 4(b) shows sharp performance degradation at H=5. Can you provide model prediction error analysis to explain why H must be restricted to 1? Does this limit the types of safety constraints PROCO can handle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jvajGFp1Rq", "forum": "tmAK9FJr5X", "replyto": "tmAK9FJr5X", "signatures": ["ICLR.cc/2026/Conference/Submission10180/Reviewer_w6WT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10180/Reviewer_w6WT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948906826, "cdate": 1761948906826, "tmdate": 1762921547620, "mdate": 1762921547620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PROCO, an offline safe Reinforcement Learning (RL) framework designed for scenarios where the pre-collected dataset contains few or no unsafe samples. Traditional offline safe RL methods struggle without risky data, often misclassifying “safe-but-infeasible” states as safe. PROCO addresses this by combining a learned dynamics model with LLM-generated conservative cost functions based on natural-language safety descriptions. It proactively simulates potential unsafe scenarios to identify infeasible states and guide safe policy learning. Experiments on 17 Safety-Gymnasium tasks show that PROCO significantly reduces safety violations compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It addresses the novel and challenging problem of learning safe policies from offline datasets that contain few or no unsafe samples, which is a real-world, high-stakes scenario.\n\n- The empirical results show improved safety performance.\n\n- It introduces an interesting application of LLMs to generate a conservative cost function from a natural language description, adapting a known technique (LLMs for reward generation) to the safety domain."}, "weaknesses": {"value": "- The paper's assumption that feasibility information is missing from the dataset is questionable. The premise states data is truncated by \"external interventions\" to prevent violations, which implies the final state of such trajectories is already known to be infeasible. This provides the exact feasibility signal that the paper's LLM and rollout components are designed to synthetically create.\n\n- PROCO's core policy learning algorithm is structurally almost identical to FISOR. The paper's primary contribution is limited to a data pre-processing step: it first strips the original data of all cost signals and then adds back synthetic cost signals via LLM-based cost generation and model-based rollouts, which the PROCO, basically FISOR, algorithm can then use.\n\n- The fundamental theoretical basis (multi-step reachability analysis for feasibility) is largely abandoned in practice as the default implementation uses an extremely limited rollout length ($\\mathbf{H=1}$).\n\n- The paper introduces a novel hard-constraint, \"safe-only\" problem, but tests it on the DSRL benchmark, which is designed for soft, cumulative constraints where violations are recoverable and expected in the data. This creates a fundamental mismatch. The paper's setup artificially removes cost signals to fit this premise, which is problematic for the chosen baselines. Soft-constraint methods like CPQ are unsuited for this zero-signal, hard-constraint task. Even FISOR, which does use hard constraints, is unfairly benchmarked, as its original design relies on observing cost signals to learn feasibility signals that were intentionally removed in PROCO's baseline comparison.\n\n- Despite pursuing a hard constraint objective targeting zero cost, the resulting policy does not achieve this goal, exhibiting mostly non-zero violation rates.\n\n- Coptidice and Fisor not sota OSRL baselines. COptiDICE, in particular, demonstrates weak safety performance in the DSRL paper. The literature review also appears incomplete, omitting several relevant OSRL publications, including:\n  - OASIS [https://arxiv.org/pdf/2407.14653]\n  - Constraint-Adaptive Policy Switching [https://www.arxiv.org/pdf/2412.18946]\n  - Trajectory Classification for Safe RL [https://arxiv.org/pdf/2412.15429]\n  - Constraint-conditioned actor-critic for OSRL [https://openreview.net/pdf?id=nrRkAAAufl]\n\n- Tables 5 and 6 are hard to parse."}, "questions": {"value": "- Please check weaknesses.\n- How exactly were the safe-only datasets constructed? were entire trajectories removed or only violating transitions?\n- In Figure 2, are the component ablations averaged over all tasks or shown for a single task, and if the latter, which task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4tqb1cV7hA", "forum": "tmAK9FJr5X", "replyto": "tmAK9FJr5X", "signatures": ["ICLR.cc/2026/Conference/Submission10180/Reviewer_8q3P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10180/Reviewer_8q3P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981112085, "cdate": 1761981112085, "tmdate": 1762921547082, "mdate": 1762921547082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}