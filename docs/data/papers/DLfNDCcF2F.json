{"id": "DLfNDCcF2F", "number": 1511, "cdate": 1756888513277, "mdate": 1763001542397, "content": {"title": "PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore", "abstract": "Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \\textbf{PrefPoE}, a novel \\textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \\textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\\% on HalfCheetah-v4 (1276~$\\rightarrow$~5375), +69\\% on Ant-v4, +276\\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \\textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.", "tldr": "We introduce PrefPoE, a general exploration framework that fuses advantage-weighted preferences with the main policy via Product-of-Experts, achieving significant improvements across both continuous and discrete RL tasks.", "keywords": ["advantage-guided exploration", "product-of-experts", "preference learning", "policy fusion", "soft trust region", "entropy dynamics", "policy fusion", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6940ecea6c70dac466527dc13f0de14d5ab2e52f.pdf", "supplementary_material": "/attachment/79a0908b2d13bd27bce75c09bb859091d764b800.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to improve sample efficiency in policy-based reinforcement learning by modifying the exploration strategy. The authors argue that exploitative exploration (favoring actions with higher advantages) outperforms entropy-based random exploration. Their technical approach involves training a separate preference head that weights actions according to their advantage estimates. During trajectory rollouts, the policy combines outputs from both the preference head and the standard policy head. Experimental results on six tasks demonstrate improvements over PPO-based methods with high-entropy exploration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Sound technical approach**: The use of PoF mechanism to reshape action distributions is technically sound. Incorporating an auxiliary preference head that considers advantage estimates provides an interpretable way to bias action selection toward potentially beneficial states."}, "weaknesses": {"value": "1. **Weak Theoretical Motivation** The central claim that *exploration should be guided by advantage estimates rather than being uniformly distributed* lacks rigorous justification. This weakens the paper's foundational motivation. Moreover, the characterization of standard trajectory rollout as \"uniformly sampling\" is inaccurate that standard policy gradient methods already bias sampling toward actions with higher probabilities under the learned policy. Value information is implicitly incorporated through policy updates.\n2. **Insufficient Support for Strong Claims** The paper makes claims that directly contradict established exploration literature (e.g., the benefits of entropy regularization for exploration). Such significant departures from accepted principles require: i) deeper theoretical analysis explaining when and why advantage-guided exploration outperforms entropy-based approaches; ii) clear statement of assumptions under which this holds; iii) justification for why existing mechanisms (e.g., tuning entropy coefficients, including negative values for more exploitative behavior) cannot address the identified issues.\n3. **Limited Experimental Scope** The experimental evaluation only compares against PPO variants, which significantly limits its contribution. To substantiate the claims, the paper should include: i) modern policy gradient methods (e.g., TRPO, SAC, A3C) ii)  methods that explicitly balance exploration-exploitation (e.g., UCB-based approaches)"}, "questions": {"value": "1. Have you conducted a Pareto frontier analysis examining the exploration-exploitation trade-off? Plotting performance metrics against exploration behavior would provide more nuanced insights into when and why your method outperforms baselines.\n2. Can you explain the specific failure mode of PPO that prevents it from exploring high-advantage actions? If PPO's policy updates are guided by advantage estimates (via its objective), why doesn't it naturally converge to such actions? What specific aspect of π_pref addresses this failure that standard policy optimization cannot?\n3. How does your method compare to simply adjusting the entropy regularization coefficient (potentially to negative values for exploitation)? This seems like a more direct baseline for your approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBSZuA69Kx", "forum": "DLfNDCcF2F", "replyto": "DLfNDCcF2F", "signatures": ["ICLR.cc/2026/Conference/Submission1511/Reviewer_FUjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1511/Reviewer_FUjx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760876424657, "cdate": 1760876424657, "tmdate": 1762915788076, "mdate": 1762915788076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "dUNxIFgKRa", "forum": "DLfNDCcF2F", "replyto": "DLfNDCcF2F", "signatures": ["ICLR.cc/2026/Conference/Submission1511/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1511/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763001541466, "cdate": 1763001541466, "tmdate": 1763001541466, "mdate": 1763001541466, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an advantage-guided exploration method using a product-of-experts fusion to balance exploration and exploitation. It is shown that the resulting algorithm outperforms PPO baselines by a substantial fraction. The authors provide nice intuitions about the workings of the EE balance, and some theoretical understanding is provided regarding the soft-max nature of the preferred policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors provide some theoretical understanding regarding the nature of the preference policy, that focuses exploration on the regions that are more promising according to the current estimates of the advantage function. Nice intuitions about how the exploration-exploitation works are provided."}, "weaknesses": {"value": "Although a comparison with PPO is provided, a comparison with the state-of-the-art SAC is not provided. For instance, in Table 1-2 in reference\n\nhttps://openreview.net/forum?id=HhbHw2yInZ\n\nthe authors can see that the SAC performance is larger than the one provided in the current article. Also, apparently, the PPO result for the ant is better than the one provided by the authors. I understand that there could be some implementation differences, but then it is unclear how robust results are to these choices and whether conclusions are generally valid or not in the current paper. \n\nA comparison with the SAC baseline is critical because entropy-regularization in SAC also provides an exploration-exploitation balance that persists over the learning and does not collapse.\n\nFurther, the theoretical part of the paper is not very strong. The fact that advantage-guided policies are soft-max is a direct consequence of entropy regularization, already shown in \n\nhttps://proceedings.mlr.press/v80/haarnoja18b\n\nIndeed, one can see that advantage-guided policy is equivalent to a Q-guided policy, which is simply a reward function regularized by a policy entropy term, with some differences in normalization. \n\nFurther, SAC already has an EE tradeoff in the form of a SoftMax policy that favors actions with larger Q."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5PwxGfsC6T", "forum": "DLfNDCcF2F", "replyto": "DLfNDCcF2F", "signatures": ["ICLR.cc/2026/Conference/Submission1511/Reviewer_Pgag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1511/Reviewer_Pgag"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503336801, "cdate": 1761503336801, "tmdate": 1762915787884, "mdate": 1762915787884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an RL algorithm called PrefPoE based on PPO that samples action from a product of normal distribution of normal PPO and a Boltzmann distribution constructed with advantages."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance of PrefPoE is very strong compared to vannila PPO on the environments of choice.\n2. The motivation is very clear"}, "weaknesses": {"value": "1. The motivation and background of the problem is clearly stated, however, the related work part does include the papers that are directly related to this problem:\n* **Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning (2021), by Whitney et al.**: This paper is most directly related to this PrefPoE, which uses the same \"fused distribution\" to sample actions. The only difference is that  they use a curiosity-based exploration policy for exploration instead of the entropy regularized exploration policy used by PrefPoE.\n* **Hyper: Hyperparameter Robust Efficient Exploration in Reinforcement Learning (2025), by Wang et al.**: This paper shares the same motivation but uses two phases rollout to explore the high-reward region instead of sampling from a fused distribution. \n* **Decoupling Exploration and Exploitation in Reinforcement Learning (2021), by Schäfer et al.**\nThe authors should discuss the papers mentioned above, and there may be other papers also related on preventing the over-exploration.\n\n2. The shown results are strong, but clearly **insufficient to conclude that PrefPoE is a good exploration strategy**. The only conclusion I managed to conclude is the following: **PrefPoE is better than vanilla PPO on dense reward environment**.\nThe choice of environments in this work is biased: HalfCheetah and Ant are two environment with most dense reward in MuJoCo domain, as there are less absorbing / terminal state compared to Hopper, Walker and Humanoid. And the experiments on Cartpole, LunarLander and FrozenLake are even simpler."}, "questions": {"value": "1. Whether the algo can perform well on sparse-reward environments? As the authors claimed PrefPoE is a good **exploration** strategy, it is necessary to prove it can explore in hard problems. Including the missing domains on MuJoCo (Hopper, Walker, Humanoid), as well as the sparse version of MuJoCo.\n\n2. Is the technique compatible with off-policy algorithms with guassian policy head like SAC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aSG6l316Bb", "forum": "DLfNDCcF2F", "replyto": "DLfNDCcF2F", "signatures": ["ICLR.cc/2026/Conference/Submission1511/Reviewer_JXbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1511/Reviewer_JXbX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602465507, "cdate": 1761602465507, "tmdate": 1762915787686, "mdate": 1762915787686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PrefPoE, a reinforcement learning framework that enhances exploration in policy gradient methods. PrefPoE introduces a Preference-Product-of-Experts (PoE) mechanism that fuses a main policy with a preference policy trained to focus probability mass on high-advantage actions. This results in more targeted exploration, aiming to balance exploration and exploitation more efficiently. The authors provide theoretical analysis showing that the preference policy converges to a Boltzmann distribution over advantages and that PoE fusion induces a “soft trust region.” Empirical results on standard continuous control tasks (HalfCheetah, Ant, LunarLander) and a few discrete tasks demonstrate substantial performance improvements over vanilla PPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Exploration remains a central challenge in RL, and the idea of guiding exploration based on advantage estimates is both intuitive and meaningful.\n\n2. The paper provides solid algorithmic detail and theoretical justification, making the contribution more credible.\n\n3. Experiments across multiple environments show strong performance gains and stability improvements.\n\n4. The authors analyze the contribution of core components, which helps clarify the source of performance improvement."}, "weaknesses": {"value": "1. The tested environments are mostly standard locomotion tasks (HalfCheetah, Ant, LunarLander) with relatively simple reward landscapes. PrefPoE may perform well there, but it remains unclear how it handles more complex or multimodal tasks (e.g., Humanoid or HumanoidBench tasks [1]) where local optima and sparse rewards dominate.\n\n2. The paper focuses mainly on PPO-based variants. Other exploration-focused methods (e.g., RND [2], ICM [3], or parameter noise approaches) are not compared. This makes it hard to judge whether the proposed method truly advances exploration research rather than just improving PPO training dynamics.\n\n3. PrefPoE introduces several new hyperparameters (e.g., β₁, α, λ_pref, w_pref, w_cons). The paper claims robustness but provides no systematic sensitivity analysis, which limits its practical usability.\n\n4. The paper suggests domain-agnostic applicability (to discrete and continuous actions), but the discrete experiments are minimal and shallow, leaving this claim only partially supported.\n\n[1] Sferrazza, Carmelo; Huang, Dun-Ming; Lin, Xingyu; Lee, Youngwoon; Abbeel, Pieter. HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation. arXiv preprint arXiv:2403.10506, 2024. \n\n[2] Random Network Distillation (RND):\nBurda, Yuri; Edwards, Harrison; Storkey, Amos; Klimov, Oleg. Exploration by Random Network Distillation. In 7th International Conference on Learning Representations (ICLR 2019). \n\n[3] Intrinsic Curiosity Module (ICM):\nPathak, Deepak; Agrawal, Pulkit; Efros, Alexei A.; Darrell, Trevor. Curiosity-driven Exploration by Self-supervised Prediction. In International Conference on Machine Learning (ICML 2017)."}, "questions": {"value": "1. Have you compared PrefPoE with PPO using tuned entropy coefficients or adaptive entropy schedules to ensure that improvements are not merely due to different exploration temperatures?\n\n2. Can you provide experiments on higher-dimensional and more complex tasks (e.g., Humanoid, HumanoidBench)?\n\n3. How sensitive is performance to the new hyperparameters introduced?\n\n4. How does PrefPoE compare with novelty-based exploration methods (e.g., RND, ICM) or parameter-space noise approaches?\n\n5. (Minor) Please correct the citation for vanilla PPO (line 342) — it should refer to Schulman et al., “Proximal Policy Optimization Algorithms,” 2017."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tX9ulpJoOb", "forum": "DLfNDCcF2F", "replyto": "DLfNDCcF2F", "signatures": ["ICLR.cc/2026/Conference/Submission1511/Reviewer_5UZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1511/Reviewer_5UZN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861153113, "cdate": 1761861153113, "tmdate": 1762915787579, "mdate": 1762915787579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}