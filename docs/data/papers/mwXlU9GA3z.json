{"id": "mwXlU9GA3z", "number": 24421, "cdate": 1758356769718, "mdate": 1760363820621, "content": {"title": "Beyond Additive Noise: DP for LoRA via Random Projections", "abstract": "We study the differential privacy (DP) of low-rank adaptation (LoRA) fine-tuning. Focusing on FA-LoRA (fixed $A$, trained $B$), where a single training step is equivalent to applying a random Wishart projection to the gradients, we prove a formal $(\\varepsilon, \\delta)$-DP guarantee without explicit additive noise. The resulting privacy parameters depend explicitly on dataset sensitivity and the projection rank $r$. Moreover, the low-rank structure reduces memory and compute by design. To place these results in a broader context, we formalize the underlying projection operation as a general projection mechanism of which FA-LoRA is an instance.  This mechanism is of independent interest as random projections are ubiquitous in machine learning.", "tldr": "LoRA is differentially private", "keywords": ["Differential privacy", "random projection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b11cc23450a4865d4a87ac64fd047b75016d4487.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}