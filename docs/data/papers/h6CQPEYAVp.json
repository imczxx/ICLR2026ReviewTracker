{"id": "h6CQPEYAVp", "number": 1202, "cdate": 1756864190706, "mdate": 1759898221619, "content": {"title": "A Unified Perspective and Review on Tree Search for LLMs Test-Time Scaling", "abstract": "As the scaling of large language models (LLMs) during training reaches diminishing returns due to increased resource requirements and limited data availability, focus has shifted toward scalable test-time algorithms. Chain-of-Thought (CoT) reasoning, which enables intermediate reasoning steps in text space, has emerged as a promising approach. However, CoT’s \\textbf{single-path exploration} is susceptible to biases and underexploration of the solution space in complex problems. This survey examines advancements in tree search-based methods for enhancing LLM test-time reasoning. Beginning with foundational search algorithms like depth-first search (DFS) and breadth-first search (BFS), we trace the evolution to heuristic-guided approaches and ultimately Monte Carlo Tree Search (MCTS). We introduce \\textbf{a unified framework} for comparing these methods, focusing on their core designs, reasoning reward formulations, and targeted applications. Our analysis highlights MCTS's capability to balance exploration and exploitation, overcoming limitations of traditional inference methods like beam search. This survey establishes a foundation for advancing scalable test-time reasoning in LLMs, with implications for improving general-purpose reasoning capabilities.", "tldr": "", "keywords": ["Test time scaling", "MCTS", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5b9a264e0f495daf7857f230e13cce039171b0e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive, systematized review of tree search–based methods for test-time scaling (TTS) in LLMs. From a unified mathematical perspective, it clearly articulates the operating mechanisms and core design choices of different tree-search algorithms, with a particular emphasis on reward formulation. Within the MCTS family, it contrasts methods along key axes—node representation granularity, process- vs. outcome-based rewards, and external evaluators vs. self-evaluation—to delineate strengths, and limitations. The survey also synthesizes major applications of these techniques and outlines current challenges together with promising future directions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive and unified framing.** The paper surveys tree-search methods for TTS—from classical DFS/BFS through heuristic search to MCTS—and proposes a unified framework that foregrounds core design choices and reward formulations, providing a clear basis for cross-method comparison.\n\n2. **Well-chosen design axes with comprehensive and detailed coverage.** The review systematically contrasts, for example, process- vs. outcome-based rewards, node-granularity choices, and external evaluators vs. self-evaluation, and explains their implications for applicability, strengths, and limitations.\n\n3. **High reference value and readability.** The unified notation, consistent taxonomy, and well-structured exposition make the survey a useful reference for practitioners and an accessible entry point for newcomers."}, "weaknesses": {"value": "1. **Lack of a clear, logical hierarchy.** While the main text is reasonably readable given the page limits, the appendices—though comprehensive—are organized in a strictly top-down manner that hurts skimmability and makes it hard to quickly locate the key takeaways. For example, in Appendix D: MONTE CARLO TREE SEARCH (MCTS), the authors first classify algorithms along *Tree Node*, *Node Evaluation*, and *Evaluation Need*, and then introduce 15 algorithms sequentially. The length and ordering make it difficult to grasp each algorithm’s core ideas, overlapping applicability, and points of comparison. A more hierarchical, comparative presentation (e.g., taxonomy tables or side-by-side comparisons designed from a reader’s perspective) would improve readability.\n\n2. **Potential drift from the central focus.** I question the value of the section comparing reward in search vs. reward in RL. The paper’s stated focus is tree search for LLM test-time scaling. Rewards in search and in RL are inherently different constructs; their distinction is fairly straightforward. It is not clear why this comparison is included here or how it advances the paper’s central argument.\n\n3. **Typos and minor errors.** For instance:\n   Section 2.2, paragraph 2, line 4: “not correctness..”, \n   Section 2.3, paragraph 2, line 3: “has been taken from s. ,”. \n   Please proofread and standardize wording and punctuation throughout."}, "questions": {"value": "1. **Task-oriented MCTS guide.** Could you provide a task-based comparison and practitioner’s guide to MCTS variants—mapping representative tasks (e.g., math, code, RAG, agents) to recommended node granularity (trace / state–action / terminal), evaluation signal (PRM / ORM / hybrid), selection/backup rules, and typical hyperparameter ranges?\n\n2. **Standardized MCTS benchmarking.** Do you plan to present comparisons of MCTS methods under a standardized protocol—covering datasets, fixed token budgets, degree of parallelism and shared metrics (e.g., *Budgeted Accuracy*, *Tokens-per-Solved*) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8JIe7XzIZJ", "forum": "h6CQPEYAVp", "replyto": "h6CQPEYAVp", "signatures": ["ICLR.cc/2026/Conference/Submission1202/Reviewer_yddj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1202/Reviewer_yddj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821551127, "cdate": 1761821551127, "tmdate": 1762915705957, "mdate": 1762915705957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a survey on tree search methods for scaling test-time compute with LLMs. The paper begins by providing introductory descriptions of various tree search methods, including uninformed search, informed search and MCTS, as well as a short section contrasting test-time search with RL. The paper then dives deeper into MCTS, explaining first the problem formulation and notation used in the MCTS literature, and then the various components of MCTS (nodes, value functions, evaluation, selection and expansion etc.). Throughout this section, the authors connect MCTS to LLM search and describe the various design choices made by recent papers in the area. The paper then proceeds by describing some modern applications of MCTS for LLMs, particularly on how MCTS is used to scale test-time compute in various LLM applications (e.g. RAG, multimodal reasoning) and to build self-improvement methods. Lastly, the authors provide a brief overview of informed search with LLMs (e.g. Tree-of-Thoughts and A* search)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The main paper is well-written and highly accessible, even for someone without prior experience working on LLM tree search research. I believe it makes for a good introductory read for someone new to the research area. Importantly, the authors do a good job connecting tree search back to other active areas of LLM research (e.g. self-improvement, scaling test-time compute), so it is immediately clear why one should care about this topic.\n- The authors scout out a good selection of relevant work and use the algorithms introduced in these works to highlight various aspects of MCTS. This paper provides the reader with a good set of starting works to begin their own in-depth exploration of MCTS.\n- The appendix contains more detailed explanations of various algorithms and design choices that were introduced in the main text. I found reading this to be especially enlightening."}, "weaknesses": {"value": "While this work may help researchers less familiar or even unfamiliar with LLM tree search/MCTS literature understand the basics, I believe that good survey papers should be useful not only to this group of readers. Rather, survey papers can also introduce useful novel insights that would be helpful even to active researchers in the area without needing to introduce novel algorithms and artefacts, for example by:\n1. Unifying disparate results, evidence and arguments found across many different papers and presenting them in a cohesive and structured way.\n2. Conducting meta-analysis on top of existing works to unlock new insights that are absent from the individual papers e.g. pointing out unfilled gaps in the literature, inconsistencies in currently-accepted experimental protocols etc. \n3. Conducting small experiments that are missing in prior works that may help us better understand how various methods compare.\n\nSuch insights are largely absent in this survey. Take, for example, the section comparing ORMs and PRMs for evaluation. Here the authors merely describe the superficial difference between the two approaches and cite some papers that use these models for MCTS without necessarily describing existing evidence for why and when one might be better than the other. A researcher who has read this survey may now have a refreshed memory of what PRMs/ORMs are and might have an updated list of works to refer to, but gained little additional insight that would be helpful to their own new research in the field.\n\nOn the actual content of the paper, I can see several gaps that I think could be worth filling:\n- Compute trade-offs: tree search algorithms/MCTS are very expensive to run. I think it could be useful including some discussion on (1) when these algorithms are likely to be useful vs. other, potentially cheaper inference-time algorithms (2) what the main \"knobs\" that we can turn are for trading compute and accuracy, and which knobs we should turn when.\n- \"Splitting\" text into states: when doing MCTS with LLMs, we need to define what a state or a partial trajectory looks like. Is it a single token? A sentence? A \"reasoning step\"? I think it could be helpful to discuss the trade-offs associated with various design choices here.\n- What are some characteristics of the kinds of problems/environments where MCTS may be most helpful? For example, I would guess that MCTS is more helpful when using LLMs in structured, grid-like game environments than when using LLMs to write emails. Knowing this could be helpful for casting light onto when MCTS/tree search should be used vs. other algorithms.\n- What are the trade-offs of MCTS vs. informed tree-search algorithms e.g. tree-of-thoughts?"}, "questions": {"value": "Despite my lengthy \"Weaknesses\" section, I do appreciate the utility of this paper as introductory material for researchers new to this line of research, and I do genuinely believe this to be very impactful. I am also somewhat uncertain on how we should consider the impact of survey papers in the main conference track (is being introductory material enough for an acceptance here, or are we looking for more?). I am therefore currently recommending this for a borderline accept, with a low confidence score. I look forward to engaging more with you and the other reviewers on this matter."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6IOn7iWAhD", "forum": "h6CQPEYAVp", "replyto": "h6CQPEYAVp", "signatures": ["ICLR.cc/2026/Conference/Submission1202/Reviewer_giMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1202/Reviewer_giMx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869793556, "cdate": 1761869793556, "tmdate": 1762915705654, "mdate": 1762915705654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This survey organizes test-time scaling for LLM reasoning around tree-search methods, moving from classical BFS/DFS to heuristic search and especially MCTS, and argues for a unified way to compare them. It distinguishes “reward as a search signal” (instance-level, transient) from reward in RL (parameter updates), then introduces a consistent notation and taxonomy spanning node granularity (trace/state-action/terminal), evaluation locus (process- vs outcome-based rewards), evaluator design (external models vs self-evaluation), composite rewards, and common MCTS adaptations to selection/expansion/backprop."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a clear, well-scoped introduction to classical search methods (BFS, heuristic search, MCTS) and surveys a broad range of recent work; the background is comprehensive and accessible.\n\n2. Figure 1 is an effective and novel visualization that juxtaposes the well-known axes of training-time scaling with their test-time counterparts.\n\n3. It provides a useful taxonomy for “how to train models to enable search,” contrasting process vs outcome rewards and external evaluators vs self-evaluation, and noting multi-critic compositions—giving readers concrete design axes for evaluator training."}, "weaknesses": {"value": "1. While the survey highlights fragmentation in evaluation protocols and benchmarks, the main text primarily describes methods and stops short of proposing a unifying evaluation framework or compiling compute-normalized results to enable apples-to-apples comparisons.\n\n\n2. Figure 2 reads mostly as a chronology of publications; in its current form it contributes limited insight beyond timing.\n\n\n3. Test-time compute is discussed conceptually, but the first 10 pages do not provide a practical recipe for compute accounting (e.g., tokens/sec or FLOPs, wall-clock, memory) or cost-normalized reporting that would make cross-paper comparisons actionable."}, "questions": {"value": "1. I am a bit fuzzy on Table 1 (in general, how you define / differentiate states and actions for LLM. So if you consider action as your policy (LLM) generating tokens, then do you consider states as tokens generated so far? To this end, this framing is different from RL in the sense that models is not interacting with an environment, which determines the states (based on action). Are you saying the state space is the same action space? It is clearly not the standard RL, are there any references to validation this formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pGOE4DQt0x", "forum": "h6CQPEYAVp", "replyto": "h6CQPEYAVp", "signatures": ["ICLR.cc/2026/Conference/Submission1202/Reviewer_f1Jb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1202/Reviewer_f1Jb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968011356, "cdate": 1761968011356, "tmdate": 1762915705469, "mdate": 1762915705469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}