{"id": "ScXx64OWus", "number": 22748, "cdate": 1758334975161, "mdate": 1762960590101, "content": {"title": "TextCAM: Explaining Class Activation Map with Text", "abstract": "Deep neural networks (DNNs) have achieved remarkable success across domains but remain difficult to interpret, limiting their trustworthiness in high-stakes applications. This paper focuses on deep vision models, for which a dominant line of explainability methods are Class Activation Mapping (CAM) and its variants working by highlighting spatial regions that drive predictions. We figure out that CAM provides little semantic insight into what attributes underlie these activations. To address this limitation, we propose TextCAM, a novel explanation framework that enriches CAM with natural languages. TextCAM combines the precise spatial localization of CAM with the semantic alignment of vision–language models (VLMs). Specifically, we derive channel-level semantic representations using CLIP embeddings and linear discriminant analysis, and aggregate them with CAM weights to produce textual descriptions of salient visual evidence. This yields explanations that jointly specify where the model attends and what visual attributes likely support its decision. We further extend TextCAM to generate feature channels into semantically coherent groups, enabling more fine-grained visual–textual explanations. Experiments on ImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and interpretable rationales that improve human understanding, detect spurious correlations, and preserve model fidelity.", "tldr": "TextCAM improves Class Activation Map (CAM) by explaining not only where models focus but also what attributes matter.", "keywords": ["Explainable AI", "Interpretability", "Class Activation Mapping", "CAM", "CLIP"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7d54fb9a577243931008f31cdf07b2e21a26fdac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tries to explain the underlying features a neural network looked at using natural language, aimed as an extension to CAM like techniques. The method dissects the dataset based on activation of the specific layer being probed, by projecting the nd layer activation into a single dimension. Using this activation score, top M scoring samples (positive) and least M scoring samples (negative) are selected. CLIP embeddings are computed for those activations, then LDA  finds a vector that divides the positve and negative samples in the CLIP space, which is then weighted with the activation response and then with the weights from CAM like techniques. Then the embedding is matched with a possible natural language concept/explanation, to generate the final natural language explanation. Experiments are performed on Imagenet, CUB-200 and CLEVR datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The intuition that CAM like saliency maps dont uncover specific features used by the network is valid, and the use of natural language to try to explain these features is valid to an extent. \n2. The paper details implementation and design choices aptly.\n3. The writing is mostly consistent"}, "weaknesses": {"value": "1. Novelty: This is not the first paper that tries to explain the  using natural language, previous works have tried to replace a concept bottleneck layer to produce textual explanations[1], while this paper extends the natural language explanations to intermediate layers, CLIP is designed to represent the vision-language co-space, it may not do well to represent complex intermediate patterns generated by the neural network, therefore, this method is useful usually only in the final layers, which is not novel.\n\n2. \"Analyze Per-channel Response Patterns.\" step in section 3.2 doesnt consider superposition[2].  The technique splits the entire dataset by a score that is basically how much did the layer activate for that particular sample, but a layer doesnt activate for just one feature, therefore just using this activation score on all the classes together would lead to multiple features being present in positive M. Therefore, since the binary positive and negative sets dont represent linearly separable classes in the clip space, LDA is likely to generate instable projection vectors.\n3. Like the paper correctly states, these textual features are mostly guesses, and are not reliable. When CAM fails, these explanations fail, moreover, its not certain that when CAM succeeds, the text generated to support the CAM is correct, especially given point 2. The paper tries to ablate this in Section 4.3.1 but the data is mostly toy data. The paper can try to generate subtle modifications to  images using diffusion models and then try counterexample experiments but given point 2, the method is still largely unreliable.\n4. As discussed in point 1, this method is mostly useful in examining non complex features, given the simple language in the CLIP space, the uses in final layer is not that helpful because the text is automatically inferred from the image, mostly its doing a kind of zero shot classification in the last layer.\n\nReferences:\n1. Yamaguchi, S., & Nishida, K. (2025). Explanation Bottleneck Models. arXiv [Cs.AI]. Retrieved from http://arxiv.org/abs/2409.17663\n2. https://transformer-circuits.pub/2022/toy_model/index.html"}, "questions": {"value": "1. How does the paper exactly compute per sample projection ? Or does it just use the dataset wide LDA projection,?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJWps2swax", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_XDQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_XDQu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761326801313, "cdate": 1761326801313, "tmdate": 1762942370605, "mdate": 1762942370605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "vIPdEZLQfX", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762960589190, "cdate": 1762960589190, "tmdate": 1762960589190, "mdate": 1762960589190, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TextCAM, an explainability framework that enriches Class Activation Maps (CAMs) with natural language descriptions to improve the interpretability of deep vision models. While traditional CAM-based methods highlight the spatial regions influencing a model’s decision, they fail to clarify what visual attributes drive these activations. TextCAM bridges this semantic gap by combining the spatial localization of CAM with the semantic alignment capabilities of vision–language models (VLMs) like CLIP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n- Modular, training-free design: This means that it can be directly applied to existing models with almost no additional cost.\n\n- TextCAM not only provides global interpretation, but also subdivides saliency maps into multiple semantic regions (such as \"long neck\", \"feathers\", \"legs\"), providing finer grained explanatory visualizations.\n\n- It can be used in conjunction with any CAM method (Grad CAM, Layer CAM, Eigen CAM, etc.) and supports CNN and Transformer structures."}, "weaknesses": {"value": "Weakness:\n\n- It seems that using language to assist vision visualization is a very new idea or insight (of course, if the author has evidence, they can list it to convince me).\n\n- The author mentioned that Grad-CAM-like procedures are unstable (**w**) in the Transformer structure, causing fluctuations in TextCAM interpretation on ViT. What is the deeper reason for this? Have you tried using VLLM representation?\n\n- Although TextCAM is training free, in actual deployment, it requires perform LDA (O (channels x feature-dim ² complexity) on each channel. Performing sparse optimization (ADMM solution) on a large vocabulary has a high computational cost and is not suitable for large-scale online interpretation scenarios.\n\n- It seems that the paper did not provide standard CAM explanatory evaluation indicators such as fidelity metrics (e.g., delivery/insertion AUC) or localization accuracy?"}, "questions": {"value": "Can you explain the main difference of this work between [1]? For example, the significant innovation or advantages of this article compared to [1]. I don't seem to see any significant innovation on top of it, as you emphasized at the beginning of the article using language to enrich CAM. This is a big confusion for me. There should be a lot of work related to using text/language to assist visualization (I may only remember seeing an article from 2021). Compared to them, your emphasis on using language to enhance innovation seems a bit weak? Also, can you compare with them? Showcasing one's strengths from certain dimensions?\n\nBeyond CLIP, why not try using a VLLM (Vision Large Language Model)? CLIP is just an instance of VLMs. Does your method only compatible with CLIP or is the model-agnostic?\n\n\n> [1] Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers 2021"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5wev3HxoN", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_uU83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_uU83"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671109794, "cdate": 1761671109794, "tmdate": 1762942370399, "mdate": 1762942370399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed an language argumentation method for CAM(class activation mapping) based explanation models, which is desired in current explainable AI techniques. The basic idea is to make use of the element-wise activation map of CAM as semantic units and adopt CLIPs to generate text explanations based on decomposed images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Although somehow primitive, the idea of using CAM to decompose image elements are interesting and promising for generate rich semantic explanations.\n\nThe paper proposed novel methods for optimizing text selection from CLIP results for purpose of explanation."}, "weaknesses": {"value": "Without proper processing, the CAM channels generally does not have clear semantic meaning, which hindered the usefulness of the approach. As we can see from the results, the generated text comprises high portion of errors. It would be beneficial to introduce image segmentation and knowledge bases (e.g., ontology) to improve the results. \n\nThere are already some similar works which uses large language models to interpret the image explanation results (e.g., MONET, Nature Medicine 2024, 1154). The authors should try comparing with them.\n\nThe survey on xAI methods is limited. Especially, counter-factual generation methods and concept-based explanation methods are not mentioned. Both also fit your framework. It would be potentially very promising to incorporate concept-based method to enhance your approach."}, "questions": {"value": "Why you encode only vocabulary at the first step? This will incur significant loss of semantic information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rHBvVkf7if", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_p88y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_p88y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974044763, "cdate": 1761974044763, "tmdate": 1762942370062, "mdate": 1762942370062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TextCAM, a method that enhances traditional Class Activation Mapping (CAM) by integrating natural language descriptions. TextCAM first compute channel-level semantic representations via CLIP embeddings and linear discriminant analysis. Then a sparse text selection and grouping strategy is introduced to generate saliency maps with more interpretable text descriptions from text-annotated saliency groups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The grouping of saliency maps based on semantic concepts is a useful extension, providing more detailed and organized insights into what the model highlights\n\n2. The paper demonstrates results across multiple datasets"}, "weaknesses": {"value": "1. Lack of Technical Depth (Main concerns):\n\nFor the technique on \"Generate Semantic Representation\", it is extended from the GAP and CAM, while relying heavily on pre-trained CLIP embeddings and standard LDA techniques. The contribution often feels incremental rather than groundbreaking;\n\nSparse optimization for text selection is built upon text-driven visual-language relevance estimation, which is common-used on both CLIP-based and GPT-based methods.\n\n2. Over-Reliance on CLIP:\n\nTextCAM is tied to the quality of CLIP's pre-trained embeddings, which raises concerns about its applicability to tasks or domains where CLIP's semantic space is less effective. The use of CLIP embeddings introduces a dependency, how about GPT-based VLMs?\n\n3. The use of LDA for channel-wise analysis and sparse optimization for text selection introduces computational overhead, lack of analysis on large-scale datasets\n\n4. The vocabulary construction process lacks rigor and seems arbitrary, how can we establish it?\n\n5. What are the failure cases of TextCAM? Are there scenarios where it produces misleading or incoherent explanations?"}, "questions": {"value": "1. Strengthen the analysis on technical contributions\n\n2. Test the method on more challenging datasets and tasks, such as multi-object scenes, occlusions, or ambiguous contexts.\n\n3. systematically identify and address scenarios where TextCAM fails to provide meaningful explanations.\n\n4. If possible, include human evaluations to validate the interpretability of the textual explanation (not necessary)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6HKingPH1d", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_xjVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_xjVc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978048405, "cdate": 1761978048405, "tmdate": 1762942369840, "mdate": 1762942369840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Class activation maps (CAM) provide little insight into what attributes underlie activations. In this work, the authors combine vision-language models with CAM in order to produce textual descriptions of salient visual evidence. Results show that the proposed approach yields accurate descriptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses an important problem - improving explainability of models by generating text descriptions from class activation maps\n- The paper is well-written and the methods are clearly explained"}, "weaknesses": {"value": "- **Insufficient analysis:** Quantitative results are limited, and whereas results in Section 4.3.1 are compelling, evaluations are performed on a small set of synthetic settings. This is evidenced by the fact that the accuracy of the proposed method is 100%. The paper could have benefitted from more complex quantitative evaluations to precisely highlight the strengths and limitations of the proposed method.\n    - While qualitative results in Section 4.2 are interesting, it is difficult to know whether the generated attributes are accurate.\n    - Do the trends observed on the CLEVR benchmark in Section 4.3.1 hold across different choices for the CAM method?\n    - It appears that all of the evaluations in Section 4 (with the exception of Section 4.4) are performed using a ResNet-50 model. Do trends hold under different choices for the CNN?\n- **Method design:** Design choices for the proposed method are not sufficiently evaluated or ablated, making it challenging to determine the utility of each contribution in Section 3."}, "questions": {"value": "Questions are listed above under weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ahWHwyzn59", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_71st"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_71st"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978293107, "cdate": 1761978293107, "tmdate": 1762942369589, "mdate": 1762942369589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The TextCAM paper proposes enriching traditional Class Activation Map (CAM) visual explanations with natural-language descriptions derived from vision–language models (VLMs) such as CLIP. By mapping channel-level activations to semantic embeddings using Linear Discriminant Analysis (LDA), TextCAM generates textual explanations corresponding to salient image regions. The paper also introduces a grouping strategy to cluster channels into semantically coherent saliency groups, offering fine-grained, text-annotated visual explanations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important gap in CAM-based interpretability by adding semantic meaning to purely visual heatmaps.\n\nThe method is training-free and can be combined with any existing CAM variant, making it broadly applicable.\n\nIntegration of CLIP embeddings is a reasonable and practical way to connect feature maps with human-understandable attributes."}, "weaknesses": {"value": "The key idea, mapping latent features or channels to textual concepts using a joint vision–language embedding space, has already been explored in earlier works such as LaViSE (CVPR 2022) and other filter-level semantic alignment methods. TextCAM’s contribution lies primarily in applying this concept to CAM-based spatial localization, rather than developing a new alignment or representation mechanism. LaViSE also provided unsupervised textual descriptions for filters and bias analysis applications, which conceptually overlap with TextCAM’s goal. The omission of this related work makes the contribution appear less grounded and potentially redundant.\n\nThe combination of CAM weights with CLIP-based LDA projections is empirically motivated but lacks formal justification. The sparse optimization and grouping procedures are heuristic and not well-analyzed in terms of stability, interpretability, or robustness.\n\nThe experiments primarily show qualitative visualizations. Claims of improved interpretability and model faithfulness are not supported by objective metrics (e.g., human studies, consistency scores, or statistical tests).\n\nThe figures mostly present clear or intuitive cases where the text matches the highlighted region, but do not include examples where the model fails or produces nonsensical descriptions. The authors also need to provide some error cases."}, "questions": {"value": "Could you clarify the conceptual or methodological innovation that distinguishes your approach beyond combining CAM localization with CLIP-based textual projection?\n\nThe paper employs Linear Discriminant Analysis (LDA) to project channel activations into the CLIP text space. Why was LDA chosen over other semantic alignment strategies (e.g., attention-based, contrastive, or regression-based mappings)? How sensitive are the results to the number of LDA components or to the choice of semantic embedding space?\n\nThe paper presents visually convincing examples, but how does TextCAM behave when the CAM highlights ambiguous or overlapping regions, or when the model misclassifies an image? Can you provide qualitative or quantitative error analysis to show robustness in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UZLKu4mRdX", "forum": "ScXx64OWus", "replyto": "ScXx64OWus", "signatures": ["ICLR.cc/2026/Conference/Submission22748/Reviewer_QHAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22748/Reviewer_QHAv"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission22748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016015046, "cdate": 1762016015046, "tmdate": 1762942369031, "mdate": 1762942369031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}