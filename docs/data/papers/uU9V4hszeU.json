{"id": "uU9V4hszeU", "number": 22360, "cdate": 1758330032037, "mdate": 1763533705232, "content": {"title": "Unsupervised Learning for Quadratic Assignment", "abstract": "We introduce PLUME search, a data-driven framework that enhances search efficiency in combinatorial optimization through unsupervised learning. Unlike supervised or reinforcement learning, PLUME search learns directly from problem instances using a permutation-based loss with a non-autoregressive approach. We evaluate its performance on the quadratic assignment problem, a fundamental NP-hard problem that encompasses various combinatorial optimization problems. Experimental results demonstrate that PLUME search consistently improves solution quality.  Furthermore, we study the generalization behavior and show that the learned model generalizes across different densities and sizes.", "tldr": "PLUME search uses unsupervised learning with permutation-based loss to boost tabu search performance on quadratic assignment problems", "keywords": ["Unsupervised Learning", "Quadratic Assignment Problem", "Permutation-based loss", "Tabu Search"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/363c761bb4a6b1947fd0f16201116f84769fa892.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose an unsupervised learning model to optimise the Quadratic Assignment Problem. The authors propose using permutation-based loss functions that allow the characteristics of the input instances to be shared with the loss, enabling solutions to be constructed in a non-autoregressive manner. The obtained solution by the UL model is provided as starting solution to a Tabu Search algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. In combinatorial optimisation, it is well known that QAP is one of the most difficult problems. Not only that, but unlike others such as TSP, VRP, and LOP, it is not possible to represent it naturally in graph form so that it can be fed into a GNN. This means that for many years, QAP has not been on the list of papers to be optimised in the field of NCO. However, this paper addresses the challenge very satisfactorily and raises new ideas. I can only congratulate the authors on this success. I have been waiting a long time for someone to write this paper :-)\n\nS2. The paper is in general well written and easy to read."}, "weaknesses": {"value": "The paper has a number of issues that the authors should address:\nW1. The unsupervised setting in combinatorial optimisation is not that common, so I think a clearer explanation is needed in the introduction and model part.\nW2. One of the most interesting parts of the paper is the idea of permutation-based losses, but it is not entirely clear how the equations are computed. I would have liked to see a section 3.3. on this.\nW3. The related work section needs improvement:\n- There is no adequate justification for why the authors adopted a UL setting, when RL has been the norm for the last eight years.\n- There are previous studies, albeit less efficient, that have addressed QAP (in addition to the 2024 study), and I believe the authors should include them.\nW4. Considering that the authors adopt algebraic notation to represent the QAP, it is somewhat curious that they then refer to soft and hard permutation matrices, rather than doubly stochastic matrices or permutation matrices. In fact, in the metaheuristic field, QAP has been worked on using DSMs in the EDAs framework (see Santucci et al. 2024, On the use of the Doubly Stochastic Matrix models for the Quadratic Assignment Problem).\nW5. The present work could have a huge impact if they carry out an exhaustive comparison with (Tan and Mu) and with the best metaheuristic for QAP (they could use DSM-EDA by Santucci et al.), or any of those reported in QAPlib (RoTS, etc.). As it is, the only thing the authors show is the possibility of doing optimization on the QAP with UL, but not its competitiveness (this also would be interesting, although not mandatory for publishing the present work).\nW6. It is common to use u.a.r. instances in ML combinatorial optimisation, but from a realistic point of view, the instances generated are very different from what happens in reality. Perhaps the authors could consider QAPlib-style instances."}, "questions": {"value": "Any progress in the weaknesses noted above would be positive for the paper, and I would reconsider the punctuation. In addition, I have some questions related:\n\nQ1. Similar structures in heuristics field has been observed previously. The so-called GRASP is a concatenation of a constructive stochastic algorithm and a local search-based methods (like TabuSearch). Is this also applicable here?\n\nQ2. How is the training process? which is the parametrisation? How do you describe it? (related to W2)\n\nQ3. Why UL is more appropriate than RL for the QAP? Motivate properly the answer. (related to W3.1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hzNuiuPrij", "forum": "uU9V4hszeU", "replyto": "uU9V4hszeU", "signatures": ["ICLR.cc/2026/Conference/Submission22360/Reviewer_hPaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22360/Reviewer_hPaN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607138391, "cdate": 1760607138391, "tmdate": 1762942185170, "mdate": 1762942185170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "g8noEGwirD", "forum": "uU9V4hszeU", "replyto": "uU9V4hszeU", "signatures": ["ICLR.cc/2026/Conference/Submission22360/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22360/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763533704492, "cdate": 1763533704492, "tmdate": 1763533704492, "mdate": 1763533704492, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **PLUME Search** (Permutation-based Loss with Unsupervised Models for Efficient Search), a **data-driven unsupervised learning framework** to improve **search efficiency in combinatorial optimization** (CO), specifically applied to the **Quadratic Assignment Problem (QAP)**. Unlike supervised or reinforcement learning approaches that rely on labeled data or sequential decision processes, PLUME directly learns from raw problem instances using a **permutation-based loss** and a **non-autoregressive neural architecture**. It provides learned heuristic initialization for classical solvers such as **Tabu Search**, improving both efficiency and solution quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- **\\[S1] Important and general problem.** The paper tackles the **Quadratic Assignment Problem (QAP)**, a fundamental and notoriously hard combinatorial optimization problem that generalizes many practical tasks (e.g., layout design, scheduling, circuit placement). Extending data-driven optimization methods to QAP is both **technically challenging** and **practically significant**, since QAP captures a wide range of real-world assignment and matching scenarios.\n\n**\\[S2] Data-driven unsupervised approach is promising.** The proposed **unsupervised learning framework (PLUME Search)** eliminates the need for labeled solutions or costly reinforcement learning setups. By learning directly from raw problem instances, the model can serve as a **neural heuristic** or **learned search initializer**, making it a scalable and general alternative to traditional hand-designed heuristics."}, "weaknesses": {"value": "See \"Questions\" below."}, "questions": {"value": "- **\\[Q1] On novelty over Min et al. (2023).** The core idea—unsupervised learning of soft permutation matrices via the Gumbel–Sinkhorn operator—appears conceptually similar to _Min et al. 2023._ Could the authors clarify what key methodological innovations distinguish PLUME Search from this prior work beyond applying it to the QAP setting (e.g., new theoretical insight, new training objective, or substantially different model behavior)?\n\n- **\\[Q2] On disentangling learning vs. post-processing contributions.** paper integrates unsupervised learning with a strong post-hoc Tabu Search. However, prior work (e.g., _Xia, Yifan, et al. \"Position: Rethinking Post-hoc Search-based Neural Approaches for Solving Large-scale Traveling Salesman Problems.\" ICML 2024_; _Bu, Fanchen, and Kijung Shin. \"On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis.\" arXiv:2506.16732, 2025_) highlights that excessive reliance on search can obscure the learning contribution. Could the authors provide controlled experiments or ablation results that isolate how much of the observed performance gain arises from the learned model versus from Tabu Search itself? For example, how does the model perform under weaker or no post-processing, and how does it work when we apply the same post-processing to random initializations?\n\n- **\\[Q3] On alternative permutation-learning formulations.** The paper focuses exclusively on the Gumbel–Sinkhorn operator for differentiable permutation learning. Yet several recent works have proposed alternative or improved formulations, such as: _Dröge, Hannah, et al. \"Kissing to Find a Match: Efficient Low-rank Permutation Representation.\" NeurIPS 2023._ and _Nerem, Robert R., et al. \"Differentiable Extensions with Rounding Guarantees for Combinatorial Optimization over Permutations.\" NeurIPS 2024._ Could the authors discuss how their method compares theoretically or empirically to these alternatives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nTmvEwFePW", "forum": "uU9V4hszeU", "replyto": "uU9V4hszeU", "signatures": ["ICLR.cc/2026/Conference/Submission22360/Reviewer_qmbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22360/Reviewer_qmbg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761361225471, "cdate": 1761361225471, "tmdate": 1762942184747, "mdate": 1762942184747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on solving the NP-hard Quadratic Assignment Problem (QAP) in combinatorial optimization through unsupervised learning. The proposed PLUME search framework features key innovations: it adopts a permutation-based loss and non-autoregressive approach, leveraging a permutation-equivariant neural architecture with facility and location encoders to generate soft permutation matrices via the Gumbel-Sinkhorn operator, which are then decoded into hard permutations to initialize tabu search. Theoretically, the framework guarantees permutation equivariance and invariance of the QAP objective function, enabling natural generalization across different problem sizes. Experimentally, PLUME search consistently outperforms random initialization across various QAP problem sizes (100, 200) and densities, achieving improvement in initialization quality and maintaining advantages in tabu search results; it also exhibits strong cross-density and cross-size generalization, and outperforms RL-based methods in both solution quality and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The model design presented in this paper is generally sound and comprehensive, covering the key components required for addressing the research problem.\n2. The ablation studies and sensitivity analyses are thorough, providing strong support for the proposed method’s effectiveness, though, the work lacks comparisons with recent state-of-the-art (SOTA) baselines, which limits the full validation of its advantages."}, "weaknesses": {"value": "1. The research motivation is not sufficiently solid. For instance, the paper claims that the studied problem leads to \"significant computational expense when building the training dataset.\" Yet, taking the Traveling Salesman Problem (TSP) as a reference, heuristic algorithms like LKH (Lin-Kernighan-Helsgaun) can generate labels efficiently. For the Facility Location Problem  focused on in this paper, it would be better to more comprehensively justify the ability of unsupervised-based approach compared to supervised.\n\n2. The paper lacks comparisons with existing works that also adopt unsupervised learning or Gumble-Sinkhorn-based for combinatorial optimization problems—especially for the Quadratic Assignment Problem (QAP), such as the studies cited in [1, 2]. \n\n3. A critical issue is that the baselines used for comparison are overly weak. In most experiments, only a random baseline is employed, which fails to demonstrate the proposed method’s competitiveness against meaningful benchmarks and greatly undermines the persuasiveness of the experimental results. Additionally, supplementing comparisons with representative supervised or reinforcement learning (RL)-based solvers would help contextualize the performance of the proposed unsupervised approach and highlight its relative merits.\n\n4. The notation in Figure 2 is ambiguous, making it difficult to fully understand the model design purely from figure. For example, the transformation \"3d→d\" is not clearly explained, as the figure and its caption provide insufficient context. Furthermore, Equation 3 and Equation 9 appear to be duplicated, requiring clarification or correction.\n\n\n[1] Wang, Runzhong, et al. \"Unsupervised learning of graph matching with mixture of modes via discrepancy minimization.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.8 (2023): 10500-10518.\n\n[2] Wang, Runzhong, et al. \"Towards one-shot neural combinatorial solvers: Theoretical and empirical notes on the cardinality-constrained case.\" The Eleventh International Conference on Learning Representations. 2022."}, "questions": {"value": "1. What is rational of design of equation 3? Does the positional lifting model captures the pairwise positional relationship?\n2. Does the design of flow encoder in equation 4 seems like a GNN with fully-connnected graph ?\n3. What is the specific implementation of \"msg\" operation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N1omCJjvFK", "forum": "uU9V4hszeU", "replyto": "uU9V4hszeU", "signatures": ["ICLR.cc/2026/Conference/Submission22360/Reviewer_qLh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22360/Reviewer_qLh7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889399984, "cdate": 1761889399984, "tmdate": 1762942184168, "mdate": 1762942184168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PLUME, an unsupervised learning framework for the Quadratic Assignment Problem (QAP). The approach is permutation-invariant and is trained via unsupervised learning, thereby avoiding the need to compute potentially expensive labels.  The model employs a permutation-equivariant encoder that embeds the flow and distance matrices symmetrically, ensuring invariance to label permutations.  Computationally, the unsupervised learning method demonstrates efficacy over random solutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Motivation**: The unsupervised learning approach is well motivated and sound, especially given that collecting labels for supervised learning is costly, and RL approaches tend to be challenging to train.  \n- **Results**: Overall, the reported results demonstrate that the unsupervised approach achieves relatively good quality solutions and is effective as a seed in Tabu search.  There are still some weaknesses in the results that I highlight below."}, "weaknesses": {"value": "- **Benchmarks**: The authors currently evaluate their approach on randomly generated QAP instances with $n=100,200$.  However, there is no discussion on how the difficulty of these instances relates to the literature, e.g., QAPLib (some of which have generation schemes making them usable in a data-driven setting).  Given that the primary focus of this paper is on QAP, a more comprehensive evaluation of standard benchmark instances is needed.  \n- **Baselines**: The baselines are relatively weak, with Table 1 comparing random solutions, and Tables 2 and 3 reporting solution quality compared to Tabu search run with random initialization, given the offline time required to train the model and the inference time, comparing against more robust seeding/initialization should be done.\n- **Results**: The authors report averaged results over the instances of each size.  However, this doesn't convey the statistical significance of the results; some distributional information, e.g., box plots, needs to be reported to compare with the best-known solutions.  In addition, the authors report inference time, but do not report training time or loss curves, which are absolutely needed. The authors do not explain how the training parameters were selected.  There should be an ablation on these. \n- **Clarity on Contributions**: Given that the approach shares similarity with [1], a more precise delineation of the contributions of this work is needed.  This paper would benefit in terms of readability from a Contributions section.  \n\n## References\n- [1] Yimeng Min and Carla Gomes. Unsupervised learning permutations for tsp using gumbel-sinkhorn\noperator. In NeurIPS 2023 Workshop Optimal Transport and Machine Learning, 2023"}, "questions": {"value": "- How long does the unsupervised approach take to train?\n- How much training data is required?\n- How were the parameters selected, and is the training sensitive to hyperparameters?\n- Can the approach be generalized to assignment problems where the number of facilities and locations differ?\n- Do the authors think that this approach can be generalized more broadly to other classes of CO problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yxr2yzoXgl", "forum": "uU9V4hszeU", "replyto": "uU9V4hszeU", "signatures": ["ICLR.cc/2026/Conference/Submission22360/Reviewer_rxTq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22360/Reviewer_rxTq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952722673, "cdate": 1761952722673, "tmdate": 1762942183893, "mdate": 1762942183893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}