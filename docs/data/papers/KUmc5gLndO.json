{"id": "KUmc5gLndO", "number": 11715, "cdate": 1758203285162, "mdate": 1759897559197, "content": {"title": "BM-CL: Bias Mitigation through the lens of Continual Learning", "abstract": "Biases in machine learning pose significant challenges, particularly when models amplify disparities that affect disadvantaged groups. Traditional bias mitigation techniques often lead to a {\\itshape leveling-down effect}, whereby improving outcomes of disadvantaged groups comes at the expense of reduced performance for advantaged groups. This study introduces Bias Mitigation through Continual Learning (BM-CL), a novel framework that leverages the principles of continual learning to address this trade-off. We postulate that mitigating bias is conceptually similar to domain-incremental continual learning, where the model must adjust to changing fairness conditions, improving outcomes for disadvantaged groups without forgetting the knowledge that benefits advantaged groups. Drawing inspiration from techniques such as Learning without Forgetting and Elastic Weight Consolidation, we reinterpret bias mitigation as a continual learning problem. This perspective allows models to incrementally balance fairness objectives, enhancing outcomes for disadvantaged groups while preserving performance for advantaged groups. Experiments on synthetic and real-world image datasets, characterized by diverse sources of bias, demonstrate that the proposed framework mitigates biases while minimizing the loss of original knowledge. Our approach bridges the fields of fairness and continual learning, offering a promising pathway for developing machine learning systems that are both equitable and effective.", "tldr": "", "keywords": ["neural networks", "continual learning", "bias mitigation", "leveling-down effect"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/218940fd448be226d6c92d40b88f3324e2b7dde4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper advocates a novel BM-CL framework that reinterprets bias mitigation as a continual learning (CL) problem. The authors draw parallels between the leveling-down effect in fairness interventions (where improving disadvantaged groups harms advantaged ones) and catastrophic forgetting in CL. The proposed BM-CL combines standard bias mitigation methods (GroupDRO, ReSample) with CL techniques (LwF and EWC). Experiments on Waterbirds, CelebA, and CheXpert datasets show improvements in worst-group accuracy with minimal degradation in advantaged groups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The core idea of framing bias mitigation as continual learning is conceptually elegant. Drawing a formal analogy between the leveling-down effect and catastrophic forgetting provides a new view that could inspire cross-fertilization between fairness and lifelong learning research.\n\n+ BM-CL integrates easily with existing bias-mitigation pipelines by adding a CL-style regularization term. Hence the approach is clear and reproducible, with minimal additional complexity for understanding or implementation.\n\n+ Evaluations on multiple benchmark datasets (spurious correlation, demographic bias, medical imaging) are comprehensive and show consistent trends. The code availability and ablation studies increase reproducibility."}, "weaknesses": {"value": "- While providing a new lens for understanding fairness-aware ML, the proposed method is largely a straightforward combination of existing techniques (LwF, EWC, and bias-mitigation baselines) with limited algorithmic innovation. The design omits potential drawbacks (e.g., stability-plasticity conflicts under multi-attribute settings, or extension to non-image modalities).\n\n- No new theoretical framework or analytical insight beyond the analogy to forgetting is developed. There also lacks justification for why continual-learning regularizers specifically address the fairness-accuracy trade-off beyond empirical correlation.\n\n- While the results show improvements, the absolute differences over strong baselines (GroupDRO, ReSample) are often small (~1-2% balanced accuracy). Quantitative gains are marginal. \n\n- It is unclear whether these improvements are statistically significant or robust across seeds and datasets."}, "questions": {"value": "1. How does BM-CL perform when the number of demographic groups increases or when group definitions overlap (intersectional fairness)?\n\n2. Can compare BM-CL against more recent fairness methods (e.g., invariant risk minimization, domain generalization-based fairness) to contextualize competitiveness?\n\n3. Could the authors provide a theoretical or empirical justification for why the continual-learning regularization term directly mitigates \"leveling-down\" beyond preventing weight drift?\n\n4. How sensitive are results to the choice of hyperparameters such as λ (regularization strength) and pretraining ratio ρ across datasets? Do these require dataset-specific tuning?\n\n5. Have examined the effect of CL regularization on model calibration or other fairness metrics (equalized odds, demographic parity, delta EO or DP)?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "The ethical framing (\"positive-sum fairness\") is interesting but not substantiated with theoretical or societal analysis."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "afK0PrzzAK", "forum": "KUmc5gLndO", "replyto": "KUmc5gLndO", "signatures": ["ICLR.cc/2026/Conference/Submission11715/Reviewer_2D32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11715/Reviewer_2D32"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693430609, "cdate": 1761693430609, "tmdate": 1762922757629, "mdate": 1762922757629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BM-CL, framing bias mitigation's \"leveling-down\" effect as a Continual Learning \"forgetting\" problem. It uses a two-stage approach: ERM training followed by fine-tuning with a bias mitigation loss and a CL regularizer (LwF/EWC) to preserve performance on initially advantaged groups."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a critical and practical problem in fairness. The \"leveling-down\" effect is a major barrier to deploying fair models in high-stakes settings where harming any group is unacceptable."}, "weaknesses": {"value": "1.Limited Novelty / Incremental Contribution: The core idea of linking \"leveling-down\" to \"forgetting\" is an interesting perspective, but the proposed solution (BM-CL) primarily combines existing, off-the-shelf techniques: standard bias mitigation losses (GroupDRO, ReSample) with standard CL regularizers (LwF, EWC) in a sequential manner. There is little fundamental algorithmic innovation presented. The contribution feels incremental rather than introducing a truly new mechanism for bias mitigation.\n\n2.Marginal Empirical Improvement: Even when compared against the selected baselines (which, as noted previously, might not be the most appropriate ones for evaluating FWH), the performance gains offered by BM-CL are not substantial. While it often reduces the leveling-down effect (LDE), this sometimes comes at the cost of lower worst-group improvement compared to simpler baselines (e.g., compare GroupDRO-LwF vs GroupDRO on CheXpert). The improvements in balancing the trade-off are marginal and may not be significant enough to justify the method's adoption.\n\n3. Unjustified Complexity vs. Benefit: The BM-CL framework introduces significant complexity compared to standard ERM or single-stage bias mitigation methods. It requires a two-stage pipeline, identification of best/worst groups, and tuning of additional CL-specific hyperparameters (p, λ). Given the limited novelty and marginal performance gains, the cost-benefit trade-off appears unfavorable. The paper does not sufficiently demonstrate that this added complexity yields a practically meaningful advantage."}, "questions": {"value": "1.Can the authors articulate the core technical novelty of BM-CL beyond the combination of existing CL regularization techniques with existing bias mitigation losses? What is fundamentally new in the mechanism proposed?\n\n2.Given that the empirical improvements over baselines appear marginal (Table 1), how do the authors justify the significant added complexity (two stages, extra hyperparameters) of the BM-CL framework from a practical cost-benefit perspective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fz66iH4CIy", "forum": "KUmc5gLndO", "replyto": "KUmc5gLndO", "signatures": ["ICLR.cc/2026/Conference/Submission11715/Reviewer_23mH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11715/Reviewer_23mH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801562989, "cdate": 1761801562989, "tmdate": 1762922757129, "mdate": 1762922757129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BM-CL, a novel framework that addresses the \"leveling-down effect\" commonly found in bias mitigation techniques. The core contribution is the reinterpretation of bias mitigation as a task-incremental continual learning problem. Experiments demonstrate that, compared to baseline methods, this approach effectively improves worst-group accuracy while significantly minimizing the drop in best-group performance , promoting positive-sum fairness without designing new complex architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It cleverly utilizes existing CL techniques in a novel way without requiring additional datasets or complex model architectures, which helps promote positive-sum fairness.\n2. The paper reinterprets the bias mitigation problem as a task-incremental continual learning problem. This is a highly novel perspective that offers a new approach to addressing the issue.\n3. While traditional bias mitigation methods often sacrifice the performance of advantaged groups, this framework leverages CL principles to improve outcomes for disadvantaged groups while effectively preserving knowledge for advantaged groups, thereby avoiding performance degradation."}, "weaknesses": {"value": "1. Although the conceptual framework is innovative, the novelty of the contribution is limited. It primarily involves adapting existing methods to the problem posed by the paper, rather than proposing an entirely new algorithm to solve it.\n2. The method assumes that explicit group labels are available during training, which may limit its broader applicability.\n3. The paper does not provide theoretical guarantees for fairness convergence. The proposed loss function is effectively a combination of two conflicting objectives. While the paper empirically demonstrates that a good balance point can be found, it does not prove that such a balance point must exist or can always be achieved. Furthermore, regarding the leveling-down boundary, although the experimental results show low leveling-down values, there is a lack of theoretical analysis to indicate how severe the worst-case leveling-down might be on more complex datasets."}, "questions": {"value": "1. The ablation study indicates that the regularization strength $\\lambda$ is a critical hyperparameter. Beyond conducting a grid search on the validation set, is there a more principled or intuitive method for setting this parameter?\n2. In Table 1, you compare BM-CL against ERM, GroupDRO, JTT, and other methods. Could you include comparisons against more recent, SOTA bias mitigation algorithms as baselines? Including these comparisons would allow for a more comprehensive evaluation of the BM-CL framework's effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bj5Ehiuomc", "forum": "KUmc5gLndO", "replyto": "KUmc5gLndO", "signatures": ["ICLR.cc/2026/Conference/Submission11715/Reviewer_VVyQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11715/Reviewer_VVyQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842958452, "cdate": 1761842958452, "tmdate": 1762922756782, "mdate": 1762922756782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bias Mitigation through Continual Learning (BM-CL), a novel framework that reinterprets the challenge of algorithmic fairness as a domain-incremental continual learning problem. The key contribution is a methodology that leverages techniques like Learning without Forgetting and Elastic Weight Consolidation to incrementally adapt models to new fairness objectives. This approach specifically mitigates the common leveling-down effect by improving outcomes for disadvantaged groups without degrading performance for advantaged groups, thereby balancing fairness with the preservation of the model’s original knowledge, as validated on diverse synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The primary strength of this work is its novel conceptual contribution of bridging the fields of fairness and continual learning.\n\nS2: By reinterpreting bias mitigation as a domain-incremental or task-incremental continual learning problem, this work leverages techniques like Learning without Forgetting and Elastic Weight Consolidation to incrementally adapt models to new fairness objectives.\n\nS3: The effectiveness of BM-CL is validated across multiple, diverse real-world image datasets such as Waterbirds, CelebA, and CheXpert."}, "weaknesses": {"value": "W1: Methods such as EWC and LwF can struggle with scalability to a very large number of fairness tasks or domains, and their effectiveness can be sensitive to hyperparameter tuning.\n\nW2: The experiments are confined to image classification tasks. It remains unclear how well the BM-CL framework would generalize to other data modalities, such as tabular data or natural language, where biases are equally prevalent and challenging.\n\nW3: The work focuses on group fairness metrics and accuracy but may not have extensively explored other important fairness notions like individual fairness or the long-term societal impact of deploying such models."}, "questions": {"value": "- How exactly is the bias mitigation task structured as a continual learning problem? Is the model first trained on the original biased dataset and then fine-tuned on a de-biased or re-weighted version? Or is it exposed to different demographic groups sequentially?\n- How sensitive are the results to the order in which the fairness conditions or groups are introduced? Continual learning performance is often highly dependent on task sequence.\n- For the Elastic Weight Consolidation (EWC) method, how were the Fisher Information Matrix, the pretraining ratio, and the importance weight (lambda) for each parameter determined? Was lambda tuned specifically for fairness, and if so, what was the objective?\n- For Learning without Forgetting (LwF), how were the soft targets from the previous model obtained and used? Was the original training data required for replay, or was it done solely with new data? If the latter, how does this affect the stability-plasticity balance?\n- Were the hyperparameters (especially the critical ones for LwF and EWC like distillation temperature, regularization strength) optimized separately for each method, or was a consistent framework used? A difference in tuning effort could explain performance gaps.\n- Beyond standard bias mitigation techniques, were other continual learning methods (e.g., experience replay with a small memory buffer) tested for comparison to better isolate the contribution of LwF/EWC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "d2yDjstS9G", "forum": "KUmc5gLndO", "replyto": "KUmc5gLndO", "signatures": ["ICLR.cc/2026/Conference/Submission11715/Reviewer_3nuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11715/Reviewer_3nuT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850794573, "cdate": 1761850794573, "tmdate": 1762922756500, "mdate": 1762922756500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}