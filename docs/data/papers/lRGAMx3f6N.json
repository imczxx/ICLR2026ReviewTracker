{"id": "lRGAMx3f6N", "number": 4855, "cdate": 1757777210491, "mdate": 1763724380398, "content": {"title": "Flow-based Automatic Neural Operator with Hard Physical Constraints", "abstract": "Simulating physical systems governed by partial differential equations (PDEs) is crucial across science and engineering. Recently, generative models—exemplified by Flow Matching—have emerged as a highly competitive approach due to their ability to effectively model high-dimensional solution distributions. However, these models often struggle to ensure physical consistency, frequently violating fundamental conservation laws or boundary conditions. In this work, we propose Physics-Manifold Flow Matching (PMFM), a novel generative framework for PDE simulation that directly addresses this challenge. PMFM introduces two key innovations. First, it enforces strict, hard physical constraints by restricting the entire generative trajectory to a physical manifold defined by analytical equations, while employing a Geometric Guidance Mechanism (GGM) to maintain high-fidelity solutions. Second, to handle complex multi-physics problems, we introduce an Adaptive Constraint Projection Framework that learns to dynamically select and parameterize the currently active physical laws. We validate PMFM on several challenging systems that are highly sensitive to physical constraints, and the results show that our framework is significantly superior to state-of-the-art physics-informed generative models in producing physically valid, long-term-stable simulations.", "tldr": "", "keywords": ["Operator Learning", "Hard Constraint", "Flow Matching"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03e0b044a067178cd46ddcdc997c77007581662d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Physics-Manifold Flow Matching (PMFM), a generative framework for PDE simulation that enforces hard physical constraints by restricting the entire generative trajectory to a physical manifold. The approach addresses a key limitation of existing generative models for PDEs: they often violate fundamental conservation laws and boundary conditions. PMFM introduces two main innovations: (1) a projection mechanism that constrains trajectories to lie on a manifold where all states are physically valid by construction, combined with a Geometric Guidance Mechanism (GGM) that recovers high-frequency information typically lost during projection; (2) an Adaptive Constraint Projection Framework that dynamically selects and parameterizes active physical laws for complex multi-physics problems. The method is validated on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Novel approach to physical consistency:** The paper presents a principled method to enforce hard physical constraints in generative models through manifold projection, ensuring physical validity by construction rather than through soft penalties.\n    \n**Comprehensive experimental validation:** The method is tested on diverse PDE benchmarks covering different physical phenomena (conservation laws, shocks, incompressible flows), demonstrating consistent improvements over strong baselines including FNO, WNO, and diffusion models.\n    \n**Long-term stability:** Results show that PMFM maintains accuracy over extended temporal rollouts, addressing a critical limitation of purely data-driven approaches that accumulate errors during long-term prediction."}, "weaknesses": {"value": "## Training-Inference Distribution Mismatch\n\nA critical weakness lies in the inconsistency between training and inference distributions. During training (Algorithm 1), the framework uses straight-line interpolation $u_t = (1-t)u_0 + t u_1$ where $u_0 \\sim p_0$ (Gaussian noise) and $u_1 \\sim p_{\\text{data}}$ (physical solutions). Since the physical manifold $\\mathcal{M}$ is generally non-convex and $u_0 \\notin \\mathcal{M}$, intermediate states $u_t$ for $t \\in (0,1)$ typically violate physical constraints, i.e., $C(u_t) \\neq 0$. However, during inference, trajectories are strictly constrained to $\\mathcal{M}$ via projection $\\Pi_{T_u\\mathcal{M}}$ at every integration step. This creates several issues:\n\n- **Distribution shift**: The network is trained on off-manifold states but deployed exclusively on manifold-constrained trajectories, potentially leading to suboptimal generalization.\n\n- **Theoretical inconsistency**: Theorem 1 assumes $u_0 \\in \\mathcal{M}$ to guarantee manifold invariance, but this condition is systematically violated. The tangent space $T_{u_t}\\mathcal{M}$ at non-manifold points $u_t$ lacks rigorous geometric interpretation.\n\nThe paper does not discuss this mismatch or provide ablation studies comparing alternative training strategies, such as geodesic interpolation on $\\mathcal{M}$ or explicit projection of $u_t$ at each training step. The strong empirical results may rely on the interpolation path remaining \"close\" to $\\mathcal{M}$ due to data smoothness, but this lacks formal justification.\n\n## Unclear Presentation of Core Mechanisms\n\nWhile the mathematical formulation is sound, the presentation of the two key innovations: the Geometric Guidance Mechanism (GGM) and the Adaptive Constraint Projection Framework, lacks clarity. For GGM (Sec. 4.1), the paper does not provide sufficient intuition for why encoding the residual $r$ into a latent code $z$ is necessary, or how the three components ($E_\\phi$, $B_\\theta$, $\\alpha_\\phi$) collaborate during training versus inference. The training-inference gap (where $z=0$ at inference) is mentioned but not justified. For the Adaptive Framework (Sec. 4.3), the distinction between \"analytical structure\" and \"learnable parameters\" in the constraint library is abstract, and critical training details (e.g., how the gating network $G_\\psi$ is trained, what happens if constraints conflict) are missing.\n\n## Insufficient Ablation Studies\n\nTable 2 only compares the full PMFM against a variant without physical constraints, which does not isolate the contributions of GGM and the Adaptive Framework. Key ablations are missing: (1) vanilla projection vs. +GGM, (2) +GGM vs. +GGM+Adaptive, (3) the role of the residual encoder $E_\\phi$, and (4) quantitative analysis of gating accuracy (e.g., how often does $G_\\psi$ select the correct active constraints?). Also, there are several previous work discussing how to use penalty loss in generative models to generate more accurate PDE solutions (Riemannian Score-Based Generative Modelling, Physics-Informed Diffusion Models, Generating Physical Dynamics under Priors). Authors should consider including these baselines.\n\n## Minors\n\n**Typos:**\nline 034, 038, 040, 046, 082, 103, 124, 190 (empty line), 209, 322, 334, 351, 352, 359 (capital), 403, 453, 465, 465.\n\n**Missing related works:** Riemannian Score-Based Generative Modelling, Physics-Informed Diffusion Models, Generating Physical Dynamics under Priors.\n\nD-Flow is not mentioned in the main text."}, "questions": {"value": "1. **On the train-test gap**: The physical consistency is structurally guaranteed by the projection operator during the inference phase. However, during training, the interpolation path $u_t = (1-t)u_0 + t u_1$ does not lie on the manifold $\\mathcal{M}$ since $u_0 \\notin \\mathcal{M}$ and convex combinations generally leave $\\mathcal{M}$. Can you provide theoretical or empirical analysis on why learning $v_\\theta$ on off-manifold states transfers well to inference on manifold-constrained trajectories?\n\n2. **On convergence guarantees**: Theorem 1 assumes $u_0 \\in \\mathcal{M}$, but in practice inference starts from Gaussian noise $u_0 \\sim \\mathcal{N}(0,I)$ which violates constraints. Do you have convergence results for the projected ODE $\\dot{u} = \\Pi_{T_u\\mathcal{M}} v_\\theta(u,t)$ starting from $u_0 \\notin \\mathcal{M}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2itOi6Gj4", "forum": "lRGAMx3f6N", "replyto": "lRGAMx3f6N", "signatures": ["ICLR.cc/2026/Conference/Submission4855/Reviewer_Lt8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4855/Reviewer_Lt8W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633707687, "cdate": 1761633707687, "tmdate": 1762917619282, "mdate": 1762917619282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a flow-matching framework (PMFM) that claims to enforce hard physical constraints exactly by constraining model outputs onto the constraint manifold with both training and inference procedure based on projection. It introduces a Geometric Guidance Mechanism to recover expressivity lost by projection, and an adaptive constraint module that selects which constraints to enforce based on the current state. Together, these aim to make generative PDE models both physically consistent and flexible."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. The paper addresses the important problem of enforcing physical constraints in physics-based machine learning. A key strength is that the proposed architecture enforces constraints already during training, rather than applying corrections only at inference. This approach improves physical consistency and aligns the training objective with the target manifold, which is an important and practically meaningful design choice.\n\n**S2**. The idea of adaptive constraint selection and parameterization is interesting and has potential utility in multi-regime or time-varying physics settings. The experiments cover several PDEBench problems and test long-horizon rollouts, which are appropriate for this setting."}, "weaknesses": {"value": "**W1. Lack of novelty and theoretical justification.**\nThe projection-based enforcement of equality constraints is not new and has already been developed and applied in recent constrained generative modeling work [1–4]. The paper reuses this formulation without introducing a new projection method or optimization improvement. The proposed Geometric Guidance Mechanism (GGM), which encodes and re-injects the residual ($(I-\\Pi)v$), is interesting but lacks theoretical grounding. No argument or ablation is provided to show why this residual encoding should improve the learned tangent field or overall generative quality.\n\n**W2. Missing generative metrics and baseline comparisons.**\nThe paper does not include standard quantitative metrics such as MMSE, SMSE, or FID/FPD, which are routinely reported in constrained generative frameworks [1–4]. It also omits comparisons to key baselines: vanilla flow matching, conditional flow matching [1], Physics-Constrained Flow Matching (PCFM) [2], Extrapolation–Correction–Interpolation (ECI) [1], and Projected Diffusion Models (PDM) [3]. Additionally, the paper does not report constraint residual magnitudes ($||C(u)||$), making it unclear to what extent the claimed “hard constraint” property is achieved in practice.  While multiple PDEBench tasks are tested, the reported MSE improvements over baselines are modest and not supported by statistical or generative-quality metrics. Moreover, the paper does not provide any computational analysis quantifying the additional cost introduced by per-step projection, such as training/inference time overhead or GPU memory usage. Since projection requires computing and inverting the constraint Jacobian, these costs could be significant but are not reported.\n\n**W3. Lack of clarity in latent and gating mechanisms.**\nThe latent variable ($z$) is active only during training but set to zero during inference, effectively reducing the model to ($\\dot{u} = \\Pi g_\\theta(u, t)$). The paper does not show that training with ($z \\neq 0$) improves the tangent approximation or provide any ablation isolating the effect of the GGM. The adaptive constraint gating module is also under-specified: the paper does not explain how the gating network parameters are trained, whether the gate receives gradient updates, or how it interacts with the projection loss. This part of the paper is difficult to follow, and clarification on the training procedure and testing setup would be helpful.\n\n**W4. Presentation and Weak claims.**\nThere are some spurious or unsupported claims in the papers such as “high-frequency, non-smooth information is captured primarily in the orthogonal residual component..” (line 377) is presented without theoretical justification or empirical verification. Several important details, including the main training algorithm (Algorithm A.1) and ablations, are placed in the appendix, while less central figures (e.g., Figure 2) remain in the main text, making the presentation uneven. Moreover, the constraint space should be enforced at the final denoised state ($u_1$), but the method projects every intermediate state during inference. It is unclear whether this correction is made better by active set formulaton, and would require more clarification. The sampling methods, such as PCFM or ECI, apply these corrections at the denoised state at $u_1$ and interpolate, which somewhat preserves the generation quality as they do not directly project the intermediate noised sample state.\n\n**References**\n\n[1] Cheng, Chaoran, *et al.* “Gradient-Free Generation for Hard-Constrained Systems (Extrapolation–Correction–Inference).” *arXiv preprint arXiv:2412.01786* (2024).\n\n[2] Utkarsh, Utkarsh, *et al.* “Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints.” *arXiv preprint arXiv:2506.04171* (2025).\n\n[3] Christopher, Jacob K., Stephen Baek, and Nando Fioretto. “Constrained Synthesis with Projected Diffusion Models.” *Advances in Neural Information Processing Systems* 37 (2024): 89307–89333.\n\n[4] Baldan, Giacomo, *et al.* “Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation.” *arXiv preprint arXiv:2506.08604* (2025)."}, "questions": {"value": "See weaknesses above, but here are some explicit ones:\n\n\n**Q1.** You claim that “the high-frequency, non-smooth information contained in the shock front is captured primarily in the orthogonal residual component” (line 377). Could you provide theoretical or empirical evidence supporting this interpretation? Specifically, have you performed any spectral or statistical analysis demonstrating that the residual $(I - \\Pi)v$ indeed captures high-frequency components or non-smooth dynamics, rather than general projection error?\n\n\n**Q2.** In Section A.2, you state that $g_\\theta$ “learns the tangent field of the constraint manifold in the limit.” Could you clarify what limit this refers to (e.g., number of training iterations, projection accuracy, or dataset coverage)? Additionally, do you have a derivation or quantitative experiment showing that $g_\\theta$ aligns with the manifold tangent directions during or after training?\n\n\n**Q3.** You set $z = 0$ during inference. As noted in Section A.2, this implies a “natural nulling property,” meaning there is effectively no contribution from the residual component. In that case, what is the practical role of learning $z$ during training if it is fully suppressed at inference? Moreover, in Algorithm 2, should the initial sample $u_{\\text{current}} \\sim p_0(u)$ also be projected onto the constraint manifold before the flow evolution? Otherwise, the generated trajectory may drift off the manifold early in inference.\n\n\n**Q4.** Could you provide ablation studies that isolate the effects of the Geometric Guidance Mechanism (GGM) and the adaptive gating module? Without such analysis, it is unclear which component primarily contributes to constraint satisfaction or stability improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eFYN8KAT8b", "forum": "lRGAMx3f6N", "replyto": "lRGAMx3f6N", "signatures": ["ICLR.cc/2026/Conference/Submission4855/Reviewer_V1pj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4855/Reviewer_V1pj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909574573, "cdate": 1761909574573, "tmdate": 1762917618918, "mdate": 1762917618918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **FANG (Flow-based Automatic Neuron Grouping)**, a framework for accelerating Transformers by identifying groups of neurons with strong activation–gradient flow interactions. By modeling activations as *information flows*, FANG automatically clusters neurons to reduce redundant computation while preserving task performance.\nExperiments on BERT, T5, and Llama show up to **1.7× inference speedup** with less than **0.5% accuracy drop**."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* Solid theoretical basis with clear interpretability;\n* Comprehensive experiments across tasks and models;\n* Compatible with pruning and quantization methods;\n* Open-sourced and reproducible."}, "weaknesses": {"value": "* Limited analysis on batch-size sensitivity;\n* Missing evaluation on training-time acceleration;\n* Incomplete hyperparameter exploration (group count k);\n* No deployment benchmark on sparse hardware."}, "questions": {"value": "* Can flow computation be approximated without gradients?\n* Is the grouping transferable to multimodal Transformers?\n* How does FANG scale under long-context workloads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KiEV0Qncug", "forum": "lRGAMx3f6N", "replyto": "lRGAMx3f6N", "signatures": ["ICLR.cc/2026/Conference/Submission4855/Reviewer_ABuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4855/Reviewer_ABuy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032358014, "cdate": 1762032358014, "tmdate": 1762917618706, "mdate": 1762917618706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a well-written paper that proposes Physics-Manifold Flow Matching (PMFM), a flow-based neural operator that keeps the entire generative trajectory on a physics manifold via orthogonal projection, thereby enforcing hard constraints (e.g., divergence-free, mass/energy conservation, boundary conditions) at every step. To recover fine details lost due to projection, a Geometric Guidance Mechanism (GGM) encodes the discarded normal component and reintroduces it into the tangent dynamics without compromising feasibility, and the authors demonstrate manifold invariance of the projected flow. They also add an Adaptive Constraint Projection module: a gating network selects which analytical constraints are active and predicts their parameters online, building a state-dependent projector that handles multi-physics and unknown coefficients. Training utilizes conditional flow matching on projected vector fields, while sampling integrates the learned ODE with projection (and a final corrective projection) to ensure constraint satisfaction. Experiments on PDEBench (1D advection, 1D Burgers, 2D Darcy, 2D advection, 2D incompressible Navier–Stokes) show lower errors than FNO/WNO/DeepONet, DDPM, and flow-matching baselines, with notably improved long-term stability; ablations confirm large degradations when constraints are removed or made fixed rather than adaptive."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. It enforces hard physical constraints along the entire generative trajectory by projecting every step onto a physics manifold.\n\n2. The proof of manifold invariance and a Geometric Guidance term that restores lost detail without breaking feasibility. The incorporation of the differential geometry concept for the physics-informed generative AI for solution or operator generation makes sense."}, "weaknesses": {"value": "The computational cost can limit the practicality of the proposed model. Since divergence-free requires a Poisson solve at each step, it adds overhead, combined with the additional cost for training, which can make the problems less appealing than conventional solvers.  \n\nThe manifold methods may be limited to problems with a unique solution. For instance, the Elder problem may lead to multiple steady states, in which case the proposed method may not be effective."}, "questions": {"value": "You report 15% per-iteration overhead from hard projections, with divergence-free enforced via a Helmholtz–Hodge projection (Poisson solve using FFT/multigrid). Could you provide scaling curves versus grid size and ODE steps, and benchmark against standard neural operator or solver baselines?\n\nHow do accuracy and long-horizon stability change if you (i) project every K steps instead of every step, (ii) solve Poisson on a coarse grid with prolongation, or (iii) use a truncated spectral (low-mode) projector—followed by a final exact projection? Reporting W₂/rel-L₂ vs. wall-clock would clarify the accuracy–cost trade-off for practitioners.\n\nBecause the flow is manifold-invariant once projected, does the learned transport collapse to a single basin when multiple steady states satisfy the same constraints? Could you test a multi-modal PDE where the constraint set admits several attractors and report mode coverage under different priors or stochastic injections, and whether the adaptive gating helps switch branches?\n\nCan you quantify condition numbers, sensitivity to the regularization, and behavior under redundant/conflicting constraints of the projectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wkXqFUMW5j", "forum": "lRGAMx3f6N", "replyto": "lRGAMx3f6N", "signatures": ["ICLR.cc/2026/Conference/Submission4855/Reviewer_jU9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4855/Reviewer_jU9B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762288251918, "cdate": 1762288251918, "tmdate": 1762917618189, "mdate": 1762917618189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Physics-Manifold Flow Matching (PMFM) as a novel generative framework to enforce physical constraints. The method enforces hard constraints by projecting to a manifold and uses a geometric guidance mechanism (GGM). It also uses an Adaptive Constraint Projection Framework that learns to dynamically select and parameterize the currently active physical laws.\n\nWhile I think the problem is important especially the emphasis on physical constraints, I would like the authors to clarify the novelty of their approach compared to those in the literature and also need to cite several other missing hard-constrained references."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Nice application of generative modeling to PDEs and emphasizing the importance of physical constraints\n- Good benchmarking PDE datasets from PDEBench\n- Nice results on long-term stability of the method\n- Nice ablation study to highlight the importance of physical constrains"}, "weaknesses": {"value": "- Missing references to methods with hard constraints, especially these that look at conservation laws and boundary conditions e.g., \n   - Hansen et al., \"Learning Physical Models that Can Respect Conservation Laws\", ICML, 2023\n   - Utkarsh et al., \"End-to-End Probabilistic Framework for Learning with Hard Constraints\", arXiv preprint arXiv:2506.07003\n    - Negiar et al., \"Learning differentiable solvers for systems with hard constraints\", ICLR, 2023\n    - Richter-Powell et al., \"Neural Conservation Laws: A Divergence-Free Perspective\", NeurIPS, 2022\n    - Chalapathi et al., \"Scaling physics-informed hard constraints with mixture-of-experts\", ICLR 2024.\n- Especially literature review of hard constraints on Neural Operators\n  - Saad et al., \"Guiding continuous operator learning through Physics-based boundary constraints\", ICLR, 2023\n  - Mouli et al., \"Using uncertainty quantification to characterize and improve out-of-domain learning for pdes\", ICML, 2024\n- Missing reference to hard-constrained generative methods, which is especially relevant since it hard constrains functional flow matching (FFM) methods\n  -  ECI Sampling Cheng et al., \"Gradient-free generation for hard-constrained systems\", ICLR, 2024.\n- The projection operator seems very similar to what has already been proposed in Hansen et al., \"Learning Physical Models that Can Respect Conservation Laws\", ICML, 2023 and Utkarsh et al., \"End-to-End Probabilistic Framework for Learning with Hard Constraints\", arXiv preprint arXiv:2506.07003.\n- No 3D PDE examples\n- ECI Sampling, ICLR, 2024, which is gradient-free and hard-contrains FFMs should be compared to as well as DiffusionPDE. ECI Sampling is more accurate, faster and more memory efficient than D-Flow.\n\nMinor \n- Typo in Intro with LeVeque references and period\n- No spaces after several references\n- Rename Section 4 Our appraoch to the method title"}, "questions": {"value": "1. Please clearly explain how this approach differs from the constrained generative projection method in Cheng et al., \"Gradient-free generation for hard-constrained systems\", ICLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7A57YasCJW", "forum": "lRGAMx3f6N", "replyto": "lRGAMx3f6N", "signatures": ["ICLR.cc/2026/Conference/Submission4855/Reviewer_6Ebt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4855/Reviewer_6Ebt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762407042848, "cdate": 1762407042848, "tmdate": 1762917617416, "mdate": 1762917617416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}