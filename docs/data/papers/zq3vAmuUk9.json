{"id": "zq3vAmuUk9", "number": 17467, "cdate": 1758276377070, "mdate": 1759897173510, "content": {"title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of  scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly  outperforms GPT-5, Clause-Sonnet-4,  DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://anonymous.4open.science/r/AgentRL-ICLR-C351 and has also been adopted for developing other open-source LLM agents.", "tldr": "", "keywords": ["LLM", "Agent", "RL"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67b0a97aa7f64c3738ad1bcaafe56b1eaf9ca94b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents the AGENTRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AGENTRL\nfeatures a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, authors design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, authors propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, Develop an asynchronous, multi-task framework AGENTRL for scalable agentic RL training and robust heterogeneous environment deployment.\n2,  Design a cross-policy sampling strategy to encourage exploration in multi-turn settings and task advantage normalization to stabilize multi-task RL training.\n3, The experiments are comprehensive, including different models on different tasks.\n4, The github seems good."}, "weaknesses": {"value": "Please see questions for the details."}, "questions": {"value": "So I am not very familiar with this topic. Could you maybe further explain why you choose Hephaestus and AgentLM as your baseline specifically and what's the difference between your proposed method and those two methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zhWyhKuns5", "forum": "zq3vAmuUk9", "replyto": "zq3vAmuUk9", "signatures": ["ICLR.cc/2026/Conference/Submission17467/Reviewer_3aoe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17467/Reviewer_3aoe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760802049059, "cdate": 1760802049059, "tmdate": 1762927349599, "mdate": 1762927349599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AGENTRL introduces a scalable, asynchronous multi-task RL framework for training LLM agents in multi-turn interactive environments. It combines cross-policy sampling to boost exploration and task-advantage normalization to stabilize training across heterogeneous tasks. Trained on five agent benchmarks, a single AGENTRL model matches specialist agents and outperforms GPT-5, Claude-Sonnet-4 and DeepSeek-R1, showing strong generalization to unseen tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The ablation studies are detailed and show the individual and combined effects of the two main algorithmic improvements.\n- The introduction of cross-policy sampling to enhance exploration and task advantage normalization to address heterogeneity in multi-task RL are practical and well-motivated improvements."}, "weaknesses": {"value": "- Mixing trajectories from different policy checkpoints introduces measurable off-policy bias, maybe more experiments and analysis to describe this bias is needed.\n- Missing hyper-parameter sensitivity analysis: Kcross-policy sampling probability, advantage-normalisation batch size, async queue capacity are fixed to single values without grid-search or Sobol sweeps, so robustness of the reported gains is unclear."}, "questions": {"value": "- Can the authors provide more granular per-task and per-instance breakdowns of reward scaling and failure modes to better demonstrate the stability of task advantage normalization?\n- How sensitive is AGENTRL to choice of reward normalization and environment design? Is there any failure cases if task heterogeneity is heightened further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QuP6SXU0Fg", "forum": "zq3vAmuUk9", "replyto": "zq3vAmuUk9", "signatures": ["ICLR.cc/2026/Conference/Submission17467/Reviewer_rknx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17467/Reviewer_rknx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816733015, "cdate": 1761816733015, "tmdate": 1762927348936, "mdate": 1762927348936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AGENTRL, a scalable framework for multi-turn and multi-task reinforcement learning of large language model agents. It introduces an asynchronous rollout–training pipeline, a unified function-call–based environment API, and novel algorithmic components such as cross-policy sampling and task advantage normalization, achieving state-of-the-art performance across five agentic benchmarks and demonstrating strong generalization to unseen tasks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The asynchronous multi-turn training framework effectively eliminates the inefficiency of synchronous rollouts, reducing GPU idle time and improving overall throughput while maintaining stable policy updates.\n  - The scalable multi-task environment infrastructure provides a unified function-call–based API and containerized deployment with centralized control, enabling efficient orchestration and management of heterogeneous environments at scale.\n  - The algorithmic innovations, including cross-policy sampling and task advantage normalization, significantly enhance exploration and stabilize multi-task reinforcement learning, resulting in consistent performance gains across benchmarks."}, "weaknesses": {"value": "- The proposed asynchronous generation-training architecture inevitably introduces a significant off-policy problem, where training data is generated by stale policies. However, for a framework based on an on-policy algorithm , AGENTRL fails to mention any mechanism to correct for the bias caused by this policy lag. This oversight of a core theoretical challenge leaves the stability and convergence of its training process theoretically unsubstantiated and makes the description of its asynchronous design ambiguous.\n- The motivation and necessity of Cross-Policy Sampling are not sufficiently justified. This method, which samples actions from historical models, directly conflicts with the assumptions of on-policy algorithms and introduces uncorrected distribution shift risks. Furthermore, the paper links its motivation to solving \"model collapse,\" yet provides no evidence that such collapse actually occurs. More critically, the experiments lack a comparison with simpler alternatives, such as standard experience replay, making it impossible to determine whether the complexity of this design is truly necessary or if its claimed benefits surpass those of established techniques\n- Task Advantage Normalization (TAN) sounds complex but offers limited practical benefits. At its core, it simply performs z-score normalization independently for each task — a common trick that lacks higher-level insight."}, "questions": {"value": "- Given the off-policy data introduced by asynchronous training, does your framework include any mechanisms (even implicit ones) to mitigate its negative impact on the on-policy algorithm (GRPO)?\n- Regarding Cross-Policy Sampling, how do you theoretically justify that training directly on samples from older policies is stable and unbiased without applying off-policy corrections?\n- Have you conducted experiments comparing Cross-Policy Sampling with standard experience replay? If not, what unique and irreplaceable advantages do you believe the former holds over the latter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHZRudH8DV", "forum": "zq3vAmuUk9", "replyto": "zq3vAmuUk9", "signatures": ["ICLR.cc/2026/Conference/Submission17467/Reviewer_DsdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17467/Reviewer_DsdZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965718502, "cdate": 1761965718502, "tmdate": 1762927348346, "mdate": 1762927348346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AgentRL, a comprehensive framework for scalable multi-turn, multi-task reinforcement learning (RL) training of large language model (LLM) agents. The work addresses significant challenges in agentic RL by introducing: (1) a fully asynchronous generation-training pipeline for efficient multi-turn RL, (2) a unified function-call based API interface with containerized environment deployment and centralized control, (3) cross-policy sampling to enhance exploration in multi-turn settings, and (4) task advantage normalization to stabilize multi-task training.The authors evaluate AgentRL on five diverse agentic tasks (ALFWorld, DB, KG, OS, WebShop) across multiple model scales (3B-72B parameters). Results demonstrate that AgentRL-trained models significantly outperform strong baselines including GPT-5, Claude-Sonnet-4, and DeepSeek-R1, achieving state-of-the-art performance with an average success rate of 70.4%. Notably, a single multi-task trained model matches the best performance of five task-specific models while showing promising generalization to unseen tasks (BFCL-v3 benchmark)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Presents AgentRL, a comprehensive framework for scalable multi-turn, multi-task reinforcement learning (RL) training of large language model (LLM) agents, addressing infrastructure, environment, and algorithmic challenges simultaneously.\n\nAchieves impressive performance gains across all tested tasks and outperforms strong proprietary baselines (GPT-5, Claude-Sonnet-4). \nThe asynchronous pipeline achieves significant throughput improvements, making the approach practically viable at scale. \n\nSuccessfully demonstrates that a single multi-task trained model can match the best performance of five task-specific models while maintaining strong generalization capabilities."}, "weaknesses": {"value": "Limited Theoretical Analysis: Cross-policy sampling's impact on policy distribution is mentioned but not formally analyzed\n\nEvaluation Scope: Limited diversity in task types (no long-horizon embodied tasks, no code generation with execution, no real-world web automation); OOD evaluation limited to one benchmark (BFCL-v3)\n\nCross-Policy Sampling Analysis: \"Stale engines\" update schedule unclear (every \"multiple steps\" is vague); Limited analysis of the exploration-exploitation tradeoff; Section 4.3.1's inference experiments use different models (Llama vs Qwen) but training uses same model - this asymmetry needs justification\n\nScalability and Cost: No discussion of computational costs or training time"}, "questions": {"value": "1. How exactly is the \"stale engine\" update frequency determined? Is it task-dependent or fixed? What is the probability distribution for sampling from different policies at each step? Have you experimented with more than 2 policies? How does performance scale with the number of policies?\n\n2.What is the maximum queue size for the data buffer mentioned in Section 3.1? How much off-policy bias is introduced? Can you provide a comparison with synchronous training in terms of training time, convergence speed, and performance ceiling? How do you ensure the trajectories remain \"as up-to-date as possible\" when the queue can accumulate data?\n\n3. Why normalize at the token level rather than the trajectory level? Have you experimented with other normalization strategies？Does this introduce any bias when tasks have very different token-length distributions?\n\n4.The BFCL-v3 results show modest improvements on single-turn tasks.  Can you provide more analysis on what makes the model generalize to new tasks? Have you tested on tasks completely different from the training distribution?\n\n5. Section 4.3.1 uses heterogeneous models (Llama vs Qwen) for inference experiments, but training uses temporal versions of the same model family. Does this mean cross-policy sampling during training merely augments offline data diversity rather than introducing fundamentally different exploration mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GxzkRG2R6D", "forum": "zq3vAmuUk9", "replyto": "zq3vAmuUk9", "signatures": ["ICLR.cc/2026/Conference/Submission17467/Reviewer_vrk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17467/Reviewer_vrk7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007208766, "cdate": 1762007208766, "tmdate": 1762927347867, "mdate": 1762927347867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}