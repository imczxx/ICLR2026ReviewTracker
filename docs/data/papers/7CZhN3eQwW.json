{"id": "7CZhN3eQwW", "number": 580, "cdate": 1756750298822, "mdate": 1763693015618, "content": {"title": "Is Softmax Loss all you need? A Principled Analysis of Softmax Loss and its Variants", "abstract": "**The Softmax Loss** is one of the most widely employed surrogate objectives for classification and ranking, owing to its elegant algebraic structure, intuitive probabilistic interpretation, and consistently strong empirical performance. To elucidate its theoretical properties, recent works have introduced the Fenchel–Young framework, situating Softmax loss as a canonical instance within a broad family of convex surrogates. This perspective not only clarifies the origins of its favorable properties, but also unifies it with alternatives such as Sparsemax and $\\alpha$-Entmax under a common theoretical foundation. Concurrently, another line of research has addressed on the challenge of scalability: when the number of classes is exceedingly large, computations of the partition function become prohibitively expensive. Numerous approximation strategies have thus been proposed to retain the benefits of the exact objective while improving efficiency. However, their theoretical fidelity remains unclear, and practical adoption often relies on heuristics or exhaustive search.\n\nBuilding on these two perspectives, we present a principled investigation of the **Softmax-family** losses, encompassing both statistical and computational aspects. Within the Fenchel–Young framework, we examine whether different surrogates satisfy consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. For approximate Softmax methods, we introduce a systematic bias–variance decomposition that provides convergence guarantees. We further derive a per-epoch complexity analysis across the entire family, highlighting explicit trade-offs between accuracy and efficiency. Finally, extensive experiments on a representative recommendation task corroborate our theoretical findings, demonstrating a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.", "tldr": "We theoretically analyze the properties of \"Softmax-family\" losses and provide a clear guide on understanding and choosing surrogates for real-world scenarios.", "keywords": ["Softmax Loss", "Fenchel-Young Loss", "Consistency", "Convergence", "Classification", "Ranking"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8538eeeb7b1ecf8787a64da7ceb45d82dbe9c0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematically analyzes the softmax loss (SM) and its variants, focusing on three theoretical aspects: (1) Consistency: based on the Fenchel-Young framework, the authors compare the consistency of SM with sparse SM variants (including Sparsemax, $\\alpha$-Entmax, and Rankmax). (2) Convergence: by estimating the first-order Lipschitz constant, the authors analyze the convergence rates of SM, the sparse variants, and the approximate variants (including SSM, NCE, HSM, RG). (3) Bias and Variance: based on the Delta method, the authors compare the asymptotic bias, curvature bias, and variance of SM and its approximate variants. The theoretical results are validated through extensive experiments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents a comprehensive theoretical analysis of the softmax loss and its variants, providing a unified view of widely used softmax-based losses.\n- The writing is clear and well-structured, the mathematical derivations are rigorous and easy to follow.\n- The experimental results match well with the theoretical findings."}, "weaknesses": {"value": "In general, this paper provides a solid and comprehensive analysis of the softmax losses. I have checked all the main theoretical results and found no major issues (except for possible typos mentioned below). The weaknesses, from my perspective, mainly lie in two aspects:\n- This paper does not provide a novel method that improves upon existing softmax variants. However, I believe the theoretical insights provided in this paper are still worth publishing.\n- This paper analyzes the general softmax losses, yet it merely conducts experiments on the recommendation tasks. It would be more convincing if the authors could validate their theoretical findings on other classification tasks, e.g., image classification."}, "questions": {"value": "**Questions for Discussion.** I have some further questions mainly for discussion purposes:\n- **Improved SSM.** While SSM has a zero asymptotic bias, when the negative sampling size $k$ is small, the variance can be large (as also shown in Table 2). A previous work [R1] improves SSM by simply applying the compositional optimization technique to facilitate small-batch training. While I understand that this method may not be subsumed under the Fenchel-Young framework used in this paper, I wonder if the authors have any insights on comparison between SSM and the proposed method in [R1]?\n- **Top-$k$ Calibration.** The top-$k$ calibration property is sufficiently discussed in the prior works and this paper. Nonetheless, in practice, SM (or SSM) is often underperforming the elaborated top-$k$ ranking surrogate losses (e.g., [R2, R3]). Given the theoretical results in this paper, do the authors have any insights on why this is the case?\n\n**Minor Comments.** Here are some minor typos I found in the paper:\n- There is a typo in Eq. (23), where $\\textbf{p}$ should be $\\hat{\\textbf{p}}$.\n- There exists a typo in the sign of Eq. (98).\n\n**References:**\n\n- [R1] Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence. ICML '22.\n- [R2] On Optimizing Top-K Metrics for Neural Ranking Models. SIGIR '22.\n- [R3] Breaking the Top-K Barrier: Advancing Top-K Ranking Metrics Optimization in Recommender Systems. KDD '25."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NmZpkBRm71", "forum": "7CZhN3eQwW", "replyto": "7CZhN3eQwW", "signatures": ["ICLR.cc/2026/Conference/Submission580/Reviewer_RSpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission580/Reviewer_RSpv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854256450, "cdate": 1761854256450, "tmdate": 1762915552477, "mdate": 1762915552477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper summarizes some properties of softmax loss and its variants. It provides a unified theoretical study of the softmax-family of loss functions, including consistency properties of different F-Y surrogates, gradient dynamics and computational complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well written and easy to follow. It provides a comprehensive summary of various softmax family losses. The introduction of the SOP/WOP and bias–variance decomposition are theoretically elegant, and offers interpretable insights into trade-offs between accuracy and efficiency of softmax family losses."}, "weaknesses": {"value": "I have some doubts about the convergence rate analysis. First, the softmax family losses are not all strongly convex, and the authors’ analysis of the condition number relies on adding an $l_2$ regularization term to the original problem. Moreover, in practice, loss functions are often nonconvex. Could the authors further explain how the analysis in Section 3.2 is more applicable to practical settings?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rlTwYlXilr", "forum": "7CZhN3eQwW", "replyto": "7CZhN3eQwW", "signatures": ["ICLR.cc/2026/Conference/Submission580/Reviewer_wSds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission580/Reviewer_wSds"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858477266, "cdate": 1761858477266, "tmdate": 1762915552291, "mdate": 1762915552291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, offers a detailed theoretical study of the Softmax loss and related methods used in classification and ranking tasks. The main goal is to understand, in a unified way, how different versions of Softmax — such as Sparsemax, α-Entmax, Rankmax, and approximate methods behave both statistically and computationally. The authors use the Fenchel–Young (F–Y) framework, a convex-analytic approach, to describe all these losses under a common theory. They show the following:\n1. The consistency properties of different F-Y surrogates\n2. Analyze the gradient dynamics of Softmax-family losses, characterizing their convergence behaviors\n3. Comparitive analysis of per epoch computational complexity\n4. Complement with extensive experiments"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper nicely brings together many known results on Softmax, Sparsemax, and their approximations under one unifiied theoretical framework\n2. The analysis showing how Jacobian spectral norms explain convergence differences between losses is insightful"}, "weaknesses": {"value": "1. Can we interpret these Fenchel–Young losses as mirror descent updates, since each loss defines its own geometry?\n2. There is a bunch of qualitative analysis but no explicit rates of convergence? Do they directly follow?\n3. I am not clear on prior related works on the fenchel-young framework. Can the authors add that too if it is relevant?\n4. I don’t really understand what the final takeaway of the unified analysis is. Maybe it is useful to emphasize that too. I see the tables in the beginning, but it might be useful to add that the abbreviations mean in that for people not in the area."}, "questions": {"value": "See weakness!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pZEOJGkWPj", "forum": "7CZhN3eQwW", "replyto": "7CZhN3eQwW", "signatures": ["ICLR.cc/2026/Conference/Submission580/Reviewer_XpJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission580/Reviewer_XpJH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903439635, "cdate": 1761903439635, "tmdate": 1762915552060, "mdate": 1762915552060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}