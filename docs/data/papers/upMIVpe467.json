{"id": "upMIVpe467", "number": 13294, "cdate": 1758216097538, "mdate": 1763726617146, "content": {"title": "Equivariant Splitting: Self-supervised learning from incomplete data", "abstract": "Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models.", "tldr": "", "keywords": ["inverse problems", "self-supervised imaging", "equivariant neural networks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e4aa9c13582afe0f018abb9c5eb54a0b2832f7d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Equivariant Splitting (ES), a self-supervised training loss for inverse problems that combines measurement-splitting ideas with equivariant modelling. Under an invariance assumption on the image distribution (Assumption 1), ES averages splitting losses computed on virtual operators formed by composing the forward operator with group transforms and shows (Theorem 1 / Proposition 1) that, when certain matrix conditions hold, minimizers of the ES loss coincide (in expectation) with the MMSE estimator. The authors introduce a reconstructor notion of equivariance (Definition 1), prove that several common architecture families satisfy it (Theorem 2), and show ES reduces to standard splitting for equivariant reconstructors (Theorem 3). Empirical evaluation covers compressive sensing, image inpainting, and MRI, comparing ES to supervised, equivariant imaging (EI), SURE baselines, and reporting PSNR / SSIM / an equivariance metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formulates a unifying view that connects splitting losses and equivariant imaging and states the assumptions under which ES is theoretically unbiased for the supervised objective. \n- Introducing a definition of equivariant reconstructors and enumerating common architectures that satisfy it is useful. \n- The proposed loss is practically implementable. The paper explains how to use Monte-Carlo sampling of transforms and splits, and how to replace the noiseless term with R2R in the noisy setting, making the method applicable to realistic measurement noise models. \n- Experiments span multiple inverse problems (compressive sensing, inpainting, MRI) and include an ablation on equivariant vs non-equivariant architectures, which demonstrates the claimed synergy between splitting losses and equivariant architectures in the tested settings."}, "weaknesses": {"value": "- The main theoretical guarantee requires (i) the distributional invariance (Assumption 1) and (ii) invertibility conditions (full rank) on the matrix $Q_{A_1}$ or $\\bar{Q}_A$ for some split $A_1$. These conditions are sufficient but seem strong and may rarely hold in practice; the manuscript gives limited practical diagnostics or guidance on verifying these conditions for real forward operators. \n- The method depends critically on choosing an appropriate group (translations, rotations, flips, etc.). The paper does not sufficiently analyze robustness when the invariance assumption is only approximate or mis-specified (e.g., natural images that are not strictly invariant to some transforms). Practical recommendations for selecting $G$ per application are brief. \n- Although multiple tasks are included, some experimental choices raise concerns: (i) use of MNIST for compressive sensing (very simple data distribution), (ii) DIV2K for inpainting with synthetic masks, and (iii) synthetically generated k-space masks for MRI. The assertion that ES achieves “state-of-the-art” self-supervised performance is not fully established across diverse, realistic datasets or against the latest baselines in unsupervised/self-supervised imaging literature (such as DDRM). \n- Reynolds averaging over large groups is noted to be impractical, and the authors use Monte-Carlo sampling, but the runtime, per-iteration cost, and the number of samples are not clearly reported. Training budgets are mentioned (up to 50 hours on a single GPU), but fair comparisons to baselines in terms of wall-clock cost or memory are missing. \n- Ablations and failure modes are not discussed in detail. The ablation on equivariant architectures is informative, but there is little analysis of when ES fails (e.g., non-unitary transforms or operators that are nearly equivariant. Corollary 1 suggests A must not commute with transforms)."}, "questions": {"value": "- In practice, how does one verify (or have you measured) whether $Q_{A_1}$ or $\\bar{Q}_A$ is full rank for realistic forward operators (e.g., the MRI coil-sensitivity map that you use)? \n- How sensitive is ES when the dataset only approximately satisfies Assumption 1? Can you provide quantitative experiments where invariance is gradually violated (e.g., introduce systematic asymmetries) and show performance degradation versus EI / supervised baselines? \n- What is the typical number of random transforms/splits used at training and at test time? What is the overhead relative to EI and supervised training? \n- Corollary 1 requires that the forward operator $A$ is not equivariant. Can you clarify and demonstrate what happens when $A$ is equivariant or nearly equivariant (e.g., radial sampling in MRI, which is equivariant with rotational transforms)? Does ES break down or merely degrade gracefully? \n- Reproducibility and released artifacts. The paper references DeepInverse and gives some training details, but can you (i) release code and trained models, (ii) provide exact seeds and the split/sampling scripts for splits/transforms, and (iii) include scripts to reproduce the key tables (Tables 1–4) and the equivariance metric computations (EQUIV)? Appendix B mentions some implementation choices, but the reproducibility checklist is incomplete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SkrYdPygcF", "forum": "upMIVpe467", "replyto": "upMIVpe467", "signatures": ["ICLR.cc/2026/Conference/Submission13294/Reviewer_vQFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13294/Reviewer_vQFi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828506366, "cdate": 1761828506366, "tmdate": 1762923963271, "mdate": 1762923963271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Equivariant Splitting (ES), a self-supervised learning method for solving linear inverse problems when only incomplete measurements from a single forward operator are available. The method combines ideas from equivariant imaging (EI), which leverages invariance of the signal distribution under a group of transformations, and measurement splitting, which divides measurements into input/target pairs. The authors introduce a new definition of equivariance for reconstruction networks and show that, under mild conditions, minimizing their proposed loss yields the MMSE estimator. They validate ES on image inpainting, compressive sensing, and accelerated MRI, reporting performance close to supervised baselines and better than existing self-supervised methods like EI."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method targets a highly relevant and challenging problem: learning from incomplete data with a fixed forward operator (e.g., fixed MRI sampling mask, fixed inpainting mask). \n* The reported results show consistent improvements over EI (especially in highly ill-posed regimes like high CS compression) and a performance closely following supervised learning methods.\n* The ablation study in Table 3  demonstrates clearly the synergy between equivariant architectures and the ES loss, validating the theoretical claims."}, "weaknesses": {"value": "* The proposed self-supervised learning strategy relies on strong assumptions on signal invariance and operator non-equivariance, while it does not explore what happens when these assumptions are violated.\n* Limited Robustnes to real-world distortions: All experiments assume perferct knowledge of the forward operator A (in the linear inverse imaging literature this assumption is known as an \"inverse crime”) and either idealized noise models (i.i.d Gaussian) or  the absence of noise. Morevoer, the studied problems are oversimplified and are not representative of problems met in real-world applications. For example, in image inpainting the authors consider a binary mask that keeps 30% of all the pixel and the absence of noise while more realistic scenarios would involve added noise and non-uniform masking.\n* Computational Overhead and Scalability Concerns\n  *  Although Theorem 3 shows that equivariant architectures reduce the ES loss to a standard splitting loss at training time, training still requires multiple random splits per sample, which increases memory and compute costs compared to simple supervised or consistency-based baselines.\n  * The use of Reynolds averaging in Eq. 18 for enforcing equivariance becomes prohibitively expensive for large groups (e.g., continuous shifts or rotations), as noted in Appendix A. The paper only evaluates small discrete groups (e.g., 90° rotations + flips), limiting generalizability."}, "questions": {"value": "Corollary 1 states that the forward operator A must not be equivariant w.r.t. $G$ for $Q_{A_1}$  to be full rank. In practice, how should one select $G$ given a fixed $A$ ? Are there automated or data-driven strategies to choose transformations that maximize the rank of $Q_{A_1}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rPnNzpmSKF", "forum": "upMIVpe467", "replyto": "upMIVpe467", "signatures": ["ICLR.cc/2026/Conference/Submission13294/Reviewer_ZYHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13294/Reviewer_ZYHS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919945536, "cdate": 1761919945536, "tmdate": 1762923962880, "mdate": 1762923962880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses a study of self-supervised method to solve inverse problems, with the approach as designing loss functions with the \"SPLIT\" technique."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Structure of the paper is clear."}, "weaknesses": {"value": "- Theorem 1 is well expected. If $(AT_g)^\\top AT_g$ has full rank, then $AT_g$ itself does not dimensionally compress information, hence the ill-conditioning of the inverse problem is no-longer a concern.\n\n- Throughout this paper, the notation is largely unclear. \n\n    - It is hence by requested that authors specify dimension for all matrix quantities for clarity.\n    - With expect to all expectations, authors explicitly explain which quantities are being integrated with respect to its distribution and with respect to which quantities is the variable still stochastic.\n\n- In the experiment section, authors should very clearly define what the measurement matrix $A$ is for all scenarios. Explicitly explain its shape and why the measurement is a compressive measurement. \n    - Because of the proposed \"SPLIT\" technique, authors should also explain clearly what $A_2$ is in every testing scenario.\n\n- In the experiment section, authors should clearly list the citation of used baselines, and list the definition of all metrics, in the main text or in the appendix. Say, what is \"EI\"?\n\n-  In the methodology section, authors should state what the final proposed algorithm is, in a crystal clear way."}, "questions": {"value": "- What does $\\mathbb{E}_{x | y, A} \\{ x \\}$ mean, exactly? Do you mean the reconstructed $\\widehat{x}$ given measurement matrix and measurement observation $y$?\n- What does $A_1 | y, AT_g$ mean, exactly?\n- In section 5.1, multiplying by \"measurement matrix with fewer columns than rows\" is not compressive, is it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0upFTbAxUc", "forum": "upMIVpe467", "replyto": "upMIVpe467", "signatures": ["ICLR.cc/2026/Conference/Submission13294/Reviewer_zSdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13294/Reviewer_zSdB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056862483, "cdate": 1762056862483, "tmdate": 1762923962560, "mdate": 1762923962560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses self-supervised learning for inverse problems in settings where practitioners lack access to ground truth data and must train from incomplete measurements. The proposed  approach relies on the assumption that the ground truth distribution is invariant to a group of transformations, as formalized in Assumption 1. Building on this assumption, the authors propose a novel self-supervised loss function, Equivariant Splitting (ES), which trains the reconstruction network by optimizing consistency between full and partial measurements, averaged over symmetry transformations of the forward operator. The authors provide theoretical justification by showing that their expected loss converges to the MMSE estimator under certain conditions. Furthermore, they demonstrate empirically that their proposed method achieves strong performance on natural images and MRI reconstruction across a diverse range of inverse problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths:**\n\n- The paper presents a novel method that addresses a more realistic setting than traditional generative prior approaches, which typically assume access to ground truth images or paired ground truth-measurement data. The combination of equivariant priors with measurement splitting is original and well-motivated.\n\n- The authors provide theoretical analysis demonstrating that the proposed loss converges in expectation to the MMSE estimator under certain conditions, offering solid justification for the approach.\n\n- The empirical results are strong, achieving performance competitive with supervised baselines that have access to ground truth data, which validates the practical effectiveness of the self-supervised approach."}, "weaknesses": {"value": "**Weaknesses:**\n\n- Assumption 1 would benefit from additional discussion regarding the practical scope of applicable transformations for different problem domains. While the general framework is elegant, natural images often exhibit partial rather than full symmetries (e.g., small rotations of 15° are reasonable, while 180° rotations may be implausible). Providing examples for specific domains would strengthen the paper and provide valuable guidance for practitioners applying this method to new problems.\n\n\n- The theoretical analysis provides useful motivation for the proposed method, though the mathematical results could be presented differently to better reflect their contribution. The proofs largely build on established techniques. This is a significant portion of the paper. \n\n- Mirror Weakness Theorem 2 makes interesting claims about Reynolds averaging and MAP estimates that would benefit from empirical validation. To my knowledge I did not see anything."}, "questions": {"value": "**Questions:**\n\n1. The equivariance framework is quite general and could potentially apply to many problem domains beyond those explored in the paper. Could the authors provide additional guidance on selecting appropriate transformation groups for different applications? For instance, a discussion of which symmetries are valid for medical imaging, computational photography, or scientific imaging would be valuable for practitioners. Understanding the broader applicability and potential impact of this framework across different domains would strengthen the paper's contribution.\n\n2. The theoretical analysis provides helpful motivation for the proposed method. Could the authors clarify which aspects of the proof techniques represent novel contributions versus adaptations of existing results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S1gUClARqi", "forum": "upMIVpe467", "replyto": "upMIVpe467", "signatures": ["ICLR.cc/2026/Conference/Submission13294/Reviewer_46Uu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13294/Reviewer_46Uu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125155033, "cdate": 1762125155033, "tmdate": 1762923962073, "mdate": 1762923962073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}