{"id": "UatVCvzqGm", "number": 20263, "cdate": 1758304226802, "mdate": 1763685795280, "content": {"title": "Self-Knowledge Without a Self? Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns", "abstract": "Generating reliable, calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model’s “self-knowledge”, i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer’s correctness that is accessible to the model itself. However, our experiments reveal that this assumption does not hold. Whether trained or training-free, an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated model attempting the same task. In other words, LLMs have negligible self-knowledge for the purposes of correctness prediction. Moreover, we hypothesize that a key factor in predicting model correctness, i.e., building a “Correctness Model” (CM), is exposure to a target model’s historical predictions. We propose multiple methods to inject this historical correctness information, including training an LLM to predict the confidences of many other LLMs, i.e., creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness of historical predictions from many LLMs and learn patterns and strategies for correctness prediction applicable across datasets and models. We then use CMs as a lens to study the source of the generalization and correctness prediction ability, adjusting their training data and finding that answer phrasing is a strong predictor for correctness. Moreover, our results suggest that a CM’s ability to leverage world knowledge about answers for correctness prediction is a key enabler for generalization. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide composable reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.", "tldr": "", "keywords": ["confidence calibration", "correctness prediction", "self-knowledge", "introspection", "Large Language Model Uncertainty"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea32bdfeef1d91048c6e945261d3977c7a2b6969.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tests whether it is possible to fine-tune an LLM to predict correctness of itself or other LLMs, including multiple LLMs at once, and finds roughly equal performance in predicting an LLM’s performance by itself and other LLMs. They conduct several ablation experiments, including removing the answer and the response from the fine-tuning prompt, effectively fine-tuning the model to predict performance based on the query only, thus forcing it to extract patterns in the question space. Some experiments with in-context learning are also conducted."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- originality: the precise experimental setup (fine-tuning a model to predict correctness from various other models) is original, even though the broader idea has been around for a while\n- quality: the experimental setup is sound and thorough, and so is the conceptualisation and interpretation of the findings\n- clarity: the text is mostly clear\n- significance: the fact that a model’s performance can be predicted as well by another model as by itself is interesting and insightful, particularly when the answer is not included; also, this finding seems to partly contradict that in https://arxiv.org/abs/2410.13787, which sets ground for interesting debate. Moreover, the strength of predictive power is significant. Also, evaluations on selective prediction is important."}, "weaknesses": {"value": "- The paper could discuss more the real-world relevance of this for questions where the ground truth answer is unknown (and for which, therefore, the model fine-tuned to predict performance does not know it). What I mean is: performacne predictors that include the answer in the prompt may rely on knowledge of the ground truth, and the authors indicate how this likely explains the increased performance over not using the answer. However, this is only true when the answer is known to the model. There may be cases where the answer is unknown (to the model and the human users), and where performance prediction is therefore even more important. In practice, one can test the predictive methods on datasets where the performance of the model fine-tuned to predict performance of another one is very low (for instance, as the facts are after the knowledge cut-off of the model trained to predict performance, but not for the target model).\n- I think an addressable weakness is not touching with a particular strand of work exploring the question of whether performance can be predicted, such as:\n    - https://ojs.aaai.org/index.php/AAAI/article/view/21487 introduces the concept of “assessors”, small models trained to predict performance of a main one starting using the question only\n    - https://ceur-ws.org/Vol-3169/paper4.pdf implements assessors for language models\n    - https://arxiv.org/abs/2409.03563 develops assessors that work across multiple language models\n    \n    I invite the authors to indicate how these approaches relate to the one they propose in their related works section. I also believe that some of these approaches can be insightful baselines to compare their method with (for instance, using simple classifiers trained on top of sentence embeddings can further indicate if a LLM is needed as the predictive model, or if instead simpler approaches allow to determine predictive patterns in the prompt space).\n    \n- some minor clarity points:\n    - lines 49-52 seem to suggest calibration is the only thing that matters for confidence, but this is not the case: one can have calibrated confidence by always predicting the success rate, with no discriminative power\n    - it would be great to make more explicit the precise finetuning prompt(s) used in the main text, particularly how the answer, response, and model name are used (for instance, putting them in a figure, instead of hidden among the text)\n        - for instance, the prompt in lines 167-168 asks how the model “will” respond to a prompt, but the authors say that this is appended to a prompt and model response. So why “will”? Is this what is actually used?"}, "questions": {"value": "- the authors’ findings seem to partly contradict those in https://arxiv.org/abs/2410.13787. How would the authors reconcile the two? Is there a substantial difference between the setups that may explain the difference in findings?\n- lines 306-307: “These results indicate that correctness prediction generalizes across families, sizes, and even held-out stronger models.”. Is this due to models failing and succeeding in very similar ways?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1ksJruQfcV", "forum": "UatVCvzqGm", "replyto": "UatVCvzqGm", "signatures": ["ICLR.cc/2026/Conference/Submission20263/Reviewer_LRpD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20263/Reviewer_LRpD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731839633, "cdate": 1761731839633, "tmdate": 1762933745061, "mdate": 1762933745061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the problem of confidence calibration for Large Language Models (LLMs). The authors propose two research questions (RQs):\n\n1. Are LLMs superior to other LLMs in predicting their own correctness?\n\n2. What role does historical information from multiple models play in calibrated correctness prediction?\n\nThe study conducts experiments on open-source LLMs to address these questions."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of LLM verbalized confidence calibration is highly important, and this research possesses significant practical relevance.\n\n2. Both RQs are critical. A comprehensive solution to this issue would have a profound impact on the community, highlighting the paper's exceptional insight.\n\n3. The authors designed a series of progressive experiments to substantiate their claims. The paper is well-structured and reads very clearly.\n\n4. RQ2 is intuitive and is supported by convincing experimental evidence."}, "weaknesses": {"value": "1. The validation for both RQ1 and RQ2 relies exclusively on logit-based confidence scores, which severely limits the paper's persuasive power. As noted in [1], logit-based confidence in large models can be unreliable due to the influence of RLHF. This is particularly problematic for RQ1: verbalized confidence generation is strongly dependent on a semantic understanding of the context. In contrast, logit confidence, which is based on probabilities adjusted by RLHF, is not semantically aligned with the concept of \"confidence.\" Consequently, this approach fails to leverage the powerful In-Context Learning (ICL) capabilities of LLMs.\n\n2. Following W1, the focus on logit-based scores restricts the study to smaller, weaker, open-source models. A critical claim like that in RQ1—which contradicts intuition and community consensus—requires validation on large-scale, state-of-the-art models (e.g., GPT-5, Claude 4, Gemini 2.5 Pro, DeepSeek V3.2) to be convincing. While the current experiments explore multiple angles (which is commendable), the most critical factors—model scale and quantity—are insufficient. The experiments for RQ1 are limited to Qwen2.5-7B and Llama3.1-8B. Even by small-model standards, this sample size is very small. A more appropriate study should include various sizes of Qwen, GPT-OSS models, Gemma, and others to provide a convincing basis for the conclusions.\n\n3. The significance of \"answerless\" confidence is unclear. This metric seems to be a direct assessment of the question's intrinsic difficulty, which could be interpreted as a marginal distribution of confidence summed over all possible answers. However, this approach fails to account for the prior distributions resulting from the model's own predictions for specific answers. The rationale for using this as a basis for an ablation study needs more detailed justification.\n\n[1] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5433–5442, Singapore. Association for Computational Linguistics."}, "questions": {"value": "1. Lines 252-254: Why does removing the model response necessarily eliminate the influence of parametric knowledge?\n\n2. Line 196: Is \"Measuring Confidence\" a typo here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dOFWphQ9iP", "forum": "UatVCvzqGm", "replyto": "UatVCvzqGm", "signatures": ["ICLR.cc/2026/Conference/Submission20263/Reviewer_bn8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20263/Reviewer_bn8A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013732119, "cdate": 1762013732119, "tmdate": 1762933744692, "mdate": 1762933744692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that a model 1) _has no privileged information about its own error distribution_ and hence that it is more effective to 2) _treat the ability to predict correctness / detect error as a downstream task on its own_. (Here I am wording the two points my own way,  hopefully the authors will agree that my phrasing represents their views well.)\n\nThe paper provides evidence for (1) by testing whether a model can predict its own correctness any better than some other model could, the observation is in the negative. The paper goes over a number of design ideas to essentially deliver (2), for which of course it uses data about a model's error distribution (that is, annotation for when a model has been wrong/correct in the past). The paper shows that by engineering this component (the paper calls it a 'correctness model') carefully, the ability to predict correctness can even generalise across a number of conditions. \n\nThe paper is generally well-written, the research is interesting and the findings are likely to be echoed / built upon. That said, I do find it to miss an entire branch of relevant literature, but I will comment on that in weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. RQ1, both its formulation and the simplicity of the observation that supports it is very refreshing. \n\nI personally think it's about time that someone dispels the tacit assumption that models have privileged information outside the merits of carefully engineered heuristics or uncertainty quantifiers (the 'careful' in it being the designer's evolving knowledge - paper after paper - of what works and what doesn't, effectively, a proxy to learning from historical data). For that, I find this paper rather refreshing. \n\n2. I have a problem with how the paper positioned RQ2 (see weakness), but the strategies developed to approach it (the various designs for the so-called 'generalised correction models' and the observations in the various settings) come across as quite thorough."}, "weaknesses": {"value": "My main problem with this paper is that it is written as if confidence and quality estimation had never been approached as ML tasks ever before. Learning from 'historical data about correctness' is precisely what these fields have always done (from SVMs, to Bayesian models, to neural models, to LLMs, with supervision from a single model or via system combination, with and without feature engineering, etc; confidence and quality estimation have been approached by everything in the ML toolbox). The oldest reference I can think of on the spot is [Specia's 2009 paper](https://aclanthology.org/2009.eamt-smart.10/), but please do the literature check, you will uncover more papers than I can list  (from small university labs to the biggest industry labs out there) and you will find that many of these papers are hugely impactful. \n\nTo the best of my judgment, the research in this submission stands, but this paper - even if inadvertently - currently obfuscates that  \"learning from historical data about correctness\" is an actual thing and it's been called QE for at least nearly two decades. \n\nI am not sufficiently close to QE myself, so I cannot tell you whether the designs proposed in the paper as 'correctness models' are at all surprising. I do suspect they aren't, but I will refrain from penalising in that dimension. Instead I will just hope that there's a QE reviewer in the loop. I am however penalising for a kind of obfuscation of literature that I consider harmful."}, "questions": {"value": "My expectation is that the paper should be transparent / clear rather upfront about the point above, namely, the framing of confidence/quality estimation as a task on its own, for which models can be trained on historical data, etc, is generally accepted, well-established and _not_ a contribution of this paper.\n\nOf course, in light of the findings surrounding RQ1, the importance of embracing that framing is clear, and this is an argument you are establishing and supporting in the submission. I appreciate that and am not challenging it. \n\nLast, in your literature check, I'd recommend looking for recent papers (for example from Unbabel or WMT submissions to the QE task; these are just some entry points into modern QE) as the concrete designs you propose might have been proposed before and, I hope you agree, it is only right to give credit where credit is due (even if, incidentally, those papers did not inspire you directly)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pGF0SQhVDr", "forum": "UatVCvzqGm", "replyto": "UatVCvzqGm", "signatures": ["ICLR.cc/2026/Conference/Submission20263/Reviewer_WbFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20263/Reviewer_WbFq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762202912024, "cdate": 1762202912024, "tmdate": 1762933744311, "mdate": 1762933744311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyzed the judgment accuracy of response content by both their own models and other models, and found that models do not exhibit higher accuracy in judging their own answers—instead, more capable models demonstrate higher accuracy in judging answers. Meanwhile, the authors trained a judgment model and observed that it could achieve higher judgment accuracy. They argue that this finding indicates model-related issues can be optimized by leveraging certain patterns and confidence levels to assess the response accuracy of models. Additionally, the authors suggest that both post-hoc calibration and in-context learning (ICL) are effective in improving accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors' writing is highly concise and accessible.\n\nThe topic of confidence is of great importance, especially in the context where hallucinations are becoming increasingly severe under the reinforcement learning paradigm brought by o1/R1."}, "weaknesses": {"value": "Although the authors focus on the confidence field, in the reviewer's opinion, they fail to conduct sufficient literature research.\n\nOn one hand, the authors overlook the field of reward models. The Correctness Model proposed by the authors appears to be a type of reward model, yet there are numerous open-source models in this domain—including General Reward Models and Process Reward Models. Notably, many existing studies in the reward model field have already reached similar conclusions to those presented in this work. But there is no baseline from reward models.\n\nDespite the rigorous logical flow of the research content, its innovation and inspirational value are, in the reviewer's view, insufficient to support its acceptance by top-tier conferences."}, "questions": {"value": "As stated in Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kO4M25DyWg", "forum": "UatVCvzqGm", "replyto": "UatVCvzqGm", "signatures": ["ICLR.cc/2026/Conference/Submission20263/Reviewer_CcM8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20263/Reviewer_CcM8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762699883966, "cdate": 1762699883966, "tmdate": 1762933743894, "mdate": 1762933743894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}