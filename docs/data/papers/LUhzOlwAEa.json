{"id": "LUhzOlwAEa", "number": 13207, "cdate": 1758215091094, "mdate": 1763742086027, "content": {"title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning", "abstract": "The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning.\nWe introduce \\textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.", "tldr": "We propose a benchmark for Thinking with Images reasoning", "keywords": ["Benchmark", "Thinking with Images"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0919cd5614158045b7727083f9a1c8c5642d905.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TIR-Bench, a novel and comprehensive benchmark designed to evaluate the agentic thinking-with-images capabilities of MLLMs. The authors argue that existing benchmarks focus on static image understanding and fail to assess the ability of models like O3 to actively use tools to transform and manipulate images during problem-solving. TIR-Bench comprises 13 diverse tasks that are intentionally designed to be unsolvable without tool use. The evaluation of 22 models demonstrates that the benchmark is highly challenging (46% accuracy for the best model, O3-TU) and that the availability of tool-use capabilities is the key differentiator for performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a significant gap in current multimodal evaluation: the lack of assessment for agentic or tool-assisted image reasoning, which is a key emergent capability of frontier models.\n2. The 13 tasks in TIR-Bench are cleverly designed with a common trait: they cannot be solved by static observation alone. This strongly compels models to adopt a \"think-tool-observe\" loop, aligning perfectly with the benchmark's evaluation goals.\n3. The experiments, especially the comparison between tool-enabled and non-tool-enabled models, clearly demonstrate that tool use is essential for these tasks and quantify the significant room for improvement, even for SOTA models."}, "weaknesses": {"value": "1. The paper's core evaluation is on \"tool use,\" but this is primarily equated to a \"Code Interpreter\" in the experiments. This makes the evaluation more of a test of a model's \"Python CV coding ability\" rather than a test of general-purpose \"tool calling\" capabilities.\n2. In the experimental setup, proprietary SOTA models (O3-TU, O4-mini-TU) are granted access to a code interpreter, while open-source models are not. This is an unfair comparison, as the poor performance of open-source models might stem from the lack of a robust execution sandbox, not from inferior reasoning.\n3. While the paper states many samples are newly created, a portion of the data is sourced from existing public datasets. It is difficult to guarantee that large models like O3 have not seen these or highly similar samples during pre-training.\n4. The best model (O3-TU) scores only 46%, meaning it fails most of the time. The paper mentions some failures but lacks a systematic analysis of why. On which tasks do models fail most? What are the common failure modes?\n5. Some tasks (e.g., Word Search, Low-Light VQA) feel overly synthetic, designed specifically to force the use of a particular tool (e.g., pixel comparison, image enhancement). While this serves the benchmark's purpose, it is debatable whether these tasks represent the diverse, unpredictable tool-use scenarios MLLMs will face in the real world."}, "questions": {"value": "1. Many tasks in TIR-Bench seem better suited for traditional Computer Vision algorithms than for MLLMs. While models can solve them by writing code to call CV libraries, it's debatable whether this tests the MLLM's \"reasoning\" or its ability to recall and execute CV library APIs.\n2. For a benchmark claiming to evaluate a \"thinking process,\" the paper relies solely on final-answer accuracy. This obscures crucial details: did the model solve the problem efficiently, or via extensive, ineffective trial-and-error? Metrics for the quality, efficiency, or plausibility of the reasoning path are missing.\n3. The pilot study in Sec 4.5 is conducted on only one task. The finding that \"Tool-Use SFT\" outperforms \"Direct SFT\" is insightful, but this conclusion can hardly be generalized to the other 12 complex tasks in TIR-Bench based on this single experiment.\n4. See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9rQRNSRLwU", "forum": "LUhzOlwAEa", "replyto": "LUhzOlwAEa", "signatures": ["ICLR.cc/2026/Conference/Submission13207/Reviewer_WUfX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13207/Reviewer_WUfX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761436663516, "cdate": 1761436663516, "tmdate": 1762923900226, "mdate": 1762923900226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all four reviewers for their time and valuable, constructive feedback. We are encouraged that the reviewers generally recognized the novelty and core contributions of TIR-Bench.\n\nWe summarize the recognized contributions and revisions in the updated paper here and provide separate responses to each reviewer.\n\n# Main contributions.\n- **Novelty.** The paper identifies and elevates the task of \"thinking-with-images\" beyond simple operations to more complex and diverse capabilities [8Z6B]; It proposes tasks that are cleverly designed to be unsolvable by static observation alone, compelling models to use a \"think-tool-observe\" loop [WufX]; The paper also identifies more difficult tasks suitable for new code-execution-based approaches than previous datasets [Gpt1].\n - **Analysis and Experiments.** The benchmark is comprehensive, covering 13 diverse tasks [ALVA]; The experiments clearly demonstrate that tool use is essential for these tasks and quantify the significant room for improvement, even for SOTA models [WufX]; The benchmark is noted as being sufficiently challenging [ALVA], highlights issues with current SOTA models [Gpt1], and includes a wide selection of model evaluations [Gpt1]. \n- **Impact.** The paper addresses a significant gap [WufX] and a critical need [Gpt1] for new benchmarks in the area of agentic or tool-assisted image reasoning; A central contribution is that the benchmark explicitly investigates how models interact with images, which is crucial for understanding agent use behavior [ALVA]; It is recognized as a potentially useful, long-term suite for measuring MLLM reasoning ability [ALVA]. \n\n# The key changes in the updated paper and more experiments and analysis included:\n- We include six additional models for a broader evaluation in **Table 2**.\n- We re-implemented PyVision for Qwen2.5-VL-72B and Qwen3-VL-235B by providing them with a code-execution environment and include the results in **Table 2**.\n- We conducted additional experiments on agentic SFT on InternVL3-8B and Qwen-2.5-VL-7B and evaluated its performance on the rotation game and OCR tasks.\n- We have significantly revised the caption of **Figure 3** to enhance clarity.\n- We analyzed the error types of the PyVision model in **Appendix H**.\n- We summarize the created tools of PyVision response for each task in **Apppendix I**.\n- We revise **Sec. 3.2** to offer a clearer description of the annotation and checking procedures.\n- We show the experimental results of synthetic data and human-annotated data on TIR-Bench on **Appendix F.** \n- We add detailed sources for each task in **Appendix B** and **Section 3**.\n- We have redesigned figures showing model responses with interleaved thinking/code (**Figure 4, 5, 24, 25**).\n- We evaluate PyVision's efficiency by analyzing interaction turns (detailed in **Appendix J and Figure 26**)."}}, "id": "b6JaXwaQQF", "forum": "LUhzOlwAEa", "replyto": "LUhzOlwAEa", "signatures": ["ICLR.cc/2026/Conference/Submission13207/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13207/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13207/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742374292, "cdate": 1763742374292, "tmdate": 1763742374292, "mdate": 1763742374292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present TIR-Bench, a benchmark for code/tool-assisted visual reasoning on images. They identify that previous benchmarks only require simple tools at most. They curate the benchmark for tasks that require complex code/tool-assisted reasoning, with a focus on dynamic or agentic approaches, with a combination of new problems and those from existing datasets. The results suggest all models have difficulty with these tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a critical need for new benchmarks in this area.\n- It identifies more difficult tasks than previous datasets suitable for new code-execution based approaches.\n- It highlights issues with current state-of-the-art models. \n- Table 2 presents a wide selection of model evaluations. \n- Many qualitative examples from the benchmark are shown.\n- Preliminary experiments on SFT are presented."}, "weaknesses": {"value": "- The data seems to need more verification; annotation by one student without at least one more pair of eyes checking may not be reliable.\n- Synthetic and hand-annotated data are mixed here, when they seem to evaluate markedly different capabilities. Perhaps they should be switched? \n- The provenance of the data is unclear, posing potential issues for usage. \n- The figures showing model responses with interleaved thinking/code, like 4, 5, 24, 25 are difficult to parse."}, "questions": {"value": "- The function calling and code comparison is interesting, but is the primary difference actually the presence of predefined tools rather than whether function calling or full code generation is used? This should be clarified more."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0fPEDaMhTz", "forum": "LUhzOlwAEa", "replyto": "LUhzOlwAEa", "signatures": ["ICLR.cc/2026/Conference/Submission13207/Reviewer_Gpt1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13207/Reviewer_Gpt1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937464601, "cdate": 1761937464601, "tmdate": 1762923899767, "mdate": 1762923899767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TIR-Bench, a new benchmark designed to evaluate the agentic visual reasoning ability of multimodal large language models (MLLMs). Such a thinking-with-images process requires the models to actively manipulate images as part of the reasoning process rather than static analysis. The benchmark consists of 13 tasks, such as rotation, maze solving, and jigsaw puzzles, and the dataset has 1,215 examples. The authors evaluate 22 MLLMs, including open-source, proprietary, and tool-using models. Results show that the TIR-Bench is universally challenging, with 46% accuracy from o3-TU as the performance. It reveals that non-agentic models perform substantially poorly, highlighting the necessity of tool-based reasoning for complex visual tasks. Finally, the authors also study function calling behaviors and run a pilot fine-tuning comparison on rotated-OCR, finding that agentic/tool-use SFT outperforms direct SFT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The task design is comprehensive, covering 13 diverse tasks that span a broad range of visual reasoning abilities. Each task forces models to manipulate images rather than passively describing them, enabling a faithful evaluation of thinking-with-images reasoning.\n- The benchmark is sufficiently challenging and ensures it is a useful, long-term suite for measuring MLLMs’ reasoning ability.\n- A central contribution of TIR-Bench is that it explicitly investigates how models interact with images, which is crucial to understand the model’s agent use behavior and assess their ability for visual reasoning tasks."}, "weaknesses": {"value": "- TIR-Bench is designed to evaluate models’ tool-based visual reasoning ability, but many evaluated models (e.g., LLaVA, Qwen2.5-VL, InternVL) cannot execute code or invoke tools. Their inclusion mainly serves as a static baseline rather than a true test of “thinking-with-images.”\n- 1215 examples are divided into 13 tasks, making several tasks have modest data sizes. It raises concerns about the benchmark robustness given the data scale.\n- Using GPT-4o to extract final answers can introduce parsing bias. However, the authors do not verify GPT-4o’s extraction accuracy. The influence of the extraction process should be excluded.\n- The agentic vs direct SFT pilot uses Qwen-2.5-VL-7B only for a single task. Broader models and tasks are needed to generalize the conclusion.\n- Some models can use external tools during reasoning, while others do not have access. This discrepancy makes it difficult to interpret the model’s intrinsic reasoning capability and limitations. The authors should clarify what external tools are accessible for each model.\n- While the paper defines thinking with images as a process in which models manipulate images through tool use, the results in Section 4.4 indicate that models do not always invoke tools unless explicitly guided by the prompt. This raises questions about whether the poorly performing models fail due to insufficient reasoning ability or simply because they were not prompted to use available tools."}, "questions": {"value": "- Could the authors clarify the rationale for including non-agentic models for tool-using ability, given that they can not use tools or write code and execute?\n- Could the authors justify whether the data scale is sufficient to get statistically meaningful and reliable conclusions?\n- Could the authors validate the effectiveness of using GPT-4o for answer extraction?\n- Could the authors provide the fine-tuning comparison on more models and different tasks?\n- For tasks that inherently benefit from external tools, please report the tools the models use.\n- Could the authors provide the exact prompts used during benchmarking and clarify to what extent the models are instructed or encouraged to use tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7zcfqXF525", "forum": "LUhzOlwAEa", "replyto": "LUhzOlwAEa", "signatures": ["ICLR.cc/2026/Conference/Submission13207/Reviewer_ALVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13207/Reviewer_ALVA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940965824, "cdate": 1761940965824, "tmdate": 1762923899384, "mdate": 1762923899384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to better evaluate the ability to think with images in the chain of thought. Current work focuses primarily on localization and cropping capabilities. To promote and inspire broader capabilities, this paper proposes a more challenging benchmark involving thinking with images. The proposed benchmark includes 13 diverse tasks, encompassing image understanding tasks such as color setting, image selection, and maze solving. The authors evaluated open-source and proprietary models in experiments, and further evaluated the proprietary model, which can utilize image processing tools."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper elevates thinking-with-images from simple localization and cropping capabilities to more complex and diverse ones.\n\n2. The authors set up a tool-using scenario, experimenting with state-of-the-art models o4-mini and o3 on the proposed benchmark, achieving the expected performance improvements."}, "weaknesses": {"value": "1. Table 2 only evaluates a limited number of open-source models, covering only Illava, Qwen2.5-VL, and InternVL3. Many large-scale open-source multimodal models have not been extensively evaluated.\n\n2. The authors did not attempt to configure tool-using on open-source models with larger parameters, such as those with over 32 bytes of parameters. Performing such evaluations on the proposed benchmark would provide a clearer understanding of the actual capabilities of the open-source models."}, "questions": {"value": "1. Refer to the issues raised in the weakness section.\n2. Figure 3 is unclear and difficult to interpret. The authors should provide a more detailed explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cA7DaQ6lRf", "forum": "LUhzOlwAEa", "replyto": "LUhzOlwAEa", "signatures": ["ICLR.cc/2026/Conference/Submission13207/Reviewer_8Z6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13207/Reviewer_8Z6B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999279730, "cdate": 1761999279730, "tmdate": 1762923899079, "mdate": 1762923899079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}