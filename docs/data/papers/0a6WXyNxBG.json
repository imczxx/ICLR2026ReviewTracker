{"id": "0a6WXyNxBG", "number": 6042, "cdate": 1757951400609, "mdate": 1759897938389, "content": {"title": "MEMORIA: A Large Language Model, Instruction Data and Evaluation Benchmark for Intangible Cultural Heritage", "abstract": "Although large language models (LLMs) have demonstrated remarkable capabilities in natural language processing, there are no publicly available LLMs specifically tailored for Intangible Cultural Heritage (ICH), instruction tuning datasets, or comprehensive evaluation benchmarks, which is critical for advancing the preservation, understanding, and transmission of cultural knowledge through artificial intelligence. This paper introduces MEMORIA, a comprehensive framework for intangible cultural heritage preservation through AI. MEMORIA includes: (1) ICHLLM, the first ICH-specific large language model based on fine-tuning LLaMA (with both 7B and 13B versions) using instruction data; (2) CHIT, a large-scale instruction dataset with 158K data samples covering diverse cultural domains and languages; and (3) ICHEB, the first comprehensive evaluation benchmark with 6 tasks and 13 datasets spanning knowledge understanding, classification, generation, and cross-cultural translation tasks. We first construct the large-scale multi-task instruction dataset CHIT, considering various ICH categories, diverse data formats and multilingual content spanning over 50 languages. Within the MEMORIA framework, we develop ICHLLM by fine-tuning LLaMA (7B and 13B versions) with the constructed dataset to follow instructions for various ICH-related tasks while maintaining cultural sensitivity and accuracy. To support the evaluation of ICH-focused LLMs, we propose a standardized benchmark that covers critical tasks including ICH knowledge question answering, cultural entity recognition, heritage classification, narrative generation, and cross-cultural knowledge translation. With this benchmark, we conduct a comprehensive analysis of ICHLLM and several existing LLMs, revealing their capabilities and limitations in understanding and preserving cultural heritage knowledge. The model, datasets, benchmark, and experimental results will be open-sourced to facilitate future research in cultural AI and digital humanities. Our anonymous code can be available at https://anonymous.4open.science/r/MEMORIA.", "tldr": "", "keywords": ["Intangible Cultural Heritage (ICH)", "Cultural AI", "Instruction Tuning", "Large Language Models", "Evaluation Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/994f64f88d0e8d267e4f17f2b5f40c8050346a9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MEMORIA, a comprehensive framework that integrates large language models (LLMs) with the preservation of Intangible Cultural Heritage (ICH). It consists of three key components:(1) CHIT, an instruction-tuning dataset containing 159K multi-task and multilingual samples that cover all five UNESCO-defined ICH domains;(2) ICHLLM, two culturally aligned models (7B and 13B) fine-tuned from LLaMA;(3) ICHEB, a benchmark containing six task types and thirteen datasets designed to evaluate both understanding and generation capabilities. Experimental results show that ICHLLM substantially outperforms general-purpose models such as GPT-4, Claude-3, and BLOOM on ICH-related tasks, exhibiting stronger cultural sensitivity, fairness, multilingual transfer, and authenticity. The authors plan to release all models, datasets, and benchmarks to support open research in cultural AI."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The application of LLMs to ICH preservation is largely unexplored. This work addresses an important gap and has significant implications for digital humanities and cultural sustainability. MEMORIA forms a closed loop from dataset construction (CHIT), model training (ICHLLM), to evaluation (ICHEB), providing a complete methodological framework. The dataset spans all five UNESCO domains and over fifty languages, incorporating oral traditions, performing arts, social practices, ecological knowledge, and traditional craftsmanship. ICHEB introduces domain-specific cultural metrics such as Cultural Score and Cultural Fidelity, demonstrating an in-depth understanding of ICH evaluation needs."}, "weaknesses": {"value": "The current model deals exclusively with textual data, while many ICH forms are inherently multimodal and embodied. Although future work is mentioned, the current contribution does not yet address this limitation, reducing its applicability to performative heritage.\n\nThe model demonstrates improvements in multilingual transfer, but the paper also acknowledges remaining challenges in under-documented or endangered languages. This limits its potential utility for the most vulnerable heritage contexts.\n\nEven the best-performing ICHLLM-13B shows a ~19.8% overall error rate in the failure analysis (e.g., cultural misclassification, temporal inconsistency). Given the sensitivity of cultural heritage preservation, such an error level remains concerning.\n\nThe CHIT dataset uses asymmetric augmentation: *understanding* tasks receive ten instruction variants per sample, while *generation* tasks receive only one. Although the authors claim this avoids repetitive patterns, it may cause the model to overfit to understanding tasks and underperform on generation ones—indeed reflected in the results where generation improvements are smaller."}, "questions": {"value": "1. The error analysis in Appendix J shows that “linguistic oversimplification” accounts for 22.4% of failures. Does this indicate a tendency toward cultural flattening, undermining the model’s intended role as a preservation tool? Could additional stylistic constraints or culture-specific regularization mitigate this?\n\n2. Regarding data imbalance: understanding tasks are expanded tenfold, whereas generation tasks are sampled only once. What empirical evidence supports this decision? Have you conducted ablation studies to confirm that multiple generation prompts indeed degrade quality?\n\n3. While Cultural Score improves over GPT-4, it remains below expert-level baselines. Would incorporating human feedback alignment (e.g., RLAIF) or expert-annotated contrastive learning improve cultural authenticity?\n\n4. Since ICHLLM aims to protect cultural heritage, have you considered incorporating human-in-the-loop or community validation to ensure that indigenous and local participants can verify and refine model outputs, rather than being replaced by AI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vyvRFqdUgu", "forum": "0a6WXyNxBG", "replyto": "0a6WXyNxBG", "signatures": ["ICLR.cc/2026/Conference/Submission6042/Reviewer_QfMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6042/Reviewer_QfMA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468258943, "cdate": 1761468258943, "tmdate": 1762918426230, "mdate": 1762918426230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces resources for intangible cultural heritage (ICH), including models, instruction-tuning data, and an evaluation benchmark. The models are based on Llama1-7B and Llama1-13B; henceforth ICHLLM-7B and 13B. The instruction tuning data consists of around 178K datapoints with a variety of languages (up to 50+), where for each raw sample, 10 prompt-completion is generated. The benchmark's tasks include 13 datasets consisting of 6 tasks: entity recognition, text classification, knowledge QA, concept matching, narrative generation, and translation. The both 7B and 13B models are benchmarked against GPT-4, Claude-3, BLOOM-176B, Vicuna-13B, Alpaca-7B, CulturalLLM-7B (Li et al., 2024). The results show that the introduced models outperform the second-ranked GPT-4 on all tasks. The paper also implements human evaluation on 100 samples from each model on a 1-5 scale for the tasks of narrative generation and translation. \n\nThe paper shows that ICHLLM-13B obtains the highest score in terms of authenticity, accuracy, educational content, and 'respect for sacred knowledge'. The paper furthermore shows several ablations regarding few-shot prompting and shows that using more demonstrations (up to 10 shots) improves performance. There is also an analysis of inference costs. Lastly, the paper shows average performance of the models across different regions in the world (e.g., Africa, Europe, Latin America, etc)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles a significant problem. Preserving intangible cultural heritage is important, especially in a larger vision where LLMs might become information providers.\n2. The paper releases a substantial amount of resources: Models, instruction tuning data, and a benchmark.\n3. The experimental results are solid and highlight the importance of procedures such as significance testing."}, "weaknesses": {"value": "1. The related work is relatively thin with respect to, e.g., open-source/open-weight LLMs. Where the most recent general LLM cited dates from 2023 -> Vicuna. There are several significant releases in the meantime, e.g., Olmo, Qwen, Mistral, Gemma, and so forth.\n2. It is unclear whether the proposed benchmark (ICHEB; Table 3) covers all the languages depicted in the instruction tuning data in Table 2. If so, for example, UNESCO-ICH would have 5690/45 = ~126 test examples per language, which is on the relatively low side; other recent multilingual benchmark contain around a minimum of 1000 examples each for each language, e.g., [1]\n3. The task setup is not entirely convincing. How do we ensure we are testing cultural heritage, not just task formatting? If we take the UNESCO-ICH subtask of entity recognition and take the example output in Table 17 in Appendix I, how should a model like GPT-4 know, instead of classifying 'ruwatan' as RITUAL (as mentioned, a more generic term) as a PURIFICIATION_RITUAL (more specific; which ICHLLM does correct), without getting the complete possible entities list in-context? If a model, such as ICHLLM, is specifically fine-tuned to output these labels, it is clear it will perform better.\n\n[1] https://openreview.net/forum?id=k3gCieTXeY"}, "questions": {"value": "1. In L1177: The paper mentions \"Differential learning rates (..) prevent catastrophic forgetting while enabling deep cultural integration, with larger models requiring gentler optimization to maintain stability on specialized domains\". I find this a rather substantial claim, how would one measure that these specific learning rates prevent catastrophic forgetting and also integrate 'deep cultural knowledge'?\n2. In the example output tables in Appendix I, it would have been helpful to know what the gold labels are. For example, in Table 19, several parts of ICHLLM's output are highlighted, which are five elements of Korean traditional medicine, but GPT-4 also appears to generate the same 'five elements'. What exactly are the exact match tokens being matched? The analysis of ICHLLM's output mentions that it provides a complete answer with \"Korean-specific terminology, distinguishes from Chinese system\", but there is no mention of this being a requirement in the Question prompt nor Cultural Context prompt. \n3. What is the use of the analysis in Table 10? The performance/param metric is highlighted for ICHLLM, but is neither the lowest nor the highest performance number across all models, why is it being highlighted here? Additionally, as BLOOM-176B being outlined taking 352 GB of memory, how did it fit on a single 80GB A100 GPU (as mentioned in the caption)?."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DWpoH4VszC", "forum": "0a6WXyNxBG", "replyto": "0a6WXyNxBG", "signatures": ["ICLR.cc/2026/Conference/Submission6042/Reviewer_Rzjz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6042/Reviewer_Rzjz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666054392, "cdate": 1761666054392, "tmdate": 1762918425783, "mdate": 1762918425783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MEMORIA, a comprehensive AI framework for preserving and promoting Intangible Cultural Heritage (ICH). MEMORIA comprises ICHLLM, the first ICH-specific large language model fine-tuned from LLaMA; CHIT, a 159K-sample multilingual instruction dataset; and ICHEB, a benchmark with six tasks and 13 datasets for evaluating cultural understanding and generation. Experiments show that ICHLLM enhances cultural sensitivity and task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a large-scale dataset for Intangible Cultural Heritage, covering multiple aspects of cultural knowledge.\n2. It finetunes ICH-specific large language model to support cultural understanding."}, "weaknesses": {"value": "1. For the benchmark, the paper doesn't hire some human annotators to verify the data samples. The data source from the community may have the bias problem which may intensify the cultural bias problem.\n2. For comparison, can you compare with other related works, e.g. CuturalBank, CulturePark?"}, "questions": {"value": "1. The paper mentions there are expert validation for the benchmark. Can you provide more details on human annotator and annotation process?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "There may be cultural bias problem on the constructed benchmark."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fiWtJ9iHGM", "forum": "0a6WXyNxBG", "replyto": "0a6WXyNxBG", "signatures": ["ICLR.cc/2026/Conference/Submission6042/Reviewer_xCVm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6042/Reviewer_xCVm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943772637, "cdate": 1761943772637, "tmdate": 1762918424912, "mdate": 1762918424912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}