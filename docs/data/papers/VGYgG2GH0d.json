{"id": "VGYgG2GH0d", "number": 15671, "cdate": 1758253712082, "mdate": 1763719561568, "content": {"title": "MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents", "abstract": "Existing multimodal browsing benchmarks often fail to require genuine multimodal reasoning, as many tasks can be solved with text-only heuristics without vision-in-the-loop verification. We introduce MMSearch-Plus, a 311-task benchmark that enforces multimodal understanding by requiring extraction and propagation of fine-grained visual cues through iterative image–text retrieval and cross-validation under retrieval noise.\nOur curation procedure seeds questions whose answers require extrapolating from spatial cues and temporal traces to out-of-image facts such as events, dates, and venues.\nBeyond the dataset, we provide a model-agnostic agent framework with standard browsing tools and a set-of-mark (SoM) module, which lets the agent place marks, crop subregions, and launch targeted image/text searches. SoM enables provenance-aware zoom-and-retrieve and improves robustness in multi-step reasoning.\nWe evaluated closed- and open-source MLLMs in this framework. The strongest system achieves an end-to-end accuracy of 36.0%, and integrating SoM produces consistent gains in multiple settings, with improvements up to +3.9 points.\nFrom failure analysis, we observe recurring errors in locating relevant webpages and distinguishing between visually similar events. These results underscore the challenges of real-world multimodal search and establish MMSearch-Plus as a rigorous benchmark for advancing agentic MLLMs.\n\nOur dataset is included in the supplementary material and the accompanying code at https://anonymous.4open.science/status/MMSearch-Plus-D54E.", "tldr": "We provide a challenging multimodal browsing benchmark and an agent framework with search tools and Set-of-Mark for provenance-aware zoom-and-retrieve.", "keywords": ["Large Multimodal Model", "AI Search Engine", "Benchmark", "Agent Framework"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b86bdc2c76669ae63e2db13d101504a07fc0bfc.pdf", "supplementary_material": "/attachment/9137953eb657a71ec3498a00a1a5d8e1a275a2af.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MMSearch-Plus, a multimodal browsing benchmark that requires models to perform iterative multimodal retrieval and cross-validation under noise. The benchmark is constructed using a spatial-temporal extrapolation curation pipeline that constructs 311 tasks requiring models to extract localized visual cues and extrapolate to out-of-image facts. The authors also proposed an agent framework with web search tools and a set-of-mark module for the proposed task. The paper further evaluates different closed-source and open-source models with the proposed framework on the proposed benchmark and provides error analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a real weakness in existing multimodal search benchmarks—tasks solvable with text-only heuristics—and systematically designs MMSearch-Plus to require true multimodal reasoning.\n\n2. The paper proposed a suitable agent framework for the proposed benchmark and conducted comprehensive evaluations for different models with multiple search modes, providing a solid empirical foundation for further research.\n\n3. The authors provide detailed error categorization, offering actionable insights into model weaknesses in long-horizon multimodal search."}, "weaknesses": {"value": "1. The dataset size is relatively small, and all samples are generated through the same spatial-temporal extrapolation pipeline. This design choice may introduce human bias and constrain the diversity of reasoning patterns, leading to potentially predictable task structures. As a result, the benchmark might be vulnerable to overfitting or data-specific “hacks,” where a model trained on a small amount of similar data could achieve disproportionately high scores.\n\n2. To better support interpretability and diagnostic analysis, it would be highly beneficial to annotate fine-grained evidence—for example, marking which visual regions are essential or sufficient for solving each task. Such annotations would allow for a deeper understanding of model failures and reasoning behaviors.\n\n3. While the paper introduces zoom-in and cropping tools within the evaluation framework, it lacks experimental evidence demonstrating whether these tools are genuinely necessary or beneficial for specific task types, or whether current models can effectively leverage them. \n\n4. Some tasks can be solved using the model’s internal knowledge rather than external retrieval, which weakens the benchmark’s diagnostic focus on search-based reasoning. Future iterations might consider constructing questions from recent or dynamically updated sources and designing a reusable pipeline to continuously refresh the benchmark content, ensuring long-term relevance.\n\n5. The low human performance reported on MMSearch-Plus raises concerns about possible ambiguity or excessive difficulty in certain questions. This suggests that some samples may not have a clearly defined or uniquely inferable answer, limiting their reliability for model evaluation."}, "questions": {"value": "Since the benchmark emphasizes iterative multimodal search and validation, the “easy” split includes samples that certain models can already answer correctly without any search steps. Why do the authors still keep these samples in the benchmark? Would it be more reasonable to have human annotators label the necessary reasoning or tool-use steps and then categorize difficulty based on that process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78L6wdVsmB", "forum": "VGYgG2GH0d", "replyto": "VGYgG2GH0d", "signatures": ["ICLR.cc/2026/Conference/Submission15671/Reviewer_sEzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15671/Reviewer_sEzM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489976765, "cdate": 1761489976765, "tmdate": 1762925925318, "mdate": 1762925925318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MMSearch-Plus is a multimodal searching benchmark with 311 tasks that forces multimodal reasoning by requiring agents to extract fine-grained, localized visual cues and propagate them through iterative image-text retrieval. The tasks also require provenance checks under retrieval noise to reach those \"out-of-image\" facts like events, dates, venues. The authors also introduce Spatial–Temporal Extrapolation to curate questions and provide an agent framework with a Set-of-Mark (SoM) zoom-and-retrieve module for searching. Results show that: even the best closed-source system reaches only 36.0% with full rollout, and SoM yields consistent gains up to +3.9 points; dominant failures involve missing relevant webpages and confusing visually similar events. The benchmark thus serves as a rigorous stress test and common yardstick for agentic MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper clearly identifies limitations in MMSearch and proposes a carefully designed dataset-curation process to construct a more challenging benchmark where information is intentionally hidden, thereby requiring genuine visual reasoning rather than shortcut cues.\n\n(2) The experiments and evaluation are comprehensive and detailed, comparing cutting-edge open-source and proprietary models across four search modes and multiple task subsets.\n\n(3) The analysis is thorough, offering interesting observations and insights alongside a well-reasoned error analysis."}, "weaknesses": {"value": "(1) The average answer length is relatively short, suggesting that many items may be closer to MCQ-style or “single-point” questions; the benchmark may under-represent open-ended QA.\n\n(2) While the benchmark is valuable for provenance-aware retrieval, it is less comprehensive for many real-world agent tasks like cross-site form/API interactions. This may bias the evaluation toward retrieval-and-verification strength while being less sensitive to interactive capabilities."}, "questions": {"value": "(1) You note that models sometimes “zoom in without subsequently performing region-based retrieval.” Could you report the proportion of zoom actions that are followed by a subimage search, and quantify the marginal contribution of that step to final accuracy?\n\n(2) Why does performance on the Easy subset decrease when moving from \"image-only\" search to the \"full-rollout\" setting? Is this due to distractor exposure or over-retrieval, and can you provide supporting diagnostics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BjRznyMbhH", "forum": "VGYgG2GH0d", "replyto": "VGYgG2GH0d", "signatures": ["ICLR.cc/2026/Conference/Submission15671/Reviewer_SjaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15671/Reviewer_SjaW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858911880, "cdate": 1761858911880, "tmdate": 1762925924871, "mdate": 1762925924871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new VQA benchmark. To answer the questions, models should understand both spatial and temporal knowledge. The authors show that existing models perform poorly on the benchmark but achieve some improvement with set-of-mark prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed benchmark, where questions require both visual and textual cues to be answered during web browsing, is interesting.\n- The experiments are comprehensive, evaluating various models across different categories and difficulty levels.\n- The proposed approach --- i.e., identifying subregions through set-of-marking and searching those regions on the website --- looks interesting and promising."}, "weaknesses": {"value": "- Novelty of the benchmark: Evaluating a model’s spatial and external temporal knowledge has been widely studied in conventional VQA tasks (e.g., [1], [2]). I feel the authors simply extend these existing works to the web-browsing domain.\n\n- The paper is somewhat difficult to follow, and its core contribution is not immediately clear at first glance.\n\n- Since you call image/text search APIs for every SoM-defined subregion, the runtime could increase substantially, which may be too costly for real-world deployment. Do you have strategies to mitigate this overhead?\n\n[1] Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?, EMNLP'23.\n\n[2] Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering, EMNLP'22.\n\n[3] GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering, CVPR'19."}, "questions": {"value": "- The text appearing within the image (i.e., scene text) could arguably be treated as both visual and textual information. Did you apply both image and text searches on this scene-text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WR5Eq9T7qK", "forum": "VGYgG2GH0d", "replyto": "VGYgG2GH0d", "signatures": ["ICLR.cc/2026/Conference/Submission15671/Reviewer_WMkC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15671/Reviewer_WMkC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929488733, "cdate": 1761929488733, "tmdate": 1762925924289, "mdate": 1762925924289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark called MMSearch-Plus (311 tasks) designed to force true multimodal reasoning by requiring Fine-grained visual cue extraction, search under retrieval noise, and multi-step visual–textual cross-validation. This addresses the drawbacks of existing multimodal browsing benchmarks, such as MMSearch, in which many tasks can be solved by text-only reasoning. It also contributes a\nmodel-agnostic agent framework with standard browsing tools and a set of mark module, which lets the agent place marks, crop subregions, and launch targeted image/text searches. The authors show that even the strongest MLLMs do not perform well on this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The core design decisions, such as requiring fine-grained, exhaustive visual reasoning for answering the question make sense."}, "weaknesses": {"value": "1. The authors did not include statistics or any reference to how many questions in MMSearch are solvable by text-only browsing."}, "questions": {"value": "How many questions in MMSearch are solvable by text-only browsing? This is an important motivation for the current benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VeWgT8BXW1", "forum": "VGYgG2GH0d", "replyto": "VGYgG2GH0d", "signatures": ["ICLR.cc/2026/Conference/Submission15671/Reviewer_xCaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15671/Reviewer_xCaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973841019, "cdate": 1761973841019, "tmdate": 1762925923618, "mdate": 1762925923618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}