{"id": "UvQOcw2oCD", "number": 12819, "cdate": 1758210555994, "mdate": 1759897482881, "content": {"title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Diffusion Diffusion Process", "abstract": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and execute corresponding actions as an embodied agent. Recent advancements have integrated future images into the understanding-action loop, enabling foresight-driven policies that reduce abstract action prediction to a more tractable inverse kinematics problem. However, existing models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. In this work, we propose Unified Diffusion VLAs, which tightly couple understanding, generation, and action in a mutually reinforcing manner. Our method optimizes the generation of actions and images jointly through a synchronous denoising diffusion process, where action tokens progressively attend to future image tokens. This iterative refinement enables actions to evolve from initialization with sufficient visual guidance, ensuring precise action execution. We introduce a hybrid attention mechanism and the Joint Discrete Denoising Diffusion Process (JD3P), which integrates multiple modalities into a unified trajectory. We also propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv, and we demonstrate its effectiveness through ablation studies and real-world evaluations.", "tldr": "", "keywords": ["Vision-language-action Model", "Unified Multimodal Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e10ed4e6fdeef08a4da5540c4b2cbaae81b0d7d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified vision-language-action model that jointly generates future images and predicts actions through a single synchronous denoising trajectory (JD3P), coupling understanding, generation, and acting in a shared tokenized space with hybrid attention. This joint diffusion approach eliminates reliance on external experts and avoids separating generation from control, delivering state-of-the-art performance on CALVIN, LIBERO, and SimplerEnv with 4× faster inference than autoregressive baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel Joint Discrete Denoising Diffusion Process that enables the generation of image and action tokens.  This formulation effectively bridges the gap between perception and control, achieving intrinsic synergy across modalities.\n\nMethods: By constructing a unified tokenized multimodal space and employing a **hybrid attention mechanism** (bidirectional within modalities, causal across modalities), the model design is principled and well-grounded.\n\nExperiments: The experiments on simulation are comprehensive (across 3 simulation benchmarks)."}, "weaknesses": {"value": "1. **Limited theoretical depth:** The motivation for JD3P is intuitively explained, yet the heoretical justification of why joint denoising leads to superior cross-modal alignment remains shallow. A more formal analysis or deeper ablation would strengthen the claim.\n2. **Fairness of comparisons:** Some baselines may differ in model capacity or data scale, and these confounding factors are not fully controlled, potentially exaggerating UD-VLA’s relative gains (**more apple-to-apple ablation baselines are needed**).\n3. **Overly dense presentation:** The paper is lengthy and symbol-heavy. Some equations (e.g., βt, κt) appear abruptly, requiring readers to backtrack for context."}, "questions": {"value": "1. The single-step mask-prediction loss (Eq. 8) might limit temporal dependency modeling. Have multi-step noise-schedule variants been explored?\n2. A key question is whether JD3P’s synchronous denoising is dependent on the prediction tasks and backbone architecture. To disentangle this, I suggest: 1 Porting JD3P to an alternative backbone and repeating the training/evaluation to assess transferability; and\n2 Performing controlled comparisons under the same backbone and training protocol against fully autoregressive and partially autoregressive baselines (e.g., VQ-based image generation followed by action decoding).\nThese experiments would help determine whether improvements stem from the synchronous denoising mechanism per se, or from the image-prediction tasks/backbone, thereby isolating JD3P’s contribution.\n3. More details of setups, demos, and experiments in the real world are required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sOcHXGZmfc", "forum": "UvQOcw2oCD", "replyto": "UvQOcw2oCD", "signatures": ["ICLR.cc/2026/Conference/Submission12819/Reviewer_jgRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12819/Reviewer_jgRu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484425300, "cdate": 1761484425300, "tmdate": 1762923625780, "mdate": 1762923625780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose joint denoising of a subsequent visual frame and the action to take using discrete diffusion over a unified token space, with a causal attention mechanism such that the visual generation does not reference the action generation.  The authors report results over a variety of simulated settings as well as a real world setting.  Speed-up is done by KV-caching and fixing identifier token positions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper lie in its engineering for fast inference (the authors report 4x faster than autoregressive techniques).  Furthermore, the hybrid attention mechanism is interesting, although it remains to be seen if it is indeed true that such a bias helps decision-making by adding thorough comparisons against visual planning techniques that also generate actions conditioned on future visual info (but future visual info generation is not conditioned on actions), as well as joint generation techniques that do not have this causal restriction (UVA, PAD).  This is elaborated further below."}, "weaknesses": {"value": "The real-world setting was not elaborated upon properly; it is not obvious what objects are considered seen or unseen or what scenes are considered novel.  Appendix Section C does not offer any clarifying details beyond the information provided in Figure 3.  There are no visualizations for any of the tasks listed of \"stacking bowls, putting blocks (into a box) and flipping towers\"; rather the only visualization in Section B is an unrelated task of \"grasp the pink block\".  Simply grasping a block is quite different from pick-and-place.  Thorough details, visualizations, and demos of the tasks are necessary - particularly what novelty means.\n\nThis reviewer believes there is a large body of prior research that is relevant but missing from the submission.  The work seems to be premised off the idea that generating a future image first then decoding the intermediate action through inverse kinematics helps (Section 4.3, Lines 418-424).  This suggests that visual planning works, which explicitly generate future visual frames and then convert them into actions to execute through an inverse dynamics model, are quite relevant to compare against.  There is a rich body of work that investigates this approach [1,2,3,4] - which aligns with the authors' finding that generating a future image (or videos) before generating the action should help with decision-making.  Referencing them and even comparing against them; naturally, one may expect performance speed to be better for UD-VLA/JD3P with joint decoding; but success rate performance comparisons would be still interesting to report.\n\nSeparately, there are works that also seek to denoise subsequent images and actions jointly (UVA [5], PAD [6]).  It would be interesting to also compare against them, as they directly perform joint generation of action and images as in this work.  In this context, perhaps more emphasis can be put on the causal attention mechanism that enforces image generation not to condition on action generation even during the joint denoising process (which once again spiritually relates to visual planning approaches) - to differentiate from these joint vision-action denoising approaches.  \n\nTo summarize, the joint decoding part reminds readers of other joint image-action decoding techniques [5,6] but the fact that the authors chose to do so with causal attention reminds readers of visual planning techniques[1,2,3,4].  Combined, there is novelty in the proposed approach, but background works and comparisons should still be done for thoroughness.\n\n[1] Wang et al., Learning Robotic Manipulation through Visual Planning and Acting, RSS 2019.\n\n[2] Du et al., Learning Universal Policies via Text-Guided Video Generation, NeurIPS 2023.\n\n[3] Du et al., Video Language Planning, ICLR 2024.\n\n[4] Luo et al., Solving New Tasks by Adapting Internet Video Knowledge, ICLR 2025.\n\n[5] Li et al., Unified Video Action Model, RSS 2025.\n\n[6] Guo et al., Visual Policy Learning via Joint Denoising Process, NeurIPS 2024."}, "questions": {"value": "In visual planning literature, IDM design $p(a \\mid s, s')$ is quite sensitive to finding an appropriate frame-skip/horizon $h$ such that $s'$ is $h$ timesteps away from $s$.  Sometimes a more aggressive future frame allows more useful information in predicting action $a$, especially if the rendering frequency is high (and there is lots of redundancy between $s$ and $s'$).  In this work, is $h$ always chosen to be 1 (is the subsequent frame generated always the temporally next frame)?  What choice for $h$ makes the method perform the best and under which environments/tasks?  Can you perform an ablation table over different choices of $h$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pi5r5Yiukd", "forum": "UvQOcw2oCD", "replyto": "UvQOcw2oCD", "signatures": ["ICLR.cc/2026/Conference/Submission12819/Reviewer_jYyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12819/Reviewer_jYyP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603081374, "cdate": 1761603081374, "tmdate": 1762923624590, "mdate": 1762923624590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Unified Diffusion VvLA, a novel VLA method that tightly integrates understanding, generation, and action through a Joint Discrete Denoising Diffusion Process (JD3P). Unlike prior approaches that separate visual generation and action prediction, UD-VLA performs joint multimodal denoising in a unified tokenized space, enabling synchronous optimization of image and action generation. The authors propose a hybrid attention mechanism, a two-stage training pipeline, and inference-time optimizations to balance accuracy and efficiency. Extensive experiments on CALVIN, LIBERO, and SimplerEnv benchmarks, as well as real-world robot evaluations, demonstrate state-of-the-art (SOTA) performance and 4× faster inference compared to autoregressive baselines"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think that paper is a timely contribution that significantly advances the state of the art. The paper demonstrates clear innovation, rigorous experimentation, and practical relevance. \n\n- JD3P unifies multimodal generation and control within a single denoising trajectory, representing a conceptual and technical advance over modular or autoregressive VLAs.\n- UD-VLA achieves SOTA on multiple simulation and real-world benchmarks, consistently outperforming strong baselines such as UniVLA, F1, and DreamVLA, with notable efficiency gains (4× faster decoding).\n- The proposed attention structure effectively balances intra-modal richness with inter-modal causality, enabling actions to be grounded in predicted future observations.\n- The paper includes well-designed ablation studies that clarify the contributions of hybrid attention, future image generation, and joint denoising.\n- Demonstrating strong generalization to unseen environments and objects enhances the paper’s practical impact and credibility.\n- Despite the technical depth, the paper is well-structured, making complex ideas accessible to both robotics and multimodal learning audiences."}, "weaknesses": {"value": "I do not think that paper has major weaknesses. Some minor ones:\n\n- While the appendix mentions that generated images lack fine-grained fidelity, the main text could better discuss potential failure cases or constraints (e.g., scalability to high-resolution images or multi-robot domains)."}, "questions": {"value": "I don’t have additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NSq65zUmfM", "forum": "UvQOcw2oCD", "replyto": "UvQOcw2oCD", "signatures": ["ICLR.cc/2026/Conference/Submission12819/Reviewer_8ozA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12819/Reviewer_8ozA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989455605, "cdate": 1761989455605, "tmdate": 1762923624138, "mdate": 1762923624138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach to build Vision-Language-Action(VLA) models. It tackles the current challenges that current methods often treat visual prediction and action prediction as two separate processes, which limits their mutual connection when training.\n\nThe authors propose the Unified Diffusion VLA (UD-VLA), a unified vision-language-action model that is built upon a Joint Discrete Denoising Diffusion Process (JD3P). In specific, it maps all modalities (texts, images, actions) into a shared discrete token space and then process with hybrid attention mask. \n\nThen, the authors design a two-stage training pipeline to activate the image generation capabilities by firstly post train the backbone for future-image prediction and then jointly train image and action. The paper reaches SOTA across CALVIN, LIBERO and SimplerEnv. More importantly, they claim four-time faster inference speed than normal autoregressive policies."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "•\tUnified Tokenization and Hybrid Attention: In this paper, all modalities are converted into a single sequence into a single sequence of discrete tokens instead of simply concatenation. Then the paper allows bidirectional attention within single modality but enforces causal attention across modalities. It’s a solid architecture design that naturally frames the task as utilizing futures images with actions but prevent leakage, improving the interpretability. Corresponding ablation study in Table 5,6 shows as a solid prof that hybrid attention is better than causal and simple bidirectional attention. In addition, future-image prediction provides more help than reconstructing the current-frame.\n\n•\tNovelty for JD3P: The idea of selectively reconstructs a subset of masked positions while leaving clean others hidden at each denoising step is interesting and therefore JD3P make sure future images and action tokens are generated within the same denoising step. It tackles the problem of weak connection in previous unified VLAs between visual generation and action prediction.\n\n•\tStrong Performance on various dataset: The paper shows SOTA performance on CALVIN, LIVERO, and SimplerEnv which proof its strong capabilities in long-horizon reasoning and execution, its strong generalization ability and its real-world transferability. \n\n•\tInference Efficiency: In Table 7, the paper demonstrates its 4 times faster compared to autoregressive methods and matin its performance advantage, which is significant."}, "weaknesses": {"value": "•\tVisual Fidelity: Like the author also mentioned, the generated future images lack high visual fidelity, likely due to the VQ tokenization and lack of large-scale generative pretraining. While the author claims that it’s sufficient, it’s unclear how low fidelity will impact on specific tasks which required fine-grained visual details in real world settings.\n\n•\tSensitivity for hyperparameter: In Equation 8, the paper introduces β down-weights the visual tokens to avoid their dominance. It remains unclear for the difficulty to tune this without any training analysis. \n\n•\tPretrained VLM: The paper introduces its two-stage design pipeline when training. However, it utilized a recent powerful pretrained VLM which is effective but make it difficult to distinguish between the contribution of performance from JD3P from previous methods."}, "questions": {"value": "•\tFigure 1 seems to be a little bit hard to follow especially for its training process and how JD3P works simply by looking at the figure.\n\n•\tIn Table 7, did the speed up represent the whole end-to-end process or simply the decoding process?\n\n•\tHow critical is the pretrained VLM working for the backbone? Can you have an additional ablation study, or some explanation clarify the performance is achieved compared to simply using baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lZnnwzzrKB", "forum": "UvQOcw2oCD", "replyto": "UvQOcw2oCD", "signatures": ["ICLR.cc/2026/Conference/Submission12819/Reviewer_qxoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12819/Reviewer_qxoz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004196480, "cdate": 1762004196480, "tmdate": 1762923623845, "mdate": 1762923623845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}