{"id": "gqIv1sduP3", "number": 15957, "cdate": 1758257617290, "mdate": 1759897270752, "content": {"title": "On the practicality of Boltzmann neural samplers", "abstract": "We tackle a challenge at the heart of the missions of computational chemistry and biophysics---to sample a Boltzmann-type distribution\n$$\np(\\mathbf{x}|\\mathcal{G}) \\propto e^{-U(\\mathbf{x}|\\mathcal{G})}\n$$\non $\\mathbb{R}^{N \\times 3}$ associated with some $N$-body system $\\mathcal{G}$, where $U$ is an energy function (termed force field) with orthogonal invariance and deep, isolated minima.\nTraditionally, this is sampled sequentially using Markov chain Monte Carlo methods, which can be so slow that one, for weeks of wall time, never breaks free from the local minima defined by the starting pose.\nNeural samplers have been designed to speed up this process by optimizing the dynamics, prescribed by a stochastic differential equation (SDE).\nThough sound and elegant in continuous time, they can be practically unstable and inefficient when discretized.\nIn this paper, we attribute this phenomena to the limited expressiveness of the finite additive transition kernels, and their inability to bridge distant distributions.\nTo remedy this, we design a new type of highly flexible prior by mixing orthogonally invariant densities (Mint), as well as a new discretized non-volume-preserving kernel, termed Jacobian-unpreserving Langevin with explicitprojection (Julep). \nTogether, MintJulep greatly improve the practical performance of neural samplers, while keeping the underlying SDE intact.", "tldr": "", "keywords": ["chemistry application", "enhanced sampling", "graph modeling"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/565dcc9d1d6dba74abd4e1ef5ab71126d4883eb3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes two new techniques to improve neural samplers. The first technique (MINT) is a more expressive initial distribution (prior). These expressive priors are constructed via mixtures of pairwise Fisher-von-Mises distribution (introduced in the work). They are optimized at the first training phase using the REINFORCE algorithm to give a simple yet more expressive “initial” guess of the ground truth distribution. The second technique (JULEP) is a more expressive discrete-time Markov kernel than the Gaussian ones used previously. The JULEP kernel is trained with the same log-variance loss (discrete version) as previous method. Experiments on Lennard-Jones and Alanine Dipetide are presented."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposal to use MINT prior is sound and reasonable. The initial phase of minimizations is computationally negligible and makes sense to use for many downstream neural samplers.\n- The JULEP kernel is similarly reasonable to use.\n- The proposed methods address shortcomings of current neural samplers\n- Experiments are shown on both synthetic and a simple molecule"}, "weaknesses": {"value": "(1) The writing of the paper can be improved significantly: \n   - Motivation and Structure: It is very reasonable to design better priors. However, this motivation is not communicated well in the text. I would recommend stressing this point more in the introduction. Throughout section 2, it is unclear to the reader what role the prior or the class of distributions that is defined plays in the context of neural samplers. Also in the contributions, “A new prior called Mint”. The word “prior” was not used in the text before and it is unclear at this point what “prior” refers to here. One needs to explain what prior is in this context (i.e. the initial distribution of the SDE).\n- The “pathology” in remark 1.1. is just postulated and no proof is given or anything. Many other reasons why neural samplers fail could be given and discretization error was so far not one of them, e.g. diffusion models have discretization errors but can express extremely complex distributions with their SDEs.\n\n(2) Some statements have insufficient proofs or no derivations are given for non-trivial statements:\n- Theorem 2.6.: It would be to say what “error” means her, i.e. what topology/metric in the space of probabilities are u considering? Further, the proof is not sufficient. Just saying that every region can be approximated arbitrarily does not suffice. More details are needed. Further, the fact that you assume a Riemann sum excludes a set of probability distributions. You need to specify what set of probability distributions you allow for (one that have a density that is Riemann integrable?). Please give a rigorous proof.\n- Line 241: “We can see that the constraint in Remark 1.1 no longer holds.” Which constraint? It was not formally defined what that constraint is. It was just postulated. So this statement is not grounded.\n\n(3) The experiments and benchmarks are limited. For example, No numerical benchmarks for alanine dipeptide are presented. Why?\n\nOverall, I found the paper interesting and would be inclined to increase my rating if the above issues are addressed."}, "questions": {"value": "- The kernel in equation (11) does not properly fit to the notation used in previous paragraphs (equation (6)) in particular. It is missing the -epsilon*\\nabla U_t term maybe?\n- Equation (3): A space (e.g.\\quad) after the SDE and before the initialization would be appropriate\n- Remark 1.1. Is not really clearly expressed. What does it mean to transport something with 1-Wasserstein distance W_1? Does it mean up to error in W_1? Does it mean that q_1 and q_0 have Wasserstein distance W_1? \n- Line 180: r\\in N, what is N? Isn’t N an integer?\n- Equation (16) requires a derivation. “Recall that” in the sentence before is inappropriate as most readers would not know that. \n- Line 241: “We can see that the constraint in Remark 1.1 no longer holds.” Which constraint? It was not formally defined what that constraint is. It was just postulated. So this statement is not grounded."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LTr0JzVYZA", "forum": "gqIv1sduP3", "replyto": "gqIv1sduP3", "signatures": ["ICLR.cc/2026/Conference/Submission15957/Reviewer_yXPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15957/Reviewer_yXPN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760738891640, "cdate": 1760738891640, "tmdate": 1762926167888, "mdate": 1762926167888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on training neural samplers for sampling n-body system, eg molecules, where we only have access to the target energy or unnormalized density but not samples from the equilibrium distribution. The core of the paper lies in finding a good prior and training a drift function to traverse from this learned prior to the target distribution by minimizing the divergence between path measures. And the main novelty lies in two parts:\n\n1. The author designs a specific prior distribution, instead of the isotropic Gaussian that is usually used, for the O(3) group. This can be seen as analogous to the Mixture of Gaussians, but preserving permutational, rotational, and translational invariance, where the mixture weights and parameters relating to means and covariances are learnable. By minimising the reverse KL of the target and this mixture prior, the author learns a good prior first.\n\n2. To train the drift function, the author follows recent works in neural samplers to optimize the log-variance divergence, while they define a different transition kernel, which sounds novel.\n\nBy first learning the prior in (1), and then optimizing the drifts in (2), the author shows that their method could achieve better results than the \"classic\" neural samplers, PIS and DDS, in both synthetic task (DW-4, LJ-13, and LJ-55) and more practical one (alanine dipeptide on vacuum).\n\nDespite novelty, there are several concerns, which mainly lie in the justification of validility, ablation study, experimental performance, and computational budget. Please see the concerns in the \"Weakness\" section.\n\nIn summary, the reviewer recommends a weak rejection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clearly written\n\n2. The method provides a contribution for task-specific heuristics of neural samplers for molecules"}, "weaknesses": {"value": "## Ablation study\n\nThe main weakness of the paper lies on the ablation study. In short, this paper proposes two novel components: the mixture prior and a new transition kernel. \n\n1. To justify the effectiveness of the mixture prior, one should ablate training neural samplers with a mixture-of-Gaussian (MoG) prior with the same number of mixtures and learnable means and covariances (or learnable isotropic variance to reduce the number of parameters). The author could consider training with a MoG prior, ie MoGJulep, to showcase the effectiveness of Mint.\n\n2. To justify the effectiveness of the proposed transition kernel, one should ablate training with the standard Euler-Maruyama (EM) kernel, ie MintEM.\n\n## Justification of the validity of proposed transition kernel\n\nIt is not clear why the backward SDE in line 236 is a valid traversal to the target distribution, and also there's lack of mathematical details talking about how the discretized kernel comes and why it holds. Elaborating more details on them in either the main context or the appendix is necessary.\n\n## Sampling from PvMF\n\nEquation (16) shows that PvMF can be asymptomatically approximated by a projected Gaussian, when $\\kappa\\rightarrow\\infty$. However, $\\kappa$ can't be infinity: if so, as mentioned in line 166, $\\exp(\\kappa)$ would be infinity and the integration over a finite-\nvolume manifold can be infinite. On the other hand, $\\kappa$ should be chosen as a finite number in practice, which means the approximation can introduce mismatch. If so, the samples at the beginning of the sampling process are not really prior samples, and therefore the calculation of the path weight can be biased, which amplifies the reviewer's concern on the MoG prior ablation mentioned previously. It would be great that \n\n1. the author could talk more about the hyperparameter setting in practice\n\n2. the author could elaborate more on the mismatch between true PvMF samples and the approximated samples from the projected-Gaussian approximation\n\n## Experimental results\n\nThe reviewer mainly concerns on the alanine dipeptide experiments in vacuum. \n\n1. Firstly, it is true that both the energy function and the EGNN can't distinguish chirality, however, the D-form samples can always be transformed to L-form by alignment. Therefore, the author should consider visualizing the Ramachandran plots (figure 2, (b) and (c)) after this D-to-L transformation. \n\n2. On the other hand, the reviewer has a question that, why not also conducting experiments on ALDP in implicit solvent? In fact, this problem is more difficult than the vacuum one. But it would be a good contribution to see if the proposed method could do it. \n\n3. Figure 2 (c) still shows significant difference when comparing to the ground truth. There are extra modes, where the reviewer thinks it is quite not likely due to the chirality, as it would make the Ramachandran plot \"mirrored\" while not making extra modes. Also, the mode weights are not correct. Therefore, the reviewer recommends using the path weight for importance sampling or resampling and shows how the reweighted/resampled samples look."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bzKWgRYMNW", "forum": "gqIv1sduP3", "replyto": "gqIv1sduP3", "signatures": ["ICLR.cc/2026/Conference/Submission15957/Reviewer_EQvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15957/Reviewer_EQvf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760901834278, "cdate": 1760901834278, "tmdate": 1762926167059, "mdate": 1762926167059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two techniques to improve Boltzmann neural samplers. First, it proposed to use a more flexible prior distribution to make the learning of transport easier. Second, it imprvoves the discretisation kernel of the forward and backward SDE to improve the learning of the transports. It denomstrates improvements on systems like DW, LJ and Alanine."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work provides valuable insights into the limitations of current neural samplers by identifying two key challenges and proposing efficient methods to address each of them. The experimental results also demonstrate the effectiveness of the proposed solutions. \n\nNotably, the approach also achieves promising performance on alanine dipeptide in Cartesian coordinates, a particularly challenging benchmark given the absence of training data.\n\nThe code is provided."}, "weaknesses": {"value": "While interesting, I have several concerns on this paper, and I do not think it reaches the bar of publication in its current shape. I will explain my reason and try to provide some suggestions:\n\n1. **No Exp Details**: I cannot find any details on the experiments done in this paper. It's fine to drop some details in the main text, but there is also no details in appendix. I can see the code is provided. However, it is important to include details instead of relying on people to read the code.\n\n2. **No Ablation study and  Many design motivation is unclear**: The paper proposes two approaches to improve the training of neural sampler. Also, for the prior, the proposed form is not tractable and requires approaximaton and langevin correction, the loss is a combination of fitting and repulsion. It is important to ablate how these components contributes to the performance. \nThere are many things unclear in the method. For example, \n- How much gain is due to each proposed component?\n- why it is proposed to use PvMF instead of its approximation form (the PN in Eq 16)? How much gain will this bring us? \n- How the repulsion prevent mode collapsing?\n- Is the loss a direct sum of eq 18 or 17? Or should they be weighted? Is the weighting important? \n- Why 19 is more flexible? Is this design supported by some evidence? \n\n\n3. **Writing need to be improved**: The structure and emphasis should be rebalanced. Key design decisions are insufficiently explained, and important components such as the Julep kernel are introduced without justification or intuitive description. In contrast, a substantial portion of the text is dedicated to standard definitions, including the alanine dipeptide energy formulation, LJ, DW, and ESS. This should be shortened or moved to the appendix. \nThe current presentation gives the impression that some parts of the paper were filled with lengthy formulas due to time constraints rather than to support the main contributions.\n\n4. The title is ambiguous and potentially misleading. The paper focuses on improving the performance of neural samplers, yet the current approach still appears far from being truly practical for real-world Boltzmann sampling.  “On the practicality of Boltzmann neural samplers” sets the expectation that the work either achieves practicality or provides a clear discussion of the remaining gap. \nThere are also many other challenges for neural samplers, like mode balancing [1] and energy evaluation efficiency [2]. \n\n5. Mssing references: [3] learns the prior as well. [4] uses also differrent kernels instead of standard EM discretisation (exp integrator)\n\n[1] Grenioux, L., Noble, M., & Gabrié, M. (2025). Improving the evaluation of samplers on multi-modal targets.\n\n[2] He, Jiajun, et al. \"No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers.\".\n\n[3] Blessing, Denis, Xiaogang Jia, and Gerhard Neumann. \"End-to-end learning of gaussian mixture priors for diffusion sampler.\" ICLR\n\n[4] Phillips, Angus, et al. \"Particle Denoising Diffusion Sampler.\" ICML."}, "questions": {"value": "I am not sure I understand how eq 19 converges to the formula in Line 236? I may miss the definitation of W_t."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zidBBYDlhp", "forum": "gqIv1sduP3", "replyto": "gqIv1sduP3", "signatures": ["ICLR.cc/2026/Conference/Submission15957/Reviewer_njMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15957/Reviewer_njMA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562356612, "cdate": 1761562356612, "tmdate": 1762926166654, "mdate": 1762926166654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces a new framework for defining and training neural generators, which the authors claim significantly enhances the practical performance of such samplers when exploring rugged energy landscapes. The proposed approach combines two main innovations: (i) the introduction of a flexible prior capable of capturing mixtures of densities of states while preserving the system’s symmetries, and (ii) the incorporation of a discretized kernel designed to increase the expressiveness of the model. The authors also present a limited set of numerical experiments that illustrate the potential of the method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The introduction of the paper is well organized, beginning with a clear discussion of the limitations of current normalizing flow approaches for accelerating molecular dynamics simulations, followed by a well-motivated hypothesis explaining why these methods may fail. This serves as an effective introduction to the authors’ proposed solution. The problem addressed by this new framework is highly relevant, and the proposed strategy to enhance the expressiveness of these models could prove to be particularly valuable."}, "weaknesses": {"value": "First, the paper is very challenging to follow. The introduction of the new framework is presented in an extremely technical manner, making it difficult to understand for readers in the computational physics or chemistry communities, which are the primary audience of this work. As a result, the details of the proposed method are hard to grasp. The manuscript would benefit considerably from moving many of the technical derivations to an appendix, complemented by a more intuitive explanation and a schematic illustration of the method in the main text. This would free space to better demonstrate the practical implications and performance of the proposed approach.\nSecond, the experimental section is clearly insufficient. The authors neither benchmark their method against established approaches nor test it on sufficiently challenging problems. The only comparison provided concerns the performance of different annealing trajectories in Table 1. However, the success of annealing trajectories strongly depends on avoiding first-order phase transitions during the annealing process, a factor that is highly problem- and trajectory-dependent. Consequently, this comparison is not particularly informative.\nFigure 2 illustrates that the method can identify most modes of a target distribution when compared to molecular dynamics simulations. Nonetheless, the manuscript lacks a discussion on whether the generated proposals would be accepted as valid moves in actual molecular dynamics or Monte Carlo simulations, which is essential to assess their potential for accelerating equilibrium sampling.\nOverall, to be suitable for publication, the paper should include more substantial experimental validation and quantitative comparisons with existing methods. In addition, a discussion of the method’s limitations—particularly regarding computational cost and the impact of the choice of the anchor atom—should be incorporated into the main text."}, "questions": {"value": "1. How does the proposed framework compare quantitatively with existing methods? A direct performance comparison—both in terms of sampling efficiency and computational cost—would greatly help to assess the practical relevance of the approach.\n\n    2. Can the authors quantify the acceptance rate of the proposed moves when evaluated through a Metropolis test? This would provide a concrete measure of whether their generated proposals are suitable for accelerating equilibrium sampling.\n\n    3. The presentation of the method is currently very technical and difficult to follow for readers outside the immediate field. Could the authors make the explanation more accessible, for instance by providing an intuitive overview and a schematic representation of the algorithm before introducing the full mathematical formalism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KzTwD4Ip2W", "forum": "gqIv1sduP3", "replyto": "gqIv1sduP3", "signatures": ["ICLR.cc/2026/Conference/Submission15957/Reviewer_ivNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15957/Reviewer_ivNK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159769254, "cdate": 1762159769254, "tmdate": 1762926166298, "mdate": 1762926166298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}