{"id": "uM0gBrXgJo", "number": 14680, "cdate": 1758241549608, "mdate": 1759897355466, "content": {"title": "Towards By-Design Interpretable Transformers via Modular Interpretability-Guided Alignment", "abstract": "Transformer models offer strong predictive performance but generally lack interpretability, limiting their adoption in high-stakes applications such as neuroscience. Existing explainable AI methods tend to produce inconsistent and biologically ungrounded explanations, which reduce their usefulness in uncovering condition-specific mechanisms. In this paper, we propose an Interpretability-Guided Alignment module designed to enhance the explainability of pre-trained Transformer models by aligning their internal representations, weights with established external biological knowledge. We introduce a novel conditional interpretable layer and a block-wise interpretability mechanism that provide localized, human-understandable insights into model decisions. Experimental evaluation on two real-world Alzheimer’s disease datasets, namely Seattle and ROSMAP, demonstrates that our approach not only achieves strong classification accuracy but also uncovers biologically meaningful interpretations by identifying key pathways supported by external biological databases such as KEGG and WikiPathways, thereby outperforming existing baselines. Specifically, our solution achieves more than three times higher biological interpretability scores for the Alzheimer’s disease condition compared to existing methods. Furthermore, our approach has the potential to enhance the interpretability of other Transformer-based models across application domains when integrated with relevant external knowledge.", "tldr": "", "keywords": ["Deep Learning", "Transformers", "Explainable AI", "Neuroscience", "Large Language Models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57c1f73dee8530af20055fced43b1ea37e7c3d15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a by-design interpretable Transformer framework called Interpretability-Guided Alignment (IGA) to enhance the explainability of biological prediction models. Instead of relying on post-hoc XAI methods, the model integrates a conditional interpretable layer and block-wise interpretability modules that align internal representations with established biological knowledge from databases like KEGG and WikiPathways. Applied to two Alzheimer’s disease single-cell RNA sequencing datasets (Seattle and ROSMAP), the method achieves strong classification accuracy while producing biologically meaningful, context-aware explanations, demonstrating the efficacy of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Instead of treating interpretability as an afterthought (as post-hoc XAI does), this work embeds it into the model's architecture and training objective.\n2. A comprehensive evaluation is performed with multiple baselines,  including standard attention mechanisms, popular post-hoc XAI methods (Integrated Gradients, DeepLIFT, etc.), and architectural alternatives (Mixture-of-Experts).\n3. The fact that their method maintains comparable predictive accuracy while outperforming all baselines on the biological interpretability metric provides compelling evidence for their claims."}, "weaknesses": {"value": "1. The entire framework is critically dependent on the existence of a high-quality \"ground truth token annotations vector\" which critically limits its generalizability to other domains with no such a priori knowledge.\n   - It makes the paper title untenable, which makes it sound like a general framework/procedure for making Transformer architecture more interpretable. \n2. The model is trained to align with this external knowledge. What if that knowledge is incomplete, biased, or incorrect? The model would be faithfully interpretable, but wrong, and its alignment could mask novel, data-driven discoveries that contradict the (flawed) ground truth. \n3. While the paper does a good job comparing against external baselines but fails to conduct any ablation studies to better assess the significance of their proposed module and loss.\n4. Lack of qualitative examples to support interpretability claims beyond numerical scores.\n5. While the proposed module and loss are formally defined the underlying intuition is not sufficiently covered."}, "questions": {"value": "1. In Equation 6, you define Output_3 = Weights_i as the Aggregation(W_global + C_i * W_context). Given that W_global and W_context are defined as 2D matrices of size (|P|+1)x(|P|+1), and the resulting Output_3 must be a 1D vector of size (|P|+1) to be compared against the 1D GT_tokens vector in your MSE loss function, what specific Aggregation function is used here? How are these 2D matrices reduced to the required 1D vector?\n2. In Section 3.3, you state, \"the interpretability-guided alignment step is performed using a randomly sampled subset comprising 50% of the total training observations\". What is the rationale for this 50% subsampling?\n3. The model-level sub-module in Equation 3 uses a simple summation to aggregate the outputs from the N Transformer blocks (Σ E_block_j). Your analysis in Section 4.4 suggests that different blocks can have specialized roles (e.g., \"block 1 pinpoints... than block 2\"). Does this simple summation not risk obscuring these distinct, specialized contributions? Did you experiment with alternative aggregation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Domf1xB3mN", "forum": "uM0gBrXgJo", "replyto": "uM0gBrXgJo", "signatures": ["ICLR.cc/2026/Conference/Submission14680/Reviewer_LPVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14680/Reviewer_LPVr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883247567, "cdate": 1761883247567, "tmdate": 1762925051468, "mdate": 1762925051468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Interpretability-Guided Alignment Module, a neural layer designed to align the internal activations of pre-trained transformers with task-specific knowledge. The Interpretability-Guided Alignment Module utilizes feedback from additional domain annotations to align the intermediate and aggregated representations of a pre-trained transformer with labels that encode important concepts for the task at hand (e.g., biological pathways). Specifically, this work explores and evaluates the proposed module for transformers trained on biological tasks in the field of neuroscience. This paper evaluates the proposed methodology on two real-world Alzheimer's disease datasets, demonstrating that the resulting models can achieve competitive predictive performance while remaining interpretable and capturing known biological features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I believe these are the paper’s main strengths:\n\n1. **[Significance, Major]** Although the proposed methodology is evaluated only on neuroscience tasks, as the paper rightly mentions, its applicability can easily extend to tasks beyond those in neuroscience. Hence, I believe this work can be of importance for those hoping to build highly predictive transformer-based models for tasks where interpretability is essential. Because of this, and the fact that this work lies within the overlap of interpretability, biology, and neural architecture design, I believe this work has a good chance of being of interest to a large proportion of the ICLR community.\n2. **[Quality, Major]** Even though this paper evaluates the proposed methodology only on two datasets, the evaluation is generally detailed and carefully thought through on those two datasets. Moreover, the interpretability results are promising, showing increased interpretability with respect to some reasonable metrics without significant sacrifices in predictive performance.\n2. **[Originality, Minor]** The exact mechanism proposed for encouraging alignment between the pre-trained transformer and domain-specific knowledge is, to the best of my knowledge, novel. Nevertheless, as explained below, I have some concerns regarding the novelty of this work in comparison to the extensive research on concept-based interpretability.\n4. **[Clarity, Minor]** The paper's writing is generally easy to follow, although, as I mentioned below, some of the technical components could've been better motivated and described."}, "weaknesses": {"value": "In contrast, I believe the following are some of this work’s limitations:\n\n1. **[Originality, Critical]** Although I do believe the specific approach taken by this work was novel, I also think the general intention and the general approach taken are very similar to those of previous methods (particularly those in the concept-based interpretability literature, e.g., [1-5]). In particular, I find the lack of comparison against these methods, and more importantly, the absence of any acknowledgement of these approaches, surprising given the overlap in motivation, approaches, and aims.\n2. **[Quality, Critical]** The lack of comparison against any other inherently interpretable models makes the interpretability results not entirely fair (as the models being compared against were not designed to be interpretable by construction and do not take the same \"level\" of supervision during training). Therefore, as further described below, I believe this work would significantly benefit from comparing itself against other interpretable-by-design concept-based approaches (e.g., [1-5]) or other popular tabular interpretable models (e.g., TabNet [6], TabTransformer [7], or even something like TabCBM [8], which combines both tabular models and concept-based interpretability even for genomics tasks).\n3. **[Clarity, Major]** Some of the key contributions and components of the proposed methodology are not very clearly explained, making it hard to fully understand the details of how and, more importantly, why specific components in the proposed module are the way they are. In particular, I believe this paper would significantly benefit from better motivation (carefully explaining why every component was designed the way it was created in the grand scheme of things before describing it) as well as better notation (both mathematical notation and naming conventions).\n4. **[Clarity and Significance, Major]** It is very unclear how specific hyperparameters were selected, and no code was provided as part of the submission. Both of these omissions may compromise the reproducibility of the results in this work.\n5. **[Significance, Minor]** The focus on only two very niche domain-specific datasets means that it is unclear whether this architecture would work, as claimed in this work, on other domains (as no evidence is provided for that claim).\n\n### References\n\n1. Koh et al. \"Concept bottleneck models.\" ICML (2020).\n2. Rigotti et al. \"Attention-based interpretability with concept transformers.\" ICLR (2021).\n3. Espinosa Zarlenga et al. \"Concept embedding models: Beyond the accuracy-explainability trade-off.\" NeurIPS (2022).\n4. Yuksekgonul et al. \"Post-hoc concept bottleneck models.\" ICLR (2023).\n5. Oikarinen et al. \"Label-free concept bottleneck models.\" ICLR (2023).\n6. Arik et al. \"TabNet: Attentive interpretable tabular learning.\" AAAI *(*2021).\n7. Huang et al. \"TabTransformer: Tabular data modeling using contextual embeddings.\" *arXiv* (2020).\n8. Espinosa Zarlenga et al. \"TabCBM: Concept-based interpretable neural networks for tabular data.\" TMLR (2023)."}, "questions": {"value": "Balancing the strengths and weaknesses mentioned above, I am leaning towards rejecting this work due to its lack of valuation and situation within the extensive literature of interpretable-by-design neural models. However, I am more than happy to be convinced that some or all of my conclusions are incorrect and to revise my recommendation based on a discussion with the authors. For this, the following questions could help clarify/question some of my concerns:\n\n1. **[Critical]** Could you please elaborate on why the concept-based approaches and tabular-specific methods discussed above have not been discussed in the paper or included as baselines? How is the proposed methodology different from existing concept-based approaches (e.g., CBMs [1]) and, more specifically, tabular-specific methods such as TabNet [7], TabTransformer [7], and TabCBM [8]?\n2. **[Critical]** Is it really fair to compare the interpretability of the proposed methodology only against methods that were not designed with interpretability in mind (therefore depending on post-hoc approaches to obtain any helpful explanations)? Similarly, is it fair to compare methods that receive extra training-time feedback (e.g., external knowledge) against those that do not? If not, then how would the interpretability scores of your proposed method look against methods that were designed with interpretability in mind such as TabNet and/or TabCBM? In particular, note that concept-based approaches such as TabCBM can also intake domain-specific knowledge, which could then be used to ensure alignment of its latent space with known task “concepts”.\n3. **[Major]** How were the different hyperparameters selected, and how sensitive are the results to these hyperparameters? For example, why is 50% fo the total training set used for the “interpretability-guided step” and how important is that? The appendix just mentions that “several experiments were conducted to select the optimal hyper-parameters” but this is not enough to ensure fair a evaluation and reproduce the results. This is against good scientific practices.\n4. **[Minor]** Why, against standard good practices, was no code included as part of the evaluation or discussed anywhere in the paper?\n5. **[Minor]** Is there no weight hyperparameter to control the strength of each term in the loss term (e.g., in a similar fashion to CBMs [1])?\n6. **[Minor]** In lines 133-135, it is stated that masked gene expressions are zeroed out. Given that zero has a key meaning in gene expressions (i.e., lack of expression), shouldn’t masking set the expression to its empirical mean rather than a zero value?\n7. **[Minor]** Do you have any evidence that the proposed architecture could “work” in non-biological datasets?\n\n### Other Suggestions and Typos\n\n1. **[Clarity, Major]** When explaining the overall method, I would strongly suggest better motivating each of the module’s components, perhaps with a running example that clarifies how everything fits together. For example, a big weakness of the way the paper is written is that it is never very clear **how** the external feedback is provided (e.g., what format it takes, how it looks in practice, etc.). Having an example that clearly illustrates this can make a significant difference in how easily it is to follow this work.\n2. **[Notation, Major]** I would strongly recommend using `\\text{…}` when writing multi-letter words in LaTeX (e.g., $\\text{Activations}_i$ vs ${Activations}_i$ and $\\text{Model}^\\text{Trained}$ vs ${Model}^{Trained}$). Otherwise, the mathematical notation looks cumbersome and harder to read. Moreover, for the sake of clarity, I would suggest using more descriptive names for variables and elements in the architecture that clearly indicate their purpose (e.g., $\\text{Output}_1$ conveys nothing about what that output represents).\n3. **[Figure Clarity, Major]** I would suggest reworking Figure 1, as right now it is not the easiest figure to follow and does not provide much clarity on the importance and role of each component of the proposed module. To improve it, consider using an example in the figure itself to showcase, hypothetically, what each component of the module would do.\n4. **[Results Clarity, Major]** In my opinion, Figure 2 would be much easier to see as a table rather than a figure (the actual visual aspect only adds to the difficulty when comparing results rather than clarity).\n5. **[Potential Typo, Minor]** The sentence fragment “… by aligning their internal representations, weights with established …” is hard to parse and seems to be grammatically off.\n6. **[Reading Flow, Nit]** Given that the introduction starts by discussing LLMs, but then the text moves directly into transformers, the reading flow is a bit convoluted (if someone doesn’t know what a transformer is w.r.t. an LLM, this text does nothing to help you understand it). Therefore, it may be worth starting the discussion with transformers and then explaining how they serve as the backbone for LLMs.\n7. **[Potential Typo, Nit]** Sentence 103 (”In the literature, scSimilarity (Heimberg et al., 2025) model for cell analysis employs an MLP backbone …”) seems off and does not naturally parse.\n8. **[Potential Typo, Nit]** In 196-197, “Each element of the vector represent a token …” should probably be “Each element of the vector represents a token …”\n9. **[Potential Typo, Nit]** At the end of the caption of Figure 5, there is a dangling colon.\n10. **[Reading Flow, Nit]** The sentence in lines 439-441 is very hard to read/parse thoroughly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uJsXGpdfTC", "forum": "uM0gBrXgJo", "replyto": "uM0gBrXgJo", "signatures": ["ICLR.cc/2026/Conference/Submission14680/Reviewer_3j7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14680/Reviewer_3j7n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892567872, "cdate": 1761892567872, "tmdate": 1762925050767, "mdate": 1762925050767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Modular Interpretability-Guided Alignment framework that makes Transformer models interpretable by design rather than relying on post-hoc explainability techniques. The method enhances pre-trained Transformers by aligning internal representations and weights with external biological knowledge, particularly in the context of Alzheimer’s disease. Experiments on two real-world  Alzheimer’s disease datasets,  show that the model matches baselines accuracy while tripling biological interpretability scores."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The authors presents an approach that integrates domain-specific knowledge into model predictions, enabling meaningful and interpretable explanations, and evaluates its performance on real-world datasets. This work demonstrates how interpretability methods can be adapted to real-world problems to enhance model reliability.\n\n* The proposed model achieves performance comparable to baseline methods on real-world datasets while demonstrating substantially higher interpretability than traditional post-hoc explanation techniques.\n\n* The presentation is clear also for readers without a background in biology."}, "weaknesses": {"value": "*  The presentation of the architecture is difficult to follow. Specifically, it would help to clarify the Shared Conditional Interpretable Layer, its purpose, and why the conditional mechanism (C_i) is necessary. Additionally, Figure 1 is hard to follow \n\n* Some architectural details are missing. The paper does not specify: (1) the total number of parameters in the model, (2) the exact number of transformer blocks used. There is also a contradiction regarding the base model: Section 3.1 states \"we fine-tune a Transformer backbone\", while Section 4.4 claims \"we defined our own transformer architecture... trained from scratch\"."}, "questions": {"value": "I would be happy to reconsider my scores after receiving these clarifications:\n1. Could you please clarify the role of the Ci in the Shared Conditional Interpretable Layer ?\n2. Something is not Clear to me about the output 3, is output 3 is input-dependent? In addition why rank(Weights) is interpretable? \n3.  Please provide more details about  (1) number of Transformer blocks, (2) total of parameter count, (3) Whether the backbone is fine-tuned or trained from scratch.\n\n\nMinor typo: you wrote loss_cassification instead of loss_classification"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y9VzeSpcts", "forum": "uM0gBrXgJo", "replyto": "uM0gBrXgJo", "signatures": ["ICLR.cc/2026/Conference/Submission14680/Reviewer_BErJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14680/Reviewer_BErJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964413494, "cdate": 1761964413494, "tmdate": 1762925050186, "mdate": 1762925050186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors design a novel, biologically inspired interpretable conditional layer designed to improve interpretability of internal transformer-based representations. They evaluate their architecture on two cell datasets, Seattle and ROSMAP. The authors find that their method produces more biologically valid and interpretable representations while retaining predictive performance.\n\nOverall, the paper is very difficult to follow and lacking important details. Most notably, Section 3 is overly complex and written in math mode mainly with text, where formulaic descriptions of layers would suffice. Some of the named components are unnecessary: the “aggregation block” is a sum over all inputs. The “projection layer” is a 1-d convolution. Others are just not defined properly.\nIn the shared conditional interpretable layer: CI_{proj_i} (first part of eq 5) — what is the Shared_layer function? Furthermore, ”the second term is activated only when the context of input tokens is deemed important” — how, and when is the context of the input tokens deemed important? This is not explained in the paper. The output-level sub-module consists of a classification head, shared layer output (unnecessarily given a different name), and a sum over weight matrices, which is unclear how it is used.\n\nAnother major concern is that the base Transformer model is not mentioned — is a module trained from scratch? Which model was used? These are questions which need to be answered within the paper. \n\nFinally, I would appreciate more information on the data used in the paper. The gene data is introduced in L132, but the datasets are not described or delineated properly. It would help if prior to the first reference to dataset specifics both the datasets and pathway databases were explained.\n\nIt is my opinion that the paper requires significant revisions in clarity and content to reach a publishable state."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The studied problem of explaining transformer internals is important."}, "weaknesses": {"value": "- The writing would benefit from improvement, especially in the method exposition.\n- Some very important methodological details are missing or underdefined, making it hard to assess the importance of the results."}, "questions": {"value": "See review"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8OlAkaWMY", "forum": "uM0gBrXgJo", "replyto": "uM0gBrXgJo", "signatures": ["ICLR.cc/2026/Conference/Submission14680/Reviewer_48YN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14680/Reviewer_48YN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001612671, "cdate": 1762001612671, "tmdate": 1762925049493, "mdate": 1762925049493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}