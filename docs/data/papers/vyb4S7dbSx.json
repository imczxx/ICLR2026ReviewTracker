{"id": "vyb4S7dbSx", "number": 5974, "cdate": 1757949105622, "mdate": 1759897941745, "content": {"title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning", "abstract": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering(KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at anonymous.4open.science/r/RL_KG-4B4E.", "tldr": "KG-R1 is an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL), achieving state-of-the-art accuracy and strong cross-KG generalization.", "keywords": ["Knowledge-Grounded QA", "Knowledge Graph Question Answering", "Retrieval Augmented Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/730bff6316a1bccd54c4850f664415d6afaade5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes KG-R1, an agentic knowledge-graph retrieval-augmented generation (KG-RAG) framework utilizes a single LLM agent with a knowledge graphs (KGs) server through end-to-end reinforcement learning (RL). KG-R1 achieves strong reasoning accuracy with fewer tokens and lower latency and computational cost against baselines. Furthermore, it can transfer to diverse KGs without additional training which shows the ability of extension."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The design is principled and clear: a single agent interacting with a KG server. The completeness and schema-agnostic design make the transferability inherent from the start.\n2. The RL approach with verifiable turn rewards and outcome-level global reward is well-suited for multi-turn KG interaction and backed by detailed equations.\n3. The results are promising, showcasing both efficiency and accuracy. The ablation study clearly shows all components are necessary.\n4. The plug-and-play features make the framework flexible in deployment across various KGs."}, "weaknesses": {"value": "1. While the experiments provide sufficient validation for the claims of the paper, I believe that the limitations of the method are not properly addressed.\n2. There are missing citations in line 918 and 923. Additionally, there are two \"of\" in line 1514, and two \"?\" in line 1514 and 1568."}, "questions": {"value": "1. Could you report the limitations and common failure cases of KG-R1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W8XAgjDuGn", "forum": "vyb4S7dbSx", "replyto": "vyb4S7dbSx", "signatures": ["ICLR.cc/2026/Conference/Submission5974/Reviewer_mum1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5974/Reviewer_mum1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760614636480, "cdate": 1760614636480, "tmdate": 1762918386975, "mdate": 1762918386975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a interesting paper that tackles a timely and important problem in knowledge graph-augmented generation (KG-RAG). The authors propose KG-R1, a new framework that reframes the KG-RAG task from a complex, multi-stage pipeline into a more streamlined, single-agent system trained with reinforcement learning. The goal is to create a system that is not only efficient but can also be easily transferred to new, unseen knowledge graphsâ€”a major challenge for current methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of replacing complex, multi-module pipelines with a single, RL-trained agent that learns to interact with a simple KG \"environment\" is elegant and well-motivated. The agent learns to perform a series of simple, one-hop retrieval actions over multiple turns, with the entire process optimized end-to-end. This approach directly targets the high inference costs and poor generalizability of many existing KG-RAG systems."}, "weaknesses": {"value": "The \"N-run beam search\" is a bit of a misnomer. The technique described is essentially running the model independently N times and taking the union of the answers. This is a form of ensembling or self-consistency, not what is typically understood as beam search in the context of sequence generation or graph traversal. \n\nThe paper could be more transparent about the practicalities of RL training. Reinforcement learning is known to be difficult to get right. While the training curves in the appendix look clean and stable, it would be valuable to have a more candid discussion about the challenges. How sensitive was the training to hyperparameters? What was the overall engineering effort required to set up the RL pipeline compared to, for example, a standard supervised fine-tuning approach? A brief discussion on this would help readers better understand the trade-offs involved."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nPDY1vGKRl", "forum": "vyb4S7dbSx", "replyto": "vyb4S7dbSx", "signatures": ["ICLR.cc/2026/Conference/Submission5974/Reviewer_wf7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5974/Reviewer_wf7t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622752584, "cdate": 1761622752584, "tmdate": 1762918386490, "mdate": 1762918386490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL), termed KG-R1. To be specific, the proposed method utilizes a single agent to interact with KGs as its environment, which attempts to learn to retrieve at each step and incorporate the retrieved information into reasoning and generation. The overall process is optimized through end-to-end RL, combining turn-wise and outcome-based rewards signals. Extensive experiments show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper is well-organized and easy to follow.\n2.\tUtilizing RL to incorporate LLMs with KGs is interesting.\n3.\tThe proposed method demonstrates both efficiency and transferability."}, "weaknesses": {"value": "1. The integration of retrieval and RL is not novel, as similar approaches have been explored in prior works such as Search-R1 [1], R1-Searcher [2]. The proposed method looks like a straightforward extension of these existing methods to the KG domain, lacking significant innovation.\n2. It may lack some representative baseline methods for comparison, such as PoG [3], GCR [4], GNN-RAG [5]. It would be better to include these approaches for a comprehensive comparison.\n3. Although the proposed method claims to be efficient, the experiments in Figure 2 seem confusing. The overall token usage of KG-R1 is higher than the baselines, as the proposed method needs to interact with the KG server multiple times and retrieve knowledge for reasoning. It is unfair to only compare the generated tokens without considering the input tokens from the KG server. Both the input and output tokens contribute to the overall cost of using LLMs.\n4. For the transferability experiments, it would be more convincing to include results on the MedQA dataset, as the current datasets are primarily derived from Freebase or Wikidata.\n5. It would be beneficial to include an analysis of the average number of times the LLMs and the KG server are called for each dataset as well as the total inference time, to provide a clearer picture of the efficiency of the proposed method.\n\n[1] Jin, Bowen, et al. \"Search-r1: Training llms to reason and leverage search engines with reinforcement learning.\" arXiv preprint arXiv:2503.09516 (2025).\n\n[2] Song, Huatong, et al. \"R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\" arXiv preprint arXiv:2503.05592 (2025).\n\n[3] Chen, Liyi, et al. \"Plan-on-graph: Self-correcting adaptive planning of large language model on knowledge graphs.\" Advances in Neural Information Processing Systems 37 (2024): 37665-37691.\n\n[4] Luo, Linhao, et al. \"Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models.\" Forty-second International Conference on Machine Learning.\n\n[5] Mavromatis, Costas, and George Karypis. \"Gnn-rag: Graph neural retrieval for efficient large language model reasoning on knowledge graphs.\" Findings of the Association for Computational Linguistics: ACL 2025. 2025."}, "questions": {"value": "1. Regarding the efficiency experiments shown in Figure 2, please clarify whether the reported token usage refers only to the generated tokens or if it also includes the input tokens. As shown in Appendix F, the token usage appears to be higher.\n\n2. How about the transferability of the proposed method to other datasets, such as MedQA?\n\n3. Could you provide an analysis of the average number of calls made to the LLMs and the KG server for each dataset, as well as the total inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EonLN5kkKl", "forum": "vyb4S7dbSx", "replyto": "vyb4S7dbSx", "signatures": ["ICLR.cc/2026/Conference/Submission5974/Reviewer_TvhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5974/Reviewer_TvhU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647658876, "cdate": 1761647658876, "tmdate": 1762918386170, "mdate": 1762918386170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KG-R1, a RL agentic fine-tuning methodology for knowledge graph guided question answering. It designs a KG agnostic action space and adopts the turn-reward to overcome the limitation of sparse reward."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments and training details are comprehensive. I quite like the ablation study of the paper, includign the reward hacking of PPO and response example to help validate the quality of RL fine-tuning. \n\n2. The performance improvments are significant, especially compared with recent baselins like ToG and ToG 2.0 on in-domain settings."}, "weaknesses": {"value": "1. The argument of cost saving and zero-shot generalization capability doesn't seem very convincing to me. First of all, the proposed method uses N-beam search and it's not clear whether the baselines use the same strategy and the token efficiency seems to be 1-beam search's token count. \n\n2. What's the performance of ToG2.0 in Table 2? Is there any reason it's missing from the table?"}, "questions": {"value": "1. In section 4.1, the author mentioned the N-run beam search, I am wondering whether it's fair for prompt-based baselines, since it includes N-run's total result. What is the value of N in practice and whether it has been calculated into Figure 2 and Table 1's token efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4kgEW0PgAF", "forum": "vyb4S7dbSx", "replyto": "vyb4S7dbSx", "signatures": ["ICLR.cc/2026/Conference/Submission5974/Reviewer_2HG5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5974/Reviewer_2HG5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115062395, "cdate": 1762115062395, "tmdate": 1762918385276, "mdate": 1762918385276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}