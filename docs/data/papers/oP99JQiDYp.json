{"id": "oP99JQiDYp", "number": 19149, "cdate": 1758293863610, "mdate": 1763731039389, "content": {"title": "Robust Reward Modeling via Causal Rubrics", "abstract": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce CROME (Causally Robust Reward Modeling), a novel framework inspired by an explicit causal model designed to mitigate reward hacking. CROME queries an oracle LLM for rubrics that are (or the oracle deems to be) causally relevant to answering a specific prompt. Then, it employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes (subset of the Oracle identified rubrics), to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our neutral augmentations are produced without any knowledge of unknown spurious factors, via question swapping and response interventions only along causal rubrics. We show that the CROME augmentation strategy using rubrics from popular LLM APIs significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.3% and achieving gains of up to 7.1% and 12.4% in reasoning and safety. The robustness of CROME is further testified by significant gains in DPO-aligned policies and Best-of-N alignment across various benchmarks, including AlpacaEval 2.0, RewardBench, safety-focused WildGuardTest, and the reasoning-specific GSM8k.", "tldr": "We introduce CROME, a novel causality-inspired technique for training reward models for LLM post-training, which achieves significantly improved reward model robustness and reduced reward hacking.", "keywords": ["Reward Modeling", "Reward Hacking", "Alignment", "Post training LLMs", "RLHF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6b661d4cd87d2bf2a72292da2e61672b11ff897.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces “causally robust reward modeling” (CROME), a data augmentation technique for training reward models for RLHF-based language model alignment that encourages them to focus on meaningful attributes that differentiate good from bad answers, rather than spurious ones (such as response length or specific formatting patterns).  The approach uses a teacher LM to generate, for a given preference example in conventional reward model training datasets, an oracle LM-produced rubric that explicitly lists meaningful (“causal”) attributes, then synthesizes alternative answers that are supposed to alter only the meaningful attributes (from both the preferred and dispreferred answers in the original preference data).  They also generate alternative questions to which both answers should receive similar reward values, aiming to help the reward model learn to ignore spurious attributes.  Relative to existing methods that attempt to improve reward models by data augmentation, this work claims to be superior in avoiding the need to explicitly state what the spurious attributes are, exactly.  The paper includes a bit of theory to explain why, under the causal assumptions made by the framework, the approach should work.  Extensive experiments show that it does, in fact, lead to better rewards and better aligned language models, on a range of benchmarks spanning safety, reasoning, and general instruction following."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The core idea is elegant and generally clearly explained.  \n\nThe ideas behind Crome and the specific setup – augmenting data while focusing on causal/spurious attributes, appear rather novel and well motivated.\n\nThe experiments are extensive and present strong empirical evidence that the approach is more effective than the baselines selected, across a range of settings and diverse benchmarks.  The paper is quite thorough in using extant base models and benchmarks to evaluate whether the approach performs as intended, and extensive ablations, such as varying the oracle LM. It is convincing that the advantages provided by Crome are robust."}, "weaknesses": {"value": "The main paper does not include a single example.  The formal notation is appreciated, but the intuition would be a lot easier to grasp with an accompanying example.  (E.g., I got hung up on the phrase “flipping the question” in one of the figures.  Awkward nomenclature would matter a lot less if the reader saw a simple illustration of the idea.)\n\nCrome relies heavily on the oracle LM. The central assumption that language models can generate reliable causal rubrics is worth evaluating empirically. Even though the empirical results of training with the rubrics are impressive, there is no attempt to confirm the quality of the rubrics generated by the teacher LM, or of the synthesized examples, even informally.  In other words the “approximations” of line 208 are accepted without any attempt to quantify how good they are. \n\nStemming from the previous point, the biases of the oracle might propagate into the rubric and into the final augmented training data. For example, if the oracle strongly prefers longer responses, then “longer responses” might be identified as a causal attribute in the rubric, and Crome does not offer a robust method to guard against that. \n\nThere is no discussion of exactly how much data augmentation is done (how many of each kind of example) or the cost of doing so (either to train the RM or inference cost in producing the new examples).  The proposed framework (augmenting + filtering per example) seems rather expensive (the method requires generating 10+1=11x data initially and then filtering).  This should be quantified so readers understand the cost of the approach, especially as datasets grow.  Ideally we’d also see some analysis of the cost-benefit tradeoff (e.g., what happens to the performance gains if I do half as much data augmentation?).  \n\nMinor suggestions:\nFigure 6 labels need to be explained somewhere in the caption or paper text."}, "questions": {"value": "Based on my understanding Crome divides attributes into two binary categories – spurious and causal. Is it possible to generalize this idea, accounting for qualitative differences in the level of importance of different attributes, or capturing the ways in which different attributes interact?\n\nHow consistent are the rubrics produced by the oracles across different runs or different choices of oracles? Are similar attributes consistently classified similarly? I would appreciate more quantitative experiments specifically studying the rubrics and the attributes identified between oracles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VcH6X5ttuH", "forum": "oP99JQiDYp", "replyto": "oP99JQiDYp", "signatures": ["ICLR.cc/2026/Conference/Submission19149/Reviewer_Ge7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19149/Reviewer_Ge7h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845256639, "cdate": 1761845256639, "tmdate": 1762931163603, "mdate": 1762931163603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses a large (potentially proprietary) language model to create rubrics for prompts, and to perturb potential responses to that prompt, to design a dataset to train a more robust reward model. They try to mitigate two failure cases in reward models: when a RM does not always correctly identify which response is worse (due to potentially small changes in the response), and when a RM is biased towards a specific response for factors that do not have any actual bearing on the quality of the response. They show strong gains on popular reward modeling benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper's experiments are carefully designed, and focused on an active area of work with reasonable baselines and good results on popular benchmarks. Their work defines clear methodology for using synthetic data generation methods to create augmentations to preference data, creating more robust and effective reward models. They also evaluate their results on best-of-n rankings for popular benchmarks, which has shown to be even better correlated with downstream performance after performing online RL than traditional RM benchmarks."}, "weaknesses": {"value": "Some of the mathematical notation feels unnecessary, and I feel like it obfuscates the (reasonable) points being made at times. E.g. much of the mathematical description in sections 3 and 4 could instead be turned into natural prose, which would be more readily understandable to people less familiar with reward modeling, etc. The points your paper is making are good, but it can be hard to fully parse the paper at times."}, "questions": {"value": "I'd recommend evaluating on the newer version of RewardBench: https://arxiv.org/abs/2506.01937, which includes a metric measuring how well models rate \"tied\" responses, as this would be quite relevant to your setup!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGHrEKCKkz", "forum": "oP99JQiDYp", "replyto": "oP99JQiDYp", "signatures": ["ICLR.cc/2026/Conference/Submission19149/Reviewer_679r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19149/Reviewer_679r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891010921, "cdate": 1761891010921, "tmdate": 1762931163114, "mdate": 1762931163114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to generate synthetic preference data for more robust preference reward modeling. The method extracts “causal” attributes from preference pairs using LLMs, and given these verbalized rubrics counterfactually generates chosen and rejected pairs. The dataset is further augmented with ties pairs. The goal is to reduce the reward models’ reliance on spurious correlations. For their experiments they train either a RM or a DPO model on that data and evaluate it on preference benchmarks and Best-of-N setups. Compared to baselines, they show improvements on most of the evaluations, especially in the safety domain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the important problem of improving preference reward models’ robustness and sensitivity towards spurious correlations. Their synthetic data generation approach both addresses how aware reward models are of real/important attributes in the completions vs. how invariant they are towards spurious attributes. \nThey performed a large set of experiments to showcase the effectiveness of their approach, such as evaluating their model on RewardBench and reWordBench, which is more adversarial. They also show how Crome (their approach) is less easily distracted by spurious correlations in the data, such as length or disguised safety instances."}, "weaknesses": {"value": "- Clarity: The paper’s writing could be improved and certain experimental design choices could be explained more clearly. Additionally, the paper’s main text very often discusses results from tables in the appendix, which makes it harder to read. Some examples where clarity could be improved are: It is never specified what reward model is being trained, at what size (same for the baselines). \n- Analysis of the data: The paper is missing some details on the synthetic data that you generate, such as a quantitative and qualitative analysis.\n- Terms are not used precisely: I worry that the terms “causal” and “counterfactual” might be slightly overselling the approach.\n- Baselines: As the baselines are barely explained in the paper, I wonder whether they are weak and whether you compared the approach to other current SOTA reward models.\n- It would be good to see scores on RewardBench2, which is more challenging and explicitly contains a ties evaluation. Since the Crome approach also trains on ties data, it would be good to see performance on the ties subset."}, "questions": {"value": "- Do you see any benefit in still incorporating more “subjective” preferences in preference tuning? While there might be spurious correlations, certain stylistic preferences could still provide relevant signal for improving an LLMs output distribution\n\n\n- This approach will probably not prevent reward hacking completely? How do you test for that?\n\n- 90: are the causal attributes specified during training?\n\n- 174: don’t understand: what does it mean for a model to be conceptual?\n\n- 204: how do you check these generations?\n- 240: how do you make sure that the irrelevant query is indeed irrelevant?\n- 270: did you do ablations on whether this is actually necessary?\n\n- Section 6.1: you never mention the size of the RMs that you train, or what they are\n\n- Table 2: main improvements in safety? Why?\n\n\n- 375: baselines are not explained at all\n\n- Table 5: unclear what is being compared, i.e. what are all the models?\n\n- 460: effect of causal attributes: but should we completely ignore things like length etc.?\n\n- 465: is that testing whether crome outperforms llm-as-judge?\n\n- I could need more detail on data + how much you generate etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oP8IVFN0Z6", "forum": "oP99JQiDYp", "replyto": "oP99JQiDYp", "signatures": ["ICLR.cc/2026/Conference/Submission19149/Reviewer_Hbwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19149/Reviewer_Hbwu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980584228, "cdate": 1761980584228, "tmdate": 1762931162777, "mdate": 1762931162777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}