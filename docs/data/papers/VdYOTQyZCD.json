{"id": "VdYOTQyZCD", "number": 15974, "cdate": 1758257954090, "mdate": 1762955800054, "content": {"title": "Mutual Transfer Learning across Physical and Architectural Priors for Operator Learning", "abstract": "Recently, the development of foundation models has garnered attention in scientific computing, with the goal of creating general-purpose simulators that can rapidly adapt to novel physical systems. This work introduces a mutual transfer learning framework for operator learning by leveraging the diversity of both model architectures and physical data. First, we introduce Semi-Supervised Mutual Learning for Operators (SSMO) and demonstrate that mutual learning between architecturally diverse models yields significant improvements in accuracy. Second, we validate that pre-training an operator on a wide range of physical dynamics enables substantially more data-efficient and rapid adaptation to new tasks. Our findings reveal that both cross-architecture mutual learning and cross-physics pre-training are effective, distinct strategies for developing more robust and efficient scientific foundation models. We believe that integrating these two strategies presents a promising pathway toward foundational models for scientific computing.", "tldr": "", "keywords": ["transfer learning", "operator learning", "physics-informed machine learning", "mutual learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e1d8f04626ec3035097ef7f440df4c07367f867c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified framework that improves generalization and data efficiency in neural operator learning through two complementary strategies:\n\nSemi-Supervised Mutual Learning for Operators, a novel adaptation of mutual learning to deterministic operator settings. Instead of aligning probabilistic outputs as in classical Deep Mutual Learning, SSMO compares pointwise prediction errors and signs to enable selective, bidirectional knowledge transfer between heterogeneous architectures.\n\nPhysics-Pretrained Neural Operators, a transfer-learning framework that pretrains neural operators on diverse PDE families and fine-tunes them to unseen systems using frozen expert ensembles plus a small residual error-correction model.\n\nExperiments show that SSMO consistently reduces MSE loss across model pairs on Burgers’ and Darcy Flow equations, and that PPNO achieves large gains in data efficiency, up to 2–4× reduction in required training data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n\nThe paper is notably original in adapting mutual learning to deterministic operator learning.\nThe proposed sign-partitioned semi-supervised formulation is novel and conceptually sound, extending mutual distillation to continuous-value PDE mappings.\nThe integration of architectural diversity and cross-physics transfer is creative and relevant to current trends in foundation operator models.\n\nQuality:\n\nThe theoretical formulation of SSMO is rigorous, with clearly defined loss functions, partition sets, and optimization algorithm.\nExperimental design is systematic, covering multiple PDE benchmarks, architectures, and evaluation metrics.\nThe design for expert ensemble and residual correction is technically coherent and well-motivated by transfer learning literature.\n\nClarity:\n\nThe paper is well written, mathematically precise, and logically organized.\nFigures and tables are informative.\n\nSignificance:\n\nThe paper contributes to the emerging area of foundation models for scientific computing, connecting architectural and data-based diversity.\nThe SSMO framework provides a principled way to enable collaborative operator learning without probabilistic supervision.\nThe ideas have broad potential for multi-operator pretraining, few-shot PDE adaptation, and hybrid simulation acceleration."}, "weaknesses": {"value": "Experimental scope is limited to low-dimensional PDEs. \nAblation studies are missing. \nThe paper compares only against independently trained models; it omits comparisons with other pretraining or co-training frameworks.\nThe fine-tuning results on unseen PDEs (e.g., KdV) are promising but limited to one dataset. \nNo theoretical discussion on why mutual learning improves generalization in function spaces."}, "questions": {"value": "Could you evaluate the effect of (1) the sign-based partitioning, (2) weighting across subsets, and (3) removing semi-supervised components to verify which drives the performance gain?\n\nHave you tested SSMO or PPNO on 3D PDEs or real-world physical systems?\n\nCould you provide mean ± std results over multiple runs?\n\nWould SSMO generalize to a multi-peer or contrastive variant, possibly enabling unsupervised operator alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKemzM6F48", "forum": "VdYOTQyZCD", "replyto": "VdYOTQyZCD", "signatures": ["ICLR.cc/2026/Conference/Submission15974/Reviewer_eShk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15974/Reviewer_eShk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856847654, "cdate": 1761856847654, "tmdate": 1762926187575, "mdate": 1762926187575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "EFix12wDGt", "forum": "VdYOTQyZCD", "replyto": "VdYOTQyZCD", "signatures": ["ICLR.cc/2026/Conference/Submission15974/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15974/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762955799199, "cdate": 1762955799199, "tmdate": 1762955799199, "mdate": 1762955799199, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents mutual transfer learning across physical and architectural priors for operator learning. The basic idea is to training multiple operators together so that they can learn from one another (regularize). Also, the authors propose learning on diverse dataset to improve training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper aims to solve an important problem - reducing data during fine tuning stage in scientific foundation model."}, "weaknesses": {"value": "While the problem statement is good, there are several weaknesses\n(a) On a minor side, the paper should have proof read better. There are typos (?) in many places.\n(b) The literature review is incomplete. The paper motivates using foundation model but did not benchmark against any of those (in fact some are not even mentioned). The paper should have benchmarked against ICON, MPP,   NCWNO, and Poseison - all of which are foundation models. Even in operator learning, LNO, CNO, WNO, MWT has not been referenced.\n(c) Some claims made in the paper are wrong. For example, in Section 3.1.1, reducing points where G1 is more accurate than G2 will reduce overfitting. This statement will not hold when the data is noisy (and it will be noisy). G2 can be more accurate than G1 at training points because it fits the noise. In this scenario, the proposed approach will reward G1 to fit the noise (overfit)."}, "questions": {"value": "I will like to hear the authors' thought on point (c) in weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gbRNs2wSif", "forum": "VdYOTQyZCD", "replyto": "VdYOTQyZCD", "signatures": ["ICLR.cc/2026/Conference/Submission15974/Reviewer_Fn9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15974/Reviewer_Fn9L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964106718, "cdate": 1761964106718, "tmdate": 1762926187234, "mdate": 1762926187234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two distinct strategies aimed at improving the data efficiency and adaptability of neural operators for scientific computing. The first contribution is the Semi-Supervised Mutual Learning for Operators (SSMO) framework. SSMO is designed to enable collaborative training between architecturally heterogeneous models (e.g., FNO and U-Net) by partitioning training samples based on relative pointwise prediction errors against the ground truth . The second contribution is the Physics-Pretrained Neural Operator (PPNO) pipeline, a transfer learning strategy. PPNO involves pre-training a \"committee\" of expert operators on diverse, fundamental PDE datasets and then adapting them to a new, unseen task by freezing the experts and training a small residual error-correction model . The authors present experiments demonstrating that SSMO can reduce MSE on benchmark tasks and that PPNO enables substantially more data-efficient adaptation to new PDEs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strongest part of this work is the clear and impactful empirical demonstration of the PPNO framework. The experiments showing that pre-training on diverse physics enables a model to adapt to a new, unseen PDE (KdV equation) with extreme data efficiency are very compelling . The result that the PPNO model fine-tuned on only 10% of the target data achieves a lower error (8.91%) than a baseline FNO trained from scratch on 100% of the data (9.12%) is a significant and practical finding. Figure 4 further reinforces this by clearly visualizing the faster convergence and lower final error of the PPNO model. This provides a strong, quantitative argument for the value of multi-physics pre-training."}, "weaknesses": {"value": "Disjointed Contributions: The primary weakness is the complete lack of integration between the two proposed methods, SSMO and PPNO. The paper introduces them as two separate ideas and concludes by suggesting their integration is a \"promising pathway\", but it never performs this integration. The most critical and obvious experiment, using the SSMO framework to pre-train the PPNO expert committee, is absent. This failure to connect the two \"distinct strategies\" makes the paper feel like two unrelated, under-developed studies packaged as one.\n\nThe SSMO framework is incorrectly described as \"semi-supervised\". The method's core logic relies on partitioning the data into $S_1$, $S_2$, and $S_3$ based on a direct comparison of each model's prediction error against the ground truth $G_T$. This requires 100% of the labels at every training step, making it a fully supervised method. This is a fundamental misrepresentation of the method\n\nThe specific logic for the $S_1/S_2/S_3$ partition, which relies on the sign of the errors, is presented without any theoretical or empirical justification. It appears to be an arbitrary heuristic. The paper provides no ablation studies to demonstrate why this complex partitioning is superior to simpler adaptations of mutual learning for regression, such as a simple $L_2$ loss between model predictions.\n\nThe experimental results for SSMO do not demonstrate a significant benefit. On the benchmark datasets, the improvements for homogeneous models are marginal (e.g., 0.320% vs. 0.283% for FNO-FNO on Darcy Flow ). These minor gains do not seem to justify the added complexity and computational cost of training two models simultaneously"}, "questions": {"value": "Can the authors provide a theoretical rationale or an ablation study to justify the specific $S_1/S_2/S_3$ partitioning logic? Specifically, why is the sign of the error ($sgn(\\Delta)$) a critical factor for deciding when models should learn from each other versus the ground truth?\n\nGiven that the SSMO algorithm requires the ground truth $G_T$ at every training step to compute the errors $\\Delta_1$ and $\\Delta_2$, which are essential for the partitioning 41and the loss in $S_3$, how can the authors justify labeling this method \"semi-supervised\"? This appears to be a factual error in the paper's description. \n\nThe paper's core premise is that integrating architectural diversity (SSMO) and data diversity (PPNO) is a \"promising pathway\". Why was this integration not tested? A crucial experiment to validate the paper's thesis would be to apply the SSMO framework during the pre-training stage of PPNO (i.e., to train the expert committee 44). Without this, the two contributions remain entirely separate. \n\nPlease clarify the role of Figure 2. Was the \"freezing connections\" technique actually used in the SSMO experiments reported in Tables 1 and 2? If so, how was it implemented, and why was it not included as part of the formal methodology in Algorithm 1? \n\nThe reported gains for SSMO on homogeneous models are very small (e.g., 0.020% $\\rightarrow$ 0.018% 46). Do the authors believe these marginal improvements provide a compelling practical reason to incur the significant additional computational cost of training two models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LeMocH4HEq", "forum": "VdYOTQyZCD", "replyto": "VdYOTQyZCD", "signatures": ["ICLR.cc/2026/Conference/Submission15974/Reviewer_Gt4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15974/Reviewer_Gt4b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996551946, "cdate": 1761996551946, "tmdate": 1762926186640, "mdate": 1762926186640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use: 1. mutul learning to take advantages of multiple neural operators, such that predictions on each location can use the results of more accurate neural operators. 2. transfer learning to fine-tune for a new task. The experiments empirically show the effectiveness of their proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear and easy to follow.\n2. The paper wraps up the method in Algorithm 1, which is easy for the paper to read.\n3. The paper applies the idea of mutual learning and transfer learning to neural operators, and the experiments empirically show the effectiveness of their proposed method."}, "weaknesses": {"value": "1. The presentation of the paper has lots of space for improvements. For example, line 448: “... on PPNO are detailed in Appendix ???”. \n2. Too many colors for the figures.w\n3. In figures, “Model 1, Model 2, Loss 1 Loss2” are starting with capital letters, while output, input target are not.\n4. Font sizes are not consistent across figures.\n5. The novelty is the biggest weakness. It is not clear if the proposed training loss for mutual learning is original or already proposed in prior works. It is also not clear and the authors haven’t provided explanations why mutual learning benefits the operator learning. Also for the transfer learning part, adding another module to learn the new task as residues is not new.\n6. In prior operator learning works, mostly L2 error or RMSE is used. I suggest the authors report their results with that. \n7. The paper has formatting issues. For example, line 278, double 4.11 sections. For line 359, there are 4.14 and 4.12."}, "questions": {"value": "1. Have the authors tried more recent operators like transformer based neural operators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjoLxcSdGK", "forum": "VdYOTQyZCD", "replyto": "VdYOTQyZCD", "signatures": ["ICLR.cc/2026/Conference/Submission15974/Reviewer_CCfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15974/Reviewer_CCfw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187706422, "cdate": 1762187706422, "tmdate": 1762926186227, "mdate": 1762926186227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}