{"id": "EWLNGN77lX", "number": 24319, "cdate": 1758355415003, "mdate": 1759896771524, "content": {"title": "Memory Retrieval in Transformers:  Insights from the Encoding Specificity Principle", "abstract": "While explainable artificial intelligence (XAI) for large language models (LLMs)\nremains an evolving field with many unresolved questions, increasing regulatory\npressures have spurred interest in its role in ensuring transparency,\naccountability, and privacy-preserving machine unlearning. Despite recent\nadvances in XAI have provided some insights, the specific role of attention\nlayers in transformer-based LLMs remains underexplored.\n\nThis study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue-based retrieval in human memory.\nIn this view, queries encode the retrieval context, keys index candidate memory\ntraces, attention weights quantify cue–trace similarity, and values carry the\nencoded content, jointly enabling the construction of a context representation\nthat precedes and facilitates memory retrieval.\n\nGuided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis.\nIn addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords.\n\nConsequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.", "tldr": "Examination of how Large Language Models retrieve memory, hypothesizing that attention layers reflect psychological principle.", "keywords": ["Human-subject application-grounded evaluations", "Linguistic theories", "Security and privacy"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/144411e0f3cf7fcafed980a34972b9249743d51d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how transformer attention layers function as memory retrieval mechanisms, grounding the analysis in the Encoding Specificity Principle (ESP) from cognitive psychology. The authors argue that attention performs cue-based retrieval analogous to human memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Conceptual: Establishes a cognitive analogy between human cue-based memory retrieval and transformer attention under the Encoding Specificity Principle.\n\nEmpirical: Demonstrates that Q, K, V implement distinct cognitive-like roles—context encoding, memory indexing, and content storage—validated through controlled swapping and perturbation experiments.\n\nMechanistic: Identifies specific attention-layer neurons encoding “keywords” that act as retrieval cues, offering a concrete locus for contextual memory inside LLMs.\n\nApplied: Suggests practical applications for machine unlearning and privacy-aware data removal, by targeting or suppressing these keyword-linked neurons to erase specific memories."}, "weaknesses": {"value": "This paper is conceptually interesting but offers limited substantive innovation. The proposed connection between transformer attention and the Encoding Specificity Principle is largely a loose analogy rather than a formal theoretical contribution. The authors do not provide a rigorous mathematical formulation or define concrete quantitative measures such as memory retrieval efficiency or cue–content overlap. As a result, the findings are primarily descriptive phenomena rather than statistically grounded or systematically analyzed results.\n\nFurthermore, the paper lacks stronger visualization or causal interpretability analysis. It does not present attention-head–level retrieval trajectories or activation dynamics that could substantiate the proposed analogy. Incorporating feature visualization or attention circuit tracing would make the conclusions considerably more convincing. Overall, the work reads more as a conceptual or idea paper than a genuine mechanistic discovery."}, "questions": {"value": "like weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FRGsoRfjBT", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_ei5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_ei5w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614440719, "cdate": 1761614440719, "tmdate": 1762943041456, "mdate": 1762943041456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the idea that transformer attention layers act like memory retrieval systems. By applying the Encoding Specificity Principle from cognitive psychology, the authors draw a direct parallel between attention mechanisms and human cue based memory as human memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents several key strengths. First, I appreciate the authors' novel idea to draw a connection between the Encoding Specificity Principle in science and transformer attention mechanisms. This matching provides a new conceptual framework for understanding LLMs.\n\nEmpirically, the validation of Q, K, and V roles through the swapping and perturbation experiments is convincing. It effectively demonstrates their distinct functions in context encoding and memory indexing, like the previous work does. Furthermore, the mechanistic analysis identifying specific keyword neurons offers concrete evidence for how contextual memory is localized, which Ios a good contribution. Finally, the proposed application to machine unlearning is very timely and suggests a practical path for privacy-aware data removal."}, "weaknesses": {"value": "This paper is conceptually interesting, but it offers limited substantive innovation and application. The proposed connection between transformer attention and the Encoding Specificity Principle is largely a loose connection rather than a formal theoretical contribution. Maybe it is just a concept thing. The authors do not provide a mathematical formulation or define concrete quantitative measures such as memory retrieval efficiency or cue content overlap. So, the findings are primarily descriptive phenomena rather than statistically grounded or systematically analyzed results (Maybe need more experiments to support).\n\nFurthermore, the paper lacks stronger visualization or causal interpretability analysis. Like, it does not present attention head-level visualization or activation dynamics that could help substantiate the proposed analogy. Incorporating feature visualization or attention circuit tracing would make the conclusions considerably more convincing. \n\nOverall, the work reads more as a conceptual or idea paper than a mechanistic interpretation."}, "questions": {"value": "like weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FRGsoRfjBT", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_ei5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_ei5w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614440719, "cdate": 1761614440719, "tmdate": 1763661856328, "mdate": 1763661856328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates memory mechanisms in Transformer-based Large Language Models (LLMs), with a specific focus on the role of attention layers. The authors propose a conceptual framework based on principles from human psychology, chiefly the Encoding Specificity Principle (ESP) and cue-based retrieval theories. This framework leads to two core hypotheses. The paper presents two main experiments to empirically validate these hypotheses using several decoder-only LLMs. The authors identify specific neurons that are highly activated by these keywords and show that perturbing the K-projection for these keywords significantly impairs memory recall far more than perturbing random tokens. The paper concludes that this evidence supports the ESP framework and identifies a pathway for extracting memory-indexing keywords, which could be used for applications like machine unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's primary strength is its novel conceptual bridge, connecting the well-established psychological theory of cue-based retrieval and the Encoding Specificity Principle directly to the architectural components of the Transformer. This provides an intuitive and human-centric lens for interpreting the \"black-box\" attention mechanism.\n\n2. This conceptual framework is supported by a very strict and rigorous experimental design. Experiment 1 is particularly well-controlled. It carefully isolates the roles of Q, K, and V by swapping them between factual and counterfactual prompts that are constrained to have the exact same tokenized length. The intervention is also minimal, applying only to the first token generation, which cleanly tests the effect of context processing. \n\n3. Experiment 2 is equally rigorous. It validates the \"keywords-as-cues\" hypothesis not just by perturbing keywords, but by benchmarking this against a crucial control: perturbing an equivalent number of random tokens. The dramatic difference in outcomes, shown in Figure 4, strongly supports the claim that these keywords are functionally special."}, "weaknesses": {"value": "1. A primary point of clarification is that the paper does not present a new formal, mathematical theory; rather, it provides an empirical validation of a conceptual mapping from psychology. Its support is based on experimental evidence, not mathematical proofs.\n\n2. The authors themselves identify limitations in their methodology. For instance, the method for selecting top neurons and the number of keywords to target for perturbation is described as \"naive\" and \"largely arbitrary,\" suggesting that the full potential of the unlearning application is not yet realized.\n\n3. A further methodological limitation, also noted by the authors, is the inability of their keyword extraction method to handle compound words or multi-word terms as single cues. The paper notes that \"White rabbit\" is a better cue than \"rabbit\" alone, but the current method cannot group these tokens, meaning the extracted keyword list may not fully capture the ideal set of contextual cues. This could under-represent the true effect of these cues."}, "questions": {"value": "1. The K-perturbation experiment successfully demonstrates that zeroing-out keywords impairs memory. How sensitive are these results to the type of perturbation? For instance, what would be the effect of replacing the keyword K-projections with random noise, or with an averaged K-vector from other keywords, instead of simply zeroing them out?\n\n2. Figure 3 shows that for each model, a single layer-head-dimension triplet is consistently the most activated by keywords across different books. Does perturbing only this single, dominant neuron have a disproportionately large impact on memory recall? How much of the memory impairment effect is localized to this one neuron versus the other high-ranking neurons?\n\n3. The paper provides strong evidence that the attention mechanism functions *like* a key-value memory. Does this framework offer any insights into how the model *learns* to associate specific keywords with the K-matrix (the index) during pre-training? Does this imply that the K-projections are specifically trained to act as a content-addressable index for salient tokens?\n\n4. Finally, the experiments focus on factual recall from texts. How robust is the \"keywords-as-cues\" hypothesis to different types of memory? Would this same mechanism (keywords indexed by K) be expected to retrieve more abstract or thematic concepts, or is it specialized for concrete factual associations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y1fZ0hP1Yf", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_pQPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_pQPV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857941981, "cdate": 1761857941981, "tmdate": 1762943041134, "mdate": 1762943041134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the hypothesis that the attention mechanism in Transformer implements memory-like functions analogous to those found in human cognition. The evidence for this hypothesis comes from two experiments. First, they swap the attention activations between two counterfactual prompts and observe the resulting outputs. Second, they decode and compare lists of keywords from the attention heads for different documents. Experimental results suggest that the attention mechanism performs and information retrieval role."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Establishing similarities between artificial intelligence and biological intelligence is an important and interesting direction.\n- The analogy between transformers and cue-based retrieval is clearly explained.\n- The authors state their hypothesis clearly and support them with experiments."}, "weaknesses": {"value": "- The paper appears to be concealing the important distinction between long-term and short-term memory. This makes the argued similarity appear somewhat odd. LLM short-term memory (prompt) si compared with human long-term memory (hippocampal subregion).\n- An important motivation of the paper is *machine unlearning*, but this is usually a concern with regard to the LLM long-term memory (weights), not the short-term memory (prompt) studied in this paper.\n- The experimental results for attention swapping are not surprising, mirroring many of the already existing causal interventions in the XAI literature.\n- Understanding transformers from a memory-retrieval perspective is not a novel idea. [1]\n\n[1] Bietti, Alberto, et al. \"Birth of a transformer: A memory viewpoint.\" Advances in Neural Information Processing Systems 36 (2023): 1560-1588."}, "questions": {"value": "- I see that the Encoding Specificity Principle is concerned with episodic (long-term) memory. Can the authors point to any evidence for a retrieval-like mechanism in human **short-term** memory?\n- Do the authors see a way that their methods (or similar) could be applied for unlearning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cr5UVi0TC2", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_Ly4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_Ly4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934689404, "cdate": 1761934689404, "tmdate": 1762943040908, "mdate": 1762943040908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the hypothesis that the attention mechanism in Transformer implements memory-like functions analogous to those found in human cognition. The evidence for this hypothesis comes from two experiments. First, they swap the attention activations between two counterfactual prompts and observe the resulting outputs. Second, they decode and compare lists of keywords from the attention heads for different documents. Experimental results suggest that the attention mechanism performs and information retrieval role."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Establishing similarities between artificial intelligence and biological intelligence is an important and interesting direction.\n- The analogy between transformers and cue-based retrieval is clearly explained.\n- The authors state their hypothesis clearly and support them with experiments."}, "weaknesses": {"value": "- The paper appears to be concealing the important distinction between long-term and short-term memory. This makes the argued similarity appear somewhat odd. LLM short-term memory (prompt) si compared with human long-term memory (hippocampal subregion).\n- An important motivation of the paper is *machine unlearning*, but this is usually a concern with regard to the LLM long-term memory (weights), not the short-term memory (prompt) studied in this paper.\n- The experimental results for attention swapping are not surprising, mirroring many of the already existing causal interventions in the XAI literature.\n- Understanding transformers from a memory-retrieval perspective is not a novel idea. [1]\n\n[1] Bietti, Alberto, et al. \"Birth of a transformer: A memory viewpoint.\" Advances in Neural Information Processing Systems 36 (2023): 1560-1588."}, "questions": {"value": "- I see that the Encoding Specificity Principle is concerned with episodic (long-term) memory. Can the authors point to any evidence for a retrieval-like mechanism in human **short-term** memory?\n- Do the authors see a way that their methods (or similar) could be applied for unlearning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cr5UVi0TC2", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_Ly4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_Ly4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934689404, "cdate": 1761934689404, "tmdate": 1763468314864, "mdate": 1763468314864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents two hypothesis and experiments around them.\n\nH1 : Q is context encoder, K as trace memory index, and V as content store (This is obvious in how the Q,K,V are named)\n\nH2 : Encoding Specificity Principle: Basically most effective circumstances for retrieval is most prominent cues during encoding are available at retrieval, so they believe this would be keywords \n\nThe findings were not new and re-iterate what's already known."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Encoding-Specificity Argument, from a conceptual standpoint the arguments and experiments they conduct are sound and logical, rigorous testing using 6 different models.\nThe figures are clear in displaying the information and takeaways. Generally good contextualization before each figure as well."}, "weaknesses": {"value": "The first experiment has very significant perturbation so the significance of their results does not support their idea too much. For the first experiment, they do not talk about how they swap the Q, K, and V matrices. So if the matrix V is swapped, what is it swapped with? \n\nWhy does is H1 even a hypothesis ? (H1 - Q encodes retrieval cues, K indexes candidate traces by those cues, and V stores\nretrievable content.) Isn't this why the matrices are called Query, Key and Value matrices? \n\nFor H2 : The method for finding keywords is unclear. Also putting key-vector activations of certain keywords is same as zeroing out their attention scores. \n\nOverall, the experiments are not explained in enough detail. For H1 experiments, it would be nice to see examples of counterfactual and fact pairs used. The biggest weakness would be that unfortunately we don't learn anything new from the results of the experiments."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6MGvtxPxc2", "forum": "EWLNGN77lX", "replyto": "EWLNGN77lX", "signatures": ["ICLR.cc/2026/Conference/Submission24319/Reviewer_DMUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24319/Reviewer_DMUK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762306672490, "cdate": 1762306672490, "tmdate": 1762943040468, "mdate": 1762943040468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}