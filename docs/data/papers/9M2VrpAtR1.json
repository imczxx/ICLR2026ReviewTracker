{"id": "9M2VrpAtR1", "number": 5117, "cdate": 1757850935177, "mdate": 1763535191682, "content": {"title": "Escaping Low-Rank Traps: Interpretable Visual Concept Learning via Implicit Vector Quantization", "abstract": "Concept Bottleneck Models (CBMs) achieve interpretability by interposing a human-understandable concept layer between perception and label prediction. \nThe foundation of CBMs lies in the many-to-many mapping that translates high-dimensional visual features to a set of discrete concepts. \nHowever, we identify a critical and pervasive challenge that undermines this process: \\emph{representational collapse}, where visual patch features degenerate into a low-rank subspace during training, severely degrading the quality of learned concept activation vectors, thus hindering both model interpretability and downstream performance.\nTo address these issues, we propose Implicit Vector Quantization (IVQ), a lightweight regularizer that maintains high-rank, diverse representations throughout training. \nRather than imposing a hard bottleneck via direct quantization, IVQ learns a codebook prior that anchors semantic information in visual features, allowing it to act as a proxy objective.\nTo further exploit these high-rank concept-aware features, we propose Magnet Attention, which dynamically aggregates patch-level features into visual concept prototypes, explicitly modeling the many-to-many vision–concept correspondence.\nExtensive experimental results show that our approach effectively prevents representational collapse and achieves state-of-the-art performance on eight diverse benchmarks.\nOur experiments further probe the low-rank phenomenon in representational collapse, finding that IVQ mitigates the information bottleneck and yields cross-modal representations with clearer, more interpretable consistency.", "tldr": "We utilize implicit vector quantisation to address the Representational Collapse issue, while delivering better visual concept learning.", "keywords": ["Concept Bottleneck Models; Visual Concept Learning; Vision-Language Models; Representational Collapse; Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0da30a4c43c0eda619e763eee8620ddb4f3c14f5.pdf", "supplementary_material": "/attachment/074808d1fd0cc844d54111c9384a2165f55205c5.zip"}, "replies": [{"content": {"summary": {"value": "This paper talk about Concept Bottleneck Models, which try to make AI more explainable by using human concepts. But they find a problem, the features collapse and lose information, making model not good. They propose a new method called IVQ and Magnet Attention to fix this, and they say it works better in many experiments. They also provide some interesting explanable visualzations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Intuitive solution -- implicit vector quantization for binding human text concepts.\nClean model architecture design with consistent performance boosts.\nExplanable results -- the visualization of the concepts learnt aligns well with human knowledge."}, "weaknesses": {"value": "W1 Typo\n---\nLine 178. \n`Building upon obtrained high-rank concept-aware features`.\nShould be obtained.\n\n\nW2 Unclear Presentation\n---\nFigure 2, top left.\nIt draws `color`, `texture` and `shape` as text encoder input, so it implies #codes should be equal to #color + #texture + #shape.\nHowever, Line 215 says that `M equals the number of textual concepts K`. Which one is true?\n\nBesides, what is the form of text inputs? Do all the baselines have access to this information? How are the correspondences between texts and visual concepts made, so as to calculate Equation (8)? The using of BCE suggests that the number of text items is equal to the number of visual concepts; but the ablation studies used different numbers of visual concepts -- How could BCE be calculated?\n\n\nW3 Unclear presentation\n---\nIn Equation (5), the normalization is conducted along dimension $K$, i.e., codes in the codebook. However, in Equation (6), dimension $L$, i.e., patch embeddings, are eliminated (weighted summed). Routinely, if you want to eliminate some dimension in a matrix multiplication, you should normalize along that dimension. Do the authors have any insights for not following the routine?\n\n```python\nZ = ...  # (L,D)\nQ = ...  # (K,D)\n# (L,1,D) (1,K,D) -> (L,K)\ndist = norm(Z[:,None,:] - Q[None,:,:], dim=2)\nA = softmax(-dist, dim=1)  # TODO XXX why not `dim=0` or `L` ???\n# (L,K)\nM = einsum(\"LK,LD->KD\", A, Z)  # TODO XXX `L` is eliminated\n```\n\n\nW4 Incomplete Analysis\n---\nFigure 7a.\nVisualizing the code indexes of explicit vector quantization is missing.\n\n\nW5 Incomplete Review\n---\nSection Related Works:\nVector-Quantization-related works should be reviewed here, given the VQ contributes a lot in the proposed method.\nPossible literatures include:\n- Neural Discrete Representation Learning.\n- Generating Diverse High-Fidelity Images with VQ-VAE-2.\n- Restructuring Vector Quantization with the Rotation Trick.\n- Addressing Representation Collapse in Vector Quantized Models with One Linear Layer.\n- Vector-Quantized Vision Foundation Models for Object-Centric Learning.\n- Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%.\n- MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization.\n- ...\n\n---\n\nThat said, I am open to revising my ratings, if the author could address my concerns."}, "questions": {"value": "Please refer to the former section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aWzqb0213w", "forum": "9M2VrpAtR1", "replyto": "9M2VrpAtR1", "signatures": ["ICLR.cc/2026/Conference/Submission5117/Reviewer_93wz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5117/Reviewer_93wz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843534332, "cdate": 1761843534332, "tmdate": 1762917890388, "mdate": 1762917890388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper talk about Concept Bottleneck Models, which try to make AI more explainable by using human concepts. But they find a problem, the features collapse and lose information, making model not good. They propose a new method called IVQ and Magnet Attention to fix this, and they say it works better in many experiments. They also provide some interesting explanable visualzations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Intuitive solution -- implicit vector quantization for binding human text concepts.\nClean model architecture design with consistent performance boosts.\nExplanable results -- the visualization of the concepts learnt aligns well with human knowledge."}, "weaknesses": {"value": "W1 Typo\n---\nLine 178. \n`Building upon obtrained high-rank concept-aware features`.\nShould be obtained.\n\n\nW2 Unclear Presentation\n---\nFigure 2, top left.\nIt draws `color`, `texture` and `shape` as text encoder input, so it implies #codes should be equal to #color + #texture + #shape.\nHowever, Line 215 says that `M equals the number of textual concepts K`. Which one is true?\n\nBesides, what is the form of text inputs? Do all the baselines have access to this information? How are the correspondences between texts and visual concepts made, so as to calculate Equation (8)? The using of BCE suggests that the number of text items is equal to the number of visual concepts; but the ablation studies used different numbers of visual concepts -- How could BCE be calculated?\n\n\nW3 Unclear presentation\n---\nIn Equation (5), the normalization is conducted along dimension $K$, i.e., codes in the codebook. However, in Equation (6), dimension $L$, i.e., patch embeddings, are eliminated (weighted summed). Routinely, if you want to eliminate some dimension in a matrix multiplication, you should normalize along that dimension. Do the authors have any insights for not following the routine?\n\n```python\nZ = ...  # (L,D)\nQ = ...  # (K,D)\n# (L,1,D) (1,K,D) -> (L,K)\ndist = norm(Z[:,None,:] - Q[None,:,:], dim=2)\nA = softmax(-dist, dim=1)  # TODO XXX why not `dim=0` or `L` ???\n# (L,K)\nM = einsum(\"LK,LD->KD\", A, Z)  # TODO XXX `L` is eliminated\n```\n\n\nW4 Incomplete Analysis\n---\nFigure 7a.\nVisualizing the code indexes of explicit vector quantization is missing.\n\n\nW5 Incomplete Review\n---\nSection Related Works:\nVector-Quantization-related works should be reviewed here, given the VQ contributes a lot in the proposed method.\nPossible literatures include:\n- Neural Discrete Representation Learning.\n- Generating Diverse High-Fidelity Images with VQ-VAE-2.\n- Restructuring Vector Quantization with the Rotation Trick.\n- Addressing Representation Collapse in Vector Quantized Models with One Linear Layer.\n- Vector-Quantized Vision Foundation Models for Object-Centric Learning.\n- Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%.\n- MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization.\n- ...\n\n---\n\nThat said, I am open to revising my ratings, if the author could address my concerns."}, "questions": {"value": "Please refer to the former section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aWzqb0213w", "forum": "9M2VrpAtR1", "replyto": "9M2VrpAtR1", "signatures": ["ICLR.cc/2026/Conference/Submission5117/Reviewer_93wz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5117/Reviewer_93wz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843534332, "cdate": 1761843534332, "tmdate": 1763631553851, "mdate": 1763631553851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses \\textit{representational collapse} in Concept Bottleneck Models (CBMs), where visual patch features degenerate into a low-rank subspace, consequently degrading concept quality. The proposed solution introduces Implicit Vector Quantization (VQ) to discretize visual features into a high-dimensional concept space. This acts as a regularizer to prevent the \"low-rank trap\" and is shown to improve both concept quality and overall task classification performance. Nevertheless, I am not an expert in Concept-based learning, so the following comments and questions could be based on potentially inaccurate understandings or biased perspectives, and I will raise the rating if the author could give a sound explanation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a critical and pervasive challenge in CBMs (representational collapse) and provides a clear mechanism for its cause. Implicit VQ is a novel and well-justified technique for regularizing the feature space and enforcing feature diversity, directly tackling the collapse problem. \n\n2. The method claims to simultaneously enhance model interpretability (concept quality) and maintain or improve predictive accuracy."}, "weaknesses": {"value": "The implicit VQ regularization might introduce computational overhead compared to standard CBM training. The concept learning process relies on pre-computed text concept embeddings, which may limit the discovery of new, fine-grained, or complex concepts not covered by the initial textual set (Algorithm 1, steps 2, 13)."}, "questions": {"value": "How sensitive is the performance (concept quality and task accuracy) to the choice of the commitment cost hyperparameter $\\beta$ in the VQ loss, and how does the size of the implicit codebook affect the trade-off between preventing collapse and maintaining expressive feature representation? Could the author give more empirical or theoretical evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UToUBaItRo", "forum": "9M2VrpAtR1", "replyto": "9M2VrpAtR1", "signatures": ["ICLR.cc/2026/Conference/Submission5117/Reviewer_w787"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5117/Reviewer_w787"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846579454, "cdate": 1761846579454, "tmdate": 1762917889494, "mdate": 1762917889494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method to utilize patch level visual representation to capture fine-grained relationships with concepts for training Concept Bottleneck Model (CBM)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a novel idea of utilizing patch level representation from CLIP for modeling fine-grained and detailed relationships between image regions and concepts.  \n- The authors demonstrate representation collapse in CBM training with patch-level details and propose novel implicit vector quantization to preserve feature diversity."}, "weaknesses": {"value": "- The claim of evaluating proposed methods on diverse benchmarks is misleading. While the authors demonstrate results on 8 diverse medical datasets, they should also evaluate the proposed CBM on standard benchmark datasets including CIFAR, CUB, ImageNet, and Places [1,2,3,4]. Such comparisons are crucial for assessing the scalability and generalization of the proposed approach across general classification tasks with increasing dataset sizes.\n- The paper is missing qualitative comparison visualizing the concepts learned by the method and with other baselines.\n- Recent studies [3, 4] have shown that CBM training can leak information through the bottleneck layer, allowing models to achieve high accuracy without actually learning meaningful concepts. To address this issue, [4] introduced the A-NEC metric to enable fair comparisons between CBMs based on their concept learning ability and IVQ-CBM should be evaluated on this metric.\n- Notations and writing in section 3.2 such as $Z_{v}, Z_{q}$ are unclear.\n\n[1] Yang, Yue, et al. \"Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[2] Oikarinen, Tuomas, et al. \"Label-free concept bottleneck models.\" arXiv preprint arXiv:2304.06129 (2023). \n\n[3] Yan, An, et al. \"Learning concise and descriptive attributes for visual recognition.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[4] Srivastava, Divyansh, Ge Yan, and Lily Weng. \"Vlg-cbm: Training concept bottleneck models with vision-language guidance.\" Advances in Neural Information Processing Systems 37 (2024): 79057-79094."}, "questions": {"value": "My primary concerns are lack of results on standard datasets and metrics and missing qualitative results. Please see weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCFyCXZemK", "forum": "9M2VrpAtR1", "replyto": "9M2VrpAtR1", "signatures": ["ICLR.cc/2026/Conference/Submission5117/Reviewer_fJ93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5117/Reviewer_fJ93"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878204005, "cdate": 1761878204005, "tmdate": 1762917889094, "mdate": 1762917889094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies representational collapse as a key obstacle in CBMs. It proposes Implicit Vector Quantization (IVQ) to preserve feature diversity without hard quantization and Magnet Attention to explicitly model many-to-many patch–concept relations. Experiments on eight benchmarks show that IVQ-CBM prevents collapse, improves interpretability, and achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It proposes a novel view of representational collapse as low-rank degeneration and introduces IVQ, a soft regularizer that avoids hard quantization.\n2. The proposed method addresses a fundamental CBM limitation. It improves both interpretability and performance with broad applicability to multimodal learning.\n3. This paper is well-organized and clearly written. The motivation and method is easy to follow."}, "weaknesses": {"value": "1. The interpretability assessment mainly relies on qualitative visualization and concept-level alignment metrics. Incorporating human evaluation or concept intervention tests would better validate the interpretability and causal faithfulness of the learned concepts.\n2. A theoretical analysis and a deeper connection between IVQ’s optimization dynamics and rank preservation would clarify its mechanism."}, "questions": {"value": "Do the IVQ-CBM generalizes to broader multimodal reasoning or relational concept tasks? Are there any experiments to validate it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TGEsqwIzdA", "forum": "9M2VrpAtR1", "replyto": "9M2VrpAtR1", "signatures": ["ICLR.cc/2026/Conference/Submission5117/Reviewer_Ycpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5117/Reviewer_Ycpf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150870816, "cdate": 1762150870816, "tmdate": 1762917888783, "mdate": 1762917888783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}