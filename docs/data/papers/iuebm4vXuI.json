{"id": "iuebm4vXuI", "number": 4777, "cdate": 1757764427477, "mdate": 1759898013958, "content": {"title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "abstract": "Large Language Models (LLMs) watermarking has shown promise in detecting AI-generated content and mitigating misuse, with prior work claiming robustness against paraphrasing and text editing. In this paper, we argue that existing evaluations are not sufficiently adversarial, obscuring critical vulnerabilities and overstating the security. To address this, we introduce *adaptive robustness radius*, a formal metric that quantifies watermark resilience against adaptive adversaries. We theoretically prove that optimizing the attack context and model parameters can substantially reduce this radius, making watermarks highly susceptible to paraphrase attacks. Leveraging this insight, we propose RLCracker, a reinforcement learning (RL)–based adaptive attack that erases watermarks while preserving semantic fidelity. RLCracker requires only *limited* watermarked examples and *zero* access to the detector. Despite weak supervision, it empowers a 3B model to achieve 98.5\\% removal success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts after training on only *100* short samples. This performance dramatically exceeds 6.75\\% by GPT-4o and generalizes across five model sizes over ten watermarking schemes. Our results confirm that adaptive attacks are broadly effective and pose a fundamental threat to current watermarking defenses.", "tldr": "We propose a RL-based attack that can effectively remove watermarks from texts using only 100 training samples and zero access to watermark detectors", "keywords": ["LLM watermarks", "RL-based removal attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/591fa4a3757eb77b76d0826e9fb8873b1c4905fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors investigate the robustness of text watermarking schemes for LLMs under adaptive adversarial conditions. They introduce the notion of an adaptive robustness radius as a formal metric for quantifying the semantic margin within which a watermark remains detectable. They also derive a KL-based relaxation that yields a tractable, distribution-level certificate of robustness. Building on this framework, they propose RLCracker, a reinforcement-learning-based watermark removal method that jointly optimizes a semantic similarity reward and a token-wise KL-divergence reward to steer generated text away from the watermark distribution while preserving meaning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors present a clear and well-motivated study exposing critical vulnerabilities in current LLM watermarking schemes under adaptive attacks.\n- They introduce the adaptive robustness radius framework and its KL relaxation that provides a formal basis for evaluating worst-case watermark robustness.\n- Their method demonstrates strong empirical performance, with RLCracker achieving high watermark removal success while preserving semantic fidelity across diverse models and watermarking methods.\n- The paper is well-written and clearly presented."}, "weaknesses": {"value": "- The central idea, training adaptive paraphrasers via reinforcement learning to remove LLM watermarks has already been presented in [1], which the paper does not cite or discuss. That prior work introduced the same adaptive attack setup, nearly identical threat model (no-box, offline, keyless attacker), and a conceptually similar optimization objective. The adaptive robustness radius and KL relaxation are reformulations of equivalent robustness objectives used in that work, not genuinely new theory. The omission of this citation is a significant scholarly lapse and undermines the paper’s originality claim.\n\n- While the adaptive robustness radius is mathematically formalized, it remains mostly a conceptual rephrasing without rigorous empirical validation or ablation tying the theory to the observed performance gains. There is no quantitative analysis showing how the proposed theoretical certificate correlates with actual attack success rates or robustness metrics.\n\n- The proposed RL-based optimization (GRPO) differs from existing DPO-based adaptive attacks only in implementation details, yet the paper presents it as a conceptual breakthrough. The authors should explicitly benchmark RLCracker against preference-optimization baselines such as DPO or PPO-based adaptive paraphrasers to substantiate the claim that their reinforcement-learning formulation yields superior performance.\n\n- The empirical comparison is narrow and misses two key papers [1, 2] on adaptive watermark removal. Without explicit head-to-head comparison or discussion, it is unclear whether RLCracker’s improvements stem from genuine methodological advances or simply better tuning and model scaling. The main surveyed methods aren't adaptive in nature.\n\n**References**\n\n[1] Abdulrahman Diaa, Toluwani Aremu, and Nils Lukas. Optimizing Adaptive Attacks against Watermarks for Language Models. arXiv preprint arXiv:2410.02440, 2025.\n\n[2] Nikola Jovanović, Robin Staab, and Martin Vechev. Watermark Stealing in Large Language Models. arXiv preprint arXiv:2402.19361, 2024."}, "questions": {"value": "- How does *RLCracker* differ fundamentally from Diaa et al. (2025)? Both share nearly identical goals, threat models, and optimization formulations. Please explicitly explain what is new beyond replacing preference optimization (DPO) with GRPO-based RL. If the contribution lies in theoretical formalization (the adaptive robustness radius), please clarify how this framework provides insights or guarantees not already implied by prior robustness definitions.\n\n- Can the authors include experiments comparing RLCracker against DPO-optimized or preference-tuned paraphrasers from prior work?\n\n- The authors define an adaptive robustness radius and derives a KL-based certificate. How does this theoretical quantity correlate with actual watermark evasion rates in experiments? \n\n- Given the dual-use risk of releasing effective watermark-removal tools, how do the authors plan to mitigate potential misuse? Are there safeguards or access restrictions envisioned for releasing code or trained models?\n\n- Could the authors show examples where RLCracker fails or partially removes a watermark, along with explanations of why?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper presents a method explicitly designed to remove or defeat watermarking mechanisms in LLM outputs. These watermarking techniques are intended for content attribution, copyright protection, and misuse prevention. However, the paper should be a cause for further advancement in robust watermark research. While the benefits of publication outweigh the risks (in the living world), I suggest that an ethics reviewer with expertise in AI safety, content attribution, and responsible disclosure of security vulnerabilities is better suited to assess if open-sourcing the implementation and trained models is okay."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8W7tYN2IRU", "forum": "iuebm4vXuI", "replyto": "iuebm4vXuI", "signatures": ["ICLR.cc/2026/Conference/Submission4777/Reviewer_nyNq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4777/Reviewer_nyNq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760606400413, "cdate": 1760606400413, "tmdate": 1762917567059, "mdate": 1762917567059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors first introduce the concept of an adaptive robustness radius to quantify the resilience of LLM watermarks against adaptive adversaries. This analysis demonstrates that LLM watermarks can be vulnerable when adversaries adaptively optimize their attack prompts and model parameters. The authors then propose a powerful RL-based watermark attack, RLCracker. With limited training data, RLCracker learns an effective adversarial paraphraser that can erase watermarks while preserving the original text meaning. Extensive experiments show that RLCracker achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, and it’s clear that the authors put significant effort into making it high-quality. I really enjoyed reading it.\n- The paper provides many interesting and valuable insights that are highly beneficial to the community, especially those discussed in Section 5.2.\n- The proposed method is interesting and achieves strong performance. Moreover, it is transferable across different models, which makes it more flexible. In addition, the method requires only a very small dataset, making it practical and realistic.\n- The experimental section is comprehensive and well-executed."}, "weaknesses": {"value": "- All attacker models are from the Qwen family. I wonder whether other attacker models, such as Llama, could also achieve similarly impressive performance.\n- To my understanding, the current RL training pipeline is not fully black-box, as the training objective relies on the watermark-induced distribution, which in turn requires access to the watermark model’s logits. This could be a limitation of the work, since such access is typically unavailable for commercial-level models."}, "questions": {"value": "- I’d like to double-check my concern mentioned above: for the token-wise KL reward, does the method require access to the logits of the watermarked model? I would assume that a more realistic threat model is one where the attacker only has access to a detector — that is, by providing a (paraphrased) text to the detector and receiving a binary watermark signal. I believe the proposed method could be generalized to such a scenario, though the performance might degrade due to the sparsity of the reward signal.\n- How does the proposed method perform across different watermarking methods? For example, if the model is trained on one watermark, can it effectively transfer to another?\n- Have the authors evaluated the attack on any semantic watermarking approaches, such as https://arxiv.org/abs/2311.08721?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "27CluNn3pC", "forum": "iuebm4vXuI", "replyto": "iuebm4vXuI", "signatures": ["ICLR.cc/2026/Conference/Submission4777/Reviewer_gbCX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4777/Reviewer_gbCX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509188634, "cdate": 1761509188634, "tmdate": 1762917566721, "mdate": 1762917566721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work develops a technique for scrubbing away LLM watermarks using an RL based adversarial optimization routine. They introduce a formal definition of adversarial robustness to such a scrubbing attack under a similarity constraint akin to bounds in classic adversarial optimization to motivate their specific implementable approach and help explain trends in their experiments.  Then they evaluate a range of watermarks in different data domains and using different models as the watermarker versus the attacker. Their results suggest that with limited samples and black box access to the watermarker's outputs, high attack success rates are possible suggesting the need for continued research into watermarking schemes that are more robust to _adversarial_ attacks rather than just benign corruptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Focus on a complementary adversarial dimension of existing LLM watermark robustness research is well motivated.\n\n2. Evaluation setup concerning watermarks, datasets, and models is thorough and in line with prior work. \n\n3. Baselines attacks considered are limited (mainly prompting) but still informative in showcasing the performance improvements of the proposed RL technique.\n\n4. Table 1 and Fig 4b: Overall headline results of the RL approach in their evaluation setting are convincing, Fig 4b feels like a great teaser or twitter figure."}, "weaknesses": {"value": "### Limitations on future-proofness\n\n### 1. \nThe RL technique tries to push the text distribution away from P_wm towards P_h which implicitly relies on this \"wm model\" versus \"human\" gap; in the limit of extremely strong models and \"non-distortionary\" imperceptible watermarks, these gaps should disappear. Stated anohter way, given lack of watermark algorithm, key or detector access, the RL based approach will rely on the distortion the wm causes and a pool of samples to learn the difference between the two distributions. This is principled, but again in the limit of progress on models and watermarks, it should become infeasible. I did not write this paper, nor do I necessarily endorse its complete technical argument, but it is a relevant example of the general point (Sadasivan et al. 2023, https://arxiv.org/abs/2303.11156).\n\n### 2. \nThere is a small but critical clarity issue in what samples are used in the RL loop for understanding the method. The RL approach relies on a pool of watermarked completions to the prompts, and another set of completions representing the \"human\" distribution. The words \"human-like\" are used in a few place though so it is unclear whether these are simply unwatermarked rollouts from the same model, or actual ground truth human written completions from internet datasets.\nOf particular interest is the potential case that it is indeed \"unwatermarked rollouts\" from the same model. While suitable for research demonstrations, this is potentially an unrealistic assumption as an API model (say Gemini or Claude) would be watermarked behind the scenes and there would not necessarily be an exact copy of the same API model to query in parallel with the same weights, just with the watermark inactive. \n\nThis might matter a lot because the objective is based on increasing KL(q||wm) and decreasing KL(q||\"hum\"), if \"hum\" is actually \"!wm\" then in some sense this is a \"single dimensional\" problem along the watermark bias axis and should be extremely sample efficient to traverse. If it however is truly held out samples from some other distribution (human text) that is not the same as the watermarking model but with the watermark turned off, then the sample complexity is expected to be worse because in KL(P||Q) the differences between P and Q are a composition of watermark bias, as well as other differences between the two and that noise should slow things down.\n\n### Issues with presentation of certain ablations\n\n### 3. \nL401 Fig 4a While an interesting point for future research, the \"think time\" or \"reasoning depth\" results are not as significant as the prose makes it seem. Most of the improvement is in the increase from zero to one think units. Relatedly, the \"Think\" method is also not described clearly enough in Sec 5.1 to understand what \"time\" means here. Is this something like tokens? Number of rewrites within the context window? Perhaps the fact that the majority of improvement is in the first unit of think time would be more apparent if it was clear what the axis referred to. (Note, I eventually looked at D.3, but core details need to be inlined in Sec 5)\n\n### 4. \nThe rotating keys or multiple watermarks setting is interesting, but unconvincing in its current form; reviewer wrote a question wondering about it early in reading before they got to this section. \n\nThe experiment appears to evaluate whether the RL approach can simultaneously find a paraphrase distribution that is separated from 4 distinct keys rather than 1, but this is unrealistic. It is hard to imagine a real setting where the provider would have a use case for _just_ 4 keys. Notably, it is expected that if every sample in the pool is keyd uniquely (reasonable if keys are tied to say \"users\" or \"domains\" or something like that), then the expected performance of the method is unclear.\nIt is actually possible that (depending on above question about what the \"hum\" texts actually are) the RL attacker might just learn to generate text that is not like the text the underlying model generates, regardless of the key.\nThis experiment is interesting because it ablates in a way that tries to uncover the actual mechanism that is being used to avoid the watermark; if in a 100 sample pool with 100 keys ESR is still high, versus ESR is near zero, this paints very different pictures about _how_ the approach is achieving the headline Table 1 results (without detracting from them, to be clear)."}, "questions": {"value": "### Misc questions and comments\n\n1. L368  and Table 2 results are slightly disjoint from main Table 1, for instance KGW vs KG_s and SIR versus SIR, and the numbers don't match indicating that they are distinct experimental results. Could the same point instead be made by extracting the \"base\" row from each of the model row groups and the 1500 token column group to present a \"zoomed in\" view that is still coherent with main results?\n\n2. Table 4 could be improved by clearly labeling the two \"Models\" axes to be more clear what model is the wm text generator and what model is the RL'd attacker. Also, including the \"diagonal\" of the table that is not a shift (same generator and attacker) would improve the clarity of the result.\n\n3. This problem feels symmetric, so, if there is a gap to be learned in evasion, the same gap can be used to learn spoofing, just an interesting followup idea."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wgJ2miP8ce", "forum": "iuebm4vXuI", "replyto": "iuebm4vXuI", "signatures": ["ICLR.cc/2026/Conference/Submission4777/Reviewer_pW6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4777/Reviewer_pW6u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759781370, "cdate": 1761759781370, "tmdate": 1762917564912, "mdate": 1762917564912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies watermark removal attacks, i.e., paraphrasing watermarked text such that the semantic meaning is preserved but the detectable watermark is removed. The paper proposes adaptive robustness radius, a formal metric which quantifies watermark robustness to adversarial paraphrasing. An attacker can optimize paraphraser parameters and context to try to minimize this radius.\n\nThe paper proposes RLCracker, an RL-based watermark removal attack that does not need access to the watermark detector and needs relatively few watermarked examples. The paper runs extensive experiments across many watermarking schemes and paraphraser model sizes, finding that RLCracker outperforms baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper takes a principled approach to propose the adaptive robustness radius.  \n* The core idea behind the RLCracker attack is simple and intuitive, which is based on moving away from the watermark distribution and moving closer to the human distribution.  \n* The method addresses limitations from previous methods by generalizing better to longer generations, only requiring few watermarked examples, and not requiring access to the watermark detector.  \n* The experiments are thorough, covering many watermarking schemes and multiple model sizes, providing evidence that the method works across various settings.  \n* Implementation details and additional experiments are provided in the appendix, which enhances the transparency and reproducibility."}, "weaknesses": {"value": "* The adaptive robustness radius and associated theoretical results do not seem very related/necessary to the RLCracker method. The RLCracker method is simply just to increase KL divergence from the watermark distribution and decrease KL divergence from the human distribution. This does not seem to rely on the formal metrics introduced earlier.  \n* The abstract and introduction state that *“optimizing the attack context and model parameters can substantially reduce this radius.”* However, the radius is defined in terms of holding for all $\\\\theta \\\\in \\\\Theta, c \\\\in \\\\mathcal{C}$ or for all distributions $Q$, so isn’t the radius a constant with respect to the space of parameters/distributions?  \n* The method uses $\\\\pi\\_{\\\\text{ref}}(o\\_{i,t} \\\\mid wr, o\\_{i, \\<t})$ to approximate $P\\_{wm}$. However, it seems difficult for the non-watermarked $\\\\pi\\_{\\\\text{ref}}$ to approximate the watermark distribution, especially for more complex watermark signals. The paper does not empirically show how good this approximation is.  \n* Another potentially important metric is text quality, which is not evaluated in this paper. A paraphrase could be semantically similar to the original, but still worse in other aspects, such as fluency, clarity, style, etc. This could happen if the paraphraser model is much smaller/less capable than the original model.  \n* There are some ambiguities and imprecisions in parts of the mathematical notation, which are listed in the questions section below.  \n* The paper doesn’t include empirical or theoretical justifications for why Assumption 1 (Sub-Gaussian Detector Score) should generally hold for watermarking schemes.  \n* Some results appear to be deferred to the appendix, but there is no pointer to these sections of the appendix in the main paper (see questions for details)."}, "questions": {"value": "1. I think that the abstract should state that the P-SP is a semantic similarity score, since readers may be unfamiliar with it.  \n2. Line 40 cites [He at al. (2024)](https://arxiv.org/abs/2402.14007) to claim that existing watermarking schemes are robust against translation, but the cited paper finds that translation is an effective attack to remove watermarks.  \n3. Since paraphraser models are usually not deterministic, is $\\\\mathbf{X’}$ a random variable?  \n   * If it is a random variable, then it would be clearer to have $\\\\mathbf{X’} \\\\sim \\\\pi\\_\\\\theta$ instead of $\\\\mathbf{X’} \\= \\\\pi\\_\\\\theta$.  \n4. The function $r(\\\\mathbf{X}; s, c, \\\\theta)$ in line 173 is never explicitly defined.  \n5. It is confusing to use $\\\\theta$ for both the language model and paraphraser parameters.  \n   * For example, in Definition 1, why does the “paraphraser parameters $\\\\theta$” appear in the watermarked output distribution $P\\_{s,c,\\\\theta}$?  \n   * Same thing for using $c$ for context for both original language model and paraphraser.  \n6. Is it generally true for most or all watermarking schemes that the detector score is sub-Gaussian around its mean?  \n7. In Theorem 1, line 213, does the $(x)\\_{+}$ operation come before the squaring? It is ambiguous from the notation.  \n8. In Theorem 1, line 215, $\\\\mathcal{C}$, $\\\\Theta$, and $R\\_{\\\\text{KL}}$ are not explicitly defined. I’m assuming $R\\_\\\\text{KL}$ is the infimum of $r\\_{\\\\text{KL}}$ over $c \\\\in \\\\mathcal{C}$ and $\\\\theta \\\\in \\\\Theta$?  \n9. Line 273: why is $\\\\pi\\_\\\\theta$ conditioned on $wr$ instead of $q$ in the KL divergence estimator?  \n10. Line 314: what does using “dynamic $w\\_1$” mean?  \n11. The experiment setup lists Qwen3-8B as an attacker model, but it does not appear in Table 1\\. It appears in Table 8 in the appendix, but without RLCracker.  \n12. KGW, UPV, and SynthID are listed in the setup, but do not appear in Table 1\\. They appear in Tables 14 and 15 in the appendix, but there is no reference to these tables in the main paper.  \n13. How does the ESR change as the P-SP score threshold varies? It would be interesting to see a plot of this.  \n14. I am having trouble seeing why using Lemma 1 leads to line 731 in the appendix. The form does not seem to match up.  \n15. Some additional potential related works are [Zhang et al. (2023)](https://arxiv.org/abs/2311.04378), [Chang et al. (2024)](https://arxiv.org/abs/2407.14206), and [Rastogi et al. (2024)](https://arxiv.org/abs/2411.05277).\n\n### Minor issues\n\n1. Line 116 typo: “fail to” \\-\\> “failing to”  \n2. Line 174: “defender’s guaranteed to zero”, is this missing a noun after guaranteed?  \n3. Definition 1, line 196: I think it would make more sense for the “s.t.” to be a comma instead.  \n4. Line 235: this is the first time the EWD watermark is mentioned, so it should have a citation here.  \n5. Line 431 typo: “ERS” \\-\\> “ESR”  \n6. Can you add a citation for Lemma 1 (Donsker-Varadhan Inequality)?  \n7. Krishna et al. (2023a) and (2023b) are duplicated references of the same paper.  \n8. Line 470 typo: “is around 2% occurs” \\-\\> “is around 2%, occurring”  \n9. Line 482 typo: “broader imapct” \\-\\> “broader impact”  \n10. Typo: “Eq. equation 2” appears in the appendix a few times."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jd1ZQlPdcz", "forum": "iuebm4vXuI", "replyto": "iuebm4vXuI", "signatures": ["ICLR.cc/2026/Conference/Submission4777/Reviewer_aaTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4777/Reviewer_aaTd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928051506, "cdate": 1761928051506, "tmdate": 1762917564389, "mdate": 1762917564389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}