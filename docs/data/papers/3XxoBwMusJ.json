{"id": "3XxoBwMusJ", "number": 18512, "cdate": 1758288661653, "mdate": 1759897098660, "content": {"title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation", "abstract": "We present 3DScenePrompt, a framework for camera-controllable video generation that maintains scene consistency when extending arbitrary-length input videos along user-specified trajectories. Unlike existing video generative methods limited to conditioning on a single image or just a few frames, we introduce a dual spatio-temporal conditioning strategy that fundamentally rethinks how video models should reference prior content. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this through introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically-consistent warped views that serve as strong spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality.", "tldr": "", "keywords": ["Scene-Consistent Video Generation; Camera-Controllable Video Generation; Video Diffusion Models;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1b31380220da37e6950b13c56e0b90e57eff1f0.pdf", "supplementary_material": "/attachment/8f8ecb9310a60a75244effcf84247eedf0ca416d.zip"}, "replies": [{"content": {"summary": {"value": "The method conditions a video diffusion backbone on both the last few temporal frames for motion continuity and projected views from a static‑only 3D scene memory for spatial consistency, where the memory is created by dynamic SLAM plus a masking pipeline that removes moving objects using optical‑vs‑warped flow differencing, backward point tracking, and SAM2 propagation to yield clean static point clouds. Static‑only point clouds are projected from spatially adjacent viewpoints to the target camera poses and concatenated with temporal latents through a frozen 3D VAE channel, enabling camera‑controllable generation without modifying the DiT backbone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Proposed new problem statement which is relevant to Video Generation which the previous camera controllable methods which conditions only on the last few frames\n2. Dual spatio‑temporal conditioning addresses both motion continuity and scene consistency, overcoming the short‑window limitation of prior future‑video models.\n3. A static‑only 3D scene memory cleanly separates persistent geometry from dynamics, avoiding the leakage of outdated moving objects into future frames.\n4. Three‑stage dynamic masking (optical‑vs‑warped flow differencing, backward point tracking, SAM2 propagation) yields clean static point clouds without ghosting."}, "weaknesses": {"value": "1. In the first video in the supplementary video, I see that the spatial structure is not preserved. In the initial video, there were supposed to be 2 chairs side-by-side but I don’t see them in the generated video even though the 3D scene memory captures it. Instead a white pillor / cupboard seems to appear which is wrong. Why is it so? Is there any error in camera pose estimation that when back projected on image space, they are not projected accurately and may lead to inconsistency?\n2. I feel like the comparisons are not fair. The video comparisons are all against methods which only do camera control video generation with only the last frame as prior. But your method takes in the few initial videos frames along with the last few frames for video generation and so automatically your method would be more consistent since it is taking more frames as input. This is an unfair comparison in my opinion. The good qualitative results maybe just because you are conditioning your model with more priors and hence more consistent results or is it because of your proposed new architectural changes that enabled better consistent results. I feel the paper didn’t differentiate these two effectively. I agree that it is a novel problem statement and hence no baselines exist but the authors could have trained CameraCtrl or any of the best model to take in more inputs and trained the model with very minimal changes to effectively clarify if the difference in clarity is from more data inputs or actual model architectural changes. \n3. There are no limitations mentioned in the main paper or the appendix. I request the authors to propose the limitations of the work and the future work. It is important for the reviewers and the community to know the limitations so that there could be more work/discussions in that direction to resolve it. \n4. Minimal video results. For a paper proposing video generation, I would expect a minimum of 15-20 videos to be shown in the supplementary. It is difficult for the reviewer to judge if the results are cherry picked or does it actually generalise well on many videos and different camera trajectory. \n5. There are no results shown in the case where given an initial video and 2-3 different trajectories, are all 2-3 videos coherent to the initial video atleast in the region of static part. Are the generated videos coherent with each other in the common regions?\n6. What about a comparison against the newly released ReCamMaster? It is an ICCV paper I assume and hence the evaluation code has been released long before ICLR deadline. I would request the authors to compare this work with their method and show few comparison."}, "questions": {"value": "I have highlighted the major questions in the weaknesses already, and I am summing them up below(I have summarised them shortly so that it is easier for the reviewer to quote the exact question they are answering. Please refer to the Weakness for the detailed problem and question asked.\n1. Why is the spatial structure not preserved in the first supplementary video where two chairs were supposed to be side-by-side, but instead a white pillar or cupboard appears?\n2. Is there any error in camera pose estimation that, when back-projected to image space, causes inaccurate projections and leads to inconsistency?\n3. Are the comparisons fair, given that your method takes both the initial few frames and the last few frames as input, while the other methods only use the last frame as prior?\n4. Are the better qualitative results mainly due to conditioning on more input frames rather than the proposed architectural changes?\n5. Why didn’t the authors train a baseline like CameraCtrl or another strong model with minimal modifications to take more input frames and verify whether improvements come from more inputs or architecture changes?\n6. What are the limitations of the proposed work?\n7. What are the potential future directions or improvements for this work?\n8. Why are there so few video results in the supplementary, only a handful instead of 15–20 videos as expected for a video generation paper?\n9. Are the generated videos coherent with each other when given the same initial video but different trajectories, especially in the static regions?\n10. Why hasn’t the paper compared its method with ReCamMaster, given that its evaluation code has been available since before the ICLR deadline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fpZshnfsEO", "forum": "3XxoBwMusJ", "replyto": "3XxoBwMusJ", "signatures": ["ICLR.cc/2026/Conference/Submission18512/Reviewer_xUs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18512/Reviewer_xUs1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512190951, "cdate": 1761512190951, "tmdate": 1762928211442, "mdate": 1762928211442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to maintain geometric and temporal consistency for camera-controlled video extension generation. For temporal consistency, the last 9 frames of the original video are given to the model as a condition. For spatial consistency, the method extracts a 3D point cloud representation of the static scene from the original video and projects the point cloud to corresponding views, and provides the model as a condition, hence the name ScenePrompting. The 3D point cloud only encodes the static scene by masking out the dynamic objects. The paper's evaluation shows improved performance compared to prior methods, and the ablation studies validate the effectiveness of the design choice."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written. The idea of using explicit 3D point cloud for long-term video generation memory is sound."}, "weaknesses": {"value": "1. Missing comparisons to competing works [1, 2, 3]. The paper completely does not mention or compare with these prior works that also utilize explicit 3D memory for video generation.\n\n2. Missing related works: CameraCtrl2 [4] explores improved camera conditioning and training on a large-scale foundational model. APT2 [5] explores long-duration and real-time camera-controlled generation. These works can be included in the related works section.\n\n3. The paper proposes methods to remove dynamic objects from video. However, erasing and inpainting dynamic objects from video is an established research topic [5]. The paper does not mention such previous work.\n\n4. The model in the paper can only generate short videos of a few seconds, which limits the actual use case of long-duration memory. The videos in the supplementary materials do not show significant improvement. Also, the method only compares against old works (CameraCtrl1 instead of CameraCtrl2).\n\n[1] Video World Models with Long-term Spatial Memory\n\n[2] Learning 3D Persistent Embodied World Models\n\n[3] WORLDMEM: Long-term Consistent World Simulation with Memory\n\n[4] CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models\n\n[5] Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation\n\n[6] Flow-edge Guided Video Completion"}, "questions": {"value": "1. Regarding weakness 1, can the authors explain why competing explicit 3D memory works are not mentioned and compared against? I think this is critical for the paper's acceptance.\n\n2. Regarding weakness 2, can the author explain the relationship between the proposed dynamic object removal pipeline and existing works?\n\n3. The paper's introduction section inspires the development of the proposed method for long-duration generation. Can the author clarify the maximum duration the model supports? And also, how long were the generated videos used for evaluation? The supplementary material only shows very short example videos, where the original videos are so short and can completely be fitted to the attention as a condition, which eliminates the need for such complex spatial prompting design, and the generated videos are also short, which does not showcase the enhancement using the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvWtmJQhea", "forum": "3XxoBwMusJ", "replyto": "3XxoBwMusJ", "signatures": ["ICLR.cc/2026/Conference/Submission18512/Reviewer_iskH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18512/Reviewer_iskH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617491853, "cdate": 1761617491853, "tmdate": 1762928211023, "mdate": 1762928211023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents 3DScenePrompt, a framework for generating camera-controllable videos from arbitrary-length input videos while maintaining scene consistency. The key innovation is a dual spatio-temporal conditioning strategy: the model conditions on both temporally adjacent frames (last 9 frames) for motion continuity and spatially adjacent frames for scene consistency. To handle dynamic elements correctly, the authors construct a 3D scene memory using dynamic SLAM that represents only static geometry, extracted via a three-stage dynamic masking pipeline (pixel-level motion detection, SAM2 propagation, and backward tracking with CoTracker3). This static-only 3D representation is projected to target viewpoints to provide spatial prompts while allowing dynamics to evolve naturally. Experiments on RealEstate10K and DynPose-100K show improvements over baselines in scene consistency, camera control, and video quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is clear: when generating the videos consistent with previous frames, the static and dynamic element must be handled differently, and the underlying 3D geometry must be kept during the camera controlling.\n\n2.  The dual spatio-temporal conditioning strategy is intuitive yet powerful. The recognition that spatial conditioning must provide only static geometry while temporal conditioning handles dynamics is key.\n\n3. Building on frozen CogVideoX preserves pretrained priors and suggests the approach could generalize to other video models."}, "weaknesses": {"value": "1. The ablation study is not sufficient, see the question parts.\n2. There are some recent works try to address the scene-consistent camera controlled video generation (Line 336), for example, Cameractrl 2 and apt2.\n3. The method only tested on CogVideoX."}, "questions": {"value": "1. What happens when SLAM fails (texture-less scenes, fast motion, dynamic-dominated scenes)?\n2. The paper shows only success cases. When does the method fail?\n3. Table 4 only compares with/without the full mask. What's the contribution of each stage (pixel detection, SAM2, backward tracking)?\n4. The temporal window size is fixed at 9 with no justification or ablation. \n5. There should be a baseline method, that the model does not condition on the projected views, instead the original chosen temporal frames.\n5. Does the method work on other video diffusion models beyond CogVideoX?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WH4Mg9qs4y", "forum": "3XxoBwMusJ", "replyto": "3XxoBwMusJ", "signatures": ["ICLR.cc/2026/Conference/Submission18512/Reviewer_hP5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18512/Reviewer_hP5a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839832104, "cdate": 1761839832104, "tmdate": 1762928209205, "mdate": 1762928209205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes 3DScenePrompt, a framework for scene-consistent, camera-controllable video generation from an arbitrary-length input video. The core idea is a dual spatio-temporal conditioning scheme: (i) a short temporal window of recent frames to preserve motion continuity, and (ii) spatial window obtained by projecting a static-only 3D scene memory built from the entire input via dynamic SLAM + dynamic masking. The static memory is rendered to the target viewpoints and concatenated with the temporal window to condition a CogVideoX-I2V backbone. Experiments on RealEstate10K and DynPose-100K report improved spatial/geometric consistency, camera controllability, and overall video quality over baselines including DFoT and recent camera-control methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Dual spatio-temporal conditioning: retrieving spatially adjacent content by pose similarity, not just the most recent frames.\n\n- Static-only 3D scene memory: A practical masking pipeline (flow residuals -> backward point tracking -> SAM2 object masks) to construct a static-only 3D scene memory by excluding dynamics before projecting to target viewpoints.\n\n- Experimental results show that the method performs favorably against baselines. There is also a nice spread of different metrics: geometric consistency, spatial revisitation metrics, camera-following errors, FVD, and VBench.\n\n- Ablations isolate the value of dynamic masking and the number of spatial frames."}, "weaknesses": {"value": "- Fairness of controllability comparisons: Competing camera-control baselines are evaluated with their standard inputs (single image or text + camera), whereas your method consumes last 9 frames + spatial prompts, which is strictly richer conditioning. This can advantage camera trajectory adherence and quality metrics. Consider a variant that is restricted to the same conditioning budget (e.g., single frame + spatial prompts vs single frame) to isolate the benefit of spatial prompts rather than extra temporal frames.\n\n- Running the entire pipeline seems computationally expensive. What is the end-to-end run time during inference compared to the baselines?\n\n- The spatial retrieval is defined by FOV overlap to the planned camera. It’s less clear how the method performs when the planned trajectory explores previously unseen regions or when SLAM coverage is sparse/erroneous.\n\n- How sensitive is the model w.r.t. the output errors/bias of the pretrained models? What are the failure modes? What happens when we use different pretrained models?"}, "questions": {"value": "In addition to the weakness section listed above, I have the following questions:\n\n- How exactly is the pose similarity is computed and what are the thresholds?\n\n- The paper argues benefits for consistency, but doesn’t show what happens when the planned camera explores regions poorly covered by SLAM. What happens there?\n\n- How are colors/features assigned when projecting? How do you handle holes or depth ordering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ykl9BPCKii", "forum": "3XxoBwMusJ", "replyto": "3XxoBwMusJ", "signatures": ["ICLR.cc/2026/Conference/Submission18512/Reviewer_3iY4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18512/Reviewer_3iY4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945544669, "cdate": 1761945544669, "tmdate": 1762928208573, "mdate": 1762928208573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}