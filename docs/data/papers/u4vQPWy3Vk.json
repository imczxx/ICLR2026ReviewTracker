{"id": "u4vQPWy3Vk", "number": 4337, "cdate": 1757664609758, "mdate": 1759898038802, "content": {"title": "C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection", "abstract": "Object detection has advanced significantly in the closed-set setting, but real-world deployment remains limited by two challenges: poor generalization to unseen categories and insufficient robustness under adverse conditions. Prior research has explored these issues separately: visible--infrared detection  improves robustness but lacks generalization, while open-world detection leverages vision--language alignment strategy for category diversity but struggles under extreme environments. This trade-off leaves robustness and diversity difficult to achieve simultaneously. To mitigate these issues, we propose C3-OWD, a curriculum cross-modal contrastive learning framework that unifies both strengths. Stage 1 enhances robustness by pretraining with RGBT data, while Stage 2 improves generalization via vision--language alignment. To prevent catastrophic forgetting between two stages, we introduce an Exponential Moving Average (EMA) mechanism that theoretically guarantees preservation of pre-stage performance with bounded parameter lag and function consistency. Experiments on FLIR, OV-COCO, and OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$ AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\\text{Novel}}$ on OV-COCO, and $35.7$ mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness and diversity benchmarks.", "tldr": "", "keywords": ["RGB-T Modality Complementarity", "Cross-modal Alignment & Fusion", "Robustness Enhancement", "Open-Vocabulary Object Detection"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de1c0f89fc5d0789909464590285ba1ad8065fab.pdf", "supplementary_material": "/attachment/f78850875ef6a6965fce5161f41e2a3bd83802b5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes C3-OWD, a curriculum cross-modal contrastive learning framework that integrates the advantages of a two-stage paradigm. While evaluated on datasets like FLIR, COCO, and LVIS, the limited scope of the experimental design challenges the broader domain relevance and generalizability of the findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The authors provide detailed theoretical analysis for the EMA mechanism with explicit bounds on parameter lag, function consistency, and loss preservation, which strengthen the credibility of the catastrophic forgetting mitigation claim.\n\n\n2.The authors provide a thorough ablation study dissecting the contribution of each major model component.\n\n3.The results show that the paper' method achieves leading performance in both multimodal robustness and open-vocabulary detection."}, "weaknesses": {"value": "1.The experimental support is insufficient. Although strong performance is reported on COCO and LVIS, these datasets mainly cover standard scenarios and do not verify generalization in extreme scenarios.The current experiments primarily rely on a single robustness dataset, lacking validation on other extreme environment datasets. This makes it difficult to fully demonstrate the method's robustness across different extreme scenarios.\n\n2.The computational complexity of the proposed framework is a concern. The combination of bidirectional RWKV blocks, MoCo-style queues, and multi-stage training likely incurs significant overhead. \n\n3.The primary contribution of this work lies in the effective integration of several existing techniques(curriculum learning, cross-modal contrastive objectives, RWKV-based fusion, and EMA)."}, "questions": {"value": "1.Can the authors supplement more experiments on both robustness and generalizability.  It would be valuable to include experimental results validating the proposed method on object detection datasets under various adverse conditions.Here are some common object detection datasets for extreme environments: DAWN, RTTS, VisDrone, and SUIM Dataset.\n\n\n2.Can the authors provide detailed evidence showing that the EMA mechanism indeed effectively preserves Stage 1 performance during Stage 2 training? The paper only provides theoretical derivation without actual evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ulRg6DyFGL", "forum": "u4vQPWy3Vk", "replyto": "u4vQPWy3Vk", "signatures": ["ICLR.cc/2026/Conference/Submission4337/Reviewer_t8SW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4337/Reviewer_t8SW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139015853, "cdate": 1761139015853, "tmdate": 1762917306581, "mdate": 1762917306581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes C3-OWD, a two-stage framework that aims to combine environmental robustness (via RGB-Thermal fusion) with open-vocabulary object detection. In Stage 1, visible and thermal features are fused through VRWKV blocks on the FLIR dataset to enhance robustness under varying illumination. Stage 2 integrates CLIP-based vision-language alignment and momentum contrastive learning on COCO and LVIS to support novel category detection. An exponential moving average (EMA) mechanism is introduced to reduce catastrophic forgetting across stages, with accompanying theoretical bounds. Experiments report 80.1 AP50 on FLIR, 48.6 AP50_Novel on OV-COCO, and 35.7 mAPr on OV-LVIS. While the results on OV-COCO outperform some ResNet-50 baselines, the evidence does not fully support the claim of “breaking the robustness–generalization trade-off.” The method performs below MMFN (81.8) on FLIR and CoDet (37.0) on OV-LVIS, suggesting it lies along the same trade-off frontier rather than surpassing it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experimental coverage across FLIR (robustness), OV-COCO, and OV-LVIS (generalization) is appropriate and shows an effort to evaluate both aspects of the claimed contribution.\n\nThe theoretical analysis of the EMA mechanism provides mathematical grounding, which adds credibility to the catastrophic forgetting mitigation claim.\n\nThe model achieves competitive results on OV-COCO (48.6 AP50_Novel), showing improvement over earlier ResNet-50-based approaches like CLIPSelf (44.3).\n\nThe inclusion of algorithmic details and ablations in Tables 3 and 4 shows an attempt at transparency and reproducibility, though key omissions remain."}, "weaknesses": {"value": "DO the (i) where L is the sequence length .... AND (ii) We then perform L rounds .....\nDo they have to be the same, or different? Also, what is the value of L?"}, "questions": {"value": "Please follow the strengths and weaknesses. \nAlso,\nHow should the ablation with and without the EMA mechanism be seen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nr9nwjYWOW", "forum": "u4vQPWy3Vk", "replyto": "u4vQPWy3Vk", "signatures": ["ICLR.cc/2026/Conference/Submission4337/Reviewer_Q7pF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4337/Reviewer_Q7pF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911897885, "cdate": 1761911897885, "tmdate": 1762917306360, "mdate": 1762917306360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes C3-OWD, a curriculum cross-modal contrastive learning framework for open-world object detection. The approach adopts a two-stage training strategy: 1. Multi-modal robustness enhancement by pretraining on RGBT datasets to improve feature stability. 2. Vision-language generalization alignment through semantic enhancement, text-modulated deformable attention, and a bi-momentum contrastive alignment mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a meaningful gap between robustness (RGBT-based detection) and open-world generalization (vision-language models), proposing a curriculum-style framework to bridge the two. The method is evaluated on three benchmarks with solid ablation studies, confirming the contribution of each component."}, "weaknesses": {"value": "1.The core ideas are all adaptations of existing paradigms (Deformable-DETR + CLIP + MoCo). The contribution seems incremental, with limited conceptual advancement.\n2.The method section is dense and symbol-heavy, especially in Stage 2. Many notations are introduced abruptly without sufficient intuition, making it difficult to follow for readers not already familiar with Deformable-DETR.\n3.The training pipeline seems computationally expensive, but no runtime or cost analysis is reported.\n4.Several hyperparameters (e.g., IoU threshold = 0.3) are presented without explanation or ablation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAKoAbDmwz", "forum": "u4vQPWy3Vk", "replyto": "u4vQPWy3Vk", "signatures": ["ICLR.cc/2026/Conference/Submission4337/Reviewer_FzF2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4337/Reviewer_FzF2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085858710, "cdate": 1762085858710, "tmdate": 1762917305704, "mdate": 1762917305704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a curriculum cross-modal contrastive learning framework that addresses the dual challenges of poor generalization and limited robustness in real-world object detection. The approach integrates the strengths of visible-infrared and open-world detection through a two-stage training strategy: Stage 1 enhances robustness via RGBT pretraining, while Stage 2 improves generalization through vision–language alignment. An exponential moving average mechanism is introduced to mitigate catastrophic forgetting between stages, theoretically ensuring performance preservation with bounded parameter drift. Experiments demonstrate strong performance in both robustness and diversity benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors propose a cross-modal curriculum learning framework that unifies RGBT robustness and open-vocabulary generalization, dynamically balancing multi-modal information through progressive learning."}, "weaknesses": {"value": "1.\tThe proposed framework includes multiple functional modules, but their motivations are not clearly explained. For example, it remains unclear what specific problem the bidirectional attention mechanism in Stage 1 addresses and why it offers advantages over the original version. The description of the method is overly brief and difficult to comprehend. \n2.\tIn line 325, what are the specific manifestations of catastrophic forgetting? What causes it, and how does the Exponential Moving Average mitigate this issue?\n3.\tIn Table 2, why are the baselines presented inconsistently on both sides, for example, CoDet? For a fairer comparison of robustness and generalization, it would be better to provide experimental results for both datasets."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S1N1RTuOZ7", "forum": "u4vQPWy3Vk", "replyto": "u4vQPWy3Vk", "signatures": ["ICLR.cc/2026/Conference/Submission4337/Reviewer_7RPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4337/Reviewer_7RPb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240595871, "cdate": 1762240595871, "tmdate": 1762917305198, "mdate": 1762917305198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}