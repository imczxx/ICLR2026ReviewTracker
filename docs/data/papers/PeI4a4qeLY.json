{"id": "PeI4a4qeLY", "number": 1552, "cdate": 1756891270763, "mdate": 1759898202661, "content": {"title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models", "abstract": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4× speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation. Our codes will be open-sourced.", "tldr": "We propose StageVAR, a training-free method that leverages semantic irrelevance and low-rank properties to accelerate text-to-image VAR inference while preserving effectiveness.", "keywords": ["Visual Autoregressive Modeling; Acceleration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86cad2ce26c51ed7d3fd000c157c7d04fcca8b4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes StageVAR, a training-free, plug-and-play acceleration framework for visual autoregressive models based on a stage-aware analysis of next-scale generation. The authors show that VAR inference naturally decomposes into three stages—semantic establishment, structure establishment, and fidelity refinement—where early steps are crucial for semantics/structure, while later steps mainly polish details. Leveraging two properties observed in the late stage—semantic irrelevance (allowing CFG=0 with a null prompt) and low-rank feature structure—they introduce a block-level acceleration combining random projection and representative token restoration to reduce compute without extra training. On Infinity-2B and STAR-1.7B, StageVAR achieves up to 3.4× speedup with negligible quality drops, outperforming existing VAR accelerators like FastVAR and SkipVAR."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper introduces a stage-aware view of VAR inference and empirically uncovers two late-stage properties—semantic irrelevance and low-rank features—then turns them into a practical, training-free acceleration.\n\n* The proposed method achieves up to 3.4× speedup with negligible quality drop on GenEval/DPG for a prominent class of models, requires no retraining, and is plug-and-play.\n\n* The paper is overall well-organized with intuitive figures and diagrams. It provides carefully designed empirical analysis and thorough ablations."}, "weaknesses": {"value": "* Metric coverage is limited. The proposed methods focus on the detail refinement stage for acceleration, but the GenEval and DPG benchmark only evaluate the high-level semantic information. I think this mismatch is a red flag for the paper. Beyond GenEval/DPG, include metrics like human preference, FID/KID. Clarify the frequency-domain analysis with quantitative statistics.\n\n* The overall analysis and solution are quite similar to the diffusion literature. For example, it is well-known that the denoising process can be split into structure/semantic generation stage and refinement stage, so we can remove cfg at the refinement stage. [1,2]\n\n* Stage boundary determination is heuristic and model-specific. The chosen split ({1–32} vs. {40,48,64}) is derived from GenEval statistics on Infinity/STAR and may not transfer to other VARs, resolutions, or prompt distributions. An automatic, per-sample criterion (e.g., stopping rules from semantic/structural convergence or uncertainty measures) would strengthen generality.\n\n* Low-rank design choices are partly ad hoc. Rank selection is precomputed offline from population stats; robustness under distribution shift is unclear. Provide per-sample adaptive rank selection (e.g., energy-based without full SVD via randomized sketches) and theoretical/empirical bounds linking rank to quality drop.\n\n[1] Yi, Mingyang, et al. \"Towards understanding the working mechanism of text-to-image diffusion model.\" Advances in Neural Information Processing Systems 37 (2024): 55342-55369.\n\n[2] Qin, Qi, et al. \"Lumina-image 2.0: A unified and efficient image generative framework.\" arXiv preprint arXiv:2503.21758 (2025)."}, "questions": {"value": "* How robust is the proposed three-stage split (semantic, structure, fidelity) beyond Infinity/STAR and the GenEval distribution? Can the stage transition be detected automatically per-sample and per-model?\n\n* Can you make rank selection and late-stage text dropping adaptive and reliable without full SVD or offline statistics, especially under distribution shift? What are the failure modes for CFG=0 in complex prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wzp2yexcDz", "forum": "PeI4a4qeLY", "replyto": "PeI4a4qeLY", "signatures": ["ICLR.cc/2026/Conference/Submission1552/Reviewer_biej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1552/Reviewer_biej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761171387622, "cdate": 1761171387622, "tmdate": 1762915811556, "mdate": 1762915811556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StageVAR, a training-free acceleration method for Visual Autoregressive Models (VAR). Based on the stage-based analysis of the VAR inference process, it proposes a stage-aware acceleration strategy that significantly improves generation speed while maintaining high generation quality. Extensive experiments on two text-to-image VAR models demonstrate that StageVAR achieves a better speed-quality trade-off compared to the baseline method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a comprehensive and detailed analysis of the generation process of the VAR model. It uses the observation to identify computational redundancy, proposing a new acceleration method that makes sense.\n\n2. The paper's presentation is very clear and well-organized.\n\n3. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method in accelerating two large-scale VAR-based text-to-image models.\n\n4. The ablation study in the paper analyzes the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Does the proposed method still demonstrate a superior efficiency-quality trade-off on the larger infinity-8B model?\n\n2. Why does StageVAR have a 3.4x speedup on infinity-2b but only a 1.7x speedup on STAR? What causes this significant difference?\n\n3. The observations in the paper seem to have some similarities to those in CoDe [1].\n\n[1] \"Collaborative decoding makes visual auto-regressive modeling efficient.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w1NL5apGeo", "forum": "PeI4a4qeLY", "replyto": "PeI4a4qeLY", "signatures": ["ICLR.cc/2026/Conference/Submission1552/Reviewer_zCp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1552/Reviewer_zCp3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803498646, "cdate": 1761803498646, "tmdate": 1762915811204, "mdate": 1762915811204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes StageVAR, a stage-aware, plug-and-play acceleration framework for Visual Autoregressive (VAR) models that requires no retraining. By identifying three distinct generation stages—semantic establishment, structure establishment, and fidelity refinement—the authors show that only the final stage can be safely accelerated. Leveraging semantic irrelevance (enabling CFG=0) and low-rank feature structure (via random projection and representative token restoration), StageVAR achieves up to 3.4× speedup with negligible quality drop."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to read and the figure is easy to follow.\n2. The idea of using random projection for low rank feature is interesting."}, "weaknesses": {"value": "1. The paper’s motivation is weak because the semantic irrelevance and low-rank observations appear decoupled, lacking a unified  empirical justification.\n2. The novelty is incremental, as the three-stage decomposition mainly refines FastVAR’s two-stage insight, with the “semantic stage” contribution limited to disabling text prompts after a certain scale.\n3. The predetermined rank r is derived from statistics on a specific benchmark and may not generalize well to other data distributions or prompts without further analysis.\n4. The impact of the index token sampling strategy is underexplored. Its design rationale, sensitivity to sampling choices, and effect on high-frequency details remain unclear."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e1UksxNGkD", "forum": "PeI4a4qeLY", "replyto": "PeI4a4qeLY", "signatures": ["ICLR.cc/2026/Conference/Submission1552/Reviewer_XWnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1552/Reviewer_XWnn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811105822, "cdate": 1761811105822, "tmdate": 1762915809941, "mdate": 1762915809941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for accelerating VAR, a Scale-Wise Autoregressive (AR) image generation model. First, the authors observe that VAR's image generation process is divided into three phases: Semantic Establishment, Structure Establishment, and Fidelity Refinement. They then find that the fidelity refinement phase exhibits a low-rank property, allowing it to be sufficiently approximated using low-rank tokens. Based on this insight, StageVAR proposes three techniques: (i) Random projection to quickly and dynamically approximate input tokens to a low rank. (ii) Representative token restoration, which restores the original dimension without solving LLS, by upscaling and utilizing tokens from the previous scale. (iii) Eliminating the use of CFG in fidelity refinement phase.\nExperimental results show that StageVAR achieves superior acceleration effects and quality compared to the original VAR and other VAR acceleration methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents new findings and observations regarding generation structure of  VAR.\n- The proposed methods are intuitive, simple to implement, and plug-and-play applicable to existing VAR models.\n- Experimental results show StageVAR can accelerate VAR generation process by ~3x without significant degradation in image quality."}, "weaknesses": {"value": "- **Novelty** : (i) The claim that VAR's image generation is divided into three phases is not highly interesting, given the sequential frequency generation nature of scale-wise methods. In fact, this was already reported in papers like FastVAR. (ii) At least 50% of StageVAR's acceleration effect comes from  CFG elimination in the fidelity refinement phase. However, this is closer to a simple heuristic than an academic discovery.\n\n- **Paper Structure** : The paper's structure is confusing. For example, Eq. (6) is presented as part of the method but is not actually used. Also, a latency breakdown for the fidelity refinement phase is absent, making it difficult to understand where the exact accleration came from. \n\n- **Experimental Results** : (i) The results are not evaluated on standard image evaluation metrics such as FID/IS. Benchmarks like GenEval do not precisely capture image quality degradation. (ii) The comparison with prior research, such as FastVAR and SkipVAR, is insufficient. Please refer to the questions."}, "questions": {"value": "- What is the quality-speed Pareto frontier compared to FastVAR or SkipVAR? Especially by using FID-latency axis?\n- What happens if CFG is not applied to the same later phases of FastVAR? Does StageVAR still maintain an advantage?\n- Why does Tab. 2(6) have better image quality than Tab. 2(5)? According to the paper's context, (6) is a fast approximation for implementing (5).\n- Is a different random projection matrix Q used for every operation, or is it fixed throughout the entire process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1X1wwP4B5", "forum": "PeI4a4qeLY", "replyto": "PeI4a4qeLY", "signatures": ["ICLR.cc/2026/Conference/Submission1552/Reviewer_xpe7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1552/Reviewer_xpe7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976195499, "cdate": 1761976195499, "tmdate": 1762915809648, "mdate": 1762915809648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}