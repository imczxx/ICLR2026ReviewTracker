{"id": "TvP90DWijM", "number": 21311, "cdate": 1758316155665, "mdate": 1759896929338, "content": {"title": "The Tutor-Pupil Augmentation: Enhancing Learning and Interpretability via Input Corrections", "abstract": "State-of-the-art machine learning models often incorporate prior knowledge or structural information about the task or data distribution. In some tasks, such knowledge may arise from first principles or emerge as simplified, learned functions that distill essential aspects of the data distribution. Model augmentation has emerged as a strategy to leverage this structured knowledge by coupling it with an auxiliary model to improve predictive performance, while preserving the interpretability offered by the simpler component. In this work, we present a new augmentation framework called the Tutor-Pupil scheme, which is designed to enhance both performance and interpretability. The Pupil is a fixed model, structurally designed for the core task, while the Tutor is a more flexible model trained to apply minimal input-level corrections to improve the Pupil’s performance on the modified input. This strict separation of roles enables the Tutor not only to compensate for the Pupil’s limitations but also to act as a diagnostic instrument. By examining the Tutor’s targeted interventions, we can identify failure modes, detect regions where the Pupil struggles to generalize, and uncover residual patterns or higher-order structures in the data not captured by the original model.", "tldr": "", "keywords": ["model augmentation", "machine learning for physical sciences"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb5e9b1b1b2c415dae5b35b8dbe4382d9ccda537.pdf", "supplementary_material": "/attachment/3c3be36a327b5f49d0f782e7dccf6b3533fe3df5.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the long-standing \"performance-interpretability trade-off\" in machine learning—where simple, interpretable models (e.g., decision trees, physics-based formulas) lack expressive power, while complex black-box models (e.g., neural networks) sacrifice transparency. It proposes a novel Tutor-Pupil augmentation framework to resolve this trade-off by leveraging \"minimal input-space corrections\" rather than output adjustments, enabling both performance gains and enhanced interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike prior work that corrects outputs (e.g., residual networks, ensemble stacking), this paper corrects inputs, preserving the Pupil’s interpretability.\n\n- The paper’s originality lies in redefining the paradigm of model augmentation, removing limitations of prior work, and creating novel links between data-driven learning and theoretical insight—all of which challenge long-standing practices in interpretable AI.\n\n- The paper does not limit the Tutor-Pupil framework to a single task type but adapts it to three distinct domains—a creative extension that proves its generality."}, "weaknesses": {"value": "- The paper strictly adopts a \"train Pupil first, then train Tutor\" serial paradigm (Pupil parameters are frozen during Tutor training but fails to explore joint training—a critical gap that limits the framework’s ability to fully leverage synergies between the two models and may amplify Pupil’s inherent flaws.\n\n- Novelty Gap: “Input-space correction” is not new. Position the paper as “systematic, global counterfactuals for interpretable models” rather than a brand-new paradigm and provide a taxonomy table that shows how Tutor-Pupil differs from (i) local counterfactuals, (ii) adversarial examples, (iii) data-augmentation policies on objectives, constraints, and evaluation metrics.\n\n- The paper validates the framework exclusively with interpretable Pupils (decision trees, logistic regression, ideal gas law but fails to test black-box Pupils (e.g., ResNet, Transformer)—a critical gap, as many real-world systems rely on complex models that need interpretive tools (e.g., medical image classifiers using CNNs)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E9IohE0sTy", "forum": "TvP90DWijM", "replyto": "TvP90DWijM", "signatures": ["ICLR.cc/2026/Conference/Submission21311/Reviewer_4yEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21311/Reviewer_4yEq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760501750329, "cdate": 1760501750329, "tmdate": 1762941688463, "mdate": 1762941688463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the auxiliary model in the model augmentation framework is used to boost prediction accuracy and interpretability. In a Tutor-Pupil scheme, the Pupil is used to learn according to the domain-specific features, while the Tutor adding a small perturbation to the input of Pupil, is used to uncover the specific failure modes and regions of uncertainty with the Pupil's predictions. In physics-informed models, higher-order global and structural information in the data space and decision boundaries is revealed leading to better modeling of the observations and the explanation of the shortcomings of the reference dynamical processes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow. The authors provide a sound a good background for the problem of interpretability-complexity trade-off of the modern architectures used in ML.\n\nThe authors propose a Pupil-Tutor framework to boost the performance and interpretability of simple architecture or numerical models. This method works for physical models and can be used to modify them according to the observations for better understanding of the underlying processes. \n\nThe authors provide other examples in the image datasets where the Tutor improves the prediction performance of a non-interpretable using this architecture."}, "weaknesses": {"value": "The scope of interpretability of the Tutor is limited to our understanding of the perturbations in the data space. The results are constrained to synthetic 2d data, 3 variable time independent physical system, which are too simple to understand the potential of the proposed architecture, and logistic regression on MNIST dataset which in I believe had vague inconclusive results on diagnostics and interpretability of the failure modes of a non-interpretable model.\n\nThis might be due to the nature of the Pupil perturbations that are performed in data space and therefore the interpretability is left to human understanding of the discernible features in the data space, which is another complex task. While portrayed as a tool for interpretability, I believe this framework is yet ineffective and cannot improve interpretability as well as e.g. [Sarvmaili'24], where a set of representative samples are produced which can be used to understand the main modes of failure from training data. The authors don't report any evaluation metrics on the interpretability of the predictions especially for high dimensional datasets. A more comprehensive study can be performed e.g. with 3dshapes dataset where the main features are clearly discernible in the data space, or applying known transformations to the MNIST dataset and training the Tutor to undo said transformations could lead to more conclusive results.\n\n[Sarvmaili'25] Data-centric Prediction Explanation via Kernelized Stein Discrepancy, Sarvmaili, Mahtab and Sajjad, Hassan and Wu, Ga, ICLR 2025"}, "questions": {"value": "Have the authors checked the robustness of the Pupil-Tutor framework? \n\nFamously, [Goodfellow'14] showed that the addition of a small but unstructured noise to the input leads for the change in the class, but not a visible difference in the image itself. In comparison addition of the noise in the latent space would likely translate to a visually significant changes in the image after decoding, but may still not be interpretable as the encoder compresses the image through entangling the meaningful features. As the results of the MNIST experiment are not very conclusive to me, can the authors explain how the Tutor perturbations are interpretable and different from a random cohesive structured blob?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MSYazkXUn7", "forum": "TvP90DWijM", "replyto": "TvP90DWijM", "signatures": ["ICLR.cc/2026/Conference/Submission21311/Reviewer_YX96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21311/Reviewer_YX96"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738649921, "cdate": 1761738649921, "tmdate": 1762941688061, "mdate": 1762941688061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Tutor–Pupil augmentation framework, a general approach to improving both model performance and interpretability through minimal input-level corrections. The Pupil is a fixed, interpretable or task-specialized model (e.g., decision tree, physical law, logistic regression), while the Tutor is a flexible neural network trained to apply small corrections to the inputs such that the Pupil produces more accurate outputs. This setup enables the Tutor not only to enhance predictions but also to diagnose failure modes of the Pupil by revealing where and how input perturbations correct errors. The framework is demonstrated on three diverse cases: (1) augmenting a decision tree for a toy binary classification problem, (2) refining the ideal gas law to account for non-ideal behaviors—discovering van der Waals-like corrections, and (3) improving handwritten digit classification via a VAE-based latent-space Tutor acting on a logistic regression Pupil, which also provides interpretable visual corrections. The results show notable gains in accuracy and interpretability across tasks, with meaningful parallels to symbolic regression and explainable AI."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is conceptually novel and elegant, offering a unified and interpretable augmentation scheme applicable across interpretable and black-box models. Its theoretical framing—training a Tutor to apply minimal, semantically meaningful corrections—is both intuitive and powerful. The work’s breadth, spanning interpretable (decision trees), physics-based (ideal gas law), and data-driven (MNIST classification) settings, convincingly demonstrates generality. The analyses are rigorous, supported by visualizations. The MNIST experiment is particularly compelling: the Tutor’s subtle adjustments (e.g., closing loops or clarifying strokes) both enhance performance ( and produce human-readable explanations that outperform conventional attribution maps. The idea of deriving physically meaningful corrections from learned input perturbations is especially original and promising for scientific ML applications."}, "weaknesses": {"value": "In the MNIST setting, the performance jump (91%→98.5%) could partly result from the use of a VAE-trained latent representation rather than purely from the Tutor’s corrective effect. \n\nAdditionally, the paper could better differentiate its contributions from related ideas like counterfactual explanations, residual learning, and gradient-based input attribution methods. \n\nFinally, the experiments, while creative, are small-scale; a larger empirical evaluation would strengthen the claims of robustness and general utility."}, "questions": {"value": "How sensitive are the Tutor’s corrections to hyperparameters such as λ (correction magnitude regularization)?\n\nCould the framework handle non-differentiable Pupils (e.g., rule-based systems) at scale?\n\nDoes the learned correction vector generalize across data distributions or must it be retrained for each Pupil or dataset?\n\nHow can one quantify interpretability improvements beyond visual inspection (e.g., through user studies or explanation fidelity metrics)?\n\nCould the Tutor–Pupil setup be extended to adversarial tutoring, where the Tutor exposes brittleness or bias in the Pupil rather than assisting it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4EL0iEYSXj", "forum": "TvP90DWijM", "replyto": "TvP90DWijM", "signatures": ["ICLR.cc/2026/Conference/Submission21311/Reviewer_Z1yU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21311/Reviewer_Z1yU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031216840, "cdate": 1762031216840, "tmdate": 1762941687675, "mdate": 1762941687675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests a new form of interpretability approach without compromising expressivity. It is a common practice to use a simple interpretable model as a primary model and use a second model for handling the residual error for performance reasons. But interpretability is usually lost in the complexity of the second model. \nThis paper proposes an alternate. They use the second model instead to generate input perturbations such that the primary model is more accurate on the perturbed input. Assuming input perturbations are interpretable, the paper argues that their approach improves performance without hurting interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Clean presentation. I enjoyed reading the paper.  \n* Neat conceptual difference. Modeling input perturbation instead of residual output error is a clean conceptual differential from earlier work. \n* In the case of MNIST modeled with logistic regression, improving accuracy through augmentation and output correction could not have led to the explanations the paper demonstrated in Figure 6. MNIST dataset, although simple, supports their claim of improved accuracy and interpretability with their approach."}, "weaknesses": {"value": "* Empirical validation. The validation in the paper looks preliminary. It requires validation with far more complex datasets to be taken seriously. For instance, CheXpert [1] or some WILDS [2] datasets.   \n* The paper assumes input edits are model-able and interpretable. The interpretability aspect is only assumed without validation. \n\nReferences   \n[1] https://www.nature.com/articles/s42256-021-00338-7\n[2] https://wilds.stanford.edu/datasets/"}, "questions": {"value": "1. If the pupil model is complex, what's stopping the tutor from selecting meaningless perturbations? I.e., how is the interpretability of perturbations enforced?   \n2. The requirement of latent space and pupil model inference with modified inputs could compromise performance due to train-test distribution for pupil model and other reasons. When do you expect far worser performance (than a non-interpretable base model) with your approach?              \n3. Please elaborate your differences from counterfactual explanations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIT9bqr5H8", "forum": "TvP90DWijM", "replyto": "TvP90DWijM", "signatures": ["ICLR.cc/2026/Conference/Submission21311/Reviewer_SNr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21311/Reviewer_SNr4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762336529608, "cdate": 1762336529608, "tmdate": 1762941687437, "mdate": 1762941687437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}