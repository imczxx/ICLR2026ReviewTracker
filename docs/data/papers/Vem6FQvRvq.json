{"id": "Vem6FQvRvq", "number": 9531, "cdate": 1758126276029, "mdate": 1759897713882, "content": {"title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "abstract": "Recently, significant progress has been made in developing reasoning-capable Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead due to the large Key-Value (KV) Cache memory overhead.\nPost-training KV Cache quantization has emerged as a promising compression technique and has been extensively studied in short-context scenarios.\nHowever, directly applying existing methods to long-CoT LLMs causes significant performance degradation due to the following two reasons: \n(1) Large cumulative error: Existing methods fail to adequately leverage available memory, and they directly quantize the KV Cache during each decoding step, leading to large cumulative quantization error.\n(2) Short-context calibration: Due to Rotary Positional Embedding (RoPE), the use of short-context data during calibration fails to account for the distribution of less frequent channels in the Key Cache, resulting in performance loss.\nWe propose Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) for long-CoT LLMs to address the above issues in two folds:\n(1) To reduce cumulative error, we design a progressive quantization strategy to gradually lower the bit-width of KV Cache in each block. Then, we propose block-wise memory allocation to assign a higher bit-width to more sensitive transformer blocks. \n(2) To increase the calibration length without additional overhead, we propose a new calibration strategy with positional interpolation that leverages short calibration data with positional interpolation to approximate the data distribution of long-context data.\nExtensive experiments on 7B–70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark performance by up to 8% over SOTA baselines under the same memory budget and achieves 2.73–5.18$\\times$ throughput over the original 16-bit LLMs.\nOur code will be released soon.", "tldr": "We propose Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) for long-CoT LLMs.", "keywords": ["KV Cache Quantization"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cb175e8fb190c8a47c3b824c1efc3abaf18f972.pdf", "supplementary_material": "/attachment/4fb968aff2c70e60ae1ea69bd0dddaca3a596fa6.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses KV cache quantization from a utility-driven perspective. The authors hypothesize that by fully utilizing the available memory and reducing the quantization bit-width only when the memory becomes full, one can achieve better CoT reasoning accuracy. Based on this idea, they propose a progressive quantization approach with an Equivalent Right Shift strategy. Specifically, all KV caches are stored in full precision initially, and once the memory limit is reached, the caches are progressively quantized by half using the proposed strategy. The authors further suggest assigning different memory budgets to each transformer block and formulate this allocation as an integer programming problem. To enable effective calibration for long contexts, they adopt positional interpolation to approximate long-context calibration using short-context data. Experimental results show that progressive quantization outperforms state-of-the-art fixed-precision quantization methods while maintaining comparable efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Approaching KV cache quantization from a utility-driven perspective is interesting and practically relevant, as it reflects real-world deployment considerations.\n\n2. The figure illustrating the main idea of the paper is well-designed and easy to follow, effectively conveying the core concept.\n\n3. The authors conduct experiments on multiple models, demonstrating the broad applicability and effectiveness of the proposed method."}, "weaknesses": {"value": "1. My main concern lies in the fairness of the experimental results presented in Table 1. The baseline comparison is rather limited, as KIVI serves as the only comparable baseline in most evaluations. Since KIVI uses a fixed precision while the proposed method can employ higher precision during generation, the comparison may not be entirely fair. It would be helpful to report the memory usage during generation for both KIVI and the proposed method to provide a more complete picture. It remains unclear whether the observed improvement primarily comes from the fact that the proposed approach occupies more memory overall during generation, which actually contradicts the reason why we apply quantization, as we want to reduce the memory usage.\n\n2. Because the proposed method depends on fully utilizing the available memory, it is important to evaluate its performance across different hardware configurations with varying memory capacities. Such an analysis would clarify how accuracy scales when the available memory changes.\n\n3. I find the use of positional interpolation insufficient to address the main challenge of long-context calibration. This method essentially distributes a small number of tokens over a wide range, leaving many positions without properly calibrated data. Moreover, according to Table 4, positional interpolation appears to offer limited improvement, which further supports this concern."}, "questions": {"value": "1. How does the method compare to KIVI that also fully utilizes the available memory? Would there be a way to conduct such a fair comparison?\n\n2. For the Equivalent Right Shift strategy, is it reasonable to keep the zero point unchanged? Would it be possible for the zero point to vary across different quantization bit levels?\n\n3. In Table 4, when s increases to 16, the use of positional interpolation appears to cause performance degradation. Does this imply that the proposed method has inherent limitations? Overall, the method’s effect on pass@1 seems marginal.\n\n4. There is a typo around line 298: should it be CMIMC-2025 or CMIMC-2024?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NhqL0NFvH7", "forum": "Vem6FQvRvq", "replyto": "Vem6FQvRvq", "signatures": ["ICLR.cc/2026/Conference/Submission9531/Reviewer_WmCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9531/Reviewer_WmCs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502087435, "cdate": 1761502087435, "tmdate": 1762921095459, "mdate": 1762921095459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PM-KVQ, a post-training quantization scheme for long-CoT LLM inference that combines: a. progressive quantization (start at higher precision and shrink the KV cache bit-width only when memory is about to run out), including an “Equivalent Right Shift” rule (`Eq. 3`) for precise bit-width shrinking, b. block-wise memory allocation, cast as a small integer program solved with CVXPY to allocate higher bit-widths to more sensitive transformer blocks; and c. calibration with positional interpolation to expose short calibration sequences to long-ctx RoPE phases. Reported results on DeepSeek-R1-Distill (7B-70B) and QwQ-32B show improved pass@1/vote accuracy over KIVI/RotateKV/MiKV at 2-4-bit KV cache, with throughput close to KIVI but below it."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear diagnosis of two long-CoT pain points (cumulative error, RoPE low-frequency channels), with concrete formulations (Eqs. 9-12) motivating the positional interpolation trick. \n- Simple but effective shrinking rule (Eq. 3) that avoids round-trip dequantization in implementation. \n- Block-wise allocation objective is standard, implementable, and explains gains when memory is partially free."}, "weaknesses": {"value": "- The paper only reports FP16 results, omitting bf16, which is the de facto standard for inference. Since bf16 offers wider dynamic range and distinct hardware behavior, excluding it leaves uncertainty about PM-KVQ’s performance and compatibility in realistic deployment settings. \n- Accuracy under fake quant: Reporting accuracy without real 2-bit/4-bit kernels weakens the claim that PM-KVQ is robust in practice."}, "questions": {"value": "- **L41**: What is the reference for the claim “to generate 128K tokens”?  \n- **Table 1**: I am skeptical of using **BS** in your setting. What is the number of output tokens? Since the reasoning model generates long-CoT with the mentioned BS and GPU memory, it’s most likely that we get CUDA OOM.  \n- **Table 1**: **QwQ** has 32B parameters. How do you use it with one A100?  \n- **Sec 3.2**: Why is **CVXPY** used? What was the reason behind that?  \n- **Sec 4.1**: Your evaluation metric is **pass@k** where *k = 1*. What is *n* (number of independent trials)?  \n- **Sec 4.1**: Regarding **Voting**, how many samples did you draw?  \n- Could the progressive quantization step, while helpful for memory control, also hinder GPU utilization by blocking parallel execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "03rLZTg7Go", "forum": "Vem6FQvRvq", "replyto": "Vem6FQvRvq", "signatures": ["ICLR.cc/2026/Conference/Submission9531/Reviewer_u27E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9531/Reviewer_u27E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858403023, "cdate": 1761858403023, "tmdate": 1762921094603, "mdate": 1762921094603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose progressive mixed precision KV Cache quantization, of which a higher bit-width is assigned to more sensitive transformer blocks tailored for long-CoT scenarios, aiming for the low memory usage and quantization error. Extensive experiments on 7B–70B long-CoT LLMs show that the proposed block-wise and mixed precision quantization method improves reasoning benchmark performance by up to 8% over SOTA baselines under the same memory budget and achieves 2.73–5.18× throughput over the original 16-bit LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The progressive mixed precision quantization is interesting: 1) initially, the high bit (16-bit) quantization is used for the short-sequence; 2) then progressively shrink the bit width, while the high sensitive transformer blocks are maintained with the high bit width to narrow the quantization error; 2) and the memory allocation is block-wise, which is adaptive to the PageAttention.\n\nSecondary, the experiments show good, especially for the long-cot tasks, the proposed method improves reasoning benchmark performance by up to 8% over SOTA baselines."}, "weaknesses": {"value": "The mixed precision quantization of KV cache is mature in the academia, such as KVTuner which allocate different bit width for different layer and K/V by optimized search algorithm. So the comparison with SOTA mixed quantization methods is not enough. And the practical benefit on the hardware is not given, such as the memory access saving and the throughput increase."}, "questions": {"value": "1. How can this method used with PageAttention, for the quantized KV Cache management?\n2. How can this method used with sparse Attention, such QuestAttention, DSA or NSA?\n3. Can you draw the theoretical analysis, why such progressive shrinking is useful for long-cot tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cmn2moqHCW", "forum": "Vem6FQvRvq", "replyto": "Vem6FQvRvq", "signatures": ["ICLR.cc/2026/Conference/Submission9531/Reviewer_nh48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9531/Reviewer_nh48"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923383795, "cdate": 1761923383795, "tmdate": 1762921094250, "mdate": 1762921094250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}