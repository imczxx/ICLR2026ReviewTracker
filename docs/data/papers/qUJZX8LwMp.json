{"id": "qUJZX8LwMp", "number": 1545, "cdate": 1756890994564, "mdate": 1763094246393, "content": {"title": "Ditto: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset", "abstract": "Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence.  Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy.  The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing. We will release our dataset, models, and code to accelerate research in this field.", "tldr": "To solve the data scarcity problem, we introduce a scalable pipeline Ditto for generating high-quality video editing data, which is used to train a new state-of-the-art instruction-based video editing model Editto.", "keywords": ["Instruction-based video editing", "diffusion models", "synthetic ddataset"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/afa484d3376a91bed3da272265c4ecdd9e32ea7c.pdf", "supplementary_material": "/attachment/8daec567506b4e900bb3e7df424ee18ae21ccece.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Ditto-1M, a large-scale instructional video editing dataset designed to address the data scarcity problem in instructional video editing. The dataset is constructed through an automated pipeline that generates editing instructions and corresponding edited videos from source videos. Based on this dataset, the authors propose Editto, a VACE-based video editing model trained on Ditto-1M. According to the reported results, Editto achieves better performance compared to previous inversion-based and feedforward methods in video editing tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Large-scale open-source dataset**: The proposed Ditto-1M dataset is substantial in scale and will be made publicly available, which represents a valuable contribution to the research community and can facilitate future work in instructional video editing.\n\n2. **Comprehensive data construction pipeline**: The entire data construction process demonstrates significant engineering effort and computational resources, involving multiple stages of video processing, instruction generation, and quality control.\n\n3. **Visually appealing results**: The edited videos presented in the paper show visually plausible results, suggesting that the proposed method can generate reasonable editing outcomes for various instruction types."}, "weaknesses": {"value": "1. **Lack of Novelty and Insufficient Comparison with Prior Work**\nThis paper lacks significant novelty. The methodology is quite similar to Senorita-2M[1], with the primary difference being the new backbone used. However, the paper does not include any comparison or discussion with that work. To illustrate the substantial similarities, we present the following table:\n| Dimension | Ditto-1M | Senorita-2M |\n|-----------|----------|-------------|\n| **Source Videos** | 200K videos from Pexels | 388K videos from Pexels |\n| **Instruction Generation** | Qwen2.5-VL | Llama 3.2-8B |\n| **Target Video Generation** | Key frame editing (Qwen-Image) + video propagation (VACE, training-free) | First frame editing (SD1.5 ControlNet/Flux Fill) + video propagation (task-specialized models trained on CogVideoX-5B) |\n| **Training Strategy** | Fine-tune VACE-14B (essentially a ControlNet) with edited key frames | Two variants: (1) Pure instruction-based (InstructPix2Pix-like), (2) First-frame guided ControlNet. Based on CogVideoX-5B-I2V/Wan2.1-1.3B/Wan2.1-14B |\n\n2. **Limitations of the Target Video Generation Pipeline for Local Editing**\n\nThe proposed target video generation pipeline appears to handle only global editing tasks effectively. As described in Section 3.4, the method utilizes VACE with three inputs: (1) an edited reference image produced by Qwen-Image, (2) dense depth maps extracted from the source videos, and (3) the editing instruction. The pipeline is expected to propagate the editing results from the reference image while preserving the original video motions and structure through the dense depth maps.\n\nHowever, this approach presents a fundamental contradiction for free-form editing tasks, such as object addition or removal. Specifically:\n\n- The **edited reference image** correctly reflects the added or removed objects\n- The **depth maps**, extracted from the source videos, do not reflect these changes—the depth information for added/removed regions remains unchanged from the original video\n\nGiven these conflicting inputs, it is unclear how the VACE model resolves this inconsistency. Does it prioritize the reference image or the depth maps? This limitation suggests the pipeline may struggle with local editing tasks that require geometric changes, potentially restricting its applicability to primarily global appearance modifications. Can the authors elaborate more about this?\n\n3. **Insufficient Details in Quantitative Evaluation**\n\nThe quantitative evaluation presented in Section 5.2 and Table 2 lacks critical methodological details, severely limiting reproducibility and interpretability of the results. Specifically, the following essential information is missing:\n\n- **Evaluation dataset**: Which dataset(s) were used for evaluation? Is it a held-out test set from the training data, or an independent benchmark?\n- **Dataset scale**: How many videos were evaluated?\n- **Video specifications**: What are the resolution, duration, and frame rate of the test videos?\n- **Baseline implementation**: How were the baseline methods executed? Were official implementations used, or were models retrained? What hyperparameters were applied?\n\nAs this represents the **only quantitative evaluation** in the paper, the absence of these details significantly undermines:\n1. The ability to assess the true performance and generalization capability of the proposed method\n2. The validity of comparisons with baseline methods\n3. The reproducibility of the reported results\n\n[1] Zi, B., Ruan, P., Chen, M., Qi, X., Hao, S., Zhao, S., ... & Wong, K. F. (2025). Se\\~ norita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists. arXiv preprint arXiv:2502.06734."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L2x2XOEhP9", "forum": "qUJZX8LwMp", "replyto": "qUJZX8LwMp", "signatures": ["ICLR.cc/2026/Conference/Submission1545/Reviewer_BeRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1545/Reviewer_BeRZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761156470795, "cdate": 1761156470795, "tmdate": 1762915806618, "mdate": 1762915806618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mGoUVQ6pYD", "forum": "qUJZX8LwMp", "replyto": "qUJZX8LwMp", "signatures": ["ICLR.cc/2026/Conference/Submission1545/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1545/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763094245504, "cdate": 1763094245504, "tmdate": 1763094245504, "mdate": 1763094245504, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DITTO, a framework designed to scale instruction-based video editing models. The core contribution is a novel, large-scale, high-quality synthetic instruction-video-edit dataset (DITTO-Data), created through an innovative pipeline that leverages powerful pre-trained Large Language Models (LLMs) and diffusion models (DMs) to automatically generate diverse, complex editing instructions and the corresponding edited videos. Based on this dataset, the authors train DITTO-Model, a video editing model which demonstrates strong capabilities in instruction following, temporal consistency, and maintaining content fidelity. The experiments show DITTO-Model achieving state-of-the-art results on several benchmarks, particularly excelling in complex, style-based, and semantic edits, validated by both quantitative metrics and comprehensive human evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High-Quality, Scalable Data Generation: The synthetic data pipeline is the major strength, addressing the prohibitive cost and complexity of manual video editing data collection. The use of LLMs for instruction diversity is particularly effective.\n2. State-of-the-Art Performance: DITTO-Model achieves superior results across multiple metrics, notably in human evaluation on Instruction Following and Temporal Consistency, which are crucial aspects of video editing.\n3. Instruction Complexity: The generated dataset and resulting model are shown to handle a wide range of instruction complexities, including appearance transformation, style transfer, and semantic manipulation, moving beyond simple object insertion/removal."}, "weaknesses": {"value": "1. Black-Box Data Quality: While the paper describes the Quality Control module, the extent to which the synthetic data truly captures the complexity and subtle detail of real-world human-labeled edits is hard to quantify. Further analysis on the \"failure modes\" of the synthetic pipeline and the resulting data distribution bias would be beneficial.\n2. Model Architecture Novelty: The DITTO-Model architecture itself is largely an assembly of existing, robust components (latent diffusion model, motion modules). The novelty lies more in the data and training strategy than the architectural innovations."}, "questions": {"value": "1. Generalization to Real Edits: While the human study uses the synthetic dataset, how does DITTO-Model perform when asked to execute complex instructions on out-of-distribution real-world videos that might contain more unusual or messy degradation patterns not fully captured by the synthetic base videos?\n2. Ablation on LLM Prompting: Could the authors provide more detail, perhaps in the appendix, on the meta-prompts used to guide the LLM to generate the diverse and complex editing instructions? This \"prompt engineering\" is critical to the dataset's quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AGKf1qP76S", "forum": "qUJZX8LwMp", "replyto": "qUJZX8LwMp", "signatures": ["ICLR.cc/2026/Conference/Submission1545/Reviewer_AcEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1545/Reviewer_AcEZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761402055489, "cdate": 1761402055489, "tmdate": 1762915806348, "mdate": 1762915806348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Ditto, a scalable synthetic data pipeline that couples (i) an instruction-based image editor to produce a high-quality edited keyframe, (ii) an in-context video generator (VACE) conditioned on the edited keyframe and a depth video for temporal structure, (iii) an autonomous VLM agent to author instructions and filter failures, and (iv) a lightweight temporal enhancer/denoiser (Wan2.2 fine stage) for quality polishing. Using ~12,000 GPU-days, the authors build Ditto-1M, a 1M-sample triplet dataset (source, instruction, edited video) and train Editto with a modality curriculum that anneals away reliance on the edited keyframe to achieve purely instruction-driven editing. Quantitative results (Table 2) and extensive visuals (Figs. 1,5,7–11) show strong instruction following and temporal consistency versus TokenFlow/InsV2V/InsViE and a commercial baseline. The paper commits to releasing dataset, models, and code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Holistic, scalable pipeline. The edited keyframe + depth + in-context generator composition is simple and effective; the VLM-driven instruction/QA removes human bottlenecks (Fig. 2). \nLarge, curated dataset.Ditto-1M (720p, 101 frames @ 20 FPS) with ~700k global and ~300k local edits is a sizable, diverse resource; the authors report strong aesthetic curation and motion screening (Fig. 3, §3.1–3.6). \nModeling insight.The modality curriculum bridges visual-to-text conditioning in a stable way (Fig. 4), improving instruction-following without the edited frame at test time. \nEmpirical results. Consistent wins across automatic and human metrics (Table 2) and convincing visuals (Fig. 5); ablations show data-scale and MCL benefits (Fig. 8). \nEfficiency consciousness. Distillation/quantization and a temporal enhancer reduce generation cost while improving temporal stability (§3.4–3.5)."}, "weaknesses": {"value": "Attribution granularity. While Fig. 9 analyzes generator contexts, the paper lacks systematic ablations over the full pipeline: e.g., removal/variation of the VLM filter, different denoisers, or no enhancer; per-stage quality/cost curves would support the “cost-quality” claims more rigorously. \nEvaluation breadth. Automatic metrics are mostly CLIP-based and a VLM score. Consider adding FVD/KVD, LPIPS-T, or user-calibrated instruction-fidelity rubrics. Also report identity preservation for local edits. \nDependence on proprietary tools. The pipeline leans on powerful closed models (image editor, VLM judge, commercial T2V denoiser). This may limit reproducibility and confound generality claims; an open-source-only variant and a sensitivity study would help. \nData/rights clarity. The dataset is sourced from Pexels; redistribution and downstream editing rights should be specified precisely (e.g., license text, usage constraints, opt-out). Provide counts rejected by safety filters and failure taxonomies. \nRisk analysis limited. Ethics statement is brief; a more thorough assessment of misuse (e.g., identity edits, misinformation), watermarking, and content provenance would be welcome."}, "questions": {"value": "1. Provide ablation results for denoiser enhancer and VLM filtering on instruction fidelity, temporal consistency, and aesthetics\n2. Test performance drop when replacing all components with open models\n3. Add evaluation on public benchmarks with FVD/KVD and identity-preservation metrics\n4. Clarify Ditto-1M license, redistribution rights, content filtering, and TOS compliance\n5. Analyze VLM judge rejection categories and failure modes"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "- Copyright/TOS compliance for third-party content (Pexels)\n- Potential misuse for synthetic video manipulation\n- Need license clarification, right-of-publicity considerations, safety filters"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d4M3WbVYyv", "forum": "qUJZX8LwMp", "replyto": "qUJZX8LwMp", "signatures": ["ICLR.cc/2026/Conference/Submission1545/Reviewer_3NGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1545/Reviewer_3NGv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652937074, "cdate": 1761652937074, "tmdate": 1762915806135, "mdate": 1762915806135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework named Ditto to address the long-standing scarcity of high-quality training data in instruction-driven video editing. The authors construct a scalable, low-cost, and fully automated synthetic data generation pipeline to create large-scale, high-fidelity, temporally coherent video editing triplets (source video, editing instruction, edited video), and release the Ditto-1M dataset based on this approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared to existing datasets, this work presents the first million-scale, high-resolution, long-sequence instruction-based video editing dataset that covers both global and local editing tasks, filling a critical gap in the community.\n2. The paper achieves state-of-the-art (SOTA) performance on both automatic metrics and human evaluations, with particularly significant margins in human assessments.\n3. The paper is clearly written and accompanied by well-designed, informative illustrations."}, "weaknesses": {"value": "1. In the post-processing stage, a Vision-Language Model (VLM) is used for filtering. However, it is well known that VLMs have limited capability in understanding fine-grained visual details. How does the method ensure consistency in non-edited regions? More specifically, how is pixel-level detail consistency guaranteed?\n2. The quantitative comparison on video editing is necessarily limited due to the scarcity of comparable methods. If expanding the set of video baselines is infeasible, the authors could instead evaluate their model on image editing tasks, where strong benchmarks exist, to better validate its core editing capabilities and the effectiveness of the proposed modality curriculum learning strategy.\n3. Although the paper reports strong human evaluation results, it lacks essential experimental details such as the number and background of evaluators, the precise definition of rating criteria, and inter-annotator agreement metrics, making it difficult to assess the reliability of these results.\n4. The approach relies on a video generator to directly synthesize the edited video, but it does not address how to ensure physical plausibility in the outputs. There is no discussion of evaluation criteria or validation for physical realism, which could lead to generated content that appears unrealistic or violates real-world dynamics."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c0NmJTjzsj", "forum": "qUJZX8LwMp", "replyto": "qUJZX8LwMp", "signatures": ["ICLR.cc/2026/Conference/Submission1545/Reviewer_4iy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1545/Reviewer_4iy5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877454492, "cdate": 1761877454492, "tmdate": 1762915806018, "mdate": 1762915806018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}