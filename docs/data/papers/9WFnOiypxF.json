{"id": "9WFnOiypxF", "number": 7821, "cdate": 1758037485094, "mdate": 1759897830029, "content": {"title": "SCoT: Self-Correction at Test-time for Image Generation", "abstract": "Test-time scaling has emerged as an effective strategy to enhance image generation quality by repeatedly generating multiple images and selecting optimal outputs. However, such best-of-N schemes essentially rely on blind resampling with different random seeds, lacking the ability to incrementally refine errors based on previously correct generations. Some improved approaches rely on external verifiers to identify textual errors and feed them back to the model for refinement. However, they do not support targeted modifications with image consistency, and introduce further computational overhead. In this work, to address these limitations, we propose Self-Correction at Test-time (SCoT), a novel framework that equips generative models with internal self-assessment and targeted revision capabilities. Specifically, SCoT is trained to preserve the correctly generated regions while autonomously modifying only erroneous parts, eliminating the need for external guidance. This self-reflective mechanism enhances visual consistency, and unlocks the model’s potential capacity for prompt-guided correction. SCoT improves over the baseline by up to 0.25, substantially surpassing prior methods, providing a more reliable, efficient, and user-aligned approach to high-quality image generation.", "tldr": "", "keywords": ["test time scaling", "image generation", "image editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/034589e3ffe3e1074ee9c3c81740da2fafb2b5ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Self-Correction at Test-time (SCoT) framework, which aims to address the issues of blind resampling, external dependence, and consistency loss in the \"test-time scaling\" methods for image generation. By constructing positive-negative sample pairs to train the model, this framework is based on the base model SANA-1.0-1.6B. It employs a VAE encoder to preserve image information and adopts a dual-branch cross-attention mechanism to enable image-text interaction, thereby endowing the model with the ability to perform local error correction without external guidance. Experimental results show that SCoT achieves a maximum improvement of 0.25 over the baseline on the GenEval benchmark, with an overall score of 0.91, outperforming methods such as  SFT and Reflect-DiT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents comprehensive experiments, with clear advantages in subtasks such as relative position and attribute binding, which validates the effectiveness of the proposed method. Meanwhile, the paper’s presentation is relatively thorough, and aspects including the model architecture and data construction are relatively clear."}, "weaknesses": {"value": "1) The authors’ core claim regarding innovation hinges on the assertion that \"existing works such as Reflect-DiT rely on external verifiers or user guidance,\" while framing their proposed method as a breakthrough in achieving autonomous correction without external support. However, the existence of prior works like [1] (which demonstrates self-check capabilities in models such as BAGEL) does introduce potential challenges to this innovation claim—the extent of the impact depends on whether the authors explicitly clarify the essential differences between their method and BAGEL-like self-check mechanisms, what unique pain points does your method solve that BAGEL cannot? For example, does it outperform BAGEL in localized error precision, computational efficiency, or generalization to complex image generation scenarios (e.g., high-resolution images or ambiguous prompts)?\n[1] Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision\n2) I have noticed that the paper Reflect-DiT also uses the example \"a photo of a dog right of a tie\"; however, Reflect-DiT can achieve correct generation for this case, which is inconsistent with the demonstration presented in Figure 4 of this paper. Could this discrepancy be attributed to certain inconsistencies in experimental settings?   \n3)   In the ablation experiments, I would like to further see the authors' insights on self-attention and cross-attention. For instance, I observe that w/o cross-attn, the overall composition of the generated images does not undergo significant modifications; whereas w/o self-attn, there are substantial differences in the composition. Is this a universal phenomenon? Could the specific roles of these two types of attention in the proposed framework be further analyzed?"}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "msFrNtUVk0", "forum": "9WFnOiypxF", "replyto": "9WFnOiypxF", "signatures": ["ICLR.cc/2026/Conference/Submission7821/Reviewer_cF4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7821/Reviewer_cF4M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921796033, "cdate": 1761921796033, "tmdate": 1762919867421, "mdate": 1762919867421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCoT (Self-Correction at Test-time), a novel framework for improving text-to-image generation reliability. \nThe core idea is to fine-tune a model to intrinsically identify and correct its own errors, moving beyond blind resampling or reliance on external verifiers. \nThe method constructs a synthetic dataset of {prompt, incorrect_image, correct_image} triplets and trains a model to perform targeted, single-step corrections. \nArchitecturally, it uses a shared VAE encoder for both the input and conditional image to prevent domain gaps and employs a dual-branch cross-attention mechanism to integrate text and image guidance. \nThe method demonstrates significant performance gains on the GenEval benchmark, particularly on challenging compositional tasks like object positioning and attribute binding, while showing superior visual consistency compared to other image editing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and Relevant Problem Formulation: \nThe paper addresses a critical challenge in generative AI—reliability—by proposing an elegant self-correction mechanism that is learned directly into the model, avoiding complex multi-stage pipelines with external verifiers.\n- Strong Empirical Performance: \nThe method achieves state-of-the-art results on the GenEval benchmark, with a substantial +0.25 improvement over its baseline. \nThe gains are especially pronounced in difficult compositional tasks (Position: +0.69, Attribution: +0.30), indicating a genuine improvement in model understanding.\n- Excellent Content Preservation: \nThe architectural design successfully enables localized edits while preserving the overall image structure. \nThis is quantitatively supported by SCoT achieving the highest CLIP-Image similarity score (0.96) among all compared one-step editing baselines.\n- Sound and Well-Motivated Architecture: \nThe design choices, including the unified VAE encoding path to mitigate domain shift and the zero-initialized dual-branch cross-attention for controlled guidance, are technically sound and well-justified for the task.\n- Clear Presentation: \nThe paper is well-written, clearly organized, and the proposed method is explained effectively with helpful diagrams and visual examples. \nThe training details are transparent, which aids in understanding the experimental setup."}, "weaknesses": {"value": "- Narrow Evaluation Scope: \nThe paper's claims are almost exclusively validated on the GenEval benchmark. \nWhile the results are strong, this over-reliance makes it difficult to assess generalization. \nThe evaluation lacks human preference studies, which are crucial for judging the subtle quality of corrections.\n- Potential Data Pipeline Bias: \nThe training data is synthesized using a pipeline that relies on GenEval templates and other powerful models (GPT-4o, FLUX). \nThis raises concerns about distributional leakage, where the model may be unintentionally fine-tuned on the specific style and structure of the test benchmark, potentially inflating performance.\n- Insufficient Efficiency Analysis: \nThe paper claims efficiency improvements but lacks a rigorous computational cost analysis. \nIt does not provide essential metrics like wall-clock time or FLOPs-per-image compared to baselines like Reflect-DiT and best-of-N sampling under an identical compute budget.\n- Limited Ablation Studies: \nThe ablations primarily focus on the presence of interaction layers. \nThe paper would be strengthened by ablations on the data itself, such as the impact of the positive-to-negative pair ratio, the effect of noise in the training data from the detector, and the importance of the zero-initialization strategy.\n---\n- General Limitations: \nThe authors should include a dedicated limitations section discussing the following:\n    - The model's performance is fundamentally dependent on the quality and biases of the third-party models (object detector, GPT-4o, FLUX) used to create the training data. \n    - The current framework is tailored to fix object-centric errors identifiable by detectors and may not generalize to more abstract, stylistic, or subjective flaws. \n    - The potential for misuse of such a targeted editing tool for subtle and potentially deceptive content manipulation is not discussed.\n    - The reproducibility of the core contribution is hampered by the reliance on a complex, proprietary data generation pipeline."}, "questions": {"value": "- Regarding data provenance, beyond filtering prompts that appear in the test set, what steps were taken to ensure the training data templates are sufficiently distinct from the GenEval test templates to prevent distributional leakage? \nCould you provide statistics on the lexical or structural similarity between your training prompts and the test set?\n- Could you provide a precise computational cost comparison? \nSpecifically, please report the per-image wall-clock time and/or FLOPs for generating a final corrected image with SCoT (Best-of-20) versus Reflect-DiT (N=20) and a naive Best-of-20 baseline, using the same hardware.\n- How does the model behave under repeated applications? \nIf a correction is still imperfect, does a second pass with SCoT continue to improve the image, or does performance saturate or even degrade? Showing a k-step (e.g., k=1 to 5) performance curve would be very insightful.\n- The data generation pipeline relies on an object detector to identify errors. \nWhat was the estimated error rate of the detector, and how does this label noise in the training data affect the model's final performance? An experiment analyzing the model's robustness to synthetic feedback noise would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "blvziCXd7x", "forum": "9WFnOiypxF", "replyto": "9WFnOiypxF", "signatures": ["ICLR.cc/2026/Conference/Submission7821/Reviewer_qXdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7821/Reviewer_qXdx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975113359, "cdate": 1761975113359, "tmdate": 1762919867005, "mdate": 1762919867005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SCoT, a method that enables image generators to correct specific erroneous regions in generated images while keeping the rest of the content unchanged.\nThe method trains on synthetic positive–negative image pairs created from Reflect-DiT outputs and GPT-4o-assisted corrections.\nArchitecturally, SCoT augments the SANA-1.0-1.6B base model with dual attention branches for image and text conditioning, improving consistency and prompt alignment."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Shows clear improvements on GenEval metrics, demonstrating effectiveness in targeted correction.\nConceptually simple --> turns test-time resampling into a self-reflective correction process.\nEfficient: adds minimal overhead and runs on modest hardware."}, "weaknesses": {"value": "- Relies on synthetic GPT-4o-generated training data, raising reproducibility concerns.\n- Appears to reduce image diversity, potentially limiting generative variability.\n- The paper is difficult to follow, with redundant phrasing and unclear explanations (e.g., line 201: “low inference cost, and fast sampling speed”).\n- Lacks quantitative evaluation of runtime gains and edit localization metrics."}, "questions": {"value": "- How much does SCoT modify the original image (e.g., pixel-level or feature-level distance)?\n- What is the actual speed-up achieved compared to standard inference?\n- Is the reliance on synthetic supervision scalable without LLM-generated data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p7U73VB4yS", "forum": "9WFnOiypxF", "replyto": "9WFnOiypxF", "signatures": ["ICLR.cc/2026/Conference/Submission7821/Reviewer_N5PS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7821/Reviewer_N5PS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991613004, "cdate": 1761991613004, "tmdate": 1762919866586, "mdate": 1762919866586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCoT for a test-time self-correction framework for text-to-image generation. SCoT aims to use the same model to generate an image from text first and then correct the generated image to improve the quality. The training data of positive and negative samples for self-correction is synthetized using GPT-4o and Flux.1-Kontext. SANA-1.0-1.6B is finetuned on the synthetic training samples after extending its attention layer for image reference. Experiments on the GenEval benchmark show performance gains and qualitative improvements compared with the previous best-of-N method for test-time scaling."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1. This work targets important and practical topics – self-correction of image generative models. The capability of self-correction can incorporate reasoning processes to improve the image generation performance as language models have brought significant performance gains by test-time scaling.\n\nS2. The proposed framework is easy to follow and logically extends prior test-time scaling works toward a self-contained correction mechanism.\n\nS3. Empirical results show outperformance than SFT (best-of-N) and previous method (Reflect-DiT), showing the potential of the proposed approach."}, "weaknesses": {"value": "W1. Limited novelty and implicit capability for self-correction. Although the framework aims to incorporate reasoning capability for self-reasoning, the trained model cannot explicitly reason about the erroneous regions but implicitly understand the errors during image-to-image generation. The proposed architecture is a well-known trick to extend the attention layer of image generation models such as IP-Adapter or ControlNet.\n\nW2. The self-correction capacity is limited in a single-step correction. As a test-time scaling approach, it’s natural to expect to replace best-of-N with a self-correction method. However, this paper still uses best-of-N, while adding one-step image correction to boost the performance. Also, the proposed method lacks multiple-step image correction to conduct test-time scaling only by this self-correction method.\n\nW3. Lack of in-depth analysis. Although the proposed SCoT outperforms best-of-N, there is no in-depth analysis to understand how the proposed method works. Please refer to the questions below.\n\nW4. Literature review can be conducted more. For example, in masked token modeling of image generation, there are some famous approaches to add self-correction of generated images to improve the performance [NewRef-1, NewRef-2].\n\n[NewRef-1] Discrete Predictor-Corrector Diffusion Models for Image Synthesis (Lezama et. al., ICLR’23).\n[NewRef-2] Draft-and-Revise: Effective Image Generation with Contextual RQ-Transformer (Lee et. al., NeurIPS’22)."}, "questions": {"value": "In addition to the weaknesses above, I think resolving my concerns below can significantly help improve this paper.\n\nQ1. Why is self-correction conducted only in a single step? What if the model conducts self-correction multiple times? I first thought that the proposed method replaces the best-of-N method with self-correction for effective test-time scaling, but the experiments still use the best-of-N method. Cannot the self-correction method replace the best-of-N?\n\nQ2. In Table 1, how is the best-of-N method conducted? Which reward function is used to select the best sample for each category? \n\nQ3. Can the authors provide more evaluation measures? Especially, if GenEval score is used for best-of-N, adding a new evaluation metric will help comparing generalization performance, considering the evaluation of image generation always requires a multi-folded approach.\n\nQ4. In Table 1, considering that the best-of-20 actually conducts 40 inferences, comparing SFT (best-of-20) with SCoT (best-of-10) is fair. In addition, comparing the performance with various N in best-of-N can also help understanding the efficacy of the proposed self-correction method.\n\nQ5. I’m wondering about the performance of the simple combination of GPT-4o and Flux 1 Kontext, which are used for training data generation. Can the fine-tuned SANA outperform the training data generation framework?\n\nQ6. Can more benchmarks be used for performance comparison? For example, T2I-CompBench can also be used for comparing the performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GRupgl78s6", "forum": "9WFnOiypxF", "replyto": "9WFnOiypxF", "signatures": ["ICLR.cc/2026/Conference/Submission7821/Reviewer_581J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7821/Reviewer_581J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146215346, "cdate": 1762146215346, "tmdate": 1762919866217, "mdate": 1762919866217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}