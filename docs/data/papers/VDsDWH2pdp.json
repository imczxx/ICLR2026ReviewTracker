{"id": "VDsDWH2pdp", "number": 9452, "cdate": 1758123032517, "mdate": 1763275781942, "content": {"title": "PLP-NER: Point-Line-Plane Context Fusion for Named Entity Recognition", "abstract": "Current state-of-the-art Named Entity Recognition systems commonly leverage an architecture that integrates BERT with Conditional Random Fields. Nevertheless, BERT is inherently constrained in capturing comprehensive global contextual semantics due to its Masked Language Modeling pre-training objective. To address this limitation, A novel “point–line–plane” contextual fusion framework is proposed. Within this paradigem, the [CLS] token functions as a “plane” that provides a compressed global representation, while the attention weights between the [CLS] token and individual tokens form a “line”, which captures semantic topological relationships. These multi-grained features are subsequently incorporated into token representations via a Graph Neural Network, considerably enriching their contextual expressiveness. Furthermore, we introduce a Dynamic Linear-Chain CRF that adaptively models label transitions using attention-mechanized probability estimates, thereby overcoming the inflexibility of conventional CRFs. Extensive experiments on multiple benchmark datasets demonstrate that our approach consistently and significantly surpasses competitive baselines, achieving a notable 3.91 point gain in F1-score.", "tldr": "", "keywords": ["NER;POINT-LINE-PLANE;Dynamic Linear Chain CRF"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3259740380c8d0fc9cb774ef6b2ae6752c37290b.pdf", "supplementary_material": "/attachment/0689163730fd221c7c106b1a035dd40ade29d638.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PLP-NER, an extension of the standard BERT-CRF framework for named entity recognition.\nThe main idea is to enhance contextual representations through a point–line–plane fusion mechanism, where token embeddings (“points”), [CLS]–token attention weights (“lines”), and the [CLS] representation (“plane”) are integrated via a multi-layer perceptron.\nA second contribution is a Dynamic Linear-Chain CRF, which adjusts label transition probabilities based on input-specific attention features.\nIn addition, a masking-based auxiliary task is incorporated to improve model robustness and generalization.\nExperiments on four datasets (CoNLL-2003, WNUT-2017, MSRA, and CLUENER) show moderate F1-score improvements over a BERT+CRF baseline. However, the empirical analysis, comparison breadth, and interpretability of the results remain limited."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses known limitations of BERT+CRF models in global semantic fusion and static transition modeling through a well-defined architectural extension.\n* The proposed dynamic transition mechanism is conceptually reasonable and could potentially capture input-dependent label correlations.\n* The modular design (PLP fusion + dynamic CRF) is clear and may be transferable to other sequence labeling tasks."}, "weaknesses": {"value": "### 1. Writing and Presentation Issues\n\n* The paper appears hastily prepared and insufficiently polished.\n\n  * The final two paragraphs of the Introduction are redundant and lack clear logical progression.\n  * Several abbreviations are undefined (e.g., NSP, PLPA, NE, DY).\n  * Figures and tables are included but not well integrated or discussed, reducing clarity.\n  * The overall structure and formatting show inconsistencies, suggesting limited proofreading.\n\n### 2. Technical and Methodological Limitations\n\n* The proposed Point–Line–Plane (PLP) fusion mainly combines existing BERT representations ([CLS], token embeddings, and attention weights) through simple concatenation and MLPs.\n  While the geometric analogy is intuitive, the method is technically incremental, resembling prior work on attention pooling, [CLS]-based aggregation, and graph-enhanced contextual modeling for NER.\n* The Dynamic Linear-Chain CRF is not clearly differentiated from previous Neural or Adaptive CRF variants that also incorporate input-dependent transitions.\n  The paper lacks analysis of model stability, computational cost, or parameter sensitivity.\n* Key implementation details—such as MLP layer count, hidden dimensionality, activation functions, and clipping thresholds—are missing, making the work difficult to reproduce and evaluate fairly.\n\n### 3. Empirical Weaknesses\n\n* The fine-grained NER results (e.g., on CLUENER) are under-analyzed. The paper claims significant improvements but provides no qualitative examples, case studies, or visualizations to explain the model’s advantages on fine-grained entity boundaries.\n* The ablation design is questionable: experiments follow an additive scheme (+MASK → +NE → +DY) rather than removing components individually, making it unclear which component contributes most.\n* The baseline comparisons rely on relatively outdated models (mostly 2019–2021), and the reference performance for WNUT-2017 and CLUENER is sparse, limiting the interpretability of reported gains.\n* There is no statistical analysis (e.g., variance, confidence intervals, or significance tests) to verify that the observed F1 improvements are robust.\n\n### 4. Insufficient Literature Review\n\nThe Related Work section lacks sufficient coverage of two major research directions:\n\n1. Contextual fusion and representation aggregation using [CLS] embeddings, attention pooling, or graph-based architectures for NER;\n2. Neural or adaptive CRF approaches that already incorporate dynamic or learnable transition potentials.\n   These omissions make it difficult to position PLP-NER relative to existing research and to assess its genuine novelty."}, "questions": {"value": "See the Weaknesses section for detailed methodological and empirical concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aB2xTb8NjV", "forum": "VDsDWH2pdp", "replyto": "VDsDWH2pdp", "signatures": ["ICLR.cc/2026/Conference/Submission9452/Reviewer_ib8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9452/Reviewer_ib8q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909306529, "cdate": 1761909306529, "tmdate": 1762921048264, "mdate": 1762921048264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "kt0QB2pqJe", "forum": "VDsDWH2pdp", "replyto": "VDsDWH2pdp", "signatures": ["ICLR.cc/2026/Conference/Submission9452/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9452/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763275781005, "cdate": 1763275781005, "tmdate": 1763275781005, "mdate": 1763275781005, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PLP-NER, a BERT-CRF–based NER model that fuses multi-granularity context via a “Point–Line–Plane” scheme: token embeddings (point), [CLS]→token attention (line), and a global [CLS] vector (plane). These signals are injected into the emission scores through lightweight fusion; a Dynamic Linear-Chain CRF further adjusts transition scores at instance level using attention-derived cues (with coarse in/boundary/out categories). An auxiliary MLM loss is used during fine-tuning to mitigate representation drift. Experiments on CoNLL-2003, WNUT-2017, MSRA, and CLUENER report improvements over a BERT-CRF baseline; the paper highlights a “+3.91 F1” gain. The work aims to enhance global/structure awareness within a standard supervised NER pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses limited global/structural awareness of PLMs for NER and the rigidity of a static CRF transition matrix.\n\n2. the PLP fusion and dynamic CRF are easy to add to a common BERT-CRF stack and appear to give consistent gains on several datasets."}, "weaknesses": {"value": "1. In current era, I’d expect zero/few-shot LLM NER or LLM-augmented baselines. Without that, the value of a small BERT-CRF variant isn’t convincing.\n\n2. The “avg +3.91 F1” looks like a single-dataset bump; please show the exact averaging math or revise the claim.\n\n3. Missing significance/key stats. No multi-seed runs, std/CI, or significance tests—hard to judge robustness.\n4. Important training details (seeds, hyperparams, compute/time/memory), code/checkpoints are missing; hard to reproduce."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t3LsaGAOej", "forum": "VDsDWH2pdp", "replyto": "VDsDWH2pdp", "signatures": ["ICLR.cc/2026/Conference/Submission9452/Reviewer_sP9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9452/Reviewer_sP9z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929553784, "cdate": 1761929553784, "tmdate": 1762921047866, "mdate": 1762921047866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PLP-NER. A “point–line– plane” contextual fusion framework is proposed.  It introduces a Dynamic Linear-Chain CRF that adaptively models label transitions using attention-mechanized probability estimates."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper’s claimed innovations lack substantive novelty; all components are combinations of existing techniques, and their necessity has not been rigorously validated."}, "weaknesses": {"value": "1.\tLack of statistical significance testing: The reported F1 improvements are modest in some cases. Without significance tests, it’s unclear whether gains are reliable, especially on smaller datasets like WNUT17.\n2.\tLimited comparison to recent SOTA: The “SOTA” row in Table 3 includes methods from 2020–2021, but omits more recent works. The claim of “surpassing competitive baselines” would be stronger with up-to-date comparisons.\n3.\tOverstated novelty of “GNN” interpretation: The paper describes the PLP fusion as a “simplified GNN,” but no actual message-passing or graph propagation occurs.\n4.\tInconsistent and Confusing Nomenclature: The method is introduced as PLP-NER throughout the abstract, introduction, and methodology, but suddenly referred to as “PLPA” in Section 4.3\n5.\tPoorly Constructed Tables: Table 3 is disorganized and lacks clear column alignment.\n6.\tThe text in paper contains redundant and circular claims.\n7.\tUninformative and Misleading Figures:Figure 1 is a generic block diagram with no architectural detail.Figure 2 claims to be a “geometric schematic” but merely overlays geometric terms on a Transformer diagram without formalizing the analogy."}, "questions": {"value": "8.\tGNN claim clarification: The paper states the PLP framework “can be viewed as a simplified Graph Neural Network.” Could the authors clarify what graph structure is assumed and how message passing is implemented? If no inter-token messages are exchanged, is the GNN analogy purely conceptual?\n9.\tStatistical significance: Were the reported F1 scores averaged over multiple random seeds?\n10.\tAttention source specification: Which attention weights are used in the “semantic lines”? Are they from the last BERT layer? Averaged over heads?\n11.\tGeneralization beyond BERT: Has the PLP framework been tested with other PLMs (e.g., RoBERTa, DeBERTa)? If the gains are consistent, it would strengthen the claim that the method addresses a general limitation of PLM-based NER, not just BERT-specific artifacts.\n12.\tDynamic CRF overfitting analysis: For WNUT17, the Dynamic CRF (+DY) lags behind +NE. Did the authors observe overfitting during training"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bi6qQ7B9PY", "forum": "VDsDWH2pdp", "replyto": "VDsDWH2pdp", "signatures": ["ICLR.cc/2026/Conference/Submission9452/Reviewer_7g7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9452/Reviewer_7g7f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970336033, "cdate": 1761970336033, "tmdate": 1762921047475, "mdate": 1762921047475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper **“PLP-NER: Point-Line-Plane Fusion for Named Entity Recognition with BERT”** introduces a novel framework enhancing BERT-CRF models through geometric contextual fusion and adaptive decoding. By treating tokens as “points,” attention relations as “lines,” and the [CLS] token as a “plane,” the model fuses multi-level semantics via a graph-based mechanism, while a **Dynamic Linear-Chain CRF** adapts label transitions contextually. A masking-based auxiliary loss further mitigates pretraining–finetuning divergence. Experiments on four NER benchmarks show consistent gains—up to **+3.9 F1 points**—especially in fine-grained recognition. Overall, it’s a well-motivated and methodologically solid contribution, though computational efficiency analysis is limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* **Clear Motivation:** The paper clearly identifies key limitations in existing BERT-CRF NER systems—namely weak global contextualization and rigid decoding—and justifies the need for multi-level semantic fusion and dynamic transitions.\n* **Novel Structure:** The proposed **Point–Line–Plane fusion** is an elegant and interpretable geometric abstraction that effectively integrates local, relational, and global information, while the **Dynamic Linear-Chain CRF** introduces adaptive, context-aware label transitions.\n* **Sound Experimental Results:** Comprehensive evaluations across four diverse benchmarks (English and Chinese, general and domain-specific) demonstrate consistent and meaningful performance gains, including a notable +3.9 F1 improvement, supported by detailed ablations verifying each component’s contribution."}, "weaknesses": {"value": "* **Limited Comparison Scope:** While the paper thoroughly benchmarks against traditional BERT-CRF and recent discriminative NER models, it omits comparisons with **decoder-only large language models (LLMs)** such as GPT/Qwen generative NER frameworks etc. Given the growing relevance of generative and instruction-tuned models for sequence labeling, such comparisons would strengthen the claim of state-of-the-art performance and clarify the advantages of the proposed fusion mechanism under modern NER paradigms."}, "questions": {"value": "1. why not compare the proposed model's performance with decoder-only language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "whuOFbozEZ", "forum": "VDsDWH2pdp", "replyto": "VDsDWH2pdp", "signatures": ["ICLR.cc/2026/Conference/Submission9452/Reviewer_d7qa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9452/Reviewer_d7qa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036386330, "cdate": 1762036386330, "tmdate": 1762921047117, "mdate": 1762921047117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}