{"id": "XjcHRIu0iF", "number": 7925, "cdate": 1758043524012, "mdate": 1759897822143, "content": {"title": "Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing", "abstract": "Masked diffusion models (MDMs) offer a compelling alternative to autoregres-\nsive models (ARMs) for discrete text generation because they enable parallel\ntoken sampling, rather than sequential, left-to-right generation. This means po-\ntentially much faster inference. However, effective parallel sampling faces two\ncompeting requirements: (i) simultaneously updated tokens must be conditionally\nindependent, and (ii) updates should prioritise high-confidence predictions. These\ngoals conflict because high-confidence predictions often cluster and depend on\neach other, opportunities for parallel updates.\n\nWe present PUNT, a model-agnostic sampler that reconciles this trade-off. Our\nmethod identifies token dependencies and removes lower-confidence tokens from\nconflicting groups. This produces sets of indices for unmasking that satisfy both\nindependence and confidence criteria. Our approach ensures improved parallel\nunmasking through approximate conditional independence testing.\n\nOur experiments show that PUNT delivers a superior trade-off between accuracy\nand compute when compared to other strong training-free baselines, especially for\ngeneration of longer sequences. On the IFEval benchmark, it achieves up to 16%\nhigher accuracy over baseline methods, including sequential generation (one-by-\none). These gains hold across different values of hyperparameters, mitigating the\nneed for brittle hyperparameter tuning. Moreover, we observe that PUNT induces\nan emergent hierarchical generation strategy, where the model first establishes\nhigh-level paragraph structure before local refinement, suggesting a planning-like\ngeneration process that contributes to strong alignment performance.", "tldr": "The paper introduces a training-free sampling algorithm for masked diffusion models that resolves conflicts among proposed tokens by deleting lower-confidence candidates", "keywords": ["masked diffusion models", "language models", "inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea6ec4f1e84181ac99246b7c6c390ac409469067.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PUNT (Parallel Unmasking with Non-influence Tests), a training-free sampling algorithm for Masked Diffusion Models (MDMs) that enables efficient parallel token generation while maintaining output quality. The key idea is to select subsets of masked tokens that are approximately conditionally independent, thereby allowing them to be decoded simultaneously without introducing dependency errors. PUNT implements a divide-and-conquer procedure that identifies such token sets using O(log |M|) model evaluations per iteration, relying on a contextual independence test based on KL divergence between conditional distributions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is built on a very clear and reasonable motivation. It correctly identifies the fundamental conflict in parallel MDM sampling: the desire to unmask high-confidence tokens versus the necessity of conditional independence between simultaneously updated tokens. PUNT's approach of directly testing for and mitigating this \"inter-token interference\" is a sound and principled alternative to purely confidence-based heuristics.\n2. PUNT demonstrates a clear Pareto improvement on long-sequence and instruction-following tasks like IFEval. The observation of an emergent hierarchical generation strategy (Fig. 2) is a significant and interesting finding.\n3. The connection between independence stability and attention sparsity offers an intuitive, architecture-aware justification of the method’s assumptions."}, "weaknesses": {"value": "1. Experimental validation is limited. The current experiments mainly demonstrate advantages on two benchmarks (IFEval and MT-Bench), while the improvement on MT-Bench over the Dilated Sampler is marginal. On short-answer tasks (§4.2), PUNT shows no clear advantage—likely due to higher per-step complexity. The evaluation lacks additional baselines such as APD (Adaptive Parallel Decoding, arXiv:2506.00413) or other few-step DLM planners that could better contextualize PUNT’s trade-offs.\n2. Definition 3.2 relies on a single, fixed sequential order ($X^i$ given $X_{<i}$). The algorithm implements this order by sorting all masked tokens $M$ based on their initial confidence (at the start of the step). This static, confidence-based ordering may be suboptimal, as token confidences are likely to change once other tokens are (tentatively) revealed.\n3. The actual implementation (Sec 3.3, Alg. 1) appears to use a much stricter test than the sequential one defined in Definition 3.2. It performs a batched test, checking all tokens in the \"test\" set $S_1$ for dependence on the entire \"anchor\" set $S_0$. This is stricter than Eq. 2, which only tests a token $r_i$ against preceding tokens $R_{<i}$(31). Does this simplification, made for the sake of parallel computation, lead to over-pruning?"}, "questions": {"value": "1. The paper proposes $\\epsilon$ as a fixed hyperparameter. Have the authors explored using a dynamic $\\epsilon$? For instance, a schedule that starts with a small, strict $\\epsilon$ (to establish the high-level structure, and then increases $\\epsilon$ in later steps to be more aggressive and rapidly fill in local details? \n2. How sensitive is PUNT’s performance to the confidence ordering strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nQyWn4E2HI", "forum": "XjcHRIu0iF", "replyto": "XjcHRIu0iF", "signatures": ["ICLR.cc/2026/Conference/Submission7925/Reviewer_cxUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7925/Reviewer_cxUA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742473428, "cdate": 1761742473428, "tmdate": 1762919948034, "mdate": 1762919948034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PUNT, a model-agnostic sampler that reconciles the trade-off between speed and quality in MDMs by using approximate conditional independence testing to identify and resolve token dependencies. PUNT delivers a superior trade-off between accuracy and compute, especially for longer sequences, without requiring brittle hyperparameter tuning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- PUNT is efficient, training-free and dynamically adapts to sequence-specific dependencies.\n- PUNT induces an emergent hierarchical generation strategy, suggesting a planning-like process that contributes to its strong performance."}, "weaknesses": {"value": "- The \"independence stability\" assumption, which PUNT relies on, is a strong approximation but it is not proven.\n- The claim of mitigating \"brittle hyperparameter tuning\" is insufficiently supported as no sensitivity analysis is provided for $\\epsilon$.\n- The method underperforms on short-answer tasks where the computational overhead of multiple forward passes per step is not amortized."}, "questions": {"value": "How was the hyperparameter $\\epsilon$ selected? How is the method sensitive to the hyperparameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ry9H73jbon", "forum": "XjcHRIu0iF", "replyto": "XjcHRIu0iF", "signatures": ["ICLR.cc/2026/Conference/Submission7925/Reviewer_vZYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7925/Reviewer_vZYQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876161969, "cdate": 1761876161969, "tmdate": 1762919947518, "mdate": 1762919947518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PUNT, a new model-agnostic sampler applied in Masked diffusion models. By making the use of a proposed Contextual Independence Assumption and the corresponding recursion algorithm,  PUNT enables to select the postions from masked positions set M that relatively independent of all |-M| umasked positions for decoding, in terms of a time complexity O(log |M|). By taking contextual independence into consideration, PUNT can efficiently decode multiple tokens in parallel, while keeps its decoding accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper designing PUNT by skillfully utilizing a contextual independence assumption and a recursion algorithm, merging the tests of subcases of one recursion level into one evaluation, which is training-free and efficient. \nThere is a point mentioned in section 3.4 that  “assumption3.3 is a direct consequence of the Transformer architecture’s attention mechansim. If the attention from position i to position j is zero, then position j has no direct influence on the representation at position i.” This point of view shows the relationship between sparse attention and the testing of contextual independence. Perhaps this can serve as an inspiration to utilize independent testing to identify sparse locations."}, "weaknesses": {"value": "I noticed that in the divide-and-conquer process, some tokens are considered as ‘dependent on the anchor’  in a previous evaluation, but soon be considered as ‘independent’ in the next evaluation (such as the token ‘mince’ in Figure 1,left). It seems that only tokens tested to be dependent on the anchors of all evaluations are considered as ‘dependent’ and absent from the current generation, which means tokens in parallel may also be dependent on each other. It seems that PUNT is less capable of separating the dependency on tokens than DILATED when NFE < 100 (Figure 3 left). \n\nAs shown Figure 4, it seems that PUNT fails to exceed DILATED at NFE 400 on MT-Bench, but  there is a lack of sufficient explanation for this phenomenon."}, "questions": {"value": "As shown in Figure 1, why to take the tokens  that are always dependent on anchors (such as ‘egg’) among all tests, rather than tokens that rely on anchors at least once among all tests (such as ‘mince’), as the final ‘rejected’?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xtdyNinSc3", "forum": "XjcHRIu0iF", "replyto": "XjcHRIu0iF", "signatures": ["ICLR.cc/2026/Conference/Submission7925/Reviewer_RJ3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7925/Reviewer_RJ3J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882407706, "cdate": 1761882407706, "tmdate": 1762919947184, "mdate": 1762919947184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the conflict between speed and quality in Masked Diffusion Models (MDMs). The authors propose **PUNT**, a **training-free sampler** that enables efficient parallel decoding by **explicitly testing for token dependencies**. PUNT identifies and prunes tokens that are not conditionally independent, using a recursive $O(\\log|M|)$ **algorithm**. Experiments show PUNT achieves a **state-of-the-art accuracy-compute trade-off**, especially on long-form generation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.  **Novel and Effective Algorithm:** PUNT is an **elegant and efficient** $O(\\log|M|)$ **solution** to a well-defined problem (confidence vs. independence in parallel sampling). The use of explicit independence testing is a strong contribution.\n\n2.  **Strong Empirical Results:** The method **clearly outperforms strong baselines** on the accuracy-compute Pareto frontier for relevant long-sequence benchmarks (IFEval, MT-Bench).\n\n3.  **Strong Theoretical Justification:** The method is well-grounded with **strong theoretical justification**, particularly by connecting its \"Independence Stability\" assumption to the properties of Transformer attention. The discovery of an **emergent coarse-to-fine generation strategy** is also a valuable insight."}, "weaknesses": {"value": "1.  **Limited Scope:** The method's advantages are **diminished on short-sequence tasks** (GSM8K, MBPP), where its $O(\\log|M|)$ NFE-per-step overhead is less efficient than simpler samplers.\n\n2.  **Critical Hyperparameter Sensitivity:** The algorithm's effectiveness hinges entirely on the **KL divergence threshold** $\\epsilon$, which is not a simple-to-tune parameter but a **fundamental trade-off between speed and quality**. For a task with highly dependent tokens (e.g., code generation), most tokens will have a high $D_{KL}$. The user is forced into an impossible choice:\n    - Set $\\epsilon$ **low** for *quality*: This respects the dependencies, but will prune nearly all tokens, **collapsing the sampler's speed** to be sequential.\n    - Set $\\epsilon$ **high** for *speed*: This ignores the dependencies to unmask more tokens, but will **destroy generation quality** by violating the independence assumption.\n    This makes $\\epsilon$ a critical, task-specific parameter that requires a costly sweep for any new model or domain.\n\nI also think that this parameter is not well ablated in the paper\n\n3.  **Unclear Baseline:** The abstract's claim of outperforming \"sequential generation\" is confusing, as the `TOPK` baseline in the plots performs poorly, suggesting it's **not a true one-token-at-a-time sequential baseline**.\n\n4.  **Misleading Efficiency Metric:** The paper presents plots against **\"Denoising steps\"** alongside **NFE**. This \"step\" metric is misleading, as a single PUNT step is $O(\\log|M|)$ more expensive than a baseline step. **NFE is the only meaningful measure of compute**, and the focus on \"steps\" can obscure the true cost."}, "questions": {"value": "I wonder if similar results could be explored in image generation pipelines like MaskGIT"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0YpY7J6DKX", "forum": "XjcHRIu0iF", "replyto": "XjcHRIu0iF", "signatures": ["ICLR.cc/2026/Conference/Submission7925/Reviewer_Z7Ub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7925/Reviewer_Z7Ub"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961033956, "cdate": 1761961033956, "tmdate": 1762919946656, "mdate": 1762919946656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}