{"id": "PLN5Ku7kxa", "number": 1962, "cdate": 1756972051982, "mdate": 1759898176327, "content": {"title": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale", "abstract": "Electrophysiological signals such as EEG and iEEG are central to neuroscience, brain–computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of multimodal electrophysiology foundation models trained on the largest and most diverse corpus to date—59k hours (1.6M channel-hours) from over 17k subjects—and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts both with language/vision scaling trends and with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 sets new state-of-the-art or competitive results across diverse neural decoding tasks and establishes concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.", "tldr": "", "keywords": ["neuroscience", "EEG", "Foundation Model", "self-supervised learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adb31137b08e3a7a61b9388f6c05c1ca89d3d0d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DIVER-1, a large-scale family of electrophysiology foundation models (EFMs) designed for EEG and iEEG data. The authors conduct the first systematic scaling law analysis in this domain, extending scaling dimensions beyond model, data, and compute to include epoch and subject diversity. DIVER-1 models (up to 1.82B parameters) are pretrained on 59k hours of neural recordings and evaluated on the NeuroProbe (iEEG) and FACED (EEG) benchmarks. The results show data-constrained scaling behaviors distinct from NLP/Vision domains, with smaller models trained longer outperforming larger models trained briefly. The authors propose practical guidelines for compute-efficient EFM scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper conducts the most extensive scaling study to date for electrophysiological models, spanning model/data/compute/subject axes and revealing data-constrained scaling laws that challenge prior assumptions borrowed from NLP/Vision.\n- The authors rigorously fit their empirical trends to data-constrained scaling law formulations, providing quantitative evidence that electrophysiology follows different scaling constants and exponents compared to language models.\n- The paper introduces several domain-specific innovations, any-variate attention, STCPE, and multi-domain reconstructio, which together improve multimodal integration and robustness to electrode heterogeneity."}, "weaknesses": {"value": "- Despite impressive pretraining coverage, the downstream evaluation is narrow. For EEG, only FACED (emotion recognition) is used; other essential EEG-based BCI or cognitive decoding benchmarks (e.g., motor imagery, sleep, attention) are missing, which weakens generalization claims.\n- Most validation relies on binary NeuroProbe classification tasks, some of which the authors acknowledge as trivial (e.g., frame brightness, flow angle). These tasks may not sufficiently challenge the models to demonstrate meaningful representation quality, limiting the impact of reported gains.\n- The model integrates multiple complex modules (any-variate attention, register tokens, multi-domain reconstruction, STCPE), yet no clear ablation quantifies each module's contribution. Likewise, the effect of loss weighting parameters $\\lambda_1, \\lambda_2, \\lambda_3$ is unexplored.\n- DIVER-1 underperforms existing models (e.g., CBraMod) on FACED, even with much larger data and model size. The explanations (\"insufficient epochs\", \"dataset mismatch\") are speculative and would benefit from diagnostic analysis."}, "questions": {"value": "- The notation for $\\mathcal{L}_{\\text{total}}$ (pretraining loss) and its weighting $\\lambda_i$ is given, but no ablation is provided. How sensitive is model performance to the choice of $\\lambda_1, \\lambda_2, \\lambda_3$? Could you include an ablation or rationale for these settings (either in-text or appendix)?\n- Given the relatively weak EEG results, have you analyzed potential causes beyond epoch count or data mismatc such as QAQC differences, patch length, or fine-tuning hyperparameters? It would be valuable to report comparisons across multiple EEG datasets.\n- Will the pretrained weights, scaling analysis code, and data preprocessing pipeline be made public to enable replication and extension by the community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DKC7QPNcGP", "forum": "PLN5Ku7kxa", "replyto": "PLN5Ku7kxa", "signatures": ["ICLR.cc/2026/Conference/Submission1962/Reviewer_6kz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1962/Reviewer_6kz9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558000754, "cdate": 1761558000754, "tmdate": 1762915977532, "mdate": 1762915977532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DIVER-1, a family of multimodal foundational models for both EEG and iEEG, training by multitask SSL objective across both modalities. The authors demonstrate that DIVER-1 can be pretrained using a large collection of diverse, across modality data with distinct electrode configurations, yielding competitive downstream performance on new iEEG data. The authors also conduct extensive scaling law analysis on the proposed method, showing that it follows data constrained scaling law of language models, and this finding is valuable for the community. However, the evaluated iEEG task is relatively simple and the finetuning performance on EEG data is consistently worse than the baseline. This suggests that more investigation is likely needed on the proposed model. Hence I don’t recommend acceptance in my rating."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- DIVER-1 is a multimodal model that can take both EEG and iEEG data which have distinct electrode configurations. Being able to incorporate heterogeneous data is essential for a foundational model of neural data.\n- Several architectural innovations are proposed in the DIVER-1 model, including spatial-temporal conditional positional embedding, spatial temporal register tokens and multitask output for SSL training.\n- A scaling analysis revealing that model follows existing data constrained scaling laws."}, "weaknesses": {"value": "- One of the major conclusions is to advocate smaller model training for more epochs, compared to larger model training for limited amount of epochs. There seems to be an assumption on a limited compute budget. But I don’t understand why that should be a pretext since the goal is to train large-scale foundational models, which is known to be compute-intensive.\n- The evaluations on the new datasets are limited. There is one iEEG dataset whose tasks are all binary classification, which is acknowledged by the authors that it can be too simplistic. And another EEG dataset where baseline methods are consistently outperforming the proposed model. Hence they are not strong evidence to support the authors' claim."}, "questions": {"value": "1. The authors mention that larger models initially underperform smaller ones (line 332 page 7). But In Figure 2 (d), except for the biggest one (XXL), increasing parameter sizes did result in better test performance at epoch 2 (e.g. XL one is the best of all). Can authors comment on the poor performance of XXL?\n2. In the same plot (Figure 2 (d)), DIVER XL has promising testing results with respect to number of epochs, and it can already outperform smaller model (DIVER Tiny) at epochs 4. Isn’t that a sign that larger models can perform well at shorter training time? And why didn’t that model train for longer epochs?\n3. Compared to the plot in the original data-contrained scaling law paper, the empirical isoLoss contour in Figure 2 (j) is much less smooth and irregular. Could authors provide insights on what could lead to the difference?\n4. Could authors elaborate on how to understand Figure 2 (k)? For example, it seems to assume LaBram was trained from 8-10 epochs and BrainBert is less than 1 epoch, whereas it was 50 total epochs listed in LaBram paper and 500k steps in BrainBert paper. Also, it looks like DIVER-1 model (the white star) is not on the compute-optimal frontier either?\n5. Could authors provide details on how STCPE operates: from Figure 1 there seems to be multiple sliding windows involved in the computation, but there is no mentioning of that in section 2.2.\n6. Two patch sizes are adopted on the input time series: 1s and 100ms. Will the use of a small patch size negatively affect the capturing of lower frequency bands, which is used by existing EEG model (e.g. Brant model cited in the paper)\n7. It is surprising to me that linear probing is better than full finetuning on the iEEG task. Are there any other finetuning strategies tried for that dataset, for example, freezing only a subset of all the parameters?\n8. In the downstream iEEG dataset, there are multiple tasks with near chance performance and the author argues that it is because they are non-informative. Could authors provide more details on why that is the case?\n9. Please provide evidence on the several factors mentioned in the EEG FACED task, which leads to poor performance of DIVER-1 on it. Have authors tried other EEG dataset for evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahSK90XMqC", "forum": "PLN5Ku7kxa", "replyto": "PLN5Ku7kxa", "signatures": ["ICLR.cc/2026/Conference/Submission1962/Reviewer_UXXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1962/Reviewer_UXXE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656439558, "cdate": 1761656439558, "tmdate": 1762915977412, "mdate": 1762915977412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DIVER-1, multimodal EEG and iEEG foundation models trained at large scale, up to 59k hours and 1.82B parameters. The key claim is a data-constrained regime where training longer on limited data beats scaling parameters alone. The authors fit scaling laws, outline a compute-optimal frontier, and report strong iEEG results on NeuroProbe with competitive EEG results on FACED. Architecture choices include any-variate attention, sliding temporal conditional positional encoding, register tokens, and multi-domain masked reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The study offers the first systematic scaling analysis for electrophysiology and provides actionable guidance on allocating compute between model size and epochs. The corpus scale and subject diversity are substantial, and the any-variate attention plus STCPE address heterogeneous montages. Multi-domain reconstruction is well motivated. NeuroProbe results are strong, and the compute-optimal frontier is practically useful for planning."}, "weaknesses": {"value": "The EEG performance on FACED lags prior state of the art despite far larger pretraining, which undermines the universality of the scaling conclusions. Several comparisons are missing or limited, for example to models that use different pretext tasks or longer temporal receptive fields, which complicates claims of state of the art. Important dataset and preprocessing choices that can swing outcomes are scattered in appendices, including QAQC criteria, referencing, resampling, clipping, and filtering. The scaling-law fits are reported for pretext loss, but mapping those improvements to downstream accuracy is not always consistent, especially across modalities. Some reproducibility elements are incomplete for a study whose central result is about principled scaling."}, "questions": {"value": "To what extent do the findings hold when the downstream tasks are regression or continuous decoding rather than binary AUROC classification, especially language embedding regression. How much of the EEG performance gap is attributable to insufficient epochs versus differences in QAQC or dataset composition quality, and do the conclusions change when EEG pretraining uses multiple epochs and clipping strategies matched to iEEG. \nAdd baselines with longer contexts and alternative SSL objectives. Fit downstream scaling relations and report correlations between pretext loss and task metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EhLB4pTJxO", "forum": "PLN5Ku7kxa", "replyto": "PLN5Ku7kxa", "signatures": ["ICLR.cc/2026/Conference/Submission1962/Reviewer_i3iQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1962/Reviewer_i3iQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105182950, "cdate": 1762105182950, "tmdate": 1762915977226, "mdate": 1762915977226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DIVER-1, a family of multimodal electrophysiology (EEG and iEEG) foundation models. The training data includes both EEG and iEEG data, with most of the data coming from EEG. The architecture design introduces several key components tailored for Ephys data. The model is pretrained using a multi-domain reconstruction objective, learning to simultaneously predict masked-out patches in the time series, spectrum (FFT), and spectrogram (STFT) domains . The paper then presents a systematic analysis of this architecture's scaling properties and its performance on downstream decoding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's core strength is its investigation of scaling laws for electrophysiology.  Interesting findings include:\n\n-- Studying cost-effective training recipes in data-constrained settings, such as not spending flops on the largest models.\n\n-- Studying architecture designs, such as a multi-domain reconstruction objective (learning time, FFT, and STFT) and a novel positional encoding (STCPE) .\n\n-- The collection of a large scale Ephys pre-training dataset\n\nThe effort put into this paper was impressive."}, "weaknesses": {"value": "I have a number of concerns about this paper.  None of these concerns are deal-breakers, but they add up to a non-trivial amount of total concern.\n\n-- The majority of the experiments focused on iEEG (e.g., most of Figure 2), where including EEG data actually hurt performance (noted in Appendix D.4).  Moreover, the limited EEG experiments show that the proposed model does not outperform existing EEG foundation models.  Since the large majority of the training set is EEG, this makes the total number quite a bit less impressive.  In fact, I find the way the abstract is written a bit misleading, since the tone the abstract takes strongly implies (to me) that the entire dataset which is mostly EEG was beneficial in the experiments.  The iEEG dataset (37 subjects for 5K+ hours) is already quite large.\n\n-- I think some of the claims are not fully supported.  Notably, it's not clear how much of the performance gains are due to the dataset, versus the model design choices.  For instance, if the PopT-style approach (which factorizes per-channel embeddings vs aggregation) was trained on a larger dataset, would those perform better?  (Note that I am not suggest the authors run this experiment, as that would be too much for this paper.  But these are discussion points worth raising in limitations.)\n\n-- I think the data-constrained scaling laws finding could be presented with more nuance.  For example, if I look at Figure 2d, it looks like the XXL model (brown curve) will perform the best if allowed train on 64 epochs.  This seems to be corroborated in Figure 2a, where eventually the largest models will reach the lowest loss.  Of course, if you have both a data-constraint AND a FLOPS constraint, then the story changes and is more aligned with your claim.\n\n\nMinor: The legends in Figure 2 need to be larger"}, "questions": {"value": "No other questions, beyond asking the authors to comment on my listed weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Mp6z4SZHG", "forum": "PLN5Ku7kxa", "replyto": "PLN5Ku7kxa", "signatures": ["ICLR.cc/2026/Conference/Submission1962/Reviewer_MZwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1962/Reviewer_MZwT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247397337, "cdate": 1762247397337, "tmdate": 1762915976916, "mdate": 1762915976916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors systematically study the scaling of foundation models for electrophysiology data from macro-electrodes (EFMs), particularly iEEG and EEG. They introduce a family of models, DIVER-1, which share a fixed architecture and training regime but vary in data sources and model sizes. They then characterize scaling properties along several axes: model size, data scale, compute, number of training epochs, and number of subjects in the training data. Their analysis yields a set of scaling laws that describe EFM scaling in a data-constrained regime analogous to that observed in language models [1]. Results show state-of-the-art performance on the Neuroprobe benchmark (iEEG movie-watching) and performance comparable to prior methods on the FACED EEG benchmark for emotion classification.\n\n---\n\n[1] Muennighoff, N., Rush, A., Barak, B., Le Scao, T., Tazi, N., Piktus, A., ... & Raffel, C. A. (2023). Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 50358-50376."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(S1) I appreciate the attention to detail in this paper to subtle yet important factors such as data quality (e.g., the QA/QC pipeline), hyperparameter tuning (thorough tuning at smaller scales and use of $\\mu$-parameterization for transfer to larger models), and choices like patch size and fine-tuning strategy that can significantly affect performance.\n\n(S2) To my knowledge, this is the first work to systematically study scaling with population-level electrophysiology data. Although the scope of results is somewhat limited (discussed in (W2) below), the derived scaling laws offer valuable guidance for future work in this area.\n\n(S3) The choice to evaluate on comprehensive benchmarks such as Neuroprobe and FACED demonstrates an emphasis on thorough evaluation, and it’s encouraging that DIVER-1 performs consistently across most Neuroprobe tasks."}, "weaknesses": {"value": "(W1) A major concern is the consistency of the Neuroprobe results. In the current version of the benchmark [1], the reported numbers for the Linear (Laplacian + spectrogram) baseline on SS-SM differ from those in the manuscript. For instance, the Global Optical Flow result for the linear baseline is $<0.62$, and DIVER-1 achieves roughly $0.62$, yet the Neuroprobe paper reports $0.625$. Some task results align with the Linear (spectrogram) baseline, others with Linear (Laplacian + spectrogram). It’s possible I’ve misunderstood, but I couldn’t find a consistent basis for comparison. Since the benchmark is concurrently being developed, some variation is expected, but it should be clarified whether baseline results were re-run or taken directly from a source, and cited accordingly. As far as I know, the Linear (Laplacian + spectrogram) results are only present in the latest version of Neuroprobe, so I would expect consistency with [1]. This paper also includes 19 tasks, several of which are absent in [1]. Additionally, including tables with full results would help, as the plots alone make it difficult to gauge precise values when differences are marginal.\n\n(W2) While robust benchmarking for iEEG data is still an open problem, the current results suggest that extensive pretraining yields only marginal gains over linear baselines. In some sense, I believe the cost of pretraining is not justified by the demonstrated results and hence the pursuit of a scaling study is not fully justified as well.\n\na) Note that on some Neuroprobe tasks, careful feature extraction on linear models (e.g., Laplacian + spectrogram vs. raw voltage) achieves greater relative improvement vs the effect of scaling or the performance gain achieved by DIVER-1 over the linear baselines. This raises questions about whether the observed gains would persist against stronger baselines. While I acknowledge the lack of robust benchmarking for iEEG reflects a broader issue in the field, I do think the choice to use the binary classification tasks in Neuroprobe as a primary evaluation metric undermines the credibility of the scaling claims.\n\nb) I would suggest leaning more on EEG benchmarks, which are comparatively better developed. Notably, DIVER-1 does not achieve SOTA performance on FACED, hence I am not confident that the current evaluation setup adequately supports the premise of the scaling study. \n\n(W3) Because DIVER-1 combines novel architectural design choices and large-scale pretraining, it’s difficult to disentangle their respective effects. The only apparent comparison with a supervised variant is in Table 6 (Appendix), which is described as “fine-tuned from scratch.” Clarification is needed on whether a supervised baseline using the same architecture was directly compared. Moreover, comparisons with SSL baselines like PopT, BrainBERT, or Brant (which wasn't compared at all) are incomplete without controlling for pretraining data and scale (e.g. by comparing supervised variants).\n\n(W4) The ablations in Table 7 (Appendix) indicate that most DIVER-1 components have limited contribution--only the multi-domain reconstruction objective and STCPE embeddings seem clearly beneficial, while the other components seem to in fact hurt performance. While it’s possible other components help at larger scales, no evidence is provided.\n\n(W5) Evaluation is limited to the SS-SM setting on Neuroprobe. Since linear probing is already used, comparisons against the SS-DM and DS-DM settings (as reported in Neuroprobe) would be lightweight to derive, greatly strengthen the analysis, and clarify generalization behavior which is a key concern for a scaling study.\n\nTo me, (W1) and (W2) are especially critical for considering acceptance of this paper. I will consider raising my score if the authors are able to address these concerns effectively.\n\n---\n\n[1] Zahorodnii, A., Wang, C., Stankovits, B., Moraitaki, C., Chau, G., Barbu, A., ... & Fiete, I. R. (2025). Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli. arXiv preprint arXiv:2509.21671."}, "questions": {"value": "(Q1) There are limited details on some architectural components. How deep is the CNN? It’s mentioned that the STCPE embeddings used were modified versus previous implementations, but how exactly? Do the authors plan to release code?\n\n(Q2) Limited details on decoder. Is it MAE-style? What is used to query the output, a learnable mask token? How about spatial coordinates, timing information? Since timing information is only used in the SA layers processing unmasked patches, and the multi-output decoder head features linear projection, do the reconstruction heads not include timing information?\n\n(Q3) Why is STFT excluded from the pretraining loss for .1s patches (which seem to perform best)? How were $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ chosen? Why is FFT emphasized less for 1s patches, but more important for .1s patches?\n\n(Q4) During input resampling, is $N’ \\leq \\min(N, 30)$ fixed for the number of subsampled temporal patches, irrespective of patch size? For .1s patches would that mean it’s subsampling 10% of the patches? Also are patches resampled totally randomly? If so, doesn’t this discard substantial information? What’s the intuition for why this wouldn’t harm training?\n\n(Q5) To confirm: the models were pretrained on 30s context windows and directly inferred/finetuned on different context windows downstream (e.g. 1s for Neuroprobe, 10s for FACED)?\n\n(Q6) Performance appears highly dependent on patch size, and I would imagine this is especially true across different tasks. Could the authors analyze how patch size affects reconstruction and downstream results? I think this could be another important axis of consideration, even at smaller scales.\n\n(Q7) Since linear probing on Neuroprobe often outperforms full fine-tuning, it would be informative to report scaling effects across all downstream tasks and analyze differences across task domains--for example, whether scaling trends differ between language and auditory tasks?\n\n(Q8) For the results throughout the text where only the four tasks (speech, onset, volume, pitch) were provided, I want to confirm that these are on the Neuroprobe splits and not on previous Braintreebank splits as in from [1]?\n\n(Q9) In pretraining with AJILE12, were blocklist periods filtered out or was the full data used? If noisy segments were included, were their potential effects considered?\n\n(Q10) AJILE12 was already preprocessed before public release. Were the QAQC steps still applied to this dataset?\n\n---\n\n[1] Chau, G., Wang, C., Talukder, S., Subramaniam, V., Soedarmadji, S., Yue, Y., ... & Barbu, A. (2025). Population transformer: Learning population-level representations of neural activity. ArXiv, arXiv-2406."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JyTUwOoEHc", "forum": "PLN5Ku7kxa", "replyto": "PLN5Ku7kxa", "signatures": ["ICLR.cc/2026/Conference/Submission1962/Reviewer_5x2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1962/Reviewer_5x2y"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762484246742, "cdate": 1762484246742, "tmdate": 1762915976458, "mdate": 1762915976458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}