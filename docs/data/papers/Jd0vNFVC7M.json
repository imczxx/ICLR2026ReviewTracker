{"id": "Jd0vNFVC7M", "number": 15056, "cdate": 1758247301117, "mdate": 1759897332820, "content": {"title": "AttentionR-GCN: Incorporating Spatiotemporal Reasoning in Heterogeneous and Partially Observed Graphs", "abstract": "Urban infrastructure networks are complex systems characterized by heterogeneous nodes and edges, partial observability, and temporal dynamics, which many graph neural networks struggle to handle. We introduce AttentionR-GCN, an extension of graph attention network based on different relational types that (i) uses attention-based message aggregation to weight node and edge signals under different relation types, (ii) uses learnable embeddings to represent missing values, and (iii) incorporates a transformer encoder to model temporal dependencies. We evaluate AttentionR-GCN on two simulated water distribution networks, predicting one-step-ahead chlorine concentrations at both monitored and unmonitored nodes under varying levels of missing sensor data. Our model outperforms different baselines, especially under high data sparsity, and demonstrates superior generalization to unmonitored nodes. Our results reveal the importance of incorporating adaptive weighting of node and edge features under different relations, learnable representations for missing values, and capturing temporal dependencies to achieve more reliable predictions in partially observed infrastructure networks.", "tldr": "A graph neural network with relation-aware node and edge attention, and learnable mechanisms to handle missing values", "keywords": ["Heterogeneous Graph Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd61f9eead20629255c9e2252dc99c479c8417a4.pdf", "supplementary_material": "/attachment/320dce707a503a199653a9a4ba484d673a1f8374.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AttentionR-GCN, for spatiotemporal prediction in partially observable, dynamically evolving urban infrastructure networks. The model combines a triple-based relation-aware attention mechanism, a learnable missing value embedding, and a Transformer temporal encoder to capture long-term temporal dependencies. Extensive experiments on two simulated water network datasets demonstrate the promising performance of AttentionR-GCN."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. A unified relationship-aware attention mechanism is proposed to jointly model nodes, edges, and relationship types, which is an extension of the existing GAT and R-GCN.\n2.   The model architecture and overall pipeline are relatively easy to understand."}, "weaknesses": {"value": "1. Overall,  the paper suffers from poor presentation quality. Figures are unclear, tables summarizing experimental results are missing, and reference formatting is inconsistent. Moreover, this paper lacks the statement on the Use of LLMs.\n2. The proposed components mainly combine existing techniques. The paper lacks a clear theoretical justification or new algorithmic insight that distinguishes it from prior work.  \n3.  Experiments are conducted only on two simulated datasets, without evaluation on real-world data. The lack of ablation studies, sensitivity analyses, and implementation details, e.g., hyperparameters, computational efficiency, prevents a full assessment of the model’s robustness and generalization."}, "questions": {"value": "1. What is the computational complexity of AttentionR-GCN? Please provide the total number of trainable parameters, as well as training and inference runtimes. Such information is essential for assessing the scalability and reproducibility of the model.  \n2. The paper lacks comparisons with recent state-of-the-art edge-aware attention and temporal-GNN models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2JSNWLbkDo", "forum": "Jd0vNFVC7M", "replyto": "Jd0vNFVC7M", "signatures": ["ICLR.cc/2026/Conference/Submission15056/Reviewer_r4ka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15056/Reviewer_r4ka"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385635544, "cdate": 1761385635544, "tmdate": 1762925375854, "mdate": 1762925375854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an extension of the graph attention network based on different relation types. They use learnable embeddings to represent missing values and incorporate a transformer encoder to model temporal dependencies. They demonstrated its performance using public benchmarks and compared it with other baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors focused on the multiple relational characteristics, sparsity, and time dependence of urban spatiotemporal data and constructed a relation-aware version of GATv2."}, "weaknesses": {"value": "1.\tThe author only provided line graphs of the results of the proposed method on various data sets, but did not provide tabular statistical indicator results, which reduced the credibility of the paper.\n2.\tThe authors did not provide a description of the use of LLM.\n3.\tThe author's innovation is incremental and is a simple extension of GATv2.\n4.\tAll the formulas in the paper are not numbered and there are many typos."}, "questions": {"value": "1.\tShould the comma in $S^{,r}_{u,v}[he]$ be deleted?\n2.\tShould there be a section describing the notion and problem definition you used?\n3.\tShould the results of the ablation experiment be given in a clearer table with specific quantitative values?\n4.\tShould the specific performance of the problem under different missing rates be displayed in the form of a table?\n5.\tShould some case studies be given to analyze the various relations learned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKHNbhtWNq", "forum": "Jd0vNFVC7M", "replyto": "Jd0vNFVC7M", "signatures": ["ICLR.cc/2026/Conference/Submission15056/Reviewer_3Sxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15056/Reviewer_3Sxo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784222707, "cdate": 1761784222707, "tmdate": 1762925375430, "mdate": 1762925375430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AttentionR-GCN, a spatio-temporal GNN for heterogeneous, partially observed infrastructure graphs. The model employs relation-aware attention that jointly conditions on the self node, neighbor node, edge attributes, and relation type, utilizes learnable embeddings to handle missing values, and integrates the graph stack with a Transformer temporal encoder to capture long-range dynamics. \n\nExperiments on simulated water distribution networks (C-Town and L-Town) evaluate one-step-ahead chlorine concentration forecasting under varying mask ratios, with results showing consistent gains over R-GCN, (r)GAT/(RGAT), UniMP, and Transformer-augmented counterparts, particularly at medium to high sparsity and on unmonitored nodes. Ablations suggest most of the gains stem from the relation-aware attention; uncertainty analyses over seeds and per-sample errors indicate stable behavior."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem (heterogeneous, partially observed urban networks), concise related-work positioning, and transparent training protocol (objectives, schedules, early stopping). \n\n- Clear architectural specification (algorithmic form of logits/normalization, layer stack, figure) and controlled baselines with/without Transformer and edge features."}, "weaknesses": {"value": "- Evaluation is confined to two simulated water networks with synthetic scenarios; no real-world telemetry or cross-domain tests (e.g., power grids, traffic) are provided. This limits claims of broad applicability. \n\n- Focuses on one-step forecasting and general ML metrics (MAE/MSE/R2); there’s no multi-horizon evaluation or physics-aware constraints (e.g., mass-balance consistency, bounded chlorine kinetics), which matter in operations. \n\n- The same set of nodes is masked across all graphs at a given ratio, which can induce distributional alignment and understate difficulty versus random per-instance masking or structured outages (blocks over space/time). \n\n- While (r)GAT/R-GCN/UniMP and Transformer variants are included, influential spatio-temporal baselines like, DCRNN, STGCN, or Graph WaveNet with edge-aware adaptations aren’t reported head-to-head, despite being discussed in related work. A direct comparison would strengthen the empirical case. Other recent baselines such as GAP-LSTM are not even discussd."}, "questions": {"value": "- How does performance change with random per-graph masking, temporally contiguous gaps, or spatially clustered outages (e.g., sensor bank down)? Results with these settings would better reflect real deployments. \n\n- Have you tried multi-horizon forecasting? A short study could reveal whether the Transformer encoder retains advantages beyond one step. \n\n- Could you incorporate simple physics-guided constraints or penalty terms (e.g., non-negativity, bounded concentration, smoothing \n\n- Since attention is relation-aware, can you visualize per-relation attention maps and show case studies where specific relations or edge attributes dominate inference at masked nodes?\n\n- Many edge attributes (flow/velocity) are noisy; what is the model performance under noised or biased edge features? A stress test would clarify fragility. \n\n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MdDej8WkRA", "forum": "Jd0vNFVC7M", "replyto": "Jd0vNFVC7M", "signatures": ["ICLR.cc/2026/Conference/Submission15056/Reviewer_h5m9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15056/Reviewer_h5m9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977809745, "cdate": 1761977809745, "tmdate": 1762925374619, "mdate": 1762925374619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}