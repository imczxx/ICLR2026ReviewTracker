{"id": "zFLCNnKY8U", "number": 1455, "cdate": 1756883744735, "mdate": 1759898208254, "content": {"title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment", "abstract": "Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.", "tldr": "We propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL.", "keywords": ["Alignment", "Self-Optimized", "fine-tuning", "reinforcement learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e3fb7ae687d11e7846345cae1a0c6f045aac007.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **GRAO (Group Relative Alignment Optimization)** — a framework that unifies **Supervised Fine-Tuning (SFT)** and **Reinforcement Learning (RL)**–based alignment. It combines a **Group Relative Optimization (GRPO)**–style reward objective with an **SFT loss** to balance imitation and exploration.\n\nThe proposed loss (Eq. 3) integrates normalized advantages, group-relative weighting, and a reference term, generalizing prior methods like **DPO** and **GRPO**.  \nTheoretical analysis claims **convergence guarantees** under standard assumptions, and experiments on *helpful* and *harmless* alignment benchmarks show improvements over SFT, PPO, DPO, and GRPO.\n\n**Claims:** GRAO unifies SFT and RLHF under one framework, improves **sample efficiency**, provides **theoretical convergence guarantees**, and claims enhanced **reasoning ability**."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "No Strengths"}, "weaknesses": {"value": "#### **1. Incomplete and Ambiguous Objective Formulation**\n- **Equation (2)** (Line 215) defines an expectation but omits the inner function $f(X)$, making the expression incomplete.  \n- **Equation (3)** (Line 219) references advantage terms $\\hat{A}_{o_i}$ and $\\hat{A}_y$ without defining the reward function $R(o_i, y)$ (Line 250).  \n- Several variables remain undefined throughout the paper:\n  - $o_i$ in Eq. (5)  \n  - $o_{\\text{pre}, i}$, $o_{\\text{post}, i}$ in Eq. (6)  \n- These omissions make it difficult interpret the proposed optimization algorithm.\n\n#### **2. Lack of Optimization Method Description**\n- Section 3.2 presents the GRAO objective but **does not specify how it is optimized**.\n- The link between the theoretical convergence proof and the actual optimization implementation is unclear.\n\n#### **3. Metrics and Evaluation Ambiguity**\n- The proposed metrics (**RAS** and **NAG**) in Section 4.1 are insufficiently defined.  \n- **RAS** relies on comparing model and reference rewards using  $R(o_{i}, y)$, and there is no clear definition of this Reward Computation\n- Similarly, **NAG** depends on $R(o_{\\text{pre}, i}, y_{\\text{ref}, i})$ and $R(o_{\\text{post}, i}, y_{\\text{ref}, i})$, yet these reward computations are **not defined anywhere**.  \n\n#### **4. Conceptual Vagueness and Poor Clarity**\n- The phrase **“unified alignment problem”** (Line 041) is introduced without definition.  \n- **“The recently released model”** (Line 044) is ambiguous — unclear which model is being referenced.  \n- Statements such as *“it has the ability of quite complex reasoning tasks”* (Line 048) are grammatically incorrect and scientifically vague.  \n- The conceptual novelty over **GRPO** and **SFT** appears incremental.\n\n#### **6. Limited Evaluation Scope**\n- Despite strong claims about “complex reasoning” improvement, **no reasoning datasets** are used — only *helpful* and *harmless* alignment benchmarks.  \n- The experiments therefore do not support the claimed advances in reasoning or generalization."}, "questions": {"value": "#### **1. Clarification of the Optimization Technique**\n- The paper presents the GRAO objective in Eq. (3), but the optimization procedure remains unclear.  \n  - Please specify the exact optimization technique used for minimizing this objective.  \n\n#### **2. Missing Definition of Reward and Trajectory Variables**\n- The core reward term $R(o_i, y)$ is central to computing normalized advantages  $\\hat{A}_i = \\frac{R(o_i, y) - \\mu_r}{\\sigma_r},$  yet the paper never defines how $R(o_i, y)$ is obtained.  \n  - In Eq. (5), the definition of $R(o_i, y_{\\text{ref}, i})$ is missing.  \n  - In Eq. (6), the quantities $o_{\\text{pre}, i}$ and $o_{\\text{post}, i}$ are introduced without definition — please clarify what these represent (e.g., pre- and post-update trajectories).  \n  - Similarly, $o_i$ (Eq. 5) is used but never formally defined.\n\n#### **3. Reward Statistics (Mean, Variance, Quartiles)**  \n  - Please report the **mean**, **variance**, and **quartile ranges** of the raw reward values $R(o_i, y)$ on the evaluation dataset. \n  \n\n#### **4. Evaluation on Reasoning and Mathematical Datasets**\n- The paper repeatedly claims that GRAO improves “complex reasoning” capability, but evaluation is restricted to *helpful* and *harmless* alignment benchmarks.  \n  - To substantiate the reasoning claim, please include evaluations on **reasoning-oriented datasets** such as GSM8K, MATH, or AIME. This  would demonstrate whether GRAO contributes to reasoning ability beyond behavioral alignment.  \n\n#### **5. Explanation of Theoretical Guarantees in Terms of Sample Complexity**\n- Abstract states that the proposed algorithm offers *convergence guarantees* and *sample efficiency advantages*, but the theoretical formulation (Eq. (4)) only proves gradient-norm convergence under standard assumptions.  \n  - Could you explicitly connect these theoretical results to the sample-complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4SYBpIpDzv", "forum": "zFLCNnKY8U", "replyto": "zFLCNnKY8U", "signatures": ["ICLR.cc/2026/Conference/Submission1455/Reviewer_dvMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1455/Reviewer_dvMB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668381532, "cdate": 1761668381532, "tmdate": 1762915774076, "mdate": 1762915774076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an unique way to train language models that combines learning from humans (SFT) and learning from feedback (RLHF) into one single process. Instead of the standard process of first training the SFT and initializing that for RL, the paper proposes a joint optimization framework via imitating good human answers, explore its own better answers, and maintain distance to reference policy. Empirical results are promising on helpfulness harmless task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach shows an unique way of comibing SFT and RL with imitiation learning and exploration type losses\n- Empirical analysis is promising showing improvement over baselines which shows faster convergence over baselines\n- Works well on both normal (dense) and mixture-of-experts (MoE) models"}, "weaknesses": {"value": "- The key weakness is explainly clearly where the benefit is coming from? I appreciate the author showing the Table 3 the ablation on removing several components, but its not clear which is the key component?\nFor example, main contributio comes from imitiation data or the dense feedback? Its not clear why training separately is an issue and additive helps? \n- How good is the performance with first SFT and then RL with varying beta? Where is the comparison of KL which makes it unclear. Since,  whats the base policy being compared to? \n- The reference policy in this case since SFT and RL is jointly trained needs to be specified and how far are you going from that for the alignment task.\n- The authors show convergence analysis which is standard for non-convex objectives, but whats the additional term or component which tells us why it improves. So basically, just show what standard RL would do and what this approach would do which term is new and why?\n- Why there are no much ablations on multiple datasets and also considering multiple other reward functions?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dwDOzCLaPG", "forum": "zFLCNnKY8U", "replyto": "zFLCNnKY8U", "signatures": ["ICLR.cc/2026/Conference/Submission1455/Reviewer_8Mni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1455/Reviewer_8Mni"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872875846, "cdate": 1761872875846, "tmdate": 1762915773943, "mdate": 1762915773943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRAO (Group Relative Alignment Optimization), a unified alignment framework that combines the benefits of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to achieve efficient and robust alignment for large language models (LLMs). The proposed optimization objective jointly leverages reference trajectories for imitation and self-sampled outputs for exploration by three key innovations, a multi-sample strategy, a new loss formulation, and reference-aware parameter updates. The evaluation shows the outperformance of the proposed framework across various alignment baselines, such as SFT, DPO, PPO and GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology is illustrated clearly.\n2. The convergence of the proposed optimization objective is proved theoretically.\n3. The experiments cover various baselines and include more than one LLM family."}, "weaknesses": {"value": "1. The hyperparameters $\\beta$ and $\\lambda$ are important. They balance exploration and imitation and control regularization strength. However, despite the default value, the author did not provide more information on these. Some ablation study is needed here for understanding how the algorithm synergizes SFT and RL.\n2. The figures for the experimental results are not fine-grained. The vector format should be used in the final version of paper.\n3. The theoretical bound in Equation 4 is not deeply discussed, and there is no corresponding experimental results to verify the message from this bound.\n4. In Section 4.3,  the authors attribute Moonlight-16B MoE model's higher gain than the dense Qwen2.5-7B model to GRAO's ability to improve on sparse MoE architectures. I think this point is not supported very well by enough evidence. Maybe the results of more spare and dense models should be evaluated."}, "questions": {"value": "1. For the optimal values of $\\beta$ and $\\lambda$, are they general or specific for the creation dataset and model?\n2. For the metrics in the experiments, could the win rates applicable here? Why not use that?\n3. Can you show the uncertainty of training dynamics in Figure 2 and 3?\n4. Does the results from Figure 4b mean that the optimal value of $\\beta$ and $\\lambda$ should be adaptive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6bz9vqRPVU", "forum": "zFLCNnKY8U", "replyto": "zFLCNnKY8U", "signatures": ["ICLR.cc/2026/Conference/Submission1455/Reviewer_PcA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1455/Reviewer_PcA2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954552625, "cdate": 1761954552625, "tmdate": 1762915773833, "mdate": 1762915773833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to study alignment capabilities within large language models by aligning a model to better produce aligned models, i.e., alignment for alignment. While the problem is timely, the paper’s core problem formulation, proposed method, and contributions are not clearly articulated. As a result, the novelty and significance are difficult to assess."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Studying whether models can be trained to improve the alignment of other models is an important research question, particularly given the increasing emphasis on automated safety supervision and scalable oversight.\n\n- The paper attempts to provide a mathematical formulation, which, if clarified and strengthened, could contribute to the emerging space of alignment studies."}, "weaknesses": {"value": "- The framing (“alignment for alignment capability”) is vague and not presented in a way that builds intuition.\n\n- It is unclear what concrete capability the paper is targeting or how success should be measured.\n\n- The introduction does not present a crisp research question or conceptual insight.\n\n- Contributions are not enumerated cleanly, and the storyline does not reveal what the reader should take away.\n\n- Several equations do not follow standard conventions from related alignment and bilevel optimization literature. Example: In Equation (2), the left-hand side is expressed in terms of parameters $\\theta$, but the right-hand side does not contain $\\theta$,  this makes the expression ill-posed as an optimization problem.\n\n-  It is unclear where performance gains originate from, and no ablations or mechanistic explanation are provided.\n\n- The experiments do not convincingly isolate the contribution of the proposed method versus baseline tuning dynamics or training noise.\n\n- There is no clear comparison with recent work on scalable oversight, teaching via RLHF, or alignment-via-distillation."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KR2lndedot", "forum": "zFLCNnKY8U", "replyto": "zFLCNnKY8U", "signatures": ["ICLR.cc/2026/Conference/Submission1455/Reviewer_df9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1455/Reviewer_df9z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010448679, "cdate": 1762010448679, "tmdate": 1762915773689, "mdate": 1762915773689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}