{"id": "gvx6SC99BI", "number": 10068, "cdate": 1758159567558, "mdate": 1763050505971, "content": {"title": "FarsightAlign: Early-Stage Test-Time Scaling for Prompt-Aligned Text-to-Image Generation", "abstract": "Text-to-Image diffusion models have achieved remarkable progress under the guidance of \"Scaling Laws\", but further performance gains are increasingly hindered by diminishing returns from scaling model size and data volume. To bypass this bottleneck, Test-Time Scaling (TTS) has emerged as a promising alternative. However, the lack of interpretable signals in the early diffusion steps forces existing TTS approaches to perform nearly complete denoising process for every candidate—resulting in high computational cost. In this work, we propose FarsightAlign TTS, a novel and efficient inference-time framework that leverages the rich semantic signals embedded in early cross-attention maps. With just a few denoising steps, FarsightAlign TTS can extract structured semantic information, such as object presence, layout, and attributes. It then leverages a lightweight scorer to prune unaligned candidates before committing to the final generation. This design significantly reduces computational cost while improving alignment with the user's prompt.\nThe experimental results demonstrate the effectiveness of our method. Furthermore, FarsightAlign TTS can function as a plug-and-play module, significantly boosting the semantic alignment capabilities of other advanced TTS frameworks with minimal additional computational overhead.", "tldr": "We propose FarsightAlign TTS, a test-time scaling method that uses early cross-attention signals to efficiently select semantically aligned candidates in text-to-image diffusion.", "keywords": ["text-to-image", "diffusion model", "test time scaling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c7f760817de70851de303f6f3ecba4f18412ea78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FarsightAlign TTS, an efficient test-time scaling method to improve how well text-to-image diffusion models follow complex prompts. The authors observe that while early-stage decoded images are blurry and uninformative, the cross-attention maps from the same initial steps already contain a clear semantic blueprint of the final image, including object layout and attributes. Leveraging this insight, FarsightAlign TTS runs multiple candidates for just a few denoising steps, extracts structured semantic information from their attention maps, and uses a lightweight LLM scorer to prune unaligned candidates before committing to a full generation. This early-pruning approach significantly reduces computational cost while outperforming existing methods in prompt alignment, and can also be used as a plug-and-play module to boost other frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well-written and logically structured, making the core ideas easy to understand. The figures are clear and effectively illustrate the main concepts.\n\n2. The motivation is straightforward and reasonable."}, "weaknesses": {"value": "1. The paper effectively demonstrates semantic extraction for prompts with a simple structure, such as conjoined noun phrases with single adjectives. However, it is unclear how the proposed method scales to prompts with greater syntactic complexity and descriptive depth. For example, in a prompt like, \"A thoughtful scientist in a white lab coat is examining a glowing blue flask, while in the background, a complex diagram is faintly visible on a chalkboard,\" the framework's ability to parse nested relationships, abstract attributes (\"thoughtful\"), and contextual clauses (\"while in the background\") is not evaluated. The method's reliance on extracting simple object, attribute, and position triplets may be insufficient for these nuanced scenarios.\n\n2. A significant gap in the evaluation is the lack of head-to-head comparisons with other methods on key benchmarks. The paper should include results on GenEval, T2I-CompBench, and WISE, as their absence makes it difficult to gauge the true effectiveness of this work against existing techniques.\n\n3. The experiments are conducted on SDXL and SD3, which are strong but no longer represent the cutting edge of text-to-image generation. Newer architectures like FLUX.1-dev or Qwen-Image have demonstrated significantly improved native prompt understanding. The critical question is whether FarsightAlign provides a meaningful boost to these already powerful models, or if its benefits are most pronounced on older architectures with known compositional weaknesses. Without testing on these stronger baselines, it is difficult to assess if the proposed method is a universally beneficial tool or a remedial technique for weaker models."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WQMQ8nopwB", "forum": "gvx6SC99BI", "replyto": "gvx6SC99BI", "signatures": ["ICLR.cc/2026/Conference/Submission10068/Reviewer_BtqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10068/Reviewer_BtqB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675994905, "cdate": 1761675994905, "tmdate": 1762921463286, "mdate": 1762921463286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5ekoPW6BIk", "forum": "gvx6SC99BI", "replyto": "gvx6SC99BI", "signatures": ["ICLR.cc/2026/Conference/Submission10068/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10068/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763050505215, "cdate": 1763050505215, "tmdate": 1763050505215, "mdate": 1763050505215, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to measure three metrics on the attention map instead of the decoded images during early diffusion steps for T2I TTS tasks. An LLM scorer is employed to score each candidate. The proposed method reduces the computational cost and improves alignment with prompts. The authors have conducted extensive experiments to illustrate the method's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using two normal distributions to dynamically determine the threshold to segment the attention map is novel.\n2. The figure is clear, and the paper is easy to follow."}, "weaknesses": {"value": "1. Lack of related work. Many earlier works adopt similar methods to apply TTS for T2I tasks like [1]. Authors should discuss the difference between the proposed method and [1]. Besides, the use of a cross-attention map to determine the alignment between the prompts and images has also been widely discussed [2, 3, 4]; the authors provide little information about this topic. The authors are expected to include these references and discuss the connections.\n2. A major concern is whether the metrics on the attention map could represent the decoded latents. For example, is a large attention score equal to the object's existence in the image?\n3. What is the purpose of using an LLM as a scorer? It seems like a rule-based scoring system could sufficiently conduct scoring for each latent.\n4. The definition of the decode loss is not clear. How do the authors obtain the object’s bounding boxes from the GT image and the image decoded from the early latent? \n5. A table should be presented for the main metrics when comparing the proposed method with others. This could provide results with more accurate numbers.\n6. In Figure 3, the term “confidence” is misspelled as “cofidence.” \n\n[1] Guo, Ziyu, et al. \"Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.\" arXiv preprint arXiv:2501.13926 (2025).\n\n[2] Chefer, Hila, et al. \"Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models.\" ACM Transactions on Graphics 42.4 (2023): 1-10.\n\n[3] Wang, Zirui, et al. \"TokenCompose: Text-to-Image Diffusion with Token-level Supervision.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024: 8553-8564.\n\n[4] Jiang, Dongzhi, et al. \"CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching.\" Advances in Neural Information Processing Systems (NeurIPS) 37 (2024)."}, "questions": {"value": "1. In the left figure in Fig. 5, why does BoN (Clip Score) fall behind the proposed method? It seems that BoN (Clip Score) should be an upper bound when testing on CLIP score.\n\nPlease refer to other questions in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qhqlo0smiA", "forum": "gvx6SC99BI", "replyto": "gvx6SC99BI", "signatures": ["ICLR.cc/2026/Conference/Submission10068/Reviewer_WJZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10068/Reviewer_WJZ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928971171, "cdate": 1761928971171, "tmdate": 1762921462934, "mdate": 1762921462934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FarsightAlign TTS, a test-time scaling method for text-to-image diffusion that prunes unpromising candidates early by reading semantic signals from cross-attention maps instead of decoding intermediate latents. After sampling many noises, it runs 5 denoising steps, aggregates token-wise attention maps, segments object regions, and derives object confidence, position, and attribute-to-object binding. These structured summaries are scored by a lightweight LLM judge, and only top candidates proceed to full denoising."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Empirically shows attention-based position error is low at early steps vs. decoded latent, which enables pruning unfavorable candidates after only 5 denoising steps.\n- Interesting observation that the intensity distribution of an attention map typically exhibits a distinct bimodal structure."}, "weaknesses": {"value": "- Overhead accounting for the LLM judge and the other elements are claimed to be negligible, yet the paper lacks a wall-clock/VRAM report of various components. \n- Benchmarks and metrics are appropriate, but no confidence intervals/significance tests or human studies on semantic alignment are reported.\n- The paper provides a well-executed engineering contribution with clear empirical value, but presents no conceptual innovation beyond integration of known components (early cross-attention analysis, LLM-based judging, etc.)."}, "questions": {"value": "1. Report per-prompt runtime/VRAM all components in the propoesd pipeline, and compare to decoding-based TTS at matched NFE.\n2. How do you select object/attribute tokens (multi-token nouns/adj., synonyms, plural forms)? Any NLP parser/heuristics? Please clarify failures and provide robustness analyses.\n3. Add CIs/paired tests for GenEval and ImageReward; per-category breakdowns (attributes/relations/spatial/multi-object) would show where gains arise.\n4. Could non-LLM or much smaller heuristic scorers approximate your LLM judge (e.g., rule-based checks on the JSON files), to further cut overhead? You ablate across LLMs; can you include a non-LLM baseline as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s3L4nKMW3L", "forum": "gvx6SC99BI", "replyto": "gvx6SC99BI", "signatures": ["ICLR.cc/2026/Conference/Submission10068/Reviewer_ankA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10068/Reviewer_ankA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938981850, "cdate": 1761938981850, "tmdate": 1762921462525, "mdate": 1762921462525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FarsightAlign TTS, a test-time scaling method designed to improve semantic alignment in text-to-image diffusion models. The approach avoids full-image decoding by extracting semantic cues (e.g., object presence, layout) from early-stage cross-attention maps. These cues are then used by a lightweight LLM scorer to efficiently prune candidate images.The authors report that FarsightAlign TTS outperforms existing test-time scaling methods, particularly in low-computation scenarios, and can be used as a modular plug-in with minimal overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well-organized and easy to follow.\n\n2.The core idea of FarsightAlign is simple yet effective. Unlike other TTS methods that score at the full-image level, FarsightAlign's approach of extracting cues from early-stage cross-attention maps is more computationally efficient and appears to be robust.\n\n3.Based on both quantitative and qualitative results, the proposed FarsightAlign achieves better performance than previous methods."}, "weaknesses": {"value": "1.Limited Attribute Handling: The paper provides weak evidence that FarsightAlign can handle complex attributes. It is unclear if the method works for actions (e.g., \"running,\" \"waving\") or is limited to simple attributes like color. The qualitative results in Figure 16 are not promising, showing an undesirable style shift (to 'comics style') rather than a targeted edit.\n\n2.Limitations of Coarse Cross-Attention: The method's reliance on early-stage cross-attention maps is a potential flaw. These maps are coarse, capturing only approximate shape and layout while lacking fine-grained semantic detail. This could lead to semantic ambiguity; for example, the model may be unable to distinguish between a 'tiger' and a 'lion' if their shapes are similar. I recommend the author to show more cases like that."}, "questions": {"value": "From figure 14 to figure 17, please specify the T2I base model and the corresponding random seed for generating the w/o scaling image."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1qNS5kNNSR", "forum": "gvx6SC99BI", "replyto": "gvx6SC99BI", "signatures": ["ICLR.cc/2026/Conference/Submission10068/Reviewer_g3Xi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10068/Reviewer_g3Xi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971305406, "cdate": 1761971305406, "tmdate": 1762921462071, "mdate": 1762921462071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}