{"id": "nIk4bviF8z", "number": 15633, "cdate": 1758253397646, "mdate": 1759897293161, "content": {"title": "Cite-While-You-Generate: Training-Free Evidence Attribution for Multimodal Clinical Summarization", "abstract": "Trustworthy clinical summarization requires not only fluent generation but also transparency about where each statement comes from. We propose a training-free framework for generation-time source attribution that leverages decoder attentions to directly cite supporting text spans or images, overcoming the limitations of post-hoc or retraining-based methods. We introduce two strategies for multimodal attribution: a raw image mode, which directly uses image patch attentions, and a caption-as-span mode, which substitutes images with generated captions to enable purely text-based alignment. Evaluations on two representative domains: clinician-patient dialogues (CliConSummation) and radiology reports (MIMIC-CXR), show that our approach consistently outperforms embedding-based and self-attribution baselines, improving both text-level and multimodal attribution accuracy (e.g., +15% F1 over embedding baselines). Caption-based attribution achieves competitive performance with raw-image attention while being more lightweight and practical. These findings highlight attention-guided attribution as a promising step toward interpretable and deployable clinical summarization systems.", "tldr": "We introduce a training-free framework that generates clinical summaries with real-time citations to text and images, improving transparency and trust in multimodal AI.", "keywords": ["Source attribution", "Multimodal language models", "Trustworthy AI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72f521a336a4b7113a3cd75a43edaf9fb585e179.pdf", "supplementary_material": "/attachment/8c8ec042114c15dcf09af8cf57f1b856e6a6e5a2.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical and timely problem of ensuring trustworthiness in summaries generated by Multimodal Large Language Models (MLLMs), a challenge of paramount importance in high-stakes domains like clinical medicine. The authors correctly identify that MLLMs are prone to \"hallucinations\"—generating statements weakly supported or entirely unsupported by the source evidence—which poses a significant barrier to their safe and effective deployment in healthcare settings where traceability is a prerequisite for clinical trust. An incorrect or misattributed claim about a patient's condition could lead to severe adverse outcomes.\n\nTo address this gap, the authors propose \"Cite-While-You-Generate,\" a novel, training-free framework that equips MLLMs with the ability to produce inline citations during the generation process. The method operates entirely at inference time, leveraging the decoder's internal cross-attention tensors to map generated summary sentences back to their most influential evidence units in the source document, which can be text spans or images. This approach avoids the need for costly retraining or the integration of auxiliary models, making it a practical solution for real-world clinical workflows.\n\nThe core of the proposed method is an three-stage pipeline designed to transform noisy, token-level attention scores into stable, interpretable citations.\n\nRecognizing the unique challenges of multimodal data, the framework introduces two complementary strategies for visual attribution :\n\n- Raw Image Attribution (IMG_RAW): This mode directly links generated tokens to image patch embeddings by aggregating attention scores over the visual tokens, enabling fine-grained visual grounding.\n- Caption-as-Source Attribution (IMG_CAP): In this mode, the image is first converted into a textual caption, which then replaces the image placeholder in the source text. This transforms the multimodal attribution problem into a purely text-based alignment task, offering a lightweight alternative when raw image processing is impractical.\n\nThe authors claim three primary contributions: (1) the development of the first training-free method for generating fine-grained, multimodal citations during summarization; (2) the introduction and comparative analysis of the `IMG_RAW` and `IMG_CAP` modes, highlighting a key trade-off between attribution fidelity and efficiency; and (3) a comprehensive empirical demonstration of the framework's superiority over strong baselines on two distinct clinical summarization benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novelty and Practicality of the Training-Free Approach:** The decision to pursue a training-free, inference-time solution is a major strength. In clinical environments, the logistical, financial, and regulatory burdens associated with retraining large-scale models are often prohibitive. Data privacy regulations (such as HIPAA in the U.S.) further complicate the process of fine-tuning on sensitive patient data. The proposed framework, by functioning as a \"plug-and-play\" module for existing, pre-trained MLLMs, offers a far more practical and scalable path toward adoption. This design choice reflects a keen awareness of the real-world constraints that govern the deployment of AI in healthcare.   \n\n**Soundness of the Core Aggregation Mechanism:** The paper's central technical innovation lies in its principled approach to stabilizing noisy attention signals. It is well-established that raw, token-level attention weights can be unstable and difficult to interpret directly. The authors' solution—combining a top-k majority vote with a final aggregation threshold—is both simple and remarkably effective. The ablation study provides compelling evidence for this design. The stark contrast in performance between the `max` attribution mode (which collapses to near-random performance with a Macro-F1 score around 12.58) and the `majority` mode (achieving a Macro-F1 of 76.33) demonstrates that this aggregation is not just a minor detail but the key to the method's success. This two-stage filtering process effectively functions as a denoising mechanism, transforming a noisy internal signal into a reliable, human-interpretable output.\n\n**Rigorous and Comprehensive Experimental Evaluation:** The empirical validation is thoughtfully designed. The authors evaluate their framework on two distinct and relevant clinical datasets: CLICONSUMMATION, which features doctor-patient dialogues, and MIMIC-CXR, a standard benchmark for radiology report summarization (specifically, the FINDINGS to IMPRESSION task). This demonstrates the method's applicability across different genres of clinical text. Furthermore, by testing on two different state-of-the-art open MLLMs (Qwen2.5-VL and LLaVA-NeXT), the authors provide strong evidence for the model-agnostic nature of their approach. The choice of baselines—embedding-based similarity (a strong retrieval method) and model self-attribution (a strong generative method)—ensures that the reported performance gains are meaningful. The results, as summarized in the table below, show consistent and substantial improvements across metrics and models. \n\n**Insightful Analysis of Multimodal Trade-offs:**\n\nThe paper goes beyond simply proposing two modes for multimodal attribution and provides a nuanced analysis of their respective strengths and weaknesses. The finding that IMG_RAW yields better text grounding and higher-quality summaries (as measured by ROUGE and BERTScore), while IMG_CAP can achieve superior joint exact match attribution, is a subtle but important result."}, "weaknesses": {"value": "**The Hidden Failure Mode of the IMG_CAP Strategy:** The IMG_CAP mode is presented as a practical and lightweight alternative, but it introduces a critical and unanalyzed vector for error propagation. The entire attribution process for an image is contingent on the accuracy of the model-generated caption. If this initial caption is factually incorrect, incomplete, or contains hallucinations, then any subsequent summary statement, no matter how perfectly attributed to that caption, will be fundamentally flawed with respect to the original visual evidence.\n\nThe paper's own results hint at this problem. As shown in the table below, summaries generated using the IMG_CAP mode are of lower quality (lower ROUGE and BERTScore) than those from the IMG_RAW mode, even while sometimes achieving higher joint attribution scores. This suggests that the model may be optimizing for faithfulness to a potentially flawed textual proxy (the caption) at the expense of faithfulness to the ground-truth visual information. This represents a subtle but dangerous failure mode, where the system appears to be functioning correctly (i.e., providing a valid citation) while propagating a critical factual error.\n\n**Significant Concerns with the Evaluation Protocol (The LLM Judge):** A major methodological weakness is the use of an LLM judge to generate the \"gold-standard\" reference citations for evaluation. While the authors acknowledge this and followed a recent protocol, this choice introduces a significant risk of systemic bias and circular reasoning. The evaluation may be measuring not how well the attention-based method aligns with human-adjudicated clinical truth, but rather how well it mimics the internal reasoning processes of another LLM.\n\nThe proposed method, being based on an internal mechanism of transformers (attention), shares a fundamental computational paradigm with the LLM judge. In contrast, the embedding-based baseline operates on a different principle (pre-computed semantic similarity). The superior performance of the attention-based method could, therefore, be an artifact of this \"in-paradigm\" evaluation. The experiment may be inadvertently demonstrating that the attention patterns of one MLLM are a good predictor of the generative reasoning of another, which is a far weaker claim than demonstrating true factual faithfulness. The absence of any human evaluation, even on a small subset of data to validate the LLM judge's outputs against expert clinical assessment, is a critical omission for a paper centered on a human-centric concept like trustworthiness."}, "questions": {"value": "Given the central role of the LLM-generated annotations in the evaluation, could the authors provide a more rigorous analysis of their quality and reliability? Specifically, was any inter-annotator agreement study conducted between the LLM judge and human clinical experts on a subset of the data? Without such validation, how can the research community be confident that the reported gains reflect an improvement in true clinical grounding rather than an alignment between the internal mechanisms of two different LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3u0vLWfuf9", "forum": "nIk4bviF8z", "replyto": "nIk4bviF8z", "signatures": ["ICLR.cc/2026/Conference/Submission15633/Reviewer_n34f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15633/Reviewer_n34f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456617283, "cdate": 1761456617283, "tmdate": 1762925896399, "mdate": 1762925896399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free, generation-time source attribution framework for clinical summarization. During decoding, the method pools decoder cross-attentions across layers/heads and maps each generated token and then each sentence, back to supporting source sentences and/or an image, yielding summaries with inline citations. Two multimodal modes are offered: IMG RAW, which aggregates attention over image patches, and IMG CAP, which replaces an image with a one-sentence caption to enable text-only alignment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n1.\tClear, unified pipeline (pooling，majority vote，sentence-level citation) with two practical multimodal modes (IMG RAW/IMG CAP). \n2.\tConsistent empirical improvements across tasks/models and well-defined metrics. \n3.\tInformative ablations identifying robust settings (k=3, τ≈0.16) and showing majority over max."}, "weaknesses": {"value": "Cons:\n1.\tAttention as attribution remains correlational; no causal/perturbation checks to validate necessity of cited evidence. \n2.\tScope limits: only two datasets; broader clinical subdomains and real-world deployment not evaluated. \n3.\tIMG RAW assignment attributes an image to only the sentence with maximal image attention—potentially too restrictive in practice. \n4.\tReference labels rely on an LLM judge rather than expert gold, risking evaluator bias."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bwJjPTjFrC", "forum": "nIk4bviF8z", "replyto": "nIk4bviF8z", "signatures": ["ICLR.cc/2026/Conference/Submission15633/Reviewer_CRHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15633/Reviewer_CRHS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651877230, "cdate": 1761651877230, "tmdate": 1762925895925, "mdate": 1762925895925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the transparency of information sources in clinical summary generation by proposing a generation-time source attribution framework that requires no additional training.  This method utilizes decoder attentions to directly provide source references (text fragments or images) for statements in the generated text, thus overcoming the limitations of post-processing or retraining methods.   Two multimodal source attribution strategies are proposed: a raw image mode and a description-based mode, enabling alignment and interpretation of plain text. Evaluations were conducted on two datasets: clinical-patient dialogues (CLICONSUMMATION) and radiology reports (MIMICCXR). The proposed method significantly outperforms embedded methods and self-attribution baseline models in both text-level and multimodal source attribution accuracy (e.g., a +15% F1 improvement compared to embedded methods)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality.  This paper defines clinically attributable summarization as the primary goal, rather than ex post-hoc explanation, emphasizing explicit attribution (text fragment or image) for each summary sentence during the generation process.  This perspective, incorporating accountability into the summary generation itself, has practical significance in the medical context.\n\nQuality.  The paper's method is training-free, directly reusing the cross-attention of existing multimodal LLMs, and stabilizing noisy token-level attention into sentence-level citations through block segmentation, attention convergence, top-k majority voting, and threshold filtering.  Experimentally, the method was tested on Qwen2.5-VL-7B and LLaVA-NeXT-7B models, consistently outperforming strong baselines on clinical tasks such as doctor-patient dialogue summarization and radiology report summarization.\n\nClarity.  The paper is written clearly, with necessary pseudocode explanations, making it easy to understand.  The descriptions of experimental details such as training data and hyperparameters are sufficiently transparent, and replication appears feasible.\n\nSignificance.  This paper uses interpretability and citation orientation as direct outputs, which means not only improving model metrics but also enhancing its acceptability in practical medical work.  Even though the method is technically an integration and reuse of existing attention attribution techniques, it demonstrates cross-model and cross-task effectiveness, thus possessing the potential for practical impact."}, "weaknesses": {"value": "1. The paper mentions that description-based attribution strategies are more lightweight, but lacks supporting experimental data.\n\n2. This paper is based on the key attribution hypothesis: attention relevance is equivalent to causal contribution, but lacks rigorous theoretical or causal verification, resulting in weak logical support.\n\n3. This paper attributes attention to the source of evidence for generated sentences, a concept highly similar to existing \"attention-as-attribution\" approaches. Therefore, it needs to be more clearly articulated: what are the key technological innovations of this paper compared to existing attention-based or nearest-neighbor retrieval-based interpretable summarization methods? Currently, the manuscript seems more like a domain transfer from existing work than a breakthrough in research paradigms.\n\n4. The method relies entirely on cross-attention weights to determine which source text or image supports this sentence. However, the community has already discussed that high attention does not always equate to causal contribution, and the meaning of attention differs across layers. The authors improve stability through averaging and majority voting, but this remains heuristic. Currently, there is a lack of quantitative or qualitative analysis of attribution errors, making it difficult to determine whether this attribution traceability is truly safe and reliable, or merely looks like an explanation.\n\n5. This paper makes some deployment-oriented claims, such as improving clinical accountability and achieving safe and transparent use of MLLM, but these claims exceed the scope of empirical research.  All evidence is limited to two summary-based scenarios: doctor-patient dialogue and chest X-ray results. Other clinical record types, such as discharge summaries, surgical records, longitudinal progress notes, other imaging modalities (CT or MRI), or real-world multi-institutional electronic medical record systems, are not evaluated. The paper also proposes a model using captions as the data source as a privacy- and compliance-conscious alternative, but since the captions are generated by the model, they may be biased and therefore do not equate to true attribution.\n\n6. Current ablation methods are all designed around parameters, but the necessity of each stage in the pipeline claimed in this paper is not verified, lacking framework component-level ablation. Hyperparameter robustness is only analyzed on a single backbone model, without demonstrating direct reuse in a second model, affecting the credibility of generalizability."}, "questions": {"value": "1. The paper claims that the caption-as-source attribution strategy is more lightweight, but it does not provide quantitative results regarding computational cost, inference latency, or memory usage. Please provide specific experimental data or complexity analysis to support the lightweight claim.\n\n2. The method presented in this paper is technically highly similar to existing attention-as-attribution or similarity-based interpretable summarization methods. Please explain the substantive innovation of this paper compared to these existing methods.\n\n3. The authors averaged multi-layer attention and used majority voting to improve stability, but these are heuristic designs. Please further analyze in what types of samples attribution failed? What is the impact of misattribution on downstream clinical interpretation?\n\n4. Please explain why these results are sufficient to support conclusions at the clinical deployment level. Are there plans to validate these results on other document types or image modalities such as CT and MRI?\n\n5. Regarding ablation experiments, please explain the independent contribution and necessity of the modules presented in this paper. In addition, can the selected hyperparameters be directly transferred to different backbone models such as LLaVA-NeXT while maintaining performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wUAfgydhYv", "forum": "nIk4bviF8z", "replyto": "nIk4bviF8z", "signatures": ["ICLR.cc/2026/Conference/Submission15633/Reviewer_yKNV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15633/Reviewer_yKNV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814275672, "cdate": 1761814275672, "tmdate": 1762925895267, "mdate": 1762925895267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Cite While You Generate is a training-free framework for doing sentence-level attribution between generated and prompt text. The authors study their framework in the context of medical summarization where evidence attribution has critical implications for patient safety. The authors detail their approach for handling text or multimodal inputs and demonstrate results in either context. The authors also conduct ablation studies to understand the importance of the hyperparameter choices of their method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a well founded, important, and timely topic in source attribution in medical text generation. The idea to use attention weights in a training-free approach is a laudable effort to ground generations using existing LLMs.\n\n- The authors demonstrate empirical results with improvements particularly in an exact match metric across multiple evaluations, a metric which is more difficult than F1 where simpler models are competitive, demonstrating the competitive edge of their approach.\n\n- The method is generally well explained with concise and clear equations, defined symbols, and clear pseudocode."}, "weaknesses": {"value": "- Evaluation datasets are very limited at N=256 for text-only and N=263 for multimodal. Additionally, the multimodal evaluation is confined to only 1 dataset and the text-only to 2 datasets. The task of automated summarization and grounding/attribution in the medical domain has been well studied with many datasets being proposed, this work needs to draw on at least some of those datasets, particularly in the text-only domain:\nhttps://archehr-qa.github.io/\nhttps://biolaysumm.org/#data\nhttps://physionet.org/content/bionlp-workshop-2023-task-1a/1.0.0/\nhttps://vilmedic.app/misc/bionlp23/sharedtask\nhttps://aclanthology.org/2021.acl-srw.30/\nhttps://aclanthology.org/2022.findings-emnlp.286/\nhttps://physionet.org/content/labelled-notes-hospital-course/1.2.0/\nhttps://pubmed.ncbi.nlm.nih.gov/31445245/\n\n- Under the proposed framework, images are attributed as a whole rather than specific image patches. However this is an oversimplification and limits the utility of attribution in, for example, the radiology domain where findings to impression summarization of a chest xray report may benefit from localization of specific observations. Moreover, under the raw multimodal framework, the image is only ever attributed to one summary sentence. Again this is a simplification where complex images may present more than one attributable detail. This is a similar problem with the caption-as-image multimodal framework, where a single caption is used to summarize the image; in many ways, one can consider the findings or impression section of a chest xray to be its caption and in either section, a single sentence rarely describes an entire xray.\n\n- While the ablations of the hyperparameters in Section 5.2.1 are a nice result, this needs to be done on a validation set. Otherwise, this constitutes a form of overfitting and leakage on the evaluation set (line 436-439), leading to inflation of results. Also, while it’s noted on lines 390-394 that Qwen did better and therefore you use it to test other hyperparameters, are there results to show that other hyperparameter combinations don’t do better for LLaVA-NeXT? It seems like the framework is relatively sensitive to such hyperparameters, as in the Qwen case in Figure 2.\n\n- The authors discuss hallucinations as a potential concern of MLLMs (line 57), however their method doesn’t really prevent them. In fact, all it does is provide supporting “evidence” for potentially hallucinated text."}, "questions": {"value": "- The precise model used as the LLM judge for construction of the reference set needs to be provided\n\n- Table 2 LLaVA-NEXT text-only attribution exact match result is incorrectly bolded, should be self attribution (last row)\n\n- Does the reference label for raw images vs image captions differ? Could that explain any of the difference in results between the two? Otherwise, is is a bit surprising that including the raw image tokens allows the model to generate more precise answers (Section 5.2.2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CkKIXmPqxZ", "forum": "nIk4bviF8z", "replyto": "nIk4bviF8z", "signatures": ["ICLR.cc/2026/Conference/Submission15633/Reviewer_b7Ae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15633/Reviewer_b7Ae"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858447770, "cdate": 1761858447770, "tmdate": 1762925894879, "mdate": 1762925894879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}