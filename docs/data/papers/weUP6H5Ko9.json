{"id": "weUP6H5Ko9", "number": 19602, "cdate": 1758297577185, "mdate": 1763753968813, "content": {"title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "abstract": "The success of LLMs has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains.  However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.", "tldr": "", "keywords": ["Reinforcement Learning", "Fine-tuning", "Agents", "Decision-Making", "Exploration", "Analysis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e32be3a4554967f49eafc33ee12cf6e179960c13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the decision-making capabilities of LLMs under uncertainty using multi-armed and contextual bandit settings. Through analyses including action coverage tracking and comparison against optimal algorithms like UCB, the authors identify three primary failure modes: greediness, frequency bias, and the knowing-doing gap.\n\nTo mitigate these, the paper fine-tunes Gemma2 models (2B, 9B, 27B) using PPO trained with CoT reasoning traces on environment rewards. The PPO method significantly improves performance over the baseline ICL methods. The study further shows that augmenting PPO with classic exploration techniques (like try-all, Œµ-greedy, and exploration bonuses) yields additional gains, with methods like adding an exploration bonus or an initial \"try-all\" phase proving most effective among the tested PPO variations, and provided ablation results on CoT, thinking time, and additional experiments on Tic-tac-toe."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a valuable diagnostic analysis of why LLMs fail in decision-making tasks, systematically identifying and quantifying specific issues like greediness, frequency bias, and the knowing-doing gap using bandit environments. This clear characterization of common failure patterns is a useful contribution.\nThe work demonstrates that PPO fine-tuning with CoT traces effectively improves LLM decision-making across various tasks (MAB, CB, Tic-tac-toe) and model scales, outperforming baseline in-context learning without requiring expert data. Furthermore, the study evaluates integrating simple exploration strategies during PPO, showing that techniques implemented via prompt/context modifications, such as initializing with a \"try-all\" phase, yield additional performance gains. Overall the paper offers novel insights into practical methods for enhancing exploration of PPO-tuned LLM agents under related decision making settings."}, "weaknesses": {"value": "Limited Generalizability: The paper claims to analyze general LLM agent behavior but bases its core analysis almost entirely on simple bandit tasks (MAB and CB). The setting is somewhat narrow (also, SOTA/commercial models were not evaluated), making it hard to generalize the findings to more complex agentic situations. The knowing-doing gap analysis is a particularly interesting topic, but the analysis is tied to the UCB structure in bandits; it would be interesting to explore a more generic analysis that can characterize the knowing-doing gap across a variety of decision-making tasks (possibly with some form of optimal algorithm as part of the LLM‚Äôs internal knowledge) and provide deeper insights (e.g., could this be a value alignment problem where the LLM‚Äôs default option is not using an optimal algorithm?).\n\nMethod Performance and Novelty: While the proposed PPO fine-tuning method improves LLM performance over the ICL baseline, it doesn't match the results of SFT using expert UCB data. Given that SFT data is relatively easy to synthesize for MAB tasks, the proposed method is less useful in this context. The PPO solution itself, while possibly the first published work on MAB/CB, is a well-established method, making it less novel. The authors are encouraged to expand the method to more decision-making settings (e.g., more text-games as in https://arxiv.org/abs/2502.17543) and showcase more solid results on improving exploration there.\n\nFinally, the Tic-tac-toe game feels less like an ablation to me. It would be interesting to have full coverage of the analysis there."}, "questions": {"value": "Have we explored prompting methods to ensure LLM strictly follows UCB algorithm? \n\nFor the ùúñ-greedy method: is it done at token level or at action level? If it‚Äôs the latter, how is it implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "25Elo2NPWX", "forum": "weUP6H5Ko9", "replyto": "weUP6H5Ko9", "signatures": ["ICLR.cc/2026/Conference/Submission19602/Reviewer_8Kgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19602/Reviewer_8Kgn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206080534, "cdate": 1761206080534, "tmdate": 1762931465689, "mdate": 1762931465689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic investigation into the causes of suboptimal performance in Large Language Model (LLM) agents on decision-making tasks. The authors convincingly identify and quantify three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. To address these issues, the paper proposes a method of Reinforcement Learning Fine-Tuning (RLFT) on the model's self-generated Chains of Thought (CoT). Through meticulously designed experiments in environments such as multi-armed bandits, contextual bandits, and text-based Tic-Tac-Toe, the paper demonstrates that RLFT effectively enhances the model's exploration capabilities and bridges the knowing-doing gap, thereby improving the overall quality of the LLM's decisions. The study is further enriched by an in-depth analysis of various classical and LLM-specific exploration mechanisms, along with insightful ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- A key strength of this paper is its thorough and systematic diagnosis of the problem before proposing a solution. The clear definition and empirical quantification of greediness, frequency bias, and the knowing-doing gap provide a valuable conceptual framework and a strong motivation for the proposed method.\n- The authors evaluate their approach across multiple model scales (Gemma2 2B, 9B, 27B) and environments of varying complexity. The inclusion of strong baselines like UCB and extensive ablation studies (e.g., on the importance of CoT, \"thinking time\") significantly strengthens the credibility of the findings. The analysis is also extended to other model families (Llama3, Qwen2.5) in the appendix, further demonstrating the generality of the results.\n- The proposed RLFT on self-generated CoT is a logical and effective approach. By directly acting on the model's internal reasoning process, it offers a more fundamental way to improve external behavior compared to fine-tuning on actions alone. The experimental results clearly demonstrate the method's positive impact on increasing action coverage and reducing cumulative regret, directly linking the solution to the identified problems."}, "weaknesses": {"value": "- The paper effectively demonstrates that \"external\" exploration scaffolds (e.g., the try-all strategy or exploration bonuses) improve performance. This suggests that the model may still lack a fundamental, intrinsic drive for exploration. A valuable direction for future work could be to explore integrating intrinsic motivation directly into the RLFT objective. For instance, adding an entropy bonus on the action distribution or novelty-based rewards (e.g., based on state visitation counts) might encourage the model to develop a more generalizable and autonomous exploration policy.\n- The paper employs a regex-based method for action extraction and applies a fixed penalty (-5) for invalid outputs. While practical, the robustness of this approach could be further discussed. A comparison with increasingly standard structured output formats (e.g., JSON objects) would be a valuable addition. Furthermore, a sensitivity analysis on the magnitude of the penalty for invalid actions would offer valuable insights into its impact on the learning dynamics."}, "questions": {"value": "- Regarding Figure 1, how significant is the position of the \"interaction history\" within the prompt on the LLM's performance? A robustness and sensitivity analysis of the prompt template would be highly beneficial for the community.\n- The paper shows that explicit exploration rewards are highly effective. As a follow-up, have the authors considered integrating intrinsic exploration motives (e.g., using an entropy bonus or curiosity-driven rewards) directly into the RLFT objective? A comparison with intrinsic exploration mechanisms would further enhance the paper's comprehensiveness.\n- On line 192, you mention: \"Instead of exploiting multiple rollouts, as used by Ahmadian et al. (2024) and Ramesh et al. (2024), we compute rewards-to-go.\" Could you please elaborate on the reasoning for this choice, perhaps in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p6Cdo7NDDC", "forum": "weUP6H5Ko9", "replyto": "weUP6H5Ko9", "signatures": ["ICLR.cc/2026/Conference/Submission19602/Reviewer_HhsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19602/Reviewer_HhsF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731946227, "cdate": 1761731946227, "tmdate": 1762931465240, "mdate": 1762931465240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates LLMs in decision-making scenario, focusing on three failure modes: greediness, frequency bias and the knowing-doing gap. The authors illustrate these failure modes on a multi-armed bandit environment, and further show that RL finetuning can help mitigate them. \n\nThe paper is generally well-written and easy to follow. However, the novelty of the work is not clear to me. LLMs have been well studied in decision-making settings, and the failure points studied here (lack of exploration, knowing-doing gap) have been noted in previous works (see weaknesses). Using reinforcement learning finetuning to improve the abilities of LLMs is not novel either. I think this work would be better suited as a workshop contribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The multi-armed bandit environment allows to illustrate the three failure modes in a controlled setting"}, "weaknesses": {"value": "Major:\n\n- W1: The results presented in this work are not particularly novel.\n    1. LLMs have been studied as agents in decision-making environment in numerous works (see for example [1] for a recent review). As the authors themselves note in the introduction, previous works have observed failure modes such as lack of exploration (cf l.41) and the knowing-doing gap (cf l.43-44). The main results of the paper (Section 4.2) illustrate these failure modes on a multi-armed bandit setting but do not provide further insight beyond observing these failure modes in a controlled environment. \n    2. The benefits of RL finetuning with verifiable rewards have been established in previous works, for example [2-5]. The authors mention [4-5] saying that in contrast their approach is ‚Äúspecialized for decision-making scenarios‚Äù (l.163), even though I‚Äôd argue that [5] is precisely focused on decision-making scenarios already.  \n    3. Results in the Section 4.5 merely illustrate well-studied methods like RL finetuning, behavior cloning/distillation, and test-time scaling. \n- W2: The related works do not cover previous works on RL finetuning and LLM-as-an-agent, which makes the novelty compared to previous works unclear.\n- W3: In method l.175, ‚ÄúIf no valid action is found, a random action is executed‚Äù. Weak LLMs might run into invalid actions often, which introduces an unwanted form of epsilon-greedy exploration where epsilon is the rate at which the LLM doesn‚Äôt output a valid action. Given the amount of exploration done by the LLM is precisely what is being studied here, it would be preferable to filter out these invalid actions instances instead.\n- W4: In Figure 6, results for 20 arms show that RLFT leads to higher regret for the 9B model, but this is not discussed in the text. Also the plots with lower noise in appendix C.2 are missing the 9B-RLFT model.\n- W5: In Section 4.2, the conclusion that ‚Äúthis suggest that frequency bias is an artifact of supervised pre-training‚Äù (l.326) is misleading: results for the 27B model shows (as the authors note l.324) that a strong model can escape the frequency bias, even though it is trained with supervised pre-training.\n- W6: In Section 4.2, the illustration provided Figure 24 suggests that the reason the LLM doesn‚Äôt pick previously unseen actions might be due to an imprecision in the prompt phrasing. Does the effect remain when specifying that the model should strictly according to the UCB algorithm? Does the result depend on most actions not having been seen previously like in Figure 24?\n\nMinor:\n\n- l.246: ‚ÄúLLMs model‚Äù is redundant\n- Why is the result on Tictactoe in the ablation section? This just seems to me like it is looking into the effectiveness of RL finetuning as in Section 4.3.\n\n\n[1] Cao, Yuji, et al. \"Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods.\" IEEE Transactions on Neural Networks and Learning Systems (2024).\n\n[2] Lambert, Nathan, et al. \"Tulu 3: Pushing frontiers in open language model post-training.\" arXiv preprint arXiv:2411.15124 (2024).\n\n[3] Luong, Trung Quoc, et al. \"Reft: Reasoning with reinforced fine-tuning.\" arXiv preprint arXiv:2401.08967 (2024).\n\n[4] Guo, Daya, et al. \"Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\" arXiv preprint arXiv:2501.12948 (2025).\n\n[5] Zhai, Simon, et al. \"Fine-tuning large vision-language models as decision-making agents via reinforcement learning.\" Advances in neural information processing systems 37 (2024): 110935-110971."}, "questions": {"value": "- Figure 3: what do the cross markers correspond to?\n- Figure 4: the plots (a) and (c) are unclear to me: as I understand the y-axis (action entropy) refers to the entropy of the distribution over predicted actions. But then why do the plots show markers for each individual action? If they correspond to the action that is repeated N times across the x-axis, why would we expect the results to look different when only the action name is changing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vrQ15AtPns", "forum": "weUP6H5Ko9", "replyto": "weUP6H5Ko9", "signatures": ["ICLR.cc/2026/Conference/Submission19602/Reviewer_Sto4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19602/Reviewer_Sto4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910432440, "cdate": 1761910432440, "tmdate": 1762931464833, "mdate": 1762931464833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically investigates why LLMs perform sub-optimally in decision making settings and identifies three primary failure modes across different model families: greediness, frequency bias, and the knowing-doing gap. Through experiments on multi-armed bandits, contextual bandits, and a text-based Tic-tac-toe environment, this work provides an empirical characterization of these issues across model scales (Gemma2 2B/9B/27B, Llama3 3B/8B/70B and Qwen-2.5 3B/7B/14B/32B). The authors have designed a suite of experiments exploring whether Reinforcement Learning Fine Tuning (RLFT) on self-generated Chain-of-Thought (CoT) rationales can help mitigate these issues, showing that RLFT increases exploration, reduces the greediness and frequency bias in decision making and also narrows the knowing-doing gap with an increased reasoning token budget. Additionally, classical and LLM-specific exploration mechanisms (like $\\epsilon$-greedy, self-correction, self-consistency) are used to further improve the post-RLFT performance. Ablation studies highlight the roles of CoT traces in RLFT, expert data for behavior cloning vs thought cloning in SFT, and reasoning token budgets for inference time computational efficiency. Overall, this paper offers an insightful analysis into the decision-making behavior of LLMs and the impact of RLFT in addressing some of the current drawbacks of decision making with different language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper identifies and rigorously quantifies three key failure modes (greediness, frequency bias, knowing-doing gap) in LLMs for decision making applications, specifically focused on bandit-style decision making with instruction-tuned large language models. \n\n- The authors use RLFT to analyze how reinforcement feedback influences reasoning and exploration, with appropriate baselines and ablations across different LLM model families.  \n\n- The paper is overall well-written with thorough description of relevant implementation details and baselines, improving reproducibility."}, "weaknesses": {"value": "- RLFT itself is a straightforward application of PPO-style RLHF training to CoT outputs, therefore the methodological novelty lies mainly in the empirical analysis and not in a new algorithm.\n- The environments are relatively simple (bandits, Tic-tac-toe) and while appropriate for controlled study, stronger claims about decision-making or agentic abilities would benefit from evaluation on richer, stateful domains.\n- The 50-step interaction limit may underestimate the difficulty of exploration and long-term adaptation with LLMs.\n- Unclear scalability to larger LLMs: Most analyses are conducted on $\\leq$27B models and the implications for larger model scales (e.g. Llama3 70B) are briefly discussed but not thoroughly tested.\n- RLFT results seem sensitive to design choices for reward shaping and action space exploration strategies. Ablation experiments in this paper demonstrate the effects qualitatively (in most likely a single trial) but they are not statistically quantified."}, "questions": {"value": "- Beyond empirical results, do authors have any intuitive understanding or insights into why RLFT improves exploration (maybe changes in CoT token distributions or entropy)?\n\n- How would the observations in this work transfer to other recent reinforcement-based fine-tuning frameworks (GRPO, DPO, or self-critique fine-tuning)?\n\n- Do the RLFT improvements transfer across unseen bandit configurations or to different reasoning prompts?\n\n- In both Fig 21 (a) and (b), a larger model does not always outperform a smaller one in terms of action space coverage. Is there any ablation experiment or empirical observation explaining this behavior? \n\n- The current evaluation for the knowing-doing gap is not extended to the Llama3 70B model, which is shown to have less severe issues from greediness and frequency bias. It would help to include empirical evidence of the relationship, if any, between the knowing-doing gap and model size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YO14zX0Cr1", "forum": "weUP6H5Ko9", "replyto": "weUP6H5Ko9", "signatures": ["ICLR.cc/2026/Conference/Submission19602/Reviewer_ytZh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19602/Reviewer_ytZh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142918966, "cdate": 1762142918966, "tmdate": 1762931464354, "mdate": 1762931464354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}