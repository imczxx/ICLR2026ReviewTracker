{"id": "wJgaHyJaDD", "number": 16758, "cdate": 1758268414378, "mdate": 1759897220999, "content": {"title": "TPOUR: Temporal Preference Optimization for Unsupervised Retrieval", "abstract": "Unsupervised retrievers offer scalability by learning semantic similarity from unlabeled documents via contrastive learning. However, they struggle to capture the temporal relevance, often retrieving semantically related but temporally misaligned documents--an important aspect when a document collection spans multiple time periods (e.g., For the query \"Who is the president in 2019?\" retrieving from related documents spanning 2018–2025 introduces temporal ambiguity if relying solely on semantics). Existing methods rely on supervised training with explicit timestamps, which are not always feasible. We propose TPOUR (Temporal Preference Optimization for Unsupervised Retriever), which integrates our novel training method Temporal Retrieval Preference Optimization (TRPO). TRPO reinterprets preference learning in the temporal dimension, guiding the retriever to favor temporally aligned documents. TPOUR constructs temporally aligned and misaligned document pairs by leveraging document corpora collected at different times and trains the retriever without supervision to prioritize temporally aligned over misaligned documents. Furthermore, TPOUR generalizes to unseen time periods by interpolating time vectors, enabling continuous temporal alignment. Experiments on temporal QA with a mixed-timestamp document collection show that TPOUR outperforms both unsupervised and supervised baselines. Compared to Nomic Embed v2 MoE, TPOUR Contriever improves nDCG@5 by +7.13 (+23.5%) on explicit and +7.76 (+25.5%) on implicit queries on average.", "tldr": "We introduce an unsupervised retrieval training method (TPOUR) that combines contrastive learning with our Temporal Retrieval Preference Optimization (TRPO) to align document retrieval with both implicit and explicit temporal contexts.", "keywords": ["Temporal Retrieval", "Information Retrieval", "Unsupervised Learning", "Contrastive Learning", "Preference Optimization", "Time Vector"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb7aecd11a2cf13f63cbd54b67bacee60ce03f4e.pdf", "supplementary_material": "/attachment/8e706b50e6a93c4e64a2d3dc4e0224577ca4be6b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces TPOUR (Temporal Preference Optimization for Unsupervised Retriever), a novel method designed to solve the problem of temporal misalignment in scalable, unsupervised retrieval by teaching the retriever a time bias without labeled data. Current unsupervised dense retrievers fail to distinguish between semantically relevant documents that are temporally aligned and those that are misaligned. TPOUR addresses this by integrating a new training signal, Temporal Retrieval Preference Optimization (TRPO), into contrastive learning. TRPO utilizes unlabeled versioned corpora (e.g., historical Wikipedia snapshots) to create implicit preference pairs, teaching the retriever to prioritize documents from a temporally aligned version over a misaligned version, thereby learning a continuous temporal preference from content updates. TPOUR substantially outperforms both unsupervised and supervised baselines on temporal QA tasks and demonstrates that Time Vector Interpolation allows it to generalize its temporal preference to intermediate, unseen time periods without requiring full retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core strength lies in how TPOUR achieves temporal awareness without requiring costly human-labeled data or explicit time annotations on every document. Traditional temporal retrieval demands supervised training with explicitly timestamped relevance scores, which is expensive and unscalable. TPOUR cleverly addresses this by introducing a Temporal Retrieval Preference Optimization (TRPO) loss and adapting the DPO framework to retrieval. This simple but ingenious mechanism enables the dense retriever to learn a complex temporal bias directly from changes in semantic content over time.\n\nIn addition, TPOUR provides a foundational fix by embedding a temporal preference directly into the latent embedding space. This allows the retriever to efficiently distinguish between documents that are merely semantically similar (low temporal score) and those that are both semantically similar and temporally correct (high temporal score), thereby significantly improving retrieval precision for time-sensitive queries. This fundamental fix moves dense retrieval closer to reliable, real-world deployment."}, "weaknesses": {"value": "1. Mismatch Between Training Signal and Evaluation Benchmark\n>TPOUR intentionally trains the model on implicit semantic drift (changes in content across document versions). However, the primary evaluation uses questions that require alignment with explicit temporal anchors (\"in 2019\"). This raises questions about whether the achieved performance truly reflects a learned temporal map or simply an effective scoring bias toward the document version containing the latest relevant semantic update. Closing this conceptual gap requires further demonstration.\n\n2. Ambiguity of Generalization Results (Figure 5)\n> The correlation in Figure 5 is not visually supported. Although a rising regression line is shown, the wide spread of data points does not convincingly demonstrate a strong correlation between a dataset's age and its optimal timing interpolation. This ambiguity weakens the conclusion that TPOUR shows a reliable general time sensitivity in external datasets.\n\n3. Missing Diagnostic Evidence for Foundational Problem\n> The paper asserts that time-unaware retrievers tend to retrieve semantically relevant but temporally misaligned documents, which is the central problem TPOUR is designed to solve. However, the paper does not provide a dedicated diagnostic analysis to formally quantify and prove the severity of this foundational problem in a baseline model. Consequently, in the absence of this explicit diagnostic proof, the necessity and added complexity of the TRPO optimization are not adequately anchored to a rigorously quantified problem. This weakens the overall justification and motivation for introducing the complex temporal preference framework."}, "questions": {"value": "1. Since the model is trained on implicit semantic changes across document versions, how can the authors provide a dedicated diagnostic evaluation (e.g., using a different benchmark, or a targeted analysis of the retrieved documents) to verify that the model has effectively learned to distinguish content updates over time?\n\n2. Could the authors please provide diagnostic evidence that rigorously quantifies the severity of temporal misalignment in baseline models, thereby justifying the necessity and added complexity of the TRPO optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HL2pjwoY9A", "forum": "wJgaHyJaDD", "replyto": "wJgaHyJaDD", "signatures": ["ICLR.cc/2026/Conference/Submission16758/Reviewer_tX1y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16758/Reviewer_tX1y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548565925, "cdate": 1761548565925, "tmdate": 1762926803319, "mdate": 1762926803319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles temporal misalignment in unsupervised dense retrieval. The authors propose TPOUR, which augments MoCo-style contrastive training with a preference loss (TRPO) that encourages a retriever to favor temporally aligned documents over temporally misaligned ones by leveraging multiple snapshots of the same corpus (Wikipedia) collected at different times. They further introduce “time vectors” extracted from fine-tuned models at different timestamps and linearly interpolate them to generalize to intermediate time periods without retraining. Experiments on custom retrieval versions of SituatedQA (2018–2021) and RealTimeQA (Jan–Dec 2023) show sizable gains over Contriever and other baselines, particularly on queries with temporal intent. They also demonstrate improvements in timestamp prediction with a mixture of TPOUR and a correlation between the creation year of the BEIR dataset and the optimal interpolation weight."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is well-motivated and clearly formulated. It tackles temporal misalignment in unsupervised retrieval, an important and under-addressed challenge.\n\n2. The method is simple and effective. It combines contrastive learning with a DPO-style preference loss guided by similarity gaps and integrates smoothly into existing training pipelines.\n\n3. The time-vector interpolation is a neat and practical idea. It is supported by experiments and enables time alignment without retraining, making deployment easier.\n\n4. The empirical analysis is thorough. The paper covers yearly and monthly settings, explicit and implicit queries, timestamp prediction, and a BEIR case study, complemented by qualitative examples, distribution visualizations, and clear reproducibility details."}, "weaknesses": {"value": "1. Dataset construction is central to the method, but key details are missing. For example, the paper should clearly specify how preference triplets (Q, D^t, D^{t'}) are sampled, how content changes across snapshots are detected, and related implementation choices. In addition, the appendix assumes topics across years are similar; while intuitively plausible, this assumption requires empirical validation.\n\n2. The baseline comparisons are not fully fair. According to the appendix, baselines rely on public checkpoints and are not trained or adapted on the same corpora, which may give the proposed method a domain/time adaptation advantage.\n\n3. The model may have learned temporal shortcuts or biases. Although Appendix E.6 shows some robustness, it does not rule out reliance on spurious time-correlated cues.\n\n4. A substantial amount of essential information is placed in the appendix, which reduces the readability and self-containedness of the main text."}, "questions": {"value": "1. Could you clarify how preference triplets are formed and how temporal content changes are identified, and consider adding a brief sensitivity analysis to show robustness to these design choices? Additionally, please provide evidence that the assumption of topical similarity across years holds in practice.\n\n2. To ensure fair comparison, could you discuss the extent of time/domain adaptation applied to baselines and, where feasible, include adapted or better‑tuned baselines—or justify why such adaptation is not practicable?\n\n3. Can you demonstrate that the method’s gains are not driven by explicit or implicit temporal cues？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sY2J9dh4kt", "forum": "wJgaHyJaDD", "replyto": "wJgaHyJaDD", "signatures": ["ICLR.cc/2026/Conference/Submission16758/Reviewer_8GPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16758/Reviewer_8GPZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907833517, "cdate": 1761907833517, "tmdate": 1762926802909, "mdate": 1762926802909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised retrieval framework called TPOUR to address the “temporal misalignment” problem in traditional retrieval models when handling time-sensitive queries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s main contribution is transferring the idea of DPO to unsupervised retrieval (TRPO) and combining it with time vector interpolation, offering a solution to the problem of “temporal awareness.”"}, "weaknesses": {"value": "1. Unclear “Unsupervised” Claim\nAlthough the method is presented as unsupervised, it relies on document timestamp metadata to construct aligned vs. misaligned preference pairs. This constitutes weak supervision, and the paper currently does not clearly acknowledge or justify this discrepancy.\n\n2. Evaluation Pipeline May Introduce Retrieval Bias\nThe construction of evaluation document sets uses Contriever itself to retrieve candidate documents before filtering. This risks closed-loop bias, potentially favoring methods architecturally similar to Contriever and inflating gains."}, "questions": {"value": "Could the authors clarify whether TPOUR should be categorized as unsupervised, self-supervised, or weakly supervised?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MZxjRBqlOM", "forum": "wJgaHyJaDD", "replyto": "wJgaHyJaDD", "signatures": ["ICLR.cc/2026/Conference/Submission16758/Reviewer_NS8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16758/Reviewer_NS8F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977581792, "cdate": 1761977581792, "tmdate": 1762926802596, "mdate": 1762926802596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TPOUR proposes a training methodology for time-aware unsupervised retrieval by adding a temporal preference signal (TRPO) on top of MoCo contrastive learning. In particular, queries are paired with (aligned, misaligned) documents drawn from Wikipedia snapshots at different dates; the model is trained to increase similarity to aligned items while decreasing it for misaligned ones. The paper also\nadapts “time vectors” (which record differences between fine-tuned weights on distinct periods) to bi-encoders, enabling interpolation across intermediate timepoints without retraining, and builds a mixture-of-TPOUR classifiers for timestamp prediction. On SituatedQA and RealTimeQA (customized for retrieval) and a BEIR case study, the method reports consistent nDCG@5/10 gains over baselines (Contriever, DPR, Nomic Embed v2 MoE, TimeR4), smooth performance peaks at interpolated α matching evaluation time, and improved year/month\ntimestamp prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a clear and grounded integration of temporal awareness into unsupervised retriever training, since TRPO re-frames preference learning along the time axis while preserving standard contrastive semantics. Despite its simplicity, the idea does not require manual labeling and is reproducible. The usage of time-vector interpolation for bi-encoders is useful in practice. Performance peaks align with test time across years/months, achieving temporal generalization without per-period retraining. The paper presents a good experimental evaluation. In particular, it employs temporal QA with explicit/implicit timestamps and a BEIR analysis indicating temporal sensitivity and enabling specific α by creation date; Results show consistent improvements over baselines. The usage of mixture of retrievers for auxiliary timestamp prediction is effective to encode usable temporal signals (e.g., 76.56% year accuracy vs. 50.18% baseline)."}, "weaknesses": {"value": "Dataset construction for “gold” documents retrieves top-k with Contriever and then filters by answer presence. Such a procedure may lead to evaluation bias, since systems may become similar to the constructor and under represent hard/rare temporal cases. The paper mentions  an alternative retriever check, but it seems to me that it is necessary a stronger, retriever-agnostic construction or multi-constructor consensus to increase trust. The paper needs to be improved regarding statistical analysis and significance. There is no multi-seed variance, confidence intervals, or significance tests across the main tables; sensitivity to random seeds and snapshot choice (details about Wikipedia dump) are not clear. The selection of the α parameter is heuristic (based on known test time) and it is not clear a practical and sound strategy to infer α at inference time without test-time leakage (beyond the separate timestamp-prediction head). A unified, end-to-end approach seems to be necessary. The mixture of TPOUR timestamp predictor is compared to a single-retriever baseline with adjusted classifier parameters, but it still benefits from multiple specialized encoders. It should be necessary to include a capacity-matched or distilled single-encoder control to clarify the source of the gains. The experimental ablations need to be extended beyond λ,  and include, for instance, contrastive-only vs. TRPO-only, with/without interpolation, different negative sampling schemes, varying temporal granularity. Such evaluations would help understanding the contributions of each component.  It seems necessary to look for some external validation, once the experiments focus on Wikipedia-style corpora and QA; it’s unclear how robust TRPO is for bursty and non-encyclopedic domains with  drift (e.g., newswire, finance, code, biomedical preprints)."}, "questions": {"value": "1. How exactly are “aligned/misaligned” pairs sampled to avoid topical confounds (e.g., same entity with different years vs. different entities altogether)? \n2. Can you report multi-seed mean±std and a simple paired significance test for the main nDCG tables?\n3. For α at inference: did you try to use the timestamp predictor to pick α automatically, or a small calibration set to learn α per query/domain?\n4. In the BEIR analysis, please clarify how overfitting is prevented through α tuning per dataset? Is it possible to freeze α by creation year only and still see the trend?\n5. How sensitive are results to the specific Wikipedia dumps/months chosen? Have you tried different adjacent snapshots?\n6. For the mixture-of-retrievers, can you include a distilled single-encoder baseline (knowledge-distill multiple πt into one) so that we are able to separate capacity from temporal specialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LmUouubJVd", "forum": "wJgaHyJaDD", "replyto": "wJgaHyJaDD", "signatures": ["ICLR.cc/2026/Conference/Submission16758/Reviewer_qMsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16758/Reviewer_qMsv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080485387, "cdate": 1762080485387, "tmdate": 1762926802070, "mdate": 1762926802070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}