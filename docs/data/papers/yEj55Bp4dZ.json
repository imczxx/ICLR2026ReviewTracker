{"id": "yEj55Bp4dZ", "number": 22087, "cdate": 1758325773575, "mdate": 1759896887117, "content": {"title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "abstract": "Agentic workflows, where multiple LLM based agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows critically depend on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.", "tldr": "ProRefine, an inference-time method using LLM agent loops to iteratively refine prompts with textual feedback, no extra training or labels. Improves multi-step reasoning by optimizing prompts; helps smaller LLM match larger LLM while reducing cost.", "keywords": ["prompt optimization", "inference-time", "multi llm workflows", "large language models", "textual feedback"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aae73fe588bdba177b1687b0c6aa685ee5b05083.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ProRefine, a per-instance test-time method to perform prompt optimization. The method iteratively enhances a \"task\" LLM with a \"feedback\" LLM, which critiques a partial model output, and an \"optimizer\" LLM, which refines the input prompt for the next iteration. An optional verifier is also employed for early termination. The authors experiment with 3 different Llama-3 variants of different sizes under a feedback and optimizer (and verifier) models on 5 benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The progressive continuation trick (i.e., using $i*k$ tokens per round $i$) seems like an interesting to catch and correct mistakes early in generation; However, the effects of this are not ablated (see weaknesses)\n- ProRefine appears to work decently in a few settings, e.g., +21% on Word Sorting for Llama-3.1-8B."}, "weaknesses": {"value": "- ProRefine relies on larger models to provide feedback and optimize the prompt. \n  - The authors bring up that they are concerned with resource-constrained environments where querying a capable feedback model is feasible. But is this realistic? Under what scenario would a practitioner be able to call a larger more capable LLM on-demand, but *only for feedback*?\n>It is designed for resource-constrained environments where deploying the largest models for every query isn’t feasible, but temporary access to a capable feedback LLM (perhaps via a separate API call) is possible for critical tasks. In such cases, the refinement process is triggered as an on-demand “expert intervention.\"\n  - Authors contend that external costs to optimizers/feedback models should not be considered. However, I do not see any proof of the existence of instances where this feedback/optimizer loop is **not** triggered. In fact, many benchmarks seem to trigger the maximum number of refinement rounds **on average** (e.g., AQUARAT for Llama-3.1-8B). As such, there's no demonstration that such a hybrid model strategy (1) is realistic and (2) actually operates as desired, e.g., only appeals to larger LLMs a small enough portion of the time such that costs can be ignored\n>Each refinement step requires additional calls to the LLM feedback and LLM optimizer , making any single query more expensive to process than a standard single-pass generation. However, this per-query cost should be evaluated within ProRefine’s intended hybrid model deployment. The strategy is not to run refinement on every query, but to use it as an on-demand intervention precisely when a more efficient base model fails.\n  - The authors note that experiments with smaller models (Llama-3.1-8B-Instruct) failed to substantially improve performance, with degradations in some instances. This raises a key question: If ProRefine operates on a per-instance level, and requires multiple model calls to a larger model, why not simply use the larger model for inference? For example, ProRefine+Llama-3.1-70B-as-verifier with Llama-3.1-8B as the task model on GSM8K achieves 88.5%. Llama-3.1-70B self-reports 95.1% (8-shot) performance with CoT. Other papers have reported similar performance with 0-shot prompting (e.g., https://arxiv.org/abs/2504.15253, Table 3). \n- Insufficient baselines: A purported benefit of ProRefine is that it is a training-free, inference-time solution. As such, I believe that it should be compared to other training-free, inference-time methods, such as debate, self-consistency, best-of-N with external verifier. It is important to demonstrate that ProRefine outperforms other methods in this category. \n- Performance improvement given extra compute is not always convincingly better: While there are some instances where ProRefine beats CoT and TextGrad by significant margins, there are others where a lot of inference-time compute is burned without much to show for it. Per Fig 3, ProRefine can make up to 25 rounds of prompt refinement (The max number of rounds set by authors; this indicates that more rounds could be possible if left uncapped). On AQUARAT, this 25 rounds of refinement yields ~2 absolute percentage increase over a vanilla CoT baseline, which requires a single model inference call. This seems like a lot of compute to spend per instance. I am not convinced that the trade-off is worth it\n- Insufficient ablations: Authors do not probe the performance as a function of token chunks $k$ or number of refinement rounds $n$. \n- Questions about potential contamination: The authors report that Llama-3.1-70B achieves 100% accuracy on some tasks, and attribute this to potential data contamination. If authors believe this to be true, why are they comfortable using Llama-3.1-70B as a verifier? If contamination is not the cause, this reinforces my first weakness.\n>In our experiments using Llama3.1-70B-instruct for LLMtask, some experiments yielded an accuracy of 1, suggesting potential data contamination."}, "questions": {"value": "- Do you have a per compute (e.g., FLOPS) comparison between ProRefine and TextGrad (and other inference-time methods)? One can approximate inference time FLOPS using well-known formulas, e.g., like that in https://arxiv.org/abs/2408.03314. As TextGrad \"training\" is purely inference driven, one can quantify total number of FLOPS required to optimize the prompt. It would be interesting to see which methods drive the most improvement per compute spent.\n- Some suggestions: \n  - It appears that Fig 2 repeats information already presented in Table 1. I would replace it with additional experiments, e.g., those suggested above or in weaknesses.\n  - There's some ambiguity in the $k$ token mechanism. My first impression from Fig 1 caption was that all rounds share a continued generation prefix of $i*k$ tokens (\"Each refinement iteration updates the prompt for future tokens; previous tokens remain unchanged.\"). However, the algorithm and examples in the appendix seem to clarify that the entire generation changes iteration by iteration, only the generation is allowed more inference-time tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QX7NDTJ0qI", "forum": "yEj55Bp4dZ", "replyto": "yEj55Bp4dZ", "signatures": ["ICLR.cc/2026/Conference/Submission22087/Reviewer_HKpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22087/Reviewer_HKpp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760743296083, "cdate": 1760743296083, "tmdate": 1762942060376, "mdate": 1762942060376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ProRefine, an innovative method for optimizing prompts during inference in LLMs. It leverages textual feedback generated by LLMs to dynamically refine prompts, enhancing the models' performance on multi-step reasoning tasks without the need for additional training or ground-truth labels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. ProRefine presents a novel approach to prompt optimization during inference, distinguishing itself from existing methods by utilizing LLM-generated textual feedback for dynamic refinement. This innovative use of feedback not only enhances the reasoning capabilities of LLMs but also addresses the limitations of prior techniques that often rely on extensive training data or fixed prompts.\n\n2. The authors provide a comprehensive evaluation of ProRefine across multiple reasoning tasks, demonstrating its effectiveness compared to established baselines like Chain-of-Thought and TextGrad. The results are presented with clarity, supported by empirical evidence that highlights the method's advantages in various scenarios."}, "weaknesses": {"value": "1. The paper does not clearly state the additional cost and latency introduced by the proposed method. Please provide quantitative results or analysis to clarify this aspect.\n\n3. The experiments primarily evaluate relatively small and weak open-source models (mainly LLaMA). It remains unclear whether the conclusions generalize to larger models (e.g., those exceeding 30B parameters) or to more capable closed-source models.\n\n3. I also wonder whether these tasks might already be relatively easy for advanced models. If state-of-the-art models can already perform well on them, how much additional value does the proposed method provide?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ltEp6qon0B", "forum": "yEj55Bp4dZ", "replyto": "yEj55Bp4dZ", "signatures": ["ICLR.cc/2026/Conference/Submission22087/Reviewer_LcqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22087/Reviewer_LcqN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946489122, "cdate": 1761946489122, "tmdate": 1762942060146, "mdate": 1762942060146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ProRefine is an inference-time prompt-refinement method that improves LLM reasoning without any model training or ground-truth supervision. The approach forms a closed loop between three roles: LLMtask, which generates partial outputs; LLMfeedback, which critiques the intermediate reasoning; and LLMoptimizer, which uses this feedback to refine the prompt for subsequent decoding steps. The model generates only a few tokens per step (k=10) and iteratively updates the prompt for up to 25 iterations, leaving previously generated tokens unchanged.\n\nThe authors evaluate ProRefine across five reasoning benchmarks, object counting, word sorting, GSM8K, SVAMP, and AQUARAT. They compare it to zero-shot CoT\tprompting and the supervised TextGrad baseline. Across all tasks, ProRefine yields consistent performance gains, improving accuracy over zero-shot CoT by 3 to 37 percentage points, and outperforming TextGrad in 11 out of 15 model-dataset combinations. The technique shows particularly large improvements on tasks requiring complex manipulation of intermediate outputs, such as sorting, and benefits more as the base model size increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tA training-free, label-free method, ProRefine improves reasoning at inference time with textual feedback, no requirement for fine-tuning; suitable for black-box LLMs. \n\n2.\tAnalysis showing how feedback quality affects performance: Comparing no verifier, verifier, and an optimal verifier shows the method’s upper bound and the centrality of verifier quality; the optimal verifier yields best results most of the times.\n\n3.\tPaper positions ProRefine for on-demand use in hybrid systems and reports that the average number of refinement iterations is low (Fig. 3), supporting practical latency/cost trade-offs."}, "weaknesses": {"value": "The key challenge I have with the paper is that it's not well positioned to the current literature. This makes the novelty of the paper unclear. I would urge the authors to compare related SOTA and even evaluate them against ProRefine. The proposed approach has been applied by previous works and what is unique contribution in this paper is not clear. \n\n1.  The evaluations and comparison to SOTA is very weak. The authors compare to just Textgrad, there are several other prompt optimization techniques such as DsPy, PromptWizard, PromptBreeder, etc. comparison to SOTA and positing the work is missing. \n\n2. Strong dependence on verifier quality: The paper shows the optimal verifier produces the best result in several model×dataset cases, and explicitly notes cases where a real verifier can incorrectly accept a flawed answer and block refinement.\n\n3. Given the reliance of LLM at every stage, t is very critical to compare the cost, latency and tokens of the proposed approach to SOTA. \n\n4. Sensitivity to hyperparameters (k, n) and fixed choices: Method granularity/duration is controlled by tokens-per-step k and max-steps n; the authors fixed k=10, n=25 based on preliminary exploration. Authors fail to discuss variations of these and how these were selected."}, "questions": {"value": "1. Describe and evaluate Prorefine with other SOTA approaches\n2. Clearly provide results on latency, tokens used, API calls of prorefine.\n3. Discuss hyper parameters used and show ablations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JysroMoNCi", "forum": "yEj55Bp4dZ", "replyto": "yEj55Bp4dZ", "signatures": ["ICLR.cc/2026/Conference/Submission22087/Reviewer_GGQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22087/Reviewer_GGQH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001057786, "cdate": 1762001057786, "tmdate": 1762942059819, "mdate": 1762942059819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}