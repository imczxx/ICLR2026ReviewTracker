{"id": "EQ3TwO84Cs", "number": 15367, "cdate": 1758250640697, "mdate": 1759897311344, "content": {"title": "Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework", "abstract": "LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects.", "tldr": "", "keywords": ["LLM-based Agents", "Memory", "Information Retrieval"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cd860b80d950a013b5d6c176c25a447bea98605.pdf", "supplementary_material": "/attachment/7445bce7e795c8a90451859162cb2aabe85b3b2a.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an adaptive memory framework with a learnable memory cycle to support the decision-making process of LLM-based agents. By modeling the continuous interactions between agents and environments as an MDP, the authors define memory storage, retrieval, and utilization as parameterized policies to optimize. For memory retrieval, they design a MoE gate function to activate different metrics. For memory utilization, they use training datasets to optimize LLMs. For memory storage, they employ SFT and DPO to optimize the parameters of LLMs. Finally, after off-policy optimization of model parameters, they further conduct the on-policy optimization with online learning to avoid distribution shifts. Experiments and ablation studies on three datasets with various difficulty levels demonstrate that this adaptive memory framework helps better agent-based decision-making with different LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation of learning to memorize is a promising direction for LLM-based decision making.\n2.\tThe proposed memory cycle effect is a good concept that the memory storage, retrieval, and utilization procedures influence each other.\n3.\tThe authors test their framework with different tasks and LLMs. The ablation study also demonstrates the effect of each part."}, "weaknesses": {"value": "1.\tThis whole framework is quite complex while the benefits of learnable memory are not significant enough for GPT-4o-mini and Qwen-2.5.\n2.\tMany details such as design choices are missing in the context. For example, why do the authors use the Bernoulli distribution for the stop signal in memory utilization?\n3.\tIt seems that the training of this adaptive framework is costly, but no details are given."}, "questions": {"value": "1.\tHow many metric functions do you use for each task? How do these metrics influence the performance of your method?\n2.\tCould the authors compare the computation efficiency of each method?\n3.\tHow are the training datasets in memory utilization optimization constructed?\n4.\tIt seems that the LLM should also be optimized. Is there any risk of the LLMs memorizing the answers to the questions?\n5.\tWhy does your method have quite different performance on GPT-4o-mini or Llama-3.1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jTdn8OE6B3", "forum": "EQ3TwO84Cs", "replyto": "EQ3TwO84Cs", "signatures": ["ICLR.cc/2026/Conference/Submission15367/Reviewer_mHev"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15367/Reviewer_mHev"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607082268, "cdate": 1760607082268, "tmdate": 1762925651473, "mdate": 1762925651473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to build, retrieve from, and utilize memory for question answering tasks. Experiment results show certain improvements with open-source models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Complex Method Design with Concrete Problem Formulation.**\n> The paper introduces all main components in the pipeline and presents mathematical formulations for them."}, "weaknesses": {"value": "**1. Lack of Novelty in the Proposed Method.** \n> Unlike what’s stated in the paper, there have been many works that consider the “memory cycle effect” by inducing, verifying, retrieving, and using memory entries [1,2]. Beyond covering non-paramatric update approaches as what’s being proposed in this work [3], some works also explore parametric updates [4]. There are many memory-related works besides what’s referenced in this comment. That being said, it is unclear what is the unique method this work proposes.\n\n**2. Unclear Empirical Improvement.** \n> From the results in Table 1, the proposed method only improves with open-source qwen and llama models (also unclear on the size), but underperforms existing methods with stronger GPT models. Therefore, it is unclear if the proposed method is effective and scales to future developments, especially as the backbone LMs keep getting stronger.\n\n**3. Limited Coverage in Benchmarks.** \n> This paper only experiments with one dataset, HotpotQA, which may not sufficiently cover the field of question-answering tasks. Yet, this paper attempts to claim its findings on “agents”, which is even less covered by this one dataset. Expanding the experiments to greater numbers and more agentic benchmarks would be helpful to address this concern. \n\n[1] Wang, Zora Zhiruo, et al. \"Agent workflow memory.\"\n\n[2] Packer, Charles, et al. \"MemGPT: Towards LLMs as Operating Systems.\" (2023).\n\n[3] Chhikara, Prateek, et al. \"Mem0: Building production-ready ai agents with scalable long-term memory.\"\n\n[4] Wang, Yu, et al. \"Memoryllm: Towards self-updatable large language models.\""}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qe2BgBVrV1", "forum": "EQ3TwO84Cs", "replyto": "EQ3TwO84Cs", "signatures": ["ICLR.cc/2026/Conference/Submission15367/Reviewer_2UtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15367/Reviewer_2UtV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421098183, "cdate": 1761421098183, "tmdate": 1762925651103, "mdate": 1762925651103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper learns a full memory policy for LLM agents across the retrieve,  utilize and store cycle: retrieval uses a MoE gate to weight relevance, utilization learns how to aggregate memories with a learned stopping rule, and storage uses task-specific reflection. The whole loop is trained with off-policy objectives plus an on-policy phase to reduce distribution shift."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) A learned MoE gate weights relevance/recency/importance/emotion per query instead of fixed cosine-only ranking.\n\n(2) Storage uses learned reflection to write task-specific memories, improving future retrieval precision.\n\n(3) An on-policy phase realigns retrieval/usage/storage with the agent’s own trajectories, reducing distribution shift and stabilizing the full loop."}, "weaknesses": {"value": "1. In terms of the novelty, feeling more like a solid systems integration than a conceptual leap.\n\n2. Main tables lack significance test, hard to judge variance, especially on easier splits.\n\n3. Results are strongest on HotpotQA. A compact test on a different agent task (e.g., web-acting, tool use) would help generality.\n\n4. Off-policy data construction and the supervision sources for SFT/DPO could be described more precisely to assess bias.\n\n5. This paper suggests naive combinations can degrade before on-policy tuning. I think more concrete guidance to stabilize joint training would be needed."}, "questions": {"value": "1. Can you add significance tests for the main metrics and the average steps/trajectory?\n\n2. What’s the exact policy for off-policy data collection and how is it mixed with on-policy during training?\n\n3. Do you have visualizations of MoE gate weights over states (when recency vs. importance vs. emotion dominates)?\n\n4. Could you share a stability way that avoids degradation when combining procedures without full on-policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yz74UHmGDc", "forum": "EQ3TwO84Cs", "replyto": "EQ3TwO84Cs", "signatures": ["ICLR.cc/2026/Conference/Submission15367/Reviewer_QCNj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15367/Reviewer_QCNj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963117310, "cdate": 1761963117310, "tmdate": 1762925650616, "mdate": 1762925650616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive approach that enables an LLM-based agent to learn what should be remembered when solving a task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is relatively easy to read.\n2. It addresses an important problem - memory mechanism in LLM-based agents.\n3. A large number of diverse baselines are used in the experiments."}, "weaknesses": {"value": "1. The Related Works section, especially the part on reinforcement learning, is too general. The purpose of such sections is to position the presented work relative to the most relevant existing studies, highlighting its novelty and significance. General information should instead be placed in the Background section.\n2. The presentation of results in Table 1, split into two blocks for each dataset, is not convenient for readability. It would be better to display all results in a single block.\n3. It is not entirely clear how the trajectories for training were collected and how high their quality is.\n4. Only non-trainable methods are used as baselines, which makes it seem inappropriate to compare a trainable method against them.\n5. It is questionable whether the formulation \"the memory cycle effect\" can be considered a contribution of the work. Agents with memory have been widely studied in reinforcement learning [1, 2, 3], and this formulation seems limited to the case of an explicit memory bank, whereas memory can also be stored in the hidden states or tokens of the model [4, 5].\n6. The first and third contributions essentially describe the same result and should be combined.\n7. The equations in the text are not numbered\n\nI am willing to revise my evaluation if the shortcomings of the work are addressed.\n\n**References:**\n1. Lampinen, Andrew, et al. \"Towards mental time travel: a hierarchical memory for reinforcement learning agents.\" Advances in Neural Information Processing Systems 34 (2021): 28182-28195.\n2. Ni, Tianwei, et al. \"When do transformers shine in rl? decoupling memory from credit assignment.\" Advances in Neural Information Processing Systems 36 (2023): 50429-50452.\n3. Cherepanov, Egor, et al. \"Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation.\" arXiv preprint arXiv:2412.06531 (2024).\n4. Morad, Steven, et al. \"Reinforcement learning with fast and forgetful memory.\" Advances in Neural Information Processing Systems 36 (2023): 72008-72029.\n5. Cherepanov, Egor, et al. \"Recurrent action transformer with memory.\" arXiv preprint arXiv:2306.09459 (2023)."}, "questions": {"value": "1. How appropriate is it to compare the proposed approach with non-trainable methods?\n2. How much computational resources are required to train the model on a single task?\n3. If an agent is trained in one environment, how well do the results transfer to another environment? For example, one with a similar structure but different actions and objects?\n4. How does the addition of the emotional relevance metric affect the results? How specific is this metric to the datasets used, and how can it be adapted to other tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WxyutrJ8ll", "forum": "EQ3TwO84Cs", "replyto": "EQ3TwO84Cs", "signatures": ["ICLR.cc/2026/Conference/Submission15367/Reviewer_QuZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15367/Reviewer_QuZ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996964490, "cdate": 1761996964490, "tmdate": 1762925649954, "mdate": 1762925649954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}