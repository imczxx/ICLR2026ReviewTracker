{"id": "v3VJXMI102", "number": 22500, "cdate": 1758331968926, "mdate": 1763714029447, "content": {"title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "abstract": "A long-standing research problem in Artificial Intelligence (AI) is to produce systems that can successfully solve any possible task. A key requirement in addressing progress in that direction is a near-infinite suite of tasks for benchmarking AI solutions. In contrast, current evaluation methods available to AI researchers in representation learning typically rely on a fixed collection of hand-picked downstream benchmarks. Hence, a large amount of effort is put into designing and searching for large collection of evaluation tasks that can serve as a proxy of our grand goal. We argue that such a rigid evaluation protocol creates a structural bottleneck in AI research. To remedy that, we define a probability distribution over downstream tasks -- Task Priors. Under this view, one can evaluate a model's performance over the set of all possible downstream tasks. Our framework is the first to provide answers to key questions such as (i) what is the average performance of my model over all possible downstream tasks weighted by the probability to encounter each task? or (ii) what is the variance of my model’s performance across all downstream tasks under the defined Task Priors? Beyond establishing a new standard for evaluation, we believe that Task Priors will accelerate the pace of research in representation learning -- where downstream task evaluation is generally the sole signal that researchers have access to.", "tldr": "", "keywords": ["Representation Learning", "Model Evaluation", "Downstream tasks", "Probabilistic Methods", "Kernel alignment", "Self-supervised learning", "Linear probes", "Benchmarking", "Generalization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edadc498e5e6e46a603e10b1abdc164694d753df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles the problem of evaluating pre-trained backbone models on a variety of downstream tasks. It addresses the limitation of current evaluation protocols that rely on a small, fixed set of benchmarks, which fail to capture the full range of real-world applications. The paper proposes a proxy evaluation framework that models downstream tasks as samples from a probability space — modelling a Task Prior distribution based on a feature kernel to compute expectation and variance of the model performance without retraining new classifiers or build more benchmarks.\n\nFrom my understanding, the authors frame downstream tasks as training linear classifiers on top of pre-trained representations $f(X)$ to predict target variables. Tasks are defined by the label graph $G = Y^{T}Y$, where $Y$ represents the target labels of a specific dataset. Each label graph $G$ is treated as a task, and the space of tasks is represented as a collection of $G$ matrices, weighted by the feature kernel $K$ derived from $f(X)$. A Gibbs distribution is imposed on this task space, serving as the Task Prior, which facilitates natural inference and sampling based on established theoretical results. The performance of a task is quantified as the loss metric evaluated based on Eq. (2). By applying the Gibbs distribution over the space of label graphs, the authors derive tractable expressions for the expected performance and variance of performance from using the pre-trained representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of tractable evaluation of model performance on diverse new tasks is very interesting and significantly useful to me. The method is elegantly simple with strong theoretical motivation and empirical results to substantiate the proposed claims."}, "weaknesses": {"value": "1) While the introduction is clear, I find the presentation of the main results in Section 2 is extremely difficult to follow, which requires me to go back and forth multiple times to finally understand the definition of tasks and performance as well as the motivation of using the label graphs and feature kernels. \n\n2) The paper would have been easier to follow if the scope of the tasks and the key concepts (as mentioned above) could be explicitly explained somewhere at the beginning. \n\n3) Furthermore, the task definition in this work is limited to tasks whose target variables lie within the same space and on a fixed input dataset. If it is a classification task, the formulation requires the same number of classes. \n\n4) For a large dataset, the computation of the kernel matrix is incredible challenging, which hinders applicability of the proposed approach, yet remains undiscussed in the current paper."}, "questions": {"value": "1) Can the framework model diverse classification tasks across different domains (e.g., image classification vs. visual question answering with multiple-choice settings) together, or is it limited to tasks with similar characteristics?\n\n2) How are the label graph and feature kernel computed efficiently for large datasets in practice? \n\n3) Could the proposed method handle more complex modality such as texts and videos? \n\n4) In Theorem 2.6 (lines 216-226), how is this additional kernel matrix $M$ defined and designed? Can any matrix based on any similarity metric or kernel be used for the computation, or what properties should $M$ satisfy to be used in Eq. (6)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DXbqQO2Txs", "forum": "v3VJXMI102", "replyto": "v3VJXMI102", "signatures": ["ICLR.cc/2026/Conference/Submission22500/Reviewer_rciQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22500/Reviewer_rciQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632500672, "cdate": 1761632500672, "tmdate": 1762942246022, "mdate": 1762942246022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Task Priors, a novel framework for evaluating pretrained models by defining a probability distribution over downstream tasks, rather than relying on fixed benchmark datasets. It defines a Gibbs distribution over label graphs that captures the likelihood of different downstream tasks based on kernel similarity, offering a probabilistic view of evaluation. Empirical applications validate the proposed metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper redefines model evaluation as a probabilistic process over all possible tasks, providing a fresh and mathematically principled alternative to the static-benchmark paradigm. \n- Strong theory that establishes a connection between supervised and self-supervised objectives via kernel alignment and trace formulations. \n- The framework unifies representation-based and task-based evaluation, potentially becoming a standard for fair model comparison."}, "weaknesses": {"value": "- Although the method is domain-agnostic in principle, experiments and formulations are focused solely on classification; it’s unclear how Task Priors extend to retrieval, regression, or generative tasks. \n- The framework heavily relies on the definition of kernel similarity. Sensitivity to kernel type, temperature $T$, and feature normalization could affect reliability. \n- No guarantee is provided that Task Priors predict performance in unseen domains."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Njt70vjtSt", "forum": "v3VJXMI102", "replyto": "v3VJXMI102", "signatures": ["ICLR.cc/2026/Conference/Submission22500/Reviewer_RNUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22500/Reviewer_RNUm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636153241, "cdate": 1761636153241, "tmdate": 1762942245678, "mdate": 1762942245678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework that moves beyond traditional representation learning evaluation approaches. Rather than assessing models on a fixed set of downstream tasks, the authors introduce the concept of task priors, a probabilistic representation of the space of all possible downstream tasks that a model might encounter."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Significance: This work, if clarity issues mentioned below are resolved, can have a broad impact on representation learning so that the standard linear probing evaluation protocol becomes unnecessary.\n2. Originality is hard to evaluate due to limited domain expertise, see clarity and quality concerns below."}, "weaknesses": {"value": "1. Motivation: I wonder why training linear probes for evaluation is a computational bottleneck that we have to avoid since linear probes are quite cheap to train (compared to the time to build the representation) and often done in a few shot manner.\n2. There are a few clarity issues about the methodology that make it hard to evaluate the quality of this paper, see questions below."}, "questions": {"value": "1. Can you add legend to Figure 1 and explain how 9 smaller figures are the right probabilistic?\n2. [Important] I wonder if authors can further give examples for what is G and what is K in the standard feature representation (supervised at least) learning context (e.g., do we need labels for downstream tasks? If yes, we still need the manual collection of downstream datasets.), and which G and K are used to generate each datapoints in Figure 4. This can help readers understand your procedure better and how to use your method in practice, especially given that you have almost 1 more page. Besides, please double check the paper and use the fixed notation for matrices such as $\\mathbf{G}$ instead of $G$. \n3. For Theorem 2.6, why do we introduce this new kernel matrix M and what is its relationship with K? Why is Tr(MG) related to the linear classifier performance, not Tr(KG)? Also please fix typos for theorem references (line 140-141).\n4. What’s the relationship between V and Z? In 293, can you explain why we can achieve a speedup?\n5. Can you explain how Algorithm 1 is used exactly to compute the mean and variance term after sampling labels?\n6. Can you compare the time you need to compute linear probes for each dataset vs. time using your method?\n7. Can you explain the relationship of your paper to “Provable guarantees for self-supervised deep learning with spectral contrastive loss” by Haochen et al. (2021), which proposes a graph-based spectral loss and provides theoretical analysis based on linear probe generalization error?\n8. Seems like this is one important assumption: “the performance on a hand-curated collection of downstream tasks should follow the distribution implied by the Task Prior.” How bad can the estimation be if this assumption is violated and how to enforce it in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VwtDIFP3RH", "forum": "v3VJXMI102", "replyto": "v3VJXMI102", "signatures": ["ICLR.cc/2026/Conference/Submission22500/Reviewer_JFBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22500/Reviewer_JFBG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881674807, "cdate": 1761881674807, "tmdate": 1762942245459, "mdate": 1762942245459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current model evaluation relies on finite, hand-curated benchmark suites (e.g., ImageNet, GLUE, MTEB), which capture only a small subset of real-world tasks. As the diversity of downstream applications grows, this fixed-benchmark paradigm forms a structural bottleneck for assessing generalization. The paper proposes a probabilistic framework to replace discrete benchmarks with continuous task distributions. To achieve this, the authors show that absolute evaluation metrics, such as classification accuracy, are equivalent to relative evaluation metrics (sample similarity). For the relative metric, the knowledge of the similarity graph suffices for evaluation. So the authors define downstreams tasks by similarity graphs and propose a way to define distributions over tasks as a distribution over graphs. Given this \"task prior\", the authors provide efficient ways to evaluate the mean performance and the variance of performance of any given representation. The authors also provide a method for sampling downstream tasks from the prior and an SSL-inspired approach to defining task priors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method connects supervised, self-supervised, and kernel-alignment evaluations under a single theoretical lens via Theorem 2.3. The authors provide efficient methods for evaluating the mean and variance of downstream performance with respect to the proposed task prior. \n\nThe paper provides good empirical coverage of experiments on diverse architectures (CLIP, SigLIP, BLIP, DinoV2). The experiments show a strong correlation between the calculated metric and the linear probe, and with real benchmark outcomes."}, "weaknesses": {"value": "The core mathematics (e.g., Gibbs measure, kernel alignment, HSIC) builds directly on established theory, albeit with a fresh interpretation for model evaluation. The effect of using different temperatures or selecting the optimal temperature is not described in detail. It would be great to provide a sequence of assumptions that motivate the effectiveness of using a Gibbs distribution to define the task prior. Currently, the prior task is very heuristic, as it is motivated by a metric that should be correlated with the mean of downstream performance. It is unclear why the variance calculated under the proposed task prior should be indicative of the variance in performance across downstream tasks. \n\nThe need to select a kernel for the task prior shifts the problem without addressing it. Results can vary depending on which model’s kernel is used as the Task Prior (e.g., DinoV2 vs. SigLIP). The implications of this bias are not fully explored. The SSL kernel does not seem very useful on its own. If we end up using a large foundation model to define the kernel, we can directly compare representation similarities by checking kernel similarities between representations. The task prior distribution (Gibbs distribution) is not very well motivated. \n\nDoes the additional step of defining the task before computing expectations improve the correlation with downstream empirical performance?\n\nThe framework also needs access to full kernel matrices (O($n^2$) memory), which becomes impractical for very large datasets."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EPbDDI9xgk", "forum": "v3VJXMI102", "replyto": "v3VJXMI102", "signatures": ["ICLR.cc/2026/Conference/Submission22500/Reviewer_jKi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22500/Reviewer_jKi3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994199839, "cdate": 1761994199839, "tmdate": 1762942244458, "mdate": 1762942244458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}