{"id": "sGiE05Dc0v", "number": 22338, "cdate": 1758329741384, "mdate": 1759896871784, "content": {"title": "Recast Your Input via a Mapping Function for Alignment", "abstract": "Alignment is promoting its critical role among the large language model (LLM) scenarios, which ensures safety, controllability, and trustworthiness of the generation. The popular alignment methods, that is, reinforcement learning from human feedback (RLHF), direct preference optimization (DPO) and such series, usually change weights of the model by elaborate algorithm. Nevertheless, they suffer from the compute drain for training, especially when the parameters' size getting huge. Worse still, people typically do not have access to the weights of the SOTA models, such as GPT-4, which consequently renders the aforementioned algorithms unimplementable. In this paper, we propose to employ a separate LM as the Refiner, an input mapping function essentially, to transform the original query into a novel formulation that impels the final generation to align with the expectations. During optimization, an evolution strategy, namely CMA-ES, is leveraged to fine-tune the LM with linkage to the generation model. We conduct extensive experiments on various refiner and generation types, and achieving surpassing results.", "tldr": "", "keywords": ["alignment", "input refiner", "CMA-ES", "posterior regularization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e237697547ac827c4f1aebb3964d51c91f7db8bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using an additional Language Model (LM) as a Refiner to improve the input, which ultimately leads to outputs with better alignment. Experimental results have demonstrated the effectiveness of this method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method reduces parameters requires training, which is more efficient for preference alignment.\n2. The effectiveness of the proposed method is validated through experiments."}, "weaknesses": {"value": "1. The approach bears a strong resemblance to existing methods that improve the performance by refining the prompt, which raises concerns about the novelty of the paper.\n2. Ablation studies suggest that the impact of the CAM-ES module is marginal or not statistically significant."}, "questions": {"value": "While the parameter efficiency is a stated advantage, the paper lacks a theoretical justification for a key finding: why does this method outperform full fine-tuning approaches like DPO and BPO? Especially since your experiments seem to confirm this counter-intuitive result, I'm confused about this experimental result."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hmIp0KnSW", "forum": "sGiE05Dc0v", "replyto": "sGiE05Dc0v", "signatures": ["ICLR.cc/2026/Conference/Submission22338/Reviewer_wsCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22338/Reviewer_wsCV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926441665, "cdate": 1761926441665, "tmdate": 1762942174932, "mdate": 1762942174932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper under review proposes a novel approach to align large language models (LLMs) with human preferences by introducing an input refiner module. This module employs a latent variable to transform the original input into a refined version that better aligns with the desired output. The method utilizes the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to optimize the refinement process, ensuring that the generated responses meet expectations without requiring access to the model's internal parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The introduction of an input refiner using latent variables and CMA-ES is a novel contribution that addresses the limitations of existing alignment methods, particularly those requiring access to model weights. By avoiding direct manipulation of model parameters, the proposed method reduces computational overhead, making it feasible for use with state-of-the-art models like GPT-4. The method is adaptable to different LLMs and can be integrated with various generation models."}, "weaknesses": {"value": "1. The concept of alignment through refining prompts is not new and somewhat outdated, as it has been extensively explored in previous literature. The authors should provide a more comprehensive discussion on how this paper distinguishes itself from prior works [1].\n\n2. Though the LLM can only be a black-box module for some closed-source LLMs like GPT-4, as you can revise its input prompt, you can also revise its output response for alignment. Doing them together could be more effective than only doing it in the input side [2].\n\n3. Posterior regularization can conflict with alignment goals if the output y is not aligned with the input x. This misalignment can introduce bias in optimizing the learning of z in Eq. (8). The authors might consider using a preference model to decide whether to apply regularization with y. The current results are not convincing and should be evaluated against more complex alignment benchmarks, such as diverse preference sets.\n\n4. The authors should provide a more detailed explanation in Section 3.1, particularly regarding Eq. (5), which lacks rigor. Variables z and xâ€² should be sampled from distributions rather than being deterministically projected.\n\n\n[1] A systematic survey of prompt engineering in large language models: Techniques and applications.\n\n[2] EFFICIENT LLM ALIGNMENT VIA HIERARCHICAL COARSE-TO-FINE REFINEMENT."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vRkTSuMZfX", "forum": "sGiE05Dc0v", "replyto": "sGiE05Dc0v", "signatures": ["ICLR.cc/2026/Conference/Submission22338/Reviewer_xkVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22338/Reviewer_xkVV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967091550, "cdate": 1761967091550, "tmdate": 1762942174341, "mdate": 1762942174341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to align black-box large language models (LLMs) by learning a \"mapping function\" to \"recast\" the user input. This function is optimized using the CMA-ES algorithm to steer the model towards more aligned responses.\n\nWhile the paper addresses an important and timely problem, its execution suffers from significant flaws in clarity, methodological explanation, and experimental validation, making it difficult to assess the true merit of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper tackles a highly relevant and challenging problem: the alignment of proprietary, black-box LLMs where access to model weights is not available.\n+ The basic idea of refining the prompt using a small LLM, like 1b/3b, seems to be practical and valuable."}, "weaknesses": {"value": "+ The paper is quite difficult to follow. The writing is disorganized, and key concepts are not introduced clearly. Notations are not defined in the proper place. The methodology section, along with Figure 2, is nearly incomprehensible.\n+ The core method is not adequately explained. Algorithm 1 is presented as the main framework but lacks a clear, step-by-step textual explanation to accompany it. The \"mapping function\" itself is not clearly parameterized, making it hard to understand what is being optimized. The paper would greatly benefit from a simple, concrete \"with/without\" example to help the reader build intuition for what the input refiner is doing.\n+ The figures and tables are not up to publication standards.\n  - Figure 2 is low-resolution and difficult to read.\n  - Tables are populated with a large number of raw scores but lack clear summary statistics (e.g., averages, confidence intervals). This makes it very difficult to interpret the results or draw conclusions.\n+ The experimental setup and analysis are insufficient.\n  - The proposed method fails to outperform the `Best-of-N` baseline, a key result that is not discussed.\n  - More troublingly, the tables show the *original* base model performing better than both BPO and DPO. This is a highly unusual result that suggests a fundamental problem with the baseline implementations, yet it is presented without comment.\n  - The very low \"tie\" ratio in the pairwise comparisons is also not analyzed, as it does not always appear in alignment papers.\n  - The results are evaluated using \"gpt-4o-turbo\" as an LLM-as-a-judge, which may not be strong enough for the evaluation.\n  - With the counterintuitive results, some experimental setups are missing. For instance, the paper uses DPO as a baseline but provides no information on how it was trained."}, "questions": {"value": "How do the authors choose the model for experiments? In the paper, llama 3, llama 3.2, mistral v0.3, and qwen 2.5 with different model sizes are included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0umG8bXVZR", "forum": "sGiE05Dc0v", "replyto": "sGiE05Dc0v", "signatures": ["ICLR.cc/2026/Conference/Submission22338/Reviewer_jAdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22338/Reviewer_jAdW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010550456, "cdate": 1762010550456, "tmdate": 1762942173950, "mdate": 1762942173950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for LLM alignment that bypasses traditional parameter-updating approaches like RLHF and DPO. The core idea is to use a separate \"Refiner\" model that transforms user queries into refined inputs that better align with desired outputs from black-box generation models. The method employs CMA-ES (Covariance Matrix Adaptation Evolution Strategy) to optimize the refiner's behavior based on feedback from the generation model, combined with posterior regularization using preference data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Practical problem setup that addresses the difficulty of finetuning closed-source models\n* Novel viewpoint of alignment by modifying inputs to achieve the alignment goal. The latent variable approach is also generally interesting."}, "weaknesses": {"value": "* Limited interpretation of latent variables. The paper claims it represents \"user preference with diversity\" and \"reasoning paths,\" but provides no theoretical or empirical evidence to support this.\n* Presentation and Clarity Issues. Figure 2 is confusing to understand given the fact that math notions appears much later.\n* Using only 256 samples for CMA-ES optimization seems insufficient for robust optimization, especially given CMA-ES's sample complexity. It would be good to have variance (or similar stuff) reported.\n* Technical confusion. Please see questions."}, "questions": {"value": "* The constraint function B(.) seems crucial but is barely explained. How was this specific form derived?\n* Given the uncertainty of latent variable z, why is the latent variable z necessary? Can we get rid of z?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LAZICL10uP", "forum": "sGiE05Dc0v", "replyto": "sGiE05Dc0v", "signatures": ["ICLR.cc/2026/Conference/Submission22338/Reviewer_TdUh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22338/Reviewer_TdUh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141610080, "cdate": 1762141610080, "tmdate": 1762942173562, "mdate": 1762942173562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}