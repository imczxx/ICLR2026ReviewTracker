{"id": "JLCTd00wr8", "number": 19289, "cdate": 1758295089805, "mdate": 1759897047699, "content": {"title": "Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models", "abstract": "Guard models are critical for ensuring the safety of large language model (LLM) outputs, yet they remain vulnerable to superficial linguistic variation. We show that semantically equivalent paraphrases can cause large fluctuations in guard model safety scores, revealing a lack of semantic grounding. To address this, we introduce a two-stage framework: (1) a paraphrasing-based evaluation protocol that quantifies semantic robustness, and (2) a robust training strategy that enforces paraphrase consistency through self-supervised regularization. Our method constructs paraphrase sets for each response, computes a conservative set-level target probability via a skew‑aware estimate, and applies parameter-efficient fine-tuning to align guard model predictions across these variants.  Our approach reduces rewording‑induced variability and even improves benchmark accuracy, whereas naive targets such as the mean or median can degrade accuracy in our ablations. These results motivate treating semantic robustness as a first‑class objective and offer a practical, parameter‑efficient recipe for guard models that prioritize meaning over surface form.", "tldr": "We diagnose guard models' sensitivity to harmless, non-semantic rewording and propose a self-supervised, parameter-efficient training method that enforces paraphrase consistency, cutting label flips by up to 60% without hurting accuracy.", "keywords": ["guard models", "safe guardrails", "superficial alignment", "semantic robustness", "Large language models (LLMs)"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be891da3cfdbde751111d6f2acc82b1c6124cadc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study addresses a key vulnerability in LLM guard models, which are designed to assess the safety of model-generated outputs: their predictions are highly sensitive to meaning-preserving paraphrases. To mitigate this issue, the authors propose a self-supervised learning approach based on consistency regularization. Specifically, they compute safety scores across multiple paraphrased responses generated by an LLM, then aggregate these scores using a skewness-aware method to derive a robust target. The guard model is trained to align individual scores with this representative value. Experimental results show that the consistency-trained guard models exhibit significantly lower label-flip rates and improved performance on the BeaverTails benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* This paper clearly exposes a weakness in current LLM safety classifiers: their predictions are highly sensitive to paraphrased inputs that preserve the original meaning. By showing that even confident safety labels frequently flip when only superficial rewording is applied, the authors reveal that these models rely more on surface-level cues than on actual semantic understanding.\n* The authors introduce a practical self-supervised training method that improves semantic consistency across meaning-preserving paraphrase sets. By combining a novel skew-aware percentile aggregation strategy with LoRA fine-tuning, the method enforces prediction stability without the need for additional labels or large-scale retraining.\n* The proposed approach not only enhances robustness but also improves overall classification accuracy and calibration. It demonstrates that improving semantic consistency can lead to more reliable and precise guard models."}, "weaknesses": {"value": "* The claimed contributions of this paper are relatively weak and somewhat overstated. For instance, using paraphrased sets to evaluate semantic robustness through label flip rate is intuitive and has been implicitly employed in prior studies, making it difficult to justify as a novel contribution. Furthermore, to convincingly demonstrate the effectiveness of the proposed method, the paper requires broader experiments on diverse benchmarks, along with deeper ablation studies to validate the practical applicability of its design choices.\n* There is insufficient engagement with prior work that has already identified similar robustness issues in safety classifiers. Studies such as Bespalov et al. (2023) and Achara & Chhabra (2025) have shown that both LLMs and guard models are highly sensitive to meaning-preserving rewordings. However, the current paper lacks a detailed discussion on how its approach differs from or improves upon these earlier findings, leaving the novelty of its observations and techniques underspecified.\n  * Achara and Chhabra (2025), Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers, NAACL\n  * Bespalov et al. (2023), Towards Building a Robust Toxicity Predictor, ACL\n* The experimental setup is under-documented, and the interpretation of results lacks depth. Key details, such as how the training, validation, and test splits are structured, or how many paraphrase sets are used, are omitted entirely. Moreover, while the paper promotes skew-aware aggregation as a core contribution, it fails to explain why it underperforms compared to median aggregation in some cases (as seen in Table 3). There is also no principled justification for choosing specific quantile values, such as using the 40th percentile under symmetry.\n* The range of baseline methods is too narrow to fully validate the effectiveness of the proposed approach. The paper only compares different aggregation strategies under a fixed L1 loss, without considering alternative robustness techniques such as adversarial paraphrase augmentation, contrastive consistency losses, or distributional smoothing. Without these broader comparisons, it is difficult to assess how the method truly compares with other well-established strategies designed to improve semantic robustness."}, "questions": {"value": "* Have you explored whether full fine-tuning yields different results compared to LoRA?\n* You mention using manually verified paraphrase sets referred to as \"controlled paraphrase sets\". Could you clarify how these are used in your experiments: are they part of training, evaluation, or both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "grOKpj8d48", "forum": "JLCTd00wr8", "replyto": "JLCTd00wr8", "signatures": ["ICLR.cc/2026/Conference/Submission19289/Reviewer_6ZjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19289/Reviewer_6ZjF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654182620, "cdate": 1761654182620, "tmdate": 1762931245375, "mdate": 1762931245375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to address a persistent problem in LLM safeguards: semantic perturbations that occur when the output is slightly changed. The work presents an adequate solution, and the overall setup is easy to follow."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. The proposed setup improves upon existing open-weight safeguard models rather than proposing a new one. This makes it easy to integrate the finetuned model into the existing pipeline. \n2. The authors examine multiple aggregation methods for bringing together paraphrased clusters and their ratings and observe that mean aggregation can sometimes overfit to the safe mode, whereas skewness-aware aggregation provides better balance."}, "weaknesses": {"value": "Weaknesses:\n1. It is not clear from the current experimental section how many paraphrases per sentence are generated, and how many of them are retained/rejected in  the constrained set. If the LLm-as-judge checking is not performed, then what is the quality of the samples generated? This is important to discuss as the aggregation method can be randomly effective or ineffective over a small sample size. \n2. More explanation and grounding are needed on how the percentile values are set for thresholding and how the conservative and optimistic biases map to this. Is there any related literature on this, or are these arbitrarily chosen? A more thorough explanation of this setup is needed. \n3. What if we shift the percentiles? Unlike mean and median, skewness seems to depend on these percentile parameters, which, if shifted, will change the flip rate. A thorough ablation of how these ranges are decided and how, if at all, they can dynamically change in each training epoch needs to be studied. \n4. In line with the previous comment, it is also essential to not just report benchmark accuracy but also precision and recall so that readers and practitioners can make better judgments of which technique and thresholding works on which safeguard. \n5. As the loss function tries to bring together semantically similar clusters, it is essential to test existing semantic loss baselines such as triplet loss and magnet loss. They were previously employed to improve the detection of implicit hate samples that are semantically similar to neutral ones and can serve as baselines for the proposed loss function (Eq. 1).  \n6. How were the human-authored paraphrases curated? How many samples were these? It is not until one sees from the examples in the appendix what refusal and agreement styles mean; this needs to be made clear in the main text."}, "questions": {"value": "While I am aware of Toxigen, I am not familiar with the Beavertails dataset. In terms of the data distribution, how similar/different are Toxigen and BeaverTails? How fine-grained are the human-annotated safety labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aR3Hzrhwr9", "forum": "JLCTd00wr8", "replyto": "JLCTd00wr8", "signatures": ["ICLR.cc/2026/Conference/Submission19289/Reviewer_H1mF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19289/Reviewer_H1mF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683034606, "cdate": 1761683034606, "tmdate": 1762931244935, "mdate": 1762931244935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper primarily focuses on a crucial aspect of existing guard models. It investigates whether these models truly comprehend the underlying meaning of a text or merely rely on superficial sentence structures when making judgments. The authors demonstrate that such models are highly vulnerable to paraphrasing, as they often produce inconsistent results even when the underlying semantics remain the same. To address this issue, the authors propose a self-supervised training strategy designed to mitigate these inconsistencies and enhance the robustness of guard models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The overall presentation of the paper is clear, well-structured, and easy to follow.\n\n- The authors address an important challenge i.e. improving the robustness of guard models against paraphrased text.\n\n- The proposed method is simple yet effective, and the detailed explanation of how to select an appropriate set-level target greatly aids in understanding the methodology.\n\n- The visualization of safety scores before and after applying the proposed method is clear, intuitive, and effectively illustrates the improvement."}, "weaknesses": {"value": "- In the paper, the authors mention that the same LLM was used for both generating and filtering paraphrases. Why was the same model employed for both tasks? Wouldn’t the approach be more robust and effective if a different LLM were used for filtering?\n\n- It would have been better if some manual filtering had been performed to check the agreement between the LLM and human evaluations. Although the LLM judge is validated using the STS-B benchmark, it would be useful to know how well it performs for this specific task.\n\n- While quantifying Semantic Fragility (Line 149), instead of relying solely on labels, wouldn’t it be more informative to directly consider the safety scores? For instance, comparing the difference in safety probabilities relative to a threshold might provide a more nuanced understanding.\n\n- When generating paraphrases using an LLM, how is it ensured that the generated paraphrases comprehensively represent all possible variations for a given text? Since multiple paraphrases can exist, what strategy is used to ensure that the selected paraphrases cover the full semantic space?\n\n- Could the authors include references for the Logit Transformation mentioned in Line 182?\n\n- In the Symmetric Distribution section, how was the value of the 40th percentile determined? If it was chosen empirically, please explain the rationale behind this choice\n\n- In Table 3, the proposed Skew-Aware strategy does not appear to show a pronounced effect when observing the LFR, although it seems effective in improving accuracy. Could the authors provide an explanation for this observation?"}, "questions": {"value": "Please refer to the weakness section and address those concerns"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J8XQ490EdU", "forum": "JLCTd00wr8", "replyto": "JLCTd00wr8", "signatures": ["ICLR.cc/2026/Conference/Submission19289/Reviewer_ysaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19289/Reviewer_ysaE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903065828, "cdate": 1761903065828, "tmdate": 1762931244463, "mdate": 1762931244463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper aims to improve semantic robustness in guard models—LLM-based classifiers used for safety filtering. It observes that even meaning-preserving paraphrases can drastically alter safety scores, revealing that current guard models rely on surface cues rather than meaning.\n2. The central problem is that these guard models are fragile to harmless linguistic variation, which undermines their reliability in practical safety pipelines.\n3. The authors propose a self-supervised training framework that enforces prediction consistency across paraphrases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a real and under-explored problem: safety classifiers should be invariant to meaning-preserving paraphrases. Demonstrating that even strong guard models (LLaMA Guard v3, Granite Guardian, ShieldGemma) fail on this dimension is valuable evidence for the community."}, "weaknesses": {"value": "1. The paper assumes that paraphrasing captures all meaningful linguistic variation, but does not test robustness under more realistic noise sources such as incomplete sentences, mixed languages, or user-typed grammatical errors. As a result, it remains unclear whether the proposed method generalizes beyond paraphrase-style rewordings to genuine user diversity.\n2. The approach depends on an LLM judge to filter paraphrases but does not analyze how this filtering bias affects training. If the judge systematically favors certain stylistic or cultural expressions, the resulting “robustness” may simply reflect alignment with that model’s bias rather than true semantic invariance."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EH8FhoQ1jV", "forum": "JLCTd00wr8", "replyto": "JLCTd00wr8", "signatures": ["ICLR.cc/2026/Conference/Submission19289/Reviewer_gPmC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19289/Reviewer_gPmC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994469590, "cdate": 1761994469590, "tmdate": 1762931243867, "mdate": 1762931243867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}