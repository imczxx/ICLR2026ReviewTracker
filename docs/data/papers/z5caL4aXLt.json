{"id": "z5caL4aXLt", "number": 3811, "cdate": 1757532498571, "mdate": 1759898068604, "content": {"title": "Take Another Look: Improving Information Extraction From Images With Multiple Encoders", "abstract": "Unstructured image data are increasingly used across diverse applications, yet standard practices for extracting predictive features remain underexplored. We redefine encoder choice as an ensemble and present three strategies for leveraging multiple pre-trained encoders to mitigate model risk associated with using a single encoder. These strategies include: feature union, which concatenates encoder features before model training, and two forms of model averaging, which weight predictions from single encoders using either equal weights or weights chosen with regression. Across six prediction applications—house prices from exterior images, poverty rates from satellite imagery, breast cancer and pneumonia detection from chest X-rays, rice disease classification from leaf images, and facial age —our results show three key findings: (i) using multiple encoders consistently outperform single encoders, with out-of-sample $R^2$  for house prices, for example, increasing from 15.01\\% (best single encoder) to 24.1\\% with feature union;  (ii) model averaging reduces error rates across all classification tasks, for example from 13.31\\% to 8.68\\% offering consistent gains over both single encoders and feature union; and (iii) using multiple encoders methods mitigate model risk, as accuracy varies widely across individual encoders in different applications. These results demonstrate that using multiple encoders provides robust, high-performing pipelines for image-based prediction without requiring extensive task-specific fine-tuning.", "tldr": "", "keywords": ["Multi-encoder learning", "model averaging", "computer vision", "deep learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10e57eaa214a185f5630071b8e3f3e17a8b7d9dc.pdf", "supplementary_material": "/attachment/fdddd4e2093abc36126a9399baee5d7009c73337.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an empirical study on how to combine frozen predictions from one/multiple encoders and test this application on different datasets belonging to different domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**S1.** The objective of the paper is relevant, since selecting the appropriate pre-trained model from a pool of models and how to optimally derive the features for downstream tasks represents a relevant research question \n\n**S2.** The experiments are run on 6 different datasets coming from different domains, and 5 CNN models, which represent a sufficiently large experimental setup\n\n**S3.** The paper reads fairly well, and it is not badly written"}, "weaknesses": {"value": "The paper has critical weaknesses (W), particularly **W1** and **W2**, that largely outweigh its strengths, rendering it unsuitable for publication at ICLR in its current form.\n\n**W1.** Novelty of contributions (C): It is well known within the community that different encoders may yield suboptimal performance for various downstream tasks due to differences in pre-training data/ architecture/ training algorithm, etc. (C1).   Additionally, it is expected that model/feature averaging will lead to better performance, as averaging reduces variance (C2 and C3). Hence, ¾ contributions do not introduce relevant new insights beyond existing knowledge in the field.\n\n**W2.** Coverage and evaluation of related work: A large body of work has studied model selection from a “model zoo” (e.g., see [1,2] and references therein). The paper is missing both a literature description of related works and an evaluation of such baselines against their methodology.\n\n[1] LogME: Practical Assessment of Pre-trained Models for Transfer Learning, ICML 2021 \n\n[2] Exploiting Model Zoo for Out‑of‑Distribution Generalization, NeurIPS 2022\n \n**W3.** Schematics presentation: Both Figures 1 and 2 are hard to understand without explanatory captions, difficult to read given the small fonts, and do not respect presentation standards for a conference of the level of ICLR"}, "questions": {"value": "I appreciate the authors’ effort and recognize the relevance of the topic. \n\nHowever, the main concerns (limited novelty, missing literature coverage, and lack of comparison to prior feature-based model-selection methods) reflect substantive rather than clarificatory issues. Therefore, I do not have specific questions for rebuttal that would likely alter my assessment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k5oeLtxJnv", "forum": "z5caL4aXLt", "replyto": "z5caL4aXLt", "signatures": ["ICLR.cc/2026/Conference/Submission3811/Reviewer_wME9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3811/Reviewer_wME9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761329554215, "cdate": 1761329554215, "tmdate": 1762917048083, "mdate": 1762917048083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the possibility of ensembling multiple pretrained image encoders to generate predictive feature sets that improve performance in image tasks. Two strategies are studied: concatenating features from different encoders (\"feature union\"), and averaging the predictive model outputs, either with a simple average or a weighted average where the weights in the average are learned. These methods are evaluated on 2 regression datasets and 4 classification datasets, using 6 \"off the shelf\" image encoders: ResNet50, VGG16, InceptionV3, MobileNet, COCO, and Ade20K. The paper claims that combining the embeddings results in better predictive models than using single embeddings alone, which seems to be generally backed up by the experimental results in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I see the main motivation of this paper being to help users -- probably users from domains outside of ML, given the introduction's focus on medical/social good applications -- avoid an expensive part of the ML pipeline: choosing which model to use to encode their data (here, the focus is on image encoding). Rather than propose a new image encoding method, the authors consider the possibility of ensembling multiple existing encoding methods. I think there is a **strength in the simplicity of the proposed approach**, which could be very easy for users of all types to implement.\n\n2. Another strength is the **diversity of datasets used for assessing model performance**, spanning many different regression and classification tasks. However, this strength is dampened a bit without a connection to how other proposed models have performed on these datasets (see weaknesses below).\n\n3. The **diversity of image encoders** -- including feature extractors and panoptic segmentation models -- is a plus. Though, these are all relatively older methods (though still reasonable choices) -- see boxes below.\n\n4. Presentation of written text: the paper is clearly written and easy to understand."}, "weaknesses": {"value": "There are several major weaknesses of this paper in it's current form. I list what I see to be the most major weaknesses in the hopes that these are constructive to the authors in a potential revision of the work. I do not think the authors will be able to address these major weaknesses in a rebuttal period, \n\n**Major Weaknesses**:\n1. There is no related work section! Model ensembling has a rich history in ML, and it's completely missing from this paper. A discussion of more recent related work for fusing outputs from multiple encoders is also missing. \n2. The models compared are all from 2019 or earlier, limiting the scope of the results. Would these ensembling methods make a difference if the encoders themselves were achieving higher accuracy individually? It's impossible to tell from the results provided.\n3. There is no comparison to past approaches on the datasets provided, and it's not clear if the train/val/test splits here are consistent with past work. For example, I believe that performance achievable for the povertymap dataset is much higher than what is reported here -- the authors could easily have compared to the performance of other methods on the leaderboard, for example, to contextualize whether their low predictive performance is expected.\n4. The results could be better tied into the motivation of the paper.  The motivation -- as I see it -- is to help users avoid the need to try different model architectures. But there is no discussion of the costs associated with ensembling -- just as relying only on a resnset50 isn't the best approach, neither is assuming that ensembling a bunch of models is the right way to go. There's no discussion of (i) the added computational costs and complexity of training and combining multiple models or when ensembling is most likely to help, for example. Given that ensembling is expected to help (see: rich history of ensembling methods in ML), these more in-depth takes would be important if the goal is to contribute user-friendly approaches that are shown to be robust across datasets and tasks. I just don't think the paper has achieved that yet.\n\n**Minor Weaknesses**\n1. The figures and tables do not stand on their own - I suggest to add more detail to the captions.\n2. Figure 1 should not be a line plot, the horizontal axis variables are categorical, not numeric."}, "questions": {"value": "I have several questions as well for the authors that I hope can be addressed during the rebuttal and discussion phase:\n\n1. In figure one panels (c) and (e), something looks off. Since R2 score is a rescaled version of MSE, I think the methods that have higher MSE should always have higher R2 score on a given dataset (blue or orange line), but this isn't always the case. Could the authors look into this and clarify?\n2. Why not test more recent encoding methods, like ViTs, for example?\n3. Why are the authors reporting R2 on classification tasks? Would there not be a more suitable score like F1 score or AUC?\n4. Did the authors run multiple random trials for experiments? If so, the figures are missing standard errors / standard deviations that are necessary to evaluate their claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9bxgu7cySg", "forum": "z5caL4aXLt", "replyto": "z5caL4aXLt", "signatures": ["ICLR.cc/2026/Conference/Submission3811/Reviewer_6A1Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3811/Reviewer_6A1Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576666403, "cdate": 1761576666403, "tmdate": 1762917047525, "mdate": 1762917047525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines a few combinations of encoders for both classification and regression models across differeng tasks. Results indicate that combining various encoders might be beneficial compared with leveraging a single image encoder."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors conduct experiments on various tasks and dataset, somehow bringing observations and insights regarding the utility of image encoders."}, "weaknesses": {"value": "1. The major focus of this research is vague. It seems that the major purpose of this study is to examine some existing models rather than propose novel method / introduce knowledge. Notably, the observations indicated in this study have already been widely noticed and accepted by the community, such as the usefulness of aggregating encoders. Therefore, this study leaves me the impression that it is a project report rather than a research paper, with marginal research contribution. \n\n2. The presentation is not very rigor. For instance, in Table 1, only the first column (comparison between prediction and ground truth) is valuable, while the comparison between outcome of different predictions does not contribute to the evaluation of encoders. In Figure 1, the authors choose to utilize line chart rather than bar chart to present results from individual encoders, which can be somewhat misleading as their performance are strictly independent. In Eq. (8), $\\hat p_{ij}^{(m)}$ has not been previously defined, which should be referred as $\\hat y$ instead.\n\n3. Moreover, organization of this paper is messy. I am assuming \"THE USE OF DEEP LEARNING MODELS IN PREDICTION APPLICATIONS\" and \"ENCODERS FOR IMAGE PROCESSING\" should serve as two subsections of the absent section of \"Related Work\", rather than two paragraphs in Introduction."}, "questions": {"value": "Please refer to my aforementioned weaknesses. Any clarification will be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "peu60CHlyD", "forum": "z5caL4aXLt", "replyto": "z5caL4aXLt", "signatures": ["ICLR.cc/2026/Conference/Submission3811/Reviewer_bmWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3811/Reviewer_bmWn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874533534, "cdate": 1761874533534, "tmdate": 1762917047259, "mdate": 1762917047259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on using multiple off-the-shelf pre-trained CNN and panoptic encoders for feature extraction on multiple datasets. Then they aim to use these features ensembled outperform single-encoder models across various tasks. They provide with empirical results and analysis to prove their insights."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written, with a clear tone and details."}, "weaknesses": {"value": "- The paper mainly ensembles Resnet50, VGG16, inception, mobilenet, coco, and ade20k, without using any state-of-the-art transformer-based encoders, features, or recent hybrid CNN-transformers- which should be considered a baseline experiment.\n- The paper does not demonstrate any technical innovation or explanation:: in model design or algorithm or feature concatenation method. It basically provides domain-specific empirical results for simple feature usage method from multiple CNN based encoders.\n- The datasets used in the paper, is very small and domain specific supervised datasets. It lacks diversity and generalizabiity. Does the claim remain true if it is used on more recent or more used datasets?\n- The paper lacks an ablation study for failure cases, if and when the method will fail.\n- Using multiple frozen encoders means users have to save each of the models and do inference for one image multiple times. This is both resource and time-intensive with the gain being relatively small.\n- The encoder ensembling topic has extensive prior work, including feature fusion and multi-layer aggregation techniques(cross layer attention, hierarchical ensembling). None of these has been reviewed or benchmarked in this work.\n- The paper does not convey well, the domain significance, broader impact of the work."}, "questions": {"value": "- Comparison with the CNN-transformer architecture to show if the results hold.\n- ablation study, on different model sizes, model train strategies, and model type: how would this affect the performance gain?\n- Usage of multiple bigger datasets for classification to show the generalizability of the claim.\n- Instead of focusing on dataset explanation in the paper, in my humble opinion, more focus should be on using multiple models and an ablation study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a2abKMQLZb", "forum": "z5caL4aXLt", "replyto": "z5caL4aXLt", "signatures": ["ICLR.cc/2026/Conference/Submission3811/Reviewer_ap1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3811/Reviewer_ap1e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898390291, "cdate": 1761898390291, "tmdate": 1762917047061, "mdate": 1762917047061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}