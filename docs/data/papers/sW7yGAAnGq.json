{"id": "sW7yGAAnGq", "number": 3463, "cdate": 1757436116727, "mdate": 1763387362700, "content": {"title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models", "abstract": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate generation and understanding capabilities within a single framework. However, a key challenge remains: a model's powerful understanding often fails to transfer into complex image generation. This often occurs because the understanding and generation modules are trained separately or leading an internal conflict during co-training. As a result, a model can accurately assess a prompt against an image but cannot generate a correct image from that same prompt. To resolve this challenge, we introduce SRUM, the self-rewarding post-training framework designed to improve the model to align its generation with its understanding module. Without needing any new human-labeled data, SRUM creates a self-improvement loop where the model's own understanding module acts as an internal ``evaluator\", providing corrective feedback by rewarding to its generation module. Our core innovation is a two-part reward system that offers comprehensive guidance: comprising a \\textbf{global reward} for overall compositional structure and a \\textbf{local reward} for fine-grained, object-level fidelity. This multi-scale feedback proves critical for complex generation. SRUM sets a new state of the art and strong generaliztion, boosting performance as on T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ResonBench from 40.7 to \\textbf{50.4} in image accuracy. Overall, our work establishes a powerful new paradigm for enabling the UMMs' understanding module to guide its own generation.", "tldr": "A post-training method for unified multimodal models' generative ability by self-rewarding.", "keywords": ["Unified Multimodal Models", "Vision Language Models", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daa437ca1bb2bfa20f328b28dda3c3db3ab018c5.pdf", "supplementary_material": "/attachment/dba30bc4cdfa60555cfa8a2a7e7fc6979f5cfa93.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SRUM, a self-rewarding method for unified multimodal models. It lets the model’s own understanding part judge and guide its image generation, so it can better match prompts without using human feedback. The method gives both global and local rewards to improve structure and details in generated images. Experiments show clear improvement on several benchmarks, meaning SRUM helps the model connect understanding and generation more effectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design of global and local rewards gives detailed and effective guidance, which can improve both scene structure and object details in generation.\n2. The experiments are thorough and comprehensive in terms of the coverage of different backbones, the ablations, and the generalization analysis with diverse benchmarks."}, "weaknesses": {"value": "1. The paper mainly compares SRUM with the raw base models instead of other post-training or reward-based approaches. This makes the improvement less convincing and does not clearly show advantages over existing post-train methods.\n2. Although claimed as self-rewarding, the evaluation still relies on strong external MLLMs (e.g., QwenVL2.5-72B) for scoring, so the method is not truly self-evaluative as described.\n3. The pipeline involves multiple stages (reward generation, bounding box detection, and reward-weighted training) with many hyperparameters, which may limit reproducibility and accessibility."}, "questions": {"value": "1. Why not compare with other methods that use RL to boost image generation?\n2. Can you provide visual case studies or detailed analysis of failure modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tPRJtUS9iY", "forum": "sW7yGAAnGq", "replyto": "sW7yGAAnGq", "signatures": ["ICLR.cc/2026/Conference/Submission3463/Reviewer_PxGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3463/Reviewer_PxGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632477821, "cdate": 1761632477821, "tmdate": 1762916736607, "mdate": 1762916736607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SRUM, a framework designed to improve the alignment between the understanding and generation modules of Unified Multimodal Models (UMMs) for text-to-image (T2I) tasks. ​ The process involves two key stages: training data creation and post-training refinement. \n\nIn the first stage, SRUM generates high-quality image candidates and bounding boxes using the UMM’s generation module, which are then evaluated by its understanding ability. ​ The understanding module acts as an internal evaluator, providing dual-level feedback through global rewards for compositional structure and local rewards for object-level fidelity. ​\nIn the second stage, these rewards are used in a reward-weighted post-training process, where the generation module is refined based on the feedback. ​ This self-rewarding loop enables the model to improve its generative capabilities without relying on external models or human-labeled data. ​ \n\nExperiments show that SRUM significantly enhances image accuracy, compositional reasoning, and generalization across benchmarks, setting new state-of-the-art results and establishing a novel paradigm for self-improvement in UMMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is good and fundemental: the gap between understanding and generation of a unified model. and post training seems a promising direction. also their approach is self-contained  which eliminates the need for external models or human-labeled data, making the approach more efficient and scalable.\n\nDual-Level Reward System: SRUM employs a two-part reward system—global rewards for overall compositional structure and local rewards for fine-grained object-level fidelity. ​ This multi-scale feedback mechanism is critical for improving complex image generation tasks.\n\n\nGood performance and generalization: The framework achieves significant improvements in image accuracy and compositional reasoning, setting new benchmarks on T2I-CompBench and T2I-ReasonBench. It also demonstrates strong generalization"}, "weaknesses": {"value": "Two main concerns:\n\n1, why choose to use the model itself as understanding model? It is nice and elegant which remove need of external VLM, but is there a real reason to have to use itself? I feel this paper lacks depth to show why it has to be itself \n\n2, the approach is too biased towards composition, which can be solved in bbox. what if the misalignment is more global? say style, or object 3d angle?  \n\ni am willing to raise my score if authors can address my first concern"}, "questions": {"value": "see weekness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k9BFbUMY75", "forum": "sW7yGAAnGq", "replyto": "sW7yGAAnGq", "signatures": ["ICLR.cc/2026/Conference/Submission3463/Reviewer_bnfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3463/Reviewer_bnfL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778913617, "cdate": 1761778913617, "tmdate": 1762916736309, "mdate": 1762916736309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SRUM, a post-training framework for unified multimodal models. The authors design a self-improvement to leverage the model's own powerful understanding module as an internal evaluator to provide corrective feedback to its generation module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a post-training method for unified multi-modal models that requires no new human-labeled data.\n\n2. The results on T2I benchmarks are good, ablations studies demonstrates considerable performance gains on BAGEL and BLIP3o, proving the effectiveness of the method."}, "weaknesses": {"value": "1. The paper needs more proof on whether self-rewarding is generalizable to other unified multi-modal models. The effectiveness of the verifier is the key to the validity of SRUM, but the persistence of verifier quality is validated only on architectures (Bagel, Blip3o) that has a substantial degree of separation between their understanding and generation components. For more deeply integrated architectures (e.g., Emu3[1], NextStep-1[2]) where understanding and generation are entangled in the same core parameters, it is probable that generative post-training would lead to catastrophic forgetting and degradation of the model's understanding capability, causing verifier collapse. The paper lacks evidence that SRUM can prevent this mutual corruption. Which leads to my next concern: \n\n2. The paper needs more justification on using UMM's internal understanding as verifier. Since the supervision is single directional (understanding -> generation), and there seems to be no mutual enhancement effect. Then **why not simply use an external expert (a strong VLM) as verifier**? Empirically, dedicated VLMs are usually stronger in perception and visual understanding than UMMs, and would be immune to potential verifier collapse as stated in the previous question.\n\n[1] Wang, et. al., Emu3: Next-Token Prediction is All You Need. In arXiv.org: Vol. abs/2409.18869.\n\n[2] Han, et al., NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale. In arXiv.org: Vol. abs/2508.10711."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kc4oHlfAJ0", "forum": "sW7yGAAnGq", "replyto": "sW7yGAAnGq", "signatures": ["ICLR.cc/2026/Conference/Submission3463/Reviewer_8hSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3463/Reviewer_8hSG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887853066, "cdate": 1761887853066, "tmdate": 1762916736033, "mdate": 1762916736033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SRUM (Self-Rewarding for Unified Multimodal Models), a post-training framework that leverages a model's understanding module to improve its generation capabilities. The authors observe that UMMs possess strong understanding capabilities that significantly outperform their generation quality—a model can identify when a generated image doesn't match a prompt but cannot generate the correct image. SRUM addresses this gap by using the understanding module as an internal evaluator to provide fine-grained feedback during training. The method decomposes rewards into two components: (1) a global reward assessing overall compositional structure and (2) local rewards providing object-level feedback on specific image regions identified through bounding boxes. Training uses a reward-weighted velocity prediction loss combined with a reference loss to prevent reward hacking. The method achieves significant improvements on T2I-CompBench (82.18→88.37) and T2I-ReasonBench (40.7→50.4 in accuracy) without requiring additional human annotations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a genuine gap in UMMs—the disconnect between understanding and generation capabilities—and proposes an elegant self-rewarding solution that exploits the model's own capabilities without external reward models or new human labels.\n2. The decomposition into global (compositional) and local (object-level) rewards is well-motivated and technically sound. This multi-scale feedback addresses the limitations of holistic scoring for complex compositional tasks.\n3. There are substantial improvements across multiple benchmarks using SRUM method and strong generalization to in-domain (GenEval, WISE) and out-of-domain (T2I-ReasonBench) tasks, while there are minimal impact on understanding capabilities (Table 2)."}, "weaknesses": {"value": "1. The paper provides no analysis of computational costs, which is critical for evaluating practicality. Since the SRUM needs to use UMM to generate the self-rewards used for the training, the additional computation cost should be analyzed and compared to the normal method SFT.\n2. The reward system may relies heavily on carefully engineered prompts (Tables 7-8 in Appendix). It's unknown how reliable these generated rewards are in a quantative view. For example, maybe using another VLM for scoring can bring more improvements.\n3. Some abaltion experiments are missing. For example, the experiments using only the global reward as the reward without local reward. And the training with some external rewards like ImageReward Models? If these external rewards are better, then the self-rewarding part of the paper may be not necessary."}, "questions": {"value": "1. Can you clarify clearly about the overall training process? From my understanding, you are generating candidate images, using UMM to generate rewards on the whole training dataset and then use it for the training. This seems to be a offline method. Have you considered online method that using the latested updated UMM to generate scores as rewards?\n2. Can you provide results for \"w/o Local\" (using only global reward without object-level rewards)? This is essential to validate the necessity of fine-grained local feedback.\n3. How robust is the method to variations in the reward prompts (Tables 7-8)? Were these prompts tuned on a validation set?\n4. How does SRUM compare to:\n- Using external reward models?\n- Simple reconstruction alignment (RecA)?\n\nTypos:\n1. line 258: \"modes\" -> \"models\"\n2. line 272: \"Table Table 1\" -> \"Table 1\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "syfCZv1mfu", "forum": "sW7yGAAnGq", "replyto": "sW7yGAAnGq", "signatures": ["ICLR.cc/2026/Conference/Submission3463/Reviewer_YiSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3463/Reviewer_YiSP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980437517, "cdate": 1761980437517, "tmdate": 1762916735840, "mdate": 1762916735840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}