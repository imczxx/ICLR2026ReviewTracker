{"id": "q4iJxp47CT", "number": 4473, "cdate": 1757686416680, "mdate": 1759898030842, "content": {"title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding", "abstract": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into compact representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing training-time memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-4B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, across six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3\\% improvement over LoRA and +29.9\\% over the baseline on average, with up to +23.4 absolute gains on AIME2024/2025 under tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.", "tldr": "", "keywords": ["efficient reasoning LLM", "KV cache", "test-time learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f27058631cf5f07750cff0eff3eed744280aad6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Progressive Thought Encoding method, a novel reasoning architecture for reasoning models. By encoding expelled intermediate reasoning tokens into fixed-size vectors and integrating them as a LoRA adapters, it avoids the high memory consumption of full-cache backtracking while preserving key reasoning signals. Experiments conducted on 3 open-source models and 6 mathematical reasoning benchmarks demonstrate that the proposed method achieves consistent improvements over LoRA and LoRA_c, while effectively reducing GPU peak memory usage and FLOPS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves better training results than PEFT methods on mainstream mathematical datasets.\n2. Monitoring of FLOPs and GPU memory confirms that the approach of compressing tokens and reducing activations indeed contributes to efficient training."}, "weaknesses": {"value": "1. Baselines. Although the ultimate goal is to reduce computational overhead, I am uncertain whether LoRA is an appropriate baseline. This work appears to be more aligned with studies on token compression and similar model architecture methods rather than PEFT methods. Have the authors considered comparing with relevant approaches? Please correct me if there is a misunderstanding.\n2. Compatibility with existing infrastructure. The compression during reasoning seems to require per-sample maintenance of a series of related states, such as global query vectors. Can these modifications to the model architecture be easily implemented on transformers or vllm libraries to adapt to the existing infrastructure?\n3. Context window. A 3K context window is generally insufficient for complex mathematical problems, especially for models trained with reasoning capabilities. The Distill-R1 model also has a longer output length. How does the performance change when the output length increases?"}, "questions": {"value": "Please consider discussing the issues raised in the weaknesses section.\n\n4. I still have some confusion regarding the end-to-end procedure of RL training. For traditional RL, storing only the model's text responses is sufficient to compute the policy gradient. For the method proposed in this paper that adds LoRA heads to the model during test time, are any additional operations required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dG4Hu6TzJT", "forum": "q4iJxp47CT", "replyto": "q4iJxp47CT", "signatures": ["ICLR.cc/2026/Conference/Submission4473/Reviewer_Dfk2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4473/Reviewer_Dfk2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930311721, "cdate": 1761930311721, "tmdate": 1762917386292, "mdate": 1762917386292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large scale chain of though RL runs with full cache make it memory and compute intensive. Simple solution is drop earlier context once cache limit hit or sliding window cache but authors show this comes at cost reasoning peformance. Further they propose to learn a global query vector $q_g$ from these evicted tokens KVs which can function like a memory. This $q_g$ is updated with a simple self-attention like update rule and then injected into a lightweight Lora Adapter via a small weight update $ \\Delta W$. The model parameters now through these remember the evicted tokens. This adds minimal computational cost but allows scaling up RL by fixing cache size. They experiment with DAPO recipe on top of qwen-2.4 4/7B models and DeepSeek-R1-Distill-Llama-8B and show reasonable improvements over slidding window cache baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of encoding the “to be evicted token” into latent vectors which are them embedded into LoRA adapter is very novel and clever. This makes the Lora adapter like a learned dynamic memory without blowing up KV cache linearly\n- Well formulated and simple idea with clear experiments\n- Good ablations and analysis"}, "weaknesses": {"value": "- Results and experimental setup is not convincing enough.\n- Unclear if it will apply to full finetuned models compared to the LoRA only setting in this paper. Also unclear if this will translate over to larger models but I understand that will be outside the scope of this study.\n- max sequence length of 3072 is not enough for reasoning RL runs and doesn’t give me confidence in the results especially given the authors tout this as benefitial for long reasoning RL. Only at longer context lengths the benefits or downside of this appraoch can be demonstrated.\n- AIME25 still has variance at pass@16 so the slight improvment in those benchmarks is not conclusive\n- Its also unclear why this approach should be benefitial over full-cache setup especially given the short max 6k context experiments. At larger contexts I can see how longer context can hurt sometimes. More thorough investigation is needed.\n- $q_g$  is just one fixed vector for the whole evicted sequence. How much capacity can it even have? As we scale up the reasoning RL which can go upto 64k sequence length in frontier RL runs, I am not convinced this $q_g$  can hold that long context well. Ofc it can be better than no context of evicted tokens in sliding window cache but unclear how much we will lose compared to full cache."}, "questions": {"value": "- What do the authors think is the impact of the cache eviction strategy on the q_g update would be. Eg bulk eviction or one at a time.\n- Contribution [1] : Is that a novel contribution or already established in the community. Authors should clarify\n- What do authors think about mamba hybrid models like nano-v2-12b https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2 as they are optimized for inference. Its a hybrid model so cache still grows up but not to the same level. Also they will still able to model the long reasoning context much better.\n- Comments on writing\n    - In line 196 Table 1 is mentioned but we don’t know what lora_c is and its not explained so confusing to read\n    - line 220 is not coherent\n    - global query is not well introduced as being the latent that learns the evicted context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4t8rlMA2y1", "forum": "q4iJxp47CT", "replyto": "q4iJxp47CT", "signatures": ["ICLR.cc/2026/Conference/Submission4473/Reviewer_MbcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4473/Reviewer_MbcZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980644779, "cdate": 1761980644779, "tmdate": 1762917385302, "mdate": 1762917385302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a parameter-efficient fine-tuning method under RL to address the compute resource challenges posed by long-context in difficult problems. The sliding-window caches or dynamic pruning of past tokens approaches, which affects inference accuracy. In contrast, the progressive thought encoding proposed in this paper, by updating the context state to preserve the complete context information as much as possible, ultimately improves the training effect while simultaneously reducing the computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method outperforms both LoRA and LoRA_c in terms of accuracy across various models and evaluation datasets, while requiring only a marginal increase in computational resources compared to LoRA_c.\n2. The method's reliability is demonstrated through comprehensive evaluation across multiple benchmark datasets."}, "weaknesses": {"value": "1. Among the three models evaluated in this paper, Qwen2.5-4B-Instruct and Qwen2.5-7B-Instruct are not LRM. Their output lengths are shorter compared to DeepSeek-R1-Distill-Llama-8B, making their persuasiveness less compelling.\n2. The maximum sequence length is only 3072, lacking evaluation experiments at longer reasoning lengths."}, "questions": {"value": "1. Experimental results for DeepSeek-R1-Distill-Qwen-7B are required;\n2. Consider supplementing with evaluation experiments at longer inference lengths;\n3. The analysis lacks an explanation for why this approach achieves higher accuracy compared to the LoRA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjsq8FvIcF", "forum": "q4iJxp47CT", "replyto": "q4iJxp47CT", "signatures": ["ICLR.cc/2026/Conference/Submission4473/Reviewer_arKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4473/Reviewer_arKo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097312313, "cdate": 1762097312313, "tmdate": 1762917384945, "mdate": 1762917384945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors  introduced  a new memory efficient  RL method to post-train LLMs under under strict memory budgets. The key idea is to keep a  constant memory capacity cache window (similar to sliding window attention) . The authors also introduce global tokens . These global tokens are used with evicted from cache tokens  for weights update. The authors compared proposed method with 2 prior methods: (1) RL + LoRA (2) RL with LoRA and SWA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Good practical method to reduce memory footprint and compute during RL stage for reasoning models"}, "weaknesses": {"value": "The comparison with prior work seems not complete.  The proposed method uses global tokens. Should not these tokens be also used in LoRA and LoRA_c for fair comparison? \n\nI would recommend to remove sections 2.1,  2.2 and 2.3 from  the \"related work\" . This is common knowledge very weakly related to proposed method."}, "questions": {"value": "The comparison with prior work seems not complete.  The proposed method uses global tokens. Should not these tokens be also used in LoRA and LoRA_c for fair comparison? \n\nHow do you explain that Mean GPU memory for \"ours\" is less than LoRA_c in the last row of Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TGyZJO5po9", "forum": "q4iJxp47CT", "replyto": "q4iJxp47CT", "signatures": ["ICLR.cc/2026/Conference/Submission4473/Reviewer_J7E7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4473/Reviewer_J7E7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161248723, "cdate": 1762161248723, "tmdate": 1762917384694, "mdate": 1762917384694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}