{"id": "B2Neq64sm6", "number": 25036, "cdate": 1758363417582, "mdate": 1763193152939, "content": {"title": "Direct Advantage Estimation for Scalable and Sample-efficient Deep Reinforcement Learning", "abstract": "Direct Advantage Estimation (DAE) has been shown to improve the sample efficiency of deep reinforcement learning.\nHowever, its reliance on full environment observability limits applicability in realistic settings.\nIn the present work, we (i) extend DAE to partially observable domains with minimal modifications, and (ii) reduce its computational overhead by introducing discrete latent dynamics models to approximate transition probabilities efficiently.\nWe evaluate our approach on the Arcade Learning Environment and find that DAE scales with function approximator capacity while maintaining high sample efficiency.", "tldr": "", "keywords": ["deep reinforcement learning", "advantage estimation", "arcade learning environment"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1f0e495161e9cae6d759f304564595956a54dc5a.pdf", "supplementary_material": "/attachment/928848115ee172d80d03b2b51635a16413f948ea.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses two limitations of the Off-policy Direct Advantage Estimation (DAE) method: (1) its restriction to fully observable MDPs and (2) its high computational cost, which stems from the need to learn a high-dimensional generative model of transition probabilities. The authors make two primary contributions:\n\n1) Theoretical Extension to POMDPs: They generalize the DAE return decomposition to POMDPs. This is achieved by reformulating the problem in terms of information vectors (histories, $h_t$).\n2) To address the high computational cost of modeling $p(r,o′∣h,a)$, the authors propose a novel discrete latent dynamics model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary contribution seems to be the discrete latent dynamics model instead of reliance on a full generative CVAE to model the transition probabilities. By combining a self-predictive objective (similar to SPR ) with a Winner-Takes-All (WTA) loss, the authors create a model that can predict a discrete distribution over next embeddings. This captures some stochasticity and bypasses the need for a high-dimensional decoder. This allows the dynamics model and the RL agent to be trained jointly end-to-end with minimal overhead , effectively solving the computational problem."}, "weaknesses": {"value": "The first weakness of the method seems to be complexity. There are many moving parts. The final agent has many new, interacting components: a WTA annealing schedule, a delayed onset for the B correction, an adaptive temperature T for the target policy, and multiple loss coefficients. How sensitive is the final performance to this new set of hyperparameters? The stability seems to hinge on the careful annealing and balancing of the various losses.\n\nThe second weakness of the method is that the extension to POMDPs seems elementary to me. It essentially replaces the state with the history."}, "questions": {"value": "How important are the specific architectural choices you made? How do they compare to previous works like [1-3]\n\n[1] Mastering Atari with Discrete World Models\n[2] Transformers are Sample-Efficient World Models\n[3] Towards General-Purpose Model-Free Reinforcement Learning\n[4] TD-MPC2: Scalable, Robust World Models for Continuous Control\n\nSee weaknesses as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sn53LvUXFd", "forum": "B2Neq64sm6", "replyto": "B2Neq64sm6", "signatures": ["ICLR.cc/2026/Conference/Submission25036/Reviewer_2ngK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25036/Reviewer_2ngK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938827752, "cdate": 1761938827752, "tmdate": 1762943291851, "mdate": 1762943291851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "c4QTes9icP", "forum": "B2Neq64sm6", "replyto": "B2Neq64sm6", "signatures": ["ICLR.cc/2026/Conference/Submission25036/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25036/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763193152207, "cdate": 1763193152207, "tmdate": 1763193152207, "mdate": 1763193152207, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a reinforcement learning algorithm for partially observable Markov decision processes. They do so by decomposing the advantage function into two components, one of which must satisfy a centering property. The constrained term is then approximated by a conditional VAE, which the authors learn efficiently by considering a loss function that operates purely in the embedding space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper considers an important and challenging problem.\n\nThe paper is well written and well organized. The key ideas are communicated in a clear fashion.\n\nThe authors provide a thorough ablation study with interesting discussions and observations. \n\nThe related work section covers relevant topics."}, "weaknesses": {"value": "I personally find Figure 1 somewhat difficult to understand. Although conditioning on x and r would understandably clog the figure, it would potentially also improve readability.\n\nThe benchmark considered by the authors does not include continuous control benchmarks.\n\nThe proposed approach is relatively complex to understand and implement.\n\nThe authors do not discuss the potential limitations of the proposed method."}, "questions": {"value": "How sensitive is the constraint to out-of-distribution data? is this a problem when the training is very heavily off-policy?\n\nLine 044: limite applicability\n\nLine 080: What is \\hat{V}? Is it the same as \\hat{V}_target?\n\nLine 107-108: Can the authors provide  a reference to the statement that the tuple “is the unique minimizer of this objective function”?\n\nThe direct advantage estimation framework is somewhat complex. Would it be possible to formulate a similar approach with a more conventional advantage estimation approach?\n\nHow does the policy perform against a baseline that has access to the full state space (full observability)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d2yVFCgzfD", "forum": "B2Neq64sm6", "replyto": "B2Neq64sm6", "signatures": ["ICLR.cc/2026/Conference/Submission25036/Reviewer_gx9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25036/Reviewer_gx9f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957247860, "cdate": 1761957247860, "tmdate": 1762943291656, "mdate": 1762943291656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends Direct Advantage Estimation (DAE) to partially observable domains and introduces discrete latent dynamics models to approximate transition probabilities efficiently. The submission conducts experiments in the Arcade Learning Environment and shows that their approach performance is comparable to Rainbow DQN."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Please see below."}, "weaknesses": {"value": "The theoretical contribution of the paper is a direct implication from prior work. There is no theoretical contribution provided by the submission. This is not a ground for rejection. However, how it is presented in the paper is misleading in the sense that it is claimed to have a theoretical contribution in the main body of the paper. In particular, the first bullet point is about their theoretical contribution. The alleged theoretical contributions of the submission is directly implied by existing work.\n\nOne of my major concerns with the submission is the comparison benchmark. The submission provides comparison against Rainbow DQN and claims that their proposed algorithm is comparable. In particular, \nthe entire comparison benchmark is just against Rainbow 200million with IMPALA CNN, Rainbow 200 million with CNN and Dreamer V3 at 20 million.  However, Rainbow is already known to be not sample efficient and there is a significant amount of publications with many proposed algorithms that perform substantially better than Rainbow. \n\nThe empirical results are based on comparisons of the submission’s proposed algorithm against algorithms that are known to be not sample efficient. Hence, the empirical analysis does not verify what is claimed in the submission.\n\nI think the submission could benefit from thoroughly thinking about their contributions to the field and gathering proper comparisons to verify and demonstrate the claims made in their paper. This is currently not present in the paper.\n\nResults in Figure 11 and Table 5 do not include standard deviations.\n\nThe submission also combines SPR with the Winner-Takes-All (WTA) loss. However, even SPR was not included in the comparison benchmark. Could the simple SPR be better than the submission’s proposal? We simply do not know. \n\nAnother problem with the submission is that the objective in reinforcement learning research is not boosting the performance at whatever cost or change and then claim that the proposed package of changes actually support the evidence that the proposed algorithm performs better compared to prior work. The comparisons should be fair and logical and all the confounding factors need to be isolated to understand the other changes made are not the main factor that contributes to the performance increase. \n\nI will provide a couple examples regarding this and the submission. But throughout the submission this is a critical problem. When the submission compares their method they use a 15-layer deep residual network instead of a three-layer convolutional network and then they also provide comparison to Rainbow which has a three-layer convolutional network. This is not a useful result or information. Furthermore, when they compare to DreamerV3 it is stated that they set DreamerV3 to a 50M parameter network, but the proposed method of the submission network is 22M parameters. It is obvious that larger networks will require more samples to train. Hence, comparing these algorithms in this way does not provide any insights or evidence to the proposed method of the submission’s performance or sample efficiency compared to prior methods.\n\nI think the paper can have the potential to be a valuable contribution. However, in its current form the submission is not ready to be published."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BRvLU3VrFf", "forum": "B2Neq64sm6", "replyto": "B2Neq64sm6", "signatures": ["ICLR.cc/2026/Conference/Submission25036/Reviewer_H9x2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25036/Reviewer_H9x2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030124074, "cdate": 1762030124074, "tmdate": 1762943291488, "mdate": 1762943291488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's extension of DAE to POMDPs is theoretically sound and addresses a significant gap in scalable RL, supported by the decomposed return formulation. The latent dynamics model effectively reduces computational overhead while maintaining performance, evidenced by robustness to latent space size |Z| and entropy analysis . Various experiments performed on ALE show the effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical generalization to POMDPs\n- Comprehensive experiment settings & detailed analysis\n- Clear writing makes the methodology easy to undertstand"}, "weaknesses": {"value": "- Lacking Runtime/FLOP comparisons between the proposed model and the original off-policy DAE’s generative architecture.\n- Lack of compared baselines. More POMDP-based RL algorithms should be compared [1] to illustrate the effectiveness of GAE.\n- The Atari benchmark is quite old, and the tasks within have been mainly solved by RL, as I know. Can the proposed DAE method be employed in robotics benchmarks, like LIBERO[2] and maniskill[3]?\n\n[1] Recurrent Experience Replay in Distributed Reinforcement Learning. ICLR 2019\n\n[2] LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. NeurIPS 2023\n\n[3] Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI with ManiSkill3. arXiv 2024"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4RlA8rPGgk", "forum": "B2Neq64sm6", "replyto": "B2Neq64sm6", "signatures": ["ICLR.cc/2026/Conference/Submission25036/Reviewer_mRMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25036/Reviewer_mRMv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177235507, "cdate": 1762177235507, "tmdate": 1762943290986, "mdate": 1762943290986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}