{"id": "v3mJ4f4Mnc", "number": 14158, "cdate": 1758229379279, "mdate": 1759897386930, "content": {"title": "Do Larger Language Models Generalize Better? A Scaling Law for Implicit Reasoning at Pretraining Time", "abstract": "Reasoning is an integral part of many tasks performed by language models (LMs). However, the effects of scaling model sizes and data on reasoning abilities at pretraining time remain understudied. To rigorously investigate this problem, we pretrain LMs from scratch on a synthetic implicit multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. We then assess the LMs' ability to complete the missing edges in the graph, which requires multi-hop reasoning that can be viewed as a simplification of implicit reasoning during real-world pretraining. \nInterestingly, we observe that overparameterization can impair the implicit reasoning performance due to excessive memorization. We investigate different factors that affect the loss curve when scaling different components of the knowledge graph, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling law that shows optimal-sized LMs can approximately reason over 0.008 bit information per parameter. This work shows counterintuitive effects of model size scaling and provides new insights into the relationship between scaling and reasoning in LLMs.", "tldr": "", "keywords": ["science of language models", "reasoning", "scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0168339f38497a452efd29a37232d9e4b1b23474.pdf", "supplementary_material": "/attachment/a3d6c23fef28c9c75ec44d6f2ae5ed3e735a53e7.zip"}, "replies": [{"content": {"summary": {"value": "The authors seek to derive a scaling law for implicit reasoning performance compared to model size by pretraining models on real and synthetic knowledge graphs. They observe a U-shaped relationship between model-size and implicit reasoning performance (measured on a held out set of knowledge graphs) and conclude that larger models “memorize” instead of reasoning on the knowledge gained from graphs, hurting test-time performance. The authors propose an empirical scaling law that demonstrates the optimal model size compared to “graph search entropy,” a metric for the complexity of a given graph."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe authors introduce a novel, controlled testbed for measuring reasoning performance and connect the synthetic knowledge graph generation to realistic knowledge graph datasets.\n-\tThe authors are very thorough in ablating over relevant parameters in section 5.1\n-\tThe paper introduces graph search entropy as a way to capture the complexity of a graph and demonstrates how the optimal model size is correlated with this metric\n-\tThey release code for reproducibility"}, "weaknesses": {"value": "-\tI have some concerns over how generalizable these findings are. First, the experiments are all run on knowledge graphs with artificial logic rules. The authors acknowledge that these findings may not carry over to natural language reasoning. Furthermore, knowledge graph completion seems very different from reasoning in the real world, particularly when the benchmark is multiple choice. The 0.008 bits/parameter claim seems misleading. Finally, the authors seem to suggest that memorization hurts reasoning performance but don’t clearly show how this is the case. Some analysis on how the large and small models are learning/reasoning differently would be interesting\n-\tThe experiment settings are odd in that all models, regardless of size, are trained for the same number of examples and steps rather than controlling for flops or tokens per parameters. Additionally, the U-shaped test loss compared to model size suggests that the overparametrized models are overfitting to the training data. Is this the case, and how realistic is scenario in large language models? The FB15K-237 experiments are run with 30 data repeats.\n-\tThe graph search entropy is a novel and creative way to illustrate the graph complexity. However, I would appreciate if the authors can provide some more intuition on how this is derived."}, "questions": {"value": "Do the U shaped curves persist when controlling for flops/data, particularly when reducing the number of data repeats?\n\nIs there a way to demonstrate that models are learning logical rules about the knowledge graphs and that the “optimal size” models are learning differently?\n\nDoes the optimal size change when altering the dataset (ie increasing/decreasing the number of answer choices"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SdQ1pWaUL", "forum": "v3mJ4f4Mnc", "replyto": "v3mJ4f4Mnc", "signatures": ["ICLR.cc/2026/Conference/Submission14158/Reviewer_7rMj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14158/Reviewer_7rMj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113013994, "cdate": 1762113013994, "tmdate": 1762924621284, "mdate": 1762924621284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether scaling up language models inherently improves their reasoning ability during pre-training. The authors design a controlled setting in which models are trained on structured knowledge graph data represented as triples ($e_1$, r, $e_2$), where entity $e_1$ has relation r with entity $e_2$. Through systematic experiments, they find that reasoning performance does not follow the typical monotonic power-law scaling trend observed for loss or accuracy. Instead, the results exhibit a U-shaped curve, suggesting that beyond a certain size, larger models may over-memorize the training data, which ultimately hinders their reasoning capability.\n\nOverall, the paper tackles an important question about how reasoning ability scales when language models are pretrained on structured data, examining the effects of model size, dataset size, and training steps. However, the study would be stronger with experiments involving larger models and more extensive training data. It is also unclear whether the conclusions drawn from relatively small models can generalize to larger-scale settings. Moreover, according to the appendix, only a single hyperparameter configuration is used across different model sizes, which raises concerns about the robustness of the study."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a fundamental and timely question, whether scaling up language models inherently enhances reasoning ability during pre-training. The topic is both conceptually important and empirically under explored.\n2. The paper presents its methodology and results clearly, with systematic comparisons across model sizes, dataset scales, and training steps, making it relatively easy to reproduce and extend."}, "weaknesses": {"value": "1. the paper could be stronger with experiments involving larger models and more extensive training data. \n2. It is also unclear whether the conclusions drawn from relatively small models (like 0.3M model) is safe.\n3. Moreover, according to the appendix, only a single hyperparameter configuration is used across different model sizes."}, "questions": {"value": "1. What is the motivation to eliminate the multiple-answer questions? Is it convincing to draw the conclusion only from single answer question?\n2. Can authors provide more details about the test set? Is the test set simply constructed by the same distribution of the training knowledge graph? Does the test set cover complex reasoning relationship?\n3. I am curious why we need (why we are exploring) a specialized large language model for knowledge graph data. Is the regular LLM reasoning not as good as LLM trained on KG data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lAyQKD4G34", "forum": "v3mJ4f4Mnc", "replyto": "v3mJ4f4Mnc", "signatures": ["ICLR.cc/2026/Conference/Submission14158/Reviewer_1bdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14158/Reviewer_1bdq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213811126, "cdate": 1762213811126, "tmdate": 1762924620874, "mdate": 1762924620874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how language model size affects implicit multi-hop reasoning during pretraining using knowledge graph based data, specifically FB15K-237. It defines implicit reasoning as inferring missing edges via multi-hop rules without chain-of-thought supervision and observes a U-shaped relationship between model size and reasoning performance. Overly large models tend to overfit and memorize, degrading implicit reasoning, while the minimum achievable reasoning loss is largely determined by the training data. The authors further show that the optimal model size depends on knowledge graph complexity and introduce a graph search entropy metric to predict it."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses scaling and reasoning in LLM pretraining, an important yet underexplored problem. It introduces a set of novel experiments that make implicit multi-hop reasoning measurable. The large-scale experiments and the proposed link between knowledge graph complexity, optimal model size, and graph search entropy provide an interpretable and potentially useful perspective."}, "weaknesses": {"value": "1. The training objective is aside from test accuracy and test loss, which might cause unfairness of the scaling law, which may weaken the validity of the proposed scaling law. Also, not sure if the author did the instruction tuning for the multi-choice question answering.\n2. Figure 2 requires more explanation; for example, it is unclear why the relation from type2 to type3 uses the same relation r2 as from type5 to type8.\n3. The empirical study relies solely on FB15K-237 despite the availability of many standard KG benchmarks. For the sake of generality, at least one additional dataset should be included."}, "questions": {"value": "1. The paper claims a scaling law for LLM pretraining. How did authors formulate the actual training loss in the common causal next-token prediction setting? Especially for the GPT-style textualization where entities and relations are converted to natural language. How is the training objective implemented?\n2. Are any general text corpora or external datasets mixed into pretraining, or is training performed exclusively on the constructed knowledge graph data?\n3. Is test accuracy evaluated directly after pretraining, or after any additional instruction tuning or adaptation stages?\n4. See (1254, 22, 765) in the triple-only example: is it entity and relation id would be tokenized like a natural text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "h9B4ULGtvu", "forum": "v3mJ4f4Mnc", "replyto": "v3mJ4f4Mnc", "signatures": ["ICLR.cc/2026/Conference/Submission14158/Reviewer_xsY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14158/Reviewer_xsY7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762557238686, "cdate": 1762557238686, "tmdate": 1762924620585, "mdate": 1762924620585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical finding that language models trained on synthetic multi-hop knowledge graph data demonstrate non-monotonic performance with increasing model size. Across both semi-synthetic (using a real knowledge graph) and fully synthetic (random graph) experiments, a similar trend is observed and a scaling law is fit to the optimal model size as a function of the \"graph search entropy\"."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting and counterintuitive result of a particular task that may not scale with model capacity.\n\n2. There are careful synthetic experiments which present a simple model that can capture a lot of the effect observed in the knowledge graph case."}, "weaknesses": {"value": "1. Some of the claims about models learning reasoning vs. memorization do not seem well substantiated. For example it is brought up in the abstract that large models are failing due to excessive memorization, but there is no clear evidence of this in any of the experiments (which only track loss/accuracy, and don't really try to define or measure memorization). There is also maybe some missing related work on the subject, e.g. this paper that studies memorization vs. reasoning in MoEs: https://arxiv.org/abs/2410.19034. \n\n2. It is not clear how much the severe epoching is changing the results. I understand that as the paper says, facts may be presented multiple times, but in those cases it will always be in different contexts and with different phrasing (especially in deduplicated data). You can see this effect in figure 1. For the GPT-generated and templated data (which contain some variation in phrasing), when the number of steps is low (i.e. less epoching), then scaling the model is generally better. It is only in the cases of extreme over-training and insufficient rephrasing where the overfitting effect is observed. In this way, it is not clear if this is picking up a real effect or just showing that with enough epoching, overfitting is indeed possible. \n\n3. In general, it is not clear how the semi-synthetic experiments connect to real world language modeling."}, "questions": {"value": "1. Is there an experiment you could do to substantiate the memorization claims?\n\n2. Is there an experiment you could do to better test the epoching claims? Can you also change the plots to be presented in terms of epochs instead of \"steps\" which is sort of a meaningless metric since it also depends on batch size?\n\n3. Is there an eval that could be run on real language model training that could observe the same effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KDDREDXZ4E", "forum": "v3mJ4f4Mnc", "replyto": "v3mJ4f4Mnc", "signatures": ["ICLR.cc/2026/Conference/Submission14158/Reviewer_nSeB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14158/Reviewer_nSeB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762630403035, "cdate": 1762630403035, "tmdate": 1762924619924, "mdate": 1762924619924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how the scaling of language model size affects reasoning capabilities that emerge implicitly during pretraining, without explicit chain-of-thought supervision. The authors pretrain language models from scratch on both real-world and synthetic knowledge graphs, where reasoning is operationalized as predicting missing edges that require multi-hop inference. Contrary to the conventional belief that larger models always perform better, the study finds a U-shaped relationship between model size and reasoning ability: models that are too large exhibit degraded reasoning performance due to overparameterization and memorization.\n\nThrough systematic experiments on synthetic graphs, the paper shows that the optimal model size depends on properties of the data, such as the number of entities, relations, and triples, but not on the number of training steps. The authors propose a new quantitative measure called graph search entropy, which captures the information-theoretic complexity of reasoning paths in a graph. They then establish an empirical scaling law linking the optimal model size to this entropy measure and show that, on average, a model can effectively reason over about 0.008 bits of information per parameter. This finding contrasts sharply with prior work on knowledge memorization capacity, which estimated around 2 bits per parameter.\n\nOverall, the work presents a controlled experimental framework for studying implicit reasoning during pretraining, reveals a non-monotonic scaling law that challenges standard assumptions about model size and performance, and introduces a theoretical construct that provides a new lens for understanding the relationship between reasoning, memorization, and scale in language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper is original in its problem formulation and experimental design. Rather than following the conventional view that larger language models always perform better, it investigates how reasoning abilities emerge during pretraining and demonstrates a striking U-shaped relationship between model size and reasoning performance. The proposed notion of implicit reasoning framed through knowledge graph completion provides a controlled and interpretable setting that isolates reasoning from linguistic or task-specific confounds. This conceptual reframing, along with the introduction of the graph search entropy measure, represents a creative and meaningful advance in understanding the link between data complexity and model capacity.\n\nThe work is of good quality, supported by experiments on synthetic datasets. The methodology is thoughtfully constructed, the results are reproducible, and the analyses are clear and consistent. The discovery that optimal reasoning capacity scales linearly with graph entropy and amounts to about 0.008 bits per parameter adds a precise, quantitative dimension to the study, distinguishing it from prior scaling law research.\n\nThe paper is clearly written and well-organized, with figures that effectively convey its main findings. Its significance lies in challenging the dominant assumption of monotonic performance gains with scale and providing a new empirical framework for reasoning-specific scaling. The insights offered have potential to influence future research on model design and pretraining strategies aimed at enhancing reasoning rather than mere memorization."}, "weaknesses": {"value": "The paper’s literature review appears incomplete because it omits several important recent seminal works on the science of language‐model reasoning, such as \"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method\" (ICLR 2024), \"Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining\" (COLM 2025), \"EvoLM: In Search of Lost Language Model Training Dynamics\" (NeurIPS 2025), \"Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?\" (NeurIPS 2025), all of which discuss how language model training affects reasoning. Without reference to these and other closely related studies, the work is less well‐situated in the broader research context.\n\nThe choice of a synthetic task setting somewhat raises concerns about the external validity of the findings. While the controlled environment is understandable, the paper does not include at least one experiment showing that performance on the synthetic reasoning task correlates with a meaningful real‐world reasoning benchmark (e.g. code, math), leaving open the question of how applicable the results are to practical large language model reasoning.\n\nThe experimental design lacks sufficient transparency and raises potential confounds: for example, the pretraining data size and how it scales (or not) with model size is not clearly specified, and it is unclear whether each model size was trained to full convergence. Without this information, the observed U-shaped relationship between model size and reasoning performance could be reflecting under‐training of larger models rather than an inherent reasoning‐versus‐memorization trade‐off. Moreover, a complete scaling law should ideally incorporate data amount (and compute) in addition to model size, which the paper does not address.\n\nThe interpretation of the drop in reasoning performance for larger models is potentially overly simplistic. The authors attribute this decline primarily to overparameterization and increased memorization, but they do not sufficiently explore alternative explanations (such as optimization difficulties, training regime mismatch, or capacity under‐utilization) nor provide deeper diagnostic analyses (like loss landscapes, generalization gaps, gradient behaviour) to support the claim. Also the theoretical construct of “graph search entropy” is indeed interesting but remains somewhat abstract and limited in interpretability. Although the paper presents an empirical linear relationship between this measure and optimal model size, it does not fully explain how the measure generalises to more complex or realistic graph‐structured data, and it does not explore the sensitivity of the empirical coefficient (~0.008 bits per parameter) to variations in task formulation or dataset design. This might raise question about how broadly applicable the proposed scaling law is beyond the constrained experimental setting.\n\nMinor ones: while the figures and tables are clear, some of the axis labels and legneds are a bit small or lack units, which slightly hampers readability. Also, the paper occasionally uses jargon (for instance “search entropy per parameter”) without a brief intuitive definition in the main text, meaning readers not deeply familiar with information-theoretic measures may struggle. I would consider raise my scores if the above-mentioend points are addressed well."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G3svgRbjSr", "forum": "v3mJ4f4Mnc", "replyto": "v3mJ4f4Mnc", "signatures": ["ICLR.cc/2026/Conference/Submission14158/Reviewer_CJHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14158/Reviewer_CJHz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762921073910, "cdate": 1762921073910, "tmdate": 1762924618829, "mdate": 1762924618829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}