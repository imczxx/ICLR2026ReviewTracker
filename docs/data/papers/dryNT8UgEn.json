{"id": "dryNT8UgEn", "number": 7640, "cdate": 1758030175214, "mdate": 1763238769688, "content": {"title": "How far can we go with ImageNet for Text-to-Image generation?", "abstract": "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over availability (closed vs open source) and reproducibility (data decay vs established collections). We challenge this established paradigm by demonstrating that one can match or outperform models trained on massive web-scraped collections, using only ImageNet enhanced with well-designed text and image augmentations. With this much simpler setup, we achieve a overall score over SD-XL on GenEval and on DPGBench while using just 1/10th the parameters and 1/1000th the training images. This opens the way for more reproducible research as ImageNet is a widely available dataset and our standardized training setup does not require massive compute resources.", "tldr": "Training text-to-image diffusion models only on Imagenet", "keywords": ["Text-to-image diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9d338f0ef5266c0072f078cd9029f92cd6797519.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues you can train competitive text-to-image (T2I) diffusion models using only ImageNet by pairing it with (i) long, synthetic captions and (ii) simple image augmentations, instead of billion-scale web data. A 300–400M-param model trained this way reports +6% GenEval and +5% DPGBench over SD-XL, while using ~1/10 the parameters and ~1/1000 the training images, with a stated budget of ~500 H100 hours. The method first turns ImageNet into a T2I corpus via detailed captions (improving FID and compositionality), then combats overfitting and boosts multi-object reasoning with CutMix/Crop augmentations. The authors also show the ImageNet-only model can be fine-tuned for aesthetics (e.g., on LAION-POP), improving PickScore/Aesthetics/HPS/ImageReward and remaining competitive at higher resolutions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing is clear and easy to follow. The intro crisply challenges “bigger is better,” motivates ImageNet as a reproducible alternative, and states the central question (“how far can we go with ImageNet for T2I?”). The paper lists concrete contributions (analysis of shortcomings, a standardized ImageNet-only training setup, models in the 300–400M range, and transfer to high-res aesthetics), which makes the narrative easy to follow.\n\n2. Solid SOTA comparisons. Benchmarks on GenEval and DPGBench show competitive performance versus much larger web-scale models."}, "weaknesses": {"value": "1. “general capabilities” claim is too broad. ImageNet is object-centric; the paper itself notes missing classes like person, and that AIO captions lack scene/interaction coverage—so action understanding, human/physics realism, etc., are under-tested here.\n\n2. Proposed solutions are \"just augmentations\", limited novelty. Authors can make the contribution explicit as a standardized, reproducible ImageNet-only T2I recipe. Authors can add stronger ablations to show the recipe is robust, not just one augmentation choice.\n\n3. Table 3 gains look trivial, some FID metrics get worse. DiT-I FID(Inception) worsens from 6.29 → 7.30 with CutMix, even though some compositional metrics improve; CAD-I FID goes 6.16 → 6.62 with CutMix. Crop sometimes preserves or slightly improves FID."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YYXJfAzqIM", "forum": "dryNT8UgEn", "replyto": "dryNT8UgEn", "signatures": ["ICLR.cc/2026/Conference/Submission7640/Reviewer_EcbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7640/Reviewer_EcbE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971461852, "cdate": 1761971461852, "tmdate": 1762919714626, "mdate": 1762919714626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for providing a feedback on our work. However, we believe the reviews miss the position of the paper and we thus withdraw it from further discussion as the comments and questions are orthogonal to our line of work.\n\n**Our work is not a replacement for large-scale text-to-image models, but a scientific experiment designed to challenge assumptions about scale.** As such, we fully acknowledge that training on ImageNet results in a model that is limited (e.g., there is not IPad in ImageNet) or biased (ImageNet is indeed object centric) compared to commercial releases. But our work is analytical and by training only on ImageNet, we move towards understanding why scale matters.\n\nWe think there are valuable and unexpected results in this paper:\n\n* Despite its limited size and diversity, training a T2I model on ImageNet works surprisingly well!\n* Surprisingly good understanding is achieved with synthetic captioning on limited data.\n* Surprisingly good compositionality is achieved with simple image augmentation, despite the object-centric bias.\n\nBesides, ImageNet provides a well-defined, stable and publicly available corpus that enables **controlled, reproducible experimentation**, thus setting a common ground for analyzing data efficiency, and generalization in T2I models. In comparison, a class-conditional setup used in all recent methodological advances (like REPA for instance) is saturated and not in line with the end goal of T2I. Instead, we provide a clean setup that allows future T2I research to evaluate and compare on a **fully aligned benchmark that has all the attribute of reproducible and open science**."}}, "id": "yGYmUbRwqg", "forum": "dryNT8UgEn", "replyto": "dryNT8UgEn", "signatures": ["ICLR.cc/2026/Conference/Submission7640/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7640/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763238768962, "cdate": 1763238768962, "tmdate": 1763238768962, "mdate": 1763238768962, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the data augmentation on ImageNet to crute a small but high-quality text-to-image dataset from a classification dataset without captions. For the text (caption) augmentation, it utilizes a open-souce MLLM as a synthetic captioner to generate comprehensive caption with diverse attrubutions. To avoid overfitting in training and improve compositionality abilities, the authors propose the image augmentation approach by CutMix on the original ImageNet samples. They trained the DiT and CAD architectures on the proposed augmented ImageNet with 1.2M samples. It can achieve outperformed or comparable performance with larger-scale models trained on larger dataset, such as SDXL, Pixart on GenEval and DPG-Bench. Further experiments on aesthetics and efficiency supports their method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The propose data augmentation method on text and image of ImageNet is clear and well-motivated by original flaws: single categorical text, easy to overfit early, few samples for compositionality.\n\n2.  The performance gain over AIO baselines is obvious and the model is comparable with larger models trained on larger dataset on classical metrics and human alignment metrics, such as GenEval, DPGBench.\n\n3. The training and data efficiency make the method is easy to reproduce and practical."}, "weaknesses": {"value": "1. Limited Novelty:  Although the data agmentation method on text and image is simple and insightful and the motivation for designs is complete and reasonable, there is limited novelty for the entire pipelines: the MLLM-based captioner and CutMix image augmentation are not new approaches, and are already widely used, such as in T2I evaluation (MLLM-based captioner) and general visual augmentation (CutMix).\n\n2. The setting of \"Task specific finetuning: aesthetics\" is flawed. The data size of LAION-POP is on the similar scale level of ImageNet: 0.6M vs 1.2M. The performance of model further trained on the LAION-POP can not serve as the support \"suggesting that the model has hidden aesthetics capabilities\". It is not a light extra training to unlock the hidden capabilities, but a substantial additional supervised training. The performance gain is limited. Also, the test-time scaling results cannot serve as a evidence for authors' claim, as TTS is sensitive with the reward model and the comparaison between TTS of original model is missing. \n\n3. Limited baselines: the comparad SDXL, SD v1.5, SD v2.1 are not between current T2I SOTA.  More advanced small baselines should be included.\n\n4. The proposed data augmentation method is only conducted on ImageNet. Extra implementation on similar classification/caption-missing/object-centered dataset can further support the design of the augmentation pipeline."}, "questions": {"value": "Questions are listed in Weaknesses section (1-4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A8NiB7ozYA", "forum": "dryNT8UgEn", "replyto": "dryNT8UgEn", "signatures": ["ICLR.cc/2026/Conference/Submission7640/Reviewer_EEuG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7640/Reviewer_EEuG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125026360, "cdate": 1762125026360, "tmdate": 1762919714197, "mdate": 1762919714197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reproducible text-to-image generation framework trained solely on ImageNet, challenging the belief that massive web-scraped datasets are essential. Using synthetic text augmentation (via LLaVA-generated captions) and image augmentations (CutMix, cropping), the authors train compact DiT-I and CAD-I diffusion models with only 1.2M images. Despite the small dataset, their 400M-parameter models outperform larger systems like SDXL and PixArt-Σ on GenEval and DPGBench benchmarks. The models also serve as strong pretraining checkpoints for fine-tuning, achieving high aesthetic quality after adaptation to the LAION-POP dataset, all within a modest compute budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Data efficiency: Demonstrates that high-quality text-to-image generation can be achieved with 1/1000th the data and 1/10th the parameters of leading models such as SDXL, challenging the “bigger is better” paradigm.\n\n2. Low compute requirement: Trains full models within ~500 H100 GPU hours, making high-quality text-to-image generation accessible to small research groups.\n\n3. Strong quantitative results: Outperforms large-scale models (e.g., SDXL, PixArt-Σ) on GenEval (+6%) and DPGBench (+5%), showing surprising competitiveness given its compact size."}, "weaknesses": {"value": "1. Restricted data diversity: Even with synthetic captions, ImageNet remains object-centric and lacks diverse scenes, human activities, and fine-grained contextual relationships. This limits the model’s ability to generalize compositionally and handle abstract or artistic prompts.\n\n2. Synthetic caption bias: The text augmentation relies on automatically generated captions using LLaVA, which can introduce hallucinations, repetitive phrasing, and dataset-specific biases that weaken genuine text–image understanding.\n\n3. Overfitting risk: Due to the relatively small size of ImageNet (around 1.2 million images), the model begins to overfit after approximately 200k steps. Although augmentations like CutMix and cropping help, they do not fully eliminate the issue.\n\n4. Limited benchmark coverage: The evaluation focuses mainly on GenEval and DPGBench, which assess compositional accuracy but not creativity, long-text reasoning, or stylistic variation, leaving gaps in holistic model assessment."}, "questions": {"value": "1. Could the authors extend their evaluation to include creative or stylistic benchmarks to assess broader generation capabilities?\n\n2.  How might the model perform if trained with additional open datasets that include humans, scenes, or artistic elements alongside ImageNet?\n\n3. During fine-tuning, is there a measurable drop in compositional or semantic accuracy that accompanies the aesthetic gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fQ918rQT2q", "forum": "dryNT8UgEn", "replyto": "dryNT8UgEn", "signatures": ["ICLR.cc/2026/Conference/Submission7640/Reviewer_MBbt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7640/Reviewer_MBbt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161087985, "cdate": 1762161087985, "tmdate": 1762919713822, "mdate": 1762919713822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}