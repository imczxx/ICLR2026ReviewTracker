{"id": "mGzeNcMGFu", "number": 19110, "cdate": 1758293651108, "mdate": 1759897059071, "content": {"title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models", "abstract": "Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.", "tldr": "", "keywords": ["Formal Math", "Lean4 Benchmark", "LLM"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/babfb9b68dd0df14e304184a56c791fa324e1e79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark to evaluate the performance of large language models on formal mathematical proving across various topics at the high school and undergraduate levels. It situates the work in relation to existing datasets, emphasizing the benchmark’s larger scale and broader topic coverage. The authors conduct empirical evaluations to analyze the weaknesses of current LLM-based provers and summarize several common error patterns. To ensure correctness and formal validity, the benchmark is constructed through a human-in-the-loop process involving both LLMs and human experts. The syntax of the generated problems is verified using Lean 4, the semantics are checked by other LLMs, and final validation is performed by human reviewers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "●\tThe authors have made significant efforts to collect and compile a large set of mathematical proof problems for the proposed benchmark. The resulting dataset covers a much broader range of topics and difficulty levels than existing benchmarks, substantially extending the scope of the math-proving domain. \n\n●\tThe increased scale of the benchmark offers opportunities for further improving, fine-tuning, or post-processing LLMs for formal reasoning tasks. It also provides a convenient and comprehensive resource for researchers and practitioners interested in evaluating models across diverse mathematical areas and levels of difficulty."}, "weaknesses": {"value": "●\tWhile I appreciate the authors’ effort to present detailed findings and observations from using the proposed benchmark to evaluate existing LLMs, very little information about the dataset itself is provided in the main paper. This limits the paper’s clarity and informativeness. It would be helpful if the authors included a brief discussion of the benchmark’s data format, data collection process, and implemented evaluation metrics. If space is a concern, the presentation of Table 2 could be optimized to make room for these essential details. \n\n●\tContinuing from the last comment on information clarity, the description of the evaluation setup is missing in Sections 4 and 5, which makes the paper less self-contained without the appendix. A concise summary of the experimental configuration would improve readability and reproducibility. \n\n●\tThe paper points out that existing formal mathematics benchmarks are limited in scope, focusing mainly on a few high school or college-level topics. While I appreciate the broader coverage this benchmark brings, the paper should give stronger justification for the kind of depth and qualitative diversity it adds that’s beyond simply combining more problems. \n\n●\tThe benchmark mainly reports the Pass@K metric, which offers a limited view of performance. Adding other evaluation measures such as final proof accuracy, intermediate reasoning correctness, or step-by-step reasoning quality would give a more complete picture of model performance."}, "questions": {"value": "●\tIn Figure 2, both the undergraduate and high school math domains include Precalculus. Are there any overlapping questions between these two categories? If both domains involve Precalculus, how was it decided which problems belong to each level? \n\n●\tWhat other metrics might be useful for evaluating LLMs’ mathematical proving abilities beyond Pass@K? If such metrics exist, how difficult would it be to incorporate them into the current evaluation framework?  \n\n●\tCould the authors provide more details on the choice of the Lean 4 framework and what this decision implies in practice? How challenging is it to represent or formalize a mathematical proving problem within the Lean 4 environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qy6anUYvs2", "forum": "mGzeNcMGFu", "replyto": "mGzeNcMGFu", "signatures": ["ICLR.cc/2026/Conference/Submission19110/Reviewer_fQki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19110/Reviewer_fQki"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681096557, "cdate": 1761681096557, "tmdate": 1762931133694, "mdate": 1762931133694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"FormalMATH,\" a new, large-scale, and challenging benchmark for formal theorem proving, spanning from Olympic style problems to undergraduate mathematics. Its preparation involved auto-formalization by a specialized model, semantic verification via LLMs, and final expert validation, resulting in 5,560 problems.\n\nA comprehensive evaluation of current models on this benchmark reveals several key findings. First, overall performance is poor. Second, there are significant performance variations among models across different domains. Third, test-time scaling offers only subtle improvements. Finally, while the default Chain-of-Thought (CoT) prompting is beneficial, requiring a CoT plan before the formal proof is counterproductive.\n\nThe paper also attributes the models' failures to several common issues: resorting to inappropriate tactics for unsolvable problems, producing incomplete proofs with placeholders, struggling with complex inequalities, and generating redundant hypotheses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a highly automated pipeline for benchmark creation, which has successfully produced a large-scale dataset. This dataset is notable for its breadth, spanning a wide range of mathematical domains, and its depth, covering difficulty levels from undergraduate math down to the Olympiad level. This high degree of automation suggests strong potential for future scalability.\n\nThe evaluation of model performance is multi-faceted and meticulous. The analysis across different mathematical domains, the investigation of test-time scaling, and the study of Chain-of-Thought (CoT) prompting provide valuable observations.\n\nThe error analysis is very detailed. The identified issues (e.g., resorting to inappropriate tactics, producing incomplete proofs, struggling with inequalities, generating redundant hypotheses) are specific and actionable, providing direct insights that can inform engineering efforts to optimize LLM-based provers."}, "weaknesses": {"value": "Regarding the Analysis and Pipeline:\n- Performance Decline on Advanced Problems: The accuracy of the proposed pipeline shows a notable decline when applied to more advanced mathematical areas. Specifically, the success rate for the competition-level AIME problems was 37.6% (371/934), whereas it dropped to just 4.6% (67/1466) for the undergraduate-level HardMath problems. This indicates potential obstacles in formalizing high-difficulty problems within the pipeline.\n- Benchmark Imbalance and Potential for Overestimation: The imbalance between Olympiad-level and undergraduate-level problems in the final benchmark may lead to an overestimation of a model's true capabilities. It is plausible that the types of problems a model is good at formalizing are also the types of problems it is good at solving, creating a potential bias in the evaluation.\n- Unsound Conclusions from Key Findings: The paper's explanations for some of its key findings could be strengthened.\n  - The data supporting Finding 4 (the negative effect of pre-defined CoT plans) appears inconclusive. The success rates of the compared methods (51.7%, 51.2%, and 49.8% for DeepSeek-V1.5-RL) are too close to draw a strong conclusion. The narrow margin of difference (less than 2%) makes this finding seem unsound.\n  - The explanation for Finding 2 (performance variation across domains) may be incomplete. The paper suggests this is due to data distribution imbalances, but this may not be the only factor. For instance, the \"precalculus\" domain accounts for only 3% of the training data (Figure 8a) yet achieves the second-highest success rate of 33.71% (Figure 3). This discrepancy suggests that other factors, such as the different skills tested by each domain, could also contribute to this performance variance.\n\nRegarding Misleading Presentation:\n\n- Lack of Context for Table 2: In Table 2, the lower part displays relative performance on the FormalMATH-lite test set. The main text does not clarify that this test set was specifically constructed from a 50/50 split of problems that DeepSeek 1.5 RL could and could not solve, which means the model implicitly serves as a baseline. This lack of clarification could cause confusion for the reader.\n- Issues with Figure 3: In Figure 3, the axes are not labeled, though one can infer they represent success rates. More significantly, the scales for different domains appear inconsistent. Plotting them on a single chart in this manner allows for comparing models within a domain but can be seriously misleading when comparing performance across different domains.\n\nMinor Issues with Specific Examples:\n\n- Error in Example B.2: There appears to be a translation error from natural language to the formal statement. The original problem asks one to prove that two quantities approaching zero are of the same order, whereas the formalized result aims to prove that the difference between these two quantities approaches zero.\n- Inappropriate Suggestion in Section 5.1: In the \"Inabilities to Handle Complex Inequalities\" part of subsection 5.1, the suggestion to prove a^3 + b^3 + abc ≥ abc using the AM-GM inequality seems inappropriate. The inequality simplifies directly to a^3 + b^3 ≥ 0, and its connection to AM-GM is not apparent.\n\nIf the identified issues, particularly those concerning the soundness of the findings and the clarity of the data presentation, were to be addressed, the paper would likely merit a higher score."}, "questions": {"value": "1. Regarding the performance drop between AIME and HardMath problems, have you investigated the specific reasons or pipeline stages responsible for this decreased formalization success?\n2. Could you clarify how a semantic error like the one noted in Example B.2 was able to pass through the pipeline's verification stages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Au8AnaDDpU", "forum": "mGzeNcMGFu", "replyto": "mGzeNcMGFu", "signatures": ["ICLR.cc/2026/Conference/Submission19110/Reviewer_edMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19110/Reviewer_edMV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713733484, "cdate": 1761713733484, "tmdate": 1762931133318, "mdate": 1762931133318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and releases FormalMATH, a large-scale benchmark for formal mathematical reasoning in Lean4. Constructed through a human-in-the-loop process, the benchmark contains 5,560 formally verified mathematical problems, covering multiple domains from high school Olympiads to the undergraduate level. The authors use this benchmark to comprehensively evaluate current mainstream Large Language Model (LLM) theorem provers, revealing significant limitations in their performance, domain generalization, and responsiveness to different reasoning strategies. Furthermore, the paper provides an in-depth analysis of common error patterns in these provers, offering valuable insights for future research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The greatest contribution of this paper is the construction of FormalMATH, a benchmark of immense scale and broad domain coverage. This work is a significant engineering achievement in itself, providing the community with an extremely valuable resource that will help to more accurately measure and advance the field of formal mathematical reasoning. The authors conduct a systematic evaluation of existing LLM provers, and their findings are very interesting and insightful. In particular, the analysis of performance discrepancies across different mathematical domains, the over-reliance on automated tactics, and the performance variations under different prompting strategies (Vanilla, CoT, NL-augmented CoT) reveal deep-seated issues with current methods. Section 5 of the paper categorizes and analyzes common proof failure modes, such as the improper use of automated tactics, incomplete proofs, and difficulties in handling complex inequalities. This section is highly valuable for future researchers looking to improve models and design new training strategies."}, "weaknesses": {"value": "Although FormalMATH surpasses previous benchmarks in scale, several similar works already exist in this area (e.g., MiniF2F, ProofNet), which limits the novelty of the task formulation and leads to a certain degree of homogenization. The data collection process described in the paper (Section 6), especially its first half, is very close to the methodology of similar projects like \"lean-workbook.\" The authors appear to have drawn from these works but have not explicitly acknowledged or cited them in the text. To clearly define the original contributions of this paper, the authors should add a detailed comparison and discussion with these related works.\nIn Finding 1, the paper states: \"SFT baseline achieves 8.97% accuracy, its reinforcement learning (RL) variant improves only marginally to 10.18%–a mere +1.21% gain that exposes the diminishing returns of rule-based sparse reward shaping...\" Based on this marginal improvement, the authors draw grand conclusions about \"reward sparsity\" and \"combinatorial search complexity\" limiting LLM provers. However, this line of reasoning is unconvincing. Firstly, the cited DeepSeek-Prover-V1.5-RL model was, according to its original paper, trained using MCTS, not the SPG method that the authors used for comparison with other models in their experiments. Secondly, the models being compared differ greatly in their training methods, data used, and base model architectures. Drawing such aggressive and general conclusions from a single, potentially coincidental result from a model in a specific setting lacks scientific rigor.\nFigure 1(a) is arguably the most important numerical results chart in the paper, as it shows the performance of various models on the full FormalMATH test set. However, the color contrast in the bar chart is too low, making it difficult to distinguish the results of different models and seriously harming the figure's readability. More importantly, when discussing the performance of these models later in the text, the paper frequently references these results without explicitly stating that it is referring to Figure 1(a) (e.g., in the discussion of Finding 1). This creates significant confusion for the reader, who must repeatedly jump between the text and figures to find the corresponding information. I strongly recommend that the authors present all detailed results for the models on the full FormalMATH test set in a clear and comprehensive table.\n\nIn Table 2, the accuracy of the DeepSeek-Prover-V1.5-RL model using the BFS search method (e.g., 4.91% in the $1 \\times 32 \\times 100$ setting) is dramatically lower than its accuracy using the SPG (Single-Pass Generation) method (47.98% in the Pass@32 setting). The gap is enormous. The paper merely presents this data without offering any explanation or analysis. Did the authors observe any interesting phenomena during the experiments (e.g., the model frequently getting stuck in local optima or generating invalid intermediate states) that could explain this?"}, "questions": {"value": "1.\tIn Table 2, the performance of the BFS-based DeepSeek-Prover-V1.5-RL is far below that of its SPG-based counterpart, which is counter-intuitive. Did you observe any specific failure modes or behaviors during your experiments (e.g., search space explosion) that could explain this performance discrepancy?\n2.\tThe first half of your data collection process is very similar to the methodology of projects like \"lean-workbook.\" Could you please clarify the relationship between your work and these projects and cite them appropriately in the paper to make your original contributions clear?\n3.\tCould you consider presenting the results from Figure 1(a) in a clearer table and placing it alongside or clearly distinguishing it from Table 2 (results on FormalMATH-Lite) so that readers can more easily compare and understand the models' performance across different test sets and settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "37gTUtDFw2", "forum": "mGzeNcMGFu", "replyto": "mGzeNcMGFu", "signatures": ["ICLR.cc/2026/Conference/Submission19110/Reviewer_nPf9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19110/Reviewer_nPf9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922667250, "cdate": 1761922667250, "tmdate": 1762931132848, "mdate": 1762931132848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FormalMATH, a formalized mathematics statements dataset for evaluating automated theorem-proving capabilities, containing 5,560 problems. The problems span topics from high-school Olympiad mathematics to undergraduate-level mathematics, covering many areas such as calculus. The dataset was constructed using a human-in-the-loop process: after supervised fine-tuning of Qwen-2.5-7B, autoformalization was applied, followed by LLM-as-a-judge semantic verification and a “Proving Its Negation” step, and finally expert verification. Experiments run with several commonly used automated theorem provers demonstrate the dataset’s quality and provide a detailed analysis of current provers’ weaknesses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dataset is large—about an order of magnitude bigger than the commonly used expert-annotated ATP benchmarks—and covers a wide variety of topics. All items received expert verification, which makes the collection highly valuable to the automated theorem proving community. It includes domains that are uncommon in automated formalization systems (e.g., applied mathematics, calculus, discrete mathematics). Unlike many large-scale automated formalization efforts that cannot guarantee statement correctness, these theorems are expert-annotated, making the dataset a strong benchmark for evaluating ATP systems.\n\n2. The experimental design is thorough. The evaluation includes many current prover models and provides a FormalMATH-lite subset to enable more aggressive compute budgets (e.g., larger sampling on the lite set and smaller sampling on the full set). This practical setup lets one observe model performance across different sampling regimes.\n\n3. The analysis of current provers’ weaknesses is insightful. The paper documents domain-dependent performance biases, shows that natural-language answers can harm proof success when used for chain-of-thought reasoning, identifies misuse of auto tactics, and so on. These findings give useful guidance for future ATP development."}, "weaknesses": {"value": "1. Limited novelty: Using an autoformalizer to formalize a dataset followed by human verification is not a new approach, and techniques such as LLM-as-a-judge and “Proving Its Negation” have been explored in prior work [1, 2, 3]. The main contribution of this paper is the scale of the dataset and the amount of manual annotation rather than a significant methodological innovation.\n\n2. Potential dataset imbalance: Undergraduate-level problems make up a much smaller share of the dataset compared to high-school problems. Table 6 shows undergraduate items account for less than 10% of the total. This imbalance is not clearly highlighted in the main text, and presenting undergraduate and high-school items side-by-side may be misleading and overstate the dataset’s quality.\n\n3. Distributional mismatch in FormalMATH-Lite: FormalMATH-Lite is intended as a representative subset of FormalMATH, but provers’ performance differs substantially between the lite subset and the full dataset. For example, DeepSeekProver V2 achieves Pass@32 ≈ 56% on FormalMATH-Lite (Table 2) but only 28.31% on the full FormalMATH (Figure 1). The authors should provide further explanation for this discrepancy.\n\n4. Formatting and presentation issues:\n\n- Figure 5 is referenced on page 3 (Line 147) but placed on page 8 (Line 432).\n- Figure 2 is cited earlier (Line 071) than Figure 1 (Line 148).\n- The color differences between segments in the pie charts of Figures 2 and 6 are too subtle, making them hard to distinguish.\n\n[1] Gao, G., Wang, Y., Jiang, J., Gao, Q., Qin, Z., Xu, T., & Dong, B. (2024). *Herald: A Natural Language Annotated Lean 4 Dataset* (No. arXiv:2410.10878). arXiv. https://doi.org/10.48550/arXiv.2410.10878\n\n[2] Lu, J., Wan, Y., Liu, Z., Huang, Y., Xiong, J., Liu, C., Shen, J., Jin, H., Zhang, J., Wang, H., Yang, Z., Tang, J., & Guo, Z. (2024). *Process-Driven Autoformalization in Lean 4* (No. arXiv:2406.01940). arXiv. https://doi.org/10.48550/arXiv.2406.01940\n\n[3] Liu, C., Shen, J., Xin, H., Liu, Z., Yuan, Y., Wang, H., Ju, W., Zheng, C., Yin, Y., Li, L., Zhang, M., & Liu, Q. (2023). *FIMO: A Challenge Formal Dataset for Automated Theorem Proving* (No. arXiv:2309.04295). arXiv. https://doi.org/10.48550/arXiv.2309.04295"}, "questions": {"value": "1. I note that the conversion rate for High School problems in Table 6 (i.e., the ratio #S.Formal / Size) is relatively high—approaching about one third—whereas the conversion rate for Undergraduate problems is much lower; for example, DEMIMATH is below 2%. I believe this is an important factor contributing to the dataset imbalance described in Weakness 2. What explains this difference in conversion rates? Can it be reduced by any mitigation strategies?\n\n2. Related to Weakness 3, please provide more explanation for why the same provers show such different performance on FormalMATH-Lite versus the full FormalMATH. This bears directly on whether FormalMATH-Lite is truly representative of the full dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BbiYbMsN5Q", "forum": "mGzeNcMGFu", "replyto": "mGzeNcMGFu", "signatures": ["ICLR.cc/2026/Conference/Submission19110/Reviewer_VLfC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19110/Reviewer_VLfC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986016613, "cdate": 1761986016613, "tmdate": 1762931132469, "mdate": 1762931132469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FormalMATH, a large Lean4 benchmark of 5,560 formally verified math problems spanning high-school Olympiad through undergraduate topics (algebra, number theory, discrete math, geometry, calculus, applied math). Problems are produced via a human-in-the-loop autoformalization pipeline (multi-LLM generation and semantic verification, negation-based disproof, and IMO-level expert review). The benchmark exposes significant gaps in current LLM theorem provers: on the full FormalMATH set under practical budgets, best reported accuracy is 28.31% (DeepSeek-Prover-V2 671B), with many systems performing far lower; provers show domain bias (stronger in algebra, weaker in calculus/discrete); test-time scaling yields only modest gains; and naïve CoT outperforms NL-augmented CoT. The authors also publish FormalMATH-Lite (425 items) to study scaling (Pass@K up to 3200), analyze common Lean4 failure patterns, and provide implementation details for faster parallel verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Scale + scope. The dataset is substantially larger than prior Lean4 benchmarks and covers diverse domains/difficulties, addressing both scope and saturation issues (Table 1; Figure 2).\n\n- Failure taxonomy grounded in Lean. Systematic analysis of misused automation, incomplete proofs, inequality handling, and redundant hypotheses with percentages (Table 3) and case studies (Table 8).\n\n- Careful, multi-stage curation. Pipeline: Lean4 compile-filter -> multi-LLM semantic checks -> negation-based disproof -> 12 IMO-level experts. Preservation rates and cost stats are reported (Figure 1b; Table 4)."}, "weaknesses": {"value": "- The negative effect of NL guidance is compelling (Figure 4), but demonstrated on two DeepSeek-V1.5 variants only; perplexity is used as a proxy of uncertainty.\n\n- Full vs. Lite performance is a little confusing. The abstract and Section 4 emphasize low accuracies on FormalMATH-All (e.g., Kimina-Prover 16.46% Pass@32; DeepSeek-V2 671B 28.31%), while Table 2 shows much higher Pass@K on FormalMATH-Lite. The selection procedure for Lite uses outcome-driven sampling with DeepSeek-V1.5-RL, which can bias difficulty."}, "questions": {"value": "- How sensitive are Table 2 results to the selection policy? If you rebuild Lite with a different seed or with multi-prover selection, do you expect that the Pass@K curves change significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bmrXJ1qRwA", "forum": "mGzeNcMGFu", "replyto": "mGzeNcMGFu", "signatures": ["ICLR.cc/2026/Conference/Submission19110/Reviewer_3nNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19110/Reviewer_3nNE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061086314, "cdate": 1762061086314, "tmdate": 1762931131963, "mdate": 1762931131963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}