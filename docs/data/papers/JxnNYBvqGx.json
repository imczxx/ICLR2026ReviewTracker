{"id": "JxnNYBvqGx", "number": 25547, "cdate": 1758369028898, "mdate": 1758806402924, "content": {"title": "Accelerating Pre-Trained Speech Foundation Model Deployment Using Randomly Recursive Transformers", "abstract": "Compressing pre-trained speech foundation models has been studied to address the high computational costs of the large-scale models. To achieve this goal, knowledge distillation is a widely applied technique that reduces the width or depth of the Transformer architectures. However, restricting the number of parameters leads to decreased model capability, which significantly limits the performance of complex speech processing tasks such as automatic speech recognition and phoneme recognition. In this study, we explore a layer sharing method for speech foundation model distillation, in which the layers are recursively shared across Transformer stacks, thereby reducing parameters while preserving performance. Furthermore, we introduce Randomly Recursive Transformers distilled on random recursions. Due to the randomness of recursion, the distilled Randomly Recursive Transformer can facilitate fine-tuning of various depth models along with recursion, unlike previous distillation methods that delay model deployment by demanding repetitive architecture-wise training for each resource requirement. To train Randomly Recursive Transformers, we propose a practical low-resource training method, stochastic batch advancing, which can train a random recursion model under limited computation. We experimentally verify the efficacy of layer recursion on various speech processing tasks using SUPERB by achieving significant performance improvements. We also demonstrate that our method can fine-tune multiple automatic speech recognition models with varying recursion using a single distillation process.", "tldr": "This paper studies layer sharing for speech foundation model compression. In particular, we introduce Randomly Recursive Transformers and its training method under a low-resource environment .", "keywords": ["speech foundation models", "knowledge distillation", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}