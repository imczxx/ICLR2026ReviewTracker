{"id": "WbV6w9R2Ox", "number": 10120, "cdate": 1758161190952, "mdate": 1759897672594, "content": {"title": "Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check", "abstract": "Current unlearning metrics for generative models evaluate success based on reference responses or classifier outputs rather than assessing the core objective: whether the unlearned model behaves indistinguishably from a model that never saw the unwanted data. This reference-specific approach creates systematic blind spots, allowing models to appear successful while retaining unwanted knowledge accessible through alternative prompts or attacks. We address these limitations by proposing Functional Alignment for Distributional Equivalence (FADE), a novel metric that measures distributional similarity between unlearned and reference models by comparing bidirectional likelihood assignments over generated samples. Unlike existing approaches that rely on predetermined references, FADE captures functional alignment across the entire output distribution, providing a principled assessment of genuine unlearning. Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas benchmark for text-to-image diffusion model unlearning reveal that methods achieving near-optimal scores on traditional metrics fail to achieve distributional equivalence, with many becoming more distant from the gold standard than before unlearning. These findings expose fundamental gaps in current evaluation practices and demonstrate that FADE provides a more robust foundation for developing and assessing truly effective unlearning methods.", "tldr": "We propose a novel metric for generative model unlearning that compares models at the distribution-level.", "keywords": ["Unlearning Evaluation", "Large Language Models", "Text-to-Image Diffusion Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c54368b3de9d8893584ef903f85b84be8c78b63.pdf", "supplementary_material": "/attachment/77a2280a39bdbb9174d11d85aaa22423d014032a.zip"}, "replies": [{"content": {"summary": {"value": "Overall, this is work provides keen insights and moves the machine unlearning field in the right direction. The observation that unlearning models should match gold standard retain models is well-motivated. The work is well-organized, and it fits well in the literature. My main concern is that it is unclear if choice of retain model affects the FADE score. The authors claim that FADE is more robust, but I did not find a sensitivity study which answers this question. Moreover, the authors observe that FADE is typically far better for the original model than for any unlearned model. While FADE appears useful for comparing unlearned models, it calls into question whether FADE is sensitive to behaviors of the retain model that aren’t important in practical scenarios. I believe this is a promising work, and I would be happy to adjust my score if my concerns are addressed.\n\nThe authors claim that the core objective of unlearning is that the unlearned model should behave indistinguishably from a retrained model and that current unlearning metrics do not accomplish this. The authors introduce FADE which measures the symmetric KL divergence of the retain model distribution and the unlearned model distribution given some condition. The proposed metric disagrees with current metrics on TOFU and UnlearnCanvas, indicating that current metrics are not good measures of equivalence with (gold standard) retrain models.\nThe authors find that forget quality (as measured by current metrics on TOFU) is sensitive to choice of reference answer, indicating that current metrics are brittle in evaluating unlearning performance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Distributional measures of similarity between the unlearned model and a retrained model are a step in the right direction for unlearning metrics, and the authors’ insights regarding this are valuable\n\nThe paper is clearly written and easy to follow. It is well grounded in literature.\n\nThe observation that unlearning should move beyond static evaluations is valid and moves the field in the right direction.\n\nThe consistency of FADE in figure 5 is compelling (compared with differing behaviors of FQ)"}, "weaknesses": {"value": "It is not immediately clear why the distribution of a single unlearned model should fit the exact distribution of a single retrain model. Exact unlearning in the literature (Nguyen et al 2025 “A Survey of Machine Unlearning”) is the case where the distribution of retrain models matches the distribution of unlearned models. Wouldn’t it be more appropriate to measure the expected FADE (or something similar) over distributions of models?\n\nThere is no study of the robustness of FADE to choice of reference model. Is this more robust than FQ?\n\nQuestions\nWhat is “functional alignment”? It seems to be matching output distributions or something to that effect.\n\nIs FADE robust to choice of reference model (i.e., is it stable when you test it with different randomly initialized and retrained models)?\n\nLine 122 “We expect to achieve more robust unlearning that better withstands such post-unlearning attacks.” If FADE measures how well unlearned models mimic a retained model, why would FADE help make unlearning methods more robust to post-unlearning attacks? My understanding is that the latent information remains inside the model and it is not revealed at the logits/output until after the attack.\n\nWhat is the variance of FADE across multiple unlearned models compared to the same reference model?\n\nOne could argue that it shouldn’t matter if an unlearned model maps the target concept to reasonable concept A vs reasonable concept B, but this could significantly impact FADE. What is the variance of FADE in this case, and should these semantics matter in practical scenarios?"}, "questions": {"value": "What is “functional alignment”? It seems to be matching output distributions or something to that effect.\n\nIs FADE robust to choice of reference model (i.e., is it stable when you test it with different randomly initialized and retrained models)?\n\nLine 122 “We expect to achieve more robust unlearning that better withstands such post-unlearning attacks.” If FADE measures how well unlearned models mimic a retained model, why would FADE help make unlearning methods more robust to post-unlearning attacks? My understanding is that the latent information remains inside the model and it is not revealed at the logits/output until after the attack.\n\nWhat is the variance of FADE across multiple unlearned models compared to the same reference model?\n\nOne could argue that it shouldn’t matter if an unlearned model maps the target concept to reasonable concept A vs reasonable concept B, but this could significantly impact FADE. What is the variance of FADE in this case, and should these semantics matter in practical scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EqiSg2TcF2", "forum": "WbV6w9R2Ox", "replyto": "WbV6w9R2Ox", "signatures": ["ICLR.cc/2026/Conference/Submission10120/Reviewer_1R1j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10120/Reviewer_1R1j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849246930, "cdate": 1761849246930, "tmdate": 1762921493630, "mdate": 1762921493630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to All Reviewers"}, "comment": {"value": "We express our sincere gratitude to the reviewers for their constructive feedback and for highlighting the significance of the problem we address. We are deeply encouraged by reviewers' acknowledgment of various strengths in our work, namely:\n- **Identifying Critical Gaps and Redirecting the Field [PTzk, 8y78, 7Aru, 1R1j].** FADE exposes a significant blind-spot in current practice by demonstrating that existing reference-specific metrics systematically overestimate unlearning efficacy, and redirects the field toward more rigorous evaluation standards.\n\n- **Principled Foundation with Strong Empirical Validation [PTzk, 8y78, 7Aru, 1R1j].** FADE provides a principled approach of measuring unlearning efficacy via distributional equivalence with retain-only models (the gold standard for exact unlearning) and is supported by compelling empirical evidence.\n\n- **Modality-agnostic Design and Applicability [PTzk, 7Aru].** FADE applies across multiple modalities (LLMs and T2I diffusion models) with a unified framework, demonstrating generality beyond existing metrics that are modality-specific.\n\nIn the following, we briefly discuss two concerns expressed by multiple reviewers, particularly on (1) **the practicality and compute-cost aspect of FADE**, and (2) **its robustness to varying decoding strategies**. More detailed responses to all comments can be found in the respective comments below.\n\n- **FADE's practicality and compute cost [for PTzk and 8y78].** Regarding FADE's use of retain-only models: while they require additional compute, **retain-only models provide ground truth baselines essential for rigorous evaluation**. In addition, **this requirement is already standard practice**, as the widely-adopted TOFU benchmark's Forget Quality metric also requires retain-only models. We will open-source all checkpoints upon acceptance to eliminate redundant costs. \nRegarding the computational cost of FADE, we provide runtime estimates of FADE in the responses below, but would also like to point out that (1) **evaluation accuracy well-justifies the cost** as inaccurate metrics risk misdirecting research far more costly than proper evaluation, (2) **FADE is highly parallelizable across multiple GPUs**, and (3) **our ablations suggest FADE remains robust with as few as 10 samples per prompt**, allowing practitioners to reduce costs while preserving key findings.\n\n- **FADE's robustness to decoding strategy [for PTzk and 8y78].** We conducted comprehensive ablation studies to assess FADE's robustness across alternative decoding strategies, such as Nucleus Sampling and Beam Search, which may introduce estimation bias unlike multinomial sampling used in FADE as default. Results suggest (1) **FADE values remain consistent across different decoding strategies**, and (2) our core finding holds throughout: **regardless of decoding strategy, existing methods fail to approach the retain-only baseline**. Please refer to the reviewer-specific comments for detailed results."}}, "id": "1kNgGrPuPT", "forum": "WbV6w9R2Ox", "replyto": "WbV6w9R2Ox", "signatures": ["ICLR.cc/2026/Conference/Submission10120/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10120/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10120/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763627044500, "cdate": 1763627044500, "tmdate": 1763627044500, "mdate": 1763627044500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a distribution-level unlearning metric FADE, to address the limitations of previous reference-specific approaches. Moreover, its experiments expose the failure of existing unlearning methods under this new metric."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper effectively reveals the problems that exist in current evaluation metrics.\n2. The experiments are comprehensive."}, "weaknesses": {"value": "1. This paper points out many shortcomings in current evaluation methods. This part is convincing but not surprising. After that, the paper proposes a new method called FADE. The main conclusion from the experiments is that under the FADE metric, existing unlearning methods perform poorly. However, the paper does not go on to propose more effective unlearning methods, making it feel quite incomplete. Based on the presented results, I cannot confirm that FADE is a flawless metric that could be widely accepted. Overall, the paper gives me the impression of lacking significant conclusions and constructive insight.\n2. The LLM and T2I parts do not feel like a cohesive whole. Moreover, the FADE mentioned in line 245 and the formula in line 266 appear to be two completely different metrics, yet they share the same name. The discussions of these two parts are also quite disconnected, making the paper difficult to read.\n3. From Figure 4, I'm not convinced by the author's claim in Line 228 that unlearned models generate inconsistent images. I think the three images from Ediff/ ESD share a similar style."}, "questions": {"value": "I notice that retrained models are finetuned for 5 epochs on the retained dataset, but the unlearned models are tuned with LoRA from the base model.   I'm wondering if LoRA fine-tuning itself introduced some stuff you missed. To verify it, you can continue to tune a retained model with LoRA for 5 epochs and denote the resulting models as LoRA-Retain models. Then you can get a new baseline like the dashed line in Figure 5 by averaging over randomness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "poopQ9O8DA", "forum": "WbV6w9R2Ox", "replyto": "WbV6w9R2Ox", "signatures": ["ICLR.cc/2026/Conference/Submission10120/Reviewer_7Aru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10120/Reviewer_7Aru"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881343526, "cdate": 1761881343526, "tmdate": 1762921493172, "mdate": 1762921493172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines current unlearning evaluation metrics for generative models, arguing that prevalent reference-specific metrics fail to assess true unlearning. The paper introduces FADE to quantify distributional similarity between an unlearned model and a retain-only model using a bidirectional likelihood comparison on generated outputs. Experiments on LLM unlearning and T2I demonstrate that unlearning methods may appear successful under traditional metrics but exhibit significant distributional discrepancies not captured by those evaluations, whereas FADE exposes these failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and convincingly demonstrates the limitations of current unlearning evaluation practices, providing empirical and theoretical support for the claim that reference-specific metrics could result in overestimating unlearning efficacy.\n2. The problem and motivation identified in the paper are novel. FADE is implemented in a way that is modality-agnostic, working for both autoregressive language models and diffusion models.\n3. The work addresses a significant and growing issue as unlearning gains attention for safety/privacy/ethical AI deployment. The results challenge prevailing practices and set a high standard for future work in the area."}, "weaknesses": {"value": "1.  The mathematical description of FADE is clear. Still, the paper does not seem to present deeper theoretical guarantees or formal links between FADE and true indistinguishability in the full probabilistic sense.\n2. The core of the proposed method lies in using comparative scores to evaluate the differences between the unlearned model and the retain-only model. That said, it seems that one potentially informative control experiment is absent- Namely, assessing the comparative scores between different unlearned models. Incorporating such an experiment could help enhance the generality of the findings and strengthen the rigor of the validation.\n3. It appears that the paper does not include a discussion or experimental evaluation of the computational/time/token cost associated with the newly proposed metric, which could be an important aspect. Addressing this point may provide a more comprehensive assessment of the metric’s practicality.\n4. Although the writing is generally clear and easy to follow, there appear to be numerous instances that resemble AI-generated text patterns (the frequent use of “–” symbols), though I may be mistaken.\n5. In practice, likelihood-based comparison metrics (such as FADE) can be sensitive to the sampling strategy. The paper does not explore whether alternative sampling schemes (diverse decoding, hard negative mining, etc.) could strengthen or weaken FADE as a metric.\n6. there’s comparatively little discussion of hyperparameter, optimizer, or architecture robustness"}, "questions": {"value": "1. Could the authors provide comparative experimental results regarding computational cost and time overhead?\n2. Would it be possible for the authors to include control experiment results involving mutual scoring between two unlearned models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VwEFlNzPA4", "forum": "WbV6w9R2Ox", "replyto": "WbV6w9R2Ox", "signatures": ["ICLR.cc/2026/Conference/Submission10120/Reviewer_8y78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10120/Reviewer_8y78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896075941, "cdate": 1761896075941, "tmdate": 1762921492814, "mdate": 1762921492814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights an existing problem in unlearning: unreliable metrics for verifying unlearning for generative models. The paper proposes Functional Alignment for Distributional Equivalence (FADE), which evaluates distributional alignment with retain-only oracles through bidirectional likelihood comparisons over generated samples. The experimental results evaluate the FADE score on language models and text-to-image diffusion models and several well-known unlearning methods. It shows that existing metrics such as unlearning accuracy can be misleading for evaluating the efficacy of unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Grounded definition. The paper frames unlearning as achieving functional alignment with a retain-only model (i.e., behaving as if the forgotten data was never seen). This aligns with the gold standard (exact perfect unlearning) and provides a clear conceptual foundation.\n\n\n\n2. Points to a significant gap in existing work. The authors highlight that current evaluation methods rely on reference-specific proxies (e.g., fixed answers, classifiers), which can mask failures and even allow recovery attacks. Thus, the work exposes significant blind spots in prevailing practice.\n\n\n\n3. FADE measures full distributional equivalence bidirectionally, not just task-specific correctness. It applies across modalities (LLMs and diffusion models) and detects subtle failures that reference-conditioned metrics overlook."}, "weaknesses": {"value": "1. Application. While the FADE metric and evaluation are important to emphasize existing problems, FADE requires having a retain-only model. Thus, it is not clear whether it can be practically implemented (or a proxy of it) to improve the unlearning method itself. \n\n2. Computationally expensive. It is computationally expensive to compute the retain-only model under different seeds since it requires training the model without the forget set samples. FADE requires Monte-Carlo style sampling and likelihood estimates (or denoising approximations for diffusion models), which may be expensive and subject to variance depending on chosen sampling strategies."}, "questions": {"value": "1. What do we learn from the FADE metric and evaluation for future unlearning methods? It would be good if the authors can comment on how they see their metric applied in evaluations and future unlearning methods.\n\n2. How sensitive is FADE to generation strategy? Since LLM measurement relies on top-k/nucleus sampling, do different decoding strategies change FADE outcomes? How consistent are evaluations across sampling choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cn7L5eIVwE", "forum": "WbV6w9R2Ox", "replyto": "WbV6w9R2Ox", "signatures": ["ICLR.cc/2026/Conference/Submission10120/Reviewer_PTzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10120/Reviewer_PTzk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977406777, "cdate": 1761977406777, "tmdate": 1762921492442, "mdate": 1762921492442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}