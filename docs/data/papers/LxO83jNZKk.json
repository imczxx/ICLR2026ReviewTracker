{"id": "LxO83jNZKk", "number": 15314, "cdate": 1758250188437, "mdate": 1763702196870, "content": {"title": "Consistency-Driven Calibration and Matching for Few-Shot Class Incremental Learning", "abstract": "Few-Shot Class Incremental Learning (FSCIL) is crucial for adapting to the complex open-world environments. Contemporary prospective learning-based space construction methods struggle to balance old and new knowledge, as prototype bias and rigid structures limit the expressive capacity of the embedding space. Different from these strategies, we rethink the optimization dilemma from the perspective of feature-structure dual consistency, and propose a Consistency-driven Calibration and Matching (ConCM) framework that systematically mitigates the knowledge conflict inherent in FSCIL. Specifically, inspired by hippocampal associative memory, we design a memory-aware prototype calibration that extracts generalized semantic attributes from base classes and reintegrates them into novel classes to enhance the conceptual center consistency of features. Further, to consolidate memory associations, we propose dynamic structure matching, which adaptively aligns the calibrated features to a session-specific optimal manifold space, ensuring cross-session structure consistency. This process requires no class-number priors and is theoretically guaranteed to achieve geometric optimality and maximum matching. On large-scale FSCIL benchmarks including mini-ImageNet, CIFAR100 and CUB200, ConCM achieves state-of-the-art performance, with harmonic accuracy gains of up to 3.90% in incremental sessions. Code is available at: https://anonymous.4open.science/r/ConCM-7385", "tldr": "To solve the knowledge conflict problem in FSCIL, we propose Consistency-Driven Calibration and Matching framework.", "keywords": ["Continual learning", "Few-shot class-incremental learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd7661e6fb4275d1256f992dade1e8c8a86f8702.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a consistency-driven calibration-and-matching approach that calibrates biased new-class prototypes and stabilizes the feature-space structure, thereby reducing confusion between old and new classes. The approach demonstrates strong performance across several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivations are clear. Biased new-class distributions caused by limited training samples, and adjustments to the feature space to accommodate new classes, are key challenges in FSCIL, as recognized by the community.\n2. Leveraging common attributes to augment the features of new classes is valid and reasonable; WordNet provides rich semantic information beyond visual cues.\n3. The proposed approach achieves strong performance on several datasets, including mini-ImageNet, CIFAR-100, and CUB-200."}, "weaknesses": {"value": "1. The idea of “Attribute Separation” is similar to PA [1], which decouples common attributes within a family and transfers them to new species. It would be helpful to discuss the commonalities and differences between your approach and PA [1].  \n[1] Prototype antithesis for biological few-shot class-incremental learning.\n2. Neural collapse theory has been introduced into the FSCIL task by NC-FSCIL. The dynamic structure matching in this paper builds on that work; for example, it replaces the hard enforcement of a computed prototype distribution with a softer approach that reduces the distance between the original and optimal distributions. While this is indeed more reasonable and effective, the core idea and methodology are not fundamentally different from NC-FSCIL; in my view, this is an incremental improvement.\n3. When performing prototype augmentation, this paper makes a strong assumption, namely, that feature distributions are Gaussian. However, many real-world class distributions do not align well with this assumption. Furthermore, based on this assumption, the authors infer the covariance of new classes by assuming that classes with similar means also have similar covariances. Is there theoretical justification for this assumption?\n4. The MPC module appears highly dependent on the base-class distribution and on WordNet. If the base classes differ substantially from the new classes, or if the names of the new classes are not present in WordNet’s knowledge base, the model’s generalization ability may be greatly affected.\n5. There are a few typos. For example, “mini-IamgeNet” at line 432 and “SOAT” at line 360 (these should be “mini-ImageNet” and “SOTA”)."}, "questions": {"value": "Please refer to the 'weaknesses'. I would consider to raise my rating if the authors could address my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zlLOj0oiQt", "forum": "LxO83jNZKk", "replyto": "LxO83jNZKk", "signatures": ["ICLR.cc/2026/Conference/Submission15314/Reviewer_Tr9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15314/Reviewer_Tr9T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644369545, "cdate": 1761644369545, "tmdate": 1762925612596, "mdate": 1762925612596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how to maintain representation consistency in Few-Shot Class-Incremental Learning (FSCIL). The authors propose a framework that calibrates new class prototypes inspired by human associative memory. The calibration module separates and completes semantic attributes to refine prototype representations for new classes, ensuring better alignment. Furthermore, a geometric optimization strategy is introduced to preserve structural consistency during incremental updates. Experimental results on multiple FSCIL benchmarks demonstrate consistent improvements over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, and the motivation is clear.\n2. The proposed consistency-driven dynamic structure matching method is theoretically grounded and achieves excellent performance.\n3. The ablation experiments are comprehensive and convincing."}, "weaknesses": {"value": "1. The paper missed several important and highly related references and comparative results. \n\n    [1]  Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class Incremental Learning.\n\n    [2] Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning.\n\n    [3] Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning.\n\n    [4] Towards Better Representation Learning for Few-Shot Class-Incremental Learning\n\n2. The pipeline of novel class prototypes calibration is the same as paper [1] except for the design of MPC network (both adopt the encode–aggregate–decode architecture). The novelty of this component should be further clarified.\n\n    [1] Prototype completion for few-shot learning. \n\n3. More recent research on CIL mainly focuses on pre-trained ViT or CLIP models. Can the proposed method be transferred or adapted to pre-trained ViT models or CLIP models?\n \n   [1] Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners."}, "questions": {"value": "1. It is unclear whether the proposed method can be generalized to other tasks, such as few-shot incremental semantic segmentation.\n2. What is the ratio hyper-parameter between the loss $L_{match}$ and $L_{Cont}$? Is it set to 1? Would it make a difference in performance when setting different values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7FB5YdTOga", "forum": "LxO83jNZKk", "replyto": "LxO83jNZKk", "signatures": ["ICLR.cc/2026/Conference/Submission15314/Reviewer_CEGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15314/Reviewer_CEGG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879721260, "cdate": 1761879721260, "tmdate": 1762925612027, "mdate": 1762925612027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the FSCIL problem by identifying two key challenges — feature inconsistency and structure inconsistency.\nThe proposed ConCM framework introduces (1) a Memory-aware Prototype Calibration (MPC) module that leverages semantic attributes to calibrate prototypes, and (2) a Dynamic Structure Matching (DSM) module that dynamically updates class geometry to maintain global consistency.\nExperiments on multiple benchmarks show that ConCM achieves better stability and accuracy than prior FSCIL methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed modules (MPC and DSM) are conceptually sound and complementary, leading to consistent and clear improvements across multiple benchmarks.\n\n2. The paper provides clear theoretical motivation and connects the design of DSM with neural collapse geometry, giving the framework better interpretability."}, "weaknesses": {"value": "1. The Memory-aware Prototype Calibration (MPC) relies on semantic attribute extraction from WordNet or class names, which might not generalize to datasets without clear textual labels.\n\n2. Conceptually, ConCM extends previous geometry-based or orthogonality-driven FSCIL ideas (e.g., OrCo, NC-based approaches), so its originality mainly lies in how these components are unified.\n\n3. Consider including at least one recent 2025 method to strengthen the comparison with up-to-date FSCIL approaches."}, "questions": {"value": "How sensitive is the performance to the semantic quality of the extracted attributes in MPC? Would ConCM still work well if semantic information is noisy or unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPrb71GN5q", "forum": "LxO83jNZKk", "replyto": "LxO83jNZKk", "signatures": ["ICLR.cc/2026/Conference/Submission15314/Reviewer_R41s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15314/Reviewer_R41s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762368893394, "cdate": 1762368893394, "tmdate": 1762925611387, "mdate": 1762925611387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ConCM, a novel two-stage framework for few-shot class-incremental learning that explicitly addresses the “dual-consistency” dilemma. By emulating hippocampal associative memory, the MPC module first calibrates few-shot prototypes with semantically related attributes extracted from base classes; the DSM module then dynamically updates the embedding geometry to satisfy both equi-distant separation and maximal matching with the previous structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe illustration is clear.\n2.\tThe method and the theoretical analysis seem solid.\n3.\tThe reported performance improvement is considerable."}, "weaknesses": {"value": "1.\tThe proposed ConCM uses WordNet to extract semantic attributes from class names, to calibrate the prototypes of the new classes. It works fine in the standard benchmarks such as cifar, imagenet, but if there are no semantic label names for each class, how would such calibration work?\n2.\tLack of results on benchmarks with more classes. ConCM relies on explicitly calibrating the feature space for each class. The paper does not discuss the influence of the increased number of classes, especially when the benchmarks in this paper contain at most 200 classes.\n3.\tTypo: Caption of Table 4 “mini-imagenet”."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FH7UEOta65", "forum": "LxO83jNZKk", "replyto": "LxO83jNZKk", "signatures": ["ICLR.cc/2026/Conference/Submission15314/Reviewer_Hr2g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15314/Reviewer_Hr2g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762418561198, "cdate": 1762418561198, "tmdate": 1762925610838, "mdate": 1762925610838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}