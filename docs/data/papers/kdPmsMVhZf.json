{"id": "kdPmsMVhZf", "number": 5709, "cdate": 1757928291819, "mdate": 1763720629629, "content": {"title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior", "abstract": "Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape–appearance ambiguities and degraded scene geometry.\nIn this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. \nWe first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. \nFurthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion.\nExtensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions.\nMoreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. See more results at https://g4splat.github.io.", "tldr": "G4Splat integrates accurate geometry guidance with generative prior to enhance 3D scene reconstruction, substantially improving both geometric fidelity and appearance quality in observed and unobserved regions.", "keywords": ["3D Scene Reconstruction", "Sparse View Reconstruction", "Generative Prior"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b97d3554cf48a230443ee028b53e041fce85812.pdf", "supplementary_material": "/attachment/9c914b70a3c9af51e5a950a13a4e79c4cfffe020.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes G4SPLAT, a geometry-guided framework for sparse-view 3D Gaussian Splatting reconstruction. Existing generative prior methods often suffer from degraded performance due to insufficient geometric supervision and multi-view inconsistency. G4SPLAT addresses these issues by deriving scale-accurate plane-aware depth maps  that leverage the prevalent planar structures in scenes. These accurate geometric cues are integrated into a geometry-guided generation process to refine visibility mask estimation, guide novel view selection, and improve cross-view consistency during video diffusion model inpainting. Experiments on Replica, ScanNet++, and DeepBlending datasets show that G4SPLAT outperforms state-of-the-art baselines in both geometric accuracy and appearance quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is clearly written, with well-explained derivations of plane-aware depth maps and the geometry-guided generation process, making G4SPLAT’s contributions and implementation easy to understand.\n- The proposed G4SPLAT takes advantage of plane structures in a scene to figure out depth with the right scale, which gives solid guidance even in areas that don’t have much information in sparse-view setups. Ablation results show that generative priors alone offer little geometric benefit and may even degrade quality, while adding plane-aware geometry modeling greatly improves accuracy and consistency."}, "weaknesses": {"value": "- The proposed method relies on the presence of significant planar structures in the scene, which may not be applicable in certain complex or natural environments. In scenes lacking prominent planar features, the generated plane-aware depth maps may not provide sufficient geometric cues, potentially affecting the quality of the final 3D reconstruction."}, "questions": {"value": "In constructing the Visibility Grid, how do the choices of voxel size and number of sampling points Q affect visibility accuracy and runtime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gL32aUNmzP", "forum": "kdPmsMVhZf", "replyto": "kdPmsMVhZf", "signatures": ["ICLR.cc/2026/Conference/Submission5709/Reviewer_3Jys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5709/Reviewer_3Jys"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904561153, "cdate": 1761904561153, "tmdate": 1762918209508, "mdate": 1762918209508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces G4Splat, a new method for sparse-view 3D scene reconstruction. Building upon existing work MAtCha Gaussians [1], the paper proposes to leverage estimated 3D planes throughout the entire reconstruction pipeline that additionally leverages a pre-trained video diffusion model as a prior. Making use of the usual existence of planar structures in 3D scenes created by humans, the authors propose to extract per-view 2D planes with SAM in order to merge them into global 3D planes.\nThese can then be used to obtain more accurate depth than from sparse-view SfM approaches like MASt3R-SfM [2] used in [1], and to compute improved visibility masks compared to rendered alpha maps, which are required as inpainting masks when combined with a generative prior.\nThe authors further leverage the 3D planes for a plane-aware selection of novel views for inpainting and finally for selecting supervision signals from the geometry-guided inpainting.\nAn experimental evaluation on ScanNet++, DeepBlending, and Replica shows that G4Splat consistently outperforms all baselines w.r.t. both 3D surface reconstruction and novel view synthesis quality. The paper further includes qualitative results for different numbers of input views and an ablation study w.r.t. the use of the generative prior, the 3D planes, and both combined.\n\n- [1] MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views. CVPR 2025\n- [2] MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion. 3DV 2025"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and mostly easy to follow and understand.\n  - The introduction motivates the topic well and discusses the idea following the Manhattan world assumption and the shortcomings of existing works leveraging generative priors.\n  - The related work section is comprehensive. I found Sec. 2.3. about the plane assumption in reconstruction especially interesting.\n  - The method section first introduces the 3D plane estimation step by step (first 2D and per view, then 3D and global), followed by all applications of the obtained 3D planes throughout the entire optimization pipeline.\n- I appreciate the method and paper story of making use of 3D planes everywhere throughout the 3D reconstruction pipeline, following the Manhattan world assumption.\n  - The step-wise estimation of the 3D planes using SAM in 2D, then merging these with RANSAC, is a reasonable and intuitive approach.\n  - The paper shows how to obtain more accurate depth maps and visibility masks using 3D planes as well as more useful novel view selection for inpainting with generative priors.\n    - Fig. 3 visualizes all of these improvements over naive alternatives well.\n- The quantitative and qualitative comparisons with baselines are overall very convincing.\n  - The proposed method consistently and significantly outperforms all baselines for both 3D surface reconstruction and novel view synthesis.\n  - The authors make sure to include a baseline of 2DGS combined with the video diffusion model that they use (See3D), as this model seems to be quite new and not used by baselines.\n  - The authors choose very recent and strong baselines, representing the current state of the art.\n- The runtime comparison shows that the proposed method is roughly as fast as previous works leveraging generative priors, while obtaining better quality.\n- The ablation study shows that all individual design choices contribute to the overall performance of the method.\n- The authors provide additional convincing qualitative results in form of videos on the anonymous website.\n- The appendix provides more results, detailed background to prior works that this paper leverages, training details, and limitations."}, "weaknesses": {"value": "- Since the method relies on 3D planes, it seems to be quite tailored for indoor and possibly city-like outdoor scenes, or in other words, scenes that actually consist of enough 3D planes. \n  - I am wondering how this method would perform on other kind of outdoor scenes, e.g., the garden or bike scenes in the MipNeRF360 dataset. Are there still enough planar structures to leverage?\n    - In the limitations section in the appendix, the authors do touch on this topic but mainly for obtaining scale-accurate depth.\n  - The paper, appendix, and website mostly show almost only indoor scenes.\n- The paper would benefit from a more precise evaluation of 3D reconstruction for observed and unobserved regions separately.\n  - It would be very interesting to see quantitatively how much each component improves observed and unobserved regions, respectively.\n- Concern about fairness w.r.t. generative prior:\n  - While the authors do provide a baseline of 2DGS + See3D, i.e., the video generative prior that they use in this paper, would MAtCha + See3D not be the better baseline in terms of fairness?\n    - In Fig.4, MAtCha looks quite accurate, just incomplete, whereas 2DGS + See3D has a lot of artifacts. Is this solely due to the generative prior See3D or also due to 2DGS being worse than MAtCha?\n  - Regarding the use of generative priors, the main paper (related work) would need some more details about which approach uses what kind of generative prior and how does it affect results.\n  - To this end, it would be beneficial for the paper to evaluate their method with the generative prior used by baselines, if that is possible. This would be important in order to precisely attribute the performance gains to a better generative prior or to the other technical contributions, e.g., using the 3D planes.\n- Some lack of clarity:\n  - The need for the background Sec. 3.1 about MAtCha Gaussians became only clear much later in Sec. 3.4 that then states that G4Splat builds on MAtCha as initialization. There is no description of the structure of Sec. 3 (after the Method title), but the paper just goes immediately into the background section after the related work, leaving the reader confused why another \"related work\" section in the method section is needed. Writing one or two sentences about the structure of the Method section and the individual subsections and why they are necessary at the beginning of the Method section would resolve this.\n  - From the main paper, it is unclear what role 3D plane extrapolation plays in the pipeline. Are 3D planes extrapolated to unobserved regions? Does this involve any additional assumptions about the layout of an indoor scene, for example?\n    - Line 74f.: \"planar surfaces allow depth extrapolation: a 3D plane can be reliably estimated from partial depth observations and then extended across the entire surface\" states something along those lines.\n  - In paragraph \"Per-view 2D Plane Extraction\" (lines 193ff.), it is unclear how you obtain the normals.\n  - Is the depth from the monocular depth estimator and scaled using the 3D plane geometry really better than the depth estimated by MASt3R-SfM? If this is the case, why? Are more recent DUSt3R follow-ups like VGGT maybe more performant?\n  - The color supervision description in lines 314f. is quite vague and refers to the appendix. If possible, it would be beneficial to move this information to the main paper to make it more self-contained, as otherwise there remain open questions that require the appendix.\n  - Tab.3 misses to explain what \"DS\" in \"Ours (DS)\" stands for. Actually, I was not able to find that information anywhere in the paper. I assume it uses a distilled video model maybe?\n  - The PM setting in the ablation study is not completely clear to me. Are the planes there only used for initialization and for depth supervision or for what parts of the pipeline exactly?\n- The paper misses a related work and potential baseline: Spurfies [1].\n\n\nReferences:\n- [1] Spurfies: Sparse Surface Reconstruction using Local Geometry Priors. 3DV 2025"}, "questions": {"value": "My suggestions to the authors are already detailed in the weaknesses section but here again concretely:\n- It would be interesting to evaluate the approach on more outdoor scenes with less planar structures like the garden and bike scenes in the MipNeRF360 dataset.\n  - While I do see the value in a method that performs particularly well for indoor scenes, this would be beneficial for the paper to better understand possible limitations.\n- Splitting the quantitative (and possibly qualitative) evaluation into observed and unobserved regions would be very interesting to evaluate the behavior of the individual contributions more precisely.\n- I suggest that the authors evaluate MAtCha + See3D (or is that already the second row, i.e., GP only of the ablation study maybe) instead of 2DGS + See3D. It would be the better comparison as a baseline in qualitative and quantitative evaluations in terms of fairness.\n- It would be interesting to also evaluate G4Splat in combination with different generative priors to see whether the 3D planes can make the use of generative priors more effective irrespective of the particular choice which prior to use.\n- Further open questions are:\n  - Are 3D planes extrapolated to unobserved regions? Does this involve additional assumptions? If so, how does it affect results?\n  - How are surface normals obtained (cf. lines 193ff.)?\n  - Why is the depth from monocular depth estimation and scaling better than from dense reconstruction approaches like MASt3R-SfM or VGGT?\n  - What does \"DS\" stand for in Tab.3?\n  - Could you give details about the PM setting in the ablation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HKFdRItDgn", "forum": "kdPmsMVhZf", "replyto": "kdPmsMVhZf", "signatures": ["ICLR.cc/2026/Conference/Submission5709/Reviewer_jjWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5709/Reviewer_jjWR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932997117, "cdate": 1761932997117, "tmdate": 1762918209288, "mdate": 1762918209288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "G4Splat proposes a reconstruction pipeline for sparse-view 3D scene reconstruction that integrates metric-scale geometry estimation (leveraging planar priors) with generative priors from pretrained diffusion/image models. Geometry guidance is injected at multiple stages (depth estimation, visibility masks, view selection, and inpainting) to reduce shape–appearance ambiguities and improve multi-view consistency in novel-view completion. Experiments on Replica, ScanNet++ and DeepBlending report improvements in both geometry and appearance, especially in unobserved regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Identifies a real limitation in prior generative-prior reconstructions (poor geometry in observed areas and inconsistency in unobserved areas) and supplies a concrete fix via planar metric depth and guided inpainting. \n\nEnd-to-end incorporation of geometry at multiple pipeline points (visibility masks, view selection) is sensible and likely to increase multi-view consistency. \n\nBroad evaluation on several standard datasets and claimed improvements on both geometry and appearance metrics."}, "weaknesses": {"value": "Reliance on planar priors: in scenes without significant planar structure (e.g., natural outdoor scenes, complex organic interiors), the metric-depth derivation may fail; robustness experiments for such cases are not prominent in the material on the forum page. \n\nIntegration with generative priors can still propagate biases from the generative model (style/appearance biases) - the paper does not analyze or mitigate such biases.\n\nComputational cost: combining geometry estimation, diffusion-based inpainting, and splatting can be expensive; readers would benefit from runtime and resource-use breakdowns and ablations on where the gains come from."}, "questions": {"value": "Provide quantitative robustness experiments on scenes with few planar structures — how does the depth prior behave and how does it affect final reconstructions? \n\nHow does G4Splat handle scale ambiguity when planar cues are incorrect or scarce? Provide failure cases.\n\nPlease report computational cost and latency for a representative scene, and ablate which component (plane-based depth, guided visibility, diffusion inpainting) provides the largest gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iq85gU1IwO", "forum": "kdPmsMVhZf", "replyto": "kdPmsMVhZf", "signatures": ["ICLR.cc/2026/Conference/Submission5709/Reviewer_HhvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5709/Reviewer_HhvE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974423384, "cdate": 1761974423384, "tmdate": 1762918209067, "mdate": 1762918209067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes G4SPLAT, a sparse-view 3DGS reconstruction method that uses planar depth priors and geometry-guided video diffusion refinement. The idea is to extract global 3D planar surfaces from input images, convert them into scale-accurate depth maps, and use these to supervise Gaussian optimization, select novel viewpoints, and constrain generative inpainting. The method iteratively refines the 3D model using inpainted novel views and recomputed plane-aware depth maps. Experiments on Replica, ScanNet++, and DeepBlending claim substantial gains in unobserved regions and single-view/unposed settings. Overall, the pipeline combines MAtCha-style plane-scaled depth, 2DGS, and video diffusion priors, but introduces significant engineering complexity in return for modest conceptual novelty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation: improving geometry for generative 3DGS. The paper correctly identifies geometry accuracy as a limiting factor in generative scene completion and explicitly seeks to address shape-appearance ambiguity. This motivation is grounded in observed weaknesses of recent diffusion-enhanced 3DGS methods.\n- Plane-based scale recovery is well-explained and technically coherent. The method extends plane fits across views using SAM, normal clustering, and RANSAC with multi-view consistency checks, yielding scale-aligned depth even in weakly observed areas. While not new, the pipeline is carefully engineered, and ablations show benefit over raw monocular depth.\n- The approach uses plane-aware visibility masks, plane-driven novel-view planning, and plane-based inpainting weighting to enforce consistency with the goal of stabilizing diffusion-assisted NVS. This produces fewer floaters and sharper planar regions compared to GenFusion / Difix3D+."}, "weaknesses": {"value": "- **Fundamental Reliance on Planar Structures:** The method's core contribution and primary advantage are fundamentally tied to the Manhattan-world assumption. While effective for artificial environments, this reliance makes the approach far less suitable for organic, non-planar scenes (e.g., natural landscapes, complex statues, foliage). The paper's solution for non-planar regions is to fall back on monocular depth estimation, which is the very technique it criticizes for scale ambiguity.\n- **Heavy Engineering Pipeline** - G4Splat is not a single model but a complex, multi-stage pipeline that glues together numerous off-the-shelf components like MAtCha, MASt3R-SfM, SAM, K-means clustering, RANSAC, a monocular depth estimator, and a video diffusion model. This high complexity makes the system brittle - a failure in any one component could compromise the entire pipeline. \n- The approach inherits a strong geometric scaffold from MAtCha, including scale-aligned depth, plane priors, and reliable surface initialization. This prior stabilizes 3DGS optimization in sparse-view regimes and likely contributes to the improved geometric integrity. In contrast, several baselines (e.g., GenFusion and Difix3D+) do not assume a comparable geometric initialization and instead operate in a more challenging setting where depth must be inferred solely from generative consistency. This makes the comparison somewhat imbalanced, and it becomes difficult to isolate how much of the improvement stems from the proposed refinements versus MAtCha's initialization advantage.\n- The qualitative comparisons in Fig. 4 appear to show holes and structural failures for generative baselines that are not typically reported in their original papers. This behavior is plausible when such models are applied without a metric depth prior or explicit plane constraints, especially under sparse or weakly posed conditions. However, the visual degradation suggests that the baselines may not have been given an equally stabilized geometric starting point. A clearer description of how pose supervision, depth alignment, and initial surfaces were handled for each competing method would help ensure confidence in the reported gaps.\n- All datasets are indoor and relatively structured - no explicit experiments test failure modes (irregular geometry, non-planar scenes, outdoor clutter). Standard view splits of 3, 6, and 9 views are followed in relevant literature (ReconFusion, CAT3D), but are not followed in this paper. \n- A strong baseline in ViewCrafter (TPAMI'25) is missing."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2u8xbYEVUm", "forum": "kdPmsMVhZf", "replyto": "kdPmsMVhZf", "signatures": ["ICLR.cc/2026/Conference/Submission5709/Reviewer_56rG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5709/Reviewer_56rG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041159556, "cdate": 1762041159556, "tmdate": 1762918208794, "mdate": 1762918208794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their time, effort, and constructive feedback. We greatly appreciate the positive recognition of our work: our paper is acknowledged as having \"clear motivation, well-explained\" (56rG, 3Jys), \"identifies a real limitation\" (HhvE), and is \"reasonable and intuitive\" (jjWR). All reviewers also highlighted that our results \"outperform state-of-the-art baselines in both geometric accuracy and appearance quality\" (56rG, HhvE, jjWR, 3Jys). We have carefully followed the comments and suggestions from all reviewers and revised our manuscript (*with changes highlighted in blue*) accordingly.\n\nWe would also like to clarify that our method performs strongly not only in planar or structured environments, but also in non-planar and less structured scenes, as it constitutes a strict enhancement of the underlying base model (e.g., 2DGS or MAtCha). To further demonstrate this, we add additional experiments on all 9 scenes from the Mip-NeRF 360 dataset using 9 input views, following the protocol of ReconFusion [1]. We report per-scene results in Appendix Table A2 and provide comparisons against baselines on all 5 outdoor scenes in Appendix Figure A5.\n\nWe hope these clarifications further strengthen your confidence in assessing our contributions. If any questions remain, we would be more than happy to address them during the discussion stage. We will continue to refine the manuscript based on all feedback, and we remain committed to delivering a high-quality paper that benefits the research community.\n\n[1] ReconFusion: 3D Reconstruction with Diffusion Priors. CVPR 2024"}}, "id": "rffwn7BOc3", "forum": "kdPmsMVhZf", "replyto": "kdPmsMVhZf", "signatures": ["ICLR.cc/2026/Conference/Submission5709/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5709/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission5709/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763719795556, "cdate": 1763719795556, "tmdate": 1763719795556, "mdate": 1763719795556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}