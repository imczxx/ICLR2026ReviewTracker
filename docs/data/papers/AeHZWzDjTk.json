{"id": "AeHZWzDjTk", "number": 13483, "cdate": 1758218469537, "mdate": 1759897433905, "content": {"title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture", "abstract": "Large language models (LLMs) predominantly use autoregressive (AR) approaches, but masked diffusion models (MDMs) are emerging as viable alternatives. A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair, as it's hard to distinguish whether observed differences stem from the paradigm itself or the architectural shift. This research evaluates MDMs within a decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms. Our investigation suggests that the standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the language's inherent left-to-right structure. (2) Investigate architectural influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that while encoder-only MDMs model a simpler conditional probability space, decoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. This work thus decouples core paradigm differences from architectural influences, offering insights for future model design.", "tldr": "", "keywords": ["Discrete Diffusion Models", "Any-Order Autoregressive Models", "Decoder-only Architecture"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/124b3a30a4dc0674183f7b03b1a22aff86bf720c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to fairly compare autoregressive (AR) and masked diffusion (MDM) paradigms by decoupling formulation from architecture. Prior works conflated these by pairing AR with decoder-only causal attention and MDM with encoder-only full attention.\n\nThe authors introduce AO-GPT, a decoder-only masked diffusion model that implements the Any-Order Autoregressive (AO-AR) objective, equivalent to MDM but averaged over all token permutations. Using identical architectures, they reveal:\n\n* AO-GPT trains slower than left-to-right GPT because many random token orders are uninformative.\n\n* Injecting 10% left-to-right data sharply improves both convergence and perplexity, showing language’s inherent sequential bias.\n\nCompared with encoder-only MDMs, decoder-based AO-GPT models a vastly larger conditional space (~ e · n!) but achieves ≈ 25× faster generation with KV-cache and efficient sampling, approaching MDM perplexity after temperature annealing. Architecturally, AO-GPT employs adaptive LayerNorm and EMA to stabilize training, achieving near-encoder-level performance while preserving decoder efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The work’s exploratory framing, decoupling modeling formulation (AR vs. MDM) from architectural choice (encoder vs. decoder), is conceptually fresh and helps clarify long-standing confusions in diffusion-based language modeling. This perspective provides some insights into how generation order and attention structure affect learning and efficiency.\n\n2. Although limited in scale, the experiments are carefully controlled and serve as useful ablations for understanding the impact of causal vs. full attention and decoder vs. encoder designs. The results are interpretable with clear reasoning, making them valuable for guiding future model design. The finding that partial left-to-right ordering stabilizes any-order training offers a practical and actionable insight for improving masked diffusion LMs without heavy architectural change, echoing ideas from hybrid AR–diffusion works like block diffusion."}, "weaknesses": {"value": "1. Lack of clear narrative and focus. The paper presents over eight separate findings spanning AR vs. AO-AR, encoder vs. decoder MDM, training dynamics, architectural ablations, and efficiency. This breadth makes the work feel more like a collection of exploratory observations than a cohesive study with a central takeaway. The main conceptual message, how “any-order” decoding interacts with causal attention, gets diluted by numerous side analyses and minor findings. The authors should consolidate around one or two key insights to strengthen the paper’s focus.\n\n2. Questionable fairness premise. The notion of “fair comparison” by forcing MDM into a decoder-only causal-attention setup is debatable. Changing an encoder-based model to match the AR architecture inherently biases the comparison toward AR-style inductive biases. A symmetric control, e.g., adapting AR to an encoder/full-attention variant, would make the fairness claim more credible. As written, the work could be seen as optimizing one paradigm (MDM) to fit another’s design, rather than evaluating them on equal footing.\n\n3. Limited novelty in methodology. The technical modifications (decoder reimplementation, order ensembling, adaptive LayerNorm, EMA) are individually incremental and borrowed from prior work (e.g., σ-GPT, diffusion model training). The novelty lies mainly in the analysis framework, but this could be emphasized more explicitly rather than presented as model contributions.\n\n4. Small-scale experiments and weak generalization claims. All experiments are conducted at ≤350M parameters. Given the known scale sensitivity of LLM training, these results may not extrapolate to the multi-billion-parameter regime. The claimed speed–quality trade-offs, convergence behaviors, and “10% L2R” benefit might differ under large-scale or instruction-tuned settings. The paper would benefit from at least partial large-scale validation or sensitivity analysis to hyperparameters.\n\n5. Underdeveloped connection to prior hybrid approaches. The observation that \"partial autoregressive ordering\" stabilizes MDM training aligns with prior “block diffusion” or “hybrid AR–diffusion” work, but this link is not acknowledged."}, "questions": {"value": "The fairness claim is very questionable, converting MDMs into a decoder-only setup inherently imposes AR-style inductive biases, so it’s unclear why this one-directional adaptation is considered “fair,” or how conclusions might change if AR models were instead adapted to an encoder/full-attention configuration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBMoBzi7yL", "forum": "AeHZWzDjTk", "replyto": "AeHZWzDjTk", "signatures": ["ICLR.cc/2026/Conference/Submission13483/Reviewer_rwo7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13483/Reviewer_rwo7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963852965, "cdate": 1761963852965, "tmdate": 1762924101204, "mdate": 1762924101204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is the first systematic and equitable comparison of masked-diffusion language models (MDMs) and standard autoregressive (AR) LMs by decoupling training paradigm from architecture. The authors show that when both are instantiated with the same decoder-only backbone, the core distinction reduces to the distribution over token orders. They therefore implement MDM as Any-Order AR within a decoder-only GPT (“AO-GPT”) and study: (1) how AR vs. AO-AR differ in modeling capacity and empirical behavior under an identical architecture; and (2) for MDMs, how encoder-only and decoder-only architectures compare theoretically and empirically. Experiments find AO-GPT converges notably more slowly early in training than left-to-right (L2R) GPT; a fixed block-wise random order interpolates between L2R and fully random in convergence; and mixing a small fraction (~10%) of L2R samples improves AO performance. Decoder-only AO-AR underperforms encoder-only variants on perplexity unless one ensembles across order contexts, which substantially narrows the gap. At the same time, decoder-only models enable linear-time generation with KV caching and deliver large speedups (up to 25×), whereas encoder-only MDMs occupy a simpler conditional space but require multi-step refinement. With careful temperature/annealing and order handling, decoder-only MDMs reach competitive perplexity, highlighting clear trade-offs between modeling space and efficiency. Overall, the work decouples paradigm from architecture to provide a fairer basis for evaluating AR and MDM and to guide future MDMs design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1). This work clearly and rigorously shows that the MDM loss function is mathematically equivalent to the AO-AR loss. This is important for putting AR and MDM on a common theoretical footing, isolating the true source of differences to the token-order distribution rather than architecture, and enabling apples-to-apples empirical comparisons that inform practical choices like order mixing, annealing, and ensembling. Moreover, the equivalence between the efficient sampling algorithm and Eq. (8) also makes the generation speedup for AO-GPT convincing and well-supported. \n\n(2). The experiments are thorough and well motivated. For example, to carefully answer the first research question, this work keeps the decoder-only backbone, datasets, and hyperparameters fixed, vary only the token-order distribution, and add targeted ablations (e.g., ~10% L2R mixing). It also reports both quality metrics like perplexity and convergence curves and systems metrics like generation time across steps. This tight isolation of variables enables causal conclusions and yields clear and actionable takeaways, underscoring the work’s practical significance.\n\n(3). This manuscript is well written. The concepts are introduced progressively, notation is consistent, experiments are well-motivated, making the findings and methodology easy to follow and build upon."}, "weaknesses": {"value": "(1). The major concern is the scalability of the findings elaborated in this work. This work only tests small model with 350M parameters. It is unclear whether the findings like the observed convergence behavior and order-mixing benefits hold true for larger models. Even though the manuscript acknowledges this, but it does not provide confirming evidence, which is indeed a non-negligible weakness.\n\n(2). This work only tests on perplexity, with limited coverage of downstream conditional tasks like QA, summarization, and long-context retrieval/reasoning. Since token-order policies and annealing and ensembling may behave differently under conditioning and at much longer sequence lengths, the validity of the findings is quite constrained.\n\n(3). Even though Finding 8.2 is convincing given the experiment results in Table 2, selecting these settings is non-trivial, appears dataset- and model-dependent, and this work does not standardize or bound the tuning budget. This raises real concerns about the feasibility of this finding. In practice, extensive per-task or per-model sweeps could erode the claimed benefits."}, "questions": {"value": "(1). It is interesting to know Finding2.2 that fixed block-wise random serves as an interpolation between Left-to-Right and purely random order in terms of convergence speed. How sensitive are these gains to the block size? Do you observe regimes where larger or smaller blocks hurt convergence or final perplexity?\n\n(2). Following Q(1), recent MDM works like Fast-dLLM-v2 [1] is also using this block-like design, achieving excellent performance. Do you think your findings on block-wise random can explain its superior performance?\n\n(3). For Finding 3.2, why do you only test 10% L2R data? What will the performance be like for other proportions?\n\n(4). Could the authors give some thoughts on a principled and low-overhead strategy to set and adapt annealing? \n\n[1]. Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo, P., Han, S., & Xie, E. (2025). Fast-dLLM v2: Efficient Block-Diffusion LLM. arXiv preprint arXiv:2509.26328."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtDbrum7VE", "forum": "AeHZWzDjTk", "replyto": "AeHZWzDjTk", "signatures": ["ICLR.cc/2026/Conference/Submission13483/Reviewer_fm5B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13483/Reviewer_fm5B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975077182, "cdate": 1761975077182, "tmdate": 1762924100575, "mdate": 1762924100575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks a good question: when people say “diffusion-style LMs are slower / worse than AR GPTs,” are we blaming the generative formulation (any-order / masked diffusion) or the architecture (encoder-only vs decoder-only)? To isolate this, the authors implement an any-order / masked-diffusion objective inside a decoder-only GPT—they call it Any-Order GPT (AO-GPT)—and compare it directly with standard left-to-right AR GPT under the same backbone. They show three main things: (i) true any-order training converges noticeably slower than L2R AR on the same model, confirming an optimization gap; (ii) adding a small fraction of L2R examples, plus stronger target-position injection (adaLN) and EMA, largely fixes this gap and brings AO-GPT close to encoder-only diffusion baselines such as MDLM/SEDD; and (iii) because AO-GPT is still decoder-only, it can decode in roughly linear time with KV-cache and even run ~25× faster than encoder-only masked diffusion samplers at equal length. The paper also analyzes why decoder-only any-order is harder—because it must model order-sensitive conditionals whose count grows like (e\\cdot n!), whereas encoder-only MDMs model order-invariant conditionals of size ($n2^{n-1}$)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-posed problem statement. The paper identifies a real confounder in current comparisons: AR↔MDM and decoder-only↔encoder-only are almost always changed together, so we don’t know which factor is responsible for the gap. Making MDM/AO run on a GPT-style decoder is a clean way to decouple these effects. This is genuinely useful for the community.\n2. Concrete, nontrivial engineering recipe. The combination “any-order objective + per-layer target-position injection (adaLN) + very slow EMA + 10% L2R mix” is not a cosmetic tweak; it is exactly what makes AO-GPT trainable at GPT-2 scale, and it empirically outperforms the more naïve σ-GPT-style injection on the same backbone. That’s a real contribution over earlier “just add an output position” works like σ-GPTs.\n3. Runtime story is compelling. Showing that a decoder-only instantiation of a diffusion/any-order objective can be linear-time and ~25× faster than encoder-only MDLM/SEDD on long sequences hits one of the most common complaints about masked diffusion LMs (“nice idea, too slow”). This moves diffusion-style LMs closer to being a practical alternative."}, "weaknesses": {"value": "1. “Fair comparison” is only partially fair. The central claim is “we decouple formulation and architecture,” but the decoder-only any-order model gets a custom training recipe (adaLN, EMA=0.9999, 10% L2R mixing) that is not re-applied and re-tuned for the encoder-only diffusion baselines it is compared against. Yet we know from MDLM and EDLM that diffusion LMs are very sensitive to the exact denoising/weighting schedule and to Rao-Blackwellization tricks. If you let the baselines also adopt “a bit of L2R bias” or decoder-style conditioning, some of the reported gap reductions may disappear. Right now the story can still be read as “you improved your variant of any-order with several extra knobs” rather than “any-order itself is fine once you remove the architecture confounder.”\n2. The order-ensemble fix weakens the main narrative. A key observation in the paper is that decoder-only any-order must model vastly more order-sensitive conditionals ($(\\approx e\\cdot n!)$) than encoder-only ($(n2^{n-1})$), so its perplexity looks worse. The proposed remedy is to average over several context permutations at inference to wash out order bias. But once you need extra test-time ensembles to reach encoder-only quality, the big advantage “we’re linear-time and GPT-like” is qualified: you either pay extra forward passes, or you accept worse PPL. And the paper doesn’t show that a small ensemble (say, 2–4 orders) is always enough across datasets and lengths; the slides and OpenReview discussion indicate benefits keep increasing with more orders. That’s an unresolved algorithmic debt.\n3. Optimization diagnosis is shallow. The paper documents the symptom—pure any-order converges slower than L2R on the same GPT-2-style model—but the explanation stops at “language has a natural L2R bias, so mixing 10% L2R helps.” This is plausible but incomplete. For example, the work doesn’t separate: (i) gradient-noise increase from sampling uniformly over huge permutation spaces; (ii) mismatch between causal masking and “predict an earlier position”; and (iii) the fact that GPT’s rotary/absolute positions are themselves L2R-biased. Without ablations that fix the positional scheme or that use permutation-invariant encodings, we can’t tell which of these is the true bottleneck. Right now the fix is “add L2R,” which is very much a band-aid.\n4. Evidence does not scale. All empirical results are at roughly GPT-2-small / medium–like scale (the arXiv and OpenReview versions both state this) and on standard LM corpora (WikiText, OWT, LAMBADA). It is precisely at larger scales that (a) AR models start to benefit most from KV-cache amortization, and (b) discrete diffusion models in other papers start to fall behind when the number of sampling steps is restricted. Without at least a 1–7B run or a partial scaling curve, the claim “decoder-only MDM is a viable alternative to AR GPT” is not yet substantiated. This is especially important because contemporaneous MDLM work does report strong results at larger scales with optimized objectives."}, "questions": {"value": "1. How essential is the 10% L2R mix, really? If you remove only the L2R mixing but keep adaLN and EMA, does AO-GPT still close the gap to encoder-only diffusion on WikiText103/1BW, or does it fall back to σ-GPT-like behavior? In other words, is the paper actually demonstrating “any-order works on decoder-only” or is it demonstrating “a mostly any-order objective regularized by a small AR prior works”? This matters for the main thesis.\n2. What is the inference-time story when you need order ensemble? The paper highlights a ~25× decoding speedup over encoder-only MDMs per order thanks to KV-cache, but to match their PPL they average over multiple permutations. For long sequences (1–2k tokens), how many permutations are actually needed before the curve saturates, and does the total wall-clock stay better than strong MDLM samplers like those in Sahoo et al. 2024? A plot of “#permutations vs PPL vs time” would make the runtime claim rock-solid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sxlGlDarPZ", "forum": "AeHZWzDjTk", "replyto": "AeHZWzDjTk", "signatures": ["ICLR.cc/2026/Conference/Submission13483/Reviewer_cfK5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13483/Reviewer_cfK5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046831313, "cdate": 1762046831313, "tmdate": 1762924099921, "mdate": 1762924099921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper decouples formulation (autoregressive vs. masked diffusion / any‑order AR) from architecture (decoder‑only vs. encoder‑only) by building AO‑GPT, a decoder‑only masked‑diffusion/any‑order model. This lets the authors compare paradigms fairly within the same causal Transformer backbone. They (i) show the training objectives for masked diffusion (LMDM) and any‑order AR (LAO‑AR) are equivalent, (ii) analyze architectural trade‑offs—encoder‑only MDM vs. decoder‑only AO‑AR—in density estimation and generation complexity, and (iii) introduce practical ingredients (explicit target‑position injection via adaptive LayerNorm; EMA; a parallel multi‑mask attention mask) that enable fast, order‑agnostic decoding with competitive perplexity and reported large speedups. Experiments at GPT‑2‑scale compare orders, mixtures with a small fraction of left‑to‑right (L2R) updates, and encoder‑only vs. decoder‑only implementations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "++ By keeping the backbone decoder‑only and varying only the order/formulation, the paper cleanly separates effects of AO‑AR/MDM vs. AR. Section 2.2 crisply contrasts training signal density, density‑estimation, and generation complexity (O(n) for decoder‑only with KV cache vs. ~O(T·n) for encoder‑only MDM), avoiding the usual apples‑to‑oranges comparisons.\n\n++ The work unifies masked‑diffusion learning and any‑order AR by showing LMDM ≡ LAO‑AR, anchoring later design choices and analyses. This is a useful reference for the community.\n\n++ Mixing ~10% L2R with any‑order improves convergence/perplexity, and the authors report ~25× generation speedups for decoder‑only MDMs with temperature annealing while keeping perplexity competitive, clarifying the trade‑offs vs. encoder‑only MDMs."}, "weaknesses": {"value": "-- Results are primarily at GPT‑2 small/medium scale; claims about competitiveness would be more convincing at ≥1B parameters and on stronger reasoning/benchmarks beyond perplexity.\n\n-- The ~25× speedup is promising but depends on decoder‑only specifics; head‑to‑head latency/throughput vs. tuned AR (Flash‑/paged‑KV, speculative decoding) and vs. well‑optimized encoder‑only MDMs (varying T) under identical hardware and sequence lengths would strengthen the claim.\n\n-- It’s unclear whether AO‑GPT and AR baselines matched total tokens/updates; the parameter and runtime overhead of target‑position encoders (AdaLN 128‑d) and EMA are not quantified."}, "questions": {"value": "1. Were training tokens, optimizer schedules, and wall‑clock compute matched across AR and AO‑GPT? Please include FLOPs and training time comparisons.\n\n2. How is the ~25× speedup measured (batch size, length, KV cache policy, decoding temperature/annealing)? Can you add wall‑clock charts vs. a strong AR baseline with KV caching and against an encoder‑only MDM across T∈{8,16,32}?\n\n3. Beyond 10% L2R, did you try curriculum or entropy‑based order schedules? How sensitive are results to the order distribution?\n\n4. What is the parameter/latency overhead of AdaLN target‑positional encodings per layer? Any impact on memory footprint vs. a vanilla GPT‑2 of the same size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cPFZnwg00b", "forum": "AeHZWzDjTk", "replyto": "AeHZWzDjTk", "signatures": ["ICLR.cc/2026/Conference/Submission13483/Reviewer_9dpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13483/Reviewer_9dpA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225706394, "cdate": 1762225706394, "tmdate": 1762924099245, "mdate": 1762924099245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}