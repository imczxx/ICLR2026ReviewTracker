{"id": "YvFsyRReeN", "number": 8077, "cdate": 1758058650085, "mdate": 1763688518590, "content": {"title": "ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation", "abstract": "Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed\nbehavior policy dataset without additional environment interaction. One common challenge that arises in this setting is the out-of-distribution (OOD) error,\nwhich occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but\nthis constrains policy improvement and may not completely prevent OOD actions.\nAnother challenge is that the optimal policy distribution can be multimodal and\ndifficult to represent. Recent works apply diffusion or flow policies to address this\nproblem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that\nenforces the less restrictive support constraint by construction. ReFORM learns a\nBC flow policy with a bounded source distribution to capture the support of the\naction distribution, then optimizes a reflected flow that generates bounded noise\nfor the BC flow while keeping the support, to maximize the performance. Across\n40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.", "tldr": "Considering offline RL, we propose ReFORM, a two-stage flow policy that realizes the support constraint by construction and avoids the OOD issue without constraining the policy improvement.", "keywords": ["Offline reinforcement learning", "support constraint", "flow model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd457f1f99ca86e6f14dea1cc584bd3cceb81710.pdf", "supplementary_material": "/attachment/9f2f2d923e451ce0bd85b7c0c21d708d30a554ea.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ReFORM, an offline RL method that builds a flow-policy constrained to the support of the behavior policy by construction. It first learns a behavior-cloned flow policy whose source distribution is a bounded hypersphere, then manipulates the latent noise via a reflected flow so that the pushforward actions remain on-support while still allowing expressive, multimodal policies. The authors prove that the reflected flow preserves support and thus avoids OOD actions without statistical-distance regularization, and they report strong results on 40 OGBench tasks using a single hyperparameter setting, with performance profiles outperforming flow-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method guarantees that the manipulated latent distribution and the resulting action distribution stay within the BC support (Theorems 1–2), directly addressing OOD actions without tuning a divergence weight.\n\nAdapting reflected ODEs to latent noise yields an expressive, multimodal policy class while rigorously maintaining the bounded domain.\n\nBy dropping statistical-distance regularization, ReFORM uses constant hyperparameters across tasks, yet performs competitively or better on performance profiles.\n\nResults across 40 tasks (clean/noisy datasets) show consistent gains against FQL/IFQL/DSRL, with informative ablations (source distribution, reflection design, distillation)."}, "weaknesses": {"value": "The work does not report results on widely used D4RL benchmarks (e.g., Locomotion/AntMaze v2), which would strengthen comparability with prior offline RL papers beyond OGBench. (Authors evaluate OGBench tasks instead.)\n\nDependence on BC support quality. Because actions are restricted to the BC support, any mis-estimated or overly conservative BC support could cap achievable performance; the paper could analyze this failure mode more explicitly (e.g., diagnostics when the BC model under-covers optimal modes)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EikSiBU45r", "forum": "YvFsyRReeN", "replyto": "YvFsyRReeN", "signatures": ["ICLR.cc/2026/Conference/Submission8077/Reviewer_NkRQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8077/Reviewer_NkRQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902589828, "cdate": 1761902589828, "tmdate": 1762920067885, "mdate": 1762920067885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Reply to All (1/2)"}, "comment": {"value": "$\\def\\a{\\color{#648FFF}{\\textsf{6RET}}} \\def\\b{\\color{#E69F00}{\\textsf{gwFF}}} \\def\\c{\\color{#DC267F}{\\textsf{Q1Am}}} \\def\\d{\\color{#0FA83C}{\\textsf{NkRQ}}}$\n\nWe thank the reviewers for their valuable comments.\nWe are excited that the reviewers have identified the novelty and efficacy of our idea ($\\a$, $\\c$, $\\d$),\nacknowledged the rigorous derivations and clear theorems ($\\c$, $\\d$), appreciated our \"hyperparameter-free\" advantage ($\\b$, $\\c$, $\\d$),\nstrong empirical results and thorough ablations ($\\b$, $\\d$), and\ngood presentation (**all** reviewers). \nWe believe that ReFORM takes a significant step towards **completely avoiding the out-of-distribution (OOD) issue of offline RL without additional regularization weights while preserving the expressive multimodal policy**. \n\n---\n\nThe primary concerns include testing on more benchmarks and higher-dimensional spaces ($\\b$, $\\c$, $\\d$), analyzing the computational cost ($\\a$, $\\c$), and comparing with more baselines ($\\b$, $\\c$). \n\nIn our revision, we provide additional experimental results and discussions to clarify **all** raised concerns. Notable changes are  marked in $\\color{#D80000}{\\text{red}}$ in the revision. \nWe provide a brief summary below.\n\n## 1. New experiments\n\nIn addition to the $40$ OGBench tasks presented already in our paper, we evaluate our method on the D4RL benchmark and OGBench visual manipulation tasks. The results are provided below. Even with image-based inputs and completely different benchmarks, ReFORM, with **constant hyperparameters**, still performs similarly with or better than all baselines even with tuned hyperparameters. \n\n**D4RL antmaze (normalized return):**\n\n| Environment               | IFQL              | FQL (L)   | FQL (M)            | FQL (S)    | ReFORM             |\n|---------------------------|-------------------|-----------|--------------------|------------|--------------------|\n| antmaze-umaze-v2          | $91\\pm7$          | $85\\pm4$  | $\\mathbf{99}\\pm1$  | $88\\pm13$  | $\\mathbf{97}\\pm0$  |\n| antmaze-umaze-diverse-v2  | $55\\pm28$         | $57\\pm10$ | $\\mathbf{88}\\pm5$  | $61\\pm26$  | $\\mathbf{83}\\pm3$  |\n| antmaze-medium-play-v2    | $3\\pm4$           | $14\\pm6$  | $\\mathbf{92}\\pm1$  | $52\\pm15$  | $85\\pm4$           |\n| antmaze-medium-diverse-v2 | $24\\pm34$         | $9\\pm4$   | $\\mathbf{81}\\pm13$ | $24\\pm30$  | $\\mathbf{80}\\pm4$  |\n| antmaze-large-play-v2     | $17\\pm21$         | $43\\pm10$ | $61\\pm21$          | $3\\pm4$    | $\\mathbf{71}\\pm4$  |\n| antmaze-large-diverse-v2  | $28\\pm27$         | $55\\pm4$  | $\\mathbf{85}\\pm8$  | $8\\pm12$   | $69\\pm9$           |\n\n**D4RL adroit (normalized return):**\n\n| Environment               | IFQL              | FQL (L)   | FQL (M)            | FQL (S)    | ReFORM             |\n|---------------------------|-------------------|-----------|--------------------|------------|--------------------|\n| pen-human-v1              | $\\mathbf{65}\\pm1$ | $48\\pm0$  | $59\\pm4$           | $31\\pm4$   | $\\mathbf{64}\\pm7$  |\n| pen-cloned-v1             | $\\mathbf{81}\\pm8$ | $61\\pm7$  | $66\\pm5$           | $57\\pm6$   | $70\\pm6$           |\n| pen-expert-v1             | $120\\pm3$         | $105\\pm7$ | $\\mathbf{128}\\pm1$ | $107\\pm10$ | $\\mathbf{129}\\pm7$ |\n| door-human-v1             | $3\\pm1$           | $2\\pm1$   | $0\\pm0$            | $0\\pm0$    | $\\mathbf{4}\\pm1$   |\n| door-cloned-v1            | $-0\\pm0$          | $0\\pm0$   | $\\mathbf{3}\\pm2$   | $0\\pm0$    | $1\\pm1$            |\n| door-expert-v1            | $89\\pm5$          | $\\mathbf{104}\\pm1$ | $\\mathbf{105}\\pm0$ | $\\mathbf{102}\\pm0$ | $\\mathbf{104}\\pm4$ |\n\n**OGBench visual manipulation results (return):**\n\n|Task|Dataset|IFQL|FQL(L)|FQL(M)|FQL(S)|ReFORM|\n|--- |---    |--- |---   |---   |---   |---   |\n|visual-cube-single-play-singletask-task1-v0|CLEAN| $-117\\pm7$ | $-150\\pm16$ | $\\mathbf{-110}\\pm9$ | $-138\\pm19$ | $\\mathbf{-108}\\pm12$ |\n|visual-cube-single-noisy-singletask-task1-v0|NOISY| $-95\\pm2$  | $-176\\pm10$ | $-103\\pm2$          | $-57\\pm3$   | $\\mathbf{-52}\\pm7$   |"}}, "id": "k2ZVoPRXkG", "forum": "YvFsyRReeN", "replyto": "YvFsyRReeN", "signatures": ["ICLR.cc/2026/Conference/Submission8077/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8077/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8077/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763688400597, "cdate": 1763688400597, "tmdate": 1763689005511, "mdate": 1763689005511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReFORM, a new method for on-support offline reinforcement learning.\nThe authors observe that existing approaches—such as CQL, IQL, and FQL—rely on explicit distance-based regularization (e.g., KL, Wasserstein, MMD) between the learned policy and the behavior policy to avoid out-of-distribution (OOD) actions.\nHowever, these constraints depend heavily on tuning hyperparameters and can lead to excessive conservatism.\n\nReFORM instead enforces the support constraint by construction.\nIt introduces a two-stage reflected flow policy:\n\nBehavior-Cloned Flow — learns a bounded latent-to-action mapping with uniform latent noise U(Bₗᵈ), guaranteeing that all generated actions lie within the dataset support.\n\nReflected Noise Flow — optimizes the latent noise inside the same bounded domain via reflection dynamics, maximizing Q values while preserving support inclusion.\n\nThe paper provides theoretical proofs that the reflected flow maintains support containment for both latent and action distributions (Theorems 1 & 2), eliminating OOD actions.\nEmpirically, on OGBench (40 tasks with clean and noisy datasets), ReFORM achieves the best average performance among FQL, IFQL, and DSRL—without hyperparameter tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\nReFORM introduces a conceptually fresh idea—noise reflection for on-support control—that differs from prior regularization-based approaches.\nThe theoretical analysis clarifies when KL or Wasserstein constraints fail to guarantee support inclusion, which is insightful.\n\nTechnical Quality:\nThe paper provides rigorous derivations and clear theorems connecting reflection dynamics to support preservation.\nThe algorithmic design (bounded latent + reflected flow + distillation) is internally consistent and implementable.\n\nClarity :\nWriting, figures, and notation are excellent.\nThe relationship between theory and algorithm is clearly explained, and the overall flow of the paper is easy to follow.\n\nSignificance:\nThe idea of enforcing support “by construction” could inspire future work in flow- or diffusion-based offline RL.\nThe “hyperparameter-free” advantage is appealing for practical use, though broader validation is needed."}, "weaknesses": {"value": "Limited Empirical Scope: Experiments are confined to OGBench; no evaluation on D4RL, Adroit, or visual RL tasks.\nThe claim of “state-of-the-art performance” is weakened by the absence of comparison with recent strong baselines such as A2PR (arXiv 2405.19909), XQL, and EDQL.\n\nLacking computational cost analysis—reflection dynamics and distillation likely add overhead.\n\nModerate Empirical Depth: Ablation results (λ, number of clusters, reflection strength) are only in the appendix; some should appear in the main text. \n\nIt is unclear how the reflected flow interacts with other policy constraints (e.g., entropy regularization or behavior cloning loss).\n\nWhile “support-by-construction” is elegant, it may restrict exploration in mixed-quality datasets."}, "questions": {"value": "How does ReFORM perform compared with A2PR (Adaptive Advantage-Guided Policy Regularization) or XQL on D4RL benchmarks?\nIncluding these baselines would help assess competitiveness against the latest SOTA.\n\nWhat is the computational overhead of the reflection ODE and distillation step relative to FQL or IQL?\n\nCould the reflection principle be combined with diffusion-based policies or used as a projection operator for general offline RL methods?\n\nHow sensitive is ReFORM to the latent domain size? Would a learned bound improve performance?\n\nCan the authors release the full code to confirm reproducibility of OGBench results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gy0lT2lBCW", "forum": "YvFsyRReeN", "replyto": "YvFsyRReeN", "signatures": ["ICLR.cc/2026/Conference/Submission8077/Reviewer_Q1Am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8077/Reviewer_Q1Am"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924430135, "cdate": 1761924430135, "tmdate": 1762920067511, "mdate": 1762920067511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an approach to offline RL that constrains the learned policy within the support of the data generating policy while still leveraging expressive policy classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents strong empirical results and thorough ablations justifying the design choices. In particular, ReFORM achieves strong performance across a variety of environments and tasks while using the same set hyperparameters, which is uncommon for offline RL algorithms."}, "weaknesses": {"value": "The paper does not analyze potential reasons for why ReFORM outperforms the baselines in certain environments and datasets (clean vs noisy) but not others.\n\nThe paper does not investigate how this approach may scale to higher-dimensional state-action spaces. Does ReFORM’s approach of constraining to the data generating policy’s support work in a higher-dimensional space such as image-based inputs?\n\nThe paper does not compare to state-of-the-art algorithms on OGBench such as SORL [1] and floq [2]. (floq may constitute concurrent work and thus may not need to be compared against.)\n\nThe paper argues that ReFORM maintains high expressivity of the policy. However, the policy that is optimized via the Q-function is a one-step distillation policy, similar to FQL. One-step distillation policies have been shown to be less expressive than the multi-step policies employed by DSRL [3] and SORL [1]. \nThe authors justify this design choice via an ablation (Figure 4), though these results seem to contradict [1]’s findings. Are there possible explanations for this?\n\n- [1] Espinosa-Dice et al., “Scaling Offline RL via Efficient and Expressive Shortcut Models”\n- [2] Agrawalla et al., “floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL”\n- [3] Wagenmaker et al., “Steering Your Diffusion Policy with Latent Space Reinforcement Learning”"}, "questions": {"value": "Did the authors consider using a Gaussian-based policy for noise generation (similar to DSRL) instead of the flow-based model? See other questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7o7oRrV96", "forum": "YvFsyRReeN", "replyto": "YvFsyRReeN", "signatures": ["ICLR.cc/2026/Conference/Submission8077/Reviewer_gwFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8077/Reviewer_gwFF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960015574, "cdate": 1761960015574, "tmdate": 1762920067052, "mdate": 1762920067052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes the ReForm Framework, to cope with the OOD problem in offline RL as well as the multimodal issue of optimal policy distribution. The method first proposes the method to map the source distribution to the supported action distribution, and then utilizes the generated bounded noise via Reflected Flow to optimize the Q value. Experiments on more than 40 challenging tasks demonstrate the superoirty of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The OOD issue, as well as the distribution of optimal action policy, are classic topics in offline RL, it is appreciated that the authors consider these issues from the new perspectives. \n\n* The use of bounded source distribution and reflect flow is quite novel and appealing. It fundamentally avoids OOD actions being explored. \n\n* The reflected flow noise generator can produce complex multimodal noise, which is helpful for some scenarios where real actions distribution are quite complex."}, "weaknesses": {"value": "1. Some related references are missing, and it is suggested to consider the related work in the manuscript. \n\n* https://arxiv.org/abs/2202.06239\n\n* https://arxiv.org/abs/1705.08868\n\n* https://arxiv.org/abs/2301.12130\n\n2. The model design is appealing, however, the performance of the model also relies on the quality of behavior cloning model. How is the model performance if the BC model is not well estimated. Does the author consider about the robustness of the proposed method? \n\n3. Although the authors suggest distillation model, the training cost still seems high. Does the author consider about this? Or it is suggested to consider some ablation study on this issue.\n\n4. From the results in Figure 4, why ReForm(NoDistill) is slightly worse than ReForm, the reviewer is confused about this. For other variants, it seems their performances are much inferior to ReForm and ReForm(NoDistill). So is there a light and efficient version of ReForm?"}, "questions": {"value": "See the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "47MsZDRvJN", "forum": "YvFsyRReeN", "replyto": "YvFsyRReeN", "signatures": ["ICLR.cc/2026/Conference/Submission8077/Reviewer_6RET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8077/Reviewer_6RET"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961124832, "cdate": 1761961124832, "tmdate": 1762920066676, "mdate": 1762920066676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}