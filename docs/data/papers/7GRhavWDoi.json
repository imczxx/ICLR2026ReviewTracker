{"id": "7GRhavWDoi", "number": 1813, "cdate": 1756941986580, "mdate": 1759898184640, "content": {"title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization", "abstract": "Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWEBENCH, where current LLMs still struggle to resolve real-world issues. A promising approach to enhance performance is test-time scaling (TTS), but its gains are\nheavily dependent on the diversity of model outputs. While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS. Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents. To bridge this gap, we introduce ENTROPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting. ENTROPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses. We validate ENTROPO by fine-tuning a\ndiverse suite of models from different families and sizes (up to 106B parameters). To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches. On the SWEBENCH leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with ENTROPO ranks 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED on the open-weight leaderboard, surpassed only by models with over 10x more parameters(e.g., >350B). These results highlight the importance of preserving diversity for effective test-time scaling and establish ENTROPO as a robust method for building powerful, interactive coding agents.", "tldr": "", "keywords": ["Coding agent", "large language model", "software engineering", "test-time compute"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffb02af111c92d640ef68eebde37c6c4dbf9187c.pdf", "supplementary_material": "/attachment/d221e02eb66af9d09f64658c812e8dd6f71275e4.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces ENTROPO, an entropy-enhanced preference optimization framework for training multi-turn, tool-using coding agents. The key insight is that standard alignment methods like DPO/KTO reduce output diversity, which weakens the benefit of test-time scaling (TTS) in challenging SWE tasks. ENTROPO augments preference optimization with explicit entropy regularization to preserve policy diversity, and generalizes it from single-turn to multi-turn trajectories. The method is paired with a hybrid best-trajectory selector combining a learned verifier with model-free filters. Experiments across multiple model families demonstrate that ENTROPO consistently improves performance with or without TTS."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper extends entropy-preserving preference optimization from single-turn to multi-turn, adapting that for agentic coding tasks.\n* Empirical results are strong, showing clear improvement over baseline methods.\n* There are detailed ablation studies demonstrating the impact of each module in training and in the TTS pipeline."}, "weaknesses": {"value": "* Methodology-wise, this paper is largely an extension of SPL [1] to the multi-turn scenario. The objective in Proposition 3.2 is the same as Equation 5 in [1]. While [1] was cited in related works, I would suggest clearly stating the relationship with that paper at the beginning of Section 3, to highlight the unique contribution of this paper.\n* The extension of SPL to the multi-turn scenario is relatively straightforward. One can bypass the derivation in Appendix C by taking a similar approach to Step-DPO, which decomposes a multi-turn trajectory of $H$ turns into $H$ single-turn trajectories, and arrive at the same objective function. \n* The introduction of ENTROPO-KTO feels abrupt at the end of Section 3.2, as the preceding discussion was mainly centered around DPO. In addition, is there a reason to conduct all ablations with KTO rather than DPO?\n* Using the longest for SWE-verified and the shortest for SWE-Lite in the step-count heuristic in TTS is somewhat overfitting to the test sets.\n\n\nReferences\n\n[1] Slocum, Stewart, Asher Parker-Sartori, and Dylan Hadfield-Menell. \"Diverse preference learning for capabilities and alignment.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Lai, Xin, et al. \"Step-dpo: Step-wise preference optimization for long-chain reasoning of llms.\" arXiv preprint arXiv:2406.18629 (2024)."}, "questions": {"value": "* How are the trajectories scored in Sec 3.3? Did you use a binary score or a more fine-grained one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HLTbwqyU3o", "forum": "7GRhavWDoi", "replyto": "7GRhavWDoi", "signatures": ["ICLR.cc/2026/Conference/Submission1813/Reviewer_q16a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1813/Reviewer_q16a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433207811, "cdate": 1761433207811, "tmdate": 1762915897758, "mdate": 1762915897758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes ENTROPO, a entropy-enhanced multi-turn preference optimization framework for aligning large language models. The paper focuses on coding agents. The core motivation is that test-time scaling (TTS) is effective when the model outputs are sufficiently diverse, while standard alignment methods like DPO and KTO often cause diversity collapse. The work adds an explicit entropy regularization term to preserve policy diversity for multi-turn, tool-using agents. The authors derive closed-form optimal policies, provide theoretical analyses for multi-turn extensions, and combine the method with a hybrid trajectory selector.\n\nExperiments on SWEBench-VERIFIED and SWEBench-LITE show positive results - the paper claims top ranking among open-weight models performance close to larger (≥ 350 B) closed models. The code and models will be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The submission looks well motivated to the reviewer. The paper argues that diversity preservation is crucial for test-time scaling, which seems to be an overlooked issue in alignment (diversity collapse) for multi-turn tool-using agents.\n\n2. The method and application scenario look sensible to the reviewer. ENTROPO generalizes single-turn DPO/KTO to multi-turn trajectories, aligning learning with the sequential nature of SWE tasks.\n\n3. Empirical results look promising. On SWEBench, ENTROPO achieves state-of-the-art among open-weights. The performance scaling with number of rollouts (N) is analyzed and supports the claimed benefits of entropy regularization."}, "weaknesses": {"value": "1. Limited baselines. The main comparisons are against SFT, DPO, and KTO. There are no direct comparisons to other diversity-preserving methods or competitive methods (e.g., Diverse-PO, Beyond-KL divergence methods, or \"Building Math Agents with Multi-Turn Iterative Preference Learning\"). Achieving good results on benchmark is good, but there could be many factors just comparing the numbers. Controlled comparison with other methods will make the contribution (the alignment algorithm) more convincing.\n\n2. The link between entropy and TTS beyond multi-turn is unclear. e.g., is the proposed method applicable to single-turn TTS? If so, the arguments in the paper may need adjustments. The paper does not clarify whether ENTROPO offers benefits independent of TTS or whether it is tied to that specific evaluation paradigm.\n\n3. Caveats of SWEBench comparisons. Although the paper reports leaderboard results, it is unclear whether all baselines were trained on identical datasets and with comparable tool scaffolds. For example, differences in training data or teacher supervision could influence performance. The fairness of comparisons and the control of base models should be clarified. This is related to the first weakness."}, "questions": {"value": "Please refer the reviews above as well.\n\nHow exactly does ENTROPO differ algorithmically from M-DPO—beyond adding entropy regularization? Could M-DPO also integrate an entropy term similarly?\n\nAre all models (e.g., Qwen3-Coder-30B vs GLM-4.5-Air) trained on the same data and under the same compute budgets?\n\nDoes ENTROPO still help in single-turn tasks or without test-time scaling? If not, what limits its generality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zC0uBcsvpc", "forum": "7GRhavWDoi", "replyto": "7GRhavWDoi", "signatures": ["ICLR.cc/2026/Conference/Submission1813/Reviewer_PbDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1813/Reviewer_PbDN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754648772, "cdate": 1761754648772, "tmdate": 1762915897604, "mdate": 1762915897604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ENTROPO, a method that enhances preference learning by explicitly preserving policy diversity through entropy regularization. Specifically, the authors augment the preference optimization objective with a policy entropy term to encourage diverse behaviors in multi-turn agent loops. In addition, they introduce a test-time scaling strategy that combines a model-based verifier with multiple model-free criteria to select the best trajectory among candidates. Experiments on SWE-Bench Verified and SWE-Bench Lite show that ENTROPO combined with the proposed TTS consistently improves performance across several base models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and practical problem: maintaining behavioral diversity during preference optimization, which is often overlooked in reinforcement fine-tuning or DPO-style frameworks.\n\n\n- Incorporating policy entropy into preference learning is an interesting idea that encourages exploration and helps mitigate reward over-optimization."}, "weaknesses": {"value": "- The TTS component appears conceptually similar to prior work (e.g., Jain et al., 2025), with only minor extensions. The claimed novelty of this part is therefore limited.\n\n\n- The step-count heuristic that favors longer trajectories is not entirely convincing. Longer trajectories can also result from inefficient reasoning loops or redundant tool calls, which might not reflect genuine improvements.\n\n\n- From Table 1, most of the performance gains seem to come from the TTS component rather than ENTROPO itself, particularly for GLM-4.5-Air. A comparison between the original model + TTS and ENTROPO + TTS would help clarify the actual contribution of ENTROPO.\n\n\n- In Section 3.3, the paper mentions constructing a preference dataset and labeling preferred trajectories via scoring, but the details are unclear — for instance, which scoring model was used and what criteria determined the preference labels.\n\n\n- The reproduced performance of Qwen3-Coder-30B on SWE-Bench Verified (37.7) is substantially lower than the publicly reported 51.6, and the paper does not sufficiently explain the cause of this discrepancy."}, "questions": {"value": "During training, why is the maximum sequence length limited to 18k tokens, while most of the evaluated models natively support 32k or longer contexts? Is this due to computational constraints, data truncation, or stability issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XEIxXUKMQw", "forum": "7GRhavWDoi", "replyto": "7GRhavWDoi", "signatures": ["ICLR.cc/2026/Conference/Submission1813/Reviewer_4jkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1813/Reviewer_4jkj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980515282, "cdate": 1761980515282, "tmdate": 1762915897437, "mdate": 1762915897437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the development of coding agents based on LLMs. A preference-optimization method is designed for such multi-turn coding agents that interact with tools and codebases. The core idea is to explicitly add entropy regularization during preference optimization to preserve policy diversity, while leveraging the corresponding closed-form solution to develop a variant of DPO. The enhanced diversity prevents the model from collapsing to a narrow behavior pattern, improving its ability to explore multiple solution paths, and is further leveraged for test-time scaling, where many candidate trajectories are sampled and the best one is selected via a hybrid verifier-plus-heuristics strategy. Applied to SWE-bench, EntroPO-trained models significantly outperform baselines, demonstrating that diversity-preserving preference learning in multi-turn settings can unlock stronger coding agents without simply scaling model size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This work studies a practical application of developing coding agents, which is becoming more and more popular recently. Despite its practical nature, the algorithm proposed is based on theoretical groundings, i.e., the closed-form solution of the optimal policy in the multi-turn regularized MDP.\n\n- The presentation and writing are of high quality. Key ideas (e.g., adding entropy regularization and test-time scaling method) are illustrated clearly.\n\n- Comprehensive experiments and comparisons have been conducted to demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "My major concern is that this work is very closely related to M-DPO (Xiong et al., 2024). The major difference on algorithm side is adding an entropy regularization, which however, only leads relatively incremental changes in the algorithm. The proposed test-time scaling method adds some contribution to the paper, whose significance, however, is also insignificant to my perspective."}, "questions": {"value": "I would love to hear the author's thoughts on the relationship between this work and M-DPO (Xiong et al., 2024). As mentioned in the weakness section, this is my only and major concern.\n\nAnother minor question is that I would like to understand how the preference is determined for different trajectories during training. If I missed the details, a pointer would be really helpful. Also, it would be really appreciated if the authors can share some more ideas on whether preference learning (instead of absolute feedback as in Deepseek-R1) is necessary for coding agent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yx9NdI38Em", "forum": "7GRhavWDoi", "replyto": "7GRhavWDoi", "signatures": ["ICLR.cc/2026/Conference/Submission1813/Reviewer_Frdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1813/Reviewer_Frdt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106927340, "cdate": 1762106927340, "tmdate": 1762915897068, "mdate": 1762915897068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}