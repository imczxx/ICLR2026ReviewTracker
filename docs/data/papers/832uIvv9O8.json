{"id": "832uIvv9O8", "number": 21207, "cdate": 1758314963716, "mdate": 1759896934784, "content": {"title": "A Probabilistic Framework For Solving High-Frequency Helmholtz Equation Via Diffusion Models", "abstract": "Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging and spectral bias blurs oscillations. We argue for a probabilistic approach, and use a conditional diffusion operator as a concrete tool to investigate it. Our study couples theory with practice: a theoretical sensitivity analysis explains why high frequency amplifies prediction errors, and suggests an evaluation protocol (including an energy-form metric) to test whether learned surrogates preserve \\emph{stable} quantities while capturing uncertainty. Across a range of regimes, the probabilistic neural operator is found to produce robust, full-domain predictions, better preserves energy at high frequency, and provides calibrated uncertainty that reflects input sensitivity, whereas deterministic approaches tend to oversmooth. These results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.", "tldr": "We present a probabilistic framework based on conditional diffusion models that approximates high-frequency Helmholtz solutions with higher accuracy and calibrated uncertainty, outperforming deterministic neural operators.", "keywords": ["Probabilistic Learning; Helmholtz Equation; Diffusion Model; High Frequency Wave; Operator Learning; Uncertainty Quantification"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2c9cbacc1972f1b604b0b873eac6a9b18592d02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a probabilistic framework aimed at explaining generalization in deep neural networks. The authors likely ground their approach in Bayesian or PAC-Bayesian theory, positioning their model as a principled lens through which to understand why large, overparameterized models generalize well. The paper may offer theoretical insights or generalization bounds, and potentially includes experiments to validate the framework’s predictions. Given the framing, this work aims to contribute a unifying and theoretically grounded explanation for generalization phenomena in deep learning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* **Important problem**: Generalization remains a core, unresolved challenge in deep learning theory. Any framework that meaningfully advances our understanding is valuable.\n\n* **Probabilistic perspective is natural and principled**: Building on Bayesian reasoning or PAC-Bayesian bounds offers a well-motivated foundation for this line of work.\n\n* **Potential to unify disparate findings**: A strong probabilistic framework could shed light on known effects like double descent, memorization, or robustness to overfitting.\n\n* **Possibly sound theoretical grounding**: If the authors introduce new, formally derived generalization bounds or insights, that would add credibility and depth."}, "weaknesses": {"value": "* **Unclear novelty**: Prior work (e.g., PAC-Bayes, Bayesian evidence, support-based generalization) already frames generalization probabilistically. Without clear departures from those, the contribution risks being incremental.\n\n* **No evident theoretical advance**: It’s not clear if the paper introduces tighter bounds, new assumptions, or fresh interpretations that move the field forward.\n\n* **Empirical component is ambiguous**: There’s no confirmation that the framework is tested empirically — if it isn’t, the paper would lack grounding in real model behavior.\n\n* **Assumptions and scalability not detailed**: Probabilistic methods often hinge on strong assumptions (e.g., Gaussian priors, infinite width). If such assumptions are used here, they must be explicitly justified.\n\n* **Presentation may lack clarity**: If core ideas are not distinguished from prior work or not clearly defined, the reader may struggle to grasp the contribution."}, "questions": {"value": "1. **What exactly differentiates your framework from PAC-Bayesian theory?** Is this a reformulation, an extension, or something entirely new?\n\n2. **What assumptions underlie your analysis?** Are there strong priors, idealized settings, or architectural simplifications that limit applicability?\n\n3. **Do you derive generalization bounds?** If yes, are they non-vacuous for modern networks and how do they compare to existing results?\n\n4. **How do you account for support and inductive bias?** Can your framework explain phenomena like double descent or label memorization?\n\n5. **Is there any empirical validation?** If so, does your framework predict generalization behavior in practice (e.g. for ResNets, transformers, etc.)?\n\n6. **How scalable is the approach?** Does it remain tractable and effective on realistic deep networks and datasets?\n\n7. **Can your framework be used practically?** Does it inform model selection, training regimes, or uncertainty quantification in a way that’s testable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3fwu0Su33t", "forum": "832uIvv9O8", "replyto": "832uIvv9O8", "signatures": ["ICLR.cc/2026/Conference/Submission21207/Reviewer_HF5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21207/Reviewer_HF5E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761326239242, "cdate": 1761326239242, "tmdate": 1762941616061, "mdate": 1762941616061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a probabilistic neural-operator framework for solving high-frequency Helmholtz equations. The authors argue that deterministic neural operators (e.g., FNO, HNO, U-Net) suffer from spectral bias and fail to reproduce oscillatory, phase-sensitive wavefields in heterogeneous media. They perform a sensitivity analysis using WKB theory to show that small perturbations in the sound-speed field lead to linearly amplified errors with increasing frequency and propagation distance. To address this, they employ a conditional diffusion model that learns a distribution p(u∣z) of solutions conditioned on the input coefficients, allowing the model to sample plausible wavefields and capture uncertainty. Extensive experiments on synthetic 1D and 2D Helmholtz benchmarks show that the diffusion-based surrogate achieves much lower L₂, H₁, and energy-norm errors than deterministic baselines and provides calibrated uncertainty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The spectral-bias problem in high-frequency Helmholtz equations is real and significant for acoustics and seismics.  \n- FNO, HNO, and U-Net implementations are described in detail, and ablations on sampler choice and diffusion steps are thorough.\n- The authors integrate analytical reasoning (sensitivity via WKB) with numerical experiments."}, "weaknesses": {"value": "- Limited generality / narrow scope: The entire framework is tailored to the Helmholtz equation, with all design choices and analyses depending on its high-frequency behavior. The work does not demonstrate applicability to other PDE families or operator-learning tasks. It is too domain specific and not seemingly written for an interdisciplinary venue like ICLR.\n- Methodological novelty is minor: The diffusion-based probabilistic operator follows well-known conditional DDPM formulations used in many prior “Diffusion-for-PDE” papers (e.g., DiffusionPDE, Fundiff, PDE-Diffusion). The contribution is primarily in application, not in new architecture, loss, or theoretical innovation for diffusion models. \n- Theoretical contribution is domain-specific: The WKB sensitivity derivation clarifies why deterministic surrogates fail for Helmholtz but does not yield a general principle for probabilistic operator learning in other PDEs. \n- Evaluation breadth: Only 1D and 2D synthetic benchmarks are tested, with no experiments on real-world or multi-physics systems. This makes the impact and generalization questionable for an interdisciplinary ML audience. \n- Positioning relative to prior probabilistic PDE works: The paper cites recent diffusion-based PDE solvers but doesn’t clearly articulate what is fundamentally new beyond applying it to Helmholtz."}, "questions": {"value": "- The best results require 1000 diffusion steps. Have you explored fast samplers or distillation techniques (e.g., consistency training, DDIM, or latent diffusion) to reduce cost while maintaining fidelity?\n- What do you view as the conceptual takeaway for the broader ML-for-PDE community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ib9XJG3SzE", "forum": "832uIvv9O8", "replyto": "832uIvv9O8", "signatures": ["ICLR.cc/2026/Conference/Submission21207/Reviewer_JcnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21207/Reviewer_JcnV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519278864, "cdate": 1761519278864, "tmdate": 1762941615455, "mdate": 1762941615455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors train a diffusion model to approximate solution operator for Helmholtz equation in high-frequency regime.\n\nThe results indicate that the diffusion model performs substantially better than deterministic neural operators."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Solution to the Helmholtz equation remains challenging for classical iterative methods in high frequency regime. Especially in $D=3$ the construction of preconditioners and relaxation techniques is still an active area of research. In addition, Helmholtz equation has many practical application. Given that, a construction of efficient neural solvers for Helmholtz equation is a well-motivated problem.\n\nThe approach authors used is conceptually clear and simple: essentially it is a straightforward application of conditional diffusion model trained with standard objective.\n\nFrom the standpoint of accuracy, the results are undoubtedly good."}, "weaknesses": {"value": "1. Questionable practical significance\n2. The reasons probabilistic model performs better are unclear\n3. Lack of novelty\n\nI expand on the weaknesses in the section below."}, "questions": {"value": "**Questionable practical significance**\n\nIn my view, the main problem is the excessive computation cost of the approach. Authors noted (in Section 6) that accuracy of the diffusion model reported in Table 1 is a result of $10$ evaluations each taking $1000$ DDPM steps. Arguably, a surrogate model is only appealing if it is cheaper than a direct solution method. It is unlikely that a $10000$ (or even $1000$) evaluation of U-Net per sample is cheaper than a direct solution of the Helmholtz problem. Given this context I have several questions:\n1. Can the authors provide data on wall-clock time needed for the solution using all baselines and numerical methods used to generate the dataset?\n2. What will happen if one reduces costs by decreasing the number of steps? How significant is the drop in accuracy?\n3. Can the authors provide plots that show sampling time and mean accuracy for different numbers of diffusion steps?\n4. Did authors try more advanced integration schemes for diffusion models, e.g., exponential integrators https://arxiv.org/abs/2204.13902?\n5. Can the authors explain why diffusion models remain interesting and useful given the excessive number of computations needed to generate single solutions?\n\n**The reasons probabilistic model performs better are unclear**\n\nAuthors provide interesting considerations on why diffusion models perform better than deterministic baselines. The main argument seems to be, roughly, that by analogy with inverse problems, probabilistic models applied to input-sensitive maps, sharply approximate different branches of possible solutions rather than a blurred average of perturbed solutions. I do not see how data gathered by authors support this picture.\n\nFrom Table 1 one can conclude that the averaged prediction of diffusion model is simply more accurate. The reported std is tiny, so sensitivity is not pronounced enough for probabilistic averaging to be significant. Similarly small std is reported in Table 3. Can the authors provide data on the std per sample and comment on the overall importance of probabilistic modelling prediction in light of small std?\n\nIn Section 5.2 authors performed sensitivity analysis and concluded that the diffusion model is superior to other approaches. I find such framing problematic since authors select results for highest frequency. From Table 1 we see that deterministic models completely failed to learn (relative errors $76\\%$, $41\\%$, $80\\%$). It is obvious that models with high relative error will show poor sensitivity results. Can the authors show results on sensitivity of deterministic methods for frequency $1.5\\times 10^{10}\\text{ Hz}$? These results will likely be much better, and if this is the case, it is not clear why probabilistic models perform better.\n\n**Lack of novelty**\n\nDiffusion and other generative models was applied as solvers of forward and inverse problems in many papers https://arxiv.org/abs/2406.17763, https://arxiv.org/abs/2410.16415, https://arxiv.org/abs/2506.08604, etc. Authors of the present contribution applied a diffusion model to the Helmholtz equation with little modifications. Can the authors comment on the novelty of their contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UwpeWU4uwG", "forum": "832uIvv9O8", "replyto": "832uIvv9O8", "signatures": ["ICLR.cc/2026/Conference/Submission21207/Reviewer_rg5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21207/Reviewer_rg5H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914025151, "cdate": 1761914025151, "tmdate": 1762941614451, "mdate": 1762941614451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a probabilistic framework for solving the high-frequency Helmholtz equation by employing a conditional diffusion operator. The authors motivate their approach by highlighting two persistent limitations of deterministic neural operators in this problem: spectral bias and strong input-to-output sensitivity. The probabilistic model learns a conditional distribution over solutions, which is illustrated to mitigate the phase ambiguity issues. The method is evaluated on a 2D J-Wave benchmark, demonstrating good stability compared to strong deterministic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-justified method for a problem—high-frequency wave propagation in heterogeneous media—where standard deterministic neural operators are known to struggle.\n\n- Clear motivation: The sensitivity analysis in Section 3.2, using the 1D WKB argument, provides a clear and insightful explanation for why high-frequency wavefields are sensitive (sensitivity grows linearly with frequency k and propagation distance l) and how the MSE-optimal deterministic predictor suffers from oversmoothing. The reason to use a probabilistic formulation in this deterministic problem is well-established.\n\n- Strong results: The conditional diffusion operator achieves the lowest errors across a wide range of frequencies when compared to neural operators and its backbone U-Net. The performance is a crucial empirical validation of the approach."}, "weaknesses": {"value": "- High inference cost. The cost of sampling makes the proposed method significantly slower than baselines. It is unknown whether better baselines (e.g., (F-)FNO with a large kernel that spans high frequencies) will close the gap."}, "questions": {"value": "Can you provide a spectral analysis of different baseline to support the claim that they indeed have spectral biases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I5puuB5CSG", "forum": "832uIvv9O8", "replyto": "832uIvv9O8", "signatures": ["ICLR.cc/2026/Conference/Submission21207/Reviewer_Tbu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21207/Reviewer_Tbu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949945892, "cdate": 1761949945892, "tmdate": 1762941613758, "mdate": 1762941613758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}