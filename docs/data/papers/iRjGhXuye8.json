{"id": "iRjGhXuye8", "number": 14450, "cdate": 1758235690757, "mdate": 1763048311076, "content": {"title": "Learning Vortex Enhancement with Angular-Speed-Invariant Importance Sampling in SPH Fluids", "abstract": "Learning vortex enhancement in SPH benefits most from what is sampled. We use angular-speed-invariant importance sampling with the Kinematic Vorticity Number (KVN) to target vortex cores across resolutions and flow speeds. Particles selected by KVN are pooled into a lightweight global token via attention. Models are trained with velocity correction targets obtained by applying a Biot–Savart mapping to the vorticity loss field. Compared with uniform and vorticity-based sampling, KVN-based sampling improves vortex coherence and advances the emergence of secondary vortices across scenes and particle counts. The gains persist under coarse and fine discretizations and scale smoothly with particle count, indicating robustness to resolution changes. Ablations further show that injecting KVN-based information also benefits alternative encoder variants, suggesting that angular-speed-invariant sampling is a simple, transferable lever for learning vortex enhancement in SPH.", "tldr": "Explore the effect of angular-speed-invariant importance sampling in learning vortex enhancement for SPH.", "keywords": ["Graphics Fluid Simulation", "SPH", "Vortical Flow", "Kinematic Vorticity Number", "Learning Fluids"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0d81072fdaa88a2518c26ab92e6e8030b04eae5c.pdf", "supplementary_material": "/attachment/28f304d3e0528fabf4c5a97d45cc6d97348cb3b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a learning-based framework for vortex enhancement in Smoothed Particle Hydrodynamics (SPH) fluids using angular-speed-invariant importance sampling. A particle-based neural network is proposed to improve energy (vorticity) conservation in Smoothed Particle Hydrodynamics flow solvers. The authors propose a particle resampling strategy based on the Kinematic Vorticity Number (KVN), which measures the degree of rigid-body rotation independently of angular velocity. Particles with high KVN values are pooled via attention into a global token that encodes vortical context. The network is trained to predict velocity corrections derived from a Biot–Savart mapping of the vorticity loss field. The authors show that integrating KVN-based features benefits different encoder architectures; the main application of the proposed approach is to recover vorticty highlighting the generality of angular-speed-invariant sampling for learning vortex enhancement in SPH."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Employing an angular-speed-invariant importance sampling metric is a clear and well-motivated contribution, offering a physically inspired sampling strategy improvement over vorticity-magnitude-based sampling.\n\n- The combination of KVN-guided sampling with continuous convolutions and global attention is conceptually elegant and well-integrated within the learning framework."}, "weaknesses": {"value": "- The rationale behind energy dissipation in SPH seems is not thoroughly discussed. The authors blame kernel approximations and coarse discretizations, but there are more to it such as NS variables choice (vorticity vs velocity-based vs impulse-based), hybrid simulation discretizations (MPM, FLIP), accuracy of pressure projection solver, etc. Solving this issue with a neural approach without understanding the root cause seems to be a sub-par choice that could lead just to additional overheads without a clear practical advantage.\n\n- The formulation of eq, (1) is not clear. Does the vorticity loss term represents the difference between consecutive steps or a cumulative dissipation measure? \n\n- The authors propose a neural approach in which the potential benefit would be to increase the efficiency of SPH solvers. The issue however is that the paper does not provide detailed timing comparisons against simply increasing the particle count of a baseline solver, leaving the theoretical computational efficiency advantage unverified.\n\n- The evaluation focuses primarily on 2D configurations, such as lid-driven cavity and rotating panel setups, which limits the assessment of generality to more complex or 3D flow regimes.\n\n- While the results qualitatively demonstrate earlier vortex formation, quantitative metrics beyond vorticity magnitude and kinetic energy would strengthen the validation (e.g., divergence error, spectral energy distribution)."}, "questions": {"value": "- Did you try enhancing the solid particles features with the obstacles normal information?\n- Could the strain tensor on equation 6 be zero? If yes, a delta would have to be added to the equation to avoid instabilities. \n- The multi-head attention is similar to self-attention correct? The text would be clearer if this is explicitly stated. Also would be interesting to see how does the proposed approach scales with the number of particles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BtAYQdXXCy", "forum": "iRjGhXuye8", "replyto": "iRjGhXuye8", "signatures": ["ICLR.cc/2026/Conference/Submission14450/Reviewer_5X6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14450/Reviewer_5X6E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983184050, "cdate": 1761983184050, "tmdate": 1762924853905, "mdate": 1762924853905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Mmt0fEPFGo", "forum": "iRjGhXuye8", "replyto": "iRjGhXuye8", "signatures": ["ICLR.cc/2026/Conference/Submission14450/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14450/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763048310428, "cdate": 1763048310428, "tmdate": 1763048310428, "mdate": 1763048310428, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for enhancing vortex structures in Smoothed Particle Hydrodynamics (SPH) fluid simulations through a physically grounded sampling mechanism called angular-speed-invariant importance sampling. The key idea is to employ the Kinematic Vorticity Number (KVN) as an importance measure to select particles that contribute most significantly to rotational motion, thereby overcoming the bias of traditional vorticity-magnitude–based approaches toward high-speed vortices. The method combines Continuous Convolution (CConv) layers for local neighborhood encoding and a KVN-guided attention pooling mechanism for capturing global flow context. The learning objective is derived from the Biot–Savart law, enabling the network to predict velocity corrections that improve vortex coherence. Experiments on multiple canonical SPH test cases are conducted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of KVN represents a meaningful and well-motivated improvement to the data sampling process in SPH-based simulations."}, "weaknesses": {"value": "1. The focus of the paper is narrow and largely application-centric, limiting its relevance to a general machine learning audience. While the work is well-executed within SPH-based vortex modeling, it lacks a clear articulation of how the proposed ideas advance the broader field of ML or operator learning, which is crucial for ICLR.\n\n2. The writing and narrative structure of the paper can be significantly improved. The flow of ideas sometimes feels fragmented with abrupt transitions. The motivation of this work to the ICLR community can be much better articulated and substantiated by improving the quality of writing of this work.\n\n3. The machine learning contribution is not sufficiently emphasized. Most of the novelty lies in the sampling metric (KVN) rather than in the learning framework itself. The method does not introduce new learning paradigms, architectures, or optimization techniques that generalize beyond this domain.\n\n4. The discussion of theoretical properties of the KVN-based sampling approach is minimal. There is no quantitative analysis of variance reduction, sampling efficiency, or convergence characteristics that would substantiate the claimed advantages of angular-speed invariance.\n\n5. The quantitative improvements reported are relatively small and often confined to specific metrics. While qualitative visualizations are convincing, it is unclear whether the observed gains are statistically significant or impactful in practical SPH workflows.\n\n6. The paper does not compare against other relevant ML-based baselines relevant to fluid simulation, such as Graph Neural Networks, Physics-Informed Neural Networks (PINNs), or differentiable SPH solvers (e.g., Neural SPH). Without such comparisons, it is difficult to assess the true performance or generalization advantage of the proposed method."}, "questions": {"value": "1. How does the proposed KVN-based sampling approach generalize to three-dimensional SPH flows or higher-Reynolds-number turbulence, where vortex interactions are more complex?\n\n2. How does this method compare to state-of-the-art ML-based fluid simulation frameworks, including Graph Neural Networks for SPH or differentiable physics models such as Neural SPH and Neural Vortex Method?\n\n3. Can the authors provide more insight into how the proposed KVN-based sampling framework might generalize to other physical domains such as AI for plasma dynamics, meteorology, or materials modeling to establish broader relevance for the ICLR community?\n\n4. How sensitive are the results to the hyperparameters used in defining KVN or in the model training process, and how do these affect reproducibility or model stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9enPvo8ufu", "forum": "iRjGhXuye8", "replyto": "iRjGhXuye8", "signatures": ["ICLR.cc/2026/Conference/Submission14450/Reviewer_4cAh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14450/Reviewer_4cAh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762478109545, "cdate": 1762478109545, "tmdate": 1762924853503, "mdate": 1762924853503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to improve vortex detection and simulation. While previous baselines make use of the vorticity magnitude, this method proposes to use Kinematic Vorticity Number instead. After introducing the  KVN, the paper describes the chosen architecture and proposes some experiment to illustrate the improvements when using KVN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental results showcase an improvement wrt to baseline. \n- The proposed method introduces a new component that was not used in the littérature, to the best of my knowledge."}, "weaknesses": {"value": "-\tI found the paper a little hard to read. It would benefit of a more fluent wrirting (especially introduction), to clearly introduce each component before reading the paper. This is even more important in general conferences such as ICLR, where people from different horizons could read this work. I am not an expert in particle fluid dynamic, however, I know well deep learning and its application to physical systems. \n-\tThe scope of applicability seems limited to a specific type of physical systems. \n-\tWhile some experiments are convincing, I think the experimental part could be improved to be even more convincing on some aspects, see questions. \n-\tMissing references : a huge litterature on mesh-free method have emerged recently. I think a paragraph to position the current work wrt these methods should be included (could be in appendices). These methods use eg either INR [1-2], attention [3-6], graphs [7-9] and many others. Moreover, since continuous convolutions are a core component of the work, I think additional related work should appear.  Moreover, a comparison with some of these method would allow the reader to understand the benefit of the proposed method wrt to others. \n\n[1] Operator Learning with Neural Fields: Tackling PDEs on General Geometries, Louis Serrano, Lise Le Boudec, Armand Kassaï Koupaï, Thomas X Wang, 2023. \n\n[2] Implicit Neural Spatial Representations for Time-dependent PDEs, Honglin Chen, Rundi Wu, Eitan Grinspun, Changxi Zheng, Peter Yichen Chen, 2023\n\n[3] Transolver: A Fast Transformer Solver for PDEs on General Geometries, Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long, 2024. \n\n[4] Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators, Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter, 2024\n\n[5] AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields, Louis Serrano, Thomas X Wang, Etienne Le Naour, Jean-Noël Vittaut, Patrick Gallinari, 2024\n\n[6] CViT: Continuous Vision Transformer for Operator Learning, Sifan Wang, Jacob H Seidman, Shyam Sankaran, Hanwen Wang, George J. Pappas, Paris Perdikaris, 2024.\n\n[7] Learning Mesh-Based Simulation with Graph Networks, Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, Peter W. Battaglia, 2020\n\n[8] Geometry-Informed Neural Operator for Large-Scale 3D PDEs, Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, Anima Anandkumar, 2023. \n\n[9] RIGNO: A Graph-based framework for robust and accurate operator learning for PDEs on arbitrary domains, Sepehr Mousavi, Shizheng Wen, Levi Lingsch, Maximilian Herde, Bogdan Raonić, Siddhartha Mishra, 2025."}, "questions": {"value": "### Presentation \n-\tCould you add details on the positioning of the paper? ie why is it so important to improve vortex detection/simulation ? I know it has a lot of industrial applications, but this will help understant the importance and use case of such methods. \n-\tThe experiment figure 2 is a very good illustration and motivating example that could be used to motivate the method at earlier stages in the paper. Moreover I think it would benefit of being more discussed : what are the expected behavior ? Why are the cases a and b wrong ? \n\n### Clarity \n-\tI think that the main figure 1, is a bit hard to read. Maybe highlighting arrows/link between blocks would help crealy identifying the role of each block. \n-\tLine 221-223 : what happend if $ \\xi > \\alpha $ and $\\beta$ ? \n-\tThe architecture is not discussed in the experiment, while being described for several paragraphs. This blurs the message, which is focused on the new KVN feature. \n\n### Experiment :\n-\tAblation on $\\gamma_g$, what removing it would do? It is mentioned line 244 that it ensure stable training. I think a small experiment to illustrate this sentence will strengthen this choice. Why are there 2 $\\gamma_g$ (line 241 and 249)? \n-\tIt is unclear for me, what is the ground truth/expected behavior of method, is it the DIMC method ? Then, while KVN seems to be far from the gt simulations on some example (eg fig 3b)? \n-\tIn figure 3, what is the network architecture used for each method? \n-\tWhat do you mean by inference on 45k particles ? are all point treated with one forward pass of the neural network ? This represents a lot of points, making the method scalable, but this is not discussed. How does scales the method wrt mesh size? in terms of memory consumption/inference time.... ? \n-\tDo you have experiment on public/standard dataset of the litterature ? This would help positionning the work with other references. I think datasets such as cylinder flow from [7] above contains vortexes, or some vorticity/smoke equations (eg from the well).\n-\tI think more recent baselines could be used to make the proposed comparison with existing method more convincing. As is, the most recent model is from 2021. There have been a huge littérature on this subject in the past few years (see eg reference in the weakness section). \n- very Little experimental details are proposed. What architecture are used ? It is important to be as precise as possible to allow reproductibility of the results. \n\n### Additional Questions\n-\tFig 4, Do you have any insight about why the vorticity magnitude strongly increase for the uniform sampling ? It is straight and start at about 8 for both experiment. \n-\tAs stated in the paper, The improvement brought by the KVN is minimal on several architecture. Doesn’t it mean that the greater wortex modelization would come from another aspect of the model ? Is sometimes even worsen the performances table 1). Wouldn't using more recent and advanced neural architecture improve the detection of vortexes?  \n\n### Minor comment\n-\tline 069 : Citation issue (Ma et. Al)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CJkKzPWjrC", "forum": "iRjGhXuye8", "replyto": "iRjGhXuye8", "signatures": ["ICLR.cc/2026/Conference/Submission14450/Reviewer_2ew8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14450/Reviewer_2ew8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762513522324, "cdate": 1762513522324, "tmdate": 1762924853131, "mdate": 1762924853131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}