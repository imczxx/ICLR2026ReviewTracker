{"id": "h1rCBN6hWi", "number": 1738, "cdate": 1756913364784, "mdate": 1759898190957, "content": {"title": "From Compression to Generalization: Language Model Distillation With Grafting", "abstract": "Knowledge distillation is a widely used technique in distilling large language models. It is applied both for strong-to-weak distillation, where large-scale flagship models serve as teachers to produce lightweight models suitable for deployment, and for weak-to-strong distillation, where previous-generation models contribute to the development of stronger next-generation models. From a model compression perspective of knowledge distillation, students may be encouraged to adopt mode-seeking behavior; however, for building generalizable generative language models, mode-covering behavior should also be considered. To address this, we conduct an experimental analysis and propose a simple yet effective grafting strategy, in which sequence trees generated at multiple temperatures for autoregressive modeling are combined into a single distillation target. Our extensive experiments demonstrate the effectiveness of the proposed grafting approach.", "tldr": "", "keywords": ["knowledge distillation", "autoregressive language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80a9aa5817ee41d40a0e11e1ca6f6900f63b6296.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address a typical trade-off in Large Language Model knowledge distillation—the balance between \"mode-seeking\" and \"mode-covering\", this paper proposes \"grafting\" a simple and easy-to-implement strategy. The method is shown to improve the student model's overall performance across various tasks, models, and algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear and explicit motivation:** The paper strongly argues that as the goal of knowledge distillation shifts from mere model compression to broader generalization, a purely \"mode-seeking\" approach is insufficient and may even limit the model's generalization capacity.\n2. **Sufficient experimental validation:** The experiments cover diverse tasks (e.g., instruction-following, math, code ), models (e.g., Qwen2.5, Llama3, DeepSeek-Coder ), and algorithms (e.g., SeqKD, GKD, DistiLLM-2 ), providing strong support for the central claims.\n3. **Simple and easy-to-implement method:** The proposed \"grafting\" strategy is simple and intuitive. It combines sequence trees generated at multiple temperatures into a unified distillation target , thereby attempting to balance \"mode-seeking\" and \"mode-covering\" behaviors."}, "weaknesses": {"value": "1. **Relatively limited performance gains:** While the performance improvements from \"grafting\" are consistent , the absolute gains are not highly significant in many cases. Even though the authors mention that sequence tree generation can be efficient (compatible with vLLM ), the limited gains might make practitioners using SOTA methods hesitate to introduce the extra workload and resource cost.\n2. **Unclear hyperparameter selection:** The method introduces new hyperparameters, namely the set of temperatures $\\mathcal{T}$ and the mixing distribution $\\pi_0$ . The paper appears to default to a uniform distribution ($\\pi_0(\\tau) = 1/|\\mathcal{T}|$ ) and a fixed set of temperatures (e.g., $\\mathcal{T}=\\{0.7, 1.0, 1.5, 2.0\\}$ ). The paper lacks a discussion on how to select these parameters.\n3. **Insufficient theoretical analysis:** The core idea of \"grafting\", mixing probabilities from sequence trees generated at multiple temperatures, appears to be a heuristic approach. The paper does not provide a deep theoretical analysis as to why this specific mixing strategy (Equation 8) is the optimal way to balance the two modes."}, "questions": {"value": "1. Considering the additional complexity that 'Grafting' introduces by requiring sequence generation at multiple temperatures, could the authors please comment on the practical significance of these gains? That is, under what circumstances do you believe the benefits of introducing the strategy proposed in the paper outweigh the additional costs?\n2. How sensitive is the method to the choice of $\\mathcal{T}$ (e.g., the number of temperatures, their range, and intervals)? Did you experiment with non-uniform mixing distributions for $\\pi_0$?\n3. Have you considered dynamic or adaptive \"grafting\" strategies? For instance, could the mixing weights $\\pi$ be adjusted based on the training step (e.g., as a form of curriculum) or even adapted based on the context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ejd3IKRKrJ", "forum": "h1rCBN6hWi", "replyto": "h1rCBN6hWi", "signatures": ["ICLR.cc/2026/Conference/Submission1738/Reviewer_Tdbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1738/Reviewer_Tdbg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669497932, "cdate": 1761669497932, "tmdate": 1762915874028, "mdate": 1762915874028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is concerned with the balance between the mode-seeking and mode-covering in knowledge distillation. It proposes a grafting strategy that integrates samples generated with diverse temperatures from the teacher LM into several existing knowledge distillation objectives (e.g., SeqKD). With this simple and effective strategy, the performance of the student LM is largely improved in both strong-to-weak and weak-to-strong scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed strategy is rather simple and universally applicable to existing knowledge distillation methods.\n2. The experiments involving both strong-to-weak and weak-to-strong cases are solid and show the strategy is promising."}, "weaknesses": {"value": "1. The proposed strategy is mostly observation-drive and lacks a proper theoretical justification.\n2. The experiments are mostly constrained to comparably smaller-scale LMs, and larger-scale LMs with tens of billions of parameters should also be considered. Similarly, LMs with different architectures (e.g., MoE LMs) would further strengthen the usefulness of the strategy."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7bSOzKhkuz", "forum": "h1rCBN6hWi", "replyto": "h1rCBN6hWi", "signatures": ["ICLR.cc/2026/Conference/Submission1738/Reviewer_wztp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1738/Reviewer_wztp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811529448, "cdate": 1761811529448, "tmdate": 1762915873816, "mdate": 1762915873816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We are sincerely grateful for all the reviewers’ dedicated time and their constructive feedback. While we are pleased to note that the contributions of our paper were generally evaluated positively, there appears to be a shared concern across the reviews regarding two specific points: computational costs and the rationale for the temperature setting. It would be nice to address these points comprehensively through this general response.\n\n---\n\n__Resolving a potential misunderstanding regarding computational costs.__\n\nReviewers FAkE and Tdbg raised concerns about the additional computational cost introduced by the grafting approach. We can confidently state that this cost is virtually nonexistent. Thanks to the use of the surrogate loss $\\hat{\\mathcal{L}}$ (justified in Section 4.3 and Appendix A), the only requirements for employing our grafting method in existing state-of-the-art distillation algorithms (such as GKD and DistiLLM-2) are generating the training data with varying temperatures and explicitly marking the temperature used for each generated sequence.\n\nWe would also like to emphasize that the total number of training sequences was kept consistent to ensure a fair comparison. Specifically, a reasonable and equitable comparison was maintained throughout the experiments, such as comparing generating 999 sequences at a single temperature versus generating the same total number (999 sequences) by sampling 333 sequences each at three different temperatures.\n\n---\n\n__Discussions on temperatures and prior weights.__\n\nReviewers ZV4z and Tdbg raised important questions regarding the setting of the temperature values and mixing weights, which are key components of our proposed grafting approach.\n\nAs Reviewer ZV4z accurately anticipated, employing an excessively large temperature leads to a degradation in performance. This observation aligns with common practice and knowledge in large language model generation, where high temperatures increase output entropy but often diminish overall quality. When this quality degradation becomes excessive—for example, in our experiments, we considered temperatures up to 3.0, a value that clearly exceeds the typical range (temperatures are generally not set above 2.0 in practice)—the effectiveness of our grafting technique is also reduced. This observed drop in efficacy would precisely represent the \"point of diminishing returns\" that Reviewer ZV4z referred to. Consequently, our grafting approach also requires the use of generally acceptable temperature levels, and the values adopted in our experiments (0.7, 1.0, and 1.5) are a reasonable choice based on common practice, rather than the product of complex hyperparameter tuning.\n\nNext, Reviewers ZV4z and Tdbg both inquired about the potential of using a weighted or adaptive mixture instead of the uniform mixture we currently employ for $\\pi_{0}$. This is an excellent segue into an intriguing discussion concerning the weighting coefficient ($\\pi_{t}$) assigned to each individual subtree. In the formulation presented in our paper, $\\pi_{0}$ can be viewed as the prior and $\\pi_{t}$ as the posterior. Since $\\pi_{0}$ reflects the state at time $t=0$ where we have no information regarding how good each temperature's subtree is, it is more rational to employ a noninformative uniform prior, even though an engineered weighted mixture might potentially yield superior results. Subsequently, we observe that $\\pi_{t}$ is conditioned on the path traversed on the grafted sequence tree up to that point. This structure effectively constitutes a posterior update based on preceding observations from the initial uniform prior, rendering the mixture, in some sense, an adaptive one. Including a detailed discussion on this $\\pi$ coefficient in the camera-ready revision will undoubtedly aid in understanding the mechanics of our grafting approach and will further solidify the paper. We are thankful for leading this constructive discussion.\n\n---\n\nIn addition to these two points, the reviewers raised several other important comments. We hope that our individual responses to each reviewer address these concerns to their satisfaction.\n\nSincerely,  \nAuthors of Submission1738"}}, "id": "R9Oh8UjxqT", "forum": "h1rCBN6hWi", "replyto": "h1rCBN6hWi", "signatures": ["ICLR.cc/2026/Conference/Submission1738/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1738/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1738/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763360131755, "cdate": 1763360131755, "tmdate": 1763360131755, "mdate": 1763360131755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the knowledge distillation of generative language models. The authors propose a core argument: traditional distillation methods focus excessively on \"mode-seeking\" behavior (i.e., high-fidelity imitation of the teacher), which, while suitable for model compression, impairs the ability of the student model (especially large-capacity ones) to capture generative diversity, thus limiting mode-covering generalization.\nTo address this tension, the paper introduces a new technique called \"Grafting.\" This method combines the sequence trees (or probability distributions) generated by the teacher model at multiple different decoding temperatures into a single, more diverse distillation target, thereby achieving a better balance between mode-seeking and mode-covering.\nThe authors theoretically justify the soundness of this approach (e.g., providing a gradient equivalence proof for forward KL and a variational upper bound for reverse KL). Experimental results show that the Grafting strategy can consistently improve the performance of various existing distillation baselines (such as GKD, DistiLLM) on general-purpose instruction-following, mathematical reasoning, and code generation tasks. Notably, under weak-to-strong distillation setups, the student model's performance can even surpass that of the teacher."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Instead of viewing distillation merely as model compression, the paper offers a novel perspective that elevates the problem to a fundamental trade-off between \"mode-seeking\" (precision) and \"mode-covering\" (generalization/recall). This framework is significant for understanding and improving the distillation of generative models, particularly large-capacity ones.\n2.The \"Grafting\" strategy is well-grounded in theory. The mathematical derivations in Appendix A are rigorous, proving that the proposed tractable loss is a valid proxy for the intractable objective. The gradient equivalence for the forward KL case is a particularly strong technical contribution.\n3.The paper's analysis is highly mature. The fidelity analysis (Figure 5c) convincingly demonstrates that performance gains do not come from higher fidelity but from stronger generalization, which perfectly supports the \"mode-covering\" thesis. Furthermore, the authors are very candid in disclosing and analyzing a failure case (the 0.5B model in Appendix B.3). This counterexample, rather than being a weakness, strengthens the paper's core hypothesis by defining the method's scope of applicability (i.e., high-capacity student models)."}, "weaknesses": {"value": "While the paper's contributions are impressive, the strength of the core claims could be further reinforced by more in-depth ablation studies on implementation details and key hyperparameters."}, "questions": {"value": "To more accurately assess the contribution of this work, I hope the authors will address or discuss the following questions in the final version:\n1.The paper claims its method differs from \"merely aggregating multi-temperature samples.\" Could the authors provide an experiment that fairly compares \"Grafting\" (sampling from the mixture distribution ) with \"Data Aggregation\" (mixing datasets sampled from each  individually) under the same total sample size? If their performance is similar, is the theoretical complexity of the \"Grafting\" approach (e.g., the proofs in Appendix A) still necessary?\n2.The paper defaults to a uniform mixture in Eq. (8) (i.e., ). Why was this chosen? Given that Fig. 5a shows different single temperatures contribute differently (e.g.,  is strongest on Avg.), did the authors experiment with a weighted mixture? For instance, might a strategy weighted towards the optimal single temperature (e.g., 50% , 25% , 25% ) yield even better results than the uniform mixture?\n3.Could the authors further explore the boundaries of this trade-off? If the \"Grafting\" strategy incorporates more or higher-temperature trees (e.g., ), is there a point of diminishing returns where the fidelity drops too low (excessive mode-covering), causing the final performance (e.g., Avg. winning rate) to decrease as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cuZ4hMgpy5", "forum": "h1rCBN6hWi", "replyto": "h1rCBN6hWi", "signatures": ["ICLR.cc/2026/Conference/Submission1738/Reviewer_ZV4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1738/Reviewer_ZV4z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094802773, "cdate": 1762094802773, "tmdate": 1762915873239, "mdate": 1762915873239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study re-examines knowledge distillation for autoregressive generative language models, emphasizing that its core value extends beyond mere model compression. While pure mode-seeking methods are applicable for compression, they restrict student models’ ability to capture linguistic diversity and hinder generalization on language modeling tasks. To address this issue, the authors propose a concise \"grafting strategy\" that balances mode-seeking and mode-covering via fusing multi-temperature sequence trees, along with flexibility for both strong-to-weak and weak-to-strong distillation scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This study conducts experiments covering three key tasks—general instruction following, mathematical reasoning, and code generation—with three major model families (Qwen, Llama, and DeepSeek-Coder). It also explores two distinct distillation scenarios: distilling from stronger to weaker models and from weaker to stronger models, effectively validating the proposed method’s effectiveness.\n2.  Knowledge distillation requires balancing mode-seeking and mode-covering. The proposed grafting strategy achieves this balance by combining multi-temperature sequence trees, thereby enhancing the model’s generalization ability.\n3.  The paper provides valuable insights into the relationship between student model capacity and grafting gains: stronger student models yield more significant benefits from the grafting strategy, while insufficient student capacity leads to limited or even no gains."}, "weaknesses": {"value": "1. The authors have not provided open-source code for the proposed method. This lack of reproducibility hinders other researchers from verifying the experimental results, building upon the work, or comparing it with alternative approaches—an essential practice in academic research.\n2. The validation scope across datasets is relatively limited. For instance, in the mathematical reasoning task, only two datasets (MathQA and GSM8K) are evaluated, while other widely adopted benchmarks such as AMC, AIME, MinervaMath, and OlympiadBench are not included. A similar gap exists for the other tasks (general instruction following and code generation), where the method’s performance on a broader set of standard datasets remains unvalidated. This limits the generalizability of the reported findings.\n3. For the mathematical reasoning and code generation tasks, the proposed method has not been evaluated on state-of-the-art reasoning-specialized models (e.g., models optimized for step-by-step reasoning or domain-specific logical deduction). Given the growing relevance of such models for these tasks, this omission makes it difficult to assess the method’s competitiveness in real-world scenarios where reasoning-capable models are commonly used."}, "questions": {"value": "1. Please supplement experiments on the broader datasets mentioned in the Weaknesses section (e.g., AMC, AIME for math reasoning) to verify the grafting strategy’s generalizability.\n2. Please evaluate the method on currently popular reasoning-specialized models and demonstrate its effectiveness on these reasoning-focused architectures.\n3. Under strict data efficiency constraints (e.g., 1/10 or less of conventional training data), might multi-temperature sample aggregation outperform your grafting strategy in the performance-efficiency trade-off by avoiding complex probability fusion overhead? Do your conclusions only hold with sufficient data, and can you supplement low-data regime experiments to clarify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Lu5xR2OLKQ", "forum": "h1rCBN6hWi", "replyto": "h1rCBN6hWi", "signatures": ["ICLR.cc/2026/Conference/Submission1738/Reviewer_FAkE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1738/Reviewer_FAkE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762785510871, "cdate": 1762785510871, "tmdate": 1762915873019, "mdate": 1762915873019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}