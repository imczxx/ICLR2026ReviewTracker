{"id": "CDdPQFYgFz", "number": 5213, "cdate": 1757867932400, "mdate": 1759897987994, "content": {"title": "Fluid-DiT: Graph-Free Diffusion Transformers for Fluid Flow Simulations Learning", "abstract": "Simulating complex fluid flows requires capturing full equilibrium distributions rather than just mean trajectories, yet high-fidelity solvers remain computationally prohibitive. Recent advances, such as Diffusion Graph Networks (DGNs), have combined diffusion models with graph neural networks to sample equilibrium states directly from unstructured meshes, enabling distributional accuracy even from short simulations. However, graph-based diffusion approaches suffer from hand-crafted architectural constraints, limited receptive fields in message passing, and costly multi-scale designs, which restrict scalability to larger and more complex domains. We propose Fluid-DiT, a Graph-Free Diffusion Transformer that replaces graph message passing with attention-based denoising, eliminating explicit graph design while preserving the ability to model distributions of chaotic flows. Our framework introduces a latent-space formulation that disentangles geometric fidelity from distributional learning, reducing high-frequency artifacts and accelerating sampling. By leveraging the transformer’s global receptive field, Fluid-DiT naturally captures both local flow structures and long-range correlations without requiring hierarchical graph coarsening. On canonical benchmarks including laminar cylinder wakes, ellipse-flow systems, and turbulent 3D wing experiments, Fluid-DiT consistently outperforms graph-based diffusion baselines in both sample quality and distributional accuracy, achieving higher $R^2$ correlations and lower Wasserstein distances. Moreover, it generalizes robustly from short, incomplete trajectories to unseen Reynolds numbers and geometries, demonstrating strong scalability across 2D and 3D domains.", "tldr": "", "keywords": ["Fluid Flow Simulations", "Diffusion Transformers"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a18edc04dd644e0b97a1cd499416907ed2da283b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Fluid-DiT introduces a graph-free diffusion transformer for learning equilibrium distributions of fluid flows. Unlike prior graph-based models (e.g., Diffusion Graph Networks, DGNs), it eliminates explicit mesh or graph message passing by using attention-based denoising in a latent space, achieving scalability and accuracy across 2D and 3D turbulent flows."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Removes the dependence on handcrafted mesh graphs, hierarchies, and message-passing architectures.\n\nWorks uniformly across 2D and 3D geometries — from laminar wakes to turbulent wings — without re-engineering connectivity.\n\nAttention layers provide global receptive fields, allowing the model to couple distant flow regions (e.g., wake formation, pressure recovery) efficiently."}, "weaknesses": {"value": "Computational and Memory Overheads of Transformers. \"All models are trained on NVIDIA A100 GPUs with a batch\nsize of 32.\" This is too expensive.\n\nNo qualitative visual evidence\n\nThe paper presents extensive quantitative results (tables of R², Wasserstein distance, RMS error), but no visual comparisons of predicted vs. ground-truth flow fields.\n\nFor a paper on fluid dynamics, where spatial and structural fidelity are central, this omission makes it hard to assess whether Fluid-DiT truly reproduces realistic vortex streets, wake structures, or turbulence features."}, "questions": {"value": "How expensive is Fluid-DiT to train compared with DGNs and LDGNs (in GPU-hours)?\nAre there scalability issues when applied to meshes with >100k nodes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x65DYNIGAI", "forum": "CDdPQFYgFz", "replyto": "CDdPQFYgFz", "signatures": ["ICLR.cc/2026/Conference/Submission5213/Reviewer_jCbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5213/Reviewer_jCbN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588194806, "cdate": 1761588194806, "tmdate": 1762917949882, "mdate": 1762917949882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Fluid-DiT, a graph-free diffusion transformer for sampling equilibrium states for CFD simulations. The framework combines a difffusion model with attention layers in a latent space and an autoencoder for encoding/decoding. The framework is tested on multiple cases and compared against graph-based diffusion baselines where it manages to outperform the competing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The general idea presented of comparing graph-based diffusion models with diffusion-approaches based on attention only is an interesting research questions and the authors present a clear comparison for the three test cases including ablation studies."}, "weaknesses": {"value": "In the Reviewers' opinion, the paper can not be recommended for acceptance at ICLR due to the following major weaknesses:\n- The title is misleading: One of the key parts of the framework, the latent encoding, is based on an graph-based or convolutional autoencoder. Therefore calling the overall framework graph-free seems to be an odd choice.\n- According to the Reviewer the novelty of the proposed approach is limited. Diffusion in a latent space has already been introduced in multiple other approaches as a way to cope with the extensive cost of high-dimensional spaces. Moreover, diffusion models that include the attention-based mechanism are also not novel by itself. The reviewer acknowledges that for physical systems with irregular meshes a detailed comparison with graph-based diffusion approaches can be of interest to the scientific community but the current paper falls short here as well in the reviewers' opinion.\n- There are major inconsistencies in the description of the experiments. In addition,  now code is available for the review process. For the first case, the main paper mentions that the flow is at Re=100 only whereas the appendix mentions a range for the Re from 100 to 400. For the third case, the Re in the paper is defined to be 2000, whereas it is 10^5 in the appendix.\n- The abstract overclaims with regards to the experiments. According to the Reviewer, to show generalization to unseen geometries and Re-numbers, a higher variability for geometries and Re-numbers has to be employed. For Re numbers in the main part of the paper moreover, there is no variability at all mentioned.\n- The paper emphasizes global coupling as one of the advantages of the attention-based mechanism, but the authors then employ block-sparse attention with a few global tokens only or even constrain attention to local neighborhoods. This should be criticality discussed.\n- The comparison is to graph-based methods only, but would strongly benefit from including neural operator based approaches or general transformer-based surrogates."}, "questions": {"value": "See weaknesses:\n-> Please especially address all the inconsistencies in the experiments section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QfuULKMGDm", "forum": "CDdPQFYgFz", "replyto": "CDdPQFYgFz", "signatures": ["ICLR.cc/2026/Conference/Submission5213/Reviewer_aMng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5213/Reviewer_aMng"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785247308, "cdate": 1761785247308, "tmdate": 1762917949681, "mdate": 1762917949681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Fluid-DiT, a graph-free diffusion transformer that replaces message passing of GNNs with global attention mechanisms and operates in a compressed latent space to generate equilibrium distributions of fluid flows more efficiently."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Although this paper does not comprehensively address the practical limitations of graph-based approaches, it validates the proposed solution across multiple standard benchmarks with ablation studies.\n2. This paper presents formalized propositions (1-4) showing that self-attention subsumes message passing and that latent diffusion preserves distributional fidelity.\n3. Fluid-DiT improves performance over baselines (DGN, LDGN) on 2D and 3D domains while enhancing inference speed."}, "weaknesses": {"value": "1. While the paper mentions limitations of graph-based methodologies, it has weaknesses in addressing research that has actually tackled these limitations and is insufficient in describing how it differs from existing methods, thereby undermining the \"graph-free\" advantage.\n2. The paper does not sufficiently explain why the diffusion-based transformer combination is important and whether it provides unique insights into fluid dynamics beyond \"replacing GNN with attention.\"\n3. Although this paper aims to improve upon the existing work of [1], it does not clearly reveal what problem it intends to solve and may be considered incremental research. Moreover, it appears to lack a comprehensive comparison with the experiments from the original study[1].\n4. Considering that the architectures of DGN and LDGN [1] are graph-free, the contribution of this paper can be viewed as incremental. Rather than methodological differences, it is unclear what problem the paper attempts to solve.\n5. Out-of-distribution tests ('Re' extrapolation, geometry transfer) showed only marginal improvements that may not justify the architectural complexity.\n6. The paper fails to discuss research on hierarchical mesh GNN[4] and Mesh Transformers [2,5] and mesh rewiring [3] that aims to address the problems of existing mesh GNNs.\n7. Propositions 1-4 are either trivial, imprecise, or lack proper proofs.\n\n\n> [1] Lino, Mario, Tobias Pfaff, and Nils Thuerey. \"Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks.\" ICLR 2025\n> \n> [2] Yu, Youn-Yeol, et al. \"Learning flexible body collision dynamics with hierarchical contact mesh transformer.\" ICLR 2024\n> \n> [3] Yu, Youn-Yeol, et al. \"PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks.\" ICLR 2025.\n> \n> [4] Fortunato, Meire, et al. \"Multiscale meshgraphnets.\" arXiv preprint arXiv:2210.00612 (2022).\n> \n> [5] Janny, Steeven, et al. \"Eagle: Large-scale learning of turbulent fluid dynamics with mesh transformers.\" ICLR 2023"}, "questions": {"value": "Q1. Regarding Proposition 1, you state that \"This shows that attention provides a strictly more expressive mechanism than message passing.\" Is there empirical or theoretical evidence that it is more expressive?\n\nQ2. Can you provide a comparative discussion with the Multi-scale DGN from Valencia et al., 2025?\n\nQ3. In Appendix D.6, regarding \"generalization to out-of-distribution Reynolds number,\" is this the same setting as Table 2 in [1]? Can you compare LDGN's original setting for a fair comparison?\n\nQ4. In the introduction, you state that \"Message passing requires multiple hops to propagate information across the mesh, making it difficult to capture global interactions such as wake formation or large-scale vortex shedding.\" However, in the mesh GNN simulation field, studies attempt to address this through hierarchical structures or rewiring. Can you address these studies in the related work or discuss the differences and commonalities through empirical comparison, thereby clearly positioning your proposed method?\n\nQ5. You claim that attention provides \"global receptive fields\" superior to k-hop message passing, but with block-sparse attention (b=32, g=4), most nodes attend to only 36 neighbors. Isn't this comparable to 2-hop message passing on typical meshes?\n\n\n> [1] Lino, Mario, Tobias Pfaff, and Nils Thuerey. \"Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks.\" ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XksPVErQAt", "forum": "CDdPQFYgFz", "replyto": "CDdPQFYgFz", "signatures": ["ICLR.cc/2026/Conference/Submission5213/Reviewer_t1vu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5213/Reviewer_t1vu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980028897, "cdate": 1761980028897, "tmdate": 1762917949492, "mdate": 1762917949492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}