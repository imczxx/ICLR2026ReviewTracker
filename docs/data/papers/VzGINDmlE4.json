{"id": "VzGINDmlE4", "number": 21252, "cdate": 1758315428757, "mdate": 1759896932171, "content": {"title": "CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?", "abstract": "We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models  to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains.", "tldr": "", "keywords": ["Crochet Pattern Generation", "Multimodal", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82880704eae48c883dda4e9bda178220e84a9ade.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CrochetBench, a benchmark for evaluating the ability of VLMs to perform fine-grained, low-level procedural reasoning in the domain of crochet. In CrochetBench, models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. The benchmark includes 4 types of tasks, including stitch classification, instruction grounding, and both natural language and image-to-DSL translation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark built around the crochet domain is conceptually interesting and could potentially introduce new challenges for VLMs."}, "weaknesses": {"value": "I believe this paper has significant issues in its presentation, which makes it hard to follow and understand, and therefore difficult to assess its contribution.\n\n- It is hard to follow most parts of the paper. There are no examples or figures to help readers understand what the benchmark is assessing. Given that this is a highly specialized domain (crochet), many readers from the ICLR community may not have the relevant background knowledge. The only section that can somehow know the background is in the introduction, but immediately after that, the paper shifts to dataset statistics and experiments. Because of the poor presentation, it is very difficult to understand the work and, consequently, to evaluate its contribution. I suggest the author to provide sufficient background in the main paper and polish the writing.\n\n- The four task types are not described clearly. The paper only mentions what ability each task is supposed to test, without explaining in detail what the model is actually being asked to do. The descriptions rely heavily on complex terminology without clarifying their meaning. For example, in Lines 140–141, the paper says: “Task A (Stitch Recognition) evaluates a model’s ability to detect symbolic primitives in crochet images, establishing the foundation for multimodal perception.” However, it is unclear what “symbolic primitives” means. \n\n- The paper uses many uncommon or domain-specific terms without explanation --  for example, “procedural crafts,” “stitch abbreviations,” and “counts.”\n\n- The paper only presents the performance, no detailed analysis why models fail and no insights. Despite Table 8 provide the error analysis, no description of how failure analysis is performed or conducted.  \n\n- Several table references are incorrect or missing (e.g., Lines 98 and 106), which further adds to the confusion."}, "questions": {"value": "The use of LLMs is not disclosed anywhere in the paper. Did the authors use an LLM for writing the paper, and if so, to what extent was it used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SfZNXayPY5", "forum": "VzGINDmlE4", "replyto": "VzGINDmlE4", "signatures": ["ICLR.cc/2026/Conference/Submission21252/Reviewer_WDXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21252/Reviewer_WDXq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866641034, "cdate": 1761866641034, "tmdate": 1762941656510, "mdate": 1762941656510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CrochetBench, a benchmark designed to evaluate the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks focused on description, CrochetBench emphasizes execution, requiring models to recognize stitches, generate structured instructions, and translate natural language or images into CrochetPARADE DSL. The dataset includes tasks such as stitch classification, instruction selection, natural language generation, and DSL translation, with outputs evaluated for executable correctness. This paper reveal significant gaps between surface-level understanding and executable precision."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is a new approach to employ crochet, a craft defined by its intricate structure and creativity, as a framework for evaluating a model's reasoning and code generation capabilities.\n- This benchmark highlights the limitations of existing vision-language models."}, "weaknesses": {"value": "- The data, only sourced from the Yarn spirations website, may be biased toward specific design styles or formats, limiting its diversity and representativeness.\n- It is unclear whether the use of GPT-4o-mini for PDF conversion and annotation involved any manual error checking.\n- The evaluation for Task D only focuses on compilation success, without comparing the geometric and topological similarity between the compiled output and the reference design.\n- There is a progressive relationship between the tasks, such as Task B potentially relying on Task A, which could lead to error propagation. Exploring the dependencies between tasks is necessary to obtain more accurate indicators of reasoning capability.\n- The evaluation relies on a limited number of models, and the open-source models are small."}, "questions": {"value": "- There are issues with the table labels, as some references are incorrectly displayed as question marks.\n- In Task C: Instruction Generation, performance is evaluated by comparing outputs to a reference answer; however, it is worth considering that a single product may have multiple valid crochet methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wb2c4s0ylO", "forum": "VzGINDmlE4", "replyto": "VzGINDmlE4", "signatures": ["ICLR.cc/2026/Conference/Submission21252/Reviewer_Xwj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21252/Reviewer_Xwj6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883289692, "cdate": 1761883289692, "tmdate": 1762941656255, "mdate": 1762941656255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes an interesting benchmark called CrochetBench, to test whether multi-modal language model can understand how to perform crochet, which is a interplay of different actions: recognize stitches, select structurally valid instructions, and generate compilable crochet procedures, effectively performing 3D-aware reasoning. When evaluating current models on the proposed benchmark, the authors find that their performance drops sharply when they must generate instructions that are actually executable and correct, revealing major limitations in their reasoning and procedural skills."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors introduced an interesting task, CrochetPARADE DSL, as it does have this nice property of verifiable, meaning it could be beneficial for other tasks, for example post-training RL.\n- I can see the challenge of crochet code generation, as it requires 3D-aware reasoning, and because it is a quite niche task, it is possible that current language models have not been trained on this tasks, making it less contaminated task and might better reflect model's performance differences"}, "weaknesses": {"value": "- the author emphasized that their benchmark focuses on the instruction fidelity, if the model can generate valid, compilable DSL code, based on multi-modal input, and  opens a new direction for multimodal research, which I don't think they are the first to do this: for example, the whole area of letting LLMs/multi-modal LLMs to generate symbolic graphics programs like SVG (2D), CAD (3D) etc. which fullfill all the requirements and properties of this crochet DSL, have already being studied before. I think this task sounds novel, but the contribution claim might not be accurate. At least the authors should provide a more comprehensive analysis to these prior works to thoroughly discuss why their DSL / benchmark is more suited to benchmark LLMs, or more into the details of the DSL differences\n- minor errors in writings (e.g., table reference wrong in line 98 / 106), related work section in the appendix\n- there needs more explanation about the benchmark generation process (what tasks have been done), to highlight the author's contribution, because currently the paper seems to be focused on the task and the DSL CrochetPARADE (which is not part of the paper) and the result analysis"}, "questions": {"value": "- crochet DSL generation benchmarking seems to be an interesting and I can imagine that it might be relevant to some readers / research, but it is a very niche task. This does not undermine the valid and significance of this task, but I think only if it has proven to be beneficial for generic visual / 3d reasoning, its significance remains limited. How can this kind of DSL/benchmark benefit generic multi-modal llms? for example through instruction finetuning, it improves the model's performance on CrochetBench, but for example, this performance gain is also valid in other multi-modal visual reasoning tasks, for example like geometric reasoning problems?\n- the benchmark results is inconsistent,  the tasks A, B, C and D have three different best performing models? even open-sourced model can perform the best (table 7), and even outperforming the other models by a large margin, showing maybe the performance on this proposed CrochetBench is not generic or varies a lot, or even data contamination within the benchmark? this might not be a good property of a benchmark"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4LAy1oiPY4", "forum": "VzGINDmlE4", "replyto": "VzGINDmlE4", "signatures": ["ICLR.cc/2026/Conference/Submission21252/Reviewer_hNU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21252/Reviewer_hNU1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950536045, "cdate": 1761950536045, "tmdate": 1762941655940, "mdate": 1762941655940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Benchmark for procedural crochet understanding: perception (stitch recognition), retrieval (instruction selection), text generation, and both natural language and image-to-DSL (CrochetPARADE) with compilation/execution as the main metric. Their core finding: surface text metrics don’t predict executability; program synthesis is the bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Execution-grounded evaluation. CrochetPARADE enables syntactic/structural validation and visualization/execution, providing a more faithful signal than BLEU/ROUGE alone.\n2. Well-structured task ladder. Tasks escalate from perception to executable synthesis with clear metrics and sizes.\n3. Dataset scale & coverage. Table 1 reports 6,085 patterns, 98.77% image coverage, 55 project types.\n4. Clear gap at execution. Performance “declines as evaluation shifts to executable correctness”, project-level CSR is low.\n5. Crochet is an under-explored domain for LLM code generation and a good test bed."}, "weaknesses": {"value": "1. Semantic equivalence vs. compilation. Compilation checks syntax/structure but can miss semantically equivalent programs. The authors motivate execution-based metrics but do not pair them with visual render agreement in main results.\n2. More qualitative results are needed for better interpretation of the results."}, "questions": {"value": "I hope the author can address my concerns in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J6OaxmkQqY", "forum": "VzGINDmlE4", "replyto": "VzGINDmlE4", "signatures": ["ICLR.cc/2026/Conference/Submission21252/Reviewer_GPYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21252/Reviewer_GPYB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995164683, "cdate": 1761995164683, "tmdate": 1762941655605, "mdate": 1762941655605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}