{"id": "f6LE0OJERY", "number": 16956, "cdate": 1758270563027, "mdate": 1759897207930, "content": {"title": "TURTLEAI: Benchmarking Multimodal Models in Turtle Graphics for Visual Programming and Reasoning", "abstract": "Multimodal vision-language models (VLMs) have achieved remarkable success in fundamental visual tasks like image captioning and visual question answering. However, their performance on complex visual tasks requiring integrated visual reasoning and problem-solving capabilities remains underexplored. To bridge this gap, we introduce TurtleAI, a multimodal benchmark to evaluate VLMs on visual programming and reasoning tasks in the Turtle Graphics domain. Our benchmark contains 823 visual programming tasks that challenge VLMs to generate Python code to replicate patterns in images. Evaluation of 20 VLMs reveals that state-of-the-art models like GPT-4o and Qwen2-VL-72B struggle with these tasks, achieving success rates of only 26.5% and 11.8% respectively. Our analysis reveals that models often fail to align their code implementation with visual reasoning. To address this misalignment, we propose TurtleAI-Datagen, a data generation framework that creates large-scale synthetic datasets consisting of task-code pairs. Using just 10 initial samples, TurtleAI-Datagen generates over 700k samples. Fine-tuning on this dataset significantly reduces errors arising from the misalignment between visual reasoning and program synthesis, improving Qwen2-VL-72B's performance by over 20%. We will release the benchmark publicly to facilitate future research.", "tldr": "A Multimodal Benchmark for Visual Programming and Reasoning", "keywords": ["Multimodal Models", "Program Synthesis", "Visual Programming", "Benchmark", "Turtle Graphics", "Visual Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b58e4e4c2234b1ff8af3df68bf1ea5c70668dbb.pdf", "supplementary_material": "/attachment/cd9f53670171f4b60d76879e77fd4066c2445eef.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a multimodal benchmark, TURTLEAI, which challenges vision-language models (VLMs) to generate Python code for drawing patterns. TURTLEAI consists of three components: TURTLEAI-DS (a collection of datasets), TURTLEAI-Eval (an evaluation framework), and TURTLEAI-Datagen (a data generation framework).\nTURTLEAI-DS contains pairs of images and their corresponding Python code. A VLM is required to generate Python code based on a given image. The generated image is then compared with the original image using TURTLEAI-Eval. TURTLEAI-Datagen is designed to generate (image, code) pairs and chain-of-thought (CoT) reasoning examples for fine-tuning VLMs.\nExperimental results demonstrate that existing VLMs struggle to perform well on these tasks, but fine-tuning the models significantly improves their performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper introduces a new benchmark to facilitate the development of VLMs.\n-\tIt reveals that existing VLMs struggle with visual programming tasks.\n-\tIt proposes a data generation framework for fine-tuning."}, "weaknesses": {"value": "- In TURTLEAI-Eval, which compares drawings in a transformed, normalized space, the line width is standardized to a fixed value of 1. However, certain shapes may require width information for accurate representation. For instance, a solid rectangle can be considered as a very thick line, and its width plays a crucial role in distinguishing it from other shapes.\n- In Symbolic comparison and Embedding-based comparison, there is a predefined threshold. What impact does this threshold have on the evaluation results? How is this threshold determined, and is there any experimental evidence to support it?\n- In TURTLEAI-DATAGEN, two codes are randomly selected from the dataset to extract their high-level mutation pattern. However, the selected codes may not have a clear mutation pattern (e.g., adding a loop to the code).\n- In stage 3 of TURTLEAI-DATAGEN, is there any quality check for the generated CoT reasoning?"}, "questions": {"value": "If the writing could clarify the code mutation process in more detail, especially the extraction of high-level mutation patterns from reference codes, and include some concrete examples, it would enhance the quality of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x8YsoOsUCr", "forum": "f6LE0OJERY", "replyto": "f6LE0OJERY", "signatures": ["ICLR.cc/2026/Conference/Submission16956/Reviewer_FsFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16956/Reviewer_FsFK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883142860, "cdate": 1761883142860, "tmdate": 1762926977759, "mdate": 1762926977759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TURTLEAI, a dataset and benchmark for visual to code generation in Turtle Graphics, along with a synthetic data pipeline that reportedly improves model performance.      However, the benchmark is evaluated only on its own synthetic distribution, and the observed improvements are likely due to domain exposure rather than genuine reasoning gains.      OOD results show that finetuning actually makes models worse on hand-drawn sketches, suggesting harmful overfitting.      No external datasets, no comparisons to existing sketch to code benchmarks, outdated baselines, and poor presentation quality further weaken the contribution.      While releasing the dataset could be valuable to the community, the scientific impact and novelty of the paper are limited."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem formulation and a well defined task scope.\nThe paper focuses on visual-to-code generation via Turtle Graphics, which is a structured setting where correctness can be objectively evaluated via execution.\n\n2.  Infrastructure contribution.\nThe authors provide a dataset, an evaluator, and code for data synthesis and benchmarking.      If fully released, the benchmark could serve as a reproducible testbed for small scale visual to program induction.\n\n3.  Diagnostic experiments.\nThe paper analyzes failure types and includes limited OOD testing, which helps reveal the limitations of current VLMs on synthetic geometric tasks."}, "weaknesses": {"value": "1.   Major validity issue: the benchmark is only evaluated on its own synthetic data, with no external datasets or established baselines.\nThe paper trains on data generated by its own pipeline and then evaluates on a benchmark created from the same distribution.     This closed loop validation prevents demonstrating scientific novelty, generality, or impact.     There is no evidence that performance gains reflect reasoning improvements rather than domain overfitting or memorization.\n\n2.  OOD experiments contradict the core claims.\nIn Figure 7, models fine-tuned on TURTLEAI data perform significantly worse on hand-drawn OOD sketches.     Since both tasks are visually to code mapping, this drop indicates the model becomes *less* general and more brittle after training suggesting harmful domain overfitting rather than capability improvement.     This undermines the core contribution of the dataset and training pipeline.\n\n3.  The baseline success rates are extremely low (~10%), making “20% improvement” uninformative.\nBecause current VLMs have never been trained on synthetic Turtle-style graphics, any improvement after domain exposure is expected and does not imply methodological novelty.     The paper does not demonstrate that its data synthesis strategy is better than simpler alternatives such as random sampling, Self-Instruct, Evol-Instruct, or manual template expansion.\n\n4.  No comparison with existing visual to program or sketch-to-code benchmarks.\nThe work ignores relevant prior datasets (e.g., SVG/TikZ program induction, sketch-to-code datasets, visual UI-to-code tasks).     Without external validation, it is unclear whether TURTLEAI measures general reasoning or just fits a narrow toy domain.\n\n5.  The data and task space are extremely toy-like.\nAll visuals are synthetic geometric primitives with perfect rendering (no noise, occlusion, perspective, thickness variation).     Claims about “general visual reasoning” or “broad program synthesis” are overstated relative to the simplicity of the domain.\n\n6.  Paper presentation quality is below conference standard.\nAll tables are mislabeled as figures, and captions are consistently placed incorrectly.     This violates standard formatting guidelines and indicates insufficient care in preparation."}, "questions": {"value": "1.  Why are there no experiments on existing sketch-to-code or visual program induction benchmarks?     Without external evaluation, how can the benchmark claim scientific relevance beyond its own synthetic sandbox?\n2.  In Fig.7, why does fine-tuning severely degrade performance on hand-drawn sketches?     Doesn’t this imply the dataset harms general visual programming rather than improves it?\n3. What proportion of generated samples are incorrect, redundant, or semantically invalid?     Is there any human auditing, or are models only self-evaluating their own output?\n4.  How do you demonstrate that the proposed data synthesis approach is superior to simpler strategies (e.g., template mutation or random augmentation)?\n5. Many baselines used (e.g., GPT-4V, Qwen2-VL) are outdated relative to 2024–2025 VLMs.     Why are recent models (GPT-4o, GPT-5，Qwen2.5-VL，Qwen3-VL，etc.) missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61j0xTRrI8", "forum": "f6LE0OJERY", "replyto": "f6LE0OJERY", "signatures": ["ICLR.cc/2026/Conference/Submission16956/Reviewer_Ecws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16956/Reviewer_Ecws"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980567620, "cdate": 1761980567620, "tmdate": 1762926977355, "mdate": 1762926977355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TURTLEAI, a multimodal benchmark designed to assess visual programming and reasoning in Turtle Graphics. It provides 823 tasks requiring models to generate Python code that reproduces target images. Experiments on 20 VLMs show that state-of-the-art models struggle, with GPT-4o and Qwen2-VL-72B achieving low success rates. The authors also propose TURTLEAI-Datagen, a synthetic data generation pipeline producing 700k+ samples from 10 seeds, improving model performance by over 20% after fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a well-structured task setup linking visual reasoning to program synthesis and conducts systematic experiments across many VLMs, yielding a solid empirical assessment of model limitations in structured visual-to-code settings.\n\n2. The synthetic data framework is executed at scale and empirically improves model performance, demonstrating practical value for enhancing VLM capability in controlled visual programming tasks."}, "weaknesses": {"value": "1. Limited novelty relative to prior benchmarks: Similar multimodal visual-to-code and graphics reasoning benchmarks already exist (e.g., NAACL 2025 TurtleBench: A Visual Programming Benchmark in Turtle Geometry https://aclanthology.org/2025.naacl-long.607/\n). The paper primarily repackages an existing paradigm (image-to-code in a constrained graphics domain) rather than introducing a fundamentally new task or evaluation angle.\n\n2. Narrow and arguably low-impact domain: Turtle Graphics is a highly simplified, pedagogical environment with limited real-world relevance. Performance in this synthetic sandbox does not clearly translate to practical multimodal programming, robotics, CAD/graphics reasoning, or general visual planning tasks. The paper lacks evidence that gains on Turtle tasks meaningfully correlate with improvements on broader multimodal program synthesis or vision-reasoning benchmarks, raising questions about external validity and actual scientific payoff.\n\n3. Evaluation and insights remain shallow: While the paper reports success rates and shows synthetic data improves scores, the analysis stops short of deeper failure categorization, ablation across visual complexity factors, or diagnostics that could reveal why models fail (e.g., perceptual ambiguity vs. planning vs. code correctness). The work also does not benchmark against alternative data augmentation or curriculum approaches, nor does it explore whether improvements generalize beyond the Turtle setting, limiting interpretability and impact of the proposed method."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "egoEPGSU7U", "forum": "f6LE0OJERY", "replyto": "f6LE0OJERY", "signatures": ["ICLR.cc/2026/Conference/Submission16956/Reviewer_wESH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16956/Reviewer_wESH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984258718, "cdate": 1761984258718, "tmdate": 1762926976770, "mdate": 1762926976770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}