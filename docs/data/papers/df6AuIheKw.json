{"id": "df6AuIheKw", "number": 18802, "cdate": 1758290980875, "mdate": 1763733432671, "content": {"title": "Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling", "abstract": "Adam is known to perform significantly better than Stochastic Gradient Descent (SGD) in language models, a phenomenon for which a number of explanations have been proposed. In this work, we revisit this \"optimizer gap\" through a series of comprehensively tuned baseline training runs for language modeling with Transformers. We exhaustively study how momentum, gradient clipping, and batch size affect the gap between SGD and Adam. Our empirical findings show that SGD with momentum can actually perform similarly to Adam in small-batch settings, if tuned correctly. We revisit existing explanations for Adam's advantage, including heavy-tailed class imbalance, directional sharpness, and Hessian heterogeneity, which struggle to directly explain this phenomenon. Towards bridging this gap in our understanding, by analyzing our Transformer training runs and simple quadratic settings inspired by the literature, we provide new insights, driven by stochastic differential equation models, into the role of batch size on the training dynamics.", "tldr": "", "keywords": ["Adam", "SGD", "Language Models", "Attention", "Transformers"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb7f69f646271931148123e015c5fa6867bfbd53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the setting where SGD can match the performance of Adam by conducting extensive hyperparameter tuning. This perspective helps verify whether previous explanations of Adam’s advantage over SGD are effective regardless of choice of batch size. They also use the tool of SDE to show how batch size can affect SGD and signgd in different ways."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author tried small batch experiments in various setting to test whether previous theoretical explanations hold. Most experiments are conducted in a scientific and fair way with ablation study. \n2. The result that SGD won’t break and can even match the performance of Adam with much more iterations is a bit novel. I am not aware other papers that actually run SGD for so many iterations."}, "weaknesses": {"value": "1. The main finding lacks novelty. Kunster et al.2023 already shows in their figure 3 and figure 4 that the gap between Adam and SGD decreases or even disappears at small size while this paper also notes the consistency and similarity with previous results.\n2. The setting of small batch size and more update iterations is practically irrelevant and this paper also admits that this setting is inconvenient in training LLMs. Therefore, this paper doesn’t help to answer this important question that why Adam outperforms SGD in the conventional LLM training setting. And it is a bit unfair to directly compare the large batch size and small batch size under the same compute budget because the small batch size setting can take many more iteration steps. \n3. Though the experiment results clearly shows SGD can match Adam’s performance in small batch size setting, it can’t strongly support the author’s other claims. For example, I don’t think it falsifies the Hessian heterogeneity explanation. Please see the detailed question below. And Kunster et al. 2024 also mentions that they do not attempt to quantify the interaction between stochasticity and class imbalance. So I think it is unfair to claim all the previous explanations struggle to explain the phenomenon in this paper. And they still make sense in the setting of large batch size setting, which is arguably the more practical setting than small batch size. \n4. The theory and claims in section 4.1 don’t make sense to me. First they only cite theorems and proof from other paper without proving anything new, so it can’t be viewed as a significant contribution of this paper. Second, I think the way interpreting the result is problematic. See questions below. \n5. The gradient clipping and learning rate grafting is disconnected to the major claims. They provide the intuition that gradient scale isn't the problem of SGD in the large batch size setting, which can be a good start of another paper. But I don't see how it contributes to this paper."}, "questions": {"value": "1. As mentioned in weakness 3, I don’t understand why the authors interpret Zhang’s result as explanation invariant to batch size. The effect of hessian definitely gets boosted when the noise is small. When there is more noise, the effect of hessian structure becomes less significant. So I think the experiment results in this paper can still be explained under the Hessian heterogeneity framework. \n2. As mentioned in weakness 4, I don’t understand the interpretation of theorem 1 and figure 9. Why does a larger drift term indicate faster convergence? A faster convergence rate needs to be rigorously proved by showing the dependence on T and noise level(batch size). Also I don’t understand why the authors conclude from figure 9 that SGD in early training is dominated by the drift term. Why do you use the same learning rate and how do you choose 1e-3? Also I think it is natural that signgd won’t perform well in the small batch size setting because the sign function can provide completely opposite/wrong update direction when the noise is too large. So I don’t understand how figure 9 can support the claim from SDE. \n3. In appendix C, how do you compute directional sharpness when it involves a huge Hessian matrix? Also for figure 17, I feel the difference between Adam and SGD in the small batch size setting is not as significant as you claimed. For Adam, the sum of two terms is also close 0 or sometimes becomes positive. So I don’t think the metrics are even aligned with the optimizer’s success or failure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3ARCAewLf", "forum": "df6AuIheKw", "replyto": "df6AuIheKw", "signatures": ["ICLR.cc/2026/Conference/Submission18802/Reviewer_Hh7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18802/Reviewer_Hh7C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760725755656, "cdate": 1760725755656, "tmdate": 1762999987881, "mdate": 1762999987881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This pager studies the gap between SGD and Adam as the batch size varies. For language model training, it finds that the gap can be small, but only when the batch size is very small. Various existing theoretical models do not explain this phenomenon well, but the paper suggests a simple comparison of how noise impacts SDE approximations of SGD vs. SignSGD provides some explanation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting observation about training dynamics in the small batch regime that is not easily explained by some existing models. In particular any model that suggests Adam is better than SGD needs to account for why that property does not hold at small batch sizes.\n2. The experiments seem to be rigorous with appropriate tuning and ablations."}, "weaknesses": {"value": "1. The SDE explanation is not really fully fleshed out. First, in the figure, it is the noise level not the batch size that is varied (I understand that they are connected, but it should be possible to make an experiment with batch if so). Moreover, it is not clear that the gradient noise model is a good one in the full language modeling case. If this is the main explanation provided by the paper, there should be some more clear experiments trying to substantiate the model on real data. For example, looking at gradient or update variance could be important. Last, the analysis does not account for momentum (which I know is challenging), but seems important since the SGD results in particular require very high momentum.\n2. All the experiments are pretty under-trained. Only doing 1.3B tokens on a 160M model is even below chinchilla (20x), and way below the practical token-to-parameter ratios encountered in practice. This seems important since most optimization will happen later in training, so only studying the early training regime may give misleading results. I would suggest running some of the main results longer to see what happens."}, "questions": {"value": "1. Why are experiments done with no weight decay?\n2. In Figure 9, is the batch size fixed across runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2734kgyr0e", "forum": "df6AuIheKw", "replyto": "df6AuIheKw", "signatures": ["ICLR.cc/2026/Conference/Submission18802/Reviewer_Kxeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18802/Reviewer_Kxeu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420478785, "cdate": 1761420478785, "tmdate": 1762999987420, "mdate": 1762999987420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits the question of the reason behind the gap in the performance of Adam and SGD, by studying examples where SGD performs comparably to Adam. It finds that the gap decreases at small batch sizes. It also finds that various factors such as Hessian heterogeneity and class imbalance can exist at small batch sizes, but the gap is still low. Finally, it proposes a theory based on SDE approximation of SGD and SignSGD for explaining the gap."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper tries to understand the gap between Adam and SGD based on the observation that the gap diminishes at small batch sizes."}, "weaknesses": {"value": "The observation that Adam and SGD gap should diminish at small batch sizes is already supported by various previous theoretical works [1,2]. These papers show that the benefits of preconditioning decrease at small batch sizes, even for quadratics. Thus, the observation is expected.\n\nSecondly, first half of the paper provides misleading results as the $\\beta_2$ of Adam is not properly tuned for small batch sizes. In my opinion, this is a significant misrepresentation for the first half of the paper, and should be either removed or clearly stated beforehand that the gap in the following section vanishes after tuning $\\beta_2$.\n\n[1] - Zhang et al. 2019 - Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model\n\n[2] - Jain et al. 2018 - Accelerating Stochastic Gradient Descent For Least Squares Regression"}, "questions": {"value": "The definition of critical batch size as mentioned in lines 455-464 seems non-standard. The critical batch size is defined as the maximum batch size till which we can linearly decrease the number of steps required to achieve a given loss. Are the two definitions the same? To clarify, for SGD, critical batch size is not 1, and the SDE can be preserved while scaling up batch size by scaling eta proportionally to batch size. Is the claim that the batch size for signSGD will precisely be the B for which the erf linear scaling fails? Can this be proven?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ds7U1e9Scg", "forum": "df6AuIheKw", "replyto": "df6AuIheKw", "signatures": ["ICLR.cc/2026/Conference/Submission18802/Reviewer_yBqU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18802/Reviewer_yBqU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884084018, "cdate": 1761884084018, "tmdate": 1762999987177, "mdate": 1762999987177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}