{"id": "h8OJb8YGNa", "number": 16523, "cdate": 1758265549164, "mdate": 1759897235330, "content": {"title": "Towards Unpredictable Worlds: Continual In-Context Reinforcement Learning in Non-Stationary Environments", "abstract": "Traditional In-Context Reinforcement Learning (ICRL) demonstrates impressive rapid adaptation, but its reliance on static environments limits its applicability.\nIn contrast, real-world scenarios are inherently non-stationary, with continuous and unpredictable changes that challenge an agent's ability to adapt. \nTo bridge this gap, we formally define and systematically investigate Continual In-Context Reinforcement Learning in Non-Stationary Environments. \nOur central question is: what model architectures and training strategies enable an agent not only to rapidly master new dynamics in a continuously evolving environment, but also to efficiently discard or isolate outdated information, thereby achieving robust online adaptation?\nTo ground our investigation, we construct a new benchmark suite featuring two complementary non-stationary domains---a symbolic reasoning task and a physics-based control task---each modified to exhibit unpredictable, intra-lifetime dynamic changes.\nOn these benchmarks, we conduct  extensive evaluations at both the model and training-strategy levels.  \nAt the model level, we compare state-of-the-art sequence model architectures.\nAt the training strategy level, we systematically analyze the influence of stationary versus non-stationary training, dynamic change frequency, context length, and interaction scale.\nOur findings demonstrate the necessity of non-stationary training and reveal critical factors shaping continual adaptation. \nThese results provide actionable insights and design principles for building agents capable of learning and adapting in truly open and dynamic worlds.", "tldr": "", "keywords": ["In-Context Reinforcement Learning", "Sequence Model", "Non-Stationary"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c8a00d92e23b9beb98843331cd7f48c806497b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Continual In-Context Reinforcement Learning (CICRL), which extends traditional in-context reinforcement learning to handle non-stationary environments where dynamics change unpredictably within a single episode. The authors formalize this new setting and create two benchmark suites: a symbolic reasoning task and a physics-based control task with dynamic shifts occurring during an agent’s lifetime. They evaluate several sequence model architectures, including Transformers, Mamba2, and GatedDeltaNet, under different training conditions. The results show that non-stationary training is essential for effective continual adaptation and that models with stronger sequence modeling capabilities adapt more robustly. Overall, the paper provides a clear framework, new benchmarks, and key insights for developing agents that can learn and adapt in continuously changing environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear and well-motivated formulation of Continual In-Context Reinforcement Learning (CICRL), addressing an important gap in adapting to non-stationary environments.\n- The introduction of two complementary benchmark suites (symbolic and physics-based) provides a good empirical foundation for future research.\n- The evaluation protocol and metrics (e.g., $\\Delta$Switch, $\\Delta$Adapt) are well-designed to isolate aspects of continual adaptation and forgetting.\n- The ablation studies provide valuable insights into how factors like context length, change frequency, and scale influence continual adaptation."}, "weaknesses": {"value": "- The authors do not provide access to the code or benchmarks, even though they state an intention to release them. This makes it difficult to assess reproducibility and verify experimental results, especially since the proposed benchmarks are a core contribution of the paper.\n- The study relies exclusively on PPO as the underlying RL algorithm, which limits the generality of the findings and makes it unclear whether the observed adaptation behaviors would hold under different RL algorithms.\n- Some results are difficult to interpret quantitatively or theoretically, with overlapping performance among models in certain settings."}, "questions": {"value": "- How sensitive are the results to the choice of PPO as the underlying RL algorithm? Would other RL algorithms yield similar adaptation behavior?\n- How do the models handle gradual versus abrupt changes in environment dynamics?\n- Have you analyzed catastrophic interference within context representations, and can the agent actively forget outdated information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EPYc6oMzFn", "forum": "h8OJb8YGNa", "replyto": "h8OJb8YGNa", "signatures": ["ICLR.cc/2026/Conference/Submission16523/Reviewer_fnJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16523/Reviewer_fnJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951022960, "cdate": 1761951022960, "tmdate": 1762926611867, "mdate": 1762926611867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Continual In-Context Reinforcement Learning (CICRL), an extension of ICRL designed to address intra-lifetime non-stationarity. It defines a framework where environments evolve dynamically without episodic resets, proposes two non-stationary benchmarks, i.e., modified XLand-Minigrid and Kinetix, and evaluates several sequence models (Transformer, Mamba2, GatedDeltaNet) under static and non-stationary training regimes. The main finding is that non-stationary training improves adaptation to dynamically changing environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed benchmarks are well-designed and may aid future reproducible research.\n\n2. The evaluation metrics ($\\Delta_{\\rm Switch}, \\Delta_{\\rm Adapt}, \\Delta_{\\rm In-Context}$) provide a structured view of adaptation and forgetting.\n\n3. The experiments are extensive and include several ablation factors (context length, change frequency, scale)."}, "weaknesses": {"value": "1. The paper may have limited conceptual novelty. CICRL is mostly a rebranding of meta-RL with drifting task distributions. No new theoretical formulation or learning mechanism distinguishes it from existing continual or meta-RL setups.\n\n2. The paper only benchmarks off-the-shelf sequence models using standard PPO. There’s no method, modification, or insight addressing how to handle non-stationarity, it only shows that training on non-stationary data helps.\n\n3. XLand-Minigrid and Kinetix involve rule or physics changes that are arbitrary and discontinuous. These synthetic shifts may not convincingly approximate real-world or open-ended non-stationarity.\n\n4. The results find that non-stationary training helps. However, there’s no analysis of why certain models perform better.  \n\n5. Since model parameters are frozen, the framework lacks actual continual learning. There are no weight adaptation, consolidation, or transfer across shifts. The agent merely performs repeated context-based inference.\n\n6. No comparisons to true continual or meta-RL methods (e.g., EWC, online PPO, fine-tuning). Without them, it’s unclear whether CICRL offers any advantage beyond being static PPO trained on varied data."}, "questions": {"value": "1. How is CICRL mathematically or behaviorally distinct from meta-RL with changing task distributions?\n\n2. How does “forgetting” occur in a model without parameter updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t3WMV4mGRt", "forum": "h8OJb8YGNa", "replyto": "h8OJb8YGNa", "signatures": ["ICLR.cc/2026/Conference/Submission16523/Reviewer_jSvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16523/Reviewer_jSvP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987853258, "cdate": 1761987853258, "tmdate": 1762926611429, "mdate": 1762926611429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Continual In-Context Reinforcement Learning (CICRL): adapting to non-stationary environments without gradient updates. Authors propose a new benchmark for CICRL and perform experiments regarding model architectures and training strategies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes an important problem, extending ICRL to more real-world-like scenarios.\n  \n- Builds a new benchmark consisting of both discrete grid-world environments and continuous physics-based environments.\n  \n- The proposed new evaluation metrics and analysis around these metrics are interesting."}, "weaknesses": {"value": "- The evaluation protocol and certain evaluation metrics seem to only take discrete, episode-wise dynamics change into account, while nonstationarity may well be continuous (e.g. a robot continuously draining its battery).\n  \n- The overall formulation could focus more on the characterization of the continual aspect of CICRL. The current one (a sequence of POMDPs) can arguably be subsumed into a single POMDP with an unobserved, time-varying environmental variable.\n  \n- Experiment results regarding model architectures are confusing. For example, non-static Mamba2 is the best in minigrid in terms of avg-return, but the worst in kinetix."}, "questions": {"value": "- How are the 'Static' models trained? Why do they mostly perform much worse than non-static models even under a zero-shot setting without any contexts?\n  \n- What is the difference between CICRL and regular POMDPs?\n  \n- In Fig.4 freq-1 results, why is Random(1-10) the overall best strategy? Wouldn't Random(1-5) be closer to the freq-1 evaluation setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VitQhQXSBM", "forum": "h8OJb8YGNa", "replyto": "h8OJb8YGNa", "signatures": ["ICLR.cc/2026/Conference/Submission16523/Reviewer_V8AP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16523/Reviewer_V8AP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997106865, "cdate": 1761997106865, "tmdate": 1762926610950, "mdate": 1762926610950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a continual in-context RL framework where the enrivonments continously change. The authors propose a new benchmark suite featuring two complementary non-stationary domains and benchmark sequence models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. clear formalization of CICRL and why standard ICRL (stationary rules) is insufficient\n2. two complementary, well-motivated environments with continous changes. They are general and reproducible\n3. the analysis and insights are interesting."}, "weaknesses": {"value": "1. This paper proposed a benchmark and conducted baseline evaluations with an analysis of the results. There is no novel algorithm development, which limits the novelty and contribution of this work. \n2. The topic itself seems to be in alignment with continual learning and continual RL, or lifelong RL. But I don't see what the difference is between this work and other continuously changing environments proposed in continual RL environments. It seems to me that, in context, learning is simply learning history-dependent policies rather than Markovian ones. \n3. The experiments are largely simplistic. I am not sure how significant the results are, given the simplicity of these problems. \n4. In RL, I think domain randomization is the primary technique for training robust policies. I think the authors miss a large portion of the literature, especially on the aspects of robot learning, which is one of the largest fields of RL applications."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0BBNLbYhtF", "forum": "h8OJb8YGNa", "replyto": "h8OJb8YGNa", "signatures": ["ICLR.cc/2026/Conference/Submission16523/Reviewer_xGSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16523/Reviewer_xGSG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997345339, "cdate": 1761997345339, "tmdate": 1762926610433, "mdate": 1762926610433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}