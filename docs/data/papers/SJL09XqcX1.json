{"id": "SJL09XqcX1", "number": 24339, "cdate": 1758355933068, "mdate": 1759896770692, "content": {"title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs", "abstract": "This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, $L^2$ multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of $L^2$ is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, $L^2$ is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The $L^2$ method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.", "tldr": "", "keywords": ["Test‑time Scaling; Token/Compute Efficiency; Data‑Efficient Reasoning; Low‑Resource Multilingual NLP; Small‑Data Fine‑Tuning;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17f67a4e2c8770e24816ed87100356038a873680.pdf", "supplementary_material": "/attachment/73815745b43035e29a68cbb7f407e5774d35f6b2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper investigates how multilingual learning can improve long-form reasoning in large language models with less data and fewer inference tokens. The authors discover that different languages yield distinct reasoning patterns, affecting both accuracy and token efficiency. Based on this insight, they propose a multilingual unification learning framework that augments a small amount of high-quality CoT data across languages and applies language-aware decoding. Experiments show that the proposed framwork significantly boosts reasoning performance while reducing test-time compute, offering a more efficient pathway to strong general reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The results are strong, showing that the proposed approach can significantly enhance long-form reasoning with very limited data.\n\n2. Leveraging multilingual diversity to improve test-time efficiency is a sensible idea, supported by clear empirical evidence.\n\n3. The L2 framework is orthogonal to existing data-efficient methods and provides a promising direction for reducing inference token usage without sacrificing performance."}, "weaknesses": {"value": "1.The evaluation is somewhat limited in scope. Including results on multilingual benchmarks such as MGSM and PolyMath would further strengthen the validation of the proposed approach. \n\n2.The naming and organization of datasets in the paper can lead to confusion when reading the methodology and experimental setups.\n\n3.Considering practical applications, users generally expect the model’s reasoning and responses to be in a single preferred language. While multilingual mixing may improve efficiency, it could be less practical or even undesirable in real-world usage scenarios where language consistency is essential."}, "questions": {"value": "1.How does the model perform when the test data are presented in different languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gI9MVwPPKn", "forum": "SJL09XqcX1", "replyto": "SJL09XqcX1", "signatures": ["ICLR.cc/2026/Conference/Submission24339/Reviewer_TAHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24339/Reviewer_TAHy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761360207766, "cdate": 1761360207766, "tmdate": 1762943048754, "mdate": 1762943048754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explored using multilingual long CoT data, combining entire long chain-of-thought annotations in different languages and the step-wise mixture of languages can boost both performance and efficiency. It shows small amounts of data can improve reasoning capabilities.  Furthermore, it propose language-based logit interventions during inference to switch language."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Multilingual thinking as a resource for efficient reasoning is compelling. Using different language to make reasoning more efficient is a novel way to explore. The idea of different languages induce distinct reasoning compression patterns is interesting."}, "weaknesses": {"value": "1. There are some previous works already discussed using multilingual data to boost reasoning performance [1], maybe you should consider to compare your method with theirs and tell more differences. \n\n2. Only using tokens number as the metric of measuring efficiency, need also consider using other metrics like the time of inference, FLOP cost, and memory usages.\n\n3. When using tokens in other languages, the factor of compression rate of the tokenizers in that language should also be considered, which wasn't discussed in the paper.\n\n3. Some of experiments are use model training on extremely small multilingual sets, I doubt whether this will make model only memorize input templates. \n\n[1] Could Thinking Multilingually Empower LLM Reasoning?"}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mbFjvg9HNU", "forum": "SJL09XqcX1", "replyto": "SJL09XqcX1", "signatures": ["ICLR.cc/2026/Conference/Submission24339/Reviewer_2NfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24339/Reviewer_2NfV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894770878, "cdate": 1761894770878, "tmdate": 1762943048531, "mdate": 1762943048531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel technique for data augmentation for LLM finetuning using multiple languages for generation.\n\nOverall, I like the idea, but I have some comments provided below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The novel data augmentation method."}, "weaknesses": {"value": "The authors compare their method to some unnamed but presumably simplistic technique for data augmentation, I think that it is better to compare to several existing techniques, e.g. starting with classic backtranslation. I suppose that the results could be explained by data augmentation itself, not the language diversity."}, "questions": {"value": "Please give more details with examples on your baseline data augmentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8dtety8Kat", "forum": "SJL09XqcX1", "replyto": "SJL09XqcX1", "signatures": ["ICLR.cc/2026/Conference/Submission24339/Reviewer_AVEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24339/Reviewer_AVEt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929249046, "cdate": 1761929249046, "tmdate": 1762943048237, "mdate": 1762943048237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a new method for improving the reasoning efficiency of LLMs at test time via multilingual data. Unlike prior work which focused on single-language CoT annotations and large amounts of fine-tuning data, the authors propose L_2 a multilingual unification learning approach that augments a small number of high-quality CoT examples into multiple languages and mixes language-step reasoning to exploit diverse reasoning patterns across languages. They design experiments by fine-tuning a base model (e.g. Qwen2.5-32B) on only a handful 6 up to ~1000 multilingual annotated samples, evaluating on datasets like AIME24, GPQA-Diamond and MATH500. Experiments show that multilingual augmentation significantly improves reasoning accuracy and also reduces inference token usage compared to monolingual baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of leveraging multilingual reasoning diversity (rather than simply more data) to increase reasoning efficiency is novel and sound.\n2. The authors showed strong empirical gains even with extremely small annotated sample sizes when augmented via multilingual CoT.\n3. Addresses both data efficiency and inference efficiency which is increasingly important in practice for LLM deployment."}, "weaknesses": {"value": "1. The experiments use relatively small and controlled benchmark sizes e.g., AIME24 with only 30 problems. It's not very clear how much it would scale to broader diverse reasoning tasks.\n2. While the reduction in inference tokens is claimed, detailed breakdowns of token savings vs accuracy trade-offs (e.g. across languages and varying lengths) are not sufficiently discussed."}, "questions": {"value": "1. How sensitive is the method to the quality of the multilingual translations or CoT annotations in non-audited languages?\n2. Does the multilingual mixing approach generalize to non-math reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QEvusP8lhI", "forum": "SJL09XqcX1", "replyto": "SJL09XqcX1", "signatures": ["ICLR.cc/2026/Conference/Submission24339/Reviewer_Gegq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24339/Reviewer_Gegq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242069343, "cdate": 1762242069343, "tmdate": 1762943047864, "mdate": 1762943047864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}