{"id": "fFgzAQAUqs", "number": 5079, "cdate": 1757844915815, "mdate": 1759897995923, "content": {"title": "SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis", "abstract": "Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding find-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. \nWe introduce **SLIM-Brain** (**S**ample-efficient, **L**ow-memory fMR**I** Foundation **M**odel for Human **Brain**), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-k selected windows, while discarding about 70% non-brain background.\nExtensive experiments across four public benchmarks show that SLIM-Brain establishes new state-of-the-art performance, improving accuracy by up to 10 percentage points on diverse tasks, while requiring only ~3% of the pre-training data   (1k vs. 32k subjects)  and approximately 30% of GPU memory comparing to existing voxel-level methods. Code and trained weights of SLIM-Brain are available at [https://anonymous.4open.science/r/SLIM-Brain-9C51](https://anonymous.4open.science/r/SLIM-Brain-9C51).", "tldr": "With ~3% of pretraining data, we build a voxel-level fMRI foundation model that sets new state-of-the-art results.", "keywords": ["fMRI", "foundation model", "Hiera", "JEPA", "Brain"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98d151ba25f34f7b0fe4457b2ea514acd2ac37a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SLIM-Brain, an atlas-free foundation model for fMRI analysis that combines three components: (1) a global MAE branch for coarse temporal features, (2) a learnable top-k selector that identifies informative time windows, and (3) a 4D Hiera-JEPA encoder for fine-grained voxel-level features. The method achieves competitive performance on several benchmarks while reducing GPU memory usage to ~30% of Swin-based approaches. Integrated-gradient analysis shows alignment with meta-analytic patterns from NeuroSynth, suggesting the model captures biologically meaningful features. However, the core claim of \"data-efficiency\" lacks validation, as experiments only use ~1k subjects while baselines scale to 32k-65k subjects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. **Novel architectural design**: The combination of global MAE, learnable top-k selection, and 4D Hiera-JEPA is an interesting approach to balance computational efficiency and representation quality.\n2. **Significant computational gains**: Achieves -70% reduction in GPU memory (8GB → 2.3GB, Table 3) and dramatically faster training (~1 hour vs. 150 hours for baselines).\n3. **Strong performance on some tasks**: Particularly impressive on fingerprint identification (98.5%, Table 1) and competitive on sex classification (91.1%).\n4. **Biological interpretability**: Integrated-gradient analysis (Figure 4) shows meaningful alignment with NeuroSynth meta-analytic maps for ADHD, suggesting the model captures neurobiologically relevant patterns.\n5. **Reproducibility**: Detailed implementation in appendix, code release promised, and extensive experimental settings provided."}, "weaknesses": {"value": "## **Major Issues**\n\n**W1. Core claim unvalidated [Critical]**\n\nThe \"data-efficient\" claim in the title lacks empirical support. All experiments use only ~1k subjects, while baselines demonstrate scaling curves up to 32k-65k subjects (Figure 1). Given that the model is \"training-efficient\" (low memory, fast training), scaling experiments should be straightforward. The absence raises critical questions: (1) Does performance saturate quickly (supporting data-efficiency)? (2) Does it improve with more data (contradicting the claim)? (3) Is the 1k result cherry-picked? The discussion acknowledges this as \"future work\" (lines 477-479), which directly contradicts the paper's central thesis.\n\n**W2. Insufficient justification for architectural choices**\n\nTable 3 shows task-dependent performance for Hiera-MAE vs. Hiera-JEPA:\n\n- Sex: 91.3% vs. 91.1% (MAE better)\n- Fingerprint: 90.0% vs. **98.5%** (JEPA much better)\n- Age: 52.6% vs. 50.2% (MAE better)\n\nJEPA substantially outperforms only on fingerprint. The rationale for selecting JEPA as the default architecture is unclear. Why not use MAE, or a task-adaptive approach?\n\n**W3. Weak improvement from key contribution**\n\nTable 2 shows top-k selection improves over random selection by only 3.2 percentage points (84.5% → 87.7% at 40 frames). Without statistical significance tests (e.g., 95% confidence intervals across multiple runs), it is unclear whether this difference is meaningful or within noise margins.\n\n## **Clarity Issues**\n\n**W4. Misleading terminology**\n\n- **\"Multi-scale\"** (Figure 2): Combining global and local features is better described as multi-granularity, not spatial multi-scale. Hiera's hierarchical architecture already provides multi-scale features; this conflates the contribution of top-k selection.\n- **\"Atlas-free\"**: While avoiding ROI parcellation, the method relies on a template-based brain mask (line 219) to remove \"~70% non-brain background.\" This mask is pre-defined, not learned—similar to atlas-based methods' fixed templates. The method is atlas-free for *functional* parcellation but template-based for *anatomical* masking. Clarification needed: (1) Which template? (2) Is it dataset-specific? (3) How is b=716 blocks determined? (4) How is 70% calculated?\n\n**W5. Figure inconsistencies**\n\n- **Figure 1**: Illegible and confusing. Why are some models shown as lines and others as points? The task (HCP sex classification) is not immediately clear. Requires 90° head rotation to read axis labels.\n- **Figure 2-c**: Shows \"x1\" and \"1 key window\" but text states k=8 windows (Appendix A.3). How are 8 windows processed—sequentially or batched? How are features aggregated (only mentioned as \"pooled\" in line 297)? Figure 3 shows multiple windows, which contradicts Figure 2-c.\n\n**W6. Important details relegated to appendix**\n\n- Total loss equation (Ltotal=LJEPA+λLMAE) only in Appendix A.6 (line 719). What is λ?\n    \n    Ltotal=LJEPA+λLMAEL_{total} = L_{JEPA} + \\lambda L_{MAE}\n    \n    λ\\lambda\n    \n- Hyperparameter k=8 only in Appendix A.3 (line 667). No ablation on k values.\n\n## **Minor Issues**\n\n**W7. Table labeling**\n\nTable 4: Which row is SLIM-Brain? \"Ours\" marker needed for clarity.\n\n**W8. Missing ablations**\n\n- No k-value ablation (why k=8 optimal?)\n- No comparison of different pooling strategies for aggregating k windows\n- No ablation on λ weight for auxiliary MAE loss"}, "questions": {"value": "**Q1.** What is the value of λ\\lambda\nλ in Ltotal=LJEPA+λLMAEL_{total} = L_{JEPA} + \\lambda L_{MAE}\nLtotal=LJEPA+λLMAE? How sensitive is performance to this choice?\n\n**Q2.** Table 2: Are the performance differences statistically significant? Please provide confidence intervals or p-values across multiple random seeds.\n\n**Q3.** How do you determine k=8? What happens with k=4, 16, 32? Is there a task-dependent optimal k?\n\n**Q4.** Figure 2-c: Please clarify the exact processing pipeline for k=8 windows. Are they processed in a single batch or sequentially? What pooling operation aggregates them?\n\n**Q5.** What brain mask template is used (line 219)? Is it consistent across HCP, ABIDE, ADHD-200, etc.? How does mask quality affect performance?\n\n**Q6.** For \"data-efficiency\": Can you provide at least one scaling point (e.g., 5k or 10k subjects) to demonstrate saturation behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "06V1S4PPTV", "forum": "fFgzAQAUqs", "replyto": "fFgzAQAUqs", "signatures": ["ICLR.cc/2026/Conference/Submission5079/Reviewer_7bTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5079/Reviewer_7bTm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760869829024, "cdate": 1760869829024, "tmdate": 1762917859844, "mdate": 1762917859844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SLIM-Brain, an atlas-free fMRI foundation model that selects top-k salient frames and prunes non-brain voxels before training a 4D Hiera-JEPA encoder. With only 1k pre-training subjects, it achieves competitive performance on different tasks and datasets, while using less compute and memory in the pre-training stage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors conduct ablation studies to justify their design choices. The top-k frame selection idea seems especially innovative and surprisingly effective.\n- Innovations such as removing the background and only keeping the brain region also seem to be a meaningful point that other researchers may have overlooked in the past.\n- The experiments show that SLIM-Brain shows strong performance. As listed below, I have issues with the HCP experiments, but for the other datasets, the proposed model seems to perform better or at least comparable to the baselines."}, "weaknesses": {"value": "1. **Emphasis on pre-training data burden.** \nI agree with the authors' point that the need for a large dataset is a huge burden. However, in terms of practical utility, I believe the size of the fine-tuning dataset is a much bigger issue than the size of the pre-training dataset. As long as there is someone that is willing to train and release the model, the user does not need to worry about the scale of the pre-training dataset. The authors demonstrate this very point, as they were able to download and deploy (32k~64k) pre-trained baselines without the need to access the pre-trained datasets. \nHowever, the fine-tuning dataset requirement can't be solved that easily, as the user needs to acquire sufficient amount of data themselves. Relying on other public datasets is not an ideal option, as cross-dataset generalization is still a tough problem in this field.\n\n2. **Unfair advantage on HCP experiments.**\nSLIM-Brain seems to have an unfair advantage over the baselines on the HCP sex and fingerprint experiments, as it is heavily pretrained using HCP data whereas the other baselines are not. This might be significant considering that cross-dataset variability is very large in this domain, due to the difference in the subject demographic, imaging apparatus, preprocessing pipeline, etc.\n\n3. **I/O burden.**\nDirect 4D models such as SwiFT and SLIM-Brain are inherently plagued by the I/O burden of the input data. In Section 4.2, the authors claim their model trains under 1 hour but with a 20-hour I/O burden. No matter how fast the actual computations are, the 20-hour overhead is a real cost that should be added to the training time for practical applications. This is despite the fact that the compared baselines are trained with 32k~64k subjects, while SLIM-Brain is only trained on 1k subjects. This will be a huge issue if one wishes to scale SLIM-Brain to the size of said baselines, as it would mean it would take 600+ hours for 10 epochs of training on 32k subjects."}, "questions": {"value": "- The 1k-trained Brain-JEPA seems to perform no better than random chance. Is there any reason why training for this model completely failed?\n- What are the evaluation results for SwiFT, as it seems to be included in Figure 1?\n- Please disclose the fine-tuning details for the baselines: number of training epochs, learning rate, how hyperparameters are searched, etc.\n- Also, please disclose the details for the fine-tuning dataset: number of subjects, subject demographics, etc.\n- There seems to be a large discrepancy between the previously reported performance of baseline models and the reported performance in this paper. For example, SwiFT in the original paper is reported to have a 92.9% accuracy in the HCP sex classification task, but in this paper, it is reported to have a ~74% accuracy. What might be the reason for this huge gap?\n- Why is ABIDE age used as a classification task instead of directly predicting the age and treating it as a regression task, just like the Brain-JEPA paper?\n- Am I correct in understanding that the top-k frame selector operates in an out-of-distribution setting, so it is frozen after the pre-training step and not fine-tuned after that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "22VnGGQde7", "forum": "fFgzAQAUqs", "replyto": "fFgzAQAUqs", "signatures": ["ICLR.cc/2026/Conference/Submission5079/Reviewer_RrvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5079/Reviewer_RrvS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209188172, "cdate": 1761209188172, "tmdate": 1762917859372, "mdate": 1762917859372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents self-supervised training on RS-fMRI for down stream tasks, predicting attributes/diagnostics and identification of subjects.\nThe self-supervision objective is predicting masked fMRI patches.\nThe model is more resource efficient by sub-sampling the temporal dimensions, by a scoring mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Competitive results compared to shown benchmarks.\n- Resource optimization by temporal sampling.\n- Efficiency regarding pretraining corpus size."}, "weaknesses": {"value": "- I believe the model is not compared to key previous works, specifically Swift (Kim et al., 2023) and the model by Malkiel et al. (2022), and I believe it underperforms them. If this is not the case and the model is shown to be competitive, I will revisit my rating.\n\n- The work proposes a \"Foundation model\" and in the same time claims to be data efficient, to me the two seem at odd with each other. Either be a foundation model trained on vast amount of data, or a data efficient approach.\n\nRefernces:\n- Peter Kim, Junbeom Kwon, Sunghwan Joo, Sangyoon Bae, Donggyu Lee, Yoonho Jung, Shinjae Yoo, Jiook Cha, and Taesup Moon. Swift: Swin 4d fmri transformer. Advances in Neural Information Processing Systems, 36:42015–42037, 2023\n- Itzik Malkiel, Gony Rosenman, Lior Wolf, and Talma Hendler. Self-supervised transformers for fmri representation. In International Conference on Medical Imaging with Deep Learning, pp. 895–913, 2022."}, "questions": {"value": "- Is the window scoring crucial, can't I sample randomly during training and take all the windows during test/take an ensembles of random temporal samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1x4ZVOpdZo", "forum": "fFgzAQAUqs", "replyto": "fFgzAQAUqs", "signatures": ["ICLR.cc/2026/Conference/Submission5079/Reviewer_BJPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5079/Reviewer_BJPh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827994340, "cdate": 1761827994340, "tmdate": 1762917859076, "mdate": 1762917859076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SLIM-Brain proposes an atlas-free foundation model for fMRI analysis that addresses data and training efficiency challenges. The method uses a two-stage pipeline: (1) a lightweight 2D ViT processes spatially downsampled full-length sequences via masked autoencoding to extract global features and rank temporal windows by \"mutual masked reconstruction\" saliency, and (2) a 4D Hiera-JEPA encoder processes only the top-k selected windows (discarding ~70% background voxels) to extract fine-grained voxel-level representations. The authors claim SOTA performance on sex classification, fingerprinting, and age tasks while using only ~3% of pretraining data (1k vs 32k subjects) and ~30% GPU memory compared to previous methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated problem:** The paper clearly describes the dual bottleneck of data efficiency (atlas-based methods need massive cohorts) and training efficiency (atlas-free methods are computationally prohibitive).\n2. **Memory efficiency gains:** Demonstrating ~70% reduction in GPU memory (8GB→2.3GB per sample) by excluding background voxels and using Hiera's unit-wise masking is a practical contribution, especially for democratizing research access.\n3. **Neurobiological validation:** Comparison with Neurosynth meta-analytic maps provides interpretability evidence that learned features align with known ADHD-related circuits."}, "weaknesses": {"value": "1. **Problems with Experimental Validation:** The authors state they retrain baselines \"on the same total number of sessions\" and report \"best validation checkpoints.\" But a lot of information is missing from this statement.\n- Did you perform hyperparameter search when retraining the baselines?\n- Missing training dynamics to show convergence plots. Brain-JEPA drops from 87.1% (32k) to 54% (1k) and fingerprint goes to 1.0% accuracy (essentially random/collapsed), this suggests the model is collapsed rather than trained. Even on the other datasets, predication is comparable to random guessing. If the authors selected \"best validation checkpoints,\" how did validation metrics look during training? Were these models training at all, or did they plateau immediately?\n- When you say \"best validation checkpoint,\" what was the validation set size? For a 100-subject fingerprint task with 60/20/20 splits, validation is only 20 subjects. Did early stopping on such small validation lead to overfitting to the validation set?\n- The words samples, subjects and sessions are used interchangeably throughout the paper making it hard to understand if they are aligning the data based on subjects or sessions. If you are comparing the HCP dataset to the 32-64k subjects in the previous works' datasets, is it a fair comparison to say 1-3% when the number of sessions per subject might be different?\n\n\n2. **Computational Cost:** For M=40 windows, do you run 40 forward passes (mask all but window m, reconstruct the rest)? If yes then this seems expensive. How does this compare to just training the full model?\nWhat happens if you train the Top-k selector jointly rather than freezing the MAE? Does this hurt or help?\n\n\n3. **OOD evaluation is weak:** Linear probing on ADHD/ABCD shows modest gaps (59.7% vs BrainMass 61% on ADHD; 69.7% vs 70% Hiera-MAE on ABCD sex). These are not convincing wins. You are missing evaluations on task state decoding (21-way HCP cognitive tasks), cognitive phenotype regression, or fMRI-to-image retrieval—all standard benchmarks. You can reference NeuroSTORM [1] which evaluated 5 categories across 10+ datasets. \n\n4. **Improvements in ablations:** You cover top-k vs random but I would like to see comparison against uniform sampling (every k window) or high temporal variance windows too\n\n5. **Missing related work for baseline evaluation:** NeuroSTORM already uses Shifted-Window Mamba + spatiotemporal masking for 4D fMRI.  Can you comment on how your work is different and why it is missing from your evaluations? Same for BrainGFM [2]\n\n6. **Missing significance tests:**  All results are single numbers. Given that fMRI is noisy and baselines show high variance, how many runs did you average?\n\n[1] Towards a general-purpose foundation model for fMRI analysis. Cheng Wang et al.\n[2] A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder. Xinxu Wei, Kanhao Zhao, Yong Jiao, Lifang He, Yu Zhang"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zVqctStdmy", "forum": "fFgzAQAUqs", "replyto": "fFgzAQAUqs", "signatures": ["ICLR.cc/2026/Conference/Submission5079/Reviewer_yBy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5079/Reviewer_yBy5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945386253, "cdate": 1761945386253, "tmdate": 1762917858704, "mdate": 1762917858704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}