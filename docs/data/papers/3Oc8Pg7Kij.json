{"id": "3Oc8Pg7Kij", "number": 6749, "cdate": 1757994473635, "mdate": 1759897896639, "content": {"title": "HT-Sparse: Training-Free Query-Guided Head–Token Sparsification for Long-Video Multimodal Inference", "abstract": "Long-video multimodal inference is limited by the quadratic cost of dense attention, cumulative KV-cache growth during decoding, and cross-modal interference, while retraining sparsity-aware variants is often impractical. We present HT-Sparse, a training-free, query-guided hierarchical sparsification that performs joint head–token computation to reduce both latency and memory without parameter updates. The method comprises two components executed adaptively across layers: (i) query-conditioned head sparsification, which ranks attention heads via analytically stable saliency statistics to retain the most informative subspaces for the current query; (ii) cross-modal token sparsification, which selects salient visual tokens by query–vision attention, enabling efficient computation and persistent KV-cache savings. We further introduce joint head–token routing in selected layers: top-ranked heads attend to the full visual token set, whereas secondary heads operate on the reduced (selected) set, preserving semantics while amortizing compute and cache. Across long-video benchmarks, HT-Sparse delivers faster inference with reduced end-to-end latency and lower KV-cache memory, while achieving equal or higher accuracy, all on the same pretrained model with no fine-tuning. The approach is model-agnostic and plug-in deployable, offering a flexible route to scalable long-video reasoning.", "tldr": "HT-Sparse is a training-free, query-guided method that accelerates long-video multimodal inference with efficient head–token sparsification.", "keywords": ["Long-video multimodal inference", "Training-free", "query-guided", "sparse attention", "head–token sparsification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edf67482cebcc8e664f2fd3b225fba6a012052c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free, query-guided hierarchical sparsification method, reduces latency and memory via joint head–token computation without parameter updates. It has two adaptive components: query-conditioned head sparsification (ranks heads to retain key subspaces) and cross-modal token sparsification (selects salient visual tokens for KV-cache savings). It also adds joint head–token routing in some layers (top heads use full tokens, secondary heads use reduced tokens). Tested on long-video benchmarks, HT-Sparse boosts inference speed, cuts end-to-end latency and KV-cache memory, maintains or improves accuracy—all on pretrained models without fine-tuning. It is model-agnostic, plug-and-play, and enables scalable long-video reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores how to enhance the efficiency of long-video understanding through cross-modal attention sparsification in a training-free manner, while achieving a certain degree of performance improvement compared to the baseline.\n\n2. This paper addresses attention sparsification from both the head-level and token-level perspectives, and verifies the effectiveness of the combination of these two approaches."}, "weaknesses": {"value": "1. The elaboration on the core contributions and innovations of this paper is insufficient. Most of the contributions mentioned in this paper, such as training-free and cross-modality, have been addressed in numerous previous works [1][2][3]. Compared with baseline methods (token-only or head-only), the improvement presented is relatively trivial. It is recommended that the authors supplement the discussion with previous works to strengthen the contributions that are exclusive to this study.\n\n2. The writing of the method section in this paper seems overly concise, making it difficult to read and lacking detailed descriptions of method details.\n\n3. The ablation study section of this paper is not sufficiently comprehensive and solid. For instance, it lacks the analysis of HT-Sparse's performance variation under different values of k. Additionally, there is no investigation into the method's performance impact on image tasks. From the reviewer's perspective, this method does not include specialized designs tailored for video tasks; thus, merely evaluating it on a few video benchmarks is insufficient.\n\n[1] AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding\n\n[2] DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models\n\n[3] FastVID: Dynamic Density Pruning for Fast Video Large Language Models"}, "questions": {"value": "My primary concern lies with the method’s core contributions and the scope of its experimental work; I believe both aspects fail to meet the acceptance criteria of the conference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lzHRSFaAWW", "forum": "3Oc8Pg7Kij", "replyto": "3Oc8Pg7Kij", "signatures": ["ICLR.cc/2026/Conference/Submission6749/Reviewer_egSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6749/Reviewer_egSG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760794371078, "cdate": 1760794371078, "tmdate": 1762919032956, "mdate": 1762919032956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HT-Sparse, a training-free method to tackle challenges in long-video multimodal inference, such as high attention costs, KV-cache growth, and cross-modal interference. HT-Sparse combines query-conditioned head sparsification, which selects the most informative attention heads, and cross-modal token sparsification, which reduces visual tokens based on query relevance. It also introduces joint head–token routing to balance computational efficiency and semantic preservation. Tested on long-video benchmarks, HT-Sparse achieves faster inference, lower memory usage, and comparable or better accuracy, all without retraining or fine-tuning, making it a scalable and plug-and-play solution for multimodal reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with the methodology and experimental results clearly and systematically presented.\n2. The research topic is highly practical, as developing a training-free method that effectively reduces inference memory usage and latency offers significant benefits for real-world deployment of models.\n3. Without requiring model retraining, HT-Sparse achieves impressive performance on several long-video benchmarks, maintaining accuracy comparable to the original model while significantly reducing memory usage and latency."}, "weaknesses": {"value": "1. The paper primarily focuses on explaining the benefits of its design choices but lacks a comprehensive comparison with prior works. In fact, the field of training-free inference pruning for multimodal large models has seen significant research. The paper does not sufficiently clarify its relationship to these existing methods or analyze its advantages over them, both from a theoretical perspective and through experimental comparisons. A thorough discussion of related work and a direct empirical comparison with prior methods is needed.\n2. The experimental evaluation is incomplete. While HT-Sparse is proposed for long-video understanding, all the benchmarks used in the paper focus on QA tasks. It is well-known that token sparsification tends to have less impact on QA tasks. However, for tasks requiring attention to fine-grained visual details, such as video captioning or video OCR, token sparsification may introduce significant drawbacks. Additional benchmarks on such tasks would provide a more comprehensive understanding of the method's performance across different scenarios.\n\nIf the authors can address my concerns, I would consider increasing the score."}, "questions": {"value": "The experiments in this paper are all based on a 7B model. Have the authors explored the performance of their method on larger models? Additionally, would the optimal pruning strategy differ for larger or smaller models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jOA2keJht7", "forum": "3Oc8Pg7Kij", "replyto": "3Oc8Pg7Kij", "signatures": ["ICLR.cc/2026/Conference/Submission6749/Reviewer_9LLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6749/Reviewer_9LLf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206013110, "cdate": 1761206013110, "tmdate": 1762919032571, "mdate": 1762919032571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies three major challenges in long-video multimodal inference: the secondary computational cost of dense attention, the cumulative growth of key-value cache during decoding, and cross-modal interference. Retraining or fine-tuning sparse-aware model variants is often impractical due to data governance, time-to-market delays, and distribution shift risks.\n\nTo address these challenges, this work propose HT-Sparse, a training-free, query-guided hierarchical sparsity reduction method. The proposed method is validated on four long-video benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is accurately and significantly identified: it clearly points out the actual pain points of multimodal inference in long videos (computation, memory, cross-modal interference), emphasizes the infeasibility of retraining the model in a production environment, and highlights the urgent need for optimization during inference.\n2. The work clearly points out the limitations of the method and emphasizes its plug-and-play nature and easy integration with existing inference stacks, which is very attractive to the industry\n3. Training free: It is done entirely at inference time and does not depend on parameter updates, making it highly deployable and model-independent."}, "weaknesses": {"value": "1. While the proposed head ranking in the paper are effective, their design is more like a heuristic. Further exploration or argumentation could be made as to why this particular metric (based on the dot product of query summary and key projection) can effectively and stably reflect the importance of the head.\n2. Experiments primarily focused on models with 7 B parameters. Effectiveness on larger models (e.g., tens or hundreds of B) remains to be verified, as the attention head behavior may differ across models of different sizes.\n3. The evaluation tasks primarily focus on question answering and localization. It can be extended to more complex tasks, such as long video summarization, dense captioning, or tasks requiring stronger temporal reasoning, to test the method's generalizability.\n4. While comparisons with baseline models and their own variants are sufficient, further comparisons with other state-of-the-art, training-free inference optimization methods could better highlight HT-Sparse's advantages over similar technologies."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pKT8VcDMeU", "forum": "3Oc8Pg7Kij", "replyto": "3Oc8Pg7Kij", "signatures": ["ICLR.cc/2026/Conference/Submission6749/Reviewer_KCxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6749/Reviewer_KCxw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895167085, "cdate": 1761895167085, "tmdate": 1762919031943, "mdate": 1762919031943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HT-Sparse, a training-free, query-guided hierarchical sparsification framework designed to enhance efficiency during inference for long-video vision-language models. The method jointly applies attention head and visual token sparsification, guided by input queries and applied adaptively across layers, without requiring model retraining or parameter updates. An optional in-attention low-rank projection is also described for extreme resource constraints. Empirical results on four established long-video benchmarks (VideoMME, MLVU, LongVB, LVBench) show that HT-Sparse, instantiated on two competitive base models, can reduce end-to-end latency and memory (especially in the KV-cache) while maintaining or marginally improving task accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is plug-and-play, does not require any fine-tuning, parameter updates, or access to training data, making it immediately deployable in production settings.\n\n2. Unlike prior approaches that fix sparsity patterns or prune only heads or tokens, HT-Sparse introduces joint head-token routing, selecting both the most relevant heads and visual tokens in a hierarchical, per-query, and per-layer adaptive manner.\n\n3. Experimental results support substantial GPU memory and FLOPs reductions relative to dense attention, with KV-cache usage during decoding cut by over 50%, and end-to-end latency dropping to about 68–69% of baseline, all while maintaining or slightly improving accuracy on multiple benchmarks."}, "weaknesses": {"value": "1. While the joint application of query-guided head and token sparsification in a training-free context is well integrated, the individual mechanisms (head importance scoring, token selection via query-key attention) have significant overlap with prior dynamic head pruning and token selection works.\n\n2.  While a variety of controlled baselines are provided (Dense, Head-only, Token-only), the absence of quantitative or qualitative comparison to the aforementioned directly relevant methods specifically designed for video-language inference efficiency is a critical omission. There are also no results on larger models, highly diverse tasks (e.g., retrieval, captioning), or practical deployment scenarios beyond 7B-class VLMs. This makes it difficult to assess the generality/extensibility of the method’s strengths.\n\n3. Although it is argued that joint routing (where at least one top head attends to all tokens) \"prevents semantic loss\", empirical evidence for this assertion is limited. There are no in-depth analyses of qualitative failure cases or edge scenarios. For instance, what happens in heavily compositional or cross-modal distractor-laden sequences? Quantitative ablation in Tables 1 and 2 supports accuracy maintenance, but a more fine-grained or error-driven analysis is lacking.\n\n4. While Figure 2 nicely visualizes structured sparsity, a more contextual explanation of what the head score heatmap and attention distribution represent (and how this justifies specific routing decisions) would help. Figure 3 offers a clear schematic of the token selection process, but additional step-by-step computational examples or a visualization of query-induced mask dynamics would strengthen clarity."}, "questions": {"value": "As shown in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sb4pFpnTo8", "forum": "3Oc8Pg7Kij", "replyto": "3Oc8Pg7Kij", "signatures": ["ICLR.cc/2026/Conference/Submission6749/Reviewer_6arx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6749/Reviewer_6arx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162692901, "cdate": 1762162692901, "tmdate": 1762919031523, "mdate": 1762919031523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}