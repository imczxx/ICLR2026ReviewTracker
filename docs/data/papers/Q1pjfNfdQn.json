{"id": "Q1pjfNfdQn", "number": 9232, "cdate": 1758115858133, "mdate": 1759897736309, "content": {"title": "Balancing Precision and Richness in Image Caption Services for Enhanced Descriptive Accuracy", "abstract": "Current image captioning services often learn to generate captions by imitating ground truth references, which are constrained by the limitations of manual annotations. This leads to overlooked details in images, causing captions to lack richness and precise descriptions, critical for enhanced image captioning services. To address this, we propose a CLIP-based image captioning framework designed to balance descriptive precision and richness enhancement. Our approach uses fine-grained pseudo tags for learning and integrates an asymmetric attention multi-modal projector to map and fuse information across modalities effectively. We also introduce an evaluation metric, Tags Coverage, to measure the granularity of generated captions and incorporate it into reinforcement learning to optimize the reward function. This eliminates the need for additional text annotations while addressing unannotated details. Experimental results on the MS-COCO Karpathy's test set demonstrate the model’s effectiveness, with improvement in CIDEr and Tags Coverage compared to state-of-the-art baselines, highlighting its potential for advancing precision and richness in image captioning services.", "tldr": "", "keywords": ["Image Understandin", "Image Captioning", "Descriptive Precision", "Richness Enhancement"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1646f7a9f36f031a1fc01470d3ce82b6d6d84fb6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an image captioning framework that augments a standard captioner with CLIP-derived pseudo-tags at multiple spatial granularities, fuses object/grid/tag features using an “asymmetric attention” multi-modal projector, and introduces a new evaluation signal, Tags Coverage (TC), which measures the fraction of words in a generated caption that match the top-K CLIP-retrieved tags for that image. TC is combined with CIDEr as a reinforcement learning reward (via NSC) to encourage captions that are both “precise” and “rich.”"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear articulation of the target trade-off (precision vs. richness) and a simple mechanism (tag retrieval + auxiliary reward) to bias generation toward more detailed captions.\n2. Technically straightforward, reproducible ingredients (frozen CLIP encoders, M2Transformer backbone, NSC fine-tuning) that practitioners already understand."}, "weaknesses": {"value": "1. Using CLIP to retrieve/score textual tokens for captioning is well-trodden (e.g., retrieval-based guidance and CLIP-based evaluation). TC is effectively a token-overlap proxy for CLIP alignment; it’s not evident how it fundamentally differs from existing CLIP-alignment or keyword-precision objectives. The paper doesn’t clearly position TC against related CLIP metrics nor analyze failure modes beyond a brief acknowledgement.\n2. TC counts token overlap with a preselected tag list (top-100) and is then used directly as a reward. This can incentivize enumerating tags rather than improving sentence semantics, locality, or factual grounding. The authors themselves note that higher TC can hurt CIDEr and that TC can include misidentified tags; yet there’s no human study or qualitative error analysis to show TC actually correlates with better user-perceived descriptions.\n3. All results are on MS-COCO (Karpathy split) with automatic metrics; there’s no evaluation on other caption datasets (e.g., Flickr30k, nocaps, TextCaps) or any human preference study/user-centric evaluation that would be especially relevant given the “service” framing. Claims of better “service-oriented” captions thus lack empirical support."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1nh8xfTl62", "forum": "Q1pjfNfdQn", "replyto": "Q1pjfNfdQn", "signatures": ["ICLR.cc/2026/Conference/Submission9232/Reviewer_R7ub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9232/Reviewer_R7ub"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790263562, "cdate": 1761790263562, "tmdate": 1762920889276, "mdate": 1762920889276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework to address the lack of richness in image captions, which it attributes to models imitating concise human annotations. It proposes using pseudo-tags generated by a pre-trained CLIP model as a target for richness. These tags are fused with object and grid features using an \"asymmetric attention\" (using conv, fully connected, sampling) projector. The authors also introduce a new metric, \"Tags Coverage\", to measure the inclusion of these tags and use it in a reinforcement learning reward function to train the captioning model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper trys to explore a novel aspects about the \"richness\" in image captioning.\n- The core idea of optimizing for a richness metric is sound. Introducing an explicit metric TC and then using it as a reward in an RL framework is a logical and principled way to steer the model toward generating more detailed captions.\n- The results of the RL optimization strategy (Table 2) are promising. They show that adding the proposed TC-based reward $R_{TC}^{\\prime}$ can improve both the standard CIDEr score (from 134.1 to 135.2) and the new TC score (from 20.0 to 23.9), demonstrating the effectiveness of the dual-reward strategy."}, "weaknesses": {"value": "- The paper’s objective is to identify “overlooked details” and “vocabularies that extend beyond the ground truth.” However, the tag repository is constructed based on the ground truth annotations in the COCO dataset (lines 223-224). This approach is circular. The model cannot learn to generate “missed” words if those words are not present in the COCO vocabulary. Consequently, this method does not address the stated problem; it merely repurposes the existing, limited vocabulary. It is akin to another form of retrieval augmentation in image captioning [1,2,3].\n- The paper tries to fuse object (O), grid (G), and tag (T) features. However, there is no ablation study that measures the impact of the tag features. A crucial experiment, like comparing a model with (O + G) features to the full (O + G + T) model, is missing. \n- The paper introduces asymmetric attention, but its explanation is lacking. While it conducted ablation studies for three types of refinements in cross attention in the appendix, the scores from these studies are much lower than the final results. This suggests that the paper may have missed the combination experiments for each pair of refinements.\n\n[1] Ramos, Rita, et al. \"Smallcap: lightweight image captioning prompted with retrieval augmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[2] Li, Wenyan, et al. \"Understanding Retrieval Robustness for Retrieval-augmented Image Captioning.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.\n[3] Tanaka, Ryota, et al. \"Vdocrag: Retrieval-augmented generation over visually-rich documents.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "- Could you explain why $R_{TC}^{\\prime}$ is better than $R_{TC}$? Since there is already an hyperparamter for $R_{TC}^{\\prime}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lDzgu4YZXb", "forum": "Q1pjfNfdQn", "replyto": "Q1pjfNfdQn", "signatures": ["ICLR.cc/2026/Conference/Submission9232/Reviewer_9bmq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9232/Reviewer_9bmq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865405312, "cdate": 1761865405312, "tmdate": 1762920888809, "mdate": 1762920888809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel image captioning framework addressing the limitations of existing methods that rely on human-annotated references. These limitations include oversimplified descriptions and poor coverage of visual details. The proposed approach aims to balance descriptive precision (accurate detail capture) and richness (diverse, granular vocabulary). Fine-grained pseudo tags are used for learning and an asymmetric attention multimodal projector is introduced to map and fuse information across modalities effectively. An evaluation metric is also proposed to address the gap in existing metrics (e.g., CIDEr) that focus on n-gram overlaps rather than semantic detail coverage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Leverages frozen CLIP to generate fine-grained pseudo tags, overcoming the semantic limitations of manual annotations and enabling systematic capture of implicit visual details (e.g., textures, contextual relationships).\n2. Introduces an Asymmetric Attention Multimodal Projector that dynamically balances modality-specific and cross-modal interactions, achieving superior integration of descriptive precision (object localization accuracy) and lexical richness (diverse vocabulary generation).\n3. Proposes Tags Coverage (TC), the first metric to quantify caption granularity by measuring alignment between pseudo tags and generated descriptions, addressing the critical gap in conventional n-gram metrics (e.g., CIDEr) that fail to assess semantic richness.\n4. Integrates TC into a reinforcement learning framework via policy gradient optimization, explicitly harmonizing accuracy (reference alignment) and richness (detail diversity) during training—a paradigm shift from single-objective optimization in existing methods.\n5. Achieves state-of-the-art results on MS-COCO Karpathy test set, with significant improvements in both CIDEr scores (semantic coherence) and Tags Coverage (unannotated detail capture)."}, "weaknesses": {"value": "This paper studies the image caption problem by training and evaluating models on the MS-COCO benchmark. In general, I think this setting to be outdated. Nowadays, vision-language models like Qwen-VL excel in tackling the image captioning problem. The authors of this paper aim to address the challenge of achieving a balance between precision and richness in image captions. They claim that their method delivers captions that are both precise and rich. However, after reviewing the results in Figures 5, 6, and 7, I believe that vision-language models could achieve better outcomes in terms of precision and richness. Additionally, the contributions of this paper are difficult to apply to vision-language models to enhance their performance further."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ywVZ60qpkI", "forum": "Q1pjfNfdQn", "replyto": "Q1pjfNfdQn", "signatures": ["ICLR.cc/2026/Conference/Submission9232/Reviewer_e7Jw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9232/Reviewer_e7Jw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904309264, "cdate": 1761904309264, "tmdate": 1762920888426, "mdate": 1762920888426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a CLIP-based image captioning framework designed to balance precision and richness in image descriptions. The approach leverages pseudo tags derived from CLIP to enhance visual detail without additional human annotations and introduces an asymmetric attention multimodal projector for cross-modal feature fusion. Furthermore, the paper introduces a new metric, Tags Coverage (TC), to quantify caption granularity and integrates it into a reinforcement learning (RL) framework based on NSC (New Self-Critical training). Experiments on the MS-COCO Karpathy split show competitive or superior results in CIDEr and TC compared to baselines such as BLIP-2 and FUSECAP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses the important challenge of balancing descriptive precision and richness in image captioning, an issue often neglected by models optimized solely for reference-based metrics.\n- The concept of using CLIP-derived pseudo tags for fine-grained supervision without additional annotation effort is original and well-motivated.\n- The proposed Tags Coverage (TC) metric is an interesting and intuitive attempt to quantify caption richness, complementing conventional metrics such as CIDEr or SPICE.\n- The model architecture and training strategy are well explained, with detailed ablation studies and hyperparameter analyses in the appendix.\n- The approach yields consistent improvements in both accuracy and descriptive detail over competitive baselines.\n- The paper is generally well written and clearly structured, with good visual examples showing qualitative improvements."}, "weaknesses": {"value": "- In the era of multimodal large language models (MLLMs) such as GPT-4V, Gemini, or Kosmos-2, it is crucial to include at least one comparison or a discussion explaining how the proposed method relates to or differs from these more general models.\n- While Tags Coverage captures richness, it does not assess factuality. The paper should include metrics that evaluate hallucinations (e.g., ALOHa) to verify that the gain in richness does not introduce factual errors.\n- The term CLIP-I (line 106, page 2) is not explicitly defined at first use. It likely refers to the image encoder branch of CLIP, but this should be clarified.\n- The acronym NSC is first explained only in the appendix; it should be defined when first mentioned in the main text.\n- Line 106, page 2: “the level of details in cations” → should read “captions.”\n- Definitions or equation labels should follow a consistent capitalization style throughout the paper.\n- Using a fixed number of tags (top-100) could penalize images with simple content, where fewer tags would suffice to describe the image accurately. A discussion of adaptive tag selection or thresholding would improve robustness.\n- The related work section focuses mainly on conventional captioning models (e.g., Up-Down, M2, BLIP-2). It should better situate the proposed method in the context of recent instruction-tuned multimodal LLMs and retrieval-augmented captioners.\n- TC may favor longer captions containing more tags, which does not always correlate with human preferences or factual accuracy. This limitation should be discussed explicitly in the paper’s conclusion or discussion."}, "questions": {"value": "- How does the proposed framework compare in performance and efficiency with modern MLLMs (e.g., BLIP-3, GPT-4V, Kosmos-2) when evaluated on the same dataset?\n- Have the authors measured or observed an increase in hallucinated content when optimizing for higher TC? Would metrics such as ALOHa confirm or contradict this?\n- Could the number of pseudo tags (top-100) be made adaptive to image complexity or confidence thresholds from CLIP?\n- What exactly does CLIP-I refer to in the paper — the image encoder of CLIP, or a modified version thereof?\n- How are the weights λ₁ and λ₂ in the combined reward function (Eq. 9) selected? Are they dataset-specific or fixed across experiments?\n- Could the proposed Tags Coverage metric be generalized to video captioning or other multimodal generation tasks?\n- Would the authors consider releasing an evaluation toolkit that includes the TC metric for reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CNE2aIwTOi", "forum": "Q1pjfNfdQn", "replyto": "Q1pjfNfdQn", "signatures": ["ICLR.cc/2026/Conference/Submission9232/Reviewer_EGKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9232/Reviewer_EGKi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103074153, "cdate": 1762103074153, "tmdate": 1762920888054, "mdate": 1762920888054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}