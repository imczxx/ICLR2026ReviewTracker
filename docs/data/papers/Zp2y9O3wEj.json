{"id": "Zp2y9O3wEj", "number": 2168, "cdate": 1757007225756, "mdate": 1759898165204, "content": {"title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search", "abstract": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning—spanning tens of steps—and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3–style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.", "tldr": "", "keywords": ["Visual Search;Thinking-with-images;Reinforcement Learning;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84ac940805ffc7f9d6ab7b571b9939704b185337.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Mini-03, a vision–language model designed for deep, multi-turn visual reasoning in complex visual search tasks. The authors propose a three-part training recipe: \n\n- VisualProbe Dataset – A challenging benchmark with high-resolution images, small targets, and distractors to necessitate iterative reasoning. \n\n- Cold-start Data Pipeline – An iterative synthesis process generating ~6,000 multi-turn trajectories from a few exemplars for supervised fine-tuning. \n\n- Over-turn Masking – A reinforcement learning modification to GRPO that prevents penalization of trajectories exceeding the turn limit, enabling test-time scaling. \n\nBuilt on Qwen2.5-VL-7B-Instruct, Mini-03 demonstrates emergent test-time turn scaling: trained with 6 turns, accuracy improves up to 32 turns at inference. It achieves state-of-the-art performance on VisualProbe-Hard (48.0% vs. 35.1% for DeepEyes) and competitive results on V*Bench and HR-Bench. Ablations confirm the necessity of cold-start SFT and over-turn masking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A reproducible recipe combining dataset, data generation pipeline, and RL. \n\n- Over-turn masking is simple yet effective, unlocking test-time scaling—a property with implications beyond vision tasks. \n\n- Promising results across benchmarks, systematic ablations, and insightful analysis of resolution vs. interaction depth trade-offs. \n\n- VisualProbe fills a gap for evaluating exploratory reasoning."}, "weaknesses": {"value": "- The paper does not explain why over-turn masking enables scaling—mode collapse prevention or exploration diversity? \n\n- Relies on a teacher VLM for trajectory synthesis; sensitivity to exemplar quality is unexplored. \n\n- Reward computation details (model choice, prompt, bias analysis) are missing -- reproducibility concerns. \n\n- Focused on visual search; generalization to other reasoning domains (math, scientific figures) is untested – narrow scope. \n\n- Systematic study of error patterns or robustness to adversarial perturbations would strengthen the paper."}, "questions": {"value": "- How sensitive is performance to the choice and diversity of cold-start exemplars? \n\n- Which LLM was used as the reward judge, and how was its reliability validated? \n\n- Can you provide theoretical or empirical analysis of why over-turn masking enables test-time scaling? \n\n- Does the approach generalize to other reasoning domains beyond visual search? \n\n- What are the failure modes—e.g., repetitive loops, grounding errors, or inability to backtrack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ABf5lQQ4Rt", "forum": "Zp2y9O3wEj", "replyto": "Zp2y9O3wEj", "signatures": ["ICLR.cc/2026/Conference/Submission2168/Reviewer_XKVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2168/Reviewer_XKVc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739047414, "cdate": 1761739047414, "tmdate": 1762916090523, "mdate": 1762916090523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Mini-o3, a method capable of performing deep, multi-turn reasoning and achieving state-of-the-art performance on challenging visual search tasks.\nThey constructed a multi-turn visual reasoning trajectory dataset and introduced an over-turn masking strategy to balance training-time efficiency with test-time scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main innovation lies in the proposal of an over-turn strategy. In addition, it demonstrate that although only up to 6-round data  are used during training, the accuracy consistently improves as the upper bound on interaction turns increases from 4 to 32 during inference."}, "weaknesses": {"value": "The setting of the 6-round budget has not been proven to be optimal."}, "questions": {"value": "Have you tried testing the performance of different sizes of QWEN-VL-2.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UUiIUVanNJ", "forum": "Zp2y9O3wEj", "replyto": "Zp2y9O3wEj", "signatures": ["ICLR.cc/2026/Conference/Submission2168/Reviewer_iPEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2168/Reviewer_iPEo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797491572, "cdate": 1761797491572, "tmdate": 1762916090135, "mdate": 1762916090135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **Mini-o3**, an open-source vision-language agent designed to perform **deep multi-turn reasoning** for visual search tasks. While prior open-source VLMs (e.g., DeepEyes, Chain-of-Focus) often exhibit shallow, repetitive reasoning with limited tool-use turns, Mini-o3 demonstrates reasoning trajectories spanning tens of steps. The authors propose a three-component recipe:\n\n1. **VisualProbe dataset**—a new benchmark of high-resolution, hard visual-search problems requiring trial-and-error exploration;\n2. **Iterative cold-start data synthesis**—an in-context prompting method for generating diverse reasoning trajectories for supervised fine-tuning;\n3. **Over-turn masking**—a reinforcement-learning modification that avoids penalizing long, incomplete trajectories, enabling scalability beyond the training turn limit.\n\nTrained with a 6-turn cap, Mini-o3 generalizes to much longer inference trajectories (up to 32 turns), achieving **state-of-the-art accuracy** on VisualProbe, V* Bench, HR-Bench, and MME-Realworld benchmarks. The model shows more diverse reasoning strategies and deeper “thinking-with-images” patterns than prior open-source agents."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses a **clear and under-explored gap**: enabling deep visual reasoning in open-source VLMs.\n* **Elegant and reproducible training recipe** with quantitative and qualitative validation.\n* **Comprehensive experiments** (four benchmarks + ablations).\n* Demonstrates **turn-scaling property**—a rare capability among existing agents.\n* Good ethical and reproducibility practice; code/data promised for release."}, "weaknesses": {"value": "- [Reward Modeling] The paper employs an external LLM as a semantic judge for reinforcement learning but provides few details about its consistency, inter-run variance, or bias. It is unclear how reward noise affects policy stability or whether calibration against human-verified scores was attempted.\n- [Ablation] While ablations exist, they focus on quantitative accuracy. The paper could include deeper analyses of emergent reasoning behaviors—e.g., trajectory diversity, failure patterns, or qualitative diagnostics showing how reasoning depth relates to success rates."}, "questions": {"value": "Please see weaknesses above.\n\n1. How sensitive is over-turn masking to the masking threshold or turn budget?\n2. Do long-turn trajectories ever degenerate into loops or repetitive zooming?\n3. How well would Mini-o3 scale with a larger base model?\n\nMinor Grammar & Typo:\n\nLine 014 - 015: “We address this limitation by scaling up tool-based interactions and introduce Mini-o3…” should be \"introducing”\n\nLine 018: “OpenAI o3–style behaviors” should be a regular hyphen"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "flnCF5orUN", "forum": "Zp2y9O3wEj", "replyto": "Zp2y9O3wEj", "signatures": ["ICLR.cc/2026/Conference/Submission2168/Reviewer_ku2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2168/Reviewer_ku2Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844535630, "cdate": 1761844535630, "tmdate": 1762916089851, "mdate": 1762916089851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mini‑o3 is a multi‑turn visual search system that scales reasoning to tens of steps by combining a new Visual Probe dataset, an iterative cold‑start trajectory pipeline, and an over‑turn masking RL strategy that avoids penalizing responses hitting the turn limit. Trained with only six turns, it generalizes to longer test‑time trajectories where accuracy rises as the allowed turns increase, achieving state‑of‑the‑art results on visual search benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Mini-o3 achieves state-of-the-art performance on several visual search benchmarks, clearly outperforming previous open models. The improvement is from allowing the model to perform more reasoning turns. The over-turn masking method improves long-turn reasoning while keeping training efficient.\n- The new Visual Probe dataset introduces difficult visual search tasks. Mini-o3 trained on Visual Probe has demonstrated some sort of trial-and-error reasoning."}, "weaknesses": {"value": "- Missing details\n    - The paper relies on an LLM judge for rewards, however the exact judging prompts/criteria are not fully specified in the main text.\n    - The dataset difficulty splits (easy/medium/hard) are introduced and used in results but their criteria are only briefly mentioned. I would suggest the author to add more details on how the data split are decided.\n    - Similarly, the cold‑start trajectory generation details (for example, exact prompts, hand-crafted in-context examples, acceptance filters) should be included in the main text or appendix.\n- I wonder whether the authors have considered some other variants for over-turn masking\n    - Mask only context-overflow instead of turn-overflow\n    - Soft penalty instead of zero-mask for over-turn content\n- Missing ablation of removing the DeepEyes portion or changing the ratio between VisualProbe and DeepEyes from the training to see how much the mixed-data balance matters."}, "questions": {"value": "- Interesting that ChartQA performance dropped after the RL training compared to Qwen2.5-VL. I am curious about what the authors think might be the reason? Is it because of the data domain used for RL training?\n- Curious how much improvements on general perception benchmarks, like BLINK, and CVBench. and also spatial reasoning benchmarks, like SAT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FO4UtpKJEo", "forum": "Zp2y9O3wEj", "replyto": "Zp2y9O3wEj", "signatures": ["ICLR.cc/2026/Conference/Submission2168/Reviewer_SzFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2168/Reviewer_SzFD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932945128, "cdate": 1761932945128, "tmdate": 1762916086230, "mdate": 1762916086230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}