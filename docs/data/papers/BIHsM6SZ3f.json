{"id": "BIHsM6SZ3f", "number": 21722, "cdate": 1758320988108, "mdate": 1759896906947, "content": {"title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.", "tldr": "We create LitmusValues to reveal AI models' value priorities that are capable of predicting AI risky behaviors in a set of scenarios relevant to AI misalignment.", "keywords": ["AI Values", "value alignment", "ai risk", "dilemma"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95d770cdbf4f6284303be0e043f8469e1ba9c256.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They gathered a set of value classes and then constructed a dataset (AIRiskDilemmas) to evaluate models on competing value based scenarios. The scenarios are generated by Claude 3.5 Sonnet and seeded from advanced-ai-risk."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Well executed and good data design."}, "weaknesses": {"value": "Cons  \n- The Human QA is quite lacking. 150 of the questions were sampled but not responses as far as I could tell.\n- I also didn't see any mention of refusal rates anywhere?"}, "questions": {"value": "Notes  \nLine 76 I'm confused about how you're differentiating and measuring what models \"actually choose\" unlike prior work. I would reword or remove this as I feel as though it mis-characterizes a lot of prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KTM7XfN153", "forum": "BIHsM6SZ3f", "replyto": "BIHsM6SZ3f", "signatures": ["ICLR.cc/2026/Conference/Submission21722/Reviewer_zpqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21722/Reviewer_zpqN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608986032, "cdate": 1761608986032, "tmdate": 1762941906617, "mdate": 1762941906617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LITMUSVALUES, a behavioral evaluation framework that infers the revealed value priorities of large AI models through their choices in high-stakes scenarios. It introduces AIRISKDILEMMAS, a dataset of 3,000 contextualized dilemmas that pit competing values (e.g., Truthfulness vs. Care) against one another in AI-relevant risk contexts. By aggregating binary choices into Elo-style scores, the authors extract a hierarchy of 16 shared AI value classes grounded in Schwartz's human value theory and industry principles such as Anthropic's Constitution and OpenAI's Model Spec. They find that (1) revealed preferences diverge sharply from stated ones, (2) value rankings are stable across reasoning effort and model scale but shift depending on whether humans or other AIs are affected, and (3) certain values like Truthfulness and Respect correlate with lower incidence of risky behaviors in both AIRISKDILEMMAS and HarmBench, whereas Care and Learning predict higher risk. Overall, the work argues that value-based diagnostics can serve as an early-warning signal for emerging AI safety risks, especially as models grow more adept at evading traditional evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Demonstrates a clear divergence between stated and revealed values, with negative correlations across models—an important finding that cautions against relying solely on self-reported alignment.\n- The AIRISKDILEMMAS dataset is diverse, well-contextualized, and thematically aligned with real AI-risk categories (e.g., deception, power seeking). It offers a reusable benchmark for studying moral conflict and risk behaviors beyond human-centric dilemmas.\n- Employs multiple statistical indicators (Elo, Spearman, Krippendorff, Relative Risk) and compares across > 20 models.\n- Connects psychological value theory (Schwartz, Haidt) to AI-safety evaluation, promoting a richer interdisciplinary dialogue between moral psychology and AI alignment research."}, "weaknesses": {"value": "- Both the generation of dilemmas and the subsequent value identification and classification are performed by the same model (Claude 3.5 Sonnet). This creates a closed feedback loop where the model's own moral priors may define both the data distribution and the annotation schema. As a result, the reported \"revealed values\" may largely reflect Claude's value norms rather than those of other models under evaluation. Cross-model generation/annotation or multi-model adjudication would be necessary to validate independence.\n- While the framework effectively diagnoses value patterns, it offers no clear procedure for practical use. Its utility currently lies in exploratory analysis rather than operational alignment practice.\n- Only two human annotators reviewed 150 dilemmas (κ ≈ 0.61). This scale is insufficient to assess annotation reliability, identify systematic biases in model-generated values, or establish cultural and contextual robustness.\n- The reported relationships between value rankings and risky behaviors are purely correlational. The study does not perform controlled interventions to test whether changes in value emphasis causally affect risk behavior frequencies. As a result, the interpretation of values as \"predictors\" of risk may remain speculative."}, "questions": {"value": "- Have you tested whether value annotations generated by a different model yield consistent rankings, to assess the impact of self-bias?\n- Could you expand human validation beyond 150 cases or include cross-cultural raters to test value universality?\n- The pipeline involves large-scale pairwise evaluations. Could you share the approximate computational cost or time requirements?\n- How do you envision LITMUSVALUES being used in practice—for example, in model audits, safety evaluations, or red-teaming pipelines? - Are there plans to automate or simplify the evaluation to make it accessible to practitioners?\n\nIf the authors can substantively address the questions and weaknesses outlined above, I would be inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OxXNuYrHrE", "forum": "BIHsM6SZ3f", "replyto": "BIHsM6SZ3f", "signatures": ["ICLR.cc/2026/Conference/Submission21722/Reviewer_Ncrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21722/Reviewer_Ncrr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817128217, "cdate": 1761817128217, "tmdate": 1762941906301, "mdate": 1762941906301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LitmusValues, an evaluation pipeline that is intended to \"discover\" the priorities of an AI model within the context of AI value classes, is introduced in the work. In order to assess the model's alignment, it suggests contrasting its explicitly expressed values with the preferences that emerge from contextualized scenarios. LitmusValues extends the advanced-ai-risk dataset by generating over 10,000 contextualized dilemmas across nine domains utilizing a large language model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach for measuring stated values against revealed values offers insights on how an LLM may behave in specific scenarios.\n- The results are detailed and discussed clearly. Insights, including values that are invariant to reasoning efforts and model size, are valuable. AI value prioritization being dependent on the target is informative as well.\n- The results are presented clearly, providing a thorough analysis of the findings. The insights indicating values are invariant to reasoning efforts or model size, are valuable. Additionally, the fact that AI value prioritization is dependent upon the target is informative.\n- Relative Risk (RR) and rank difference of values show that the pipeline can reveal additional information about the model's value alignment."}, "weaknesses": {"value": "- The dilemmas presented in this paper are not realistically based and are rather simple. The presented dataset is largely built by an LLM to evaluate LLMs.\n- The dilemmas have only two choices, and the model is forced to choose one of them rather than providing a full solution and then evaluating the reasoning of the model to measure alignment.\n- The selection of randomly choosing 3000 dilemmas out of 9000 generated dilemmas to construct AIRiskDilemmas is not sound. It would be more appropriate to select more realistic scenarios that accurately depict real-world situations to construct AIRiskDilemmas rather than randomly sampling them. \n- The method employed to obtain the stated values is simplistic, as it utilized five prompts for each potential pair of SharedAI values to ascertain the stated values of each LLM.\n- While the dataset AIRiskDilemmas is a useful contribution, the pipeline employed to assess risk lacks sufficient rigor. The pipeline mandates the LLM to provide an answer to the dilemma, subsequently measuring alignment based on the output.\n- It should be mentioned in the ethical consideration that these dilemmas are simplified and do not reflect real-world scenarios, which might be complex. Furthermore, the authors should highlight in the section ethics statement that the majority of this dataset was constructed by an LLM.\n- It is imperative to acknowledge that these ethical dilemmas are simplified and do not accurately represent the complexities of real-world scenarios. Furthermore, the authors should emphasize that the majority of this dataset was constructed by an LLM."}, "questions": {"value": "- How can expressed preferences be captured/measured through \"vibe checking\" mentioned in the introduction?\n- The title reads as anthropomorphizes LLMs and implies LLMs have intent. How much intent of LLMs can be measured using your pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HtOHBrz7gv", "forum": "BIHsM6SZ3f", "replyto": "BIHsM6SZ3f", "signatures": ["ICLR.cc/2026/Conference/Submission21722/Reviewer_SseR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21722/Reviewer_SseR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952210993, "cdate": 1761952210993, "tmdate": 1762941905929, "mdate": 1762941905929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the relative importance of various values according to which LLMs are taking decisions. Its starting point is a list of non-contextualized questions from the advanced-ai-risk dataset (Perez et al., 2023) and a list of values/principles from the literature and released by OpenAI and Anthropic. They then use an LLM pipeline to generate their own dataset, AIRiskDilemmas, by giving the questions some real-world context and making it a choice dilemma between two actions that represent two distinct values. Then, evaluating what choices an LLM takes in these dilemmas reveals what values it prioritizes over others. The dataset is specifically oriented towards detecting one of seven common risk behaviors studied in the AI safety community (such as alignment faking, deception, power seeking, etc.)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Interesting study with various findings (stated value preferences can diverge significantly from revealed value preferences, LLMs understand value definitions, Length of Reasoning does not affect value preferences, effects of target being a human or an AI, what value prioritization correlates with what risky behavior, and more) \n- the paper is great at both giving statistical experiment details as well as interpretation conclusions from the experiments"}, "weaknesses": {"value": "- I am not a great fan of LLMs playing a major role _in the creation_ of the a dataset, but maybe the community does not mind this. I appreciate that the authors had human annotators validate a subsample of 150 dilemmas out of the 3000 questions dataset whether the dilemma is indeed representing a choice between the two intended values of interest\n- I am missing a methodology comparison to prior literature on identifying revealed value preferences in human subjects (non LLM literature). I only see a side mention of those related works in line 067 right now, but I expect there to be quite a few lessons to potentially draw from that literature"}, "questions": {"value": "- Section 4.1 is about correlations of values and risky behaviors, correct? Because the section itself is phrased repeatedly as if you are reporting causal findings here. Can you be bit more careful here about phrasing such that such an impression can be avoided?\n\nMinor:\n- what is the mathematical notation you use in line 211 with 16_P_2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RYsuYnhMnF", "forum": "BIHsM6SZ3f", "replyto": "BIHsM6SZ3f", "signatures": ["ICLR.cc/2026/Conference/Submission21722/Reviewer_p4m7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21722/Reviewer_p4m7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033798590, "cdate": 1762033798590, "tmdate": 1762941905650, "mdate": 1762941905650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}