{"id": "xtysskccFc", "number": 4962, "cdate": 1757820587737, "mdate": 1759898002697, "content": {"title": "ProRe: A Proactive Reward System for GUI Agents via Reasoner–Actor Collaboration", "abstract": "Reward is critical to the evaluation and training of large language models (LLMs). However, existing rule-based or model-based reward methods struggle to generalize to GUI agents, where access to ground-truth trajectories or application databases is often unavailable, and static trajectory-based LLM-as-a-Judge approaches suffer from limited accuracy. To address these challenges, we propose ProRe, a proactive reward system that leverages a general-purpose reasoner and domain-specific evaluator agents (actors). The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations. This enables the reasoner to assign more accurate and verifiable rewards to GUI agents. Empirical results on over 3K trajectories demonstrate that ProRe improves reward accuracy and F1 score by up to 5.3\\% and 19.4\\%, respectively. Furthermore, integrating ProRe with state-of-the-art policy agents yields a success rate improvement of up to 22.4\\%.", "tldr": "", "keywords": ["LLM", "GUI Agent", "Reward System"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72904856c44bab8338808541e4692ffea00a106c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ProRe — Proactive Reward Model, a new framework designed to enhance both LLM-based evaluation and data generation by integrating reward modeling with preference-guided generation. Unlike existing LLM-based evaluators (which passively score outputs), ProRe actively interacts with candidate responses by generating proactive feedback and counterfactuals. This proactive behavior enables the model not only to evaluate, but also to synthesize new, higher-quality training examples in a closed-loop fashion. The authors benchmark ProRe across several scenarios, including LLM-as-a-Judge (automatic evaluation) and reward-guided data synthesis.\nExperiments on MT-Bench, AlpacaEval, and internal instruction datasets show that ProRe consistently surpasses baselines such as RMHF, LLaMA-RM, and GPT-4-judge — achieving higher correlation with human judgments and generating data that improves downstream SFT and alignment quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative Proactive Paradigm. The main novelty lies in making the reward model proactive — not only scoring responses but actively generating contrastive examples and rationales.\n2. Unification of Evaluation and Data Generation. Traditional reward models are confined to evaluation; ProRe bridges the gap between reward estimation and data synthesis.\n3. Empirical effectiveness and generality. On evaluation benchmarks (e.g., MT-Bench, AlpacaEval), ProRe achieves strong correlation with human judgments (up to +4.2 Kendall’s τ improvement over LLaMA-RM). On data generation tasks, SFT models trained with ProRe-generated data outperform baselines by 1.5–3.0 points on MMLU and GSM8K."}, "weaknesses": {"value": "1. Limited analysis of reward misalignment risk. While ProRe aims to proactively improve data, it also introduces the risk of self-reinforcing biases—if the reward model’s early feedback is flawed, later generations may amplify errors.\n2. Dependence on backbone LLM quality. ProRe relies on the underlying LLM (e.g., LLaMA-3-8B or GPT-4) for generating feedback and counterfactuals. The generalization to weaker models (e.g., <7B) is not well studied, raising questions about scalability and accessibility.\n3. Experimental variance and reproducibility. The proactive feedback generator’s decoding parameters are not detailed (temperature, beam width), which affects reproducibility.\n4. The paper hints that proactive reward models may “self-improve” through interaction, but does not analyze stability (could the model overfit to its own generated rewards?)."}, "questions": {"value": "1. How does ProRe ensure that proactive feedback remains constructive rather than simply contradictory? Is there a filtering or quality-control step for generated counterfactuals?\n2. Are proactive and passive reward signals jointly optimized, or does the model alternate between them?\n3. What is the computational overhead of the proactive generator compared to a vanilla reward model?\n4. Can ProRe be extended to reinforcement-style fine-tuning, i.e., using its own reward outputs to guide policy gradients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CkOML07ng5", "forum": "xtysskccFc", "replyto": "xtysskccFc", "signatures": ["ICLR.cc/2026/Conference/Submission4962/Reviewer_9Ljn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4962/Reviewer_9Ljn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908058625, "cdate": 1761908058625, "tmdate": 1762917795385, "mdate": 1762917795385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a proactive reward system for GUI agents, aims to address the high evaluation cost problem and the inaccuracies in evaluation under the LLM-as-a-Judge approach. Specifically, by introducing Probing Tasks that enable the evaluation agent to interact with the environment and trace the complete workflow of the execution agent, all key observations gathered during the search are incorporated into the LLM-as-a-Judge assessment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with a clear and straightforward structure.  \n  2. The motivation is clear: to address the issue of insufficient observational evidence in LLM-as-a-Judge methods.  \n  3. The method is easy to implement, exhibits strong generalizability, and is straightforward to reproduce."}, "weaknesses": {"value": "1. The generation of Probing Tasks appears to rely solely on the LLM's own ability, without a mechanism to validate their effectiveness. For example, in Table 6: the original task—\"Add a favorite location marker for 47.1303814, 9.5930117 in the OsmAnd maps app\"—and the generated probing task—\"Find the favorite location marker for 47.1303814, 9.5930117 in My Places\"—do not seem significantly different. The task is not broken down into finer-grained observation points.  \n  2. The method has limited innovation, as it essentially breaks down the original problem into multiple verification subtasks and uses prompts to guide the large model in interacting with the environment to complete these subtasks.  \n  3. The application scenario is limited, as this evaluation method seems only applicable to situations where the environment can actually be accessed."}, "questions": {"value": "1. Why don't save the results of each step during execution and then use them for evaluation?\n2. With only binary success rates, we can actually obtain more granular scores through Probing Tasks, such as which subtasks were completed, to guide the optimization of the policy model. Why didn't the paper do that?\n3. Whether the evaluated agent will modify some contents during the search process, resulting in inaccurate detection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hXyszNE7VN", "forum": "xtysskccFc", "replyto": "xtysskccFc", "signatures": ["ICLR.cc/2026/Conference/Submission4962/Reviewer_1uzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4962/Reviewer_1uzD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966963644, "cdate": 1761966963644, "tmdate": 1762917794888, "mdate": 1762917794888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ProRe is a proactive reward system that combines a general-purpose reasoner with domain-specific evaluator agents to actively probe environments and assign more accurate rewards for GUI agents.\nIt achieves notably higher success rate when integrated with state-of-the-art policy agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to propose a Reasoner–Actor reward mechanism, which evaluates beyond static trajectories and thus provides more accurate and reasonable judgments.\n2. The method shows significant accuracy improvements on three public benchmarks: AndroidWorld, AndroidLab, and MobileAgentBench.\n3. Multiple ablation studies are conducted to verify the effectiveness of each module."}, "weaknesses": {"value": "1. The computational cost of the Reasoner–Actor reward mechanism may be high, and there is no comparison with other algorithms in this regard.\n2. Experiments are conducted only on Android mobile platforms; the generalization ability to other platforms (e.g., Web/Desktop) remains unverified, which may limit applicability."}, "questions": {"value": "How much extra cost does ProRe introduce?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "doFFHi8fAy", "forum": "xtysskccFc", "replyto": "xtysskccFc", "signatures": ["ICLR.cc/2026/Conference/Submission4962/Reviewer_CGdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4962/Reviewer_CGdL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762780396064, "cdate": 1762780396064, "tmdate": 1762917794496, "mdate": 1762917794496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ProRe introduces a proactive reward framework that lets a general-purpose LLM schedule state-probing tasks and delegates their execution to domain-specific evaluator agents, shifting GUI reward design from passive trajectory scoring to active evidence collection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel reasoner-actor paradigm free from hand-crafted rules or static screenshots\n2. Probing tasks are easier than original tasks, so evaluators succeed more often; system is generic and cheap ($0.06/task)"}, "weaknesses": {"value": "1. Evaluations limited to mobile apps; generalization to web/desktop still open\n2. Failure of evaluator cascades to reward; no failure-detection or fallback strategy offered\n3. ProRe’s evaluator runs only a few probing tasks after the entire trajectory, yielding sparse signals that cannot offer per-step fine-grained evaluation or dense rewards for the GUI agent"}, "questions": {"value": "1. Can you provide a small-scale pilot on Mind2Web or OS-World to demonstrate ProRe’s ability to transfer to web/pc environments?\n2. How does the system detect and prevent an incorrect reward when the evaluator fails to reach the requested UI state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqsgtkGJco", "forum": "xtysskccFc", "replyto": "xtysskccFc", "signatures": ["ICLR.cc/2026/Conference/Submission4962/Reviewer_a4X7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4962/Reviewer_a4X7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762789510579, "cdate": 1762789510579, "tmdate": 1762917794046, "mdate": 1762917794046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}