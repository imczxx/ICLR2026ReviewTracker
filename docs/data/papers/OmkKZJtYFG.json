{"id": "OmkKZJtYFG", "number": 15940, "cdate": 1758257381303, "mdate": 1759897271884, "content": {"title": "A Universal Source-Free Class Unlearning Framework via Synthetic Embeddings", "abstract": "Class unlearning in neural classifiers refers to selectively removing the model’s ability to recognize a target (forget) class by reshaping the decision boundaries. This is essential when taxonomies change, labels are corrected, or legal or ethical requirements mandate class removal. The objective is to preserve performance on the remaining (retain) classes while avoiding costly full retraining. Existing methods generally require access to the source, i.e., forget/retain data or a relevant surrogate dataset. This dependency limits their applicability in scenarios where access to source data is restricted or unavailable. Even the recent source-free class unlearning methods rely on generating samples in the data space, which is computationally expensive and not even essential for doing class unlearning. In this work, we propose a novel source-free class unlearning framework that enables existing unlearning methods to operate using only the deployed model. We show that, under weak assumptions on the forget loss with respect to logits, class unlearning can be performed source-free for any given neural classifier by utilizing randomly generated samples within the classifier’s intermediate space. Specifically, randomly generated embeddings classified by the model as belonging to the forget or retain classes are sufficient for effective unlearning, regardless of their marginal distribution. We validate our framework on four backbone architectures, ResNet-18, ResNet-50, ViT-B-16, and Swin-T, across three benchmark datasets, CIFAR-10, CIFAR-100, and TinyImageNet. Our experimental results show that existing class unlearning methods can operate within our source-free framework, with minimal impact on their forgetting efficacy and retain class accuracy.", "tldr": "We propose a source-free class unlearning framework that removes target classes from neural classifiers using only intermediate embeddings—no data or input-space generation required—while preserving accuracy on retain classes.", "keywords": ["Machine unlearning", "Class unlearning", "Source-free unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea96203db79431b605702385dacb7141235b619a.pdf", "supplementary_material": "/attachment/1b48a4c6c3d8375009b6ca79a4700fe685da1eff.zip"}, "replies": [{"content": {"summary": {"value": "This paper is targeted at source-free class unlearning task. The authors propose one method that only needs to randomly sample from the intermediate distribution space of the classifier. Using the sampled features to calculate the loss of forgetting and retaining is sufficient. The authors use the proposition to support their claim. The experiments also show the effectiveness of their method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is easy to follow and understanding.\n\n2. The proposed methods is clear and easy to implement.\n\n3. The method is supported by the proposition proved by the authors.\n\n4. The experiments show the effectiveness of the method."}, "weaknesses": {"value": "1. The results in Table 1 for ResNet-18 is unusual, where the model retrained with access to original samples of forget and retain classes perform worse than all methods. But it should be the upper bound of this task.\n\n2. All experiments are conducted when the number of class to forget is only 1 and the method is also based on this presumption. Could you please conduct experiments when the number of class to forget is greater than 1 and forget the classes one by one/at the same time?"}, "questions": {"value": "1. I wonder how the method perform if the sampled features are exactly the features extracted from real samples of forget/retain classes.\n\n2. I wonder whether the method is applicable to other tasks such as detection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAbtqmiIeh", "forum": "OmkKZJtYFG", "replyto": "OmkKZJtYFG", "signatures": ["ICLR.cc/2026/Conference/Submission15940/Reviewer_KPAB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15940/Reviewer_KPAB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539664481, "cdate": 1761539664481, "tmdate": 1762926152205, "mdate": 1762926152205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that class unlearning can be achieved without any data by randomly sampling intermediate embeddings, pseudo-labeling them, and applying existing unlearning losses, making class unlearning truly source-free, simple, and computationally efficient. Overall, while the idea is simple and empirically effective, the paper’s novelty is limited, theoretical analysis is shallow, and the motivation for random embeddings is not sufficiently justified. Strengthening these aspects could significantly improve the manuscript."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1)The proposed method can be universally compatible with existing unlearning methods.\n\n(2)Strong empirical performance across multiple datasets and architectures."}, "weaknesses": {"value": "(1)Limited methodological novelty. Although the paper frames the proposed approach as a universal source-free unlearning framework, the core methodology essentially acts as a “wrapper” that adapts existing unlearning methods to a data-free setting. The central idea, randomly sampling embeddings from intermediate feature space and applying pseudo-labeling, is conceptually simple and lacks technical depth. No new loss function, model design, or optimization technique is introduced, making the methodological innovation relatively limited.\n\n(2)The theoretical justification relies heavily on a monotonicity assumption on the logits with respect to the forget loss. This assumption is extremely weak and is satisfied by almost all existing gradient-based unlearning methods. Consequently, the theory provides limited explanatory power or insight into why random embeddings should be effective in practice. The current analysis feels more like a sanity check than a deep theoretical contribution.\n\n(3)The paper lacks a strong motivation for why random embeddings should be preferable or even comparable to learned or generative embeddings. Existing source-free methods (e.g., GKT, DSDA) invest effort in synthesizing realistic or adversarial samples for a reason, namely, to approximate meaningful decision boundaries. This work assumes that random vectors are sufficient but does not provide a compelling argument or empirical analysis to explain why this is the case. As a result, the motivation behind the proposed approach feels shallow and underdeveloped.\n\n(4)Why are random embeddings sufficient? The results suggest that using arbitrary embeddings is enough to drive decision boundary updates, but the manuscript does not explore why this is true empirically. Are there situations where random embeddings would fail (e.g., highly overfitted models or extremely imbalanced forget classes)?\n\n(5)The current framework is class-specific and relies on pseudo-labeling; it is unclear whether this mechanism can be extended to instance-level settings, which are arguably more practical and challenging.\n\n(6)While the reported results are promising, the main results section suffers from over-claiming, limited mechanistic analysis, lack of robustness discussion, and insufficient explanation of why random embeddings work as well as real embeddings.\n\n(7)In figure 2, the paper shows an empirical trend that performance improves with more synthetic embeddings and saturates after a certain point, but offers no deeper reasoning for why 100–200 random embeddings are sufficient to approximate the decision boundaries of real classes. Without theoretical insights or geometric analysis of the embedding space, the conclusion remains largely observational."}, "questions": {"value": "(1)Insufficient theoretical depth.\n\n(2)Only supports class-level unlearning.\n\n(3)Does the proposed method have failure-case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXeAd1TXfx", "forum": "OmkKZJtYFG", "replyto": "OmkKZJtYFG", "signatures": ["ICLR.cc/2026/Conference/Submission15940/Reviewer_ANbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15940/Reviewer_ANbF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880921934, "cdate": 1761880921934, "tmdate": 1762926151824, "mdate": 1762926151824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of source-free class unlearning. This work proposes to randomly generate intermediate features and utilize them for unlearning. A theoretical analysis is provided under an assumption that the forget loss function is monotonically increasing/decreasing with respect to the logit for forget/retain classes. Experimental results show the effectiveness of the proposed methods on image datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Machine unlearning is a timely topic, and taking advantage of randomly generated intermediate features sounds interesting.\n\n+ Experiments are conducted with both CNN and ViT backbones."}, "weaknesses": {"value": "- The goal of this paper might not be considered to be \"unlearning.\" The proposed method deliberately makes the model not to classify inputs to a target class, no matter what the inputs are. This still requires the model to be aware of the target class, to avoid classification to the target class. Rather, the definition in the intro that removing \"the influence of specific instances or classes\" would be more widely accepted definition of unlearning.\n\n- Theoretical analysis is flawed. Eq. (6) is invalid, as it argues that a scalar is equal to a vector.\n\n- It is not clear how Proposition 1 can be interpreted to draw new information.\n\n- No explanation on the proposed method. L234--240 is not enough to understand the details of the method. For example, what is the probability distribution p_z(z)? If it is parametric, how is it determined? Figure 1 gives a hint that the output of the feature extractor is used, but there is no further description. Furthermore, if p_z(z) depends on the output of the feature extractor, then it is not really \"randomly generated.\"\n\n- No efficiency analysis on the proposed method. As this paper argues that \"generating samples in the data space is computationally expensive,\" the proposed method should be compared with this setting to support this claim.\n\n- Experimental setting is questionable. All baseline methods are using retain and/or forget class data, and adding the proposed method does not directly imply that they do not use such data anymore. That is, it is necessary to explain how they modified baseline methods to eliminate the necessity of retain/forget class data.\n\n- No ablation study on the design choices of the proposed method.\n\n- This paper employs a different font and it paragraph is more dense, compared to other ICLR submissions."}, "questions": {"value": "What is the definition of unlearning, and why do you think so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q03zOkZGjL", "forum": "OmkKZJtYFG", "replyto": "OmkKZJtYFG", "signatures": ["ICLR.cc/2026/Conference/Submission15940/Reviewer_sU5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15940/Reviewer_sU5Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932313168, "cdate": 1761932313168, "tmdate": 1762926151387, "mdate": 1762926151387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a source-free class unlearning, utilizing the embedding in the intermediate space. It performs classification on these embeddings and generates pseudo-labels to construct synthetic forget and retain sets. And then use these sets for later training. The authors theoretically justify that effective unlearning can be achieved independent of the embedding distribution.  Empirical studies on  multiple datasets and backbones also demostrate the effectiveness of the proposed method. Further experiments show that existing unlearning algorithms can be seamlessly adapted to this source-free setting with minimal performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. the proposed method is simple but effective. The idea of leveraging intermediate embedding for creating the forget and retain sets is very good and effective. \n2. the theoretical proof strengthen the paper's contribution\n3. It is very good that the method could be easily applied to existing methods without performance degrade.\n4. writing is neat and easy to follow."}, "weaknesses": {"value": "1. It would be better if the experiments could address some critical applications that unlearning is suitable.\n2.  Many results show near-perfect AUS (~1.00), raising concerns about the sensitivity or discriminative power of the evaluation metric under this setting.\n3.  it would be better to do some analysis regarding the pseudo-label。\n4. novelty could be a problem as it is essnetially create forget and retain sets and access them in a traditional way."}, "questions": {"value": "The questions are related to the weakness.\n\n1. Could the authors provide examples or experiments demonstrating how the proposed method applies to critical real-world applications where unlearning is particularly useful?\n2. How do the authors ensure that the AUS metric remains sensitive and discriminative in evaluating subtle performance differences?\n3. Could the authors include an analysis of the pseudo-labels such as their reliability, stability, and effect on unlearning performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m6Hb0gItay", "forum": "OmkKZJtYFG", "replyto": "OmkKZJtYFG", "signatures": ["ICLR.cc/2026/Conference/Submission15940/Reviewer_bMou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15940/Reviewer_bMou"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154344938, "cdate": 1762154344938, "tmdate": 1762926150958, "mdate": 1762926150958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}