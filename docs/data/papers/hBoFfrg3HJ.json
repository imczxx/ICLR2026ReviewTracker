{"id": "hBoFfrg3HJ", "number": 1061, "cdate": 1756832152660, "mdate": 1759898230211, "content": {"title": "Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials", "abstract": "Large Language Model (LLM)-based agentic frameworks increasingly adopt the paradigm of dynamically generating task-specific agents. We suggest that not only agents but also specialized software modules for scientific and engineering tasks can be generated on demand. We demonstrate this concept in the field of solid mechanics. There, so-called constitutive models are required to describe the relationship between mechanical stress and body deformation. Constitutive models are essential for both the scientific understanding and industrial application of materials. However, even recent data-driven methods of constitutive modeling, such as Constitutive Artificial Neural Networks (CANNs), still require substantial expert knowledge and human labor. We present a framework in which an LLM generates a CANN on demand, tailored to a given material class and dataset provided by the user. The framework covers LLM-based architecture selection, integration of physical constraints, and complete code generation. Evaluation on three benchmark problems demonstrates that LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalization to unseen loading scenarios and extrapolation to large deformations. These findings indicate that LLM-based generation of physics-constrained neural networks can substantially reduce the expertise required for constitutive modeling and represent a step toward practical end-to-end automation.", "tldr": "We show that LLMs can automatically generate physics-constrained neural networks, reducing the expertise needed for constitutive modeling in solid mechanics.", "keywords": ["Large Language Models (LLMs)", "Physics-constrained neural networks", "Constitutive Artificial Neural Networks (CANNs)", "Automated model generation", "Data-driven solid mechanics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc15ccfc0cd1c76a949b30e35fe88e4aa2cca36d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GenCANN, a framework that leverages LLMs to automatically design CANNs for modeling the mechanical behavior of materials. By combining CANNs with the automation and flexibility of LLM-based code generation, the approach enables on-demand creation of physics-constrained neural networks tailored to specific materials and datasets. Experiments show that the LLM-generated models match or exceed the accuracy and generalization performance of manually engineered CANNs, demonstrating the potential of LLMs as autonomous designers of scientific, physics-informed machine learning models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper utilizes LLM to automatically design CANNs for modeling the mechanical behavior of materials.\n\n- The authors evaluate their approach on three representative benchmarks (brain, rubber, and skin)."}, "weaknesses": {"value": "- The technical contribution is limited. The work directly integrates two existing ideas (CANNs and LLM-based code generation). There is no deeper theoretical exploration or experimental analysis of the system (physical consistency, generation success rate, failure cases, …). The writing reads more like an engineering report than a scientific research paper.\n\n- The experimental scope is narrow, being restricted to hyperelastic materials. This significantly underutilizes the potential breadth of LLM-driven scientific modeling. Broder scenarios should be included to justify the value and meaning.\n\n- According to the method section, the LLM seems only to be responsible for generating the CANN architecture, while other parts such as data preparation and training still remain manually configured. This makes the system resemble a “customized API” rather than a genuinely autonomous agentic framework."}, "questions": {"value": "- All examples in the paper are limited to simple MLP-based architectures. The LLM appears to only determine the number of layers and neurons. Can the framework generate architectures with more structural diversity or physical inductive bias (e.g., CNNs, GNNs, Transformers or operator networks)?\n\n- Regarding MLP generation, many existing techniques such as neural architecture search (NAS) and AutoML frameworks can already perform automatic architecture optimization. What are the concrete advantages of using an LLM over these established approaches in this specific context? The theoratical and experimental comparisons are necessary.\n\n- From the Table 2, all GenCANNs have more neurons than CANNs.  Does this mean that GenCANN just generates MLPs with more parameters rather than design the architecture reasonably?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4XHBv0xh5g", "forum": "hBoFfrg3HJ", "replyto": "hBoFfrg3HJ", "signatures": ["ICLR.cc/2026/Conference/Submission1061/Reviewer_cWjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1061/Reviewer_cWjb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760861943758, "cdate": 1760861943758, "tmdate": 1762915667590, "mdate": 1762915667590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for automating constitutive modeling in solid mechanics. The authors propose using an LLM to generate the complete code for a specialized, physics-constrained neural network architecture known as a Constitutive Artificial Neural Network (CANN). The proposed GenCANN approach is tailored on-demand to a specific material class and dataset. The framework combines easy-to-use LLM-based systems with highly accurate, physically consistent CANNs. The authors demonstrate their approach on three real-world experimental datasets comprising human brain tissue, rubber, and porcine skin. The experiments show that the LLM-generated GenCANNs achieve performance that is on par with or better than CANNs, while clearly outperforming a previous LLM-based approach."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The core contribution is simple, elegant, and highly effective. Letting an LLM generate a specialized, physics-informed model is a powerful paradigm. This approach leverages the LLM’s strength in code generation, and combines it with existing research on constitutive modeling techniques.\n- The paper is really well-written and easy to follow. The motivation is clear, the background is explained concisely, and the proposed method is presented logically. The figures are informative and effectively support the results. The authors accurately and concisely position their work within the existing literature and highlight the unique novelty and contribution of their method.\n- The paper is supported by a strong set of experiments on three distinct and challenging real-world datasets. The evaluation includes extrapolation to unseen invariants and stretches. In all experiments, the proposed model is on par with or better than existing CANNs, while being easier to handle. Similarly, it is significantly better than the LLM-based CSGA baseline, which has similar complexity for the user."}, "weaknesses": {"value": "- According to Table 2, the GenCANN architectures are consistently and significantly larger (i.e., more neurons and/or layers) than their manually-designed counterparts. For instance, the baseline CANN for skin has \"12, 12\" neurons per layer, while the GenCANN has \"128, 128, 64, 32\". This raises the question of whether the superior performance of GenCANN is due to the LLM's intelligent design or simply its increased model capacity. While the core contribution of automatically generating CANNs is not affected by this, the larger model architecture seems like a potentially unfair advantage of the GenCANN model.\n- The LLM design process is a bit opaque, and the risks and failure modes that are associated with it are not investigated. The paper successfully shows that an LLM can generate a high-performing CANN, but it does not explore cases where the LLM behaves unexpectedly or fully fails. Especially with complex LLM-generated code, such as the example CANN provided in the appendix, these failure modes are the be expected in some cases, and a way to deal with them would strengthen the method and its applicability.\n- Related to the previous point, the paper mentions that the LLMs are given three iterations to refine their proposed CANN, using R2 score as the main feedback. Here, it would be interesting to see how much (or if) this refinement actually helps, especially since the R2 score is a pretty sparse metric that does not provide much feedback to the LLM.\n- All experiments seem to be based on one random seed. A more rigorous scientific evaluation with more repetitions would validate the statistical significance of the method and, potentially, give insight into the frequency of failure for the LLM-generated solutions."}, "questions": {"value": "- Could the authors please comment on the significant difference in architectural complexity between the GenCANNs and the baseline CANNs? How much of the observed performance improvement can be attributed to the LLM's design choices versus the fact that the GenCANNs are simply larger models? It would strengthen the paper to include results from a baseline CANN that is manually scaled up to a similar complexity as the GenCANN.\n- The iterative refinement process is an interesting component of the proposed framework. Could the authors provide more detail on how the LLM improves its generated code over multiple rounds given only a single R2 score as feedback? An example showing the evolution of a generated CANN from one iteration to the next would be very illuminating. Similarly, have other iterative approaches been considered?\n- What are the failure modes of this approach? Were there instances where the LLM generated code that was syntactically incorrect, did not adhere to the required physical constraints, or simply failed to train effectively? How common are these failures?\n- In Figure 6, the CSGA baseline performs poorly on the pure shear data (which is in-distribution) but seems to extrapolate better than the baseline CANN in other regions. Failing in-distribution and generalizing well out-of-distribution is a pretty uncommon behavior. Is there an intuition for these specific results?\n- In Figure 7, the test data points seem to be missing or mis-labelled (i.e., there are only circles denoting training data, no squares for the test data in the plots). Could the authors update this figure to better showcase the relation between training and test data?\n- From the provided CANN example, it is not directly clear how large the CANN design space actually is. Can the authors provide a bit of clarification on the kind of decisions that need to be made by the LLM, maybe by highlighting the “interesting” choices in the generated example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9xHgTpg7Ku", "forum": "hBoFfrg3HJ", "replyto": "hBoFfrg3HJ", "signatures": ["ICLR.cc/2026/Conference/Submission1061/Reviewer_JyT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1061/Reviewer_JyT9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396692065, "cdate": 1761396692065, "tmdate": 1762915667462, "mdate": 1762915667462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for automating constitutive modeling in solid mechanics. The authors propose using an LLM to generate the complete code for a specialized, physics-constrained neural network architecture known as a Constitutive Artificial Neural Network (CANN). The proposed GenCANN approach is tailored on-demand to a specific material class and dataset. The framework combines easy-to-use LLM-based systems with highly accurate, physically consistent CANNs. The authors demonstrate their approach on three real-world experimental datasets comprising human brain tissue, rubber, and porcine skin. The experiments show that the LLM-generated GenCANNs achieve performance that is on par with or better than CANNs, while clearly outperforming a previous LLM-based approach."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The core contribution is simple, elegant, and highly effective. Letting an LLM generate a specialized, physics-informed model is a powerful paradigm. This approach leverages the LLM’s strength in code generation, and combines it with existing research on constitutive modeling techniques.\n- The paper is really well-written and easy to follow. The motivation is clear, the background is explained concisely, and the proposed method is presented logically. The figures are informative and effectively support the results. The authors accurately and concisely position their work within the existing literature and highlight the unique novelty and contribution of their method.\n- The paper is supported by a strong set of experiments on three distinct and challenging real-world datasets. The evaluation includes extrapolation to unseen invariants and stretches. In all experiments, the proposed model is on par with or better than existing CANNs, while being easier to handle. Similarly, it is significantly better than the LLM-based CSGA baseline, which has similar complexity for the user."}, "weaknesses": {"value": "- According to Table 2, the GenCANN architectures are consistently and significantly larger (i.e., more neurons and/or layers) than their manually-designed counterparts. For instance, the baseline CANN for skin has \"12, 12\" neurons per layer, while the GenCANN has \"128, 128, 64, 32\". This raises the question of whether the superior performance of GenCANN is due to the LLM's intelligent design or simply its increased model capacity. While the core contribution of automatically generating CANNs is not affected by this, the larger model architecture seems like a potentially unfair advantage of the GenCANN model.\n- The LLM design process is a bit opaque, and the risks and failure modes that are associated with it are not investigated. The paper successfully shows that an LLM can generate a high-performing CANN, but it does not explore cases where the LLM behaves unexpectedly or fully fails. Especially with complex LLM-generated code, such as the example CANN provided in the appendix, these failure modes are the be expected in some cases, and a way to deal with them would strengthen the method and its applicability.\n- Related to the previous point, the paper mentions that the LLMs are given three iterations to refine their proposed CANN, using R2 score as the main feedback. Here, it would be interesting to see how much (or if) this refinement actually helps, especially since the R2 score is a pretty sparse metric that does not provide much feedback to the LLM.\n- All experiments seem to be based on one random seed. A more rigorous scientific evaluation with more repetitions would validate the statistical significance of the method and, potentially, give insight into the frequency of failure for the LLM-generated solutions."}, "questions": {"value": "- Could the authors please comment on the significant difference in architectural complexity between the GenCANNs and the baseline CANNs? How much of the observed performance improvement can be attributed to the LLM's design choices versus the fact that the GenCANNs are simply larger models? It would strengthen the paper to include results from a baseline CANN that is manually scaled up to a similar complexity as the GenCANN.\n- The iterative refinement process is an interesting component of the proposed framework. Could the authors provide more detail on how the LLM improves its generated code over multiple rounds given only a single R2 score as feedback? An example showing the evolution of a generated CANN from one iteration to the next would be very illuminating. Similarly, have other iterative approaches been considered?\n- What are the failure modes of this approach? Were there instances where the LLM generated code that was syntactically incorrect, did not adhere to the required physical constraints, or simply failed to train effectively? How common are these failures?\n- In Figure 6, the CSGA baseline performs poorly on the pure shear data (which is in-distribution) but seems to extrapolate better than the baseline CANN in other regions. Failing in-distribution and generalizing well out-of-distribution is a pretty uncommon behavior. Is there an intuition for these specific results?\n- In Figure 7, the test data points seem to be missing or mis-labelled (i.e., there are only circles denoting training data, no squares for the test data in the plots). Could the authors update this figure to better showcase the relation between training and test data?\n- From the provided CANN example, it is not directly clear how large the CANN design space actually is. Can the authors provide a bit of clarification on the kind of decisions that need to be made by the LLM, maybe by highlighting the “interesting” choices in the generated example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9xHgTpg7Ku", "forum": "hBoFfrg3HJ", "replyto": "hBoFfrg3HJ", "signatures": ["ICLR.cc/2026/Conference/Submission1061/Reviewer_JyT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1061/Reviewer_JyT9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396692065, "cdate": 1761396692065, "tmdate": 1763034156849, "mdate": 1763034156849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted work, *GenCANN*, proposes a framework in which an LLM automatically generates a physics-constrained neural network for constitutive modelling. Specifically, the framework provides a template for constructing Constitutive Artificial Neural Networks (CANNs) tailored to user-specified materials and datasets. The authors evaluate the LLM-generated code on hyperelastic deformation problems involving rubber, skin, and brain tissue under various loading conditions. Results are compared against both a manually designed CANN and a Scientific Generative Agent adapted for constitutive modelling (CSGA, Tacke et al., 2025)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is very well written and clearly structured, making it easy to read and follow.  \n2. The experimental section is comprehensive, covering multiple material types (e.g., skin, rubber) and including both real and synthetic datasets. The authors also test generalisation to unseen loading conditions and visualise the results neatly."}, "weaknesses": {"value": "Despite the clarity and reasonable set of experiments, the paper has several major weaknesses regarding its _significance_ and _originality_.\n 1. Limited contribution \n\nThe core contribution appears minimal. The authors essentially provide a prompt template for the LLM to generate code implementing a CANN. The method section is very brief (roughly half a page) and mainly describes the two-part LLM prompt (summary of continuum mechanics + skeleton code).  However, this lacks genuine research insight or technical contributions. There is no clear takeaway from the experiments beyond demonstrating that the LLM can replicate existing successful architectures (CANN).\n\n2. Unfair experiments\n\nThe claims of performance improvement over human-designed CANNs are questionable. For example, the paper states in the abstract: “LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalisation to unseen loading scenarios and extrapolation to large deformations.”\nHowever, this performance gain appears to result from _larger network architectures_ chosen by the LLM (e.g., 128–128–64–32 units) compared to the smaller manually designed models (e.g., 12–12 units).  This is the case for the skin data experiment (see ~L426) and for the rubber ones (see ~L372). Increased capacity naturally improves accuracy, but it's just a hyperparameter. Therefore, the comparison as presented is not insightful.\n\n3. Lack of scientific insights\n\nBeyond demonstrating that an LLM can generate working CANN code, the paper offers little analysis or interpretation. There is no exploration of the right way to integrate LLMs for scientific code generation, how prompts influence performance, or which design patterns emerge in the generated code. As a result, the scientific contribution remains minimal and superficial.\n\nMinor comment - the first two figures are not referenced in the text."}, "questions": {"value": "1.  L158 states, “This approach reduces the tensor-to-tensor mapping to a compact scalar regression, enforces thermodynamic consistency, and improves interpretability.”  Could you kindly clarify exactly how CANN enforces thermodynamic consistency or interpretability?  \n2. _Section 2.3:_ Please explain why CSGA performs worse than CANN. What limitations account for its lower accuracy? Are these models less physics-informed or less constrained, for example? This is relevant as CSGA is one of the primary comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qE5ES6Iwrs", "forum": "hBoFfrg3HJ", "replyto": "hBoFfrg3HJ", "signatures": ["ICLR.cc/2026/Conference/Submission1061/Reviewer_GdJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1061/Reviewer_GdJT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670172234, "cdate": 1761670172234, "tmdate": 1762915667307, "mdate": 1762915667307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"GenCANN,\" a framework where an LLM is prompted to generate and iteratively refine a physics-constrained neural network (CANN) for material modeling. The system is evaluated on three mechanics datasets, where it is shown to match or exceed the performance of \"human-designed\" baselines.\n\nWhile the goal of automating scientific modeling is relevant, the paper in its current form is not suitable for publication. Its central claims are invalidated by a combination of critical factors: 1) The problem formulation is a \"toy-level\" task that is orders of magnitude simpler than established code-generation/agentic (eg, MLE bench) benchmarks, making the LLM's success unsurprising. 2) The experimental methodology is fatally flawed, primarily by failing to account for the stochasticity of LLM outputs and by using confounded, unfair baselines. 3) The work is fundamentally irreproducible as submitted, lacking the necessary artifacts (prompts and conversation logs) to verify its claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important and popular application area: the use of LLMs to automate and lower the barrier to entry for complex constitutive modeling tasks.\n\nThe proposed framework, which combines LLM-based code generation with a physics-informed constitutive model ML framework (CANN), is clearly presented."}, "weaknesses": {"value": "- The premise of using LLMs as code-generating/agents for scientific tasks is not novel; in fact, the paper's own literature review cites numerous recent examples. The specific task delegated to the LLM is a simple, \"fill-in-the-blanks\" hyperparameter selection for a small regression model within a predefined, human-authored code skeleton. This is far simpler than rigorous, established benchmarks like SWE-bench (which requires fixing real-world GitHub issues) or MLE-bench (which involves end-to-end ML competitions). Given that LLMs are known to struggle on these complex benchmarks, their success on this paper's highly constrained, simple regression task is entirely expected and provides no new insight.  \n\n- The core method involves an LLM in a stochastic 3-hop refinement loop, where the \"best-performing version is kept\". However, the results in Table 1 are presented as single, deterministic $R^2$ scores. The paper makes no mention of if running this 3-hop-trials multiple repeats, nor does it report any mean, standard deviation, or variance for its results. This is a critical omission. As presented, the results are not scientific findings; they are single, unreplicated cherry-picks or \"lucky runs.\"  \n\n- The comparison to a \"human-designed CANN\" is not clearly stated. The paper states that these baselines are static, pre-existing models taken from prior literature (e.g., \"the best CANN reported in the literature (Pierre et al., 2023)\" and \"the initial CANN publication (Linka et al., 2021)\"). This is an \"apples-to-oranges\" comparison. The GenCANN is a 3-hop search algorithm allowed to optimize for the specific dataset, while the \"human\" baseline is just a single, static point from previous results.  \n\n- The paper's own analysis reveals the confounding variable that likely explains its results: the LLM-generated models are simply massively larger than the baselines. For the Skin dataset, the paper admits the GenCANN's (128-128-64-32 layers) superior accuracy \"likely reflects its larger architecture\" compared to the baseline's (12-12 layers). The finding that \"a much larger network achieves a better R2 score on a regression task\" can be trivially found by human too (without nearly no effort) and does not show any advantage of \"LLM-auto-design\".\n\n- The paper's own data shows that $R^2$ is a flawed and non-robust feedback signal. For the synthetic rubber dataset, Table 1 reports a \"perfect\" R2=1.00 for both the GenCANN and the baseline. However, Figure 6(c) clearly shows the exact same baseline model failing catastrophically (high relative error) at the x-axis boundaries. The aggregate $R^2$ score completely hides this critical failure. Using this highly-compressed metric to guide the LLM's refinement does not make sense-how did it lead to a better results in Figure 6, while the $R^2$ is also 1 for a poor architecture? \n\n- The work is unverifiable. The authors do not provide the exact prompts or the full conversation logs from the 3-hop refinement process. The \"Exemplary GENCANN IMPLEMENTATION\" shows only the final product, not the process, and the \"Reproducibility Statement\" links only to code repositories, not these critical generative artifacts."}, "questions": {"value": "- Did the authors run the 3-hop refinement experiment only once for each dataset? If not, what are the mean and standard deviation of the R2 scores over (e.g.) 10 or 20 independent trials? Without this, the results in Table 1 are meaningless.\n\n- How can the authors justify comparing a 3-hop search algorithm (GenCANN) against a single, static, published baseline (CANN)? A fair baseline would be a human expert given the same 3-hop refinement process, or a standard hyperparameter optimization (e.g., random search) run with a similar budget.\n\n- Given that Figure 6 clearly demonstrates that R2 hides catastrophic model failures at the boundaries, why was this aggregate metric chosen as the sole feedback signal for the LLM, rather than a more robust, physics-aware metric (e.g., max relative error)?\n\n- Will the authors provide the full, unedited prompts and complete conversation logs for all refinement trials to allow for verification of the generative process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qeFXYsK6Vh", "forum": "hBoFfrg3HJ", "replyto": "hBoFfrg3HJ", "signatures": ["ICLR.cc/2026/Conference/Submission1061/Reviewer_aqh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1061/Reviewer_aqh8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987561302, "cdate": 1761987561302, "tmdate": 1762915667172, "mdate": 1762915667172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}