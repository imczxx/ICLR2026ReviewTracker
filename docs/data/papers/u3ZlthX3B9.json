{"id": "u3ZlthX3B9", "number": 18008, "cdate": 1758282877705, "mdate": 1759897139715, "content": {"title": "PreDiff: Leveraging Data Priors to Enhance Time Series Generation with Scarce Samples", "abstract": "The fundamental motivation for time series generation tasks lies in addressing the pervasive challenge of data scarcity. However, we have identified a critical limitation: existing time series generation models are prone to substantial performance degradation when trained on limited data. To tackle this issue, we propose a novel framework that integrates data priors to enhance the robustness and generalization of time series generation under data-scarce conditions. Our framework is structured around a two-stage pipeline: pre-training and fine-tuning. In the pre-training stage, the model is trained on synthetic time series datasets to learn data priors, which encode the fundamental statistical properties and temporal dynamics of time series data. Subsequently, during the fine-tuning stage, the model is refined using a small-scale target dataset to adapt to the specific distribution of the target domain. Extensive experimental evaluations demonstrate that our framework mitigates performance degradation caused by data scarcity, achieving state-of-the-art results in time series generation tasks. This work not only advances the field of time series modeling but also provides a scalable solution for real-world applications where data availability is often limited.", "tldr": "An approach that effectively leverages data priors to overcome quality degradation in time‐series generation under data‐scarce conditions.", "keywords": ["Time Series Generation", "Diffusion Model", "Data Scarcity"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd547f6598b1e40e3ded85eff486d908b6a22447.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies time series generation models, particularly diffusion models, which suffer performance degradation when trained on scarce data. To address this, the authors propose PreDiff, a two-stage training framework. The first stage pre-trains a diffusion model on a large synthetic prior dataset), on the latter half of the denoising process. The second stage fine-tunes the model on the small target dataset), on the initial half of the process, with the parameters from the first stage frozen. The authors claim this method effectively leverages general priors to mitigate overfitting and achieves state-of-the-art results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Training Heuristic**: The core idea of splitting the diffusion process at a point t0​ for pre-training and fine-tuning is a novel and clever heuristic. The intuition of learning varying time series data structure from a large prior and, then learning details from the target data is conceptually sound.\n\n2. **Strong Empirical Results**: The ablation in Figure 3(a) is the most compelling part of the paper. It clearly demonstrates that the proposed two-stage split method outperforms simpler alternatives like \"Pre-training Only,\" \"Fine-tuning Only,\" and \"Datasets Mixing,\" which validates the efficacy of the proposed training strategy over these baselines. Also, the tables 1 and 2 show that the proposed algorithm outperforms several baseline algorithms."}, "weaknesses": {"value": "1. **Some Results Weakening the Motivation**: The paper's core premise is to resolve data scarcity. However, the authors state in Section 5.5 (and show in Table 5) that the method performs well when the prior data is close to the target data. This implies that one must already have access to a large, well-matched dataset or a synthetic generator that knows the target's core properties. I think this is a form of transfer learning from a known, similar source, which somewhat contradicts the \"data scarcity\" scenario where such well-matched priors are, by definition, unavailable.\n\n2. **Lack of Technical Rigor and Theoretical Grounding**: \n - In Section 3, the paper defines $\\mu_\\theta$​ as the model predicting the mean of the reverse process. However, the loss functions $L_{\\text{pre}}$​ (Eq. 2) and $L_{\\text{ft}}$​ (Eq. 3) train this model to predict the noise $\\epsilon$ (i.e., $|| \\mu_\\theta (\\cdot) -\\epsilon||^2$. This is a bit confusion between the mean-predictor $\\mu_\\theta​$ and the noise-predictor $\\epsilon_\\theta$​, demonstrating a lack of technical precision.\n\n - Un-grounded Heuristic: The main contribution, the $t_0$​ split, is presented as an ad-hoc heuristic without any theoretical reasons. The paper claims $[t_0​,T]$ maps to \"coarse structure\" and $[0,t_0]$ to \"fine details\" but provides zero evidence for this assertion."}, "questions": {"value": "1. Your results in Table 5 show that the prior seems to be \"highly relevant\" to the target for the proposed method to perform well. How do you argue this requirement with the paper's \"data scarcity\" motivation? I think a true scarce-data scenario implies such a large, well-matched prior is not available.\n\n2. Can you please clarify the critical inconsistency in your method? Section 3 defines $\\mu_\\theta$​ as the mean-predictor (denoising process), but Equations 2 and 3 train it to target $\\epsilon$​. Which is it? Also, please add $\\epsilon$ to the expectation. \n\n3. The core claim is that $[t_0, T]$ learns \"coarse\" priors and $[0, t_0]$ learns \"fine\" details. What theoretical or empirical evidence (e.g., visualizations of samples at step $t_0$​) can you provide to support this assertion? Without this, I think $t_0$​ split appears to be just an un-grounded, dataset-specific hyperparameter."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bqxspJo46g", "forum": "u3ZlthX3B9", "replyto": "u3ZlthX3B9", "signatures": ["ICLR.cc/2026/Conference/Submission18008/Reviewer_uvZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18008/Reviewer_uvZ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419492595, "cdate": 1761419492595, "tmdate": 1762927801761, "mdate": 1762927801761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses performance degradation in diffusion-based time series generation models under data scarcity. It proposes PreDiff, a two-stage framework: pre-training on a large data prior, synthetic or real, and fine-tuning on the target scarce dataset. Its claimed novelty is a specific \"split-step\" training strategy: pre-training focuses on later diffusion steps t_0 to T, global structure, while fine-tuning exclusively updates earlier steps 0 to t_0, fine details. Experiments aim to show improved generation quality under scarce data conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 Addresses the critical problem of TSG under data scarcity. The split-step training strategy, linking diffusion stages to transfer learning, is a conceptually distinct idea within this context.\n\n2 Motivation is clear. The two-stage framework is presented logically. Experiments use standard benchmarks and show empirical benefits, particularly in severe data scarcity 10% data."}, "weaknesses": {"value": "1 The central assumption linking diffusion steps to transferable features lacks strong theoretical support or broad empirical validation across diverse TS types/diffusion models presented here. Its effectiveness may be context-dependent.\n\n2 Success hinges on a relevant, high-quality data prior X_prior. The paper offers little practical guidance on selecting or assessing prior suitability, posing a major barrier to reliable application and risking negative transfer.\n\n3 Performance is likely sensitive to the split point t_0, yet guidance on its selection is minimal beyond empirical observation(Appendix I) . Lack of a principled selection method adds significant tuning difficulty.\n\n4 As the pre-train/fine-tune paradigm is standard, the overall contribution relies heavily on the split-step strategy. If its universality is questionable, the novelty might be seen as incremental.\n\n5 Comparison with ImagenFew highlights sensitivity to preprocessing, potentially affecting fairness and conclusions.\n\n6 Reliance on potentially massive priors implies significant computational costs, limiting practicality, especially if priors need tailoring per domain."}, "questions": {"value": "1 Can you provide stronger theoretical arguments or broader empirical evidence (e.g., across diverse datasets, different diffusion models) to support the universality of the hypothesis that pre-training high-noise steps and fine-tuning low-noise steps is an optimal transfer strategy for diffusion models?\n\n2 How can a practitioner reliably select an effective data prior X_prior for a given scarce target dataset X_target? What happens if a truly relevant large prior is unavailable? Please elaborate on the risk and mitigation of negative transfer.\n\n3 Given its likely sensitivity, how should t_0 be chosen in practice? Is there a risk that optimal t_0 heavily depends on the specific prior-target pair, requiring extensive tuning for each new application?\n\n4 Considering the need for a massive relevant prior, the cost of pre-training, and the tuning required for t_0, how practical is PreDiff for real-world users facing data scarcity?\n\n5 Could you clarify the impact of preprocessing differences and potentially provide results comparing PreDiff and ImagenFew under identical preprocessing settings to ensure fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8bTR4PL9RL", "forum": "u3ZlthX3B9", "replyto": "u3ZlthX3B9", "signatures": ["ICLR.cc/2026/Conference/Submission18008/Reviewer_G8K1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18008/Reviewer_G8K1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488536712, "cdate": 1761488536712, "tmdate": 1762927801289, "mdate": 1762927801289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this study, the authors investigate the problem of time series generation under low-data regimes. In particular, they propose a two-step training procedure including (1) pre-training on synthetically generated data and (2) fine-tuning on target data. The experiments on four real-world datasets show that the proposed approach is applicable even when fine-tuned on 10% of the available target data. Ablation studies are conducted to assess the advantages of a two-stage training procedure and of pre-training on snythetic data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors investigate a very interesting research question, trying to learn time series features purely from synthetic data.\n2) The paper is well structured and easy to follow.\n3) The authors evaluate their method on established benchmarks.\n4) The authors conduct ablation studies to provide insights on the effectiveness of the proposed components."}, "weaknesses": {"value": "1) The authors state that 'detailed configurations, hyperparameters, and implementation specifics for each baseline are meticulously documented in Appendix C' (see ll. 230-232), while they only provide basic information. \n2) The authors have mistakenly highlighted their method to achieve the best results in Table 6, while actually ImagenFew is superior. For instance, ED in the 70%, 40%, and 10% setup of Energy and ED and DTW in the 10% setup of Stocks. In light of this, the results of the work need to be treated with caution.\n3) The authors state that 'When selecting data priors, we aim to choose those that are highly relevant to the data distribution of the target task' (see ll. 407-408). This suggests that pre-training is task-specific and does not achieve generalisable time series features, which would be desirable. \n4) The authors do not report their results across multiple seeds to guarantee robustness. \n5) The authors do not support reproducibility by making their code publicly available for evaluation. \n6) The authors do not discuss the limitations of their work."}, "questions": {"value": "1) Is there a benefit of using synthetic data over real-world data from other domains than the target? \n2) Why is dataset mixing inferior to synthetic data only, as indicated by the results in Figure 3a? Does real-world data not increase the data diversity, which is beneficial to learn generalisable features?\n3) Why is the proposed method performing substantially worse when applying a full-range training? \n4) Why is the proposed method performing worse when increasing the training samples of the Monash dataset from 100k to 10M, as indicated by the results in Table 5?\n5) Finally, how does the proposed method advance the field of time series analysis? The authors state that 'one should prioritize priors whose distribution closely matches that of the target data' (see ll. 404-405). In light of this, it seems that models for time series generation are still task-specific. However, it would be desirable to have a single, task-agnostic model that can be pre-trained on synthetic data once and be applied to any downstream task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qa25JNQeIT", "forum": "u3ZlthX3B9", "replyto": "u3ZlthX3B9", "signatures": ["ICLR.cc/2026/Conference/Submission18008/Reviewer_L4JQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18008/Reviewer_L4JQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837220935, "cdate": 1761837220935, "tmdate": 1762927800908, "mdate": 1762927800908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PreDiff introduces a two-stage diffusion based framework for time series generation under data scarcity. It first pretrains on synthetic priors  to capture general temporal structures, then fine-tunes on limited real data to adapt to specific domains. The method aims to mitigate degradation in diffusion-based time series generation when data is scarce."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Data scarcity in time series is pervasive and underexplored in diffusion literature.\n* The two-stage pretrain, finetune strategy is analogous to foundation model training in NLP/CV.\n* Compared against 6 strong baselines \n* PreDiff outperforms baselines across multiple datasets and scarcity levels.\n* The pseudo-code and diagrams are well organized and readable"}, "weaknesses": {"value": "*  Conceptually similar to “pretrained diffusion + fine-tuning,” which has analogues in vision and text domains.\n*  No theoretical justification for why segmenting the diffusion process into [t0,T] and [0,t0] yields better transfer.\n*  Effectiveness may rely on the quality of external priors rather than intrinsic model improvements.\n*  The paper mentions varying priors but doesn’t deeply analyze how prior target similarity influences results.\n*  Missing visual analysis. Few qualitative samples of generated time series are shown to demonstrate realism.\n\nI believe these things are easy to add and can increase the value of the paper."}, "questions": {"value": "* How sensitive is PreDiff to the choice of segmentation point t0?\n\n* Can the method generalize to multimodal or irregularly sampled time series?\n\n* Does pretraining on synthetic priors ever lead to overfitting or “prior bias” in domains with very different dynamics?\n\n* what about many datasets present in UCR dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6YyM0DwQ63", "forum": "u3ZlthX3B9", "replyto": "u3ZlthX3B9", "signatures": ["ICLR.cc/2026/Conference/Submission18008/Reviewer_chFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18008/Reviewer_chFj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945997914, "cdate": 1761945997914, "tmdate": 1762927800444, "mdate": 1762927800444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}