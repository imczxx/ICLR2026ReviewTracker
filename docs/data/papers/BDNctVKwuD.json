{"id": "BDNctVKwuD", "number": 14507, "cdate": 1758237535745, "mdate": 1759897366097, "content": {"title": "Improving Feature Alignment in ConvNets using ContrastiveCAMs and Core-Focused Cross-Entropy", "abstract": "Despite the ubiquity of modern deep learning, accurate explanations of network predictions remain largely elusive. HiResCAM is a popular interpretability technique used to visualize attention maps (i.e., regions-of-interest) over input images. In this paper, we theoretically show a limitation of HiResCAM: the HiResCAMs for a given input are not uniquely determined, allowing an arbitrary spurious shift by a common matrix $M$ while corresponding to the same prediction. We further propose *ContrastiveCAMs*, which are invariant to the spurious shift $M$ hence improving robustness of explanations, while additionally providing granular class-versus-class explanations. With the additional granular explanations, experiments reveal that networks often focus on regions unrelated to the class label. To address this issue, we leverage the knowledge of core image regions and propose *Core-Focused Cross-Entropy*, an extension of cross entropy, which encourages attention on core regions while suppressing unrelated regions, improving feature alignment. Experiments on Hard-ImageNet and Oxford-IIIT Pets show that ContrastiveCAM provides more faithful attention maps and our method effectively improves feature alignment by primarily extracting predictive performance from core image regions.", "tldr": "We develop and leverage faithful interpretability along with core-region masks to improve feature alignment in image classification tasks.", "keywords": ["Interpretability", "Alignment"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2116eb6dcb9057cd9b506b85ebe817edafb0e122.pdf", "supplementary_material": "/attachment/5bfc49ac73f16ada2c634280a501fe35cc4b0f3b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a interpretability framework to improve feature alignment in CNNs. The authors theoretically demonstrate that HiResCAM explanations are not unique because of the softmax activation, and adding a shift via a common matrix \n can distort attention maps. To solve this, they propose ContrastiveCAMs, which remove this redundancy by producing shift-invariant, attention maps that more accurately highlight discriminative image regions. Experiments show that models depend on non-core, irrelevant regions, specially when regions occupy small portions of images.They then develop Core-Focused Cross-Entropy, a modified loss function that suppresses attention to non-core areas while also making target features more relevant. The method performed good across multiclass and binary classification tasks and some segmentation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The work is theoretically grounded. They theoretically show that HighResCAM explanations are not uniquely determined because of arbitrary shifts via softmax invariance, and provide formal proofs. \n- The paper proposes a solution by connecting interpretability output with training objective to improve feature alignment. Such as actionable interpretability technique is impressive."}, "weaknesses": {"value": "- This paper investigates CNNs, not ViTs. I think the whole field has shifted to ViTs long time ago. I am not saying CNNs are obsolete, but having ViT variants (along with CNN variants) are a must now. What about CLIP models? \n- The paper investigates one (out of many) variation of CAM (here, HighResCAM ), and HighResCAM is not even a published work. I am not sure whether this problem is worth investigating in the first place. What about other CAM methods? What about other explanation methods (saliency-based)?\n- The finding is not novel, in essence similar works have observed the same thing, see [R1]. This work is not even cited. \n- There are many ways of explanation-guided learning. See [R2]. The authors did not consider these works in their comparisons. Is their method better to [R1, R2 methods]? Does it work on other explainability methods? \n- There are no results on ImageNet. The authors report on Hard-ImageNet but not ImageNet. Does the method improve performance on the ImageNet? \n- The introduction lacks motivation. It lists some works and their importances, and then jumps directly to \"In this work, we develop...\".\n- The related work section provides some similar works but the most important part of that section is missing; the authors should clearly state the difference between the related works and their own work, how they tackle the problems and the limitations of those works, and why their method is better.\n\n\n[R1] Consistent Explanations by Contrastive Learning\n[R2] Studying How to Efficiently and Effectively Guide Models with Explanations"}, "questions": {"value": "I think this paper is not ready for ICLR due to the weaknesses mentioned. In particular, it investigates only CNNs, and a very special case (HighResCAM) of a very special case (CAM) of the wide variety of explainability methods. Furthermore, the paper lacks motivation, it has problems in how the related work is written, and there are many comparisons with key works missing. My decision will therefore be a reject."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No issues"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OUdrc84xOF", "forum": "BDNctVKwuD", "replyto": "BDNctVKwuD", "signatures": ["ICLR.cc/2026/Conference/Submission14507/Reviewer_BTYW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14507/Reviewer_BTYW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638654696, "cdate": 1761638654696, "tmdate": 1762924902712, "mdate": 1762924902712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the reliability of HiResCAM, a commonly used deep learning interpretability method that generates attention maps highlighting image regions important for a model’s predictions. The authors theoretically demonstrate  that HiResCAMs are not uniquely determined and admit arbitrary, spurious\n\nTo overcome this, the authors propose ContrastiveCAM, a new visualization technique that is invariant to the spurious shift and provides class-versus-class contrastive explanations, offering more precise insight into what differentiates one class from another.\n\nUsing these improved explanations, they discover that networks frequently attend to irrelevant image regions. To correct this, they introduce Core-Focused Cross-Entropy, a modified loss function that encourages attention on core (label-relevant) image regions and suppresses attention elsewhere, thereby improving feature alignment between visual regions and class semantics.\n\nExperiments on Hard-ImageNet and Oxford-IIIT Pets show that ContrastiveCAM produces more faithful and robust attention maps, and that Core-Focused Cross-Entropy leads to better predictive performance derived from semantically meaningful regions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors identify two main issues in existing methods. From a theoretical perspective, they reveal a limitation of HiResCAM, showing that its attention maps are not uniquely determined. To overcome this, they propose ContrastiveCAM, which eliminates this ambiguity. They also observe that networks often focus on irrelevant image regions, and to address this, they introduce Core-Focused Cross-Entropy, a loss function that encourages attention on label-relevant areas.\n\nExperiments conducted on three different datasets demonstrate the effectiveness of their approach, particularly the Core-Focused Cross-Entropy loss, in improving interpretability and performance."}, "weaknesses": {"value": "The authors discuss the limitation of HiResCAM only from a theoretical perspective, without providing experimental evidence to validate their claim.\n\nFurthermore, they do not convincingly demonstrate that ContrastiveCAM outperforms other modern CAM-based methods. Their comparisons are limited to GradCAM, which is relatively outdated, while many recent and more advanced CAM variants could have been included for a more comprehensive evaluation. In addition, the paper lacks experiments directly comparing ContrastiveCAM and HiResCAM, making it difficult to assess the claimed improvements.\n\nFinally, although the authors claim to have improved feature alignment in convolutional networks, their experiments are conducted only on ResNet. Evaluating their approach on a broader range of convolutional backbones would provide stronger and more generalizable evidence for their conclusions."}, "questions": {"value": "Number of examples used per dataset for quantitative results is unclear — needs clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aIt32SdD5R", "forum": "BDNctVKwuD", "replyto": "BDNctVKwuD", "signatures": ["ICLR.cc/2026/Conference/Submission14507/Reviewer_Thjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14507/Reviewer_Thjn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798536716, "cdate": 1761798536716, "tmdate": 1762924902160, "mdate": 1762924902160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces an improvement over HiResCAM methodology through contrastive alignement. Experiments are conducted on Hard ImageNet. and Pets and Pascal VOC. It is mostly compated to GradCAM and HiResCAM. ResNet50 backbone is only used for validation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The description of the work is clear. And the attribution-based explanations are important research field."}, "weaknesses": {"value": "This work has a lot of issues that I am worried about:\n\n- first claim is that HiResCAM are popular. Unfortunately I could not find that even this method was published. Only an arxiv version from 2021. And even so, they are cited by 208 works on Google Scholar, and when we will compare it to other CAM methods such as GradCAM (over 30k) and GradCAM++ (over 4k). \n\n- Lack of baselines, GradCAM is a really outdated method, and recent attribution method are LeGrad [1], OMENN [2] or CheferLRP [3].\n\n- Lack of other backbones in experimentations, especially ViTs. Other work do have them. \n\n- Lack of contextualization, broader discussion about LRPs, other CAMs and other attribution-based methods such as B-Cos [4] are missing. \n\n- Visualizations of explanations are not convincing, and there is no experimentation to prove it is better. \n\n- There is no user study to showcase that users better perceive those explanations. \n\n- No XAI benchmarks such as FunnyBirds [5]\n\n[1] Bousselham, Walid, et al. \"Legrad: An explainability method for vision transformers via feature formation sensitivity.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n\n[2] WrĂłbel, Adam, MikoĹ Janusz, and Dawid Rymarczyk. \"OMENN: One Matrix to Explain Neural Networks.\" arXiv preprint arXiv:2412.02399 (2024).\n\n[3] Chefer, Hila, Shir Gur, and Lior Wolf. \"Transformer interpretability beyond attention visualization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[4] Böhle, Moritz, Mario Fritz, and Bernt Schiele. \"B-cos networks: Alignment is all we need for interpretability.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[5] Hesse, Robin, Simone Schaub-Meyer, and Stefan Roth. \"Funnybirds: A synthetic vision dataset for a part-based analysis of explainable ai methods.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."}, "questions": {"value": "Seeing missing baselines, general not backed up by the literature statements and poor comparisons, I do not got into much methodological details as I do not that see this work can improved during rebuttal period enough to be ready for publishing. \n\nI put more details in the weaknesses section showcasing the limitations of the work, especially contextualization, validation of the method and strong statements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "py9TMzdxWz", "forum": "BDNctVKwuD", "replyto": "BDNctVKwuD", "signatures": ["ICLR.cc/2026/Conference/Submission14507/Reviewer_KJVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14507/Reviewer_KJVo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862057556, "cdate": 1761862057556, "tmdate": 1762924901785, "mdate": 1762924901785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}