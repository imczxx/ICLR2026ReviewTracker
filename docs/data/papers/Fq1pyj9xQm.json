{"id": "Fq1pyj9xQm", "number": 11250, "cdate": 1758194346686, "mdate": 1763386471886, "content": {"title": "Length Generalization with Log-Depth Recurrent Units", "abstract": "Length generalization remains a persistent challenge for neural networks: recurrent models tend to suffer from positional biases, while Transformers are constrained by fixed computational depth. Regular languages provide a frequently used testbed for evaluating length generalization, as any sequence can be exactly verified to determine its label. We propose the Log-Depth Recurrent Unit (LDRU), which composes token embeddings through a learned pairwise operator inspired by monoid composition, yielding uniform logarithmic depth across tokens. On 21 regular tasks, consisting of standard benchmarks and new prefix languages, the LDRU achieves 100\\% out-of-distribution accuracy on 18 tasks and at least 96\\% on the remaining 3, consistently outperforming recurrent and attention-based models. These results establish the LDRU as an effective architecture for length generalization on regular languages and a promising direction for compositional sequence modeling.", "tldr": "", "keywords": ["length generalization", "log-depth recurrent unit", "ldru", "regular languages", "reduction", "automata theory", "monoids"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/628d2d1d1f118b164c9b9dd29b201ca4b91eadef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Log-Depth Recurrent Unit (LDRU), a novel recurrent architecture that composes token embeddings via a learned pairwise reduction operator inspired by monoid composition, achieving logarithmic computational depth. The authors evaluate LDRU on 21 regular language tasks, including newly proposed prefix languages to test long-range dependencies. The model demonstrates near-perfect length generalization, achieving 100% OOD accuracy on most tasks and outperforming strong baselines such as RNNs, LSTMs, Transformers, RegularGPT, and state-space models. The work is well-motivated, clearly written, and provides strong theoretical grounding and empirical evidence. While its experiments are confined to synthetic regular languages and the practical efficiency of log-depth computation is not yet verified, the contribution represents a meaningful advance in systematic generalization and architecture design for sequence models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed LDRU is a novel architecture according to the Reviewer's expertise.\n- LDRU’s log-depth reduction mechanism is a clever hybrid of RNN recurrence and Transformer parallelization.\n- The design explicitly encodes compositional inductive bias linked to formal language theory.\n\n2. This paper provide strong empirical results, demonstrating the effectiveness of LDRU on systematic length generalization.\n- Comprehensive evaluation on 21 tasks, including new benchmarks.\n- Consistent and large performance gap over state-of-the-art baselines.\n\n3. Others: the authors introduce a benchmark (Prefix Languages) which provides a systematic way to test long-range dependency modeling. The paper is well-structured, with a clear logical flow from theory to method to experiment to analysis."}, "weaknesses": {"value": "1. Restricted Scope: The evaluation domain is narrow; all tasks are regular or near-regular, where compositional structure is simple. It’s unclear how LDRU would generalize to non-regular or natural language tasks where equivalence classes are not well-defined. Besides, it is doubtable how LDRU is compatiable with existing foundation language models.\n\n2. While $O(\\log n)$ depth is theoretically appealing, practical runtime or memory benchmarks (on GPUs) are not provided and the reduction tree might introduce non-trivial communication or synchronization overhead. Besides, no formal proof is given for why or when LDRU will learn correct compositions (though inspired by monoid theory)."}, "questions": {"value": "1. Please refer to the weaknesses part.\n2. The authors neatly compare LDRU with standard Transformers and RNNs on working complexity and depth. Can you provide some empirical results to show the superiority (or the tradeoff) of LDRU, in comparison with Transformers and RNNs?\n3. I think LDRU's performance does not show much improvment over LSTM (both near to $100\\%$ accuracy). Though LDRU is designed to be more effective on the processing depth ($O(\\log n)$ versus $O(n)$), the reviewer wonders whether LDRU beats LSTM by a large margin in some length generalization tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jJmIX34q7X", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Reviewer_mVcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Reviewer_mVcQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761242851212, "cdate": 1761242851212, "tmdate": 1762922409727, "mdate": 1762922409727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the log-depth recurrent unit (LDRU) architecture, which reduces information along the sequence dimension via a sequence of aggregations of pairs that requires log (in sequence length) depth to aggregate information from the entire sequence. This is compared to transformers and RNNs on length generalization as measured by synthetic regular language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting idea that combines some of the relative strengths of transformers and RNNs. \n2. The length generalization results with the new architecture seem consistently strong.\n3. The results interpreting the patterns of composition that are needed in the data for length generalization to arise are interesting."}, "weaknesses": {"value": "1. This paper neglects important related work (e.g. delta nets https://arxiv.org/abs/2406.06484 , PSMs: https://arxiv.org/abs/2506.10918 and especially log-linear attention: https://arxiv.org/abs/2506.04761). These papers present very similar methods with more general and scalable experimental results, although less of a focus on length generalization. The authors would need to carefully read and discuss this work and how it relates to this paper. Especially log-linear attention and PSMs present the idea of generic parallel scans for doing log-depth recurrence. \n2. The paper is missing comparisons of parameters + FLOPs across architectures. Adding the MLP layers will add parameters and make the networks larger than the baselines. This seems like it potentially makes the comparisons unfair as a result. Moreover in terms of actually understanding the cost of the new architecture, we need a more detailed analysis of the FLOPs required both for training and for inference. \n3. It seems that the architecture likely does not allow easy parallel training. We have seen in recent years that this is important for scalability (e.g. transformers + SSMs). It is not even obvious to me from the paper whether computation is being re-used during training, or if for each token during training the entire LRU over the sequence is being recomputed. Full description of the training complexity for next token prediction would be useful. \n4. The transformer baseline seems somewhat weak by only using NoPE positional embeddings. It is now fairly standard to compose sliding window RoPE layers with global NoPE layers for length generalization. The local layers are needed to create better representations of local tokens that is difficult with NoPE. Here is one example paper with dramatically better length generalization with better positional encodings: https://arxiv.org/abs/2402.01032. \n5. There is no strong state space model baseline. In particular, the gated delta net is the SoTA architecture and is substantially more expressive than Mamba and similar architectures. This baseline is needed. \n6. The results of LDRU without the non-linearity seem fairly similar to with the non-linearity. When it is linear, it seems that the LDRU may also become equivalent to some form of state-space/linear attention."}, "questions": {"value": "1. What is the performance of each model on the training distribution? It seems fair if we only care about testing generalization to train until there is 100% accuracy on short sequences before testing on longer ones."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CUecunBMM4", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Reviewer_hrDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Reviewer_hrDN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420384557, "cdate": 1761420384557, "tmdate": 1762922409201, "mdate": 1762922409201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Addressing scope, practical computation scaling/comparison, and additional Transformer PE baselines"}, "comment": {"value": "Dear Reviewers,\n\nWe would like to thank all of you for providing detailed feedback. We have uploaded a revised manuscript that we hope addresses the majority of concerns. Below, we summarize and respond to the three common themes across reviews: **scope**, **computational scaling**, and **Transformer baseline strength**.\n\nIn the revision, we (i) broaden evaluation beyond regular languages, (ii) add practical computational measurements on GPUs, and (iii) strengthen Transformer baselines with two recent positional encodings. All additions are highlighted in the manuscript: Fig. 3 (compute), Fig. 4 (ListOps), Table 2 (Transformer PE baselines), and Table 4 (NLP results), with further experimental details given in the appendix.\n\n### 1) Scope Beyond Regular Languages  [n6mp, B7Z7, E7NM, mVcQ]\n\n1. New NLP evaluations (8 sequence classification tasks, presented in Table 4).  \n    To evaluate generality beyond regular languages while maintaining a consistent modeling setup, we added eight standard NLP classification tasks: GLUE (CoLA, SST-2, QQP, MNLI, QNLI, MRPC), AG’s News, and DBPedia. These experiments are intended to demonstrate the viability of the LDRU outside regular-language settings. We compare against Transformers using three standard positional encodings (NoPE, Sinusoidal, ALiBi). All models are trained from scratch under identical budgets (optimizer, training steps), and we perform a small hyperparameter sweep for each architecture on AG’s News and reuse those settings for the remaining tasks.\n    \n    The LDRU performs comparably to the Transformer baselines with NoPE, Sinusoidal, and ALiBi (Table 4), and notably outperforms all baselines on QQP, MNLI (matched/mismatched), and DBPedia. We view this as initial evidence that the LDRU’s inductive structure can be effective on non-regular, real-text data, even though the model was originally motivated by regular-language structure. To maintain focus, we do not attempt generative modeling in this work; this is a natural direction for future research.  \n\n1. ListOps systematic generalization study (presented in Fig. 4)\n    We further evaluate the LDRU on ListOps across three dataset sizes (100k, 500k, 1M), comparing against BBT-GRC (a Tree-RvNN), Transformers with three positional encodings (NoPE, Sinusoidal, ALiBi), and LSTM (Fig. 4; additional datasets in the appendix). The performance ordering is stable across sizes:\n    \n\tBBT-GRC > LDRU > Transformer (ALiBi) $\\approx$ LSTM > Transformer (NoPE/Sinusoidal).\n\t\n\tThe LDRU consistently outperforms the LSTM and all Transformer variants on out-of-distribution (length-extrapolation) tests. Interestingly, BBT-GRC’s advantage increases with more data. We hypothesize that the GRC cell’s inductive structure is better aligned with ListOps—particularly for learning the SM operator—whereas the operator we propose is designed to approximate recurrent processing and is therefore better aligned with associative structure (as in regular languages). Even in this misaligned setting, the LDRU remains competitive with mainstream architectures and demonstrates partial length generalization.\n\nFinally, we note that in both the NLP and ListOps experiments, we introduced an associativity-regularization term for the LDRU and found via hyperparameter sweeps that this improved performance, suggesting that associative biases are a promising future research direction.\n\n---\n### 2) Practical Computational Scaling [B7Z7, hrDN, mVcQ] (Fig. 3)\n\nWall-clock time, FLOPs, throughput, and memory usage on GPU (RTX 6000 Ada)  \nTo address concerns regarding practical cost, Fig. 3 reports four measurements for the LDRU, a simple RNN, and a Transformer (all ~160k parameters) with batch size 32 and sequence lengths up to 2048:\n\n- End‑to‑end step time (forward+backward): The empirical curves show that LDRU’s runtime scales with sequence length as predicted by its parallel reduction structure and becomes faster than the RNN at longer lengths.\n- Throughput vs. length: The LDRU exhibits stepwise throughput drops at powers of two due to requiring $\\lceil \\log_2 (n)\\rceil$ reduction steps. When the sequence crosses a power-of-two boundary, the reduction depth increases, and throughput temporarily drops, increasing again as the sequence length approaches the next power of two. The other plots also reflect this trend.\n- Estimated FLOPs: Profiling indicates that the LDRU uses more FLOPs than a simple RNN, but scales with the same order.\n- Peak memory usage: The LDRU consumes more memory than the RNN, as it must store intermediate results from the parallel reduction.\n\nThese results clarify the runtime/memory trade-offs and position the LDRU as an architecture that provides a recurrent form of computation with increased parallelizability: offering an intermediate choice between RNNs and Transformers.\n\n---\n[Continued in following comment]"}}, "id": "cvgsRZ6WL0", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763386616503, "cdate": 1763386616503, "tmdate": 1763386616503, "mdate": 1763386616503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Log-Depth Recurrent Units (LDRU) and shows that it can generalize well in multiple regular language tasks. LDRU is effectively implemented as a balanced binary tree recursive neural network with a new gated cell as the parametrized binary operator."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* It demonstrates that binary balanced tree-based recursive neural networks with a modern gated cell can perform nearly perfect in multiple parity tasks."}, "weaknesses": {"value": "* The contribution seems incremental compared to RegularGPT. RegularGPT seems to be essentially the same idea (a balanced binary tree recursion - except uses Transformer as the recursive cell) - and also performs near perfectly (better in the original paper). The improvement in this paper seems like due to minor implementational difference rather than exposing any theoretical leap. \n* Limited evaluation on anything else besides algorithmic regular language tasks. Does not show if it can be anything more than something that works well in \"toy\" tasks. While I do appreciate evaluation on algorithmic/synthetic tasks - particularly because they can be harder to \"hack\" by finding spurious shortcuts and easier to analyze - however, restricting the whole study to them when introducing a supposedly general purpose model restricts the scope severely. \n* Seems to miss a lot of critical related paper:\n   - First of all, LDRU seems like a neologism for Balanced Tree Recursive Neural Networks - which have been explored in multiple prior works [1,2,3].\n   - Moreover it misses any comparison with more recent proposals like Recursion-in-Recursion (RIR) which also proposes logarithmic-depth recurrence/recursion and shows effectiveness in algorithmic tasks - like propositional logical inference, listops (in multiple OOD settings) - alongside benchmarks in LRA and other [4] and utilizes a modern gated recurrent cell inspired from Ordered Memory. At the very least, I would be curious how the proposed method compares against RIR in ListOps and Logical Inference on the same length generalization and argument generalization settings in RIR. But to be frank - even if the benchmarks are provided I would likely not lean towards acceptance unless the other weaknesses are rebutted well enough. \n\n\n[1] Neural Tree Indexers for Text Understanding - Munkdhalai et al. \n\n[2] Sliced Recurrent Neural Networks - Yu et al.\n\n[3] On Tree-Based Neural Sentence Modeling - Shi et al. \n\n[4] Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability - Ray Chowdhury et al."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0t6VhswaD", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Reviewer_E7NM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Reviewer_E7NM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569692295, "cdate": 1761569692295, "tmdate": 1762922408657, "mdate": 1762922408657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Cont. shared concerns"}, "comment": {"value": "### 3) Stronger Transformer Baselines [n6mp, hrDN]\n\nResponding to the request for more modern Transformer setups, we added ALiBi and randomized RoPE as PEs that are designed for length generalization (Table 2):\n\n- ALiBi provides modest gains on several Tomita and prefix tasks but does not yield uniform generalization.\n- Randomized RoPE produces the strongest baseline performance on $D_{12}$, generally improves $D_n$​ and Even Pairs, but regresses on Modular Arithmetic, Tomita-3, and Tomita-4. This suggests that while randomized RoPE improves length extrapolation, it does so for specific subclasses of regular languages.\n\nAcross tasks, these positional encodings reorder Transformer variants but do not eliminate the underlying failure mode: Transformer performance remains task-dependent, with large drops on Even Pairs, Modular Arithmetic, and several prefix languages. The LDRU’s gains reflect its architectural structure rather than reliance on a particular positional encoding.\n \n---\n\nWe appreciate the reviewers’ insights, which have led to substantial improvements in the manuscript. While this general response focuses on the major shared concerns, we recognize that reviewers also raised specific points that are not addressed here. We respond to all of those individually in the per-reviewer comments. We believe the expanded evaluations and analyses meaningfully strengthen the work, and we thank the reviewers again for their time and consideration.\n\nThe Authors"}}, "id": "E67sIJiCFp", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763386646425, "cdate": 1763386646425, "tmdate": 1763386646425, "mdate": 1763386646425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Log-Depth Recurrent Unit (LDRU), a novel neural architecture designed to address the challenge of out-of-distribution (OOD) length generalization. The LDRU is inspired by the algebraic structure of monoids and processes sequences using a learned pairwise operator in a balanced reduction tree, resulting in $O(\\log n)$ computational depth. The authors evaluate the LDRU on a comprehensive suite of 21 regular language tasks, including standard benchmarks and a new \"prefix language\" benchmark they introduce to specifically test long-range dependencies. The empirical results are very strong, showing that the LDRU achieves 100% OOD accuracy on 18/21 tasks and near-perfect (>=96%) on the remaining three, consistently outperforming RNN, LSTM, Transformer, and RegularGPT baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The LDRU architecture is with a theoretical motivation. Grounding the architecture in the concept of monoid composition and the reduction algorithm provides an alternative to standard recurrent (state-based) or attention-based (fixed-depth) approaches.\n2. The LDRU's performance on the 21 regular tasks is better than other architectures (RNN, Transformer, LSTM)."}, "weaknesses": {"value": "1. The primary weakness of this paper is its exclusive focus on regular languages. While the authors justify this as a rigorous and verifiable testbed, regular languages are the simplest class in the Chomsky hierarchy. The paper provides a compelling proof of concept, but it leaves the most critical question unanswered: does this approach scale to more complex, non-regular tasks? It is entirely unclear if the LDRU's inductive bias, which aligns so well with monoids (and thus regular languages), will be beneficial or detrimental for context-free or, more importantly, context-sensitive languages like natural language.\n2. While the $O(\\log n)$ depth and $O(nd^2)$ work complexity are good, the paper doesn't compare LDRU directly to baselines (like RNNs or Transformers) in terms of wall-clock time or throughput."}, "questions": {"value": "Could you conduct more experiments to address weaknesses 1 and 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qCyt0NDCAf", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Reviewer_B7Z7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Reviewer_B7Z7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910975129, "cdate": 1761910975129, "tmdate": 1762922408217, "mdate": 1762922408217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new architecture, Log-Depth Recurrent Unit (LDRU). LDRU provides a trade-off between width (i.e. work complexity) and depth between current architectures such as RNN and Transformer. Experiments on a wide range of generalization tasks demonstrate the improved performance of LDRU over RNN, LSTM, Transformer and RegularGPT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents an interesting way to model sequences in a binary-tree manner. It is intuitive that the computation overhead can be reduced in this way. There are many experiments in both the main pages and the appendix, and many examples in the appendix to help"}, "weaknesses": {"value": "1. I have only a little background on DFA and I feel the preliminary section is technically heavey and hard to follow. I would suggest to relate the preliminaries with examples in natural language as it is the final testbed for proposed LDRU.\n\n2. The evaluation tasks of this work are not on natural language. I wonder if it is possible to model natural language with LDRU (for example, next-token generation, reasoning, long-context modeling, etc). The tasks in this paper are specific tasks, not general language modeling.\n\n3. The baselines in this paper seem not enough, as RNN, LSTM, and Transformer have been introduced many years ago, and positional encodings are deactivated as described in line 236. I wonder if more recent variants could be compared with the proposed LDRU."}, "questions": {"value": "1. I don't quite get how the proposed neural architecture is related to the finite automatons and prefix languages. I think I missed this part, but I only understand the proposed LDRU as a new kind of attention that is not fully-connected?\n\n2. At first glance I thought this work is about long-context modeling, but turns out it's not. Could you let me know what's the difference between the area of this research from long-context modeling (e.g., context window of Mistral is 8192, but we can extend it with some techniques)? I guess I'm assigned to review this paper because of my background on long-context modeling, and this could be why I cannot follow this paper well, as the tasks in this paper are not standard long-context benchmarks.\n\n3. Are the tasks too simple that LDRU (and some baselines) can achieve 100.0 +- 0 performance? Are we reaching the limit of this research direction as we're achieving 100.0?\n\n4. If we can do binary tree, how about ternary and more generally n-ary trees? Will it be challenge because LDRU is relying on pairwise operations (which only applies for binary tree structure)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZrG5vSuzmC", "forum": "Fq1pyj9xQm", "replyto": "Fq1pyj9xQm", "signatures": ["ICLR.cc/2026/Conference/Submission11250/Reviewer_n6mp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11250/Reviewer_n6mp"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946672717, "cdate": 1761946672717, "tmdate": 1762922407850, "mdate": 1762922407850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}