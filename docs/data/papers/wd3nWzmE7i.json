{"id": "wd3nWzmE7i", "number": 8297, "cdate": 1758077822923, "mdate": 1759897793687, "content": {"title": "Multimodal Few-Shot Point Cloud Segmentation via Agent Adaptation and Discriminative Deconfusion", "abstract": "Few-shot 3D point cloud segmentation (FS-PCS) aims to leverage a limited amount of annotated data to enable the segmentation of novel categories. Most existing studies rely on single-modal point cloud data and have not fully explored the potential of multimodal information. In this paper, we propose a novel FS-PCS framework, Multimodal Agent Adaptation and Discriminative Deconfusion (MAD).  MAD incorporates three modalities: images, point clouds, and category text embeddings. To fuse multimodal information, we propose the Multimodal Semantic Agents Correlation Aggregation (M-SACA) module, which fuses multimodal features through agent-level correlation and uses text affinity for category semantic learning. To alleviate semantic gaps between the support set and query set in multimodal features, we propose the Semantic Agents Prototypes Adaptation (SAPA) module, which generates multimodal agents for query and support sets, adjusting prototypes to adapt the query feature space. To alleviate intra-class confusion, we introduce the Discriminative Deconfusion (DD) module, which preserves intra-class consistency through residual adapters and generator weights. \nExperiments on the S3DIS and ScanNet datasets demonstrate that MAD attains state-of-the-art performance, improving mIoU by \n3%–7%. Our method can significantly improve segmentation results and suggest valuable insights for future studies. The code will be publicly available.", "tldr": "", "keywords": ["Few-Shot 3D Point Cloud Segmentation; multimodal data; Semantic Agents Correlation Aggregation; Discriminative Deconfusion; Semantic Agents Prototypes Adaptation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ee019e2ff8760410bbdc8e7402c1f057bac6df9.pdf", "supplementary_material": "/attachment/95912b4a3d8f15141f04f01efc4c4fbf87e91374.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Multimodal Agent Adaptation and Discriminative Deconfusion, a framework for multimodal few-shot 3D point cloud semantic segmentation. MAD simultaneously models multimodal information in both query and support sets. The framework consists of three main components: Multimodal Semantic Agents Correlation Aggregation, Semantic Agents Prototypes Adaptation, and Discriminative Deconfusion. Experiments on the S3DIS and ScanNet benchmarks demonstrate that MAD consistently improves performance compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is generally clear, and the figures help in understanding the technical flow.\n2. The framework design is well-motivated, addressing challenges about cross-modal fusion, semantic gap adaptation, and intra-class confusion.\n3. Extensive comparisons on two benchmarks and detailed ablation studies demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. In the Discriminative Deconfusion section (lines 313–315), the definition of the generator is unclear. While generator-based adaptive weighting improves results, the paper does not provide intuition for why it performs better than averaging or summing.\n2. The ablation study should also include an analysis of Eq. (13), specifically by comparing the predictions of p^{TxT} and p^{DD}.\n3. Jointly fusing query and support multimodal correlation features is computationally expensive. As reported in the paper, MAD’s computational cost is noticeably higher than that of MM-FSS. A deeper analysis of scalability to larger or more complex scenes would strengthen the work.\n4. Minor grammar issues remain. For example:\n   - '... fuses multimodal features for query and support set fusion through multimodal semantic agents correlation aggregation …' (lines 83–85).\n   - '... eliminate discriminative deConfusion from the residual connection …' (lines 309–310)."}, "questions": {"value": "Please refer to the weakness part and address the concerns there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R80pHjsqvv", "forum": "wd3nWzmE7i", "replyto": "wd3nWzmE7i", "signatures": ["ICLR.cc/2026/Conference/Submission8297/Reviewer_dyZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8297/Reviewer_dyZ2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760889601453, "cdate": 1760889601453, "tmdate": 1762920227285, "mdate": 1762920227285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issues in the few-shot 3D point cloud segmentation (FS-PCS) field, where existing methods rely on single-modal data, suffer from insufficient multi-modal fusion, semantic gaps, and intra-class confusion, by proposing a Multi-modal Agent Adaptation and Discriminative Deconfounding framework (MAD). This framework integrates three modalities—image, point cloud, and text embeddings—and achieves synchronized multi-modal association feature fusion between query and support sets through the M-SACA module. The SAPA module adaptively adjusts prototypes to mitigate semantic gaps, while the DD module eliminates intra-class confusion via residual adapters and dynamic weighting. Experiments on the S3DIS and ScanNet datasets show that compared with the current state-of-the-art method MM-FSS, MAD improves mIoU by 3%–7%, and the code will be made publicly available. The manuscript is well-structured, the technical approach is clear, focuses on the core challenges of the FS-PCS field, and has definite research value."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel multimodal alignment perspective: Existing multimodal FS-PCS methods only handle query-side correlations. This work simultaneously constructs multimodal correlations and aligns prototypes for both support and query, making it conceptually more unified and technically a natural progression from correlation modeling → prototype adaptation → discriminative deconfounding in a three-step pipeline. \n2.Significant and consistent improvements: Outperforms the strong baseline MM-FSS across multiple settings on S3DIS and ScanNet, including 1-way/2-way and 1-shot/5-shot scenarios, with clear average gains and visualization results. \n3.Clear modularity and thorough ablation: Independent ablations are conducted on the three main modules, loss terms, modality contributions, and generator weights, allowing the sources of performance to be pinpointed."}, "weaknesses": {"value": "The manuscript mentions 'the first exploration of simultaneously fusing multimodal association features of query and support sets,' but it does not provide a detailed comparison with the multimodal fusion approaches used in vision-language models (such as 3D VLMs and GFS-VL), nor does it clearly define the fundamental differences between 'agent association aggregation' and existing cross-modal attention mechanisms, which may result in a lack of uniqueness in the proposed innovation"}, "questions": {"value": "1. Clarity of assumptions and dependencies: What assumptions does the method make about the availability of images and class texts during the testing phase? The experiments seem to assume that all three modalities are provided at inference; robustness and degradation strategies when any modality is missing are not reported. Is the text encoder (LSeg's text/visual encoder) frozen during training/testing? The impact of different freezing strategies on performance is not specified.2. Statistical significance and variance: Few-shot learning is usually sensitive to episode sampling. Tables 1 and 2 do not provide mean ± variance or confidence intervals, nor report statistical tests.3. Generalization and extrapolation: Validation is conducted only on indoor datasets S3DIS/ScanNet. There is no evaluation on outdoor LiDAR data or cross-dataset transfer (e.g., A→B few-shot), making it difficult to assess robustness to changes in scene or sensor.4. Fairness with strong multimodal baselines: The paper mentions that all three use the \"same 2D-aligned pre-trained backbone,\" but specific details on alignment/pre-training (data, image resolution, handling alignment errors) are not fully described; differences in 2D-3D alignment quality could directly affect the results.5. Method details still need to be filled in: Implementation details of Fagent (clustering radius/adjacency definitions, clustering and reverse assignment after FPS, sensitivity to NA/NP) are not fully explained; hyperparameter sensitivity and ablation studies (e.g., β, τ, channel dimension D′) are missing; time/memory costs and episode-level throughput are not reported, only FLOPs/parameters.6. Writing and terminology consistency: The paper occasionally mixes SA-CAPA with SAPA/M-SACA; a unified naming is recommended. Some statements and figure captions could be further polished to facilitate reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zkwRzjXky1", "forum": "wd3nWzmE7i", "replyto": "wd3nWzmE7i", "signatures": ["ICLR.cc/2026/Conference/Submission8297/Reviewer_aMUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8297/Reviewer_aMUB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726822780, "cdate": 1761726822780, "tmdate": 1762920227015, "mdate": 1762920227015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of few-shot 3D point cloud segmentation by proposing a multimodal framework that leverages both RGB-text and 3D-text pairs during training. The key idea is to learn modality-agnostic cross-modal alignment, such that at test time, the model can perform segmentation using only 3D-text input, even though 2D data was available during training. The core contributions include: a modality-complementary contrastive learning loss that aligns RGB-text and 3D-text embeddings jointly, a modality-invariant meta-learning pipeline for better generalization across modalities, experiments on ScanNet, S3DIS, and ScanRefer demonstrating the effectiveness of their method under both standard and generalized few-shot settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Setting: The proposed training-testing mismatch (RGB-text used only during training, 3D-text at test time) is practical and not widely explored in current literature. This setting is well-motivated and could inspire further work.\n- Multimodal Fusion Strategy: The method uses a well-designed dual-branch architecture with contrastive losses that promote shared representation learning across modalities. The idea of learning from richer modalities but testing on limited modalities is both elegant and impactful.  \n- Good Clarity: Figures and architecture diagrams are clear; the training/testing pipelines are easy to follow. The motivation is well-articulated."}, "weaknesses": {"value": "- Incremental Contribution: While the training-testing mismatch setting is interesting, the core method is essentially a combination of known techniques: contrastive learning for modality alignment, standard meta-learning with prototypes, feature fusion with transformers\nThese parts are not fundamentally novel on their own. There’s no architectural breakthrough or new theoretical formulation.\n- Over-Reliance on Pretrained CLIP: The RGB and text encoders are frozen CLIP backbones. The method’s performance may heavily depend on this pretrained knowledge, making it unclear how much the proposed training scheme contributes beyond CLIP’s strong prior.\n- Besides, the authors use CLIP as text encoder, but do not cite the paper. \"Category names are encoded as text embeddings using the text encoder in LSeg Li et al. (2022)\", while in LSeg they mentioned \"and we use the pretrained Contrastive Language–Image Pre-training (CLIP) throughout (Radford et al., 2021)\"\n- Missing Real-World Applications: The paper motivates the method as applicable to real-world tasks where only 3D data is available at test time, but no concrete application or real robot / AR / scene understanding demonstration is provided.\n- Limited Analysis of Modal Gap: The paper assumes that RGB-text and 3D-text can be aligned effectively, but doesn’t deeply analyze or quantify the actual modality gap. A t-SNE or retrieval-based analysis comparing embeddings could help."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uubNj7ZJES", "forum": "wd3nWzmE7i", "replyto": "wd3nWzmE7i", "signatures": ["ICLR.cc/2026/Conference/Submission8297/Reviewer_XLc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8297/Reviewer_XLc1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900790292, "cdate": 1761900790292, "tmdate": 1762920226360, "mdate": 1762920226360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors present a method for few-shot point-cloud segmentation. As far\nas I can see, the contributions are the integration of textual\nguidance and joint embedding of query and support sets. Experiments on\ntwo datasets show very impressive results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Results are quite good.\n+ Integration of textual guidance is an interesting and popular\n  direction."}, "weaknesses": {"value": "- The method description is not very detailed and clear.\n  + The problem definition does not present a segmentation problem but\n    rather a classification problem.\n  + Authors use N to represent the number of samples in the query set\n    as well as the number of categories.\n  + The definition of the mask $M$ and where it comes from is\n    unclear. It seems like the mask is the segmentation and both the\n    query and the support sets have it.\n  + Notation starts being used without introduction, e.g. $F_q^i$\n    on line 179.\n  + It is unclear how authors obtain a \"background\" text embedding?\n  + Why are IF and UF defined and why are they different?\n  + What does \"agent clustering\" mean and what are \"agent points\"?\n    \n- This paper is very difficult to read. The proposed method is\n  explained through many acronyms and new module names. This makes it\n  very difficult to follow. Furthermore, justification and intuition\n  behind different choices do not seem to be well\n  substantiated. On top, the paper uses a very heavy notation but it\n  is not always well explained."}, "questions": {"value": "- Is it necessary to have a different name for each model component?\n  In my opinion, this is not necessary and even further, makes it\n  extremely difficult to follow the text.\n- I do not understand the difference between the 1st and 2nd claimed\n  contribution on lines 83 to 89. Can you please explain why these are\n  two different contributions?\n- I do not understand why extensive experiments showing the proposed\n  model achieves good results is a main contribution. Isn't this\n  simply the necessary condition for writing a scientific article?\n- I could not understand your paper despite spending considerable\n  amount of time. I can suggest you to spend time to explain the\n  method clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lpU2DtFRk5", "forum": "wd3nWzmE7i", "replyto": "wd3nWzmE7i", "signatures": ["ICLR.cc/2026/Conference/Submission8297/Reviewer_Mgfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8297/Reviewer_Mgfs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027831162, "cdate": 1762027831162, "tmdate": 1762920225905, "mdate": 1762920225905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}