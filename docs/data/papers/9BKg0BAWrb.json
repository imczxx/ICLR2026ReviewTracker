{"id": "9BKg0BAWrb", "number": 981, "cdate": 1756826197165, "mdate": 1759898232945, "content": {"title": "K²-Agent: Co-Evolving Know-What and Know-How for Hierarchical Mobile Device Control", "abstract": "Existing mobile device control agents often perform poorly when solving complex tasks requiring long-horizon planning and precise operations, typically due to a lack of relevant task experience or unfamiliarity with skill execution. We propose $\\textbf{K²-Agent}$, a hierarchical framework that models human-like cognition by separating and co-evolving declarative (\"knowing what\") and procedural (\"knowing how\") knowledge for planning and execution. K²-Agent’s high level reasoner is bootstrapped from a single demonstration per task and runs a Summarize–Reflect–Locate–Revise (SRLR) loop to distill and iteratively refine task-level declarative knowledge through self-evolution. The low-level executor is trained with our curriculum-guided Group Relative Policy Optimization (C-GRPO), which (i) constructs a balanced sample pool using decoupled reward signals and (ii) employs dynamic demonstration injection to guide the model in autonomously generating successful trajectories for training. On the challenging AndroidWorld benchmark, K$^2$-Agent achieves a new $\\textbf{state of the art}$ with $\\textbf{76.7\\% success rate}$, ranking $\\textbf{1st}$ among all methods $\\textbf{using only raw screenshots and open-source backbones}$. Furthermore, K²-Agent shows powerful dual generalization: its high-level declarative knowledge transfers across diverse base models, while its low-level procedural skills achieve competitive performance on unseen tasks in ScreenSpot-v2 and Android-in-the-Wild (AitW).", "tldr": "", "keywords": ["LLM/VLM Agent", "Self-Evolving Agent", "Mobile Device Control", "Decision-Making"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c8ede6d67e8e620b6811f6934eea6bcb3044c8b.pdf", "supplementary_material": "/attachment/5384847eef3bda427b7dbdcf157952d673930aba.zip"}, "replies": [{"content": {"summary": {"value": "The paper **“K²-Agent: Co-Evolving Know-What and Know-How for Hierarchical Mobile Device Control”** introduces a cognitively inspired hierarchical agent that explicitly separates declarative (“know-what”) and procedural (“know-how”) knowledge to improve performance on mobile device control tasks. The system combines a **symbolic planner** that refines task knowledge through the **SRLR (Summarize–Reflect–Locate–Revise)** self-evolution loop, and a **reinforcement learning executor** trained with **C-GRPO (Curriculum-Guided Group Relative Policy Optimization)**. This dual-loop architecture enables co-evolution: the planner learns better strategies from execution feedback, while the executor gains structured guidance from the planner’s task decomposition. Experiments on the AndroidWorld benchmark demonstrate that K²-Agent achieves a **76.7% success rate**, surpassing prior open-source baselines while requiring minimal supervision (one demonstration per task). The work bridges symbolic reasoning and embodied learning in a cognitively grounded way, suggesting potential for broader applications in generalist AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel cognitive framework:** The clear distinction and co-evolution of declarative and procedural knowledge mirror human learning mechanisms, offering both conceptual clarity and practical benefits.  \n- **Methodological innovation:** The SRLR loop provides a systematic mechanism for reflective reasoning and self-correction, while C-GRPO elegantly handles data imbalance and sparse rewards through curriculum design.  \n- **Strong empirical results:** The system achieves state-of-the-art performance on AndroidWorld with extremely low data requirements, showing both efficiency and robustness.  \n- **Interpretable learning process:** The explicit task knowledge base and revision history improve transparency and allow fine-grained analysis of reasoning and execution errors.  \n- **Broader relevance:** The cognitive analogy to human dual-memory systems and the hierarchical design are relevant to multiple domains beyond mobile control, such as robotics and multimodal planning."}, "weaknesses": {"value": "- **Limited evaluation diversity:** The experiments focus primarily on AndroidWorld; results on other benchmarks (e.g., WebArena, mobile navigation, or real-device tasks) would strengthen claims of generalization.  \n- **Scalability concerns:** The SRLR loop may involve significant overhead as task complexity grows, especially if symbolic reasoning steps become large or interdependent.  \n- **Ambiguity in error localization:** The “Locate” phase of SRLR appears manually defined or heuristic-based; it is unclear how reliably the system identifies and generalizes error points without human intervention.  \n- **Comparative analysis:** The work could benefit from deeper comparisons with alternative hybrid systems that combine reasoning and RL (e.g., ReAct, Voyager, or RPA-style architectures).  \n- **Reproducibility limitations:** Key implementation details—such as how knowledge base edits are represented or parameterized—are under-specified, making replication challenging for other researchers."}, "questions": {"value": "1. How scalable is the SRLR loop when applied to tasks requiring multiple interdependent app contexts (e.g., cross-app workflows)?  \n2. Could C-GRPO be adapted for domains beyond visual mobile control—such as web navigation or embodied robotics—and if so, what adjustments would be required?  \n3. Does the declarative knowledge base support compositional reuse, allowing the agent to combine skills learned in separate tasks into new ones?  \n4. How sensitive is the system to the initial demonstration quality—can it recover from a poorly demonstrated “seed” trajectory?  \n5. How are symbolic edits validated to prevent catastrophic forgetting or the accumulation of contradictory task rules over long SRLR iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "cY4uksTusg", "forum": "9BKg0BAWrb", "replyto": "9BKg0BAWrb", "signatures": ["ICLR.cc/2026/Conference/Submission981/Reviewer_UG52"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission981/Reviewer_UG52"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395062898, "cdate": 1761395062898, "tmdate": 1762915652431, "mdate": 1762915652431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles long‑horizon mobile device control and argues that agents should separate two kinds of knowledge for accurate planning and execution: (i) declarative “knowing what” for task planning, and (ii) procedural “knowing how” for precise UI actions. The proposed K²‑Agent is a two‑layer planner–executor:\n\nHigh‑level planner (know‑what): a training‑free VLM that bootstraps from one demonstration per task and maintains a textual task knowledge base (KG) updated by a Summarize Reflect Locate Revise (SRLR) loop. SRLR distills an initial plan from a demo, analyzes failures, identifies the wrong decision points, and applies atomic edits to the knowledge base.\n\nLow‑level executor (know‑how): a trainable VLM optimized with Curriculum‑guided GRPO (C‑GRPO). C‑GRPO (1) builds error‑decoupled replay pools for type vs. parameter errors, and (2) uses dynamic demonstration injection to prepend variable‑length expert prefixes for difficult samples; the policy is updated with GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The experimental results are strong. Especially, the KG graph being transferred to different VLMs, and the low-level executor being transferred to AiTW and ScreenSpot-v2 is interesting.\n* Proposing to train the low-level executor with reinforcement learning while refining the planner’s knowledge base through training-free self-evolution (SRLR) offers an efficient division between low-level actions and declarative knowledge planning & refinement."}, "weaknesses": {"value": "* The overall flow of section 4 itself was easy to understand, but specifically what each component is and how they operate is unclear in several parts. Specifically, (1) what 'atomic' edits are and how they are applied to the KG was unclear until looking at the appendix. (2) In dynamic demonstration injection, what 'variable length expert prefix' is unclear. (3) What the single demonstration that is provided is, and whether this is identical or different from the AndroidWorld benchmark is unclear\n* If the benchmark provides single expert demonstrations per task category, this may be providing more information than other results"}, "questions": {"value": "* What is the motivation of applying the dynamic length prefix? Why does this improve performance?\n* What is the dynamic length expert prefix and how is the length dynamically applied? For example,  if the expert prefix something like 'Be sure to answer in the format of ...', how do you dynamically change the length of this prefix?\n* Regarding the single demonstration provided, is the provided task excluded from the benchmark? Also, if a successful task is provided per category of the AndroidWorld benchmark, it seems not fair to say that this method utilizes less information than other methods that only use screenshots or A11y trees. \n* How is the KG constructed from the high-level planner utilized by the low-level executor? The reverse is clear to me, the demonstrations of the low-level executor fails is provided to the planner."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mkQIPZPUPQ", "forum": "9BKg0BAWrb", "replyto": "9BKg0BAWrb", "signatures": ["ICLR.cc/2026/Conference/Submission981/Reviewer_tNhe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission981/Reviewer_tNhe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720009600, "cdate": 1761720009600, "tmdate": 1762915652298, "mdate": 1762915652298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes $K^2$-Agent, a hierarchical framework for mobile device control that explicitly decouples declarative “know-what” (a high-level planner distilled via a Summarize–Reflect–Locate–Revise, SRLR, loop) from procedural “know-how” (a low-level executor trained with a curriculum-guided C-GRPO objective). On AndroidWorld (116 tasks across 20 apps), the authors claim state-of-the-art 76.7% success “ranking 1st … among methods using only raw screenshots and open-source backbones” and show transfer to ScreenSpot-v2 and Android-in-the-Wild (AitW) without fine-tuning. The intended scope is screenshot-only agents on Android emulator tasks, plus cross-benchmark skill transfer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear hierarchical decomposition (SRLR + C-GRPO) with reasonable mathematical formalization (PPO-style clipping; explicit content/format rewards).\n* The paper’s split between a high-level SRLR planner (“knowing what”) and a low-level C-GRPO executor (“knowing how”) isn’t just conceptual; you show the actual SRLR prompts and output formats. This makes the approach inspectable and reuse-friendly.\n* Even with screenshot-only I/O, the result on AndroidWorld is pretty strong as shown in Table 1.\n* The executor’s reward is specified precisely (format + content, Eq. (5) and total reward definition) and the action space is enumerated (Table 6), which helps reproduction and critique."}, "weaknesses": {"value": "* The authors mention that “Human experts achieve about 80% average success” on AndroidWorld without details (line 373): annotator count, expertise, task sampling, time limits, or inter-rater protocol.\n* The AndroidWorld results do not report of seeds, dispersion, or confidence intervals (CIs). Without multi-seed evaluation, the “SOTA” claim is statistically fragile.\n* The executor is trained on 606 single-step samples derived from 116 demos (Appendix B.2.2). Even with the stated seed-based split controls, the paper doesn’t quantify app- or template-level overlap between training demonstrations and evaluation task distributions; leakage through UI idiosyncrasies remains plausible without stricter controls or OOD app tests."}, "questions": {"value": "* How many seeds were used for Table 1 and Figure 1? Please report mean ± std over ≥3 seeds and add 95% CIs to plots. \n* Right now, it is hard to tell whether the gains come from C-GRPO itself or. from extra compute or curriculum variations. You should compare GRPO vs C-GRPO with everything else identical: model init, data, rollout length, batch size, optimizer, KL/clip settings, and exactly the same number of environment interactions (and similar wall-clock within ±5%). Also please report the learning curves of these runs.\n* Vary the key curriculum knob(s) you introduce (e.g., mixing ratios, scheduling temperature T if applicable) across a small grid (e.g., 3–5 values). For which hyperparameters is your method brittle or robust?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gfmp4eMAUl", "forum": "9BKg0BAWrb", "replyto": "9BKg0BAWrb", "signatures": ["ICLR.cc/2026/Conference/Submission981/Reviewer_2B17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission981/Reviewer_2B17"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967331968, "cdate": 1761967331968, "tmdate": 1762915652165, "mdate": 1762915652165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a new mobile control agents built on (V)LLMs with a hierarchical design. Specifically, the authors proposed a high-level planner that is capable of reasoning with the task goal, storing knowledge from execution and analysing failure patterns to better solve the problem. A new C-GRPO method is proposed to fine-tune the low-level execution policy for better performance in long horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well written and easy to follow.\n\nThe proposed method of C-GRPO is new.\n\nExperimental results on benchmark environment show positive gains.\n\nAblation study shows the effectiveness of some high-level designs proposed in this work."}, "weaknesses": {"value": "The overall novel in the developed technique is not strong. In the domain of AI planning with LLM/VLM, approaches like task decomposition, building memory and knowledge graph or reflective planning have been widely used in different applications. This work largely follows these patterns as well.\n\nSome technical design lacks sufficient motivations or analysis. It is unclear in some technical sections what the key problem targeted to address is and why the process is designed in the current way. Please see the following several detailed points:\n- Section 4.2.1: it is proposed to summarize the knowledge into rules or checklist, but there is no analysis on how robust this representation is. In general, what type of knowledge can the summarize induce? What are the unknow knowledge or failure cases that the summarize cannot induce? Will the summarizer induce wrong knowledge that harms the following steps?\n- Section 4.2.2 - Task-level: It would be better to illustrate what kind of root-cause of the failure can be captured by the reflection from the episode trajectory, and what cannot. Are there any cases where the VLM fails to identify the reason of the failure? For example, where the failure happens underlying in the operation system that cannot be directly observed from screenshots.\n- Section 4.2.3:\n  - Prepending a variable length expert prefix to the model input is used to gradually improve the training. However, the reason why the policy on original query got improved through training with query and several hints is less well explained. It would be better to show the performance like the reward of the original query with training on query + hints during the curriculum training process. This can improve the understanding of generalizing from query + hints to query alone.\n  - Another existing similar approach in RL to guide exploration is hindsight experience reply. It would be better to have a study on this approach also.\nOverall, the proposed system as whole seems provide gains in training and application, but the developed approach did not provide strong technical insights on why individually proposed component bring improved performance.\n\nFigure 5, there is no standard deviation on the reward in figure b."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aNLEzFS2An", "forum": "9BKg0BAWrb", "replyto": "9BKg0BAWrb", "signatures": ["ICLR.cc/2026/Conference/Submission981/Reviewer_MQ9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission981/Reviewer_MQ9r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995502413, "cdate": 1761995502413, "tmdate": 1762915652030, "mdate": 1762915652030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}