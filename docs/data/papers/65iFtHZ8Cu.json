{"id": "65iFtHZ8Cu", "number": 16329, "cdate": 1758263261158, "mdate": 1759897247268, "content": {"title": "Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval", "abstract": "Reflecting the greater significance of recent history over the distant past in non-stationary environments, $\\lambda$-discounted regret has been introduced in online convex optimization (OCO) to gracefully forget past data as new information arrives. When the discount factor $\\lambda$ is given, online gradient descent with an appropriate step size achieves an $O(1/\\sqrt{1-\\lambda})$ discounted regret. However, the value of $\\lambda$ is often not predetermined in real-world scenarios. This gives rise to a significant \\emph{open question}: is it possible to develop a discounted algorithm that adapts to an unknown discount factor. In this paper, we affirmatively answer this question by providing a novel analysis to demonstrate that smoothed OGD (SOGD)  achieves a uniform $O(\\sqrt{\\log T/1-\\lambda})$ discounted regret, holding for all values of $\\lambda$ across a continuous interval simultaneously. The basic idea is to maintain multiple OGD instances to handle different discount factors, and aggregate their outputs sequentially by an online prediction algorithm named as Discounted-Normal-Predictor (DNP). Our analysis reveals that DNP can combine the decisions of two experts, even when they operate on discounted regret with different discount factors.", "tldr": "", "keywords": ["Online Convex Optimization", "Discounted Online Learning", "Adaptive Algorithms"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fd6223dfc69c1ec45e87a052c121dc79ba71172.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies Discounted Online Convex Optimization (OCO) to minimize the λ-discounted regret. The\nauthors introduce a Smoothed OGD (SOGD) algorithm to achieve uniform discounted regret $O(\\sqrt{\\log(T)/(1-\\lambda)})$\nacross a continuous interval of discount factors without prior knowledge of $\\lambda$. They also provide a novel analysis of DNP-cu under discounted payoff settings, showing that it can combine experts operating under different discount factors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The application of DNP-cu to combine experts with different discount factors is novel and the analysis of DNP-cu under discounted payoffs (Theorem 2) is a key contribution.\n\n2. The paper provides a uniform regret bound across a continuous interval of $\\lambda$ with detailed and well-organized proofs in the appendix.\n\n3. The paper is well-structured with clear motivation, problem setup and technical exposition. The presentation of figures and algorithms help illustrate the main ideas.\n\n4. The authors provide clear statement of problem setting and experimental setup, supporting the reproducibility of both the theoretical and empirical findings."}, "weaknesses": {"value": "1. The experiments conducted in the paper only compare the performance of SOGD with typical OGD. It would be valuable to compare with other adaptive or meta-learning baselines.\n\n2. Computational Practicality: The theoretical bounds of this paper depend on several constants and assumptions, which may lead to limited practicality and expensive computational cost in high-dimensional or large-scale settings."}, "questions": {"value": "1. Can the problem be extended to settings with strongly convex or exp-concave losses? What regret bounds could be achieved?\n\n2. How sensitive is SOGD to the choice of $Z$ and $\\tau$? Are there practical guidelines for setting these parameters in real applications?\n\n3. How does SOGD compare empirically and theoretically with state-of-the-art algorithms with strong adaptive regret when evaluated under discounted regret?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AN1JbEdxA7", "forum": "65iFtHZ8Cu", "replyto": "65iFtHZ8Cu", "signatures": ["ICLR.cc/2026/Conference/Submission16329/Reviewer_uNa2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16329/Reviewer_uNa2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849650375, "cdate": 1761849650375, "tmdate": 1762926466622, "mdate": 1762926466622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the setting of online convex optimization with $ \\lambda $-discounted regret. Prior work assumes known $ \\lambda $ and shows online gradient descent achieves $O ( (1-\\lambda)^{-\\frac12} ) $ discounted regret. In this work the authors consider the setting of \\emph{unknown} $ \\lambda $ and show that a carefully designed aggregation of online gradient descent instances can achieve $ O ( \\log T (1-\\lambda)^{-1} ) $ discounted regret for any $ \\lambda $ in a defined continuous interval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper studies an important open problem: that of achieving discounted regret bounds with an unknown $ \\lambda $. The paper does a decent job of motivating that in some settings $ \\lambda $ is truly unknown and is not just a tunable hyperparameter.\n\n* In contrast to typical aggregation mechanism in meta algorithms with multiple instances of an online gradient descent or experts algorithm, this work relies on the less well-known mechanism of discounted normal predictor with conservative updates. The authors show that this aggregator, works used in a particular order of $ \\lambda $.\n\n* The authors also provide some empirical results that reflect their theoretical findings.\n\n* The authors rely on existing algorithms (e.g., aggregation with DNP-cu was already used in Zhang et al. (2022)). However, they prove an important property of DNP-cu: that it can aggregate across different discount factors."}, "weaknesses": {"value": "* There is a related line of work on online convex optimization with unbounded memory (Kumar, Dean, Kleinberg, NeurIPS 2023). One special case is $\\rho$-discounted infinite memory, where the loss in each round depends on the entire history of decisions and each past decision is weighted by a geometric factor of $\\rho$. This paper does not discuss similarities and differences from this line of work.\n\n* Algorithm 3 is hard to read - I had to keep jumping around to look at the algorithm and at the equations that it references. It would be easier for the reader if you wrote the equations inline.\n\n* Since the algorithm is the same in Zhang et al. (2022) but you prove an additional property of DNP-cu that allows it to be used in the $\\lambda$-discounted setting, can you discuss a bit more what are the differences between this work and Zhang et al.?"}, "questions": {"value": "* Please see the weaknesses section about lack of discussion on online convex optimization with unbounded memory. Could you discuss when using one model over the other is preferable? Can their techniques be extended to $\\lambda$-discounted regret? Can your techniques be used to derive results for their model?\n\n* Can you expand a bit more on why the ordering is crucial when aggregating the difference OGD instances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "353KfOU2S6", "forum": "65iFtHZ8Cu", "replyto": "65iFtHZ8Cu", "signatures": ["ICLR.cc/2026/Conference/Submission16329/Reviewer_hZvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16329/Reviewer_hZvg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877885403, "cdate": 1761877885403, "tmdate": 1762926466185, "mdate": 1762926466185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies discounted online convex optimization and demonstrates that it is possible to achieve regret results for unknown bounded discount factors. It achieves this by analyzing and utilizing the existing algorithm DNP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe paper creatively applies the DNP algorithm to the discounted optimization problem and remove the need to know the discount factor.\n\nQuality:\nThe submission seems technically correct. Experiments are a plus. The algorithms and the experimental settings appear reproducible.\n\nClarity:\nThe submission is clear in general.\n\nSignificance:\nTheoretical novel findings in the form of discounted regret results for unknown discount factors are obtained."}, "weaknesses": {"value": "I am leaning towards rejection. Below are the reasons.\n\nUtilizing DNP-cu seems to help in arriving to a clean result, but it seems any combiner could have worked. I am not sure if the issue of different discounted performance measures as explained in Figure 1 is as great as advertised. Since the combined $\\lambda$ values have a difference of $1/T$, the regret redundancy propagating due to mismatches seems to be finite at each combination node, similar to DNP-cu.\n\nAside from that, exponentially growing step-sizes and iterative combination of experts (with intermediate experts in the mix) are established methods in the literature."}, "questions": {"value": "Questions:\n\nPage 6 Line 301: is the effective window size the sum of the discount coefficients?\n\nPage 6 Line 305: for $\\lambda$, $\\tau$ is related to the lower-bound, while the footnote is talking about the upper-bound. Is there a mistake here?\n\nPage 8 Line 409: why $Z=1/T$, why not smaller?\n\n\nSuggestions:\n\nGive more substance to the need to use DNP as opposed to another mixture of experts algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5r79Zy81gh", "forum": "65iFtHZ8Cu", "replyto": "65iFtHZ8Cu", "signatures": ["ICLR.cc/2026/Conference/Submission16329/Reviewer_LMJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16329/Reviewer_LMJ1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937359335, "cdate": 1761937359335, "tmdate": 1762926465712, "mdate": 1762926465712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses online convex optimization (OCO) with discounted regret, where recent losses are weighted more heavily than distant ones via a discount factor $\\lambda$. Existing papers assume the discounting factor is known, whereas this manuscript considers the unknown $\\lambda$ case. The main idea is to run many instances of SOGD with a specific lambda, and then run a meta selection algorithm to aggregate the outcome."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a clear open problem in discounted OCO, by providing a discounted regret bound for uniform range of lambda. The learner does not know lambda a priori. \n\nThe technical challenges of the problem were clearly presented. The prior aggregation framework needed to operate under a uniform performance metric, whereas in this paper each expert has a different metric. \n\nProvide a step by step derivation on the motivation behind the algorithm design which is insightful.\n\n Provides rigorous theoretical analysis and complete proofs. Simulations are provided as well."}, "weaknesses": {"value": "-\tI encourage the authors to provide **more concrete** motivations on the relevance of the problem of unknown lambda, why it is practically relevant other than the mere theoretic interest. \n-\tAs mentioned in the intro, part of the motivation is that the user’s preference might change, indicating a time-varying lambda. Can the paper be extended to the time-varying lambda case? i.e. the learner has some feedback signal that is indicative of lambda, and can adapt itself to optimize the regret with time-varying lambda?"}, "questions": {"value": "-\t(17) (18) are not too obvious, providing more details will help. \n\n-\tCompare theorem 3 to theorem 1, the dependence on lambda is worse. I wonder whether the bound is tight or if there are potential ways to further improve it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RvWpuwDBkz", "forum": "65iFtHZ8Cu", "replyto": "65iFtHZ8Cu", "signatures": ["ICLR.cc/2026/Conference/Submission16329/Reviewer_DsSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16329/Reviewer_DsSC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962778285, "cdate": 1761962778285, "tmdate": 1762926465216, "mdate": 1762926465216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}