{"id": "WXFr8Tfj0J", "number": 3574, "cdate": 1757478505814, "mdate": 1759898080667, "content": {"title": "SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human Annotated Dataset and Fine-Tuned Instruction Generation", "abstract": "Many medical MLLMs post strong scores on curated VQA-style benchmarks yet still struggle on real clinical questions because their training/supervision expose them to too little clinically grounded knowledge and prevailing benchmarks contain too few diagnostic-reasoning Q&A items. We introduce \\textbf{SemiHVision}, a semi-human–validated multimodal instruction dataset built with a multimodal retriever; to our knowledge, this is the first dataset to leverage a unified image--text retriever to integrate real-world clinical information into data construction, thereby strengthening models' clinical diagnostic reasoning. Our pipeline retrieves image- and context-relevant evidence and performs retrieval-augmented synthesis to produce clinically grounded instruction Q&A and captions across major modalities (X-ray, CT, MRI, ultrasound, histopathology), while standardizing heterogeneous annotations into a training-ready schema. For model fine-tuning, we train \\textbf{SemiHVision-8B-AN}, surpassing public medical models like HuatuoGPT-Vision-34B (79.0% vs. 66.7%) and private general models like Claude3-Opus (55.7%) on standard benchmarks (SLAKE, VQA-RAD). On the \\textbf{JAMA Clinical Challenge}---a benchmark that directly probes diagnostic reasoning aligned with clinical practice---we evaluate SemiHVision-AN and it achieves a GPT-4 rubric score of 1.29, exceeding HuatuoGPT-Vision-34B (1.13) and Claude3-Opus (1.17), indicating the effectiveness of SemihVision datasets.", "tldr": "", "keywords": ["Medical Multimodality QA"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ea7f29ba9f0b5680b25e2deba98a053cbb79b17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identified two critical issues in current medical datasets for MLLM: (1) they rarely capture end-to-end workflows, and (2) they often lack multi-view evidence beyond image-text pairs like laboratory tests, medications, etc. To this end, this paper integrates several existing medical image datasets and curates them into a multimodal instruction dataset, SemiHVision, where each case links medical images to multi-view/multimodal clinical evidence and is organized by staged diagnostic workflow, aided by a lightweight image-text retriever. Reported gains cover standard VQA benchmarks and a rubric-graded JAMA Clinical Challenge, with the strong scores after annealing on expert cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper targets a clinically meaningful gap: training signals that mirror how radiologists and clinicians actually reason across views, slices, and clinical variables. The case-centric restructuring, explicit alignment of findings to ROIs and study views, and staged supervision are thoughtful and, if validated, likely to transfer beyond the reported tasks. I also appreciate the attempt to reduce shortcutting through normal/negative constructions and near-miss distractors, and the small human audit with inter-rater agreement that at least probes construction fidelity. These are good designs for moving from caption recall to case reasoning."}, "weaknesses": {"value": "I think the core problem of this paper is that existing experiments do not seem to validate the effectiveness of the proposed dataset (and curation pipeline). \n\n**1. [Major concern] Lack of ablation study.**\n\nThe key experiment I care about most is a head-to-head ablation to validate the effectiveness of the proposed data curation pipeline and the quality of SemiHVision dataset. Specifically, I was expecting to see such a comparison: \n- (i) an MLLM trained on a naïve mix of the data sources for curating SemiHVision. (So neither L1 nor L2 is addressed); \n- (ii) an MLLM trained on SemiHVision-L1, a version that removes multi-view clinical context (So L2 is not addressed);\n- (iii) an MLLM trained on SemiHVision-L2, a version that removes workflow structures in the paired texts. (So L1 is not addressed); \n- (iv) an MLLM trained on the final version of SemiHVision. \n\nThe four MLLMs are initialized in the same way, following exactly the same architectures and training procedure, trained with (at least approximately) equal number of data.\n\nThis experiment should directly validate the core contribution of this paper which claims to obtain high-quality dataset by addressing L1 and L2. Surprisingly, it is missing in this paper.\n\n\n**2. [Major concern] Conclusions drawn from Table 1 are over-interpreted.**\n- In Lines 388--391, the authors conclude that “study-aware, clinically grounded supervision delivers larger gains on recall-heavy VQA metrics than caption-centric pretraining alone”. This isn’t well supported because the comparisons mix many factors at once (e.g., architecture, data quantity/distribution, training recipes). To support that claim, compare conditions that differ only in paired-text style, e.g., Experiments (iii) vs (iv) as above.\n- Table 8 shows that training episodes from the VQA benchmarks (VQA-RAD, SLAKE, PathVQA, PMC-VQA) appear in the SemiHVision-8B training. That makes it hard to credit the gains to “clinical grounding” rather than simple familiarity. Notably, HuatuoGPT-Vision trains only on LLaVA general-domain data plus PubMedVision (no overlap with VQA-RAD, SLAKE, and PathVQA) yet remains competitive with SemiHVision-8B-20M and SemiHVision-8B.\n\n\n**3. Important dataset details live in the appendix, which makes the main text hard to follow.**\n\nFor example, “Quilt-1M” appears without prior introduction, and the training narrative mixes “PMC Data,” “GPT-4o Synthesis Data,” and “Human-annotated Data” without a clear map and relationship between the data sources. I suggest summarizing the sources and composition (per source, per modality, human vs synthetic) into the main text and keep the terminology consistent throughout sections.\n\n\n**4. Lack of discussion with related works.**\n\nMedPix2 is a similar clinical-case dataset that includes workflow and non-imaging evidence, but is not discussed in the paper. Readers need to see how SemiHVision truly differs (scope, linkage granularity, etc).\n\n\n**5. Lack of details.**\n- The authors claimed that their SemiHVision dataset is “the first dataset to leverage a unified image-text retriever to integrate clinical information for data construction”, which shows the critical role of the retriever in their proposed pipeline. However, such a retriever is only superficially described in the main text and many details are missing. \n- “Rebalance” (Line 182) is undefined. What target proportions are used, how is it done, and does it introduce near-duplicates or redundant slices/studies? A concise definition and a short analysis of diversity before/after would solve this. \n\n---\n\nBelow are minor issues:\n- There are multiple misuses of hyphens (“-”) and en dashes (“–”) throughout the paper. For example, in the caption of Figure 1, en dashes in “PMC–OA”, “list–style”, “single–image” should all be hyphens. For another example, in Sec. 2.1, en dashes in “report–paired”, “image–caption” should also be corrected. The authors should carefully revise all occurrences to ensure consistent and proper usage.\n- Missing references. For example, “SLAKE” and “VQARAD” in Line 94, “UniIR” in Line 209, “Quilt-1M” in Line 301, “PathVQA” in Line 311. Note that related work should be referenced where they first appear, instead of in Appendices. \n- In Figure 1, middle part. “PuMedVision” → “PubMedVision”.\n- In Line 093, “To valid our pipeline could work” → “To validate our pipeline”. “we train LLM on our datasets” → “we train an LLM on our datasets”.\n- In Lines 239--240, repeated sentences: “Finally, we conducted a human evaluation of data quality.”\n- In Line 900, I am assuming “300k datasets” to be “300k examples” in SemiHVision dataset."}, "questions": {"value": "Please see \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ftDEswTjxs", "forum": "WXFr8Tfj0J", "replyto": "WXFr8Tfj0J", "signatures": ["ICLR.cc/2026/Conference/Submission3574/Reviewer_9RCw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3574/Reviewer_9RCw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203187919, "cdate": 1761203187919, "tmdate": 1762916836135, "mdate": 1762916836135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SemiHVision, a semi-human-verified instruction data pipeline and training strategy for medical multimodal LLMs. It aims to address two essential shortcomings of current medical MLLMs: (i) the lack of end-to-end clinical workflow supervision, and (ii) insufficient multi-view / multi-modality evidence alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly identifies gaps in current medical MLLMs regarding clinically grounded reasoning.\n\n2. Proposes a workflow-supervised data construction approach that aligns more closely with real diagnostic practice.\n\n3. Builds a multimodal, semi-human verified dataset that covers diverse modalities and integrates clinical context.\n\n4. Demonstrates consistently strong empirical performance, especially on VQA benchmarks, suggesting improved clinical reasoning capability."}, "weaknesses": {"value": "1. Only three clinicians reviewed 100 samples, which is a very small fraction of the corpus. It raises concerns about annotation noise and potential over-reliance on automated synthesis.\n2. The contribution lies mainly in data curation and instruction-tuning design rather than architectural advancement. While impactful, the approach largely follows established VLM tuning paradigms.\n3. Baselines are not retrained on similarly structured or enriched clinical instruction data, making it difficult to disentangle whether improvements stem from better methodology or simply more tailored supervision.\n4. The paper lacks systematic analysis to quantify which pipeline elements—retriever, human edits, negative sampling, annealing—most contribute to observed performance gains.\n5. The dataset is assembled from specific case sources; performance on other clinical distributions or multi-center data remains unclear, leaving questions about scalability and robustness in real-world deployment."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7reRr7vse8", "forum": "WXFr8Tfj0J", "replyto": "WXFr8Tfj0J", "signatures": ["ICLR.cc/2026/Conference/Submission3574/Reviewer_PymD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3574/Reviewer_PymD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832018978, "cdate": 1761832018978, "tmdate": 1762916835899, "mdate": 1762916835899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SemiHVision, a case-centric, semi–human-validated medical multimodal instruction corpus that targets two gaps in current medical MLLM data: (1) supervision rarely mirrors the real diagnostic workflow (history → findings → differential → impression/plan), and (2) cases often lack linked multi-view or multi-modal evidence within the same study (e.g., AP/LAT, CT/MRI, ultrasound, histopathology). The authors build a two-branch pipeline combining human-annotated clinical cases (Eurorad, Radiopaedia) with unannotated images that are expanded using a unified image–text retriever and GPT-4o, then unified into a training-ready schema. On this corpus, they fine-tune an 8B model with an additional annealing stage on high-quality diagnostic data. The resulting model, SemiHVision-8B-AN, matches or exceeds strong public medical MLLMs such as HuatuoGPT-Vision-34B on SLAKE, VQA-RAD, PathVQA, and PMC-VQA, and attains the best GPT-4 rubric score (1.29) on the JAMA Clinical Challenge. Evaluation with accuracy, UMLS-F1, and a blinded GPT-4o rubric supports the claim that the gains come from better workflow- and evidence-aware supervision rather than model size alone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The study clearly identifies significant data gaps in the existing medical Vision-Language Models (VLMs). Specifically, it highlights two major shortcomings: (i) the supervision provided rarely aligns with actual diagnostic workflows, and (ii) many cases lack associated multi-view or multi-modal evidence. To address these issues, the authors propose SemiHVision, which shifts the supervisory approach from caption-like descriptions to a clinically relevant framework that emphasizes stepwise reasoning.\n- Evidence-based, workflow-aware construction is employed in this approach. A unified multimodal retriever is utilized to anchor synthetic instructions within established clinical guidelines and analogous cases prior to the expansion of GPT-4o. The resulting samples are then normalized into a schema that is ready for training. This methodology enhances specificity and mitigates shortcut learning, as responses are grounded in concrete evidence rather than relying on unstructured hallucinations.\n- Strong empirical improvements have been observed in comparison to public medical MLLMs. The model SemiHVision-8B-AN achieves an average score of 79.0% across the SLAKE, VQA-RAD, PathVQA, and PMC-VQA benchmarks, clearly outperforming notable public medical models such as HuatuoGPT-Vision-34B, which has an average score of 66.7%. This indicates that the proposed data and training methodology are effective beyond mere model size."}, "weaknesses": {"value": "- Limited coverage and potential anatomical bias in human-annotated data. Although SemiHVision substantially improves the balance of clinically relevant imaging modalities (CT, MRI, X-ray) compared to PMC-like sources, the authors explicitly acknowledge that high-quality, human-annotated medical cases are still unevenly distributed across body parts. As a result, representation for certain anatomical regions and subspecialties remains sparse. The paper acknowledges this limitation but does not provide per-region performance breakdowns or robustness analyses, so it is unclear how much the current imbalance would hurt generalization to underrepresented regions or rare findings in real clinical settings. \n- Insufficient transparency on the retrieval component. The dataset construction pipeline relies critically on a lightweight multimodal retriever (described as UniIR with fusion scoring) to fetch guidelines and similar cases, especially for the unannotated branch. However, the paper does not fully specify the retrieval strategy: it is unclear how image–text similarities are fused, how top-k results are selected and de-duplicated, or whether any negative or hard-negative sampling is applied to guard against noisy pulls. This is important, because in the unannotated setting, the retrieved context directly conditions GPT-4o’s generation, so retrieval errors can propagate into the final instruction data. The current level of detail therefore limits strict reproducibility and makes it difficult to assess robustness to irrelevant or low-quality retrievals.\n- Limited evaluation granularity and missing ablations. Table 1 (Page 7) reports averaged results over four standard medical VQA benchmarks (SLAKE, VQA-RAD, PathVQA, PMC-VQA), and Table 2 (Page 8) adds a diagnostic-oriented evaluation on the JAMA Clinical Challenge. However, the paper does not further decompose these gains along the very axes it claims to improve, such as imaging modality (X-ray vs CT vs MRI vs ultrasound), evidence layout (single view vs multi-view), or diagnostic workflow step (findings vs differential vs final impression). In addition, there is no ablation study that isolates the contribution of individual pipeline components, for example: (i) the multimodal retriever, (ii) synthetic GPT-4o–augmented data vs human-annotated clinical cases, and (iii) ROI/evidence linking. As a result, it is hard to tell which parts of SemiHVision are actually responsible for the observed 79.0% average and which parts are auxiliary. Finally, while Table 2 reports multiple metrics for diagnostic reasoning, it does not include a fine-grained error analysis (e.g., failures in cross-view synthesis vs failures in evidence attribution), which would help practitioners understand the remaining bottlenecks."}, "questions": {"value": "1. You mention that high-quality, human-annotated cases are still unevenly distributed across body parts. Please provide a detailed breakdown of the final training corpus by anatomical region or system (e.g., thoracic, neurological, musculoskeletal, abdominal, breast, pediatrics) to explicitly illustrate this imbalance. Additionally, do you have validation results stratified by anatomy or subspecialty (even from a small held-out set) that demonstrate the extent of performance degradation in underrepresented areas?\n2. Please clarify whether your workflow-focused supervision exhibits comparable generalization capabilities for localized pathologies in small regions of interest (ROIs), such as small lung nodules and focal liver lesions, where visual evidence is limited.\n3. The retriever is a critical part of the unannotated branch. You should provide a detailed description of the specific fusion/scoring formula employed (e.g., weighted late fusion of image and text similarities, learned fusion techniques, or rank-based merging). Additionally, please specify the values of k that are retrieved from each source.\n4. How do you handle noisy or semantically close but clinically irrelevant retrievals? Is there any hard-negative or near-miss filtering before passing the context to GPT-4o?\n5. You should conduct an ablation study in which you (i) remove the retriever, (ii) substitute it with a text-only retrieval method, and (iii) replace it with an image-only retrieval approach. Additionally, please report the impact of these changes on at least one Visual Question Answering (VQA) benchmark as well as on the JAMA Clinical Challenge.\n6. Why not consider including an ablation table that begins with the “human branch only” configuration, followed by the addition of “+ synthetic GPT-4o,” then “+ retriever,” and finally “+ ROI/evidence linking” ? This approach would allow readers to clearly identify where the most significant gains are achieved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c8jHDYxHGR", "forum": "WXFr8Tfj0J", "replyto": "WXFr8Tfj0J", "signatures": ["ICLR.cc/2026/Conference/Submission3574/Reviewer_BYv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3574/Reviewer_BYv7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836787305, "cdate": 1761836787305, "tmdate": 1762916835294, "mdate": 1762916835294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SemiHVision, a semi-supervised hierarchical learning framework for medical visual question answering (MedVQA). The method introduces hierarchical feature fusion and pseudo-labeling strategies to exploit both labeled and unlabeled data, aiming to improve generalization in limited-annotation medical domains. \nExperiments are conducted on four standard VQA benchmarks, showing reported gains over several baselines metrics. The paper argues that semi-supervised training can effectively leverage unlabeled medical images to enhance MedVQA performance under low-data conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important limitation in medical VQA: the scarcity of labeled data, by incorporating a semi-supervised learning approach that combines hierarchical visual-linguistic fusion with pseudo-labeling. The hierarchical architecture is conceptually sound and aligns with the structured nature of medical reasoning. \n The method is evaluated on standard datasets, and the results demonstrate consistent improvements under few-shot or partially labeled settings. The paper is also clearly written and easy to follow, with well-structured methodology and ablation studies supporting the core claims."}, "weaknesses": {"value": "The paper’s evaluation and comparative scope are limited. It benchmarks primarily against traditional VQA baselines but fails to include state-of-the-art medical-domain models, such as MedTrinity-25M, which have established strong baselines for MedVQA tasks. Without these comparisons, it is difficult to gauge the true impact of SemiHVision within the current landscape of medical multimodal modeling.\nAnother major concern is the inconsistency in reported results. For instance, the paper reports GPT-4o achieving only 45.9 on VQA-RAD and 37.9 on PathVQA, which appears unexpectedly low compared to known benchmarks and other recent papers. Similarly, the reported LLaVA-Med performance deviates from previously published results [1], raising questions about experimental setup, evaluation protocols, or prompt design. The lack of transparency regarding implementation details, prompt templates, and evaluation scripts further complicates reproducibility.\nFrom a conceptual perspective, while the semi-supervised strategy is well-motivated, it largely follows existing semi-supervised paradigms without introducing substantial innovation in algorithmic design. The hierarchical fusion module is incremental relative to prior cross-attention architectures, and the paper does not clearly separate the effects of semi-supervision versus architectural tweaks. Moreover, the datasets used (VQA-RAD, PathVQA) are relatively small and outdated, limiting the generalizability of conclusions to broader multimodal medical tasks. The claimed improvements, though consistent, remain modest and potentially sensitive to implementation or sampling choices.\n\n[1] Xie, Yunfei, et al. \"Medtrinity-25m: A large-scale multimodal dataset with multigranular annotations for medicine.\" arXiv preprint arXiv:2408.02900 (2024)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C1g4BRPK9K", "forum": "WXFr8Tfj0J", "replyto": "WXFr8Tfj0J", "signatures": ["ICLR.cc/2026/Conference/Submission3574/Reviewer_vn5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3574/Reviewer_vn5F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984691137, "cdate": 1761984691137, "tmdate": 1762916834833, "mdate": 1762916834833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}