{"id": "IvLVPbeoRx", "number": 11838, "cdate": 1758204186791, "mdate": 1763647076378, "content": {"title": "Physics-informed learning under mixing: How physical knowledge speeds up learning", "abstract": "A major challenge in physics-informed machine learning is to understand how the incorporation of prior domain knowledge affects learning rates when data are dependent. Focusing on empirical risk minimization with physics-informed regularization, we derive complexity-dependent bounds on the excess risk in probability and in expectation. We prove that, when the physical prior information is aligned, the learning rate improves from the (slow) Sobolev minimax rate to the (fast) optimal i.i.d. one without any sample-size deflation due to data dependence.", "tldr": "We prove that adding correct prior domain knowledge to nonparametric learning with dependent data speeds up learning", "keywords": ["learning with dependent data", "physics-informed machine learning", "convergence rates", "complexity-dependent bounds"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d096d5c27a560ed6e5089563c7b2d4a036e660e1.pdf", "supplementary_material": "/attachment/30a70960b83cfbfa0d93e0c6a61bcdd4bfc3b155.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript presents a theoretical analysis of physics informed machine learning with data are dependent. The work is an attempt to address the open challenge why incorporating physics prior can benefit data-drive learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses the open challenge: the theoretical soundness of physics informed ML, beyond empirical evidence and intuitive understanding that the prior physics knowledge is conductive to learning. \n2. Theoretical analysis appears to be rigorous (but I didn't check all the proofs)."}, "weaknesses": {"value": "1. The organization and exposition of the manuscript can be improved. Without loss of theoretical rigor, it would be great if the intuitive explanations can be provided for deep learning practitioners the meaning of the theoretical results in practice. \n2. Related to the first question, intuitive understanding of the importance of conditions related to key variables such as $T$, as in Theorem 5.2 and $\\lambda_T$ as in theorem 5.1, would greatly benefit the reader.\n3. It's understandable this is a theoretical paper, but the only numerical experiment does add the weight to the paper. A well thought-off experiments to demonstrate the conditions related to $T$ and $\\lambda_T$ would also greatly benefit the readers, see the concern above."}, "questions": {"value": "1. Minor formatting issue: all equations should be numbered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pNqICDZ6FG", "forum": "IvLVPbeoRx", "replyto": "IvLVPbeoRx", "signatures": ["ICLR.cc/2026/Conference/Submission11838/Reviewer_7BGW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11838/Reviewer_7BGW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967518241, "cdate": 1761967518241, "tmdate": 1762922857823, "mdate": 1762922857823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a rigorous statistical-learning theory for physics-informed learning under dependent (mixing) data. It studies regularized empirical risk minimization where the regularizer encodes known physical laws through a linear differential operator, and the data arise from a stochastic dynamical system $X_{t+1}=f_{\\star}(X_t)+W_t$. Using tools from Sobolev-space analysis, the small-ball method, and martingale offset complexity, the authors prove complexity-dependent excess-risk bounds showing that when the physical prior is aligned with the ground-truth dynamics (i.e., PDE residual of $f_{\\star}$ is nearly zero), the learning rate acclerates from the traditional Sobolev minimax rate $O(T^{-d})$ to the fast i.i.d optimal rate $O(1/T)$, even when samples are correlated. A simple unicycle-dynamics experiment empirically confirms the predicted speed-up, demonstrating that properly aligned physics-based regularization can provably improve sample efficiency in learning dynamical systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important topic at the intersection of physics-informed learning and statistical learning under dependence. The authors aim to provide a theoretical foundation for when incorporating physical structure can mitigate the challenges of temporally correlated data. In this context, the paper offers several notable positive aspects:\n\n1. The central idea of using elliptic differential operators to encode physical priors and recover i.i.d.-like learning rates (under suitable alignment) despite sample correlation is novel and insightful.\n\n2. The theoretical analysis is mathematically sound. The assumptions are clearly stated, and the main theorems follow logically from the lemmas and proof techniques provided. The paper carefully extends existing theory to accommodate a Sobolev-based physics regularizer in the presence of Markovian data, which is nontrivial. While I did not look into the proofs in detail and some proof components rely on established techniques, the overall combination is coherent and technically competent.\n\n3. The results contribute to a better theoretical understanding of physics-informed learning, particularly in settings where data dependence is unavoidable, such as system identification and scientific modeling. Showing that incorporating correct physical structure can mitigate the adverse effects of temporal correlation is a useful insight, although demonstrated under idealized assumptions."}, "weaknesses": {"value": "While the paper makes a meaningful theoretical contribution and is clearly written, several limitations temper its overall impact and practical relevance. Most of these relate to the idealized nature of the assumptions and the gap between the theory and empirical applicability. The following points highlight areas where the work could be strengthened or where additional clarification or experimentation would improve the contribution:\n\n\n1. The fast-rate $O(1 / T)$ convergence is achieved only under an idealized knowledge alignment condition, namely when $\\|\\|D\\left(f_{\\star}\\right)\\|\\|_{L^2} \\approx 0$. In practice, the physical operator $D$ is rarely known with such precision, and even modest mismatch can revert the rate to the slower $O\\left(T^{-d}\\right)$ regime. The paper does not provide a quantitative robustness analysis that would clarify how sensitive the rates are to partial or imperfect alignment, which limits the applicability of the theoretical claims in realistic settings.\n\n2. The theoretical guarantees depend on $\\lambda_T$ choices tied to latent problem quantities (e.g., $\\Psi(f_{\\star}), \\sigma_W^2$ ). While the authors note that cross-validation could be used in principle, the paper offers limited practical guidance or empirical validation for tuning $\\lambda_T$ in realistic settings.\n\n3. The single toy experiment (unicycle) is supportive but narrow (one setting, small MLP, no real-world data/baselines), so the robustness of the phase transition across architectures/noise regimes remains unclear. Even in this controlled synthetic setting, implementing the physics-informed regularization term $\\Psi(f)= \\|\\|D(f)\\|\\|_{L^2}^2$ may require evaluating higher-order derivatives and Sobolev norms, which can be computationally demanding in higher dimensions. In addition, the paper does not discuss how discretization, numerical differentiation, or instability in PDE solvers or neural approximators would affect the performance or validity of the theoretical bounds, leaving a gap between the continuous theory and practical implementation.\n\n4. The framework relies on $D$ being a known, linear elliptic operator. Many modern scientific machine learning applications involve unknown or nonlinear physics, or operators that must be learned jointly with the model (e.g., operator learning, neural PDE surrogates, and PINNs). As the current analysis does not extend to such settings, it is unclear how the insights would generalize to applications where the governing equations are only partially known or inherently nonlinear."}, "questions": {"value": "In relation to the weaknesses stated above, please see my questions/comments below:\n\n1. The paper presents a two-term bound combining the $T^{-d}$ and $T^{-1}$ rates, but it is unclear how to interpret the intermediate regime. Could the authors clarify whether the transition between the two regimes is smooth or abrupt, and provide any threshold conditions under which the fast term becomes dominant? In addition, it'd be great if the authors could shed some light on the following: how robust this behavior is to imperfect knowledge alignment? For example, if $\\|\\|D\\left(f_{\\star}\\right)\\|\\|_{L^2}$ is small but nonzero, does the convergence rate degrade gradually (and remain faster than $T^{-d}$ ) or does it collapse sharply to the slower regime?\n\n3. Since $\\Psi(f)=\\|\\|D(f)\\|\\|_{L^2}^2$ may involve higher-order derivatives, do the authors foresee computational or numerical challenges when scaling beyond low-dimensional synthetic problems? Any guidance on discretization or numerical stability when implementing this regularizer in practice would be helpful.\n\n3. The analysis assumes that $D$ is a linear and elliptic operator, which can be restrictive. Could the authors comment on whether any part of the analysis may extend to mildly nonlinear or non-elliptic operators, or if linear ellipticity is fundamentally required for the proof techniques used?\n\n4. The paper imposes $s \\geq 2 d_X$ to ensure the burn-in term vanishes. Is this threshold believed to be intrinsic to the problem, or could the two-phase rate behavior persist under weaker smoothness assumptions (e.g., $s>d_X$ or $s>3 d_X / 2$ ) with possibly different constants?\n\n5. Could the authors provide any preliminary numerical results or intuition on how large $T$ must be for the asymptotic behavior to manifest in practice? Even a brief discussion of the finite-sample regime would help readers assess when the theoretical rates become observable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ms1clVzy5v", "forum": "IvLVPbeoRx", "replyto": "IvLVPbeoRx", "signatures": ["ICLR.cc/2026/Conference/Submission11838/Reviewer_6PJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11838/Reviewer_6PJM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982540457, "cdate": 1761982540457, "tmdate": 1762922857285, "mdate": 1762922857285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "First of all, we would like to thank the Reviewers for the careful reading of our paper and for their insightful comments.\n\nMotivated by the interest and success of physics-informed machine learning algorithms, our paper aims at theoretically proving their underpinning conjecture, namely, that aligned prior knowledge speeds up learning. Set in the context of statistical learning, our construction provides the first convergence rates for physics-informed learning under dependent data, deriving results that are entirely algorithm-agnostic and hold for any nonparametric approach.\\\nMany of the Reviewers' questions regarded practical aspects such as implementation, neural network architecture and optimization errors. \nWe appreciate that these are issues of tremendous interest in machine learning practice, but answering them would require an extensive empirical study that is out of the scope of this paper, which does not focus on any specific algorithm. Nevertheless, we made a more explicit connection to practical implementations, enriching the bibliography on physics-informed machine learning and deep networks. Also, following the suggestion of Reviewers tMrX and 7BGW, we expanded the explanation of our results to make our work more accessible -- please find the proposed revised version in the Supplementary material, where we uploaded the new version with modifications in blue-printed fonts.\n\nAlong the same lines, we would like to point out that the numerical illustration presented in the paper is just to display a sample behavior of what is predicted by the theory: the empirical excess risk decreases faster if injected with correct physical knowledge. Its goal is not to claim superiority of one algorithm compared to a baseline. However, we modified its presentation, featuring more comments on practical issues like burn-in quantification, finite-sample behavior, selection of $\\lambda_T$ and impact of non-aligned prior knowledge (Reviewers 3SEU, 6PJM and 7BGW). \n\nFurthermore, we appreciated the Reviewers' comments on the standing assumptions in our paper:\n- Reviewers tMrX, 3SEU, 6PJM asked about relaxing Assumption 4 on having the differential operator to be linear and elliptic. While unfortunately our analysis does not carry over seamlessly to nonlinear operators (which will be a compelling future research direction), we were able to relax the requirement on the PDE to be elliptic. Indeed, such a condition lends us the norm inequality (C.2), which we use only in Proposition C.4 to compute the covering-number bound for the effective hypothesis space $\\mathscr{F}^{\\rho}$. However, elliptic regularity is only a sufficient, not necessary condition for inequalities like (C.2) to hold: similar results can be obtained (i) for second-order parabolic operators, via $\\mathscr{L}^2$-regularity in anisotropic Sobolev spaces for $\\mathscr{D} = \\partial_t - \\mathscr{A}$, with $\\mathscr{A}$ being the spatial differential operator ([1], Ch.7.1.3); and (ii) for second-order hyperbolic operators, relying on energy estimates ([1], Ch.7.2.3). Thanks to this, we were able to extend the applicability of our results to other priors of interest, such as those given by the heat and wave equations.\n- Reviewers tMrX and 3SEU consider Assumption 5 (on having the unknown function to be estimated to be contained in the hypothesis space) very restrictive. In general, such an assumption is quite common in the learning theory literature and allows us to focus on the stochastic component of the excess risk, deriving the results presented in our paper. If Assumption 5 were violated, the excess risk bounds would feature an additional deterministic term given by the choice of the hypothesis space. Bounds for such a bias have been derived in the statistical learning literature leveraging, e.g., interpolation theory [2] -- however, being entirely data-independent, these bounds are essentially decoupled by the ones derived in our paper, and our analysis would not advance their theory. Nevertheless, we expanded the comment on Assumption 5 based on these considerations.\n- Reviewers 3SEU and 6PJM ask about different avenues to relax Assumption 2 on the Sobolev space order $s$, which is indeed a very interesting matter. At the moment, the stricter assumption on $s$ is needed to have a well-defined burn-in time, which in turn emerges from the whole construction of martingale offset complexity bounds. Even if intuitively one can explain that a larger smoothness degree is needed to cope with dependent data, we do not exclude that different set-ups could lead to relaxing the assumption on $s$, possibly introducing some restriction elsewhere.\n\nPlease refer to the individual comments to Reviewers for further detailed responses. \n\n**References:**\\\n[1] L. Evans. Partial differential equations. Vol.19. American mathematical society, 2022.\\\n[2] F. Cucker and D.-X Zhou. Learning theory: An approximation theory viewpoint. Cambridge University press, 2007."}}, "id": "ltppEgbOLe", "forum": "IvLVPbeoRx", "replyto": "IvLVPbeoRx", "signatures": ["ICLR.cc/2026/Conference/Submission11838/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11838/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11838/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763648545711, "cdate": 1763648545711, "tmdate": 1763648545711, "mdate": 1763648545711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper derives complexity-dependent bounds that formally characterize the impact of aligned prior domain knowledge on learning rates in physics-informed machine learning settings. This is approached by focusing on bounds of the excess risk in regularized ERM under dependent data generated from non-linear dynamical systems. Results under the combined assumptions of physics-informed regularization and non-IID data have yet to be shown in existing literature, and the derived bounds reveal new insights into the relationship between statistical learning theory and physics-informed ML."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-organized and has a presentation that's easy to follow from start to finish. I appreciated the clear groups in Section 3 and the visual distinction for all stated assumptions.\n- The authors address an important theoretical challenge in the physics-informed ML space, with a fundamental connection to all PIML architectures and practitioners looking to tackle scientific problems. Characterizing the impact of well-aligned domain knowledge takes a clear step toward helping researchers and practitioners alike when it comes to justifying additional time spent refining assumptions and calibrating inductive biases.\n- Relaxing the IID assumptions from prior work (e.g., Doum√®che et al. 2024) appears to be an important adjustment that broadens the reach of the resulting bounds. In most PIML settings, one is working with heavily dependent sequential data, so this seems a prudent step toward establishing useful bounds that mirror the real world."}, "weaknesses": {"value": "- Assumption 5 is quite strong, limiting the applicability of the results in many real-world physics-informed modeling scenarios. For instance, unless additional constraints are applied, popular methods like physics-informed neural networks would presumably violate this assumption, and in these cases it's unclear how one should think about the applicability of the results. I understand details on approximations were stated as out of scope, but it would be nice to include some discussion/analysis of the likely ways practitioners may violate assumptions in practice and what elements of the original bound still apply (if any). \n- Similar to the above point, Assumption 4 also excludes common practical scenarios that leverage non-linear PDE priors. Discussion on what remains of the bound in these cases would be helpful for position the paper's analysis in a broader context.\n- This is somewhat tangential to the core theoretical aims of the paper, but it would be instructive to include a more holistic case study. In particular, the impact of using misaligned/incomplete prior physics knowledge and how this empirically threads between the \"with knowledge\" and \"without knowledge\" scenarios. The sample sizes at which various levels of knowledge alignment outperform others, if only for short ranges of $T$, would also be helpful to characterize insofar as they relate to the paper's analysis of sample size."}, "questions": {"value": "- Are there any promising avenues for achieving the stronger burn-in requirement while relaxing $s\\ge 2d_X$ toward the standard $s\\ge d_X/2$, or are there clear reasons to believe this is a necessary tradeoff? Can factors/assumptions be loosened in its place while maintaining the original bound, giving another route to the same result?\n- In the numerical experiment and Figure 2, does the reported slope fit account for burn-in (if even needed in this case)? It would be instructive to see how this shows up here, and/or when burn-in exceeds realistically attainable sample sizes in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FZP46QPJNB", "forum": "IvLVPbeoRx", "replyto": "IvLVPbeoRx", "signatures": ["ICLR.cc/2026/Conference/Submission11838/Reviewer_3SEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11838/Reviewer_3SEU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998319789, "cdate": 1761998319789, "tmdate": 1762922856807, "mdate": 1762922856807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical analysis of physics-informed learning in the presence of dependent data. The authors analyze empirical risk minimization with a physics-informed regularizer that encodes known physical priors in the form of elliptic PDE constraints. Using the small-ball method and martingale offset complexity, they derive complexity-dependent excess risk bounds both in probability and expectation. The main result shows that when the physical prior regularizer is aligned with the true dynamics, the convergence rate improves from the slow Sobolev minimax rate to the optimal i.i.d. rate, even with dependent samples. The theoretical framework is supported by a unicycle dynamics experiment illustrating the empirical benefit of incorporating physics-informed regularization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper rigorously extends the small-ball method and offset complexity analysis to dependent data settings with physics-informed regularization, which is technically novel.\n\n2. The paper establishes a clear theoretical connection between physical priors (encoded as elliptic PDE constraints) and improved learning rates, addressing a long-standing gap in understanding the benefits of physics-informed models.\n\n3. The proofs and appendices appear detailed, well-grounded in functional analysis, Sobolev space theory, and dependent process theory.\n\n4. The  experiment, though simple, can demonstrate the theoretical prediction that physical priors accelerate convergence."}, "weaknesses": {"value": "1. The analysis assumes elliptic linear PDE operators; it is unclear whether the results generalize to non-elliptic, nonlinear, or mixed-type operators often seen in physics-informed neural networks (PINNs). The gap or difficulty is not well addressed. \n\n2. The optimization error's influnene is not discussed in the theoretical part and the numerical example. Note that the physics-informed regularizer will increase the stiffness of the Hessian matrix and increase difficulties for the optimization in PINNs, which is not aligned with the main result (convergence rate improves with physics-inforemed regularizer added)\n\n3. Too many assumptions may not hold for complex real-world systems, a more interpretable or verifiable condition would strengthen the practical relevance.\n\n4. Only one low-dimensional example is provided. Additional tests on nonlinear PDE systems or stochastic dynamical systems would reinforce the claims."}, "questions": {"value": "1. see in the weakness\n\n2. The phsyics-informed regularizer is limited to linear elliptic PDEs. However, the data is dependent, which is always occured in dynamical systems. Is this two conflicting? The numerical example is an ODE, not elliptic PDEs. How the convergence rate behaves for common PINNs problems such as Poisson equation or Darcy flow problem? How can the proposed theory be connected to the optimization landscape of neural networks trained with PINNs?\n\n3. Does the empirical rate in the numerical example persist when neural architectures differ from MLPs, or with stochastic training?\n\n4. Can the presentation be more friendly to the general ICLR audience without additional intuition or graphical explanation of the key proof mechanisms? The current form is mathematically dense and more suitable to journals like JMLR, not general top conferences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eMfuLA0HVo", "forum": "IvLVPbeoRx", "replyto": "IvLVPbeoRx", "signatures": ["ICLR.cc/2026/Conference/Submission11838/Reviewer_tMrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11838/Reviewer_tMrX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762570917225, "cdate": 1762570917225, "tmdate": 1762922856344, "mdate": 1762922856344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}