{"id": "c6dwCJM0CK", "number": 13231, "cdate": 1758215418422, "mdate": 1759897453900, "content": {"title": "Coverage-Driven KV Cache Eviction for Efficient and Improved Inference of LLM", "abstract": "Large language models (LLMs) excel at complex tasks like question answering and summarization, thanks to their ability to handle long-context inputs. However, deploying LLMs is costly, not only due to the high computational demands of quadratic complexity of self-attention and auto-regressive generation, but also because of the significant memory overhead required for storing the key-value (KV) cache during inference. To reduce the memory cost, existing KV-cache eviction strategies leverage the sparsity in attention to selectively store a subset of tokens. While reducing the memory footprint, such approaches show a considerable drop in performance, especially in tasks that require long-context reasoning. We identify that the drop in performance is linked to a reduction in the coverage of unique tokens. Additionally, we theoretically show that reduced coverage limits the mutual information between inputs and outputs, thereby impairing predictive accuracy. To this end, we introduce K-VEC, a novel coverage-aware KV-cache eviction strategy that prioritizes token coverage while evicting tokens in the cache. K-VEC introduces a cross-head and a cross-layer coverage module to enhance token retention across attention heads and model layers, mitigating performance degradation caused by low coverage. Evaluated on 16 LongBench subsets, K-VEC exhibit up to 10.35 points improvement over the existing methods under the same eviction rate and memory constraint. Comprehensive evaluations validate the effectiveness of our approach and demonstrate its potential for efficient LLM deployment in resource-constrained settings.", "tldr": "", "keywords": ["LLM", "Efficient inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d791bdb618b2cbdfdb1f2f37ec23cb2f7d9dde2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies that performance degradation in KV-cache eviction stems from reduced token coverage. The authors propose K-VEC, a coverage-aware eviction strategy with cross-head and cross-layer coverage modules that prioritizes retaining unique tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and straightforward."}, "weaknesses": {"value": "1. The attribution of performance degradation to coverage is not sufficiently convincing. I personally believe that the pattern skewed towards the end, as mentioned in Figure 2, is actually the key factor rather than coverage itself. This is a well-known issue in SnapKV-like approaches that use a local window for score computation. This problem has been demonstrated in related work and addressed through Global-Local Importance strategies.\n\nReference:\n\nEMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance\n\n2. The experimental setup is inconsistent and raises concerns. For instance, while baselines such as Pyramid, SnapKV, Ada-Pyramid, Ada-SnapKV, HeadKV, and GemFilter exist, most tables (e.g., Tables 1, 2, and 3) omit certain baselines. In the Qwen2.5-7B experiments, only SnapKV is retained. Could the authors explain the rationale behind this experimental arrangement?\n\n3. The algorithm involves numerous hyperparameters, and the results demonstrate insufficient robustness. Although the authors provide ablation studies, the results show significant sensitivity. The main experiments report an average improvement of less than two points, yet the ablation studies show that any parameter change causes fluctuations of approximately one point. This substantial impact on results raises doubts about the algorithm's practical robustness.\n\n4. The algorithm appears to severely compromise prefill efficiency. The experiments show that prefill throughput drops from 5,672 to 3,440-a reduction of 39%. The authors' argument that \"the prefill stage is a one-time operation\" is unconvincing. On the contrary, prefill time is a critical overhead, and existing KV cache compression methods typically preserve this crucial efficiency with minimal impact."}, "questions": {"value": "The questions raised are detailed in the weaknesses above. \n\nAdditionally, the authors need to present more convincing arguments, analysis, or experimental evidence to establish that coverage is indeed the root cause of performance degradation, rather than simply the problem of local scoring induced by local window mechanisms. Optimization should target the fundamental cause rather than a proximate symptom in the causal chain. This is my primary concern and the basis for my rejection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Em1GPjH6vA", "forum": "c6dwCJM0CK", "replyto": "c6dwCJM0CK", "signatures": ["ICLR.cc/2026/Conference/Submission13231/Reviewer_DKxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13231/Reviewer_DKxX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948353008, "cdate": 1760948353008, "tmdate": 1762923918039, "mdate": 1762923918039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new perspective by examining KV cache eviction through the  token coverage. Based on this insight, the authors propose K-VEC, a coverage-aware eviction strategy that incorporates cross-head and cross-layer coverage modules to preserve diverse tokens across attention heads and model layers. Experimental results demonstrate that K-VEC achieves consistent improvements across LongBench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel perspective by examining the problem from the token coverage.\n\n2. K-VEC demonstrates consistent and effective improvements on LongBench."}, "weaknesses": {"value": "1. In my view, the authors' claim that coverage is the root cause of performance degradation is insufficiently justified. If the issue stems primarily from attention being skewed towards the end, why not directly optimize token selection across different positions rather than focusing on coverage?\n\n2. Following the above point, the theoretical analysis appears insufficient. The analysis lacks grounding in the underlying mechanisms of LLMs and seems hastily constructed, offering limited insight. It fails to convincingly establish why coverage is the fundamental cause of performance degradation.\n\n3. The proposed algorithm introduces numerous hyperparameters, raising concerns about hyperparameter sensitivity.\n\n4. Expanding the observation window from O to O' cannot be considered a genuine improvement. Previous work selected observation windows primarily to control the computational overhead of the compression algorithm itself. For instance, in the efficiency tests, K-VEC significantly increases this overhead, which would severely impact TTFT  in practical deployment-a critical concern in long-context LLM serving.\n\n5. The experimental evaluation is insufficient, as experiments are conducted only on Llama-3.1-8B. Additionally, why do LongBench and Needle-in-a-Haystack use different baselines? I recommend providing complete results across all baselines for both benchmarks."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xghjkBpWai", "forum": "c6dwCJM0CK", "replyto": "c6dwCJM0CK", "signatures": ["ICLR.cc/2026/Conference/Submission13231/Reviewer_ckcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13231/Reviewer_ckcg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699457380, "cdate": 1761699457380, "tmdate": 1762923917669, "mdate": 1762923917669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes K-VEC, a coverage-driven KV cache eviction policy that reduces redundant token selection across heads and layers. It expands attention for low-focus heads and prioritizes under-represented tokens across layers. Based on benchmark LongBench with Llama-3.1-8B-Instruct, K-VEC improves performance under tight KV budgets with minor prefill overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Give empirical evidence that existing strategies exhibit recency bias and low coverage.\n2. Well-motivated decomposition into cross-head vs. cross-layer effects.\n3. Improvements are consistent across multiple memory budgets."}, "weaknesses": {"value": "1. Coverage as a general assumption, the paper’s own limitation section acknowledges that many tokens “contain no relevant information” and that full coverage is not beneficial. This weakens the universality of the central premise. It is better to demonstrate failure modes where coverage hurts.\n2. The paper does not clarify whether SnapKV’s sliding-window smoothing is applied before STD computation. Since STD is noise-sensitive and may affect head ranking stability, an analysis with and without smoothing is needed to assess robustness.\n3. LongBench includes many tasks where coverage matters weakly. To support the hypothesis, the paper should include more information dense tasks, such as NeedleBench.\n4. The key novelty is STD-based head reweighting, yet the paper lacks comparisons against random, top-STD, or fully entropy-based selection. Without these baselines, it is unclear whether the signal is truly informative or arbitrary."}, "questions": {"value": "1. Did we smooth attention scores before STD computation? What is the effect without smoothing?\n2. How many heads actually classify as low STD in practice (distribution across layers)? Is this model dependent or a general observation.\n3. Can coverage ever degrade performance on tasks where few key spans dominate relevance?\n4. Algorithm 1 returns updated eviction scores but does not explain how they are thresholded, masked, or applied over time. Practical guidance for stability—especially regarding the γ focus term—is missing.\n5. Does early-layer coverage matter more? How consistent are the selected low-STD heads across layers and prompts? Are they stable or input-dependent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2aE674QepP", "forum": "c6dwCJM0CK", "replyto": "c6dwCJM0CK", "signatures": ["ICLR.cc/2026/Conference/Submission13231/Reviewer_FpJY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13231/Reviewer_FpJY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935976325, "cdate": 1761935976325, "tmdate": 1762923917219, "mdate": 1762923917219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The idea of promoting coverage across heads and layers for KV eviction is interesting and makes intuitive sense, and the results look reasonable in those settings.\n\nMy main concern is that, if I understand correctly, the method is stated as being applied during prefilling. However, in practice, the cache is still fully built first, and pruning only occurs once after the prefill stage completes. Since prefilling is parallel, there is no actual token-by-token eviction taking place. This makes the approach less general and not directly applicable to streaming or incremental scenarios.\n\nThe evaluation is also relatively narrow. All experiments are based on a single LLaMA model, while KV-cache behavior can vary significantly across different architectures such as Qwen, DeepSeek, and Mistral. Without broader evidence, it’s difficult to be fully convinced that the method will generalize well.\n\nFinally, the design feels more aligned with retrieval style tasks than reasoning tasks. For math, where only a small part of the prompt matters, “coverage” may not be the right signal.\n\nOverall, the idea is promising, but the one shot pruning, limited model coverage, and unclear benefits beyond retrieval tasks make me want to see more evidence before fully buying in."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea is simple and intuitive, preserving diverse parts of the prompt instead of just the highest-attention tokens. \n2. The method is easy to plug into existing models and it shows steady gains under tight KV budgets. \n3. The paper grounds the idea in a clear empirical observation."}, "weaknesses": {"value": "1. The evaluation is mostly tied to Llama model, so it’s hard to tell if the method really generalizes across architectures like Qwen, DeepSeek. \n2. The paper says eviction happens during prefill, but in practice the full KV cache is still built first and pruning only happens once, which limits the method in streaming or incremental settings. \n3. It’s unclear how well it transfers to reasoning or math problems where only a small part of the prompt matters."}, "questions": {"value": "1. How would this approach behave on tasks where only a small portion of the prompt matters, like math or reasoning, where broad coverage may not be as meaningful?\n2. Do the authors expect the same behavior on Qwen/DeepSeek architectures, which often have different attention patterns? Any reasons to think the coverage heuristic generalizes beyond Llama?\n3. Since coverage statistics are only computed once after prefill, have the authors looked at scenarios where the output is extremely long, or multi-turn?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4GV7xBWYhT", "forum": "c6dwCJM0CK", "replyto": "c6dwCJM0CK", "signatures": ["ICLR.cc/2026/Conference/Submission13231/Reviewer_Srin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13231/Reviewer_Srin"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000243106, "cdate": 1762000243106, "tmdate": 1762923916901, "mdate": 1762923916901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}