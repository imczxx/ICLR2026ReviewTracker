{"id": "IKnuyyPHCV", "number": 2258, "cdate": 1757042741062, "mdate": 1762987089059, "content": {"title": "RECODE: A Benchmark for Research Code DEvelopment with Interactive Human Feedback", "abstract": "Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE, a benchmark of 102 tasks from research papers and repositories that evaluates LLMs through multi-turn interactions with human feedback. It includes structured instructions, unit tests, and a five-level feedback hierarchy to reflect realistic researcher–agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experimentswith leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation.", "tldr": "A multi-turn interactive research code generation benchmark.", "keywords": ["Code generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ca30640eaafd10712e039b6fbb748f9b98bf9ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors present a new benchmark called RECODE, which is a set of 102 code execution tasks from research papers and their respective repositories. Their dataset is built via a clever combination of expert human annotation effort (using PhD-level experts to select the target code and repositories and provide code generation instructions) and LLM boostrapping using LLMs (e.g., for helping to annotate code explanations and construct on-the-fly unit tests). The particular focus of their dataset is on explicitly modeling multi-turn coding (intuitively, they say that the benchmark aims to capture standard programming where developers \"interatively refine implementations through cycles of execution, debugging and feedback\") and on testing directly the effect of different forms of feedback using a novel feedback hierarchy that they define in Section 3.2. This benchmark looks similar to those they cite in the RelatedWork, as well as some they don't cite, notably the work below (where the landmarks annotations appear to be similar in spirit to the types of feedback they collect):\n \n[Bogin et al.] SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories. EMNLP 2024\n\nMost uniquely, however, their problems are annotated with different levels of feedback (generated using LLMs) using the feedback hierarchy from Section 3.2, and a set of concrete unit tests that allow them to measure functional code correctness. \n\nThey couple their benchmark with a new agent design called ReCodeAgent. Based on how this agent is described, both in Figure 1 as well as in the begining of Section 4, is it really unclear how this agent is nothing more than a ReACT agent or a closely related variant such as a Reflexion agent. Indeed, their earlier claim (starting on line 035) that \"existing benchmarks .. for evaluating LLMs in research code generation mainly adopt a one-shot setting, where models are expected to produce final code in a single interaction* simply seems incorrect given that the models in virtually all of the studies they cite involve ReACT agents, much like ReCodeAgent, that engage in precisely the kind of act-observe loop they show visually in Figure 1. This point needs to be directly addressed by the authors and is the main source of my  concern about this paper. \n\nTheir main empirical results are show across seven LLMs (including 4 LLM model familities) in Table 2, where they also carefully report the performance effect of different types of feedback (not surprisingly, the most detailed feedback, level 4, is clearly the most hepful in improving end task performance).  This is coupled with other fairly expected conclusions, e.g., (citing the authors) *Model size and capability play a clear role in performance*. Further error analyis is provided (Table 3)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-- A new research coding benchmark with explicit feedback annotations that allow for more granular analysis of multi-turn coding. I could imagine this benchmark being used by others working in this area."}, "weaknesses": {"value": "-- **Misleading motivation and discussion of past work**. As noted above, claims like \"existing benchmarks for evaluating LLMs in research code generation mainly adopt a one-shot setting, where models are expected to produce final code in a single interaction\" and \"the ability of [LLMs] to generate correct and execute code remains limited\" (line 011) seems inconsistent with virtually all of the papers cited. In the latter case, most studies involve REACT agents that are by design multi-turn agents. I would like to see the authors directly address. \n\n--  **Limited to No Novelty of their ReCodeAgent** As noted above, their proposed solution, and their sole modeling approach, seems to be nothing more than a ReACT or Reflexion agent. \n\n-- **Limited empirical validation** Experiments are limited only to their new dataset. Especially if they claim that their coding agent is unique, it would be expected that this approach shows improvements on other tasks."}, "questions": {"value": "-- In what way is the agent workflow in Figure 1 (top left) not standard ReACT?\n\n-- If it is different from ReACT, did you try to compare against a standard ReACT or relfexion approach?  Or compare your approach on other benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XYF47Se8Q", "forum": "IKnuyyPHCV", "replyto": "IKnuyyPHCV", "signatures": ["ICLR.cc/2026/Conference/Submission2258/Reviewer_oRzg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2258/Reviewer_oRzg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761251873638, "cdate": 1761251873638, "tmdate": 1762916165784, "mdate": 1762916165784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RECODE, a benchmark of 102 research coding tasks in AI/ML to study the interactions between LLM-based agents and user feedback. The user feedbacks are simulated with LLMs and controlled to have five different granularity levels of information. Experimental results show that LLMs benefit from additional feedback, especially straightforward ones, but still struggle with those requiring deeper understanding of the research tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper adds a nice new dimension around user feedback to the evaluation of agents for research coding, which is helpful and adheres to the real-world use cases of such agents.\n2. The paper presents a reasonable amount of analysis of experimental results. The error analysis and feedback adoption analysis are interesting and may facilitate future research."}, "weaknesses": {"value": "1. While it is appreciated that the authors assembled a team of 26 annotators, their roles in the entire annotation process are not very clear to me. It seems that LLMs (Gemini 2.5 Pro and GPT-4o-mini) are used to perform many annotations. What are the jobs of the annotators and how their involvement ensures the benchmark’s quality and real-world utility?\n2. Relatedly, the quality of LLM-generated unit tests is unclear and not thoroughly discussed in the paper, which is critical to the reliability of the experimental results. It is also unclear to me how the “unit tests” can be leveraged to evaluate some tasks, such as training machine learning models or analyzing data. \n3. The authors should list and cite all the papers adapted. Meanwhile, discussions and tests of data contamination are also missing.\n4. The validity of “level 4” feedback, which provides ground truth code, may not be a valid setting since it directly “supervises” the code generation process and plays an exceptionally helpful role for most models. The evaluations would be more sound and clean by just stopping at “level 3.”\n5. Some example tasks in the appendix would be appreciated to help the benchmark description be more grounded."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "All of the papers and resources used by this benchmark are not cited or attributed. This may hurt the original authors' intellectual properties and be subject to copyright and terms of use issues."}}, "id": "5OLrCy7VId", "forum": "IKnuyyPHCV", "replyto": "IKnuyyPHCV", "signatures": ["ICLR.cc/2026/Conference/Submission2258/Reviewer_61B7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2258/Reviewer_61B7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921771431, "cdate": 1761921771431, "tmdate": 1762916165573, "mdate": 1762916165573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose RECODE, a new benchmark for generating research code in the novel setting of multi-turn interactions with a (simulated) human, which iteratively provides hints/details/corrections to help steer the system. They show that interactive feedback helps, while also highlighting that models still struggle."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introducing the interactive multi-turn angle is novel and refreshing, and adds some extra realism to the coding challenge\n - novel framing, with levels of feedback\n - benchmark looks like a useful product of a lot of hard work\n - evaluation is thorough"}, "weaknesses": {"value": "- The interactive feedback is (for reproducibility) actually simulated so has a degree of artificality associated with it (even though humans helped create it). In what ways does this setup differ from a (truely) real setup with actual humans, e.g., noise, imprecision, etc. Could you include any of those elements in your framework?\n - The conclusions from the experiments seem obvious (e.g., more feedback helps). What did you learn that was surprising/interesting/informative? It seems to me that should be more nuanced findings. The error analysis is perhaps more informative. If you were to advise future researchers on where to invest their energy in building better coding agents, what would you tell them based on the learnings from your work?\n\nMinor:\n - \"evaluates LLMs through multi-turn interacations with human feedback\" - makes it sound like there's a human in the eval loop. Perhaps \"(pre-collected) human feedback\" or \"(simulated) human feedback\" or something else to indicate the actual eval is fully automated.\n - Figure 2 would be more readable showing % rather than fractionals (e.g., \"1.4\" rather than \"0.014\" etc.)"}, "questions": {"value": "See weaknesses. Also:\n - Isn't level 4 feedback basically giving the system the answer? Why doesn't the coding agents score 100% as a result? Adding some discussion around this would be interesting, in particular that \"coding\" requires more than just knowing lines of code. What would you need to include in a \"level 5\" feedback to ensure the coding agents did score 100%?\n - Do your results suggest any advice for what *kinds* of feedback people should be giving to their coding agents, i.e., provide insights that make interactive coding agents more usable/effective?\n  - The Conclusion seems pretty weak, surely there's more to conclude than just that LLMs continue to face coding challenges? What are the big insights you found?\n   - Building/exanding this benchmark looks very labor intensive. Can you think of ways you might reduce the cost / semi-automate the process to extend it further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ckhuPbXbL", "forum": "IKnuyyPHCV", "replyto": "IKnuyyPHCV", "signatures": ["ICLR.cc/2026/Conference/Submission2258/Reviewer_AynR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2258/Reviewer_AynR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938764866, "cdate": 1761938764866, "tmdate": 1762916165400, "mdate": 1762916165400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RECODE, a benchmark for 102 code-generation tasks, based on LaTeX snippets from recent ML papers. They introduce a ReAct-style multi-turn ReCodeAgent that interacts with a code repo and receives feedback from a simulated expert (similar to LLM as a judge). The authors find that richer feedback substantially improves performance, though with diminishing returns. The paper also analyzes error types and how effectively models adopt feedback."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The empirical findings are valuable to broader coding agents: early feedback levels yield large improvements, while higher levels show smaller gains; and that most model failures stem from misunderstanding the paper or repo semantics.\n2. The benchmark targets realistic, interactive research-coding scenarios, which are increasingly relevant as LLM agents enter scientific domains.\n3. Solid engineering effort, clear and intuitive definitions and hierarchies of feedback"}, "weaknesses": {"value": "1. It is not clear how the papers were selected and the list of papers is not disclosed. How do you ensure that evaluated LLMs haven’t already seen the source repositories during pretraining?\n2. There is no code editing tool in the system prompt. The replace tool replaces the entire file content, but the prompt mentions \"aim for editing the code more\" -- it is not clear how this is implemented.\n2. LLM-generated feedback is clean and complete; real human feedback is often noisy or partial. But results with human-generated feedback on these tasks are missing.\n2. Prior benchmarks like SciReplicate-bench and ResearchCodeBench already explore research-code generation; the paper needs a clearer statement of what’s uniquely new here."}, "questions": {"value": "See above weaknesses.\nI am curious if you had results with a single-turn format. It would be interesting to see if the multi-turn format gives a big lift.\nMinor typos/issues: Fig 7 and 8 have same captions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QjwWJltmt7", "forum": "IKnuyyPHCV", "replyto": "IKnuyyPHCV", "signatures": ["ICLR.cc/2026/Conference/Submission2258/Reviewer_6h4u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2258/Reviewer_6h4u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970930318, "cdate": 1761970930318, "tmdate": 1762916165197, "mdate": 1762916165197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}