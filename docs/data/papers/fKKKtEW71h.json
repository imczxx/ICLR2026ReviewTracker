{"id": "fKKKtEW71h", "number": 20151, "cdate": 1758303071827, "mdate": 1762939053728, "content": {"title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL", "abstract": "Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing.  Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training.  Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31\\% in Pass@1 and 100\\% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.", "tldr": "We post-train large language models with agentic reinforcement learning and a hierarchical reward design to generate more syntactically correct and semantically faithful OpenQASM circuits for quantum optimization tasks.", "keywords": ["Quantum optimization algorithm", "Quantum circuit generation", "OpenQASM", "Large language models", "Agentic reinforcement learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d3299f76cf7d5f4575f1a231fec6a2209ea96d70.pdf", "supplementary_material": ""}, "replies": [{"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NmErlPywPe", "forum": "fKKKtEW71h", "replyto": "fKKKtEW71h", "signatures": ["ICLR.cc/2026/Conference/Submission20151/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20151/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936403810, "cdate": 1762936403810, "tmdate": 1762936403810, "mdate": 1762936403810, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QUASAR, an agentic reinforcement learning (RL) framework that augments large language models (LLMs) with external quantum simulators for quantum assembly code generation. The goal is to improve the syntactic and semantic correctness of OpenQASM 3.0 quantum circuits. QUASAR integrates supervised fine-tuning with a four-level hierarchical reward mechanism that incorporates syntactic validity, distributional alignment (via Jensen–Shannon distance), expectation-value alignment, and optimization progress. The approach is evaluated by augmenting a 4B-parameter LLM and compared with GPT-4o, GPT-5, DeepSeek-V3, and RL-only or SFT-only baselines. Results show high syntactic validity (99.31% Pass@1) and improved circuit quality on several quantum optimization benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe topic is timely and relevant, addressing quantum circuit generation using tool-augmented LLMs.\n2.\tThe hierarchical reward mechanism is conceptually well motivated.\n3.\tThe evaluation covers both syntactic and semantic metrics with clear quantitative reporting.\n4.\tThe experimental setup includes multiple baselines and ablation studies."}, "weaknesses": {"value": "1.\tIn Section 4.2 and Figure 3, the hierarchical reward mechanism, while interesting, lacks theoretical grounding or ablation analysis showing why the chosen four components (syntax, entropy, expectation value, optimization) are optimal. Other plausible metrics could exist, but justification is not provided.\n2.\tThe comparison with GPT-4o and GPT-5 in Table 1 does not constitute a fair baseline against a fine-tuned model. The paper should clarify hyperparameter settings, prompt design, and reproducibility details for these baselines.\n3.\tFigure 5 reports ΔE distributions but omits units and normalization conventions. Without specifying whether values correspond to expectation differences or normalized eigenvalue gaps, it is difficult to interpret it.\n4.\tThe Agentic RL in Section 3.3 largely reiterates standard GRPO methods (Shao et al., 2024) without adaptation to quantum contexts. The contribution seems incremental, since it applies an existing RL algorithm to a new domain with minimal innovation.\n5.\tThe quantum verification pipeline in Section 4.1 is described only superficially. Implementation details of the “Quantum Tool Server” and simulation fidelity are missing. It is unclear whether noise, decoherence, or realistic hardware constraints were modeled.\n6.\tThe reward normalization in Eq. (3) and Eq. (4) assumes bounded eigenvalues, but many Hamiltonians used in QAOA/VQE have variable scaling. This could bias the reward and affect convergence; no normalization consistency checks are discussed.\n7.\tSection 2 (Related Work) misses recent key works on quantum circuit compilation via differentiable programming and symbolic optimization. The related work is dominated by LLM-based citations and omits competing non-LLM approaches.\n8.\tFigure 2 and accompanying description do not specify the optimization problem (Hamiltonian) associated with the illustrated ansatz. It would be recommended to provide context, to improve clarity.\n9.\tThe evaluation metrics in Section 5.1 rely on Pass@K-style measures, which are adapted from code generation. These metrics may not align with physical correctness or execution fidelity on real quantum backends. Including hardware-executed validation would strengthen the paper.\n10.\tThe presentation has recurring typographical and formatting errors (e.g., “desigin” in the introduction, inconsistent use of subscripts in formulas), which reduce readability. Figures also have low resolution.\n11.\tThe hierarchical reward ablation in Table 2 suggests only marginal gains from additional reward components, implying that most improvements could stem from data scale or SFT pretraining rather than from RL itself.\n12.\tWhile it has been promised to release the code after acceptance, it would be preferable to make the code available during the review in the supplementary material.\n13.\tThe paper’s novelty lies mostly in combining existing techniques (OpenQASM simulation, GRPO RL, and LLM fine-tuning) rather than introducing a fundamentally new algorithmic insight."}, "questions": {"value": "1.\tHow sensitive is the performance to the relative weighting of the four reward components?\n2.\tWhat mechanisms prevent reward hacking when circuits add extraneous qubits?\n3.\tHow does QUASAR scale beyond 9-qubit or 12-qubit benchmarks?\n4.\tCan the hierarchical reward framework be generalized to other DSLs beyond OpenQASM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FJ8fw7G3wZ", "forum": "fKKKtEW71h", "replyto": "fKKKtEW71h", "signatures": ["ICLR.cc/2026/Conference/Submission20151/Reviewer_GbsY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20151/Reviewer_GbsY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291862073, "cdate": 1761291862073, "tmdate": 1762999986768, "mdate": 1762999986768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes QUASAR, an agentic RL framework to fine-tune LLMs to generate OpenQASM 3.0 programs for quantum optimization tasks. The method augments a 4B SFT model with a tool-use loop that calls an external quantum simulator and optimizes the policy with GRPO using a four-level hierarchical reward: (i) syntactic validity; (ii) distributional alignment via Jensen–Shannon distance with a qubit-mismatch penalty; (iii) expectation-value proximity to the ground truth problem Hamiltonian; and (iv) optimization-progress that rewards fewer classical optimization steps and better final value. The pipeline improves both syntax and semantics over SFT and strong prompting baselines on a dataset of graph-based optimization instances. Ablations suggest the distributional alignment term is the dominant driver, with expectation-value and optimization-progress giving complementary benefits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The reward shaping is well-designed. Clear decomposition into a hierarchy of four levels: syntax, distributional alignment, expectation value, and optimization progress. The qubit-mismatch penalty addresses a common failure mode of wrong wire counts. Ablations reinforces the effectiveness of RE term.\n- Realistic training stack and reproducible high-level settings."}, "weaknesses": {"value": "- **Overclaimed scope**. The title claims to be \"quantum assembly code generation\", and the introduction targets at general quantum circuits. However, all tasks, rewards, and metrics presuppose Hamiltonians + parameterized ansatzes. Nothing addresses general OpenQASM programs (e.g., QFT/PE, arithmetic, mid-circuit measurements, control flow, etc.). As far as I can see, the rewards and metrics cannot be adopted directly to universal quantum circuits, which limits the usage of this framework.\n- **Limited conceptual novelty**.  The framework largely repackages a common agentic RL template. The quantum parts are well-crafted instantiations rather than new principles.\n- **Gains over SFT are modest**. QUASAR’s main gains are semantic but the margins over SFT are somewhat incremental. An analysis of marginal improvement vs. extra compute would strengthen the case. \n- **Threshold choices & metric redundancy (minor).** The SREV tolerance $|E(C)−E^\\star∣\\le 0.2$ is not justified. Sensitivity to this threshold should be reported. HQCR is defined as RE within 0.1, which is not necessary as a metric given RE in my opinion. \n- **Lack of open-sourced code**. The abstract claims to provide training code at GitHub, but the linked repository is empty. Also an unanonymous link violates the double-blind review policy, which in principle should be desk-rejected."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VLAQvcPmoS", "forum": "fKKKtEW71h", "replyto": "fKKKtEW71h", "signatures": ["ICLR.cc/2026/Conference/Submission20151/Reviewer_1ovZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20151/Reviewer_1ovZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715127013, "cdate": 1761715127013, "tmdate": 1762999987636, "mdate": 1762999987636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes QUASAR, an agentic reinforcement learning framework for post-training large language models to generate parameterized quantum circuits in OpenQASM 3.0. It also introduced 4 hierarchical reward mechanism to enhance the effectiveness of the RL training process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Improves LLMs’ proficiency in PQCs generation.\n- Well-structured and easy to follow\n- A clear summary of quantum optimization problems."}, "weaknesses": {"value": "- The motivation is not accurate\n- Gap between reward mechanism and evaluation metrics"}, "questions": {"value": "- What is QUASAR's motivation? QUASAR look more like to enhance LLM generate better PQC structure and initial parameters, rather than generate quantum circuit. The distinction between these goals should be made clearer.\n- What is the calculate process of the expectation-value reward? In Section 4.2 the expectation-value reward is calculated by the distance between the eigenvalues of the generated circuit and the ground truth circuit. However, in section 4.2.3 this was calculated by the problem specific cost Hamiltonian.\n- Why use JS divergence as a reward. The JS divergence reward encourages the model to generate circuits whose unitaries closely match those of the ground-truth circuits. In contrast, the expectation-value reward and optimization-step reward aim to produce circuits that better approximate the target Hamiltonian. However, there remains an inherent gap between the dataset circuits and the ideal Hamiltonian solution. Therefore, the three rewards are not fully consistent in optimization direction. As the number of qubits increases, the JS divergence metric loses accuracy in capturing the difference between the two distributions, making it a less effective reward for high-dimensional quantum systems.\n- Why SREV is used as the evaluation metric instead of directly using the expectation value percentage? Since the expectation value measures how closely the parameterized quantum circuit (PQC) approximates the target Hamiltonian, it is unclear why SREV was chosen as the primary indicator. According to the paper, SREV appears to better capture the approximation degree between the generated PQC and the target Hamiltonian. However, the experimental results show that increasing the expectation-value reward actually decreases SREV performance, which is counterintuitive. A detailed explanation of this discrepancy and the rationale behind selecting SREV over direct expectation-value measures would greatly improve the clarity of the evaluation section.\n- How the training and test datasets are partitioned.\n- How the prompts are constructed. It's better to give an example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IL8ZqA1pTZ", "forum": "fKKKtEW71h", "replyto": "fKKKtEW71h", "signatures": ["ICLR.cc/2026/Conference/Submission20151/Reviewer_nPXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20151/Reviewer_nPXL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927403949, "cdate": 1761927403949, "tmdate": 1762933192927, "mdate": 1762933192927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors attempt to build quantum circuits efficiently with a specially trained LLM. In the measures defined by the authors, it outperforms unoptimized LLMs like GPT-5."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The application area of the paper is certainly novel. It indicates that superior performance can be obtained from an LLM even on tasks that are not well-suited if it is refined further."}, "weaknesses": {"value": "The central idea, while imaginative, is not sufficiently grounded in the technical realities of quantum computing. Key challenges — such as the exponential scaling of gate requirements with qubit count — are not adequately addressed. Moreover, the paper provides limited information about the scale of the experimental setups, leaving it unclear how complex or realistic the tested instances are."}, "questions": {"value": "- Could the authors clarify the size of the experiments, in terms of qubit count and circuit depth?\n- Do you observe any changes in code generation performance as problem size increases?\n- Are there notable differences in quality when generating dense versus sparse circuits?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Ethics statement is evidently wrong, LLMs were used for generating the results."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LTcecIWkOz", "forum": "fKKKtEW71h", "replyto": "fKKKtEW71h", "signatures": ["ICLR.cc/2026/Conference/Submission20151/Reviewer_Evyu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20151/Reviewer_Evyu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999697650, "cdate": 1761999697650, "tmdate": 1762933179875, "mdate": 1762933179875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}