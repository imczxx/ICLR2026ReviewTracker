{"id": "BjQqvH2LgW", "number": 12496, "cdate": 1758208212844, "mdate": 1759897505870, "content": {"title": "UnCoVAEr: Estimating Causal Concept Effects under Visual Latent Confounding", "abstract": "Estimating the effect of human-interpretable concepts on model predictions is crucial for explaining and auditing machine learning systems, as well as for mitigating their reliance on spurious correlations. Most existing approaches assume complete concept annotations, but in practice some concepts may remain unobserved and act as confounders, biasing causal effect estimates. We introduce **UnCoVAEr** (Unobserved Confounding Variational AutoEncoder), a latent-variable model that partitions image latent representations into confounder-related and non-confounding residual components. This allows us to (i) identify which observed concepts are confounded, (ii) obtain corrected unbiased effect estimates via backdoor adjustment, and (iii) learn confounder-proxy variables that align with underlying latent factors. On a controlled semi-synthetic MorphoMNIST benchmark, we show that UnCoVAEr yields substantially less biased effect estimates than prior methods, providing practitioners with a practical tool for trustworthy concept-level causal inference in partially annotated image datasets.", "tldr": "UnCoVAEr provides a principled approach for estimating causal concept effects on images in the presence of latent confounders", "keywords": ["Concept-based explanations", "Causal effect estimation", "Latent confounders", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/591c7e57d77849de19d2d6c77d974985ee6983a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method called UnCoVAEr to estimate causal effects of interpretable concepts on model outcomes. UnCoVAEr identifies the observed concepts that share a hidden confounder with outcome and estimate causal effect of those concepts by performing backdoor adjustment on the estimated hidden confounder. Experiments are performed on a semi-synthetic dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is written well and easy to understand.\n2. The problem of identifying hidden confounders in image datasets and adjusting them for causal effect estimation is timely and useful to study."}, "weaknesses": {"value": "1. In L259-262, from causal graph in Figure 2, we have $p(Z_C,Z_S\\mid X, C, Y) \\neq p(Z_C\\mid X, C, Y)\\times p(Z_S\\mid X, C, Y) $ because conditional on $X$, $Z_C, Z_S$ become dependent. But such dependence is not modeled in the variational encoder model.\n2. Experiments are performed on only one (semi-synthetic) dataset. This limitation is acknowledged but the paper needs to show results on real-world datasets. Additionally, the experimental setup that the concepts such as thickness/width having causal effect on label is unrealistic.\n3. From the results shown in the main table, UnCoVAEr underperforms compared to baselines in three out of six columns. Overall, the current experimental evidence is not enough to appreciate the usefulness of UnCoVAEr."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQ6fJtcSFH", "forum": "BjQqvH2LgW", "replyto": "BjQqvH2LgW", "signatures": ["ICLR.cc/2026/Conference/Submission12496/Reviewer_Njet"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12496/Reviewer_Njet"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161331179, "cdate": 1761161331179, "tmdate": 1762923371586, "mdate": 1762923371586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to account for unoberved confounding via proxies in high-dimensional (computer vision) settings. They define a causal graph first, for which their assumptions hold, and introduce further assumptions to make the proxy task allow to properly identify the treatment effects they target for. The DAG is designed to hold for tasks in which we assume that observed concepts cause both the image and the targets, while there are also further unobserved confounders present, causally influencing all of the three variables. They further model task-irrelevant latent features that are unobserved as well. Their proposed method is a VAE where the main ELBO logic borrowed from CEVAE, while they additionally actively disentangle latent space into unobserved latent confounding and task-irrelevant features via CLUB-MI. They verify their model performance on the Morpho-MNIST dataset against other methods, ranging from causal proxy estimation to concept-based models from explainable AI."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well written. The idea is well-communicated via figures and text equally. The authors contextualize the work well, making it easy for readers and reviewers to evaluate the current state, the relations to older and newer work. The paper is well structured and easy to follow. Argumentation and motivation are clearly articulated. This reviewer further appreciates that all the assumptions are made explicit and logical. While this should be the norm in causality or causally-inspired papers, it unfortunately is not; this makes the paper stand out in this aspect.\n\nThe method borrows heavily from CEVAE [1], and can be seen as a neat and important extension of this method. Thus, it justifies its significance. This reviewer generally likes the idea of using proxies when possible, to get better estimations. This work fills in this gap nicely in the field of computer vision.\n\nFinally, the authors submitted their code with this submission, which is highly appreciated and heavily increases credibility in terms of reproducibility.\n\n[1] Causal Effect Inference with Deep Latent-Variable Models"}, "weaknesses": {"value": "Although this work shows potential, it does not meet the quality of papers publisehd at ICLR:\n\nMajor 1: The results are not convincing. While transparently showing that the proposed methods do not outperform in every category is highly appreciated, the multi-confounding and even common confounder parts raise questions regarding the efficacy of the method or the integrity of the data to a certain degree. This problem is directly related to major issue 2.\n\nMajor 2: only a single dataset. The results don't suggest a clear trend on the data. The manuscript would benefit from an evaluation on more datasets, or a detailed exploration why the evaluation has oddities like naive being good at multi-confounding vs. complex methods being bad. The dSprites dataset as used in [1] would be a natural extension. This dataset has even been used not only for instrumental variable regression, but also in proxy tasks, similar to this work, see [2, 3]. Other papers also solved similar evaluation issues with downstream or proxy tasks. In this case, it would have been to estimate causal effects for robust/unbiased predictions in the field of bias mitigation or domain generalization.\n\nMajor 3: While the contextualization of the paper is very good and very relevant papers are cited, some of these are missing as appropriate baselines. It is understandable why the authors chose the CaCE method as a baseline, although it is from a different field (XAI), as they faced similar issues in disentangling the latents for unobserved confounding. However, CBM and Res-CBM seem less suitable. It would have been a much better comparison against other causal or proxy methods. DFPV [3] and NMMR [4] are good candidates for example. Another method to conceptually compare to is [5]. While [5] uses a different causal graph, it can be broken down to the same structure. They further separate the latent space into causally relevant and irrelevant parts, also employing an interesting training / inference procedure using VAEs and predictive models. They especially include robustness and bias mitigation experiments to have a proxy task to validate that their method is focusing more on causally stable features rather than spurious / confounding attributes. Finally, another method that comes to mind also disentangles causally relevant and irrelevant unobserved latents in a combined VAE + diffusion approach in adversarial robustness [6]. This method especially resembles the one of this submission, as they also use the CLUB-MI criterion to disentangle these two latent parts.\n\nMedium: while the code base is provided, a summary of implementation details in the appendix would be highly appreciated, given that hyperparameters, and model/architecture design-choices can impact a ranking of models heavily. After skimming the code and config files, a hyperparameter search or model choices could not be identified, which somewhat limits the significance of the results.\n\n[1] Learning Deep Features in Instrumental Variable Regression, Xu et al.\n[2] Deep Learning Methods for Proximal Inference via\nMaximum Moment Restriction\n[3] Deep Proxy Causal Learning and its Application to\nConfounded Bandit Policy Evaluation\n[4] Deep Learning Methods for Proximal Inference via\nMaximum Moment Restriction\n[5] Causal Transportability for Visual Recognition\n[6] CausalDiff: Causality-Inspired Disentanglement\nvia Diffusion Model for Adversarial Defense"}, "questions": {"value": "Could the authors provide additional results as suggested in the weaknesses section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kBeGfuNhTA", "forum": "BjQqvH2LgW", "replyto": "BjQqvH2LgW", "signatures": ["ICLR.cc/2026/Conference/Submission12496/Reviewer_kuUE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12496/Reviewer_kuUE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890269910, "cdate": 1761890269910, "tmdate": 1762923371067, "mdate": 1762923371067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a latent-variable model that explicitly partitions an image's latent representation into two independent parts: a confounder-related component and a non-confounding residual component.The model is trained by maximizing ELBO in conjunction with auxiliary losses and a CLUB-based mutual information estimator. This approach enables the model to estimate the ATE of an arbitrary concept $C_i$ on a label $Y$, thereby reducing bias.In the experiments, the authors evaluated the model's ability to estimate concept ATE. UnCoVAEr demonstrated superior performance across all three different confounder settings evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is clearly written and easy to understand\n- This paper proposes a novel bias-reduction model that achieves unbiased causal effect estimation via the backdoor adjustment.\n- The experiments demonstrate UnCoVAEr's effectiveness at ATE estimation across various scenarios."}, "weaknesses": {"value": "- Although the authors suggest the model's potential applicability in medical imaging scenarios, the paper lacks experimental validation on actual medical datasets. The evaluation was restricted to synthetic data.\n\n- The training process is complex and training loss is a weighted sum of multiple components. However, the paper provides no analysis of how these weights were determined, lacks a sensitivity analysis of these hyperparameters, and does not discuss whether this weighting scheme generalizes to other cases."}, "questions": {"value": "- Given the model's  potential for medical imaging, could the authors provide an evaluation on a relevant medical dataset, eg. the X-ray Grading dataset or other real datasets?\n\n- Could the authors provide a parameter sensitivity analysis? Specifically, it would be important to know how sensitive the model (e.g., the loss component weights) is to different datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rlF87xqnse", "forum": "BjQqvH2LgW", "replyto": "BjQqvH2LgW", "signatures": ["ICLR.cc/2026/Conference/Submission12496/Reviewer_S2Mj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12496/Reviewer_S2Mj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137229246, "cdate": 1762137229246, "tmdate": 1762923370608, "mdate": 1762923370608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the estimation of causal effects, Average Treatment Effect (ATE), and Individual Treatment Effect (ITE), when an image variable is involved in a backdoor setting. Let $X$ denote an image and $C_i$ an observed concept extracted from that image. The goal is to estimate $E[Y \\mid do(C_i)]$ (ATE) and $E[Y \\mid do(C_i), X]$ (ITE), under the assumption that some unobserved concepts within the image can serve as a backdoor set for the pair $(C_i, Y)$.\n\nTo achieve this goal, the paper proposes UnCoVAEr, a VAE-based framework that learns a latent representation $Z_c$ from images, intended to serve as a proxy for the unobserved confounders. And then using the learned $Z_c$ for estimating ATE. The effectiveness of UnCoVAEr is demonstrated on a semi-synthetic dataset derived from Morpho-MNIST."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. The motivation and the worked problem are interesting, essential, and important when estimating ATE with high-dimensional variables. \n\n**S2**. The ablation study of UnCoVAEr explores the design choices from multiple valuable perspectives, and the results are insightful."}, "weaknesses": {"value": "**W1**. The paper lacks theoretical guarantees or discussion on **why UnCoVAEr should be able to learn a valid backdoor representation** $Z_c$. \nIn other words, it is unclear why the architecture proposed in Section 4 satisfies the causal graph assumptions presented in Figure 2. The intuition seems to be that $Z_c$ captures image concepts correlated with $C_i$, while $Z_s$ captures concepts independent of both $C_i$ and $Z_c$. However, correlation does not imply causation: $Z_s$ may contain **descendant concepts** of $C_i$, and adjusting for such descendants could lead to biased causal effect estimates. Therefore, a more formal theoretical justification is needed to explain why the proposed architecture recovers a latent representation aligned with the causal goal.\n\n**W2**. The design of the encoder \\{\\phi_c, \\phi_s\\} for estimating the posterior $q(Z_c, Z_s \\mid X, C, Y)$ is confusing. In **line 261**, the variational posterior is factorized as:\n\n$q_{\\phi_c, \\phi_s}(Z_c, Z_s \\mid X, C, Y) = q_{\\phi_c}(Z_c \\mid X, C, Y)q_{\\phi_s}(Z_s \\mid X, C, Y)$\n\nFirst, this factorization appears inconsistent with the independence constraints in Figure 2: $Z_s$ should not be independent of $Z_c$ given $X$, because $X$ is a collider. Second, in the implementation, $\\phi_c$ and $\\phi_s$ share the same backbone (implying that they are dependent), while mutual information between $Z_c$ and $Z_s$ is minimized via a regularizer. It is unclear whether the factorization in line 261 is encouraged or discouraged during training, and this should be clarified.\n\n**W3**. **The model design for handling multiple observed concepts $C_i$ is not clearly explained**. Although the paper provides a high-level discussion (lines 293â€“296), it remains unclear how the encoder and decoder are structured when different causal relationships exist among the observed concepts. For example, if $C_j$ is a parent, descendant, or independent of $C_i$, should $C_j$ still be included as input to the encoder and decoder? Should such $C_j$ be included in Equation (7) when estimating ATE for a specific $C_i$?\n\n**W4**. The experimental evaluation is limited in terms of **dataset diversity**. All experiments are conducted on a semi-synthetic Morpho-MNIST dataset. It remains unclear how the method performs on higher-dimensional or more realistic data. At minimum, a semi-synthetic experiment based on another dataset (e.g., CelebA-HQ or a medical imaging dataset) would strengthen the empirical validation.\n\n**W5**. The design of the synthetic **data generation process** is limited. The experiments only consider three settings and involve just four variables. Both the ground-truth generation functions for $C$ and $Y$, and the number of variables, should be more diverse. It would be beneficial to define a family of generation functions and sample from it to create multiple ground-truths. Also, additional variables (e.g., color, background texture) could be incorporated to create richer causal structures.\n \n**W6**. The experiments lack **robustness analysis**. It is important to evaluate how violations of key assumptions (e.g., assumption P1 or the causal structure in Figure 2) affect causal effect estimation. Understanding sensitivity to assumption violations is critical before applying the method in real-world scenarios.\n\n**W7**. Although the paper discusses Individual Treatment Effect (ITE) estimation, there is no corresponding evaluation in the experiments. The experiments focus only on ATE estimation, and it remains unclear whether the proposed method can accurately estimate ITE or how its performance compares to baselines."}, "questions": {"value": "**Q1**. In Figure 3, **the confounding detection rate appears lower when one confounder is observed compared to the case where all confounders are unobserved**. Could the authors provide insights into why this occurs? If this behavior consistently appears across different settings, does it imply that the proposed method should, in some cases, reject or ignore certain observed inputs in order to achieve better performance?\n\nPlease refer to the weaknesses section for additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CNrJSesrz3", "forum": "BjQqvH2LgW", "replyto": "BjQqvH2LgW", "signatures": ["ICLR.cc/2026/Conference/Submission12496/Reviewer_zT3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12496/Reviewer_zT3G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138898855, "cdate": 1762138898855, "tmdate": 1762923369713, "mdate": 1762923369713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}