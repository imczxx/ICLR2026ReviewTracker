{"id": "C8PzDMO8Cs", "number": 4061, "cdate": 1757594132454, "mdate": 1759898055189, "content": {"title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration", "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) is a powerful method for enhancing the reasoning abilities of Large Language Models, but its full potential is limited by a lack of exploration in two key areas: \\textbf{Depth} (the difficulty of problems) and \\textbf{Breadth} (the number of training instances). Our analysis of the popular GRPO algorithm reveals a bias that down-weights difficult, low-accuracy problems, which are crucial for improving reasoning skills. To address this, we introduce Difficulty Adaptive Rollout Sampling (DARS), a method that re-weights difficult problems by using targeted, multi-stage rollouts. This approach increases the number of rollout outcomes for these harder problems according to our proposed re-balancing schedules and leads to consistent gains in \\textit{Pass@K}. We also found that simply enlarging the rollout size isn't effective and can even harm performance. We also investigated the role of breadth by scaling the batch size and using full-batch updates. This significantly improved \\textit{Pass@1} performance by maintaining high token-level entropy, which indicates continued exploration and reduced gradient noise. Finally, we present DARS-Breadth, a combined approach that uses DARS with a large breadth of training data. This method demonstrates simultaneous gains in both \\textit{Pass@K} and \\textit{Pass@1}, confirming that depth (adaptive exploration) and breadth (scaling the training data) are orthogonal and essential dimensions for unlocking the full reasoning power of RLVR.", "tldr": "A method to improve both Pass@1 and Pass@K performance for RLVR in complex reasoning.", "keywords": ["RLVR", "Reasoing with LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/875134699c774e3d50c8e0ecc5a4e557e34de4cc.pdf", "supplementary_material": "/attachment/ed9eb53ab4d4562a63f4b8b2534964156db1fc23.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes RLVR and argues that existing methods such as GRPO underweight hard problems (depth bias). It proposes Difficulty-Adaptive Rollout Sampling (DARS) to allocate more rollouts to low-accuracy samples and examnines breadth scaling (large-batch updates) as a complementary factor. The authors calim that combining the two (DARS-breadth) yields simultaneous improvements in Pass@1 and Pass@K across math reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides the intuition that GRPO (variants) emphasizes medium-difficulty problems and train less on challenging problems, through math derivation by showing A_{group} = 2 N u (1 - u). \n2. The paper evaluates the DARS and variants on AIME, OlympiadBench, Minerva, MATH500 with multiple model sizes, showing consistent performance trends."}, "weaknesses": {"value": "1. The performance comparison between GRPO and DARS is not controlled for solution length. Since solution length is generally positively correlated with correctness, and DARS places more emphasis on hard problems that naturally require longer reasoning, it is possible that DARS merely encourages longer generations rather than genuinely improving reasoning ability.\n2. The “breadth scaling” results are unsurprising—larger batch sizes often stabilize RL training—and the idea itself is not novel.\n3. The claimed synergy between depth and breadth is not convincingly demonstrated. The improvements appear largely additive rather than reflecting a meaningful interaction between the two dimensions.\n4. The methodology section is difficult to parse, with inconsistent notation and somewhat heavy presentation. The paper would benefit from clearer mathematical exposition and cleaner notation."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WqOnk4cWh8", "forum": "C8PzDMO8Cs", "replyto": "C8PzDMO8Cs", "signatures": ["ICLR.cc/2026/Conference/Submission4061/Reviewer_wQQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4061/Reviewer_wQQe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523294470, "cdate": 1761523294470, "tmdate": 1762917159417, "mdate": 1762917159417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper firstly tries to present a bias in GRPO which allocates more attention to medium-difficulty problems. Then, they present DARS: Difficulty Adaptive Rollout Sampling, a technique that allows simultaneous improvements in Pass@1 and Pass@k metrics during RLVR training."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is presented well and studies an important problem, that is, trying to mitigate diversity collapse during training with GRPO which has been observed in a couple of different papers in the literature. Their technique is also novel and doesn't seem too hard to implement. They also provide code in the supplementary material which is useful for the reviewer."}, "weaknesses": {"value": "This paper has a couple of issues:\n\n1. Firstly, they do not discuss how they estimate pass@k. This is an extremely important thing to figure because this quantity has a couple of different estimators. Digging through the code, I could figure out that when there are 128 rollouts, and you want to estimate pass@128, the estimator essentially becomes $$\\mathbb{1} \\left[\\ r_1 = 1 \\lor r_2 = 1 \\lor \\cdots \\lor r_{128} = 1\\right]$$. This is important to place in the paper.\n\n2. It is very important to note that the above estimator has high variance (compared to a plugin or bootstrap estimator) especially when pass rates for a prompt are very low. Therefore, it is very hard to believe the gains are consistent unless there are confidence intervals for pass@128. Could the authors make confidence intervals and specifically describe how they made them for the numbers in Table 1? \n\n3. The numbers for the base model in Table 1 for the 1.5B on Math500 seems rather low. For instance, take a look at the Table 2 of Qwen2.5 Math Technical Report [1]. The reported number in this paper is 35.1 and the number in the tech report is 49.8. This seems to be a rather large discrepancy. Could the authors try to replicate their numbers and see how far they can get? \n\n4. What is the 'cumulative advantage' term that the authors define supposed to represent? There is no mathematical justification of this term. The authors mention that this term is supposed to represent the bias in GRPO. However, there is no clear definition of what quantity is exactly biased here. Could the authors elaborate on what this term is supposed to mean.\n\n5. The number of extra rollouts for the harder prompts is mostly heuristic and lacks any solid mathematical basis. \n\n\n[1] QWEN2.5-MATH TECHNICAL REPORT: TOWARD MATHEMATICAL EXPERT MODEL VIA SELF-IMPROVEMENT (https://arxiv.org/abs/2409.12122)"}, "questions": {"value": "Please address the concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5V87YWMnnl", "forum": "C8PzDMO8Cs", "replyto": "C8PzDMO8Cs", "signatures": ["ICLR.cc/2026/Conference/Submission4061/Reviewer_qq1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4061/Reviewer_qq1K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797015704, "cdate": 1761797015704, "tmdate": 1762917159171, "mdate": 1762917159171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies RLVR for reasoning LLMs and shows that two important factors: depth (pushing on hard problems) and breadth (batch size)—must be optimized together. It first shows that GRPO/Dr.GRPO’s group-based cumulative-advantage underweights high-difficulty items, capping Pass@K.   The authors then propose DARS, which does a light pre-rollout to estimate per-question difficulty and then reallocates extra rollouts to harder items via Equal-Treatment (ET) or Hardness-Weighted (HW) schedules; in parallel, they scale breadth by replacing PPO mini-batches with full-batch updates across multiple epochs to sustain exploration entropy and raise Pass@1."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The main concepts are well defined, and the analysis of both is clear. I also appreciated Figure 2, which succinctly illustrates the issues with default sampling and how DARS resolves them.\n\n2. The paper is well written and easy to follow.\n\n3. I really like the Pass@1-Pass@k visualization idea and the analysis is very clear."}, "weaknesses": {"value": "1. The evaluation protocol is potentially problematic. For the baseline, the authors pick the checkpoint with the highest Pass@1, which (per Figure 7) may not correspond to the best Pass@128. For DARS, they then choose the checkpoint that surpasses the baseline on Pass@1 and has the highest Pass@128. This selection strategy may inflate the reported Pass@128 improvement.\n\n2. In Table 1, please report uncertainty (e.g., error bars or standard deviations). Also, which sampling temperature is used for each model? Prior work suggests RLVR sharpens the distribution [1], so using a single temperature across models may be unfair [1, 2, 3]. It would be more informative to report the best Pass@128 under by performing a temperature sweep per model/checkpoint. Do the gains remain meaningful with error bars and sampled at model-specific optimal temperatures?\n\n\n3. While Eq. 2 (cumulative advantage) is used to characterize training effects, it would help to also discuss/update magnitudes (e.g., gradient norms or per-example gradient contributions). A large cumulative advantage does not necessarily imply a large training impact if the underlying gradients are small."}, "questions": {"value": "1. In Table 1, “Pass@1 (Avg@128)” is unclear. Are these the same quantity? Pass@1 can be computed via greedy decoding (temp=0) or estimated from 128 rollouts at nonzero temperature using an unbiased estimator. Which definition are you using?\n\n2. In Figure 9, the gains appear more substantial and consistent for Llama-3, while Pass@K improvements seem to diminish for Qwen. Do you have comparable baselines for Llama-3? Any hypotheses for why the effect is stronger on Llama-3?\n\n3. Why focus on Qwen-Math models given their extensive math pretraining? Have you tried non-math tasks (e.g., Reasoning Gym) on Qwen models, or alternative bases such as Qwen-Instruct or other families?\n\n\n\n\n[1] Reasoning with Sampling: Your Base Model is Smarter Than You Think https://www.arxiv.org/abs/2510.14901\n\n[2] Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach? https://arxiv.org/pdf/2502.17356v1\n\n[3] Can GRPO Help LLMs Transcend Their Pretraining Origin? https://arxiv.org/abs/2510.15990"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9phvNvyPDc", "forum": "C8PzDMO8Cs", "replyto": "C8PzDMO8Cs", "signatures": ["ICLR.cc/2026/Conference/Submission4061/Reviewer_8YWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4061/Reviewer_8YWC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968202629, "cdate": 1761968202629, "tmdate": 1762917158464, "mdate": 1762917158464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}