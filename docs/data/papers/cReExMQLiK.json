{"id": "cReExMQLiK", "number": 14231, "cdate": 1758230838600, "mdate": 1759897382377, "content": {"title": "How Reliable is Language Model Micro-Benchmarking?", "abstract": "Micro-benchmarking offers a solution to the often prohibitive time and cost of language model development: evaluate on a very small subset of existing benchmarks. Can these micro-benchmarks, however,  rank models as consistently as the full benchmarks they replace? And can they rank models more consistently than selecting a random subset of data points? In many scenarios, we find that the answer is no. We introduce a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. This approach can determine which model pairs can be ranked correctly by a micro-benchmark, allowing for a finer-grained analysis of the trade-off between micro-benchmark size and reliability.\nPrior work has suggested selecting as few as 10 examples; we find that no micro-benchmarking method can consistently rank model pairs 3.5 points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In order to consistently rank model pairs with relatively similar performances, we show that often as many as 250 examples must be selected, at which point random sampling is competitive with existing micro-benchmarking methods. When comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25 examples, we find that more than half of pairwise comparisons are not likely to be preserved. Our work provides actionable guidance for both micro-benchmark users and developers in navigating the trade-off between evaluation efficiency and reliability.", "tldr": "", "keywords": ["efficient evaluation", "meta-evaluation", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a8921ba19ed9dfedc1689ce7ab0b45130d6972c.pdf", "supplementary_material": "/attachment/ebba49c85fd1c6e3951e33e335f99cea30b3e7cd.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates the reliability of micro-benchmarks, small subsets of larger evaluation datasets, in faithfully ranking model performance. The authors introduce a new meta-evaluation metric, MDAD (Model Disagreement Agreement Distance), which quantifies how consistently a micro-benchmark preserves the pairwise ranking of models relative to the full benchmark. Unlike other meta-benchmarking strategies, the authors claim that MDAD provides a finer-grained view of evaluation stability, revealing when and why model rankings diverge under extreme data reduction. By analyzing MDAD across various subset selection strategies, the study highlights the trade-offs between sampling efficiency and ranking fidelity, offering practical insights into how different sampling techniques perform as dataset size decreases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The experiments demonstrate MDAD’s potential to offer deeper insights into selecting the most reliable micro-benchmarking approach (Fig. 3).\n* The experiments further show that MDAD provides more fine-grained and informative analysis than existing meta-evaluation metrics for micro-benchmarks (Fig. 4).\n* The authors appropriately situate their work within the broader literature, clearly articulating how their contributions extend prior research."}, "weaknesses": {"value": "* Some figure titles and labels are difficult to read; improving their legibility would make the results clearer and more accessible.\n* Given that meta-evaluations are conceptually dense and rely heavily on prior work, the paper would benefit from a thorough proofreading and clarity-focused revision to improve readability.\n* Including comparative tables or summary diagrams that explicitly contrast this meta-evaluation method with existing approaches would help highlight its unique advantages.\n* The paper’s contribution is summative and complementary, it extends and provides additional insights useful to decide on existing micro-benchmarking techniques rather than proposing an entirely new framework.\n* The authors provide source code in the supplementary materials, but it remains unclear whether these materials will be publicly released to support transparency and reproducibility."}, "questions": {"value": "* What potential applications beyond evaluation do you foresee for the proposed metric? Could MDAD, or a variant of it, be extended to support data selection strategies for supervised fine-tuning (SFT) or other training workflows?\n* Which experimental assets will be publicly released, for example, code, sampled subsets, or analysis scripts? Please provide specific details regarding the planned scope and accessibility of these materials."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YPcVxmFUmG", "forum": "cReExMQLiK", "replyto": "cReExMQLiK", "signatures": ["ICLR.cc/2026/Conference/Submission14231/Reviewer_E4xW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14231/Reviewer_E4xW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854101273, "cdate": 1761854101273, "tmdate": 1762924685382, "mdate": 1762924685382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Minimum Detectable Ability Difference (MDAD), a meta-evaluation measure. It uses MDAD to assess different micro-benchmarking methods, compare MDAD to other meta-evaluation measures, and analyze micro-benchmarking efficiency-reliability tradeoff."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and structured.\n2. The proposed measure is well motivated and intuitive.\n3. The experiments are well designed, including a few benchmarks, a large number of models, across different subset sizes.\n4. The figures are effective in showcasing the relevant results.\n5. Sections 5.1 and 5.3 are doing a good job and demonstrating why MDAD adds value above other methods."}, "weaknesses": {"value": "1. Figures are hard to read.\n2. The conclusions themselves are relatively known and expected. This is reasonable. The new measure and quantifiable results are new and interesting."}, "questions": {"value": "The results echo a broader message of \"the bitter lesson\". Specified methods work well in small-scale settings (model sizes, number of examples, etc.) but do not generalize to larger scales. What is your perspective from that context? What is its relation to the micro-benchmarking methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1K4XOuHh3l", "forum": "cReExMQLiK", "replyto": "cReExMQLiK", "signatures": ["ICLR.cc/2026/Conference/Submission14231/Reviewer_1D3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14231/Reviewer_1D3U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916989815, "cdate": 1761916989815, "tmdate": 1762924684966, "mdate": 1762924684966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a comprehensive study on the existing techniques for language model micro-benchmarking, which aims to improve the evaluation efficiency by only selecting a representative subset of the whole benchmark for evaluation. Specifically, this paper introduces a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. Evaluation shows that no micro-benchmarking method can consistently rank model pairs when the performance difference is small and, when the performance difference becomes larger, random sampling has a comparable performance with other existing micro-benchmarking techniques."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper explores an important and interesting research direction.\n- The paper is well-written and easy to follow.\n- This paper proposes a novel metric, namely MDAD, to evaluate the effectiveness of micro-benchmarking, which is more robust and reliable than existing metrics such as mean\nestimation error and rank correlation.\n- The evaluation has involved a large number of comprehensive experiments to study the performance and impact of existing micro-benchmarking techniques."}, "weaknesses": {"value": "- While the paper has conducted a very comprehensive evaluation, there’s a lack of concrete case studies in the paper to provide a more intuitive discussion of the results. The readability of the paper can be further improved with more case studies."}, "questions": {"value": "None beyond the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yaqO52QJnZ", "forum": "cReExMQLiK", "replyto": "cReExMQLiK", "signatures": ["ICLR.cc/2026/Conference/Submission14231/Reviewer_h3U2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14231/Reviewer_h3U2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983198312, "cdate": 1761983198312, "tmdate": 1762924684018, "mdate": 1762924684018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an evaluation framework for micro-benchmarking. Given a full benchmark $D_{full}$ and a selection strategy *S*, the proposed metric evaluate how faithfully the ranking obtained on the subset $D = S(D_{full}) \\subset D_{full}$ reflects the ranking on the full benchmark $D_{full}$. They define the *Minimum Detectable Accuracy Difference* (MDAD), which is the smallest performance gap on the full benchmark for which pairwise model rankings on a micro-benchmark remain consistent with those on the full benchmark $D_{full}$. Their findings are multiple. For example, with a budget of only 10 examples,  none of the selection strategies they consider can correctly rank all pair of models which differs by 3.5 points on MMLU-Pro with at least \"80% confidence\". Moreover, as the size of the micro-benchmark increases , random selection increasingly becomes competitive with more sophisticated methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a theoretical approach for evaluating micro-benchmarking. It is important in practice, since assessing large models across multiple full-scale benchmarks can quickly become prohibitively expensive. The proposed approach is valuable in that it provides a way to determine whether given a selection strategy and a budge of $n examples, one can reliably distinguish between models whose performance differs by a specified range `[a, b]`.\n- The experiments cover multiple selection strategies, benchmarks and setups which help broadly assessing how their metric behaves."}, "weaknesses": {"value": "- The authors appear to derive MDAD by applying statistical testing principles to micro-benchmarking (somewhat analogous to subset selection) but this is never made explicit. The paper does not clearly define hypotheses (e.g., $H_0$,  $H_1$ ) or discuss error types, leaving the methodological novelty of their approach uncertain. While the formulation itself may be original, the overall idea seems less so.\n- Despite the discussion in **Section 5.1**, the benefits of MDAD over metrics such as *mean estimation error* and *rank correlation* remain unclear. Each strategy has its own strengths and weaknesses, but the provided examples fail to convey the broader implications."}, "questions": {"value": "- Why is the **conclusion** merged with the **discussion**?\n- Why is **Anchor points** more performing than the other strategies when selecting only few examples ($|D| \\leq 100$) and more generally why is it that it better \"tells apart\" models with a small performance difference better than the other (this advantage disappears as the number of selected examples |$D$| increases)?\n- L235: `We use 50 trials, each with a partition of data points and models.` Does it mean you consider 50 seeds and for each seed you partition your data into two sets (one for selection and the other for generalization)?\n- Figure 3 (right)/Figure 4: Why does **Anchor points** stagnates (in terms of MDAD) and performs worse than the other strategies when the number of selected samples reaches a certain point (typically $\\geq 500$)?\n- Line 458-459: What could explain the fact that selecting micro-benchmarks of subtasks independently results in some sort of slight overfitting?\n- What is the impact of varying the confidence of the ranking in the MDAD from 80% to say 95%? \n- How expensive/time-consuming is it to compute the MDAD?\n- Do you think it is possible to adapt the MDAD to buckets of relative/absolute performance difference in terms of scores given as percentage ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cUEVwv6pax", "forum": "cReExMQLiK", "replyto": "cReExMQLiK", "signatures": ["ICLR.cc/2026/Conference/Submission14231/Reviewer_3saE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14231/Reviewer_3saE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989035249, "cdate": 1761989035249, "tmdate": 1762924683464, "mdate": 1762924683464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}