{"id": "w70fv82Vft", "number": 15640, "cdate": 1758253499155, "mdate": 1759897291599, "content": {"title": "SLAKE: Softmax-Approximated Training-Free Linear Attention with KV-Cache Eviction for Long-Sequence LLMs", "abstract": "Recent advances in transformer-based large language models (LLMs) have enabled inference over contexts as long as 128K tokens. However, the quadratic computational and memory costs of full self-attention remain a fundamental bottleneck at such scales. Prior efforts to mitigate this challenge largely fall into two camps: (i) structural approximations (e.g., linear attention) that reduce asymptotic complexity but typically require costly retraining, and (ii) KV-cache optimizations (e.g., eviction or merging) that are training-free yet inevitably discard information. We introduce Softmax-Approximated Training-Free Linear Attention with KV-Cache Eviction (SLAKE), a novel framework that unifies the complementary advantages of these two paradigms. At its core, SLAKE employs Partially Taylor-Approximated Attention (PTAA), which leverages a first-order Taylor expansion to selectively linearize the Softmax attention kernel. This design enables tokens deemed low-importance via eviction scoring to be processed efficiently with linear attention, while preserving exact Softmax computation for high-salience tokens. To further improve cache efficiency, we propose Value-Aware Budget Scoring (VABS), a new allocation strategy that incorporates value contributions and overcomes key limitations of previous eviction heuristics. Extensive experiments on LLaMA-3 8B demonstrate that SLAKE delivers up to 10$\\times$ inference speedup and 30.8\\% peak-memory reduction on 128K-token sequences, while keeping accuracy loss below 4\\%. To our knowledge, SLAKE is the first training-free approach to jointly integrate linear attention with KV-cache eviction, establishing a new state of the art among long-context, training-free methods.", "tldr": "SLAKE is a training-free framework that combines linear attention and KV-cache eviction to achieve efficient, accurate 128K-token long-context inference.", "keywords": ["large language models", "linear attention", "kv cache compression", "self attention approximation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd2f7ec15646bb68a35f837fc7ca78f706efb1ac.pdf", "supplementary_material": "/attachment/e7d5539f4e15c7e3e6ef802b0afbb307f42c85f6.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a framework that improves the efficiency of long-context inference in large language models without retraining. It introduces Partially Taylor-Approximated Attention (PTAA), which applies a first-order Taylor expansion to partially linearize the Softmax attention kernel so that low-importance tokens can be processed with linear attention while high-importance tokens retain exact Softmax computation. To complement this, it presents Value-Aware Budget Scoring (VABS), a dynamic cache-budget allocation method that accounts for both the approximation error of PTAA and the influence of the value matrix when selecting tokens for eviction. Together, PTAA and VABS combine the advantages of linear attention and KV-cache compression in a training-free manner. Experiments on LLaMA-2-7B, LLaMA-3.1-8B, and Mistral-7B-v0.3 using the LongBench benchmark show that SLAKE achieves up to a 10× inference speedup and 30.8% peak-memory reduction for 128K-token sequences while maintaining less than 4% accuracy loss, establishing a new state-of-the-art among training-free long-context methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel combination of linear attention and KV-cache eviction into a single training-free framework. Its Partially Taylor-Approximated Attention and Value-Aware Budget Scoring components offer new ways to linearize Softmax and allocate cache budgets by explicitly modeling value contributions.\n2. The paper is well-structured, and the main ideas and methodology are easy to follow."}, "weaknesses": {"value": "1. The ablation study is not sufficiently comprehensive to fully support the individual contributions of PTAA and VABS. The current results show average LongBench scores under three configurations (eviction only, +PTAA, +VABS on LLaMA2-7B with a 128-token cache budget).\n2. The experimental design lacks sensitivity analyses for the approximation hyperparameters (e.g., Taylor truncation order, scaling factors in VABS). Reporting how performance and stability vary with these parameters would clarify the robustness of the method.\n3. Several experimental details are missing, including decoding hyperparameters such as temperature, maximum generation length, and top-p values. Reporting these settings would improve the reproducibility and interpretability of the experimental results."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tWNWoG1ywx", "forum": "w70fv82Vft", "replyto": "w70fv82Vft", "signatures": ["ICLR.cc/2026/Conference/Submission15640/Reviewer_4zgC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15640/Reviewer_4zgC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673733093, "cdate": 1761673733093, "tmdate": 1762925902480, "mdate": 1762925902480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SLAKE, a training-free framework to address the quadratic complexity bottleneck in LLM long-sequence inference. SLAKE is the first training-free approach to jointly integrate linear attention with KV-cache eviction. Its core PTAA mechanism uses exact Softmax computation for high-salience tokens while processing low-importance tokens with a Taylor-approximated linear attention. Furthermore, it introduces VABS, a novel cache allocation strategy that incorporates the influence of the Value matrix to overcome limitations of previous eviction heuristics. Experiments show SLAKE outperforms existing training-free methods on LongBench, achieving up to a 10x inference speedup and a 30.8% peak-memory reduction on 128K-token sequences ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces SLAKE, the first framework to be training-free while unifying two distinct optimization paradigms: linear attention and KV-cache eviction. Its core mechanism, PTAA, innovatively retains information from evicted tokens via Taylor approximation instead of discarding it, which directly addresses the information loss problem of standard eviction methods.\n2. SLAKE achieves state-of-the-art results, consistently outperforming other training-free eviction methods like H2O and CAKE on the comprehensive LongBench benchmark. This accuracy gain is driven in part by VABS, a more insightful scoring metric that corrects a key flaw in prior methods by accounting for the Value matrix's influence."}, "weaknesses": {"value": "1. Ambiguous Computational Cost: The paper is unclear about the prefill stage computational cost. Both VABS (requiring \"true attention\") and PTAA (requiring $x_{i,max}$) could hide an $O(N^2)$ step, which would weaken the efficiency claims.Weak Retrieval \n2. Performance: On NeedleBench (Table 4), SLAKE's performance drops significantly compared to Full KV, suggesting the approximation struggles with high-fidelity information retrieval tasks.\n3. Hyperparameters: VABS introduces hyperparameters $\\alpha$ and $\\beta$ (Table 3), and the paper does not discuss their sensitivity or the cost of tuning them.\n4. Limited Model Scale Validation: The paper's experiments are confined to 7B and 8B models. While sufficient to compare against other training-free eviction methods, cited related work (like Linearized LLM) has explored 13B models. This lack of validation on larger-scale models leaves the method's scalability in question."}, "questions": {"value": "1. VABS Cost: How is the VABS score (Eq. 15), which requires the \"true attention output,\" computed during the prefill stage without incurring an $O(N^2)$ cost?\n\n2. PTAA Cost: How is the $x_{i,max}$ normalization term for the Taylor approximation calculated? Is it from all $N$ tokens (implying $O(N^2)$) or only the $w$ kept tokens (which would be an inaccurate normalizer for the evicted tokens)?\n\n3. Prefill vs. Decoding: Does the 10x speedup refer only to the $O(N)$ decoding phase, or is it an end-to-end time including prefill?\n\n4. VABS Tuning: How sensitive is VABS performance to the $\\alpha$ and $\\beta$ hyperparameters? What is the cost of tuning these for a new model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l744D8BldZ", "forum": "w70fv82Vft", "replyto": "w70fv82Vft", "signatures": ["ICLR.cc/2026/Conference/Submission15640/Reviewer_MgG5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15640/Reviewer_MgG5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935048064, "cdate": 1761935048064, "tmdate": 1762925901774, "mdate": 1762925901774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SLAKE, a training-free framework combining linear attention and KV cache attention for efficient long context inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of combining linear attention and KV cache eviction is interesting.\n\n2. The presentation is clear and straightforward."}, "weaknesses": {"value": "1. The main concern I have with this paper is the limited novelty of combining linear attention and KV cache eviction. First, the Taylor-based method to approximate linear attention has been explored in prior work [1]. Moreover, the improvement of considering value tensors seems incremental, offering marginal benefits in both algorithm and system evaluations.\n\n2. The motivation for this paper is still unclear to me. According to Figure 4, there is no significant difference comparing prior methods in terms of memory usage and throughput.\n\n3. More model sizes should be evaluated for scalability, such as 13B/30B.\n\n4. Some system-related works on KV cache compression are missing [2-4].\n\n\n\n\n\n[1] ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention, HPCA 2023.\n\n[2] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management, OSDI 2024.\n\n[3] Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference, MLSys 2024.\n\n[4] ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching, ISCA 2024."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VDS8AvCOgH", "forum": "w70fv82Vft", "replyto": "w70fv82Vft", "signatures": ["ICLR.cc/2026/Conference/Submission15640/Reviewer_xhuW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15640/Reviewer_xhuW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939409769, "cdate": 1761939409769, "tmdate": 1762925901298, "mdate": 1762925901298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SLAKE proposes training-free long-context inference for LLMs by selectively mixing exact Softmax attention with a first-order Taylor linear approximation. A novel Value-Aware Budget Scoring (VABS) decides which tokens stay in the KV-cache; the rest are handled by the cheap linear path. On 128 k-token inputs SLAKE gives ≈ 10× speed-up and 30 % peak-memory reduction versus full-cache while losing < 4 % accuracy on LongBench (Llama-2-7B, Llama-3.1-8B, Mistral-7B). The method is model-agnostic, needs no retraining, and is orthogonal to FlashAttention-2."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes the first approach to unify linear attention with KV-cache eviction without any gradient updates, delivering large wall-clock & memory gains on consumer GPUs.\n2. PTAA kernel keeps pre-trained weights intact; VABS explicitly models value-matrix error amplification, yielding consistent gains across 16 long-context datasets.\n3. Ablation studies, three model families, two cache budgets, needle-in-haystack stress test, and hardware numbers (H100 latency / peak mem) are all reported."}, "weaknesses": {"value": "1. Taylor linearisation of Softmax and “important vs. rest” attention mixing are well-explored ideas; SLAKE’s contribution is largely combinational.\n2. Only average scores are given; no per-task statistical significance, error bars, or worst-case degradation analysis—crucial for safety-critical uses.\n3. Hyper-parameter fragility: VABS needs manually tuned $\\alpha$, $\\beta$, $\\gamma$ per model & budget; no adaptive or online scheme, and no study on sensitivity to these constants."}, "questions": {"value": "1. How does SLAKE behave with longer contexts (256 K–1 M) or larger models (70 B+) where the approximation error may accumulate?\n2. Have you evaluated on code generation, tool-use, or multilingual tasks that may exhibit different attention patterns?\n3. Can VABS be made online & input-adaptive instead of relying on fixed $\\alpha$, $\\beta$, $\\gamma$, and what is the computational overhead of such adaptation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AFtSWucLIA", "forum": "w70fv82Vft", "replyto": "w70fv82Vft", "signatures": ["ICLR.cc/2026/Conference/Submission15640/Reviewer_1mfi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15640/Reviewer_1mfi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090180834, "cdate": 1762090180834, "tmdate": 1762925900245, "mdate": 1762925900245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}