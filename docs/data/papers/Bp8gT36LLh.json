{"id": "Bp8gT36LLh", "number": 10048, "cdate": 1758158533377, "mdate": 1759897678486, "content": {"title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning", "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.", "tldr": "", "keywords": ["LVLMs", "video understanding", "CoT"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8e4e669334989c1103775914b341dff897cf731.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VCR-Bench, a novel benchmark framework designed to comprehensively evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Vision-Language Models (LVLMs) for video. The authors argue that existing benchmarks fall short by focusing only on final answer accuracy, failing to assess the reasoning process itself or to distinguish between perception and reasoning failures. VCR-Bench consists of 859 videos and 1,034 high-quality question-answer pairs, each with manually annotated, step-by-step CoT rationales. Each step is tagged as either 'perception' or 'reasoning'. The paper also proposes a \"CoT Score\" to evaluate the quality of the entire reasoning chain. Extensive experiments on VCR-Bench reveal significant limitations in current LVLMs and pinpoint that the primary bottleneck is in spatio-temporal perception rather than logical reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe work addresses a critical and timely gap in video understanding. By shifting the focus from mere answer accuracy to the quality of the reasoning process, the paper pioneers a more insightful evaluation paradigm that is crucial for understanding and improving advanced LVLMs.\n2.\tVCR-Bench is comprehensive, built from diverse video sources and covering seven distinct task dimensions. The evaluation method, which decouples perception from reasoning and calculates a CoT score based on recall and precision, is rigorous and provides far deeper insights than traditional metrics.\n3.\tThe paper evaluates a wide range of state-of-the-art models, providing a solid and valuable baseline for the research community. The finding that models struggle more with visual perception than logical reasoning is a significant contribution, as is the strong positive correlation (r=0.89) shown between the CoT score and final accuracy, which validates the framework's effectiveness."}, "weaknesses": {"value": "1.\tThe paper states that annotations were generated by Gemini 2.0 and then followed by \"human verification.\" More detail on this verification process is needed to fully assess the quality of the ground truth data. For instance, details on the guidelines given to verifiers and the inter-annotator agreement (IAA) scores would significantly strengthen the credibility of the benchmark.\n2.\tThe experiments reveal a near-total failure of all models on the Temporal Spatial Grounding (TSG) task. While the paper attributes this to the need for precise coordinate extraction, a more detailed qualitative error analysis would be beneficial. Understanding how the models fail (e.g., format errors vs. wildly incorrect localization) could provide more targeted insights for future research.\n3.\tThe framework relies entirely on GPT-4o to judge step matches and correctness. This introduces model-dependent bias—if GPT-4o misinterprets reference steps, CoT scores may be inaccurate. The paper acknowledges this limitation but offers no mitigation (e.g., multiple independent annotators, objective matching metrics)."}, "questions": {"value": "1.\tYou mention extracting 64 frames for models without native video support. How did you select these frames (e.g., uniform sampling, keyframe detection)? Did you test if frame selection methods impact model performance, and if so, why did you choose 64 frames specifically?.\n2.\tCould you elaborate on the human verification stage of your annotation process? Specifically, what guidelines were provided to the human verifiers, and did you measure the inter-annotator agreement to ensure consistency?\n3.\tThe near-zero accuracy on the TSG task is a striking result. Beyond the difficulty of coordinate extraction, did your analysis reveal any common failure patterns? For example, do models tend to identify the correct object but fail at localization, or do they fail at understanding the prompt altogether?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "waslqnfI5V", "forum": "Bp8gT36LLh", "replyto": "Bp8gT36LLh", "signatures": ["ICLR.cc/2026/Conference/Submission10048/Reviewer_YcUw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10048/Reviewer_YcUw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760622087872, "cdate": 1760622087872, "tmdate": 1762921447940, "mdate": 1762921447940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VCR-Bench, a novel benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in Large Vision-Language Models (LVLMs). The authors correctly identify a critical gap in existing evaluations: current benchmarks typically only assess the accuracy of the final answer, neglecting the intermediate reasoning process. This makes it impossible to determine if a failure stems from flawed perception or flawed logic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. VCR-Bench provides 1,034 high-quality question-answer pairs over 859 videos. The core contribution is that each sample includes a detailed, manually-annotated, stepwise CoT rationale. Crucially, each step in this rationale is tagged as either a perception or reasoning step, allowing for an unprecedentedly granular analysis.\n2. VCR-Bench proposes a novel \"CoT score\" that measures the precision and recall of generated reasoning steps against reference steps. Experimental results demonstrate a strong correlation between the proposed CoT score and model accuracy."}, "weaknesses": {"value": "1. The benchmark does not evaluate several recently strong multimodal systems (e.g., GPT-5, Gemini-2.5-Pro, Claude 4, and leading open-source models). Including them would better characterize the current frontier and reduce the risk that conclusions reflect outdated model capabilities.\n2. Results rely on one judge family. Please assess stronger or different judges (e.g., GPT-5 or dedicated reasoning models such as Grok, plus at least one open-source judge) and report agreement (e.g., κ/ρ) and rank stability across judges.\n3. Several takeaways (e.g., perception underperforms reasoning; scaling helps) align with widely reported community observations. To strengthen novelty, consider analyses that go beyond confirmation, such as controlled perturbations (occlusion, motion blur, temporal shuffles), error taxonomy by step type, or causal probes that isolate which perception failures drive end accuracy."}, "questions": {"value": "1. See Weaknesses\n2. You report high correlation between CoT score and answer accuracy. Does this imply accuracy alone suffices? Please quantify cases where accuracy is correct but CoT is low (or vice versa), and analyze whether CoT captures diagnostic failures (e.g., missed perception steps) that accuracy alone cannot reveal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNpE06K5yX", "forum": "Bp8gT36LLh", "replyto": "Bp8gT36LLh", "signatures": ["ICLR.cc/2026/Conference/Submission10048/Reviewer_pcgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10048/Reviewer_pcgp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473300469, "cdate": 1761473300469, "tmdate": 1762921447586, "mdate": 1762921447586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VCR-Bench, a novel benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Vision-Language Models (LVLMs) on video understanding tasks. The authors argue that existing benchmarks are inadequate as they primarily focus on final-answer accuracy, failing to assess the intermediate reasoning process and distinguish between failures in perception versus logical reasoning. VCR-Bench consists of 859 videos and 1,034 high-quality question-answer pairs, spanning seven distinct task dimensions. Crucially, each sample is manually annotated with a detailed, stepwise CoT rationale. The authors propose a new evaluation framework that deconstructs the reasoning process into \"Visual Perception\" and \"Logical Reasoning\" steps. It then uses a GPT-4o-based system to calculate recall, precision, and an F1-based \"CoT score\" by comparing the model's generated reasoning steps against the ground-truth annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical gap in the evaluation of LVLMs. As models become more capable of generating complex, multi-step responses, evaluating only the final answer is insufficient. The motivation, clearly illustrated in Figure 1, is compelling and highlights the need to scrutinize the reasoning process itself.\n\n2. The core contribution—the CoT evaluation strategy—is well-conceived. The deconstruction of reasoning into Visual Perception and Logical Reasoning is an insightful and actionable distinction that allows for a more granular diagnosis of model failures.\n\n3. The paper presents a thorough evaluation across a wide array of 19 models, including top-tier proprietary models and popular open-source alternatives. The findings are highly valuable:"}, "weaknesses": {"value": "1. The authors missed the comparison and discussion about the previous sota video reasoning model, e.g. Video-R1[1], LongVILA-R1[2], and the benchmark proposed missed the discussion of solid long video reasoning benchmark - LongVideo-Reason[2]. I suggest the authors add the discussion and analysis of these related works and datasets.\n\n2. The paper is motivated by the phenomenon of models arriving at the correct answer through a flawed process (Figure 1). While the CoT score is designed to detect this, the paper misses an opportunity to explicitly quantify this effect. An analysis showing the distribution of CoT scores for questions that were answered correctly would powerfully reinforce the benchmark's necessity.\n\n\n[1] Video-R1: Reinforcing Video Reasoning in MLLMs\n\n[2] Scaling RL to Long Videos"}, "questions": {"value": "1. Regarding the LLM-as-Judge methodology: Could you provide data on the reliability of using GPT-4o for the recall and precision judgments? Have you performed an analysis of the agreement between GPT-4o's assessments and those of human evaluators on a sample of the results? This would be critical for validating the evaluation pipeline itself.\n\n2. The performance on the Temporal Spatial Grounding (TSG) task is near zero for almost all models. Do you believe this is primarily a failure of the models to generate coordinates in the specified format, or does it reflect a more fundamental inability to link objects in time and space and report their location? Have you experimented with alternative output formats for this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DCd0vy7cZj", "forum": "Bp8gT36LLh", "replyto": "Bp8gT36LLh", "signatures": ["ICLR.cc/2026/Conference/Submission10048/Reviewer_Yjo8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10048/Reviewer_Yjo8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542415486, "cdate": 1761542415486, "tmdate": 1762921447193, "mdate": 1762921447193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the CoT evaluation of Video-LMM.\nIn detail, this paper comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs, where each pair is manually annotated with a stepwise CoT rationale.\nFurthermore, this work evaluates several closed-sourced and open-sourced MLLM, and shows most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The contribution of CoT VideoQA benchmark is meaningful.\n2. Experiments on several LMMs shows there're still large room for developing LMMs."}, "weaknesses": {"value": "1. The size of the dataset is limited.\n2. For the reasoning process, there may be many possible routes. However, the author cannot handle different reasoning traces.\n3. This paper fail to compare with SOTA closed-source and open-source LMM, *e.g.*, Gemini2.5-pro, GPT-5, GPT-o3, Qwen3, etc.\n4. The reasoning process is annotated by human, however, humans may have reasoning bias. How does the author handle such bias?\n5. I notice the model performance on TSG is extremely low? Could the author provide more analysis on such results? It seems against my observation."}, "questions": {"value": "refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SqacFC5m47", "forum": "Bp8gT36LLh", "replyto": "Bp8gT36LLh", "signatures": ["ICLR.cc/2026/Conference/Submission10048/Reviewer_eAs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10048/Reviewer_eAs4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631286932, "cdate": 1761631286932, "tmdate": 1762921446870, "mdate": 1762921446870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}