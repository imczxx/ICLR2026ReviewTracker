{"id": "tNsrshOGDa", "number": 20006, "cdate": 1758301422839, "mdate": 1759897006623, "content": {"title": "Provable Guarantees from Practical Regularization for Alignment with Human Preferences", "abstract": "Reinforcement learning with human preference feedback is the gold-standard approach for making current AI systems helpful, safe, and aligned with human values. Recent research has demonstrated that there is a tight connection between the objective functions used for alignment with human preferences, and voting rules from social choice theory that aggregate diverse preferences. This connection provides a principled way to study the advantages and disadvantages of a given alignment objective by analyzing the social-choice theoretic properties of the corresponding voting rule. Prior work in this direction has focused on variants of standard alignment objective functions, and connected them with well-known social choice rules such as the Borda count and von Neumann winner rules. However, practical alignment algorithms typically perform regularization to a reference policy in order to maintain the capabilities from pre-training. Such regularization could potentially distort the objective and hence change the social-choice theoretic properties of the corresponding voting rule. To address this question, we study the effect of regularization on the social-choice rules corresponding to standard alignment methods, and discover that in the case of the alignment objective corresponding to the von Neumann winner, regularization strictly improves the social-choice theoretic properties of the rule. At the same time, we prove that the standard RLHF objective, which corresponds to the Borda count rule, offers no such improvement and indeed has clear social-choice theoretic drawbacks compared to the von Neumann winner. Taken together, our results provide principled justification from social choice theory to use the von Neumann winner objective for practical alignment with human preferences.", "tldr": "", "keywords": ["Social choice", "reinforcement learning from human feedback", "AI alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfff9edb34eb9ed772b88c401c7d37b044e97830.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies alignment through probabilistic social choice by inserting a KL term with respect to a reference policy into a pairwise preference relation. For the von Neumann winner (maximal lotteries), the authors optimize a maximin objective over distributions; for Borda, they define a Borda score plus KL relation. They report that the KL-regularized von Neumann winner admits a unique solution, satisfies independence of irrelevant alternatives and population consistency, and is approximately strategy-proof with coalition gain bounded on the order of $1/n$. Pareto is obtained only in a regularized sense due to the reference policy. For the Borda variant, the authors show independence of irrelevant alternatives fails even with KL."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a well motivated question about understanding alignment from a social choice perspective and in particular focuses on the impact of KL divergence."}, "weaknesses": {"value": "### Scope compared to alignment\n\nThe first thing I want to raise is the difficulty of positioning this paper within the broader landscape of alignment and social choice. Below I’ve detailed a few points. I think the paper would be stronger if these were at least addressed or mentioned in the main text\n\n\nThe paper analyzes a maximin game over distributions with KL inside the game for the vNW rule, and a Borda score plus KL objective for the Borda section. This is not the mainstream “maximize expected learned reward subject to a KL penalty to a reference policy” used in standard RLHF. The distinction matters because the pipeline, guarantees, and failure modes differ.\nMoreover, the modeling fixes one candidate set $A$ and studies a single game over $\\Delta_A$. The paper could have benefited from clearly stating this difference up front and framing all connections to Bradley–Terry and Borda as aggregation properties rather than training-objective equivalences.\nAlignment in practice trains a context-conditioned policy with a KL term defined per prompt, not a single global lottery over a static $A$. It neither models the per-prompt ballots that generate comparisons nor the outer expectation over prompts that determines reward and policy updates in practice. Section 6 explains that Bradley–Terry based methods “produce the same rankings as Borda,” then defines a regularized Borda preference over distributions. That analysis ranks alternatives after aggregating pairwise data and is not the same as optimizing the PPO-style RLHF objective with a learned reward and a per-token KL penalty.\nStrategy-proofness is presented as central, but the motivation for RLHF is thin. The paper does not explain when real raters would have incentives or the ability to coordinate misreports in modern preference data collection, nor how this metric maps to downstream harms in model behavior.\n\n### Limited technical depth and novelty\n\nThe main proofs lean on strong convexity on the simplex, which yields stability of the unique equilibrium under small perturbations. These arguments are correct but routine and do not appear to deliver tight constants.\n\n### Axioms applied to a regularized individual relation are normatively odd\n\nThe definition of $\\tilde{P}_h$ makes each rater’s “preference” depend on the reference policy $\\mu$ rather than only on their ranking over alternatives. This departs from the standard unanimity idea that aggregates human preferences rather than mixing them with a modeling prior. That means the rule can rank $\\pi$ over $\\pi'$ even when all raters prefer $\\pi'$ in the ordinary sense, as long as $\\pi$ is closer to $\\mu$. If alignment is meant to elicit preferences independent of the current policy, this redefinition needs a stronger justification than is provided.\n\n### Typos and notation issues\n\nThe paper is hard to read and contains many notational inconsistencies and typos. For example:\n• The paper alternates between “strategy-proof,” “strategy proof,” and “strategy proofness.”\n\n• $n$ denotes the number of raters in Sections 3–5 and the number of alternatives in Section 6.\n\n• Inconsistent notation for $D_{\\mathrm{KL}}$.\n\n• Spelling mistakes at lines 129, 299, 400, 457."}, "questions": {"value": "Please address my concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jJTGIt6Zk7", "forum": "tNsrshOGDa", "replyto": "tNsrshOGDa", "signatures": ["ICLR.cc/2026/Conference/Submission20006/Reviewer_FGJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20006/Reviewer_FGJM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755796247, "cdate": 1761755796247, "tmdate": 1762932911482, "mdate": 1762932911482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes AI alignment through social choice theory, showing that KL-regularization makes the von Neumann winner rule approximately strategy-proof while preserving key axioms like independence of irrelevant alternatives. In contrast, the standard RLHF Borda count rule remains problematic even with regularization, providing theoretical justification for using von Neumann winner objectives in practical alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides an original analysis showing that KL-regularization actually improves the social choice properties of the von Neumann winner rule by making it approximately strategy-proof, while preserving other desirable axioms. This counterintuitive result that regularization can enhance rather than degrade axiomatic properties is a significant theoretical contribution.\n\nThe paper bridges abstract social choice theory with concrete alignment algorithms, providing principled justification for preferring von Neumann winner objectives over standard RLHF (Borda count) methods. \n\nThe systematic evaluation of multiple social choice axioms (independence of irrelevant alternatives, population consistency, Pareto optimality, strategy-proofness) for both regularized and unregularized versions provides a thorough theoretical characterization that advances understanding of alignment methods."}, "weaknesses": {"value": "The theoretical analysis assumes that individual rater preferences can be cleanly aggregated through pairwise comparisons, but real human preferences may be noisy.\n\nThe paper connects to von Neumann winner algorithms like Nash-MD and SPO, it could benefit from discussing how approximation errors, finite sample effects, or optimization challenges in these practical algorithms might affect the theoretical properties. \n\nThe paper provides only one synthetic experiment with 3 alternatives and 64 raters using a constructed counterexample from the proof."}, "questions": {"value": "The authors analysis assumes exact computation of preference relations P_H, but in practice these must be estimated from finite preference data. How do estimation errors affect the theoretical guarantees, particularly the strategy-proofness bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OIUU8IBbwC", "forum": "tNsrshOGDa", "replyto": "tNsrshOGDa", "signatures": ["ICLR.cc/2026/Conference/Submission20006/Reviewer_fHB9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20006/Reviewer_fHB9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763771807, "cdate": 1761763771807, "tmdate": 1762932910654, "mdate": 1762932910654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how KL-regularization to a reference policy alters the social-choice properties ofpreference-based alignment rules. The authors show that the KL-regularized von Neumann winner becomes approximately strategy-proof with individual gains bounded by $O(k/\\tau n)$ while it retains independence of irrelevant alternatives (IIA) and population consistency, but only satisfies a “regularized” Pareto notion. In contrast, KL-regularized Borda (i.e., Bradley-Terry/RLHF-style) still violates IIA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and a pleasure to read.  \n- The paper cleanly explains what is and is not preserved under regularization (i.e., IIA and population consistency are preserved but regularized vs. full Pareto). Although the results are not particularly surprising (and have their limitations, see below), the analysis of social-choice-style properties of the von Neumann winner under regularization is well executed.\n- I also appreciate that the proofs are presented nicely and easy-to-follow (though they are not very involved)."}, "weaknesses": {"value": "- The paper makes unrealistic assumptions about how rater preferences are observed. The whole analysis assumes that we receive every rater’s *full ranking* over all candidates. That is obivously very different from what happens in actual alignment pipelines, where you get noisy pairwise comparisons and the estimation errors / lack of coverage play a crucial role. Even thought the authors motivated their work with the use KL-regularization in practice, this assumption means that there is a large gap between this stylized setting and reality.\n-  The main takeaway, that KL regularizing the preference optimization objective improves strategyproofness, is very expected. Since KL regularization limits how far the solution can deviate from the reference policy, it naturally reduces the influence of individual agents. Though, the paper's contribution here lies in the explicit bound on the approximate strategyproofness. \n-  The regularized Pareto notion feels a bit unsatisfying. It can violate unanimity if the reference policy assigns low mass to good outcomes, and the paper does not offer a principled way to avoid this issue."}, "questions": {"value": "- Can you say anything about the case where we only observe pairwise preference samples from raters? \n- How sensitive are the axioms and bounds to the choice of the reference policy $\\mu$? Are there cases where a poorly chosen $\\mu$ undermines the guarantees or makes the outcome undesirable? Does the choice matter here at all? \n-  The strategyproofness of standard RLHF (i.e., reward maximization/Borda) has been analyzed in recent work [1]. It would be helpful to discuss whether your results connect to or contrast with those findings, especially the known trade offs between social welfare and strategyproofness.\n\n[1] Strategyproof Reinforcement Learning from Human Feedback; Thomas Kleine Buening, Jiarui Gan, Debmalya Mandal, Marta Kwiatkowska, NeurIPS (2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HUi1j3SfVo", "forum": "tNsrshOGDa", "replyto": "tNsrshOGDa", "signatures": ["ICLR.cc/2026/Conference/Submission20006/Reviewer_bi5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20006/Reviewer_bi5D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921950306, "cdate": 1761921950306, "tmdate": 1762932910167, "mdate": 1762932910167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that Nash equilibrium-based preference optimization, when coupled with KL regularization, has much stronger strategy-proofness against misreports of preference, and validates that with numerical experiments. It then shows that classical methods for preference optimization continues to suffer from violation of independence of irrevelant alternatives after KL regularization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Soundness: I followed the derivations/proofs in the body and found no error. I did not check the appendix.\n- Clarity: The motivation and approach is clear. The structure of the paper is clear and the writing is accessible.\n- Significance: There have been incidents of data poisoning of language models, so strategy-proofness of alignment is a timely topic (although the authors included limited experimental validation). Introducing regularization into the picture makes the setup significantly more realistic, and may be a substitute for complicated ways of modeling structures (e.g. Rademacher complexity) of the hypothesis class in the social choice theory of alignment.\n- Originality: I am not aware of any prior work with significant overlap."}, "weaknesses": {"value": "- The claim in the abstract (and elsewhere) that \"the standard RLHF objective [...] offers no such improvement\" is potentially misleading. The paper showed that regularization does not restore IIA for Borda count, but does not rule out, e.g., approximate IIA, or approximate strategy-proofness. When the core message of the paper is comparing vNw against BC and claiming that regularization strengthens the former's position in an asymmetrical way, it would make sense to compare the impact of regularization on vNw vs BC on comparable dimensions.\n- The empirical experiment is a minimal one, essentially a numerical replication of the main theorem statements. It will be helpful to see the extent to which the theory aligns empirically with language modeling experiments, by comparing the impact of distorted preference data on MD/SPO-trained vs RLHF/DPO-trained language models."}, "questions": {"value": "Regarding Section 7 (Experiments):\n- What is the reason for using Nash MD for regularized training, and SPO for unregularized training? There seems to be no obvious reason why Nash MD can be regularized while SPO cannot.\n\nRegarding practical relevance:\n- One traditional justification for approximate strategy-proofness is that the cognitive cost for computing how to misreport preferences outweighs the gains from misreporting [1]. In the case of preference alignment for language models, is this true? What are the simplest examples of good strategies for misreporting preference, when it comes to language model training?\n\n\n[1]  Approximately Strategy-Proof Voting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cuycDypvBR", "forum": "tNsrshOGDa", "replyto": "tNsrshOGDa", "signatures": ["ICLR.cc/2026/Conference/Submission20006/Reviewer_Ysbu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20006/Reviewer_Ysbu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931632987, "cdate": 1761931632987, "tmdate": 1762932909784, "mdate": 1762932909784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}