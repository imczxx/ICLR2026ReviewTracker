{"id": "1OAGf7ntSE", "number": 16016, "cdate": 1758258653668, "mdate": 1759897267340, "content": {"title": "Language Identification in the Limit with Computational Trace", "abstract": "Training on Chain-of-Thought (CoT) traces has empirically shown to dramatically improve the capabilities of Large Language Models (LLMs), yet a formal understanding of its power remains limited. \nIn this work, we investigate the role of training on such computational traces from the perspective of language learnability. We introduce a new learning model, identification in the limit with trace, which augments Gold's classic paradigm [Gold'67] by providing the learner not only with examples from a target language but also with computational traces from the machine that accepts them. \nOur results reveal that access to these traces dramatically enhances the power of the learner. We first prove that with perfect computational traces, the class of all computable languages (those recognizable by Turing Machines) becomes identifiable in the limit. This stands in sharp contrast to Gold's famous impossibility result, which holds even for the simple class of languages that are recognizable by deterministic finite automata.\nWe then analyze the more challenging scenario where the learner has only partial information regarding the computational traces, which are also subject to adversarial corruptions. In this setting, we establish a set of trichotomic results on the amount of error that can be tolerated for the successful identification of language classes across the Chomsky hierarchy.", "tldr": "We define a theoretical model of language identification with CoT, where CoT is defined as having access to computational traces, and we show that with this extra information we can learn Turing machines, thus circumventing classical lower bounds.", "keywords": ["language identification", "complexity theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6ea5a9233d61201936b95732ef73108ff4ae0f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends Gold’s seminal formal language learning framework from learning only based on positive examples to learning based on positive examples and computational traces associated with them. This is relevant to modern language models as chain-of-thought traces have been theoretically linked to such execution traces, making the setting useful to study learnability of algorithms.\nThe authors find that, in stark contrast to Gold’s result, traces make large classes of languages, including all recursively-enumerable languages, learnable. The authors then study learnability under corrupted traces, and find that it makes the learning problem markedly harder; while regular languages remain learnable under a constant fraction of errors, context-free languages and Turing machines require much stricter restrictions on the corruption."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I believe the paper studies a very interesting and useful problem and puts a new spin on an old/classic setting \n\t- In particular, it provides another possible contributing factor behind how CoT helps improve models\n- The exposition and motivation are clear; it’s easy to discern what the paper’s contributions are, andthe  methodology was used/developed\n\t- For example, the proofs are first well-described on simpler models (finite-state automata)\n\t- The related work section is thorough and useful"}, "weaknesses": {"value": "- Although this is not a major drawback, I feel like the connection to generation in the limit, which first appears in the Introduction, is not really justified; I’m not sure how learning with traces is any more connected to generation in the limit than the original learning in the limit setting\n- Very minor, and I don’t think this undermines the theoretical contributions: It is slightly unclear how the results translate into practice; maybe at least describing how this could be used or tested in practice could be useful"}, "questions": {"value": "- As you mention, the results are asymptotic in nature, which is okay. I was just wondering if you have any ideas for next steps, i.e., how one would proceed/extend the results to some complexity bounds? I imagine the methodology would have to be quite different.\n- Can you elaborate on the connection to generation in the limit? It seems like, since there are fewer impossibility results there, traces would not be as useful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8gUpiXazHL", "forum": "1OAGf7ntSE", "replyto": "1OAGf7ntSE", "signatures": ["ICLR.cc/2026/Conference/Submission16016/Reviewer_XJpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16016/Reviewer_XJpB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034027445, "cdate": 1761034027445, "tmdate": 1762926222045, "mdate": 1762926222045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proves theoretical results concerning language identification in the limit in the case when the learner is given an enumeration of positive strings plus the computational traces that demonstrate that an automaton for the language to be identified accepts each string (e.g., a sequence of states in a finite automaton). The main result of the paper is this: unlike the classical case consisting only of positive strings  without computational traces, in which case most interesting language classes are not identifiable, the class of computable languages is identifiable in the limit when computational traces are provided. The authors then prove results for cases when the computational traces contain a certain number of errors for the classes of regular languages, deterministic context-free languages, and computable languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this is an interesting paper with significant, original theoretical results. As the authors point out, these results are relevant to the use of CoT to train LLMs. The paper is written clearly and does a good job of contextualizing itself amid prior work. The robustness results are quite interesting."}, "weaknesses": {"value": "The paper would benefit from some clarifications; see my comments in the Questions section.\n\n1. A minor point, but I would point out in the abstract that you are assuming that the learner only has access to positive examples, not negative examples.\n1. 072: DPDAs correspond to DCFLs, not CFLs.\n1. 183: The definition of DPDA is missing constraints on $\\delta$ that make it deterministic. I think you need to allow $\\varepsilon$ as the popped symbol too."}, "questions": {"value": "1. 088: Do you mean that the number of errors per trace is $O(1)$ with respect to length, not finite?\n1. Fact 2.2: In this example, what is the alphabet $\\Sigma$?\n1. Theorem 3.1: Do we assume all the TMs are deciders?\n1. 310: Does this work if the alphabet is not fixed to {0, 1} ahead of time?\n1. 380: How is it possible for $m(x)$ not to be 0? $U$ is already the set of all states that occur in all traces. Can different instances of the traces for the same $x$ be edited in different ways, or are they always consistent?\n1. 388: This statement doesn't make sense to me. Do you mean \"not every accepting state is from the set $U$\"?\n1. Def 6: What about non-scanning transitions?\n\nTypos:\n1. 267: the the\n1. 371: set states -> set of states"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qNTGAInHe9", "forum": "1OAGf7ntSE", "replyto": "1OAGf7ntSE", "signatures": ["ICLR.cc/2026/Conference/Submission16016/Reviewer_WbPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16016/Reviewer_WbPo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598396655, "cdate": 1761598396655, "tmdate": 1762926221618, "mdate": 1762926221618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An existing result shows that only a very small set of languages can be identified in the limit (i.e., eventually the identifier produces the same prediction forever) from exclusively positive examples provided. The work in this paper demonstrates that a larger class, all Turing Complete languages, can be recognized in the limit when provided with an exact computational trace (the state of one machine which recognizes the language at every step of its computation). Additionally, it demonstrates that this is possible even if there are errors in the trace, with different bounds on the error rates depending on the class of languages discovered."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "While I did not precisely check every single aspect of the theorems, what I did check appears to be completely accurate, and the theorems are quite interesting in their results.\n\nThe fact that the computational trace enables a lot more identifiability is interesting\n\nThe text is written quite clearly"}, "weaknesses": {"value": "This is primarily a framing issue, but I don't really see the relationship between this work and chain of thought in particular, it seems to be mostly about utilizing information about intermediate states in computational models in order to theoretically learn languages in an unbounded computational setting (with no limits on time to process each sample or the number of samples). In practice, chain of thought uses a very small number of examples and an extremely bounded computation.\n\nMinor errors/suggestions:\n\nYou should emphasize early on that “constant number of errors\" means a constant per trace, not a constant overall.\n\nRevisiting the regular language example with traces might be helpful, it made the utility of the trace more obviously useful when I went through the example with a trace and realized that the main thing it provides is a distinction between a model that accepts everything (single accept state) and a model that accepts N things (many states)."}, "questions": {"value": "The algorithm for identifying robustly only seems to me to use the traces solely to identify the number of states. Is this accurate? If so, it should be explicitly stated in the text, as this sounds like a much weaker assumption than having access to the full traces with errors. If not, the additional information gained should be discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OGYLWhhPAO", "forum": "1OAGf7ntSE", "replyto": "1OAGf7ntSE", "signatures": ["ICLR.cc/2026/Conference/Submission16016/Reviewer_evx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16016/Reviewer_evx1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847218053, "cdate": 1761847218053, "tmdate": 1762926221233, "mdate": 1762926221233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the benefit of CoT from the perspective of language identification in the limit with computation. While Gold proved the impossibility of recognizing most interesting language classes without computational traces, the authors show that as long as each $L\\in\\mathcal{L}$ is recognizable by some $M\\in\\mathcal{M}$, the class $\\mathcal{L}$ is identifiable by $\\mathcal{M}$ in the limit if computational traces are available. Furthermore, the authors consider robust language identification, concluding that identification is achievable with finite error, but robust language identification remains impossible even under diminishing error."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work offers an interesting TCS perspective on the role of CoT, connecting the LLM phenomenon with the theory of language identification in the limit.\n\n2. The results on robust identification provide valuable insights into the significance of CoT quality."}, "weaknesses": {"value": "1. The paper lacks intuitive explanations regarding how CoT contributes to identification.\n\n2. There appear to be conceptual gaps between the theoretical model and realistic CoT. For example, the paper’s model consider enumeration over a language, whereas real-world CoT operates on individual instances (i.e., strings $x\\in L$). Moreover, while the robust identification results highlight sensitivity to noise, empirical studies suggest that other factors, such as CoT format or length, may outweigh the correctness of CoT."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9dyt7pjnUi", "forum": "1OAGf7ntSE", "replyto": "1OAGf7ntSE", "signatures": ["ICLR.cc/2026/Conference/Submission16016/Reviewer_6Cej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16016/Reviewer_6Cej"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915496746, "cdate": 1761915496746, "tmdate": 1762926220829, "mdate": 1762926220829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}