{"id": "zHBCOMxuoJ", "number": 8416, "cdate": 1758082467756, "mdate": 1759897785483, "content": {"title": "Learning Privileged Degradation Priors for All-in-One Image Restoration", "abstract": "The central challenge in all-in-one image restoration lies in learning degradation-specific priors to effectively modulate a restoration network. Prevailing approaches tackle this by learning representations that can distinguish between degradation types, often via proxy tasks like classification or contrastive learning. However, a representation optimized for discrimination is not necessarily optimal for restoration, leading to a fundamental objective mismatch. To address this, we introduce the Learning Using Privileged Information (LUPI) paradigm. Our method employs a teacher network granted privileged access to both degraded and clean images during training, allowing it to learn a prior directly guided by the final restoration quality. This process yields an ideal, inherently ``restoration-aware\" prior, which a student network—observing only the degraded input—is then trained to approximate. The learned prior dynamically modulates a restoration backbone for adaptive recovery, enabling our unified model to achieve state-of-the-art performance on benchmarks. Visualizations confirm the learned prior space is semantically structured, revealing intrinsic relationships between degradation types and effectively distinguishing their intensities. The code will be made publicly available upon acceptance of the paper.", "tldr": "", "keywords": ["All-in-One Image Restoration", "Privileged Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39f4beae8f5b232adcfc082ea6f264b71d0642d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper's approach closely resembles DiffIR [1]. In stage 1, ground truth (GT) is directly used to generate degradation-related representations, and in stage 2, a network approximates these representations. However, I am curious why the fusion of clean and degraded images results in a representation called \"Degradation\".  Since clean images do not show degradation, does combining them affect the process of extracting degradation information?\n\nWriting: This paper is well-structured and logically coherent.\n\nExperiments: This paper still lacks validation on more comprehensive datasets. However, the description in the introduction is vague and hard to understand.\n\nOverall, despite some unclear points and motivations, with proper explanations, this paper can contribute to the field of image restoration.\n\n[1] DiffIR: Efficient Diffusion Model for Image Restoration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured.\n2. The significant improvement on the 5D all-in-one restoration task demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. I have some questions about the third paragraph of the introduction. The paper mentions that the current challenge lies in \"to disentangle the unknown degradation from the unknown clean content using only a single, mixed signal.\" Is there any experimental verification for this statement? For example, can the restoration model not distinguish between clean and degraded images? Secondly, \"it can only learn a good prior if the restoration network is already effective enough to provide a clear gradient.\" What is a clear gradient? Why do networks with better restoration results obtain \"clear\" gradients during backpropagation?\n\n2. Comparisons with some recent works, such as MaskDCPT [1], which has achieved over 32dB PSNR on a 5D all-in-one restoration task, seem to prove that using explicit degradation classification can yield outstanding results. Does the problem mentioned in the paper (second paragraph of the introduction) really exist?\n\n3. I noticed that the authors mention on the first page, lines 47-48, that \"this strategy is inherently inflexible as it struggles to capture the continuous variation of degradation intensity within the same class.\" However, this paper also doesn't provide a complete comparison of different levels under the same degradation. Taking Gaussian denoising as an example, can the method proposed in this paper achieve significant performance improvements under different variances?\n\n4. In 3D all-in-one restoration experiments, the proposed method did not show a significant performance improvement. For example, compared to DCPT [2], the average improvement is only 0.1 dB. However, on the 5D all-in-one restoration, the average improvement is 0.6 dB. I wonder if the proposed method has better capabilities in task scaling.\n\n5. Experiments on mixed degradation [3] are lacking.\n\n[1] Universal Image Restoration Pre-training via Masked Degradation Classification.\n\n[2] Universal Image Restoration Pre-training via Degradation Classification. ICLR 2025.\n\n[3] OneRestore: A Universal Restoration Framework for Composite Degradation. ECCV 2024."}, "questions": {"value": "In stage 1, ground truth (GT) is directly used to generate degradation-related representations, and in stage 2, a network approximates these representations. However, I am curious why the fusion of clean and degraded images results in a representation called \"Degradation\". Since clean images do not show degradation, does combining them affect the process of extracting degradation information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AXBSmkZYgz", "forum": "zHBCOMxuoJ", "replyto": "zHBCOMxuoJ", "signatures": ["ICLR.cc/2026/Conference/Submission8416/Reviewer_r96R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8416/Reviewer_r96R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887830282, "cdate": 1761887830282, "tmdate": 1762920314677, "mdate": 1762920314677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LUPI-based prior learning for all-in-one image restoration. A teacher network with privileged access to clean images learns a restoration-aware prior; a student network (degraded input only) is trained to mimic this prior. The prior dynamically modulates a shared restoration backbone. Claims SOTA on unified benchmarks with semantically structured prior space. Code to be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Elegant use of LUPI to resolve objective mismatch in prior learning.\nPrior space visualizations show semantic clustering (e.g., blur vs. noise intensity).\nStrong empirical gains on all-in-one benchmarks.\nClean formulation and ablation on prior distillation strategies."}, "weaknesses": {"value": "LUPI is not new — heavily borrowed from Vapnik et al. (2009); framing as \"paradigm shift\" is overstated.\nTeacher has oracle access to clean image — unrealistic in real deployment; no discussion of practical deployment gap.\nNo comparison to simple baselines like AdaIR + MLE loss or MoCE-IR with stronger regularization.\nPrior distillation bottleneck: student performance capped by teacher quality — no analysis of teacher-student gap.\nOverclaims on intensity handling — no fine-grained intensity regression task (e.g., predict σ for Gaussian noise).\nNo runtime or parameter efficiency — two-stage training likely increases cost."}, "questions": {"value": "How does performance degrade if teacher uses noisy clean labels (e.g., +1% Gaussian)?\nCompare against AdaIR with end-to-end intensity regression head — is LUPI strictly necessary?\nReport teacher-student prior cosine similarity per degradation type — where does distillation fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ne8faT4wMw", "forum": "zHBCOMxuoJ", "replyto": "zHBCOMxuoJ", "signatures": ["ICLR.cc/2026/Conference/Submission8416/Reviewer_HCUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8416/Reviewer_HCUy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905715254, "cdate": 1761905715254, "tmdate": 1762920314304, "mdate": 1762920314304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage, teacher-student framework for all-in-one image restoration, grounded in the Learning Using Privileged Information (LUPI) paradigm. The central idea is to train a \"teacher\" network that has privileged access to both the degraded input ($I_d$) and the clean target ($I_c$) to learn an optimal, \"restoration-aware\" degradation prior . A \"student\" network, which only sees $I_d$ at inference, is then trained to approximate this ideal prior via a distribution alignment loss . This learned prior is used to modulate a U-Net-based restoration backbone . The method demonstrates state-of-the-art (SOTA) quantitative and qualitative results on standard 3-task and 5-task synthetic benchmarks ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work is empirically strong, presenting a well-executed framework that achieves SOTA results across all evaluated benchmarks, and the visualization of the learned prior space in Figure 5 is insightful."}, "weaknesses": {"value": "While the results are strong, I have several concerns regarding the work's overall contribution and practicality.\n\nFirst, the core conceptual novelty is somewhat overstated. The paper frames the work as solving an \"objective mismatch\" inherent in previous methods. However, the motivation of learning a degradation-discriminative prior to guide a unified restorer is not new and is shared by many existing works (e.g., MioIR, DCPT). These methods also learn degradation-specific representations, albeit through different proxy tasks like classification  or contrastive learning. The paper does not sufficiently ablate or argue why its LUPI-based distillation is fundamentally superior to, for instance, a more direct multi-task learning approach where a shared encoder is jointly optimized for restoration and an explicit degradation-identification task. The claim of a fundamental \"paradigm\" shift feels incremental, as the goal remains the same, even if the mechanism (distillation from an oracle vs. learning from a proxy) is different.\n\nSecond, the method introduces a significant and unaddressed training cost. The two-stage paradigm requires training a full teacher system ($D_T$, $R_T$) for 150 epochs, then freezing $D_T$ and training a full student system ($D_S$, $R_S$) for another 150 epochs. This 300-epoch process effectively doubles the training budget compared to end-to-end models, which are already costly. While inference efficiency is discussed in the appendix , this substantial increase in training overhead is a major practical limitation that is not discussed or justified.\n\nThird, and most importantly, the paper's validation is limited entirely to synthetic, seen degradations. The core assumption is that the student $D_S$ can learn the mapping from $I_d$ to the teacher's ideal prior $d_T$. However, this mapping is only learned for degradations where $D_T$ knows the ideal prior (i.e., the 5 training tasks). The paper provides no evidence that the model can generalize to unseen or real-world degradations. For a novel, out-of-distribution degradation (e.g., real-world sensor noise, complex motion blur, or JPEG artifacts), the teacher $D_T$ itself was never trained to find an \"ideal\" prior, so the student's output $d_S$ would likely be arbitrary. The claim of a \"structured\" prior space is only validated within the training distribution. The lack of any cross-domain validation on real-world benchmarks (e.g., DRealSR, Real-worldRain) makes the method's practical utility highly questionable."}, "questions": {"value": "How does the model perform, even in a qualitative sense, on a completely unseen type of synthetic degradation (e.g., JPEG artifacts or Gaussian blur with a large, anisotropic kernel) that was not part of the 5-task training set? This would be a more robust test of whether $D_S$ has learned a general-purpose degradation encoder or has simply memorized the 5 training distributions.\n\n Could the authors provide a comparison of the total wall-clock training time (or GPU-hours) for the proposed two-stage, 300-epoch method versus a baseline like AdaIR (trained to convergence)? This is essential for evaluating if the reported PSNR gain (e.g., ~0.29 dB over AdaIR on the 3-task benchmark ) justifies the (presumed) 2x training cost.\n\n The $\\mathcal{L}_{align}$ loss   forces the student $d_S$ to match the teacher $d_T$. However, $d_T$ is derived from both $I_d$ and $I_c$. Isn't it possible that $d_T$ contains \"privileged\" information about the content of $I_c$ that is impossible for $D_S$ to predict from $I_d$ alone? Could this create a difficult or impossible learning target for the student, and did the authors observe any instability during this alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x0Ga9tZ4pO", "forum": "zHBCOMxuoJ", "replyto": "zHBCOMxuoJ", "signatures": ["ICLR.cc/2026/Conference/Submission8416/Reviewer_bsYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8416/Reviewer_bsYg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008525702, "cdate": 1762008525702, "tmdate": 1762920313907, "mdate": 1762920313907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}