{"id": "WhVkbzclCp", "number": 8989, "cdate": 1758106111698, "mdate": 1763003419768, "content": {"title": "MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation", "abstract": "Advanced autoregressive models have reshaped multimodal AI. However, their transformative potential in medical imaging remains largely untapped due to the absence of a unified visual tokenizer---one capable of capturing fine‑grained visual structures for faithful image reconstruction and realistic image synthesis, as well as rich semantics for accurate diagnosis and image interpretation. \nTo this end, we present MedITok, the first unified tokenizer tailored for medical images, encoding both low-level structural details and high-level clinical semantics within a unified latent space. \nTo balance these competing objectives, we introduce a novel two‑stage training framework: a visual representation alignment stage that cold-starts the tokenizer reconstruction learning with a visual semantic constraint, followed by a textual semantic representation alignment stage that infuses detailed clinical semantics into the latent space. \nTrained on the meticulously collected large-scale dataset with over 30 million medical images and 2 million image-caption pairs, MedITok achieves state-of-the-art performance on more than 30 datasets across 9 imaging modalities and 4 different tasks. By providing a unified token space for autoregressive modeling, MedITok supports a wide range of tasks in clinical diagnostics and generative healthcare applications. \nModel and code are available in the supplementary material.", "tldr": "A novel training framework to produce MedITok, the first unified visual tokenizer for medical images, effectively encoding visual details and clinical semantics, achieving SOTA performance across diverse medical modalities and tasks.", "keywords": ["visual tokenizer", "foundation model", "general medical"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5b237944b530bfb99869efd356ba3d1307a79d2f.pdf", "supplementary_material": "/attachment/5bb5b787aeca3fb8890c9d50d1a83e84f1601c95.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenge of unified discrete representation for medical imaging: preserving low-level structural fidelity for image synthesis while encoding high-level clinical semantics for visual understanding. The authors propose MedITok, a two-stage training framework where Stage 1 focuses on reconstruction with light semantic alignment via a pretrained vision encoder, and Stage 2 aligns visual tokens with textual semantics using paired image-text data to form a unified token space. The method is validated across 30+ datasets, 9 modalities, and 4 tasks, showing consistent superiority over multiple strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **The problem is important and well-motivated.** Building a unified tokenizer that jointly supports understanding and generation is central to current multimodal medical AI research. MedITok effectively achieves structure–semantics consistency through its two-stage training, offering a potential foundation for unified multimodal modeling.\n\n2. **The data pipeline itself is a valuable contribution.** The authors collect over 30 million medical images and 2 million image-text pairs from 300+ sources, applying automated filtering rules. If released, this dataset would substantially benefit future work on medical tokenizers and representation learning.\n\n3. Experiments are extensive and cover multiple modalities and tasks, including reconstruction, classification, synthesis, and VQA, demonstrating the generality of MedITok.\n\n4. **Ablation studies are thorough and address key hypotheses.** Table 5 systematically compares the impact of different alignment objectives, data scales, and the two-stage design, confirming that both large-scale data and stage-wise training contribute significantly to improved structure–semantic consistency."}, "weaknesses": {"value": "1. **Visual understanding benchmarks are limited.** Although MedITok achieves strong gains on VQA-RAD and SLAKE, these benchmarks are relatively small and dominated by closed-form, single-entity questions. They may not fully capture the tokenizer’s ability to extract and represent complex semantic information from diverse medical images and modalities.\n\n2. **Semantic consistency verification remains indirect.** While the paper claims to construct a shared semantic–structural token space, current evidence relies mainly on downstream task performance. More direct evaluations of semantic alignment between image tokens and semantics are missing.\n\n3. **Validation of unified understanding–generation capability is incomplete.** MedITok is evaluated separately on understanding and generation tasks, without a unified multimodal framework to assess potential synergy. If the unified tokenizer hypothesis holds, one would expect it to narrow the gap between these tasks or even yield mutual enhancement effects through shared representations."}, "questions": {"value": "1. The evaluation of visual understanding is mainly based on VQA-RAD and SLAKE. Do the authors plan to test MedITok on more challenging and diverse benchmarks such as MMMU-Med, PMC-VQA, or OmniMedVQA to better assess its semantic generalization?\n\n2. Since semantic consistency is only indirectly evidenced through downstream results, could the authors add direct validation, for example using token–caption retrieval or cross-modal similarity metrics, to quantify the alignment between visual tokens and semantics?\n\n3. Given that MedITok aims to unify understanding and generation tasks, do the authors plan to evaluate it within a shared multimodal framework (e.g., a Multimodal Large Language Model) to demonstrate its potential in reducing the task gap and promoting semantic–structural synergy? Such evidence would further substantiate the feasibility of joint multimodal optimization.\n\nIf the authors can address these questions and provide additional empirical or analytical evidence, I would consider raising my overall score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C36DiRioUt", "forum": "WhVkbzclCp", "replyto": "WhVkbzclCp", "signatures": ["ICLR.cc/2026/Conference/Submission8989/Reviewer_tTgo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8989/Reviewer_tTgo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580576031, "cdate": 1761580576031, "tmdate": 1762920718753, "mdate": 1762920718753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the chairs and reviewers for their time and feedback on our submission. \n\n1. Regarding the novelty of our work, we wish to highlight two key aspects:\n  - **Technical Contribution:** Different from previous work that directly combines reconstructive objectives and textual semantic learning objectives, we introduced an extra middle stage: visual representation alignment, which serves as a cold-start within the training framework of a unified image tokenizer. This is a new approach to mitigate the potential conflicts of learning objectives.\n  - **Application Contribution:** This two-stage framework is designed to **effectively scale with the vast amount of unlabeled images** that are abundant in the medical domain, a practical challenge our method is uniquely adapted to address.\n\n2. Regarding the **motivation**, we want to clarify that our objective **differs drastically** from pure CLIP-based methods. While those models excel at encoding high-level semantic representations for interpretation, our work aims to build a **unified medical image tokenizer**, which requires encoding *both* high-level clinical semantics *and* the low-level structural details necessary for medical images.\n\nWe appreciate the helpful comments aimed at strengthening our work, and plan to further revise our paper. Therefore, we are withdrawing the current submission."}}, "id": "xDbzjwyHXN", "forum": "WhVkbzclCp", "replyto": "WhVkbzclCp", "signatures": ["ICLR.cc/2026/Conference/Submission8989/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8989/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763003418972, "cdate": 1763003418972, "tmdate": 1763003418972, "mdate": 1763003418972, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MedITok, a unified medical image tokenizer trained through a two-stage framework that first aligns visual representations on 33 million unpaired medical images and then integrates textual semantics from 2 million image-caption pairs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is generally well written and organized, allowing readers to follow the technical ideas and experimental setup without difficulty.\n\n2.The proposed two-stage training framework is conceptually reasonable and shows consistent, though moderate, improvements over existing tokenizers across several medical imaging tasks."}, "weaknesses": {"value": "1. The paper’s technical novelty is somewhat limited, as it primarily extends existing VQ-based tokenization frameworks with incremental modifications rather than introducing novel mechanisms (such as FSQ in ICLR 2023).\n\n2. The reliance on large-scale proprietary or composite datasets raises concerns about reproducibility and accessibility for other researchers.\n\n3. The experimental section, while comprehensive, mainly evaluates common reconstruction and classification metrics without sufficient clinical validation or expert assessment of diagnostic utility.\n\n4. The ablation and comparison analyses do not clearly isolate the individual contributions of each training stage, making it difficult to attribute improvements to specific design choices."}, "questions": {"value": "1.Could the authors clarify what key innovations distinguish MedITok from prior VQ-based or unified tokenization methods, beyond the two-stage training setup?\n\n2.How do the authors plan to improve reproducibility and accessibility given that much of the training data comes from large-scale aggregated or partially restricted sources?\n\n3.Can the authors provide any clinician-involved evaluations or case studies to support the claim that MedITok improves real-world diagnostic reliability rather than just quantitative metrics?\n\n4.Would the authors expand their ablation studies to better isolate the effect of each training stage or module, so readers can more clearly see which components drive performance gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "As mentioned in the paper\" This work uses only publicly\navailable datasets with clear licensing; no new human or animal subjects were recruited and no\nprotected health information beyond what is already de-identified in the source data was used."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RcmcFB0voo", "forum": "WhVkbzclCp", "replyto": "WhVkbzclCp", "signatures": ["ICLR.cc/2026/Conference/Submission8989/Reviewer_oHgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8989/Reviewer_oHgR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907634020, "cdate": 1761907634020, "tmdate": 1762920718432, "mdate": 1762920718432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MedITok, a unified visual tokenizer for medical images that aims to encode both low-level structural details and high-level clinical semantics. The authors introduce a two-stage training framework: (1) visual representation alignment using unpaired images with light semantic constraints from a pretrained vision encoder, and (2) textual semantic alignment using image-caption pairs. The model is trained on 33M medical images and 2.4M image-text pairs, and evaluated on over 30 datasets across 9 imaging modalities for reconstruction, classification, synthesis, and visual question answering tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive evaluation: The paper provides extensive experiments across multiple tasks (reconstruction, classification, synthesis, VQA) and imaging modalities, demonstrating broad applicability.\n\nLarge-scale data curation: The authors make a significant effort in collecting and curating 33M+ medical images from 300+ public sources with quality control procedures.\n\nPractical utility: The unified tokenizer could potentially serve as a foundation for various downstream medical AI applications, which has practical value for the community."}, "weaknesses": {"value": "### 1. Limited Novelty and Insufficient Differentiation from Prior Work\n\nThe core contribution—aligning visual representations with pretrained encoders through contrastive learning—is not novel. Several recent works have explored similar ideas:\n\n- **Contrastive learning of medical visual representations from paired images and text** (MLHC 2022) proposes contrastive learning between medical images and radiology reports, achieving unified multimodal representations with superior data efficiency (requiring only 10% labeled data compared to ImageNet pretraining).\n\n- **Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition** (ICCV 2021) extends ConVIRT by proposing global-local representation learning that contrasts image sub-regions with words in paired reports, demonstrating label-efficient medical image recognition.\n\n- **MedCLIP: Contrastive Learning from Unpaired Medical Images and Text** ( EMNLP 2022) shows how to effectively decouple images and texts for multimodal contrastive learning, scaling training data in a combinatorial magnitude while eliminating false negatives.\n\n- **Enhancing representation in radiography- reports foundation model: a granular alignment algorithm using masked contrastive learning (Nature Communications, 2024) has similar motivation and method\n\n- **A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics (Nature biomedical engineering, 2023)** has similar motivation and method\n\n- **Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports (Nature Machine Intelligence, 2022)** has similar motivation and method\n\nThe claimed novelty of the \"two-stage training framework\" is questionable:\n\n1. The first stage essentially performs standard VQ-VAE training with a lightweight semantic regularization (λ_vision = 0.1), which is minimal.\n\n2. The second stage is standard contrastive alignment between discrete tokens and text embeddings, similar to the approaches in ConVIRT and GLoRIA, but applied to quantized representations rather than continuous features.\n\n3. The authors claim that \"naïve joint optimization causes mutual interference\" (line 60), but Table S1 shows that combined training (row i vs. row ii) only has marginal differences (PSNR: 29.20 vs 30.03, mAP: 81.10 vs 80.09). This does not strongly support the necessity of two-stage training.\n\n**Missing critical comparisons**: The paper does not provide quantitative comparisons with these important medical vision-language models:\n- **ConVIRT**,  **Enhancing representation in radiography- reports foundation model: a granular alignment algorithm using masked contrastive learning (Nature Communications, 2024)**,  **A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics (Nature biomedical engineering, 2023)**, **Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports (Nature Machine Intelligence, 2022)** use bidirectional contrastive objectives between images and text\n- **GLoRIA** uses attention-based global-local contrastive learning  \n- **MedCLIP** addresses false negatives through semantic matching loss\n\n\nWithout comparing to these methods, it's unclear whether MedITok's approach (discrete tokenization + two-stage training) provides meaningful advantages over existing continuous-space contrastive learning methods (ConVIRT, GLoRIA) or alternative discrete approaches.\n\nThe paper would be strengthened by:\n1. Direct comparison with among methods\n2. Ablation showing discrete tokens significantly outperform continuous representations when both use similar semantic alignment\n3. Analysis of what unique advantages the two-stage discrete approach provides beyond existing medical image-text alignment methods\n\n### 2. Questionable Design Choice: Discrete vs. Continuous Tokenization\n\nThe paper strongly advocates for discrete tokenization but does not provide sufficient justification:\n\n1. **Reconstruction quality trade-off**: Table 1 shows that MedVAE (continuous) achieves comparable or better PSNR on some modalities (e.g., CT: 36.46 vs 36.32, X-ray: 36.23 vs 34.42). The discrete codebook necessarily loses information through quantization.\n\n2. **Semantic encoding**: The paper claims discrete tokens are necessary for AR modeling, but recent work shows continuous representations can be effectively used:\n   - **Latent Diffusion Models** (Rombach et al., CVPR 2022) work directly with continuous VAE latents for high-quality image generation\n   - Continuous representations from ConVIRT and GLoRIA have been successfully used for classification, retrieval, and generation tasks\n   \n3. **3D medical imaging**: Table S2 shows that slice-based processing of 3D volumes is suboptimal. Continuous representations might be more natural for 3D medical imaging where spatial continuity is crucial.\n\n4. **Codebook collapse**: Figure S6 shows clustering in Stage 1, suggesting the discrete codebook is not efficiently utilizing its capacity. Why not use a larger continuous latent space?\n\nThe authors argue discrete tokens enable \"unified latent space for visual synthesis and interpretation\" (Appendix E.1), but this is not convincingly demonstrated. VQA and classification experiments in Tables 2 and 4 could work equally well with continuous features from ConVIRT or GLoRIA, which have proven effective for medical image understanding.\n\n### 3. Experimental Design Issues\n\n**Baseline selection concerns**:\n- Several baselines (VQGAN, Emu3-VQ, VAR-VQ) are trained on natural images, not medical images. This is an unfair comparison. Why not train these methods on medical data, or compare with medical-specific methods like ConVIRT and GLoRIA?\n- TokenFlow and UniTok are recent (2024-2025) and may not have stabilized architectures. More established medical imaging baselines would be valuable.\n- The paper compares against MedVAE and PUMIT but not against the widely-used medical vision-language models (ConVIRT, GLoRIA, MedCLIP).\n\n**Evaluation metrics**:\n- For reconstruction, the paper uses rFID with ImageNet-pretrained features (line 274). This is problematic: (1) ImageNet features may not capture medically-relevant qualities, (2) FID is known to be unreliable for small distribution shifts.\n- For synthesis (Table 3), only FID and diversity are reported. What about medical fidelity? Do generated images contain anatomically plausible structures? The visual Turing test (Table S3) is limited to 75 X-rays and one radiologist.\n\n**Statistical significance**: \n- No error bars or confidence intervals are provided in main results (Tables 1-2)\n- Table 3 shows standard deviations but they overlap significantly (e.g., MedITok: 76.78±1.91 vs UniTok: 80.71±3.18)\n\n### 4. Methodological Concerns\n\n**Data leakage risks**: \n- The authors state \"We tried our best to avoid any overlap\" (line 236), but provide no concrete evidence. With 300+ source datasets and complex preprocessing, rigorous deduplication is critical.\n- The reconstruction evaluation includes datasets that might overlap with training data sources (e.g., both use public CT scans from TCIA).\n\n**Hyperparameter choices lack justification**:\n- Why λ_vision = 0.1 and λ_text = 1.0? Only one ablation value is tested (Table S1, row vi).\n- Why 8 codebooks with 4,096 entries each? What about 4 codebooks with 8,192 entries?\n- Training for only 3+2 epochs seems limited. Have the models converged?\n\n**Caption quality**: \n- The paper filters BIOMEDICA from 24M to 1.2M pairs based on tags (Appendix A.2.2), but doesn't analyze caption quality. Medical captions from publications may contain references, figure labels, or be overly technical.\n- No examples of actual captions used for training are provided.\n\n### 5. Limited Analysis of Learned Representations\n\n- **What exactly is being learned?** The paper shows t-SNE visualizations (Fig S6) but provides little insight into what semantic structure the tokens capture.\n- **Comparison with existing methods**: How do the learned representations compare with those from ConVIRT or GLoRIA? Do discrete tokens capture different or complementary information?\n- **Interpretability**: Can we identify which tokens correspond to specific anatomical structures or pathologies?\n- **Failure analysis**: Only 2 histopathology examples are shown (Fig S7). What are the systematic failure modes?\n\n### 6. Writing and Presentation Issues\n\n1. **Overclaimed contributions**: \n   - \"First unified tokenizer tailored for medical images\" (Abstract) - Other methods already provide unified multimodal representations for both understanding and generation tasks\n   - \"State-of-the-art performance\" - This is only true for some metrics on some datasets, and comparisons with other methods are missing\n\n2. **Inconsistent terminology**: \n   - \"Cold-start\" is used informally without clear definition\n   - \"Unified token space\" is not precisely defined relative to existing work\n\n3. **Missing related work**:\n   - Insufficient discussion of medical vision-language models and their relation to this work\n   - Limited discussion of why discrete tokenization is superior to continuous representations used in these works"}, "questions": {"value": "Comparison with vision-language models: How does MedITok compare with methods like GLoRIA, BiomedCLIP finetuning, or REFERS that also align medical images and text? Please provide quantitative comparisons.\n\nNecessity of discrete tokenization: Can you provide experiments showing that discrete tokens significantly outperform continuous representations (e.g., VQ-VAE latents vs. standard VAE latents) when both are trained with the same semantic alignment objectives?\n\nTwo-stage training necessity: Table S1 (rows i-ii) shows minimal difference between combined and two-stage training. Can you provide more convincing evidence that the two-stage approach is essential? What happens with different λ_vision values?\n\nGeneralization: How does MedITok perform on imaging modalities not seen during training (e.g., PET, angiography)? The microscopy results in Fig 4 are encouraging but not quantified.\n\nComputational cost: What is the computational cost compared to continuous tokenizers or direct fine-tuning of vision-language models? Table S4 shows some metrics but not training cost.\n\nClinical validation: Have any clinical experts evaluated the quality of generated or reconstructed images for diagnostic utility?\nData contamination: Can you provide explicit analysis showing no overlap between training and evaluation datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8VysDH4Jo", "forum": "WhVkbzclCp", "replyto": "WhVkbzclCp", "signatures": ["ICLR.cc/2026/Conference/Submission8989/Reviewer_zg58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8989/Reviewer_zg58"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993126298, "cdate": 1761993126298, "tmdate": 1762920717953, "mdate": 1762920717953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedITok, a unified visual tokenizer for medical images designed to encode both low-level structural details (supporting image reconstruction and synthesis) and high-level clinical semantics (enabling image interpretation and classification). To balance these competing objectives, the authors propose a novel two-stage training framework:\n\nVisual Representation Alignment Stage: A cold-start using a large corpus of unpaired medical images, focusing on reconstruction fidelity with a light semantic constraint.\n\nTextual Semantic Alignment Stage: Fine-tuning on high-quality image-caption pairs to infuse detailed clinical semantics into the latent space by aligning visual tokens with textual embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-Designed Two-Stage Training: The staged approach effectively mitigates conflicts between multiple objectives and efficiently leverages the abundance of unlabeled images common in the medical domain.\n\nRigorous Data Curation: Implements a multi-faceted filtering pipeline (dynamic range, resolution, information content, etc.) for quality control and provides detailed data sources, enhancing reproducibility."}, "weaknesses": {"value": "Limited Novelty in Core Concepts: While novel for the medical domain, the core ideas (two-stage training, visual-text alignment via contrastive loss) are adaptations of existing methods from the general vision domain (e.g., VILA-U, UniTok). The primary contribution lies in the tailored application and scaling to medical data, rather than proposing fundamentally new mechanisms.\n\nInsufficient Justification of Medical Image Synthesis Utility: While the paper demonstrates strong quantitative results in the image synthesis task, it provides limited discussion on the concrete clinical value and practical applications of generating medical images. The significance of this capability in real-world medical scenarios remains underexplored."}, "questions": {"value": "See weakness. \nI will raise my score if concerns are well resolved after rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bAGgoMxE2C", "forum": "WhVkbzclCp", "replyto": "WhVkbzclCp", "signatures": ["ICLR.cc/2026/Conference/Submission8989/Reviewer_T3Vu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8989/Reviewer_T3Vu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088532268, "cdate": 1762088532268, "tmdate": 1762920717373, "mdate": 1762920717373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}