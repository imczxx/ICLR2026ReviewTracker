{"id": "eZ5jtFuk3e", "number": 14454, "cdate": 1758235958341, "mdate": 1759897368999, "content": {"title": "Learned Meta-Tokens for Language Modeling", "abstract": "Transformer-based language models (LMs) notably struggle to reliably capture distant contextual information. This work introduces a novel approach using meta-tokens -- special tokens injected during pre-training -- paired with a dedicated meta-attention mechanism to guide LMs to use these tokens. We pre-train a language model equipped with meta-attention in addition to causal multi-head attention on <100B tokens, achieving strong performance on a suite of synthetic tasks. Our method facilitates length generalization up to 2$\\times$ the context window after extension with YaRN. We provide an information-theoretic analysis which reveals that meta-tokens \\textit{sharpen} the positional encoding, allowing them to operate as content-based anchors that compress preceding context and “cache” it within the meta-token. We empirically confirm this by visualizing model internals to study the residual stream. Together, our findings demonstrate that meta-tokens and meta-attention provide a simple, data-efficient pre-training method, grounded by new mechanistic insights into their role in enabling length generalization behavior.", "tldr": "We pre-train language models with inserted meta-tokens, demonstrating strong performance and length generalization on synthetic tasks for long-context modeling. We explain these results based on positional encoding and implicit context compression.", "keywords": ["meta-tokens", "language models", "pre-training", "positional encoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4efd160a50d7c69747e3270df0eed9114b4e00d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method to improve the long-context reasoning and length generalization capabilities of Transformers. The authors propose two key components: meta-tokens and a meta-attention mechanism.\nMeta-tokens are special, learned tokens that are injected randomly into the input sequence during pre-training (on <100B tokens) and then placed deliberately at key points during fine-tuning.\nMeta-attention is a dedicated, sparse attention layer added to the Transformer architecture. This layer's mask ensures that meta-tokens can only attend to other meta-tokens, creating a separate information pathway.\n\nThe central idea is that these meta-tokens learn to function as \"content-based anchors\" that compress and \"cache\" information from the preceding context. The meta-attention layer allows the model to build and access a \"summary stream\" of this cached information.\n\nThe authors train a 152M parameter model and show it outperforms baselines (including a GPT-Neo model trained on 3x more data) on synthetic tasks. The model also demonstrates strong length generalization, performing well on sequences up to 2x its training context length, even after extension with YaRN.\n\nThe paper provides a good theoretical and empirical analysis, arguing that meta-tokens work by \"sharpening\" the attention mechanism (reducing its entropy) and serving as a more efficient (in a rate-distortion sense) method of context compression. A key finding is that model performance improves when the positional encoding for these meta-tokens is zeroed out, suggesting the model learns to rely on their compressed content for localization, not their absolute position."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- I really like the idea of exploiting something like attention sink to actually build a coordinate system for the model using the meta tokens. And the paper doesn't just present empirical results, it also provides a solid mechanistic explanation for why it works. The information-theoretic analysis (theorem 5.1 on entropy reduction, theorem 5.2 on rate-distortion) provides a formal basis for the \"sharpening\" and \"caching\" hypotheses.\n- The idea of a dual-pathway system (standard attention for content, meta-attention for a \"summary stream\") is very interesting. It's not just another modification to positional encodings but a new architectural component with a clear, explainable purpose.\n- The fact that zeroing out the positional encoding of meta-tokens boosts performance is a core result. I think it supports the claim that these tokens are not just positional markers but have become true content-based anchors."}, "weaknesses": {"value": "- The main limitation is that the model is only evaluated on synthetic tasks designed to probe recall (list recall, copying, parity, etc). While this is excellent for analyzing the specific mechanism of memory, it doesn't really provides evidences that these gains will translate to real-world, general-purpose language tasks. Plus, the experiments are performed on a 150M parameters model. So, it's unknown if this meta-attention mechanism will scale effectively or provide the same benefits in billion-parameters models.\n- The paper doesn't fully explains why the meta-tokens are injected uniformly at random during pre-training but are placed deliberately (as _PAUSE_ tokens) at task-specific locations during fine-tuning and it still works. \n- The paper claims \"little overhead\", but it does introduce an entire new attention layer (meta-attention) and increases the sequence length by 10% ($k=0.1$) during pre-training."}, "questions": {"value": "- Your work shows that meta-tokens, trained with a dedicated meta-attention layer, function as effective content-based anchors for caching context. This quite similar to the attention sink phenomenon, where a single, fixed token (like the BOS token) implicitly learns to aggregate information from the entire context via the standard attention mechanism.\nWhat do you think are the main differences? Do you believe the explicit, sparse meta-attention layer is the critical component that distinguishes your method's success and enables robust length generalization, or could a similar caching behavior be explicitly trained into standard tokens (or a single 'sink' token) without requiring a separate attention pathway?\n\n- Plus, not a question but this paper actually gets to very similar conclusions about how having a high attention on specific tokens can create a sort of reference frame that is necessary for proper processing. https://arxiv.org/abs/2508.02546\nThis paper is more specifically about positional encodings and attention sink, but I think it's strongly related and it could strengthen your claims, especially because they show that the sink phenomenon is related to the positional encoding scheme (and of course, the softmax pressure) and in particular, RoPE rotations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ihbwBVsy3j", "forum": "eZ5jtFuk3e", "replyto": "eZ5jtFuk3e", "signatures": ["ICLR.cc/2026/Conference/Submission14454/Reviewer_2ANK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14454/Reviewer_2ANK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760976794425, "cdate": 1760976794425, "tmdate": 1762924857922, "mdate": 1762924857922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Contributions:  \n1. Novel meta-attention mechanism that uses meta-tokens to improve performance on synthetic recall tasks\n2. Theoretical analysis on context compression\n3. Empirical experiments showing length generalization in relation to positional encodings. Zeroing out the positional encodings on meta-tokens long context improves recall."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well motivated, long context modeling is a very pressing issue in LLM research.\n2. Strong theoretical grounding, Theorem 5.1 provides a clear explanation for the effectiveness meta-tokens.\n3. Many synthetic evaluations, tasks, ablations.\n4. Meta-attention is original and novel. The algorithm is fairly simple and can be used during pre-training."}, "weaknesses": {"value": "1. There are only synthetic evaluations, no evaluations on standard NLP evaluations or proxies to realistic long-context applications.\n2. Degradation of inference speed, no benchmarks for training speed.\n3. Meta-token placement strategy is not explored, no ablations on the actual placement strategy of meta-tokens.\n4. Inference-time deliberate placement of the meta-tokens requires prior knowledge of the task the LLM is trying to solve, hindering generalization.\n5. Limited scale, 150M parameters is very small by modern standards. Tested context length is also very short (512), which does not necessarily translate to longer contexes.\n6. No comparison against other popular long context attention algorithms."}, "questions": {"value": "- Does meta-tokens also improve the pretraining perplexity? Language modeling is also a very important metric that evaluates the average \"quality\" of the text generation, which is not always related to the recall accuracy.\n-  Have you tried fixed periodic placement strategies for the meta-tokens? \n- What do you think about adaptive placement strategies based on perplexity? Randomly placing during training currently doesn't match the inference time's deliberate per-task placement.\n- Why was YaRN necessary here? There can be counfounding factors which YaRN by itself improves length generalization on the model. Have you tried using alternatives like sliding window attention combined with meta-attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GZREHa953K", "forum": "eZ5jtFuk3e", "replyto": "eZ5jtFuk3e", "signatures": ["ICLR.cc/2026/Conference/Submission14454/Reviewer_tDK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14454/Reviewer_tDK3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947161980, "cdate": 1761947161980, "tmdate": 1762924857559, "mdate": 1762924857559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a novel method to improve long-range context modeling in Transformer-based language models through the introduction of meta-tokens and a corresponding meta-attention mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Provides information-theoretic and empirical analyses that explain how meta-tokens function as content-based anchors and sharpen the positional encoding."}, "weaknesses": {"value": "1. While the abstract claims that “meta-tokens and meta-attention provide a simple, data-efficient pre-training method,” the paper does not clearly demonstrate this aspect.\n\n2. The empirical validation is conducted solely on four self-designed synthetic benchmarks. Although these tasks are useful for controlled analysis, they may not sufficiently reflect the behavior of the proposed method on real-world or natural language datasets. This limits the generalizability of the findings.\n\n3. The paper does not appear to provide a theoretical or empirical analysis of the upper bound of long-range generalization enabled by the proposed method.\n\n4.  All experiments are confined to a 152M parameter model, which is insufficient to demonstrate that the proposed method can scale to larger, contemporary architectures. This lack of scalability analysis is a critical missing piece of experimental evidence. Furthermore, the paper fails to provide a detailed analysis of the computational overhead and inference latency introduced by the meta-attention mechanism."}, "questions": {"value": "1. All experiments are conducted on a 152M parameter model, leaving scalability to contemporary 1B+ models unevaluated. Furthermore, the analysis of computational overhead in Appendix F.7 is minimal. Could the authors provide a more detailed analysis of the added latency (ms) and TFLOPs, and critically, provide any evidence (empirical or theoretical) that the method's benefits will scale to much larger models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ml6CYZgkr8", "forum": "eZ5jtFuk3e", "replyto": "eZ5jtFuk3e", "signatures": ["ICLR.cc/2026/Conference/Submission14454/Reviewer_8ygB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14454/Reviewer_8ygB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989742702, "cdate": 1761989742702, "tmdate": 1762924857219, "mdate": 1762924857219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to insert \"meta tokens\" during pretraining and apply an additional \"meta attention\" layer to sharpen the positional encoding, allowing for better long context capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is novel.\n- The method is well-motivated\n- Very good presentation. \n- It went further by presenting evidence that meta-tokens can compress context in their representation and formalizing into theories."}, "weaknesses": {"value": "- The meta token method, though theoretically simply, requires a change in the architecture which is sometimes hard to adopt.\n- When performing length extension at post-training, one small nitpick is that I wonder if it is suitable to use YaRN as a baseline to compare with. YaRN is a technique that applies to a pretrained model without any additional effort in pretraining, whereas the current paper in a sense already put int a lot of additional preparation in pretraining dataset and architecture changes. But it is relatively small because I don't have a good alternative."}, "questions": {"value": "- have you compare with a strong baseline that already pretrained for longer context? Although not related to claiming SoTA, this is useful to see how much regret there still is in the post-training context extension."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Q1hCXQUqE", "forum": "eZ5jtFuk3e", "replyto": "eZ5jtFuk3e", "signatures": ["ICLR.cc/2026/Conference/Submission14454/Reviewer_4UF2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14454/Reviewer_4UF2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991553787, "cdate": 1761991553787, "tmdate": 1762924856782, "mdate": 1762924856782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes meta-tokens and a per-layer lightweight meta-attention step that enable decoder-only LMs to cache compressed summaries in-sequence, improving long-range recall and length generalization with minimal architectural changes and modest compute."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method appends a meta-attention layer after each standard self-attention block, creating a lightweight in-sequence caching pathway that is minimally intrusive and easy to integrate.\n- The paper provides mutually consistent interpretability evidence—entropy visualizations and a rate–distortion perspective—that corroborates the empirical findings.\n- The approach is compatible with length-extension techniques such as YaRN, facilitating deployment within existing long-context pipelines."}, "weaknesses": {"value": "- The evaluation focuses primarily on synthetic recall tasks and small to mid-sized models, with limited validation on real long-document tasks and larger-scale settings.\n- There is no direct comparison against recent strong long-context baselines, such as Infini-attention."}, "questions": {"value": "- Because a meta-attention layer is added after every self-attention block, the parameter count rises from 124M to 152M; could the observed gains be primarily due to the increased parameter count rather than the proposed mechanism? If the parameter budget were held fixed, would the method still yield performance improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZjcSI1J4a0", "forum": "eZ5jtFuk3e", "replyto": "eZ5jtFuk3e", "signatures": ["ICLR.cc/2026/Conference/Submission14454/Reviewer_ViJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14454/Reviewer_ViJ9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001993594, "cdate": 1762001993594, "tmdate": 1762924855988, "mdate": 1762924855988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}