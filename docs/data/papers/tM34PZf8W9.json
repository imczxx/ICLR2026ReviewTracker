{"id": "tM34PZf8W9", "number": 19930, "cdate": 1758300690117, "mdate": 1763566357028, "content": {"title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces", "abstract": "Reinforcement learning (RL) struggles to scale to large, combinatorial action spaces common in many real-world problems. This paper introduces a novel framework for training discrete diffusion models as highly effective policies in these complex settings. Our key innovation is an efficient online training process that ensures stable and effective policy improvement. By leveraging policy mirror descent (PMD) to define an ideal, regularized target policy distribution, we frame the policy update as a distributional matching problem, training the expressive diffusion model to replicate this stable target. This decoupled approach stabilizes learning and significantly enhances training performance. Our method achieves state-of-the-art results and superior sample efficiency across a diverse set of challenging combinatorial benchmarks, including DNA sequence generation, RL with macro-actions, and multi-agent systems. Experiments demonstrate that our diffusion policies attain superior performance compared to other baselines.", "tldr": "We show how to train discrete diffusion models as policies for reinforcement learning in large combinatorial action spaces, achieving state-of-the-art results and better efficiency than autoregressive models.", "keywords": ["Reinforcement learning", "discrete diffusion"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f99e6133cc0b49049358922bf8674871b2655f48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel framework to address the challenge of scaling Reinforcement Learning (RL) to large, discrete, and combinatorial action spaces using discrete diffusion models as the policy representation. The method frames the policy update as a distributional matching problem where an expressive diffusion model is trained to replicate a target distribution derived from Policy Mirror Descent (PMD). While the combination of expressive generative models like diffusion models with RL is a highly relevant research direction, the current submission suffers from fundamental weaknesses concerning algorithmic novelty, sample efficiency, and experimental validation in truly large-scale or combinatorial settings. Specifically, the on-policy nature of the training presents a major sample efficiency bottleneck that is neither justified nor quantitatively compared against standard methods. Furthermore, the core discrete diffusion adaptation appears straightforward, and the experimental environments are not sufficiently demanding to validate the paper's central claims. I recommend rejecting this paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The application of discrete diffusion models to directly model policies in a discrete/combinatorial RL setting is a novel and timely area of exploration. This represents an attempt to leverage the strong expressive power of modern generative models for complex policy representations.\n\nQuality: The overall framework is logically constructed, and the connection between the policy update and Policy Mirror Descent (PMD) provides a clear theoretical grounding for the target distribution definition. The paper is generally well-written and easy to follow.\n\nClarity: The implementation details of the discrete diffusion process and the overall training loop, while basic, are presented clearly.\n\nSignificance: A successful integration of diffusion models that genuinely scales RL to vast combinatorial action spaces would be highly significant. However, the current submission does not convincingly demonstrate this scalability."}, "weaknesses": {"value": "Poor Sample Efficiency of On-Policy Training: The decision to employ an on-policy training scheme is a critical limitation. Training highly parameterized generative models like diffusion models is computationally intensive and typically requires many gradient steps. The paper fails to provide any quantitative comparison of the sample efficiency (e.g., performance vs. number of environment steps) against standard on-policy or, more importantly, off-policy RL baselines on common benchmarks. Given the high cost of data collection in real-world scenarios, this lack of efficiency evidence is a major roadblock to the method's practical adoption.\n\nLimited Algorithmic Novelty in Diffusion Adaptation: The discrete diffusion implementation appears to be a direct, simple translation of the standard Denoising Diffusion Probabilistic Model ($\\text{DDPM}$) for discrete data (e.g., a variant of Categorical $\\text{DDPM}$) to the RL domain. The paper lacks a substantive discussion on:\n\nThe specific technical challenges and adaptations required to make discrete diffusion stable and effective as a policy within an RL loop.\n\nWhy this diffusion formulation is superior in performance or expressiveness compared to other generative policy models (e.g., VAEs, normalizing flows) specifically for the RL task.\n\nUnsubstantiated Claims on Combinatorial Spaces and Missing Baselines: Despite the title, the algorithm seems restricted to fixed, discrete action spaces. Its generalizability to truly combinatorial action spaces (e.g., generating sets, sequences, or permutations of arbitrary length/size) is not demonstrated. Furthermore, the paper entirely omits necessary comparisons with methods used for similar tasks:\n\nThere is no discussion or comparison with techniques that handle large action spaces via differentiable approximations, such as the Gumbel-Softmax trick, especially in its continuous formulation, which could offer insights into policy smoothness.\n\nLack of Advanced Diffusion Techniques and Objective Optimization: The paper relies on the standard Evidence Lower Bound ($\\text{ELBO}$) for the diffusion objective. This choice is often suboptimal for computational efficiency and performance in modern diffusion models. The authors should explore and discuss more advanced alternatives:\n\nThe possibility of replacing the full $\\text{ELBO}$ with a simpler and more computationally efficient loss function, such as a $\\text{KL}$ divergence or a noise-matching loss (similar to the simplified objective in standard continuous $\\text{DDPMs}$).\n\nThe potential for integrating techniques based on Tweedie's theorem or score matching to accelerate the reverse sampling process, which is the core of policy inference and execution. This omission suggests a basic implementation that does not leverage recent advances in the diffusion modeling literature.\n\nInsufficient Experimental Scale: The experimental environments are not adequately challenging to support the paper's claims about handling large and complex action spaces. To truly validate the method's utility, experiments must be conducted on tasks with significantly higher dimensionality and action-space size. A strong validation would require testing the approach on larger-scale discrete environments, such as a high-dimensional, large grid-world variant of Frozen Lake (e.g., $64\\times64$ or higher, perhaps with dense observations), following the spirit of scaled-up discrete $\\text{DQN}$ research [1].\n\n[1] Stochastic Q-learning for Large Discrete Action Spaces. 2024 ICML"}, "questions": {"value": "Efficiency Justification: Can the authors provide a rigorous comparison of the sample efficiency (environment steps) between the proposed on-policy diffusion method and a well-tuned off-policy baseline (e.g., $\\text{DQN}$ or $\\text{SAC}$) on a common benchmark like the discrete control tasks? What is the core algorithmic reason for choosing an on-policy approach over a more data-efficient off-policy formulation, given the computational cost of training the diffusion model?\n\nCombinatorial vs. Discrete: Please provide a clear definition of the \"combinatorial action spaces\" tested. If the algorithm is applicable to truly combinatorial problems (e.g., generating a set of resources, ordering a sequence of deliveries), can the authors provide an experiment on a problem instance that is not simply a large, flat discrete space, and elaborate on how the diffusion chain structure is modified for that combinatorial output?\n\nObjective Function Selection: Did the authors experiment with or consider replacing the $\\text{ELBO}$ loss function with a simpler noise-matching objective (e.g., $\\text{KL}$ or $\\text{MSE}$) for the diffusion model training? If not, what is the theoretical or empirical justification for using the full $\\text{ELBO}$ instead of a simpler, more computationally tractable surrogate loss common in $\\text{DDPM}$ literature?\n\nSampling Acceleration: Have the authors investigated methods for accelerating the reverse sampling process, such as leveraging techniques based on Tweedie's theorem or incorporating techniques from fast sampler literature? Sampling efficiency is critical for policy deployment, and a discussion of this is highly relevant.\n\nScaling Experiments: To address the weakness of experimental scale, could the authors confirm if they have tested the approach on a significantly larger state/action space environment, such as a high-dimensional, large-scale grid-world task (e.g., a $64\\times64$ grid with dense state observations) where the benefits of a highly expressive policy would be more pronounced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tk3luIb7X5", "forum": "tM34PZf8W9", "replyto": "tM34PZf8W9", "signatures": ["ICLR.cc/2026/Conference/Submission19930/Reviewer_xatV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19930/Reviewer_xatV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761220503154, "cdate": 1761220503154, "tmdate": 1762932097229, "mdate": 1762932097229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of RL with large, combinatorial discrete action spaces. The authors propose a new framework, RL-D^2, which uses a discrete diffusion model as the policy parameterization. The authors reframe the policy improvement step as a distributional matching problem. They leverage policy mirror descent (PMD) to define a stable target policy distribution and derive two practical loss functions. The framework is evaluated across three distinct and challenging domains: DNA sequence generation, online RL with long-horizon macro-actions in Atari, and cooperative multi-agent RL (MARL) in Google Research Football. The results show that the diffusion-based policies achieve state-of-the-art performance, demonstrating superior scalability and sample efficiency compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors demonstrate the effectiveness of the proposed method across three fundamentally different domains.\n2. The empirical results are very strong. The method achieves state-of-the-art performance in all three domains.\n3. The proposed idea of decoupling the RL objective from the representation learning is novel."}, "weaknesses": {"value": "1. The core methodology (Section 4) is conceptually dense. A flowchart/figure illustrating the data flow of a single policy update would have vastly improved the paper's clarity and made the central contribution much easier to understand.\n2. Figure 1 left is of low quality.\n3. More explanations are needed on the baseline selection protocol."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k4DCTTL6uU", "forum": "tM34PZf8W9", "replyto": "tM34PZf8W9", "signatures": ["ICLR.cc/2026/Conference/Submission19930/Reviewer_7Dh3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19930/Reviewer_7Dh3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702662571, "cdate": 1761702662571, "tmdate": 1762932096490, "mdate": 1762932096490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces RL-D2, a novel online RL framework that employs discrete diffusion models as expressive policies for large combinatorial action spaces.\n\nBy framing policy improvement as distributional matching to a stable target derived from policy mirror descent (PMD), the method decouples RL optimization from representation learning, yielding robust training dynamics. \n\nThe authors propose forward and reverse KL objectives, with practical approximations for the intractable likelihood ratio in the reverse KL case, and introduce on-policy diffusion learning to align training with the policy’s generative process.\n\nExtensive experiments across DNA sequence optimization, macro-action RL in MinAtar/Atari, and multi-agent football demonstrate state-of-the-art performance and superior efficiency compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- RL-D2 creatively combines discrete diffusion models with online RL via PMD-guided distributional matching, introducing on-policy diffusion learning and practical KL approximations\n\n- Theoretical derivation  and thorough experiments deliver consistent SOTA results with strong efficiency gains\n\n- Well-structured, with clear preliminaries, intuitive FKL/RKL distinctions, and effective figures/tables"}, "weaknesses": {"value": "- PMD Normalization Error: Eq. (3) incorrectly writes the denominator as Z(s)−1 instead of Z(s)\n\n- FKL vs. RKL: A crucial missing experiment is an ablation study comparing FKL and RKL on the same benchmark."}, "questions": {"value": "The use of K-step macro-actions implies an open-loop execution policy. In temporally sensitive environments like Atari, a single non-optimal action in the generated sequence $a = (a_1, ..., a_K)$ could be amplified, as the resulting state $s_i$ may render the rest of the planned sequence $(a_{i+1}, ..., a_K)$ suboptimal or even catastrophic. A standard 1-step, closed-loop policy $\\pi(a_t|s_t)$ avoids this by re-evaluating the state at every step.\n\nCould the authors justify the choice of this open-loop macro-action framework, which seems highly susceptible to compounding errors, over the seemingly more robust 1-step closed-loop approach for these environments? How does the framework mitigate this significant risk?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ey83gKLS4t", "forum": "tM34PZf8W9", "replyto": "tM34PZf8W9", "signatures": ["ICLR.cc/2026/Conference/Submission19930/Reviewer_11iw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19930/Reviewer_11iw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900667073, "cdate": 1761900667073, "tmdate": 1762932095436, "mdate": 1762932095436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles RL in very large, combinatorial discrete action. It proposes RL-D2: parameterize the policy with a masked discrete diffusion model and perform policy improvement by matching the diffusion policy to the analytic Policy Mirror Descent (PMD) target distribution. The authors (i) define the PMD target $ \\pi_{MD} $ and frame improvement as minimizing a divergence between $ \\pi_\\theta $ and $ \\pi_{MD} $, deriving practical forward-KL (FKL) and reverse-KL (RKL) objectives, and (ii) introduce “on-policy diffusion learning” to align the diffusion training distribution with inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method provides a clear treatment of both FKL (“mean-seeking”) and RKL (“mode-seeking”) updates with diffusion-specific practicalities (ELBO-based bound for FKL; IS-ratio approximations for RKL), which is a thoughtful design space exploration. \n\nThe PMD foundation and the discrete diffusion setup are explained succinctly; the paper states contributions plainly and situates them against AR models and prior discrete diffusion work. \n\nIf validated, RL-D2’s combination of stability (via PMD), expressivity (via diffusion), and efficiency (non-autoregressive sampling; fewer diffusion steps) could make combinatorial RL far more practical in macro-action planning and MARL. Gains over baselines across 3 domains bolster impact."}, "weaknesses": {"value": "RKL inherits PMD’s guarantees, but the paper cannot compute exact likelihoods for diffusion policies and uses ELBO-based or single-step ratios. The bias factor $\\Gamma$ in the ELBO-ratio estimator is acknowledged but unanalyzed; conditions under which this bias is small (or controlled) are unclear. A finite-sample or asymptotic analysis, or at least diagnostics correlating η̂ with true ratios in toy settings, would strengthen the claim of “strong theoretical guarantees” in practice. \n\nFootball comparisons use an AR transformer baseline; it would be more convincing to include or justify against strong MARL baselines (e.g., MAPPO-style, value-decomposition methods) with joint-action coordination, not only sequence modeling. As is, the diffusion-vs-AR conclusion might conflate architectural class with training details. \n\nThe paper shows RL-D2 scales with macro length and notes DQN-Macro fails for long macros. However, it’s not fully clear that baseline architectures/hyperparameters are tuned comparably at larger action cardinalities, nor how replay/priority settings affect them. A more stringent hyperparameter search for macro baselines, plus reporting compute used per method at each macro length, would address confounds. \n\nMinor: Several figures/tables say “confidential intervals” instead of “confidence intervals”"}, "questions": {"value": "In practice, do you use the hard-KL dual update every iteration? What target-KL values worked across domains, and how sensitive is performance to this hyperparameter? Any instability modes?\n\nYou use FKL for DNA/macro-Atari and RKL for MARL. Can you provide guidance/heuristics for selecting FKL vs RKL (e.g., entropy/temperature schedules, action-space sparsity, reward multimodality), with ablations on a common domain? \n\nCould you include or justify against MAPPO/QMIX-style baselines with similar feature extractors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v9X1EioH32", "forum": "tM34PZf8W9", "replyto": "tM34PZf8W9", "signatures": ["ICLR.cc/2026/Conference/Submission19930/Reviewer_gKvW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19930/Reviewer_gKvW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963971630, "cdate": 1761963971630, "tmdate": 1762932094878, "mdate": 1762932094878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Summary of Revisions"}, "comment": {"value": "We thank the reviewers for their constructive feedback. We have a major revision of the manuscript to address all the questions reviewers have raised, while the major one is to provide a comprehensive analysis and direct comparison of the two proposed policy improvement objectives: RL-D$^2$ with Forward KL (FKL) and Reverse KL (RKL). Key updates include:\n\n1. **Direct comparison of FKL and RKL**: We extended our evaluations to compare FKL and RKL within the same environments, both In Google Research Football (Table 2) and in Atari games (Figure 15). In general, RL-D$^2$-FKL showed a faster initial learning and strong performance with data or computational budget. RL-D$^2$-RKL demonstrated stronger asymptotic performance with sufficient amount of data. Moreover, **we first achieved ~100% percent win rate in the most challenging Google Football task**, *11 vs 11*, demonstrating strong empirical performance of the proposed algorithm.\n\n2. **Comprehensive Ablation Studies**: We included new ablation studies focusing on temperature tuning, batch sizes, (both in Appendix E.2), and low sampling regime (Table 2), which clarifies empirical implementation details and hyperparmeter sensitivity of both FKL and RKL methods\n\n3. **Discussions on Empirical Trade-offs** (New Section 5.4): Based on these results, we added Section 5.4 to analyze the trade-off and provide practical guidance on selecting between objectives:\n\n    - FKL favors exploitation and has better data efficiency. It outperforms RKL in low-data regimes (e.g., ~70% win rate vs. ~0% in 11vs11-sm) and computational bottlenecks (Atari), but requires a careful temperature tuning mechanism.\n\n    - RKL favors stability and asymptotic performance. It achieves higher final win rates in complex multi-agent tasks given sufficient compute and data (e.g., Google Football), but learns slower initially.\n\n    - Please refers to [**A2** in Response to Reviewer gKvW](https://openreview.net/forum?id=tM34PZf8W9&noteId=zWUO3FBt6g)  for detailed explanations.\n\nWe also include other small modifications mentioned by the reviewers such as typo fix, adding flowchart and baseline selection protocal, etc."}}, "id": "D9FvSFMNkr", "forum": "tM34PZf8W9", "replyto": "tM34PZf8W9", "signatures": ["ICLR.cc/2026/Conference/Submission19930/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19930/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission19930/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763528714488, "cdate": 1763528714488, "tmdate": 1763528714488, "mdate": 1763528714488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}