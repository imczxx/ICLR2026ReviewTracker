{"id": "N4BB09TXad", "number": 566, "cdate": 1756747764179, "mdate": 1759898253372, "content": {"title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence", "abstract": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code, benchmark, and checkpoints will be made publicly available.", "tldr": "", "keywords": ["Multimodal LLM", "Data Synthesis", "Code Generation", "Data Visualization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3529330cd909c25c0f628f042d0059d580441bae.pdf", "supplementary_material": "/attachment/a0419eef8274f481a6edd878bd13c5af2422ce69.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces JanusCoder, a family of multimodal code-generation models that integrate textual, visual, and programmatic information. Its key contributions include:\n1.  JanusCode-800K Dataset: A new dataset covering both text-centric and vision-centric tasks;\n2.  DTVBench Benchmark: A new benchmark for dynamic theorem visualization using Manim and Mathematica;\n3.  Strong Performance: Comprehensive evaluations on eight benchmarks show JanusCoder performs on par with or surpasses GPT-4o and specialized open-source models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.  This paper introduces a comprehensive dataset (JanusCode-800K), which fills a clear gap by integrating visual, textual, and programmatic modalities at scale;\n2. The paper is well-written, with clear motivation and structured sections to detail the data curation process;\n3. JanusCoder obtains strong empirical results across >8 benchmarks;"}, "weaknesses": {"value": "1. DTVBench’s limited scale (~102 tasks) may constrain statistical reliability;\n2. Why does JANUSCODERV-8B perform worse than InternVL3.5-8B on DesignBench and WebCode2M?\n3. When considering visual information, besides benchmarks purely focused on visualization, there are also some algorithmic or reasoning-related benchmarks [1]. Has the paper evaluated or discussed model performance on such tasks?\n4. A question mark appears at line 253.\n\n\n[1] MMCode: Benchmarking Multimodal Large Language Models for Code Generation with Visually Rich Programming Problems"}, "questions": {"value": "See the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HXaElRxBY0", "forum": "N4BB09TXad", "replyto": "N4BB09TXad", "signatures": ["ICLR.cc/2026/Conference/Submission566/Reviewer_uHBB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission566/Reviewer_uHBB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891487150, "cdate": 1761891487150, "tmdate": 1762915548234, "mdate": 1762915548234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "JanusCoder expands code intelligence from purely text-based inputs to jointly reasoning over code and visual outputs. It introduces a scalable multimodal code-data synthesis pipeline and builds JANUSCODE-800K, the largest multimodal code corpus to date. The authors develop unified models that handle text-centric and vision-centric coding tasks, achieving performance comparable to or surpassing commercial systems while offering insights into aligning program logic with visual expression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a unified visual-programmatic interface and a complete multimodal code data synthesis toolkit, enabling code models to reason jointly over textual and visual programming tasks.\n\n2. The work builds JANUSCODE-800K, the largest multimodal code corpus, and conducts extensive experiments across diverse benchmarks and modalities, demonstrating careful design and thorough empirical evaluation.\n\n3. Results show strong performance, with 7B, 14B models approaching or surpassing commercial systems across text-centric and vision-centric code tasks."}, "weaknesses": {"value": "1. Unclear data release plan: The data synthesis pipeline and JANUSCODE-800K corpus are central contributions, yet public availability is not guaranteed at submission time, raising concerns about reproducibility and community impact. The paper should explicitly clarify the dataset release schedule and scope.\n\n2. Judge-based evaluation bias: The Stage-3 refinement heavily depends on LLM/VLM judges, which risks evaluation circularity and bias toward the same models used for filtering. More human evaluation or cross-model adjudication would strengthen reliability and reduce bias.\n\n3. Limited methodological novelty: Despite strong engineering effort, the paper reads largely as a dataset and data-pipeline contribution. The model setup closely follows existing architectures and training paradigms, making the work feel more like a large-scale data release than a method advance."}, "questions": {"value": "JANUSCODE-800K is central to the contribution. Will the full dataset be publicly released at camera-ready?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n65MbnoYcz", "forum": "N4BB09TXad", "replyto": "N4BB09TXad", "signatures": ["ICLR.cc/2026/Conference/Submission566/Reviewer_NYgb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission566/Reviewer_NYgb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973232809, "cdate": 1761973232809, "tmdate": 1762915548131, "mdate": 1762915548131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents JanusCoder, a suite of multimodal models aiming to unify code generation and visual reasoning through a “visual–programmatic interface.” The authors introduce a large-scale data synthesis toolkit and release JanusCode-800K, a multimodal corpus covering charts, WebUIs, visual artifacts, and animations. Two models—JanusCoder (text-centric) and JanusCoderV (vision-centric)—are trained on this corpus using Qwen and InternVL backbones. The authors also propose DTVBench, a benchmark for dynamic theorem visualization tasks. Experiments across seven benchmarks show that JanusCoder models outperform open-source baselines and sometimes rival GPT-4o. Ablation studies indicate that cross-domain synergies and reward-based data filtering contribute significantly to performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Ambitious and timely goal: a unified model bridging code logic and visual semantics.\n\n- The JanusCode-800K dataset appears large, diverse, and potentially impactful for future research.\n\n- Strong empirical results on both text- and vision-centric benchmarks, including new ones created by the authors."}, "weaknesses": {"value": "- Model novelty is limited. The architecture largely reuses Qwen and InternVL; the “unified interface” claim feels more conceptual than technical.\n\n- Weak quantitative evidence for data quality improvements. The reward model and filtering pipeline are described but not systematically validated.\n\n- Overextended scope. The paper attempts to be both a dataset, benchmark, and model paper, which dilutes its main scientific contribution.\n\n- Lack of human evaluation for subjective visual tasks (animations, WebUIs)."}, "questions": {"value": "- What exactly differentiates the “unified visual-programmatic interface” from standard multimodal fine-tuning?\n- Could you quantify the impact of reward modeling (e.g., pre- vs. post-filtering data quality metrics)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NO7hNoPYRD", "forum": "N4BB09TXad", "replyto": "N4BB09TXad", "signatures": ["ICLR.cc/2026/Conference/Submission566/Reviewer_or79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission566/Reviewer_or79"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993907218, "cdate": 1761993907218, "tmdate": 1762915548016, "mdate": 1762915548016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}