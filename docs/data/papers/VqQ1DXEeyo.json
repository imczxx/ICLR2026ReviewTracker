{"id": "VqQ1DXEeyo", "number": 12185, "cdate": 1758206213284, "mdate": 1759897526673, "content": {"title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "abstract": "Conventional large language model (LLM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch: attackers overfit to obsolete exploits, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning (RL) algorithm, where a single model alternates between co-evolving attacker and defender roles---generating adversarial prompts and safeguarding against them---while a reward model adjudicates outcomes. Each role uses hidden Chain-of-Thought, which enables agents to reason about how to formulate and defend against attacks. Grounded in the game-theoretic framework of two-player zero-sum games, we establish a theoretical safety guarantee that motivates our method: if self-play converges to a Nash Equilibrium, the defender is assured to generate safe responses against any adversarial input. Empirically, Self-RedTeam demonstrates strong generalizability across four model sizes from both the Llama and Qwen families. We not only uncovering more diverse attacks (e.g., +17.80% SBERT), but improve the safety of models trained with industry-standard safety fine-tuning procedures like RL from Human Feedback (RLHF) by as much as 95% across 12 safety benchmarks.Our results motivate a shift from reactive patching to proactive co-evolution, enabling scalable and autonomous self-improvement of LMs via MARL.", "tldr": "We used self-play reinforcement learning and hidden Chain-of-Throught to discover more diverse adversarial attacks and to align safer language model", "keywords": ["Reinforcement Learning", "LLM Safety Alignment", "Language Gamification", "Self-play", "Multi-agent LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/034e7ae6d859e1a40c3c2fce6fdcc3d28d9cd886.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel online multi-agent reinforcement learning framework for improving the safety of Large Language Models (LLMs). The core idea is to move away from the conventional static, reactive patching of vulnerabilities towards a proactive, co-evolutionary process. The method frames LLM safety as a two-player, zero-sum game where a single, shared-parameter LLM alternates between an attacker role (generating adversarial prompts) and a defender role (safeguarding against them)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It formulates red-teaming as a two-player zero-sum game with a formal safety guarantee at Nash Equilibrium.\n\n- It shows strong empirical results, showing consistent gains across 12 benchmark and multiple model families and sizes.\n\n- Extensive ablations show the effectiveness  of each proposed components."}, "weaknesses": {"value": "- Reward model  and policy (defender and attacker) share the same parameter $\\theta$, which looks confusing. Given that the WildGuard is used for reward model, it must be a notational mistake.  I think it would be better to use different parameters and explicitly state the reward model is frozen during entire training. \n\n- The KL term in Eq. is undefined. I guess the authors might use token-wise reverse KL, but it would better to explicitly define the term for clarity.\n\n- There is no direct head-to-head against red-teaming baselines. Rainbow Teaming [1] looks a relevant baseline if we use the same seed prompts for Rainbow Teaming.\n\n- It is unclear why using the same backbone for both defender and attacker is helpful other than computational efficiency. The same model competes with conflicting objective. I wonder how the proposed method enables stable training and performance improvement. \n\n- It heavily relies on initial seed prompts. Even though the attacker generated diverse attack prompts, they are still variants of the initial seed prompts, rather than new type of attacks.\n\n\n\n## References\n\n[1] Samvelyan, Mikayel, et al. \"Rainbow teaming: Open-ended generation of diverse adversarial prompts.\" Advances in Neural Information Processing Systems 37 (2024): 69747-69786."}, "questions": {"value": "- If we use a model that is already capable of generating think part, how the proposed method would work?\n\n- What happens if seed prompts are not available for training? \n\n- Are there any examples of reward hacking? The reward model is not perfect, which might lead to false harmful attack prompts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GVDCVa2AGm", "forum": "VqQ1DXEeyo", "replyto": "VqQ1DXEeyo", "signatures": ["ICLR.cc/2026/Conference/Submission12185/Reviewer_mcqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12185/Reviewer_mcqS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532072211, "cdate": 1761532072211, "tmdate": 1762923136101, "mdate": 1762923136101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SELF-REDTEAM, an online self-play reinforcement learning framework for LLM safety alignment, where a single model alternates between attacker and defender roles with a hidden Chain-of-Thought (CoT). The idea of framing safety training as a zero-sum game between co-evolving agents is novel and theoretically grounded, providing a clear motivation for proactive rather than reactive safety alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n- Conceptually appealing formulation of LLM safety as a self-play MARL problem with a Nash equilibrium–based safety guarantee.\n\n- Solid empirical results across multiple model families (Llama, Qwen), demonstrating significant robustness gains (up to 95% ASR reduction) with minimal performance degradation.\n\n- The Hidden CoT mechanism is an elegant addition, improving attack diversity and mitigating over-refusal."}, "weaknesses": {"value": "Weaknesses:\n\n- The theoretical guarantee relies heavily on the quality of the reward model; practical convergence to Nash equilibrium is not verified.\n\n- Some evaluation benchmarks (e.g., WildGuard/WildJailBreak) overlap with training data, potentially inflating results.\n\n- Experimental section could be more transparent about compute cost and stability during training."}, "questions": {"value": "1. How do you measure or verify convergence toward the proposed Nash Equilibrium in practice?\n\n2. How sensitive are the results to the choice or bias of the reward model used for safety evaluation?\n\n3. Does the observed 95% ASR reduction generalize to unseen or multi-turn adversarial prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vHCt84xMp5", "forum": "VqQ1DXEeyo", "replyto": "VqQ1DXEeyo", "signatures": ["ICLR.cc/2026/Conference/Submission12185/Reviewer_4Pif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12185/Reviewer_4Pif"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879642489, "cdate": 1761879642489, "tmdate": 1762923135538, "mdate": 1762923135538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-RedTeam, an online self-play RL algorithm that trains a single model to co-evolve attacker and defender roles in a two-player zero-sum game, generating adversarial prompts and safeguarding against them. They show that if the models reach Nash Equilibrium, the model is theoretically guaranteed to be safe — although this is likely impossible to achieve in practice. The approach demonstrates empirical improvements across multiple safety benchmarks (including WildGuard and HarmBench), and two model families (Llama3, Qwen2.5). The authors also offer a nice study on the distribution of the attacks generated by their method, showcasing how their method generates more diverse attacks over only fine-tuning an attacker LLM against a static defender. I have a few comments with regards to the theoretical justification as well as the evaluations (which could potentially be improved), but overall I am leaning towards a weak accept as the approach is nice, the work is very polished and well executed, and would be of interest to the research community."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is practical and efficient with a thorough analysis on the overhead of their approach (~45% longer than baseline with online generation). The framework is general and can be applied to any safety training pipeline, with a reasonable improvement to the refusal rate on the models tested.\n2. The paper is very well written and polished; the authors conduct many experimental results, including comparisons to other safeguarding baselines (LAT, CircuitBreakers), as well as ablations on the various components of their approach (self-play, CoT, SFT). The appendix is detailed and provides all necessary implementation details one would need to reproduce their work."}, "weaknesses": {"value": "1. If I understood correctly, the evaluations were done against *static* adversarial prompts (with the exception of X-teaming); stronger non-static attacks should be considered for the evaluations (i.e. applying some of the algorithmic methods to the final trained model itself, rather than using the preexisting attacks on other models). If the paper is indeed missing these evals, I would strongly recommend them for the discussion period.\n2. Results indicate that the improvements are decent but not spectacular; some evals have good improvements but others are very modest; I think it would be critical to see how well this approach fares to what was discussed in W1.\n3. The observation that over time, the attacker sometimes refuses to generate harmful attacks does suggest that it might not be ideal to use the same attacker/defender model\n4. I understand that the Nash Equilibrium arguments are just to show that you are optimizing for the correct target. However, it feels a bit ad-hoc because it was presented as a justification for the approach, then acknowledged that convergence is unlikely in practice, and then no longer discussed. I think it would be nice to have more discussion of either how close the training can converge to the NE in practice, and/or what happens when it doesn’t."}, "questions": {"value": "1. Re: emergent attacker refusal, it does feel like the attack/defense are in tension because the model is rewarded for breaking itself whilst also trying to be come more robust. Intuitively I would expect the results to improve if you had different models for the attacker and defender; have you experimented with this and seen anything to verify/disprove this hypothesis? How does this tie in to your theoretical motivation, as you require need strong attacker for robust defense?\n2. The reliance on WildGuard 7B to evaluate components used in the reward could have biases and be prone to reward hacking; it would be nice to see some discussion on how sensitive the method is to the judge model’s quality/biases, what happens if the judge has exploitable weaknesses, and if attacks are able to game the specific reward model.\n3. Why does the CoT help more for Llama than Qwen models?\n4. Do attacks transfer to other LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bCHL2DZntk", "forum": "VqQ1DXEeyo", "replyto": "VqQ1DXEeyo", "signatures": ["ICLR.cc/2026/Conference/Submission12185/Reviewer_h9EC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12185/Reviewer_h9EC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762381477828, "cdate": 1762381477828, "tmdate": 1762923135159, "mdate": 1762923135159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}