{"id": "0CZAimzcVr", "number": 8996, "cdate": 1758106456114, "mdate": 1759897749083, "content": {"title": "DR-Submodular Maximization with Stochastic Biased Gradients: Classical and Quantum Gradient Algorithms", "abstract": "In this work, we investigate DR-submodular maximization using stochastic biased gradients, which is a more realistic but challenging setting than stochastic unbiased gradients. We first generalize the Lyapunov framework to incorporate biased stochastic gradients, characterizing the adverse impacts of bias and noise. Leveraging this framework, we consider not only conventional constraints but also a novel constraint class: convex sets with a largest element, which naturally arises in applications such as resource allocations. For this constraint, we propose an $1/e$ approximation algorithm for non-monotone DR-submodular maximization, surpassing the hardness result $1/4$ for general convex constraints. As a direct application of stochastic biased gradients, we consider zero-order DR-submodular maximization and introduce both classical and quantum gradient estimation algorithms. In each constraint we consider, while retaining the same approximation ratio, the iteration complexity of our classical zero-order algorithms is $O(\\epsilon^{-3})$, matching that of stochastic unbiased gradients; our quantum zero-order algorithms reach $O(\\epsilon^{-1})$ iteration complexity, on par with classical first-order algorithms, demonstrating quantum acceleration and validated in numerical experiments.", "tldr": "", "keywords": ["DR-submodular Maximization", "Stochastic Biased Gradients", "Zero-Order Optimization", "Quantum Gradient Estimation", "Approximation Algorithms"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a13ab47d1b8bef974ca2dc3482b61e402ac773a8.pdf", "supplementary_material": "/attachment/7bc45e059cf0db278334218ed862e2f62c1f0345.pdf"}, "replies": [{"content": {"summary": {"value": "The paper studies a quite general optimization problem, namely maximizing a function F on [0,1]^n under some contraints defining a feasible set $\\cal C \\in [0,1]^n$. The function is DR-submodular. The feasible set is convex. Such a problem is typically solved using gradient ascend, and there is a huge literature on these techniques.\n\nThere are 3 contributions.\n\n1. Extending the Lyapunov framework, to allow gradients to be imprecise, with a bias and a noise.\n2. Proposition an 1/e approximation for non-monotone DR-submodular maximization over a convex set. The novelty is the assumption on a largest feasible point. This overcomes a 1/4 upper bound on the general optimization problem.\n3. Providing a quantum algorithm, based on the improved quantum Jordan algorithm from 2023, which has the same performance guarantees as its classical counterpart but improves in cubic iteration time.\n\nSome performance guarantees are proven, and experiments are conducted with standard benchmarks.\n\nMy background is too weak in this area to judge the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The domain is important and has a huge literature. The paper has 3 contributions, all of which seem central and important. The paper has a theoretical and a practical side."}, "weaknesses": {"value": "The paper seems to be hard to follow for an outsider."}, "questions": {"value": "- at some moment it would be good to say what DR stands for (diminishing return)\n- Page 3 line 127. I think you mean $\\textbf{x} \\vee \\textbf{y} = (\\max\\\\{x_i, y_i\\\\})_{i\\in [d]}$\n- Also here dimension is d and later it is n\n- Page 3, line 150. I could not understand the difference of a bias and a noise. Is the noise consistent, in the sense that n is a deterministic function? What is the domain of $\\xi$? What is known to the algorithm? Does it know the functions $b,n$ and the parameter $\\xi$ or only some of them? Does the algorithm knows the assumed bounds $m,\\eta_b, M, \\eta_n$?\n- Page 4 line 205. It wasn't clear to me before that the function x(t) is an algorithm.\n- Page 5 line 228. What does it mean to maximize two values? Or do you mean to maximize the maximum of the two values?\n- Page 6 line 277: which we have newly introduced -> Potential author name revealing\n- Page 8 line 418 Lipschiz -> Lipschitz \n- Page 10. Some acronyms should be uppercase, i.e. in curly brackets in the bibtex file. Such as DR, SGD."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1wohvWDEQJ", "forum": "0CZAimzcVr", "replyto": "0CZAimzcVr", "signatures": ["ICLR.cc/2026/Conference/Submission8996/Reviewer_3kKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8996/Reviewer_3kKo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826383953, "cdate": 1761826383953, "tmdate": 1762920725144, "mdate": 1762920725144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies continuous DR-submodular maximization under stochastic biased gradient oracles. It extends Du's Lyapunov framework to handle bias and variance in gradient estimators. Based on this, it provides approximation algorithms under three constraint classes: general convex, down-closed convex, and convex sets with a largest element. The paper develops zeroth-order algorithms, where the classical version achieves $O(\\epsilon^{-3})$ while the quantum version achieves $O(\\epsilon^{-1})$ iteration complexity, matching the performance of classical first-order methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extending Du's Lyapunov framework to stochastic biased gradients seems interesting. The framework explicitly characterize the effect of bias and noise, and the resulting analysis looks useful beyond this problem setting.\n- The constraint set given by a convex set with a largest element is well-motivated. Bridging the convex and down-closed settings leads to a provable $1/e$ guarantee.\n- The paper shows that quantum gradient estimation can close the gap to first-order methods in ratio and complexity."}, "weaknesses": {"value": "- The experiments are limited to $d=3$ due to simulating quantum algorithms in classical computers. To improve empirical evidence, larger-scale tests would make the claim of quantum acceleration more convincing."}, "questions": {"value": "- How tight is the $1/e$ approximation for convex sets with a largest element? \n- Would it be possible to provide alternative constructions for $a(t)$ and $b(t)4 that possibly improve the current setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NcKE4SRqr7", "forum": "0CZAimzcVr", "replyto": "0CZAimzcVr", "signatures": ["ICLR.cc/2026/Conference/Submission8996/Reviewer_VyE8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8996/Reviewer_VyE8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888111007, "cdate": 1761888111007, "tmdate": 1762920724559, "mdate": 1762920724559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of continuous DR-submodular maximization under the practically relevant yet theoretically challenging setting of stochastic biased gradients. The authors extend the Lyapunov framework, traditionally developed for exact or unbiased stochastic gradients, to handle gradient estimators that contain both bias and noise, thereby characterizing their effects on convergence and approximation guarantees. They further introduce a new class of constraints, namely convex sets with a largest element, that naturally arise in resource allocation and similar applications. Under this setting, the paper proposes a $1/e$-approximation algorithm for non-monotone DR-submodular maximization, which surpasses the known $1/4$ hardness bound for general convex sets. Building upon this framework, the authors design both classical and quantum zero-order algorithms, showing that the quantum version achieves the same approximation ratio with only $O(\\varepsilon^{-1})$ iteration complexity, demonstrating a quantum acceleration compared with classical zero-order methods that require $O(\\varepsilon^{-3})$. Numerical experiments on quadratic and coverage-type DR-submodular functions validate the theoretical results, showing that quantum algorithms converge faster and achieve comparable solution quality to classical first-order methods. Overall, the paper provides a unified theoretical and algorithmic treatment of DR-submodular maximization in the presence of biased gradients and connects classical optimization with emerging quantum techniques."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper extends the Lyapunov-based analytical framework to accommodate stochastic biased gradients, a setting that more faithfully represents real-world learning and optimization scenarios where gradient estimates are noisy and biased. The work also successfully integrates quantum computation into continuous submodular optimization, showing that quantum zero-order algorithms can match the convergence rate of classical first-order methods, offering a clear demonstration of quantum speedup. The results are rigorously proven, the methodology is well grounded in prior literature, and the experimental findings, though small in scale, corroborate the theoretical analysis."}, "weaknesses": {"value": "The paper currently lacks experimental results on runtime performance, which makes it difficult to assess the practical efficiency of the proposed algorithms.\n\nMoreover, it would be beneficial if the authors could provide additional real-world examples or application scenarios to better justify the practical relevance of the studied problem, since DR-submodular maximization has so far attracted more attention from the theoretical community rather than from most of the ICLR audience.\n\nMinor comment: In Figure 1, the legends in the third and sixth subplots appear partially covered by light-colored text, suggesting a plotting or rendering issue that should be corrected for clarity."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t5aP3DN3Kh", "forum": "0CZAimzcVr", "replyto": "0CZAimzcVr", "signatures": ["ICLR.cc/2026/Conference/Submission8996/Reviewer_AyjJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8996/Reviewer_AyjJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325522914, "cdate": 1762325522914, "tmdate": 1762920724177, "mdate": 1762920724177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of DR-submodular maximization.  They consider a few combinations of monotone/non-monotone functions over the hypercube and classes of convex constraints (general, down-closed, largest element).  The authors first propose an extension of a Lyapunov framework from exact to stochastic and biased gradients.  The authors consider a new constraint setting (largest element) for the non-monotone setting and obtain an improved approximation ratio (over using a general convex region based method).  The authors also show significant improvements in complexity for the value oracle setting using a quantum algorithm for gradient estimation.  Lastly, the authors run several experiments to demonstrate the improvements of the quantum based method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a new approximation ratio for the case of non-monotone DR-submodular maximization over convex constraints with a largest element.\n- The authors show that there is a notable convergence speedup using quantum algorithms for gradient estimation (for the value oracle setting).\n- The authors extend the Lyapunov framework for DR-submodular maximization to handle stochastic and biased oracles.\n- The authors include experiments that show improved convergence for quantum algorithms in the value oracle setting."}, "weaknesses": {"value": "- I had some uncertainty about the extent of technical challenges and novelty for some parts\n    - For the Lyapunov extension to handle stochastic biased gradients, it was unclear to me from the description in the main section what technical challenges were encountered compared to past works.   Could the authors summarize the challenges?\n        - in line 047 (Du, 2022) is cited as having a unifying Lyapunov framework unifying many previous methods.  That reference is not brought up in Section 3 for the framework, though the authors are upfront that they generalize a previous framework to handle stochastic (and biased) gradients.\n        - Du’s work applied the Lyapunov framework for DR-submodular, and though that work presumed exact oracles, for convex optimization at least has there been Lyapunov based approaches that handled stochastic and biased (or at least stochastic) gradients? If so, are there unique challenges in extending the Lyapunov framework for DR-submodular problems from exact to stochastic (and biased) gradients? \n    - For the quantum section, the authors adopt a quantum algorithm for gradient estimation for DR-submodular maximization similar to past works in convex optimization.  In light of past works (both from classic gradient estimation in DR-submodular works and wrt Augustino et al 2025 for quantum methods in convex optimization), were there some key steps that were particularly challenging?  \n\n\n### Minor\n- Table 1 I’d suggest listing DR-max approx. bounds (and complexities) for stochastic first order based methods for reference.  \n\n- Non-monotone max with largest element is new setting considered, so appx ratio is first, but not clear  how tight it may be (no lower bound)\n\n- Theorem statements referencing algorithms were imprecise, \n    - eg line 360 “returned by quantum algorithms satisfies” – do the bounds in Theorem 2 hold for any quantum algorithms, or the (single) specific algorithm adopted from van Apeldoorn et al 2023?  \n    - Theorem 3 line 377 “there are some quantum zero-order algorithms achieving” \n- Fig 1 the axes’ fonts are too small\n\n### Very Minor\n- “Lipschitz” not “Lipschiz”\n- line 407 “with [A]ssumption[s] 1 …”\n- line 366 “The query complexity of the value oracle” should that be the complexity of the algorithm?"}, "questions": {"value": "- Do the complexity bounds depend on the dimension?\n- I found Section 4 on quantum acceleration hard to follow.  I am not familiar with quantum methods.  I have some uncertainty about the specific set up and some uncertainty about whether the impressive complexity results using quantum algorithms are purely of theoretic interest or if in the (near) future there could be the potential for real-world use.\n    - From line 354, just to confirm, the set up is identical in terms of the environment (the (biased) value oracle)?  A learner that has access to a quantum computer can use quantum Jordan algorithms to achieve the speedup in terms of query complexity over a learner that only has access to classical computers, but the environment itself and how they interact with the environment is identical?\n    - Could the authors remark on example situations where there could be a practical benefit in terms of total run-time?  Eg Figure 1 is measured in terms of iterations.  How would that map to clock time?  I understand that in the experiments the authors were simulating a quantum algorithm on a classical computer, so there could be large overhead.     \n        - For readers unaware of how much overhead, just looking at Fig 1 results it might be tempting to consider using a simulated quantum algorithm even for a smaller number of iterations if the overhead is low enough.  What were the (rough) run-times?\n        - Would the authors be familiar enough with current quantum computers on the market if they would be big enough to be used for this type of problem? If not, is there an educated guess for how close quantum computing is to the scale even to be used for the small $d=3$ experiments here?\n        - For current/near future quantum systems, is there a rough sense for how long each iteration (for the quantum Jordan algorithm) might take in clock time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8QnjozI3NS", "forum": "0CZAimzcVr", "replyto": "0CZAimzcVr", "signatures": ["ICLR.cc/2026/Conference/Submission8996/Reviewer_bbTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8996/Reviewer_bbTg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762891420280, "cdate": 1762891420280, "tmdate": 1762920723823, "mdate": 1762920723823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}