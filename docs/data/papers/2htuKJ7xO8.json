{"id": "2htuKJ7xO8", "number": 8488, "cdate": 1758086791007, "mdate": 1759897781255, "content": {"title": "Adaptive Jailbreak Defense: A Self-Evolving Framework for Large Language Models", "abstract": "While (multimodal) large language models (LLMs) have attracted widespread attention due to their exceptional capabilities, they remain vulnerable to jailbreak attacks.\nVarious defense methods have been proposed to mitigate jailbreak attacks.\nThese methods typically incorporate specific defense mechanisms into the model during training or deployment, aiming to enhance the LLM's robustness against jailbreak attacks in advance.\nHowever, as new jailbreak attack methods continue to emerge, defense methods with static resistance mechanisms can frequently be bypassed during testing.\nTo address these limitations, we propose a defense framework, called Test-Time IMmunization (TTIM), which can adaptively defend against various jailbreak attacks through a self-evolving mechanism during testing.\nSpecifically, TTIM first trains a gist token for efficient detection, which is subsequently employed to detect jailbreak activities during inference.\nWhen jailbreak attempts are detected, TTIM implements safety fine-tuning using the identified jailbreak instructions paired with refusal responses.\nFurthermore, to mitigate potential performance degradation of the detector caused by parameter updates during safety fine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments conducted on both LLMs and multimodal LLMs demonstrate that, starting from non-guarded models, TTIM effectively defends against various jailbreaks during testing with few jailbreak samples. Code is attached as supplementary material.", "tldr": "", "keywords": ["Jailbreak Defense", "Multimodal Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8dcefe7a6eda6295b08110d4d5bf81361da5e3e0.pdf", "supplementary_material": "/attachment/6ee3e473291e1436f0bcb5af0850b706b9720591.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a test-time jailbreak detection approach. The ideas include including a gist token at the end of the response and a classifier that is continuously finetuned for jailbreak detection. The proposed approach is evaluated on two attacks and compared against a number of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "On the positive side, I enjoy reading the draft for the following reasons.\n\nFirst, developing jailbreak detection methods that work for both LLM and VLM is an interesting and important task."}, "weaknesses": {"value": "Second, the proposed idea of training a gist token is an interesting one.\n\nOn the less positive side, the draft can be improved from the following aspects.\n\nFirst, the authors perhaps overlooked many recent progress on jailbreak detection, such as those based on representation engineering and behavior steering, which avoids the cost of the gist token and the online training of the detector. In fact, the baselines on LLM are rather out-dated.\n\nSecond, it is not clear how generalized the trained detector is. In fact, the GCG results, presented in the appendix and on one model only, perhaps suggests that it is not as effective as for the other two attacks reported in the main text - due to perhaps generalization problems. \n\nThe following are some detailed comments. \n\nPage 3: Section 2.2 JAILBREAK DETECTION AND DEFENSE\n\nComment: The discussion is missing closely related works on representation engineering and behavior steering. \n\nPage 5: “A simpler alternative would be to remove the gist token and directly use the hidden state of the last token to perform detection. However, intuitively, the hidden state of the last token is used for generation and may not encapsulate the information relevant to the harmfulness of the response.”\n\nComment: There are alternative approaches such as detection based on hidden states at the time of the first token generation, which one could argue is more efficient since we don’t have to wait for the token to be completely generated. \n\nPage 5: “Like how the body detects and responds to pathogens, our system treats jailbreak activities as threats and uses a detector to identify them.”\n\nComment: Not sure how helpful this metaphor is since this is a standard detection and learn a classifier approach. The real question is: how do you avoid overfitting to the seen jailbreaking instructions?\n\nPage 6: “... and Retokenization (Jain et al., 2023) and SmoothLLM (Robey et al., 2023) for LLM”\n\nComment: These baselines for LLMs are rather out-dated given the many more recent approaches, such as LLMScan and arXiv:2505.15753."}, "questions": {"value": "Can you compare your method with recently proposed LLM jailbreak detection approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8YUKKTL7B", "forum": "2htuKJ7xO8", "replyto": "2htuKJ7xO8", "signatures": ["ICLR.cc/2026/Conference/Submission8488/Reviewer_1UUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8488/Reviewer_1UUP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198965370, "cdate": 1761198965370, "tmdate": 1762920364283, "mdate": 1762920364283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TTIM (Test-Time IMmunization), a defense framework that adaptively protects large language models (LLMs) and multimodal LLMs against jailbreak attacks during testing, rather than relying on pre-defined defense mechanisms."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Dynamic adaptive defense: TTIM continuously learns and adapts during testing, achieving 0% ASR after encountering only 10 jailbreak samples without requiring prior knowledge of attack methods.\n\n2. Unified cross-modal framework: The method demonstrates consistent effectiveness across both LLMs and MLLMs against diverse attack types with minimal computational overhead."}, "weaknesses": {"value": "**1. Limited Novelty: Core Idea Exists in Constitutional AI[1][2]**\n\n**Fundamental approach not novel**: Detecting harmful outputs and performing self-alignment is Constitutional AI's established framework. The gist token for detection is primarily an engineering optimization rather than a conceptual breakthrough, and Constitutional AI's self-reflection may be more natural and better aligned with model distributions. The paper lacks clear differentiation from Constitutional AI's iterative self-improvement paradigm.\n\n**2. Outdated Model Selection**\n\nThe experimental evaluation uses outdated models that limit the generalizability of findings. For LLMs, the paper only tests on LLaMA2-7B and LLaMA3-8B, missing current models like LLaMA3.2, Qwen2.5-7B, and Mistral-7B-v0.3. Similarly, for MLLMs, evaluation is limited to LLaVA-v1.6 variants and Qwen2VL-7B, while state-of-the-art models like Qwen2.5-VL, InternVL-3.5, and LLaVA-NeXT are absent. This raises serious questions about whether TTIM's effectiveness generalizes to modern models with improved safety alignments.\n\n**3. Insufficient Attack and Defense Baselines**\n\nThe baseline coverage is severely imbalanced and incomplete, particularly for LLMs. While MLLM evaluation includes comprehensive attacks (Figstep, MM-SafetyBench) and defenses (Adashield, VLGuard), LLM evaluation only tests against I-FSJ and GCG, missing mainstream attack methods like AutoDAN[3], AmpleGCG[4], StrongREJECT[5] and WildJailbreak[6]. More critically, defense comparisons are limited to SmoothLLM and Retokenization, while key methods are absent: Constitutional AI (the most relevant comparison given the conceptual similarity noted in Weakness 1, Goal Prioritization[8] RAIN[7] and GradSafe[9] (relevant comparison that perform detection before the final output stage). This significant imbalance undermines the paper's claims of unified effectiveness across both LLMs and MLLMs.\n\n[1] Constitutional ai: Harmlessness from ai feedback.\n\n[2] Break the breakout: Reinventing LM defense against jailbreak attacks with self-refine.\n\n[3] AutoDAN: Generating stealthy jailbreak prompts on aligned large language models.\n\n[4] AmpleGCG: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed LLMs.\n\n[5] A StrongREJECT for Empty Jailbreaks\n\n[6] WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models\n\n[7] RAIN: Your language models can align themselves without finetuning.\n\n[8] Defending large language models against jailbreaking attacks through goal prioritization.\n\n[9] GradSafe: Detecting unsafe prompts for LLMs via safety-critical gradient analysis"}, "questions": {"value": "1.  How does TTIM fundamentally differ from Constitutional AI's self-refinement mechanism?\n2. Can the authors provide results on state-of-the-art models?\n3. Why are mainstream LLM attack methods (AutoDAN, StrongREJECT, WildJailbreak) and critical defense baselines (especially Constitutional AI,goal prioritization,  RAIN) missing from the evaluation, and can comprehensive comparisons be added?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P8UJ5KA1rg", "forum": "2htuKJ7xO8", "replyto": "2htuKJ7xO8", "signatures": ["ICLR.cc/2026/Conference/Submission8488/Reviewer_9HAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8488/Reviewer_9HAx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392913147, "cdate": 1761392913147, "tmdate": 1762920363802, "mdate": 1762920363802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TTIM, a test-time immunization framework that detects jailbreaks with a lightweight “gist” token and classifier, then fine-tunes a LoRA adapter online on detected jailbreak instructions paired with refusal responses. Detection and defense are parameter-decoupled to avoid loss of performance. Evaluated on MLLMs (LLaVA variants, Qwen2VL-7B) with Figstep and MM-SafetyBench, and on LLMs (LLaMA2-7B-chat, LLaMA3-8B-Instruct) with I-FSJ and GCG.  It achieves small ASR and ODR values while keeping inference overhead relatively low."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a simple and deployable mechanism in which a single learned “gist” token and lightweight classifier enable fast, low-cost jailbreak detection without requiring a separate guard model.\n* The design supports online, adaptive defense using parameter-decoupled LoRA updates, allowing the model to improve its safety behavior during deployment while minimizing drift and preserving modularity.\n* The approach achieves consistently low ODR relative to other baselines.\n*  It includes a clear and favorable cost analysis demonstrating competitive overhead."}, "weaknesses": {"value": "1. While it is true that many defense strategies rely on auxiliary LLMs or input augmentation, there are other detection approaches (e.g., classifier-based log-likelihood deviation detectors or anomaly scoring methods) [1-4] that should be acknowledged and compared against.\n2. The LLM experiments are limited to the Llama family, including additional model families would strengthen the claims.\n3. Although the proposed method is presented as one that “learns as it sees new samples,” the paper lacks numerical evidence showing robustness to more advanced or subtle attacks (for example, tool-invocation chains, hidden prompt injection, or meta-instruction pivot attacks) [5-7].\n4. The rejection counting metric can be misleading because a refusal phrase may normally appear during a safe response. So, the reported ASR and ODR values may not reliably reflect actual performance. A more robust evaluation, such as using an auxilary LLM  for detecting harnmfullness should be used.\n5. Additional results in Appendix B are not sufficiently explained. They lack detailed explanation, do not clearly benchmark against baselines, and it is hard to interpret them meaningfully.\n\n***Minor remarks:***\n1. Table captions could be more detailed to improve clarity and help readers better understand the reported results.\n2. Highlighting the best performing values in each table would make comparisons more intuitive and visually more clear.\n3. In line 373, the reference should point to Tables 1–2 rather than Tables 1–3.\n4. Separating the analysis of LLMs and VLMs into distinct paragraphs would enhance readability.\n5. The GCG results currently located in the appendix are important to the paper’s contributions and should be moved into the main text.\n\n[1] Chen, G., Xia, Y., Jia, X., Li, Z., Torr, P., Gu, J. (2025) LLM Jailbreak Detection for (Almost) Free!\n\n[2] Galinkin, E., Sablotny, M. (2024). Improved Large Language Model Jailbreak Detection via Pretrained Embeddings.\n\n[3] Xie, Y., Fang, M., Pi, R., Gong, N. (2024). GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis.\n\n[4] Hu, X., Chen, P.-Y., Ho, T.-Y. (2024). Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes.\n\n[5] Andriushchenko, M., Croce, F., Flammarion, N. (2025). Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.\n\n[6] Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.J., Wong, E. (2023). Jailbreaking Black Box Large Language Models in Twenty Queries.\n\n[7] Mehrotra, A., Zampetakis, M., Kassianik, P., Nelson, B., Anderson, H., Singer, Y., Karbasi, A. (2023). Tree of Attacks: Jailbreaking Black-Box LLMs Automatically."}, "questions": {"value": "1. My understanding is that the detector is trained only on benign and harmful sentences, rather than true jailbreak examples. Could you clarify whether jailbreak-style prompts appear in the training data at all?\n2. Do the harmful goals used to generate adversarial attacks overlap with sentences present in the training set? For example, if “how to make a bomb?” appears during training, could that create unintended leakage? It would be helpful to evaluate on fully seperate harmful sets as a control.\n3. Could you provide an ablation examining how the maximum memory size influences performance? The paper currently uses a limit of 40 examples; what motivated that specific choice?\n4. How are the jailbreak attack samples collected or generated? Are they entirely self-generated or sourced from existing benchmarks or public datasets?\n5. For each table, over how many evaluation samples are the ASR values computed? Please include those counts for transparency.\n6. The simultaneous use of ASR, ODR, TPR, and FPR can be difficult to interpret. Could you more clearly differentiate TPR from (1 – ASR) and provide guidance on when each metric should be the primary reference?\n7. In line 395, the statement that adaptation requires “fewer than 10 samples” is ambiguous. Could you include concrete numbers or adaptation-curves for each dataset and model?\n8. The ODR of the vanilla model reported in Table 9 appears inconsistent with Table 2. Since ODR should not depend on the attack strategy applied, could you explain this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n4N4k8A99P", "forum": "2htuKJ7xO8", "replyto": "2htuKJ7xO8", "signatures": ["ICLR.cc/2026/Conference/Submission8488/Reviewer_Vhk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8488/Reviewer_Vhk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862722749, "cdate": 1761862722749, "tmdate": 1762920363373, "mdate": 1762920363373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a defense technique against jailbreaking attacks in  LLMs, called Test-Time Immunization (TTIM), which can adaptively defend against various jailbreak attacks through a self-evolving mechanism during inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a defense technique against jailbreaking attacks against both LLMs and VLMs.\n2.\tThis paper introduces a new defense technique that can continuously learn to defend against various jailbreak attacks by itself during inference."}, "weaknesses": {"value": "1.\tThe paper includes investigations on a limited number of SOTA models, mostly relying on the open-sourced models. The lack of inclusion of commercial models, GPT-5, Claude, and Gemini 2.5, and reasoning models, o3 or R1 models, should be included in the evaluation. \n2.\tAccording to the definitions of TTIM provided in the paper and compared with the biological immune system, in the real world, how effective is the defense against emerging jailbreaking attacks in the first place? \n3.\tIn lines 266-267, the immunity becomes stronger for future threats. How effective is it for the current or past threats?\n4.\tIn table 1, Why the performance on LLaVa -13B model is not as strong as 7B? Appropriate explanation is required.\n5.\tThe proposed defense is tested against a limited number of attacks. More recent attacks, e.g., MMJBench, Shuffle Inconsistency, IDEATOR, and BAP, these attacks should have been evaluated to better generalize the performance of the TTIM against jailbreak attacks.\n6.\tWhat’s the defense-time trade-off for the proposed defense? \n7.\tThe paper should also discuss the failure cases of the TTIM against the attacks and the potential reasons behind those failure cases.\n8.\tDoes the method fine-tune the target LLMs during test-time training phase?"}, "questions": {"value": "Please follow the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YlOldiDTkS", "forum": "2htuKJ7xO8", "replyto": "2htuKJ7xO8", "signatures": ["ICLR.cc/2026/Conference/Submission8488/Reviewer_xvrh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8488/Reviewer_xvrh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939622149, "cdate": 1761939622149, "tmdate": 1762920363059, "mdate": 1762920363059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the jailbreak defense of LLMs and propose a self-evolving framework called TTIM. TTIM is a test-time defense method that does not require finetuning the target model and thus can apply to most LLMs. This paper also designs a low-cost, high-efficiency detector using gist tokens, avoiding reliance on auxiliary models. A parameter decoupling strategy is introduced to ensure the stability of both detection and defense training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method proposed in this paper does not rely on overly restrictive assumptions, enabling its application to a broad range of models."}, "weaknesses": {"value": "The main weakness of this paper is that the models employed in the experiments and the comparative methods are outdated. Comparing the proposed method with the SOTA methods would enable a more rigorous and reliable evaluation of its effectiveness."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WQs3h0cDZp", "forum": "2htuKJ7xO8", "replyto": "2htuKJ7xO8", "signatures": ["ICLR.cc/2026/Conference/Submission8488/Reviewer_Pmkn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8488/Reviewer_Pmkn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971275016, "cdate": 1761971275016, "tmdate": 1762920362642, "mdate": 1762920362642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}