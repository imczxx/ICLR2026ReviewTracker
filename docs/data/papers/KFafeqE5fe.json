{"id": "KFafeqE5fe", "number": 21618, "cdate": 1758319732202, "mdate": 1759896912338, "content": {"title": "Curating High Quality Pretraining Data for Language Models via Compression Ratios", "abstract": "The quality of pretraining data determines the capabilities of language models, yet identifying high-quality data among billions of web documents remains computationally prohibitive. We introduce Compel, a simple and scalable data processing step that isolates high-quality text using lightweight, compression-based signals. Our key insight is that the compression ratio of text serves as a robust, model-free proxy for information density: low compression ratios typically reflect repetitive or boilerplate content, whereas high ratios may indicate noisy or unnatural text (e.g., HTML spam or phone numbers). Compel improves dataset quality by retaining only those documents whose compression ratios fall within a chosen range, determined empirically from high-quality reference datasets, without relying on additional model training or heuristic classifiers. Compel improves benchmark performance by around 0.5–1.1% across leading open web-scale datasets - DCLM, FineWeb, and FineWeb-EDU— all while requiring only a fraction of the computational resources of traditional filtering methods. These results show that compression-based filtering is a practical, compute-efficient complement to prevailing quality controls, capable of boosting pretraining data quality.", "tldr": "", "keywords": ["Data Curati"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f6e86d320866da645773d6b99496effa360be2a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Compel, a simple and fast method for improving the quality of pretraining data for language models. Instead of using complex classifiers or models, Compel uses a document’s compression ratio as a signal for text quality. The main idea is that well-written and information-rich documents fall in a “sweet spot” of compression and it's not too repetitive and not too noisy. By keeping only the documents within a certain range, this method can improve dataset quality efficiently. They test approach on FineWeb, FineWeb-EDU, and DCLM, and show small but consistent performance improvements on multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using compression ratio as a filtering signal is a clever and refreshing idea. It’s not common in data filtering pipelines, and it avoids the need for training additional models or using expensive heuristics.\n2. The experiments are solid and cover multiple datasets and model sizes. Results are consistent and clearly show that the method works.\n3. The paper is easy to follow, with clear figures and explanations that make the concept intuitive even for readers without a deep technical background in data filtering."}, "weaknesses": {"value": "1. The thresholds are manually set and may not work equally well for all domains or languages. It would be better if the paper show how sensitive results are to these values or suggested an adaptive method.\n2. There’s limited theoretical justification for why this compression range works best or how it might relate to linguistic quality in different contexts.\n3. The discussion of limitations could include more practical guidance , such as how users could combine Compel with other filters in real pipelines."}, "questions": {"value": "1. How stable are the results if different compression algorithms are used instead of LZ4?\n2. Does Compel work equally well for multilingual data or code datasets?\n3. Since compression ratio is language-agnostic, is there a risk of keeping data with good structure but misleading or low factual value?\n4. Could you share more details about the computational savings compared to perplexity-based filters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fd58xpV335", "forum": "KFafeqE5fe", "replyto": "KFafeqE5fe", "signatures": ["ICLR.cc/2026/Conference/Submission21618/Reviewer_BatL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21618/Reviewer_BatL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761047917693, "cdate": 1761047917693, "tmdate": 1762941856486, "mdate": 1762941856486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a lightweight model-free filtering method that uses document compression ratio as a proxy for information density. By discarding documents with very low (repetitive) or very high (noisy) compression ratios, it improves the quality of pretraining data with minimal compute cost. Experiments on FineWeb, FineWeb-EDU, and DCLM show consistent gains across 13 benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple and interpretable idea that aligns well with intuition about data redundancy and noise.\n- Requires no model training, labeling, or tokenization and is extremely efficient.\n- Strong empirical evidence with diverse benchmarks.\n- Qualitative examples nicely illustrate the filtering effect."}, "weaknesses": {"value": "- Thresholds for compression ratio appear manually tuned and not rigorously justified.\n- Missing analysis of compressor choice or sensitivity across domains.\n- Gains are relatively small and not clear how meaningful.\n- Focuses only on English web data; unclear generality to other languages.\n- Limited exploration of combining compression with other lightweight signals."}, "questions": {"value": "- How sensitive are results to the selected range?\n- Would other compressors yield similar patterns?\n- Have you tried domain- or language-specific thresholding?\n- Does Compel meaningfully alter topic or style distributions in the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sm13eInlCP", "forum": "KFafeqE5fe", "replyto": "KFafeqE5fe", "signatures": ["ICLR.cc/2026/Conference/Submission21618/Reviewer_QeRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21618/Reviewer_QeRZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728138582, "cdate": 1761728138582, "tmdate": 1762941856188, "mdate": 1762941856188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel way of filtering large scale text data used for LLM pretraining. The novel method relies on the insight that compression ratio can be indicative of text quality, since it can detect repeated text (highly compressable) or malformed text (not compressable). This method does not require training models, it does not rely on underlying model embeddings, nor does it rely on human labels as standard classifier approaches do. Authors show that marginal gains can be achieved if their method is applied on top of existing filtered corpora such as fineweb and DCLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- They decide to invest their compute budget on more tests on datasets and LM instead of more compression methods for LM training. Makes the contribution stronger on downstream evaluations.\n- The premise that a lightweight compression algorithm can be used to filter data to improve the performance of an LM is interesting and compelling.\n- Experiments are conducted at good scales, at 1B and 8B scales.\n- Their approach requires no human labels, which is an interesting property."}, "weaknesses": {"value": "- The difference between C4 and DCLM in Figure3 does not seem so significant, both have similar top range in y axis, with similar spread on the x axis, differences do not seem to be so evident that compression captures the high noise of C4 and rewards the quality of DCLM.\n- Figure 4 is used to convey that the compression ratio is well aligned with the FineWeb-EDU decisions, however in this plot the overlap between kept and discarded is large. It is not convincing that the compression ratio is well aligned with the edu classifiers. In fact, it seems to imply that if you discard, then it is of high compression, but the inverse causality (the relevant one) is not shown. If you use this CR metric, you will not get the edu classifiers (as also shown in Table 1 by the lower performance).\n- In Figure 4, it also shows that examples with small compression ratios should be kept, which goes against your claim.\n- There is no rigorous way of getting the decision thresholds for CR cutoffs, it is based on plot observations against Fineweb-EDU, which is not a scalable and well grounded approach. Additionally, at 0.65 there are more documents to be kept than discarded, it is not clear why that would be the lower bound cutoff…\n- For Figure 4, they should have tried with more compression algorithms besides LZ4 to better strengthen their choice, this is not an expensive plot to run as it is CPU-only and can be done on a subset of the data. LZ4 does not have entropy coding as most compression algorithms do, which makes it fast, but it might not be the best choice.\n- FineWeb and DCLM are built from raw, crawled text that is extremely hard to filter. Compel is only applied on top of these datasets, not on raw text, so it does not show to be a replacement for such approaches, but potentially as a way to slightly improve over already highly curated datasets. If compared against the edu method, Table 1 shows that the increase from Fineweb to EDU (0.018) is higher than from fineweb to fineweb+compel (0.011), which if the results in the table are significant, is a large difference.\n- Compel claims to filter without incurring the costs of models, supervision or inference, however all results are over datasets that were already filtered in that way, so compel can only be applied after those costly steps have been done, it is not proved its efficacy in raw corpora (over which people applied models, etc… as claimed by the authors).\n- The improvement over FineWeb-EDU of 0.007 and over DCLM of 0.004 seem small, given that there is no std of runs or statistical significance tests, it is hard to understand if these are meaningful, but seem small compared to the improvements over FineWeb.\n- Table 1 does not say on how many tokens the models were trained. It is hard to know if they were trained well or if they are undertrained.\n- The claims are that the modest gains they achieve, if done at larger scale, can have a very large impact, however this is never proven. I understand there are compute limitations, but these claims cannot be made if they cannot be proven.\n- I notice they mention some of my concerns in section 8, which means the authors are aware of such limitations. I think the paper is not finished. Work towards those directions is needed to have something that is concrete, what is presented is a nice proof-of-concept but not a finished paper.\n- They show only 1 baseline of filtering which is the EDU baseline, that outperforms their approach, more baselines or comparisons are needed to strengthen their claim. There are many works on filtering data, and they perform no comparisons.\n- Claims on efficiency are made, but not proven/shown."}, "questions": {"value": "- If DCLM performs better, why is the work trying to replicate the choices made by FineWeb-EDU?\n- In Table 2 it is not clear how the macro and micro average were done as it is not in the main text. Could this be explained?\n- Could they provide, in the appendix, a more thorough study/comparison of the high compression ratio text? They say it is artificial text, but the example provided simply mixes two languages which is easily discardable by language id or any other easy metrics. What does English-only artificial text that gets filtered out look like? The examples in section 7 are the same ones as in the beginning, a more thorough analysis is needed to improve the quality of the paper.\n- They claim their approach is more lightweight and efficient. There are, however, no speed, or compute costs comparison. Is it possible to run baselines (such as the edu classifiers), and compare the time spent on all approaches, and how much does it compare to the training time of the LLMs.\n- For large scale LLM training, is data filtering a bottleneck in terms of time? Is it a relevant percentage compared to time of training and development of the models?\n- I want to see how compression scales with quality of training, i.e. compression rate vs LLM performance, this is not done. I understand that compute is limited, but at least 1 experiment should be done, the scale does not need to be 1B, it can be at lower scale (e.g. 500M scale, or even lower at 200M scale and evaluated only on tasks such as HellaSwag and not with QA)\n- Could the authors show systematically in texts they know are information dense and not, that compressing them indeed reveals the property they claim that CR can effectively correlate with this. I do not mean over a large dataset as they did. I mean over a small set of examples that are dense and not dense and see how well the compression predicts the density.\n- How was the lower cutoff value of 0.65 chosen?\n- Why was MMLU not included in the tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hg257ch3Uy", "forum": "KFafeqE5fe", "replyto": "KFafeqE5fe", "signatures": ["ICLR.cc/2026/Conference/Submission21618/Reviewer_ZcWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21618/Reviewer_ZcWT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901159016, "cdate": 1761901159016, "tmdate": 1762941855953, "mdate": 1762941855953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a compression-based data filtering method, Compel, for language model pretraining. This method monitors the compression ratio of documents and selects documents with a compression ratio within a range, where they calibrate the threshold using high-quality reference datasets. The paper shows that their method improved downstream performance across 3 large datasets based on evaluation across 13 tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple, clean, and most importantly, very scalable. It can be a high value for pre-training data curation. \n\n2. Experiments and their setup are very clearly presented and easy to follow."}, "weaknesses": {"value": "1. Compel needs to be compared with the baseline classifier- and perplexity-based filtering methods for both performance and computational cost dimensions. Showing a conservative improvement over the base datasets is not enough to judge the benefits of the method. For instance, since a high-quality dataset needs to be used for rthe eference threshold range, the same dataset can be used for classifier-training to compare. \n\n2. It is quite understandable that the experimental setup is relatively small (in terms of token budget) due to the cost of pretraining. However, for a pre-training data filtering work, it is important to understand how robust the method is when the token budget is larger, since the language models are commonly pretrained with trillions of tokens. Additional ablations that have an increasing token budget (to at least show the trend) are required. \n\n3. Figure 4 could be misleading -- documents accepted occupy the selected compression range; however, there is no distinct trend showing that discarded documents are out of this range."}, "questions": {"value": "Why did you not use Fineweb-Edu as a comparison point rather than a pre-training dataset? The results from 1.4B show higher performance with Fineweb-EDU (0.442 vs 0.435) \n\nI would like to see how Compel Fineweb filtering compares with Fineweb-Edu itself (LLM classifier), also in the 8B model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCeWmH6RbU", "forum": "KFafeqE5fe", "replyto": "KFafeqE5fe", "signatures": ["ICLR.cc/2026/Conference/Submission21618/Reviewer_G5rH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21618/Reviewer_G5rH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078743590, "cdate": 1762078743590, "tmdate": 1762941855486, "mdate": 1762941855486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}