{"id": "WMEt1oVb3W", "number": 7418, "cdate": 1758021011028, "mdate": 1763390459704, "content": {"title": "Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "abstract": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of implicit rewards, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce **Adaptive Meta Fine-Tuning (AMFT)**, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a meta-gradient adaptive weight controller that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via  \\url{https://anonymous.4open.science/r/anonymous-amft-6B5B/}.", "tldr": "This paper introduces an algorithm that optimizes the SFT-RL trade-off by meta-learning the optimal, dynamic balance between imitation and exploration for superior LLM alignment.", "keywords": ["LLM post-training", "reasoning", "generalized domains"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6919186675a2154370d64b2cabde7711589915a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Adaptive Meta Fine-Tuning (AMFT), a novel single-stage algorithm designed to align Large Language Models (LLMs) by dynamically balancing Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). The core proposal is an adaptive meta-controller that treats the SFT-RL balance (controlled by a parameter $\\mu$) as a learnable parameter. This controller uses meta-gradients computed with respect to a long-term validation objective to autonomously learn an effective training curriculum. The paper unifies SFT and RL by reframing SFT as the optimization of an implicit, dense, path-level reward, complementary to the explicit, sparse, outcome-based reward of RL. AMFT includes an SFT warm-up phase and an entropy heuristic for stability. Extensive experiments on mathematical reasoning, abstract visual reasoning, and vision-language navigation demonstrate that AMFT achieves promising performance, improved generalization, and better sample efficiency compared to existing SFT, RL, sequential SFT$\\to$RL, and other hybrid methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a meta-controller that dynamically adjusts the weight between SFT and RL objectives, guided by a long-term validation signal. This approach provides an adaptive balance mechanism for SFT and RL.\n\n2. Experimental results demonstrate that the proposed method consistently achieves better performance on various reasoning tasks, including out-of-distribution scenarios. This indicates improved generalization capabilities.\n\n3. The inclusion of an SFT warm-up phase and an entropy-based heuristic contributes to a more stable and efficient learning process, addressing common instability issues in reinforcement learning."}, "weaknesses": {"value": "1. This work appears to be a combination of RL and SFT. Although it learns the mixing parameter $\\mu$ via meta-gradients, this brings in new problems regarding the computational viability of the meta-gradient (see below).\n\na) The AMFT method frames the problem as a bi-level optimization, where the outer loop updates the balance controller $\\mu$ based on the performance of the inner loop model $\\theta$. To calculate the gradient $\\nabla_{\\mu} L_{total}$, the optimization process requires differentiating through the entire sequence of inner loop updates. To update $\\mu$, the optimizer must effectively ``unroll'' $K=20$ steps of the inner optimization loop (which consists of combined SFT and RL updates) and then backpropagate the meta-loss (e.g., validation accuracy) through all intermediate weight states and activation memories. The runtime comparisons for this work and other baselines are absent.\n\nb) To relieve the computational complexity of meta-gradient involving second-order differentials, this work adopts the First-Order MAML (FO-MAML) approximation. However, it has not explicitly justified that the FO-MAML used here is a reliable approximation for the second-order differentials with concrete evidence. For example, what is the mean relative error of this first-order approximation to the second-order counterpart in the tasks?\n\n2. The authors state that in Line 1874 that “average out some of the noise from single-batch\ngradient estimates”. However, the noise being averaged out is the meta-gradient noise, not the inner-loop batch noise. The benefit of increasing $K$ (the unroll length) is not simply ``noise reduction''. Besides, a longer unroll ($K=20$ vs. $K=1$) means the meta-loss is evaluated on a parameter $\\theta_{K}$ that is closer to the optimum determined by the current $\\mu$. This yields a more representative gradient for $\\mu$ (i.e., how $\\mu$ affects the long-term convergence) but does not necessarily reduce the variance of the meta-gradient itself; in fact, a longer unroll often increases the overall variance and cost.\n\n3. A widely-used method for preventing catastrophic forgetting during RL fine-tuning (e.g., PPO or DPO) is to add a KL-divergence penalty between the current policy and the original SFT policy (or the initial pre-trained model). This penalty directly regularizes the policy's deviation from its original knowledge base. Relying solely on the presence of the SFT loss (even adaptively weighted) may not be sufficient to completely stop forgetting of specific SFT knowledge not frequently activated by the combined SFT/RL sampling procedure. Therefore, the authors should conduct ablation experiments to verify that the dynamic weighting is superior to standard KL-based regularization."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j8UB3PNcAO", "forum": "WMEt1oVb3W", "replyto": "WMEt1oVb3W", "signatures": ["ICLR.cc/2026/Conference/Submission7418/Reviewer_gaMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7418/Reviewer_gaMK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842877732, "cdate": 1761842877732, "tmdate": 1762919538886, "mdate": 1762919538886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Meta Fine-Tuning (AMFT), a single-stage framework that unifies supervised fine-tuning (SFT) and reinforcement learning (RL) by interpreting SFT as optimizing an implicit, path-level reward and RL as optimizing an explicit, outcome reward. AMFT introduces a learnable mixing weight µ between SFT and RL losses, updated by a meta-gradient with respect to a validation utility (explicit task reward), combined with an entropy-based heuristic for short-term stability. The method thus treats the SFT-RL balance as a bilevel optimization, approximated via a one-step meta-gradient. The authors argue this provides a principled, forward-looking alternative to heuristic balancing in prior single-stage approaches (e.g., SRFT, LUFFY, SuperRL, SASR, DyME).\n\nExperiments fine-tune Qwen2.5-Math-7B for math reasoning and LLaMA-3.2-Vision-11B for multi-modal reasoning (General Points, V-IRL). AMFT reportedly establishes new SOTA on five in-distribution math benchmarks and three OOD general reasoning benchmarks, and outperforms baselines on visual tasks (both ID and OOD variants). Ablations suggest both the meta-gradient and entropy terms are necessary; efficiency analysis claims fewer training steps and fewer RL rollouts to reach target performance. Theoretical sections sketch SFT-as-implicit-reward and connect the SFT term to a data-driven KL proxy; limitations of the one-step meta-gradient and validation-set dependence are discussed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear reframing of SFT as implicit reward and RL as explicit reward; positions the problem as learning a dynamic Lagrange multiplier for an imitation constraint.\n\n2. Principled controller: Using meta-gradients on a validation utility is a forward-looking alternative to entropy- or density-based heuristics; the one-step Jacobian-vector approximation is standard and computationally feasible.\n\n3. Solid empirical coverage: Cross-domain evaluation (math ID/OOD, visual ID/OOD), with competitive baselines (SRFT, LUFFY, TAPO, ReLIFT, two-stage SFT→RL).\n\n4. The paper provides deep insights into why AMFT works: The training dynamics visualizations (Figures 2, 3, 6, 7) are extremely effective at illustrating how AMFT autonomously discovers a training curriculum and avoids policy collapse; The ablation study (Table 4) is conclusive, demonstrating that both the meta-gradient (long-term signal) and the entropy heuristic (short-term stabilizer) are essential, synergistic components; The efficiency analysis (Table 5) provides a compelling practical argument for AMFT, showing it converges faster and with fewer expensive RL rollouts. They make the approach very solid in the empirical aspect."}, "weaknesses": {"value": "1. Causal attribution vs. confounding: Although ablations are included, it remains unclear how much of the gains stem from the validation-driven µ as opposed to implicit regularization from continual SFT or careful hyperparameters. Missing controls: e.g., a “learned heuristic” baseline (learned schedule without meta-gradient), or a meta-optimized fixed KL to \\pi demo vs \\pi_ref.\n\n2. Validation leakage/overfitting risk: The meta-objective depends on Dval; no safeguards or analyses on bias/leakage, distribution shift between Dval and test, or robustness when Dval is small/noisy. No report of multiple random seeds or confidence intervals; significance is unclear.\n\n3. Theoretical rigor gaps: The “SFT as implicit reward” derivation relies on particular choices (TV divergence, assumptions about Vπ) and sketches high-level equivalences; proofs lack full conditions for LLM sequence modeling with long horizons and PPO/GRPO specifics. The identification of LSFT as a KL proxy to πdemo is standard, but the jump to “optimal time-varying Lagrange multiplier” lacks a formal bilevel optimality/convexity analysis or regret bounds for the \\mu-updates.\n\n4. Limited fairness diagnostics: Differences in rollout budgets, prompt sampling, and inference settings can tilt outcomes. While steps are matched (500), no variance across seeds, no per-benchmark breakdown with error bars, and some baselines (e.g., TAPO, ReLIFT) are simplified implementations rather than author-released code, risking under-tuning.\n\n5. Practicality/scalability: Meta-gradient updates add overhead and require a curated validation set. The claimed overhead is not quantified (wall-clock, GPU hours). For frontier-scale models, the additional cost and complexity could be non-trivial.\n\n6. Reward design and sparsity: For math, reliance on Math-Verify and OAT-Grader is reasonable, but for multi-modal tasks, reward sparsity and correctness signals are less well specified; details on verifiers, shaping, and failure modes are light.\n\n7. Reproducibility risk: Anonymous code link is promised, but many strong baselines were re-implemented with nontrivial choices (e.g., TAPO “simplified”), which can affect conclusions."}, "questions": {"value": "1. How large is Dval, how is it split from training, and how is leakage prevented? Results with multiple Dval seeds/sizes? What happens when Dval distribution is shifted from test?\n\n2. Report mean/std over ≥3 seeds for all benchmarks; are improvements statistically significant per benchmark?\n\n3. Quantify wall-clock, GPU hours, and added VRAM/time for meta-updates at K=20 vs K=100 and vs SRFT/LUFFY. How does overhead scale with model size?\n\n4. Compare against (i) learned heuristic µ via supervised regressor on training-time signals; (ii) bandit/HPO-style online µ tuning (e.g., Population Based Training), and (iii) adaptive KL to πdemo instead of LSFT mixing. Also compare meta-only vs entropy-only at matched compute.\n\n5. Use official implementations where possible; provide hyperparam sweeps and best-of-n settings for SRFT, LUFFY, TAPO, ReLIFT. Include off-policy data ratios and PPO/GRPO configurations. Report per-task ablations on PPO-clip, KL to πref, etc.\n\n6. Can you provide bounds or regret-style analysis for the one-step meta-update of \\mu under smoothness assumptions? Any diagnostics of the meta-gradient variance? Empirically, how often does \\mu saturate at clip bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMA8wWillf", "forum": "WMEt1oVb3W", "replyto": "WMEt1oVb3W", "signatures": ["ICLR.cc/2026/Conference/Submission7418/Reviewer_gedw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7418/Reviewer_gedw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885435443, "cdate": 1761885435443, "tmdate": 1762919538439, "mdate": 1762919538439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Meta Fine-Tuning (AMFT), a single-stage post-training framework that unifies supervised fine-tuning (SFT) and reinforcement learning (RL) by optimizing a joint objective with a learnable trade-off weight. \n\nImplementation-wise, AMFT uses a meta-gradient controller that adjusts μ to maximize validation reward and an entropy-based short-term heuristic to prevent policy collapse. A brief SFT warm-up will be performed before mixing SFT batches and on-policy RL rollouts each step.\n\nOverall, I think this is a good piece of work that steps beyond the stage-level pipelined post-training method and is worthy of acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed meta-learned parameter is the key technical contribution and it empirically works by learning from signals from the validation set: empirical effectiveness is validated across multiple domains and model scales, with stable training curves and improved sample-efficiency relative to strong baselines.\n\n2. AMFT is compatible with diverse reward types used in RLVR.\n\n3. Comparison against post-training baselines are fair and complete."}, "weaknesses": {"value": "Major Concerns: \n\n1. Ablations do not fully isolate the source of gains. In addition to the provided studies, the paper should compare against (i) fixed-KL+PPO with careful tuning and (ii) implicit-reward SFT (e.g., DPO-style) + RLVR, to separate the benefit of meta-learning from having a strong regularizer.\n\n2. Despite the “single-stage” framing, AMFT still relies on an SFT warm-up. The degradation when shortening or skipping warm-up is under-explored.\n\n\nMinor issues:\n1. Several key files in the anonymized repository are missing, e.g. all config files can not be accessed.\n\n2. Figure 2 does not show adaptive weight, at least no curve in the figure corresponds to the legend."}, "questions": {"value": "1. How is the validation split constructed for the meta-gradient, given it plays a vital role in parameter update? I don’t see the details of how it was constructed. \n\n2. What is the frequency and compute overhead of meta-updates (e.g., every k steps; unroll length), and what is the end-to-end throughput/latency impact compared to PPO-only or SRFT under the same hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1TT6cB7Cx8", "forum": "WMEt1oVb3W", "replyto": "WMEt1oVb3W", "signatures": ["ICLR.cc/2026/Conference/Submission7418/Reviewer_nCwV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7418/Reviewer_nCwV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890340239, "cdate": 1761890340239, "tmdate": 1762919538023, "mdate": 1762919538023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AMFT, a single-stage post-training method that mixes SFT and RL with a learned weight updated by a validation-based meta-gradient and an entropy heuristic. Empirically, AMFT yields consistent ID/OOD gains on math and multi-modal reasoning benchmarks over sequential SFT→RL and single-stage baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Well structured and easy to follow. Clear algorithm and one-step meta-gradient derivation; results span multiple tasks with sensible training-dynamics plots.\n\n* Empirically, AMFT delivers consistent ID/OOD gains on math and multi-modal reasoning.\n* Ablations are convincing, removing the meta-gradient or the entropy term hurts performance, and the method reaches target scores with fewer RL rollouts."}, "weaknesses": {"value": "* Potential concerns regarding the fairness of computational comparisons. Baselines have heterogeneous training budgets (e.g., SFT-only for 3 epochs vs. RL methods for 500 steps) and use different RL objectives (PPO and GRPO are both mentioned), which complicates a direct comparison of efficiency and performance\n* The meta-signal optimizes a held-out set; robustness to validation shift/noise is not tested."}, "questions": {"value": "* Can you provide computational metrics (e.g., total tokens, RL rollouts, wall-clock time) for baseline methods to ensure a fair comparison?\n\n* How sensitive is AMFT's performance to the quality, size, and distribution of the validation set ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YgMCocgaGe", "forum": "WMEt1oVb3W", "replyto": "WMEt1oVb3W", "signatures": ["ICLR.cc/2026/Conference/Submission7418/Reviewer_2biJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7418/Reviewer_2biJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957930271, "cdate": 1761957930271, "tmdate": 1762919537432, "mdate": 1762919537432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}