{"id": "Ja0jG1Pvtg", "number": 22961, "cdate": 1758337565312, "mdate": 1759896838737, "content": {"title": "DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding", "abstract": "We study structure learning for linear Gaussian SEMs in the presence of latent confounding. Existing continuous methods excel when errors are independent, while deconfounding-first pipelines rely on pervasive factor structure or nonlinearity. We propose **DECOR**, a single likelihood-based and fully differentiable estimator that jointly learns a DAG and a correlated noise model. Our theory gives simple sufficient conditions for global parameter identifiability: if the mixed graph is bow free and the noise covariance has a uniform eigenvalue margin, then the map from $(B,\\Omega)$ to the observational covariance is injective, so both the directed structure and the noise are uniquely determined. The estimator alternates a smooth-acyclic graph update with a convex noise update and can include a light bow complementarity penalty or a post hoc reconciliation step. On synthetic benchmarks that vary confounding density, graph density, latent rank, and dimension with $n<p$, DECOR matches or outperforms strong baselines and is especially robust when confounding is non-pervasive, while remaining competitive under pervasiveness.", "tldr": "DECOR jointly learns a DAG and correlated noise in linear Gaussian SEMs with latent confounding, uses bow-free plus eigenvalue-margin conditions for identifiability, and outperforms baselines.", "keywords": ["causal discovery", "latent confounding", "deconfounding", "optimization framework", "identifiability", "Gaussian noise", "directed acyclic graphs", "structure learning", "differentiable causal discovery", "graphical models", "machine learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ca3c5aeb81644e35263d613101d15f3e52bfc85.pdf", "supplementary_material": "/attachment/e36801c14b783aa0cd2a422a8a9ec5efce7d5c54.zip"}, "replies": [{"content": {"summary": {"value": "The idea of the paper is to propose a continuous optimization-based algorithm for the identifiability of the structure and parameters of linear Gaussian models with confounders. This can be achieved under the bow-freeness assumption and the margin-eigenvalue assumption, which is theoretically demonstrated (Theorem 3.5) and can be viewed as a variation of the theory presented by Drton et al. (2011)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses two important problems, causal discovery in confounded settings, with differentiable-based approaches (a line of causal discovery methods started with NOTEARS and as of today quite established). They provide a sound rephrasing of the theory of Drton et al. (2011) to guarantee identifiability of parameters of the data-generating process, in a way that is amenable to defining a differentiable loss function."}, "weaknesses": {"value": "### Missing citations\n\n- In the nonlinear additive case, Proposition 4 in Montagna et al., 2025 adopts an assumption similar (although stronger) to the bow assumption, where they basically ask that the effect of observed parents can be decoupled from that of latent confounders. Given the similarity, I think it is worth discussing/mentioning in the related works.\n- The trek separation literature (Sullivant et al., 2008, Huang et al., 2022, Dong et al., 2024) is concerned with the same problem of linear models with latent variables, so it is worth discussing in the related works.\n- Broken citation at L53\n\n---\n### Presentation issues\n1. A main issue is that the key contribution of this work is a novel algorithm, but the authors end up having only one page to present and discuss their experiments. Most of the experimental results are confined to the appendix. This should be revised, leaving much more space to the experiments and their analysis in the main text\n2. The only plot relative to the experiments in the main text also has issues: the font size chosen by the authors make it really hard to consult.\n3. I think that some definitions are missing in the text; making them explicit would make the reading easier. For example:\n    1. What is an in-arboresence (L211)?\n    2. What is an induced subset (L204)?\n4. I got lost in many boxes that tried to explain intuitions: e.g., for the “Graphical intuition” paragraph in L203, I think it would be more effective to have a figure to visualize things. \n\n---\n### Soundness issues\n\nI think there are some soundness issues in the proposed algorithm. \n\n**What the theory gives.** Under acyclicity, bow-freeness, and a uniform eigenvalue margin on the error covariance, the covariance generated by a linear Gaussian model can be mapped to a unique pair of parameter matrices $B, \\Omega$. Thus, with population $\\Sigma$, both structure and noise are uniquely determined.\n\n**What the algorithm actually solves.** The authors first propose a bow-free constraint to restrict the search space to bow-free solutions (L310). Then, however, they say that such bow-freeness penalty makes the optimization too hard, and replace it with post-hoc enforcing of bow freeness by thresholding. My interpretation, then, is the following: without the bow-free penalty (and without an eigenvalue margin enforcement), the space of possible solutions $\\hat B, \\hat \\Omega$ compatible with the population's covariance $\\Sigma$ is not a singleton, but some equivalence class. Given that the proposed algorithm never restricts the search space with a bow-free penalty, any solution in the equivalence class should be optimal. There is no guarantee that post-hoc bow-enforcement recovers the unique solution that exists according to the identifiability results. Consequently, it is unclear how the theoretical injectivity (proved under bow-freeness + margin) controls the unconstrained optimization trajectory or the post-hoc bow-free solution.\n\n**Further issues with thresholding.** About the thresholding, I am a bit confused about how it is implemented in practice. How do the authors suggest choosing the thresholds in real-world problems, where all we have access to are the $X$ observations?\n\n---\n\n### References\n\nTrek separation for Gaussian graphical models, Sullivant et al., 2008\n\nLatent Hierarchical Causal Structure Discovery with Rank Constraints, Huang et al., 2022\n\nA Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables, Dong et al., 2024,\n\nScore matching through the roof: linear, nonlinear, and latent variables causal discovery, Montagna et al., 2025"}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1exCD8FxvA", "forum": "Ja0jG1Pvtg", "replyto": "Ja0jG1Pvtg", "signatures": ["ICLR.cc/2026/Conference/Submission22961/Reviewer_CoR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22961/Reviewer_CoR9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761214900115, "cdate": 1761214900115, "tmdate": 1762942452619, "mdate": 1762942452619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "# Summary\nThe paper proposes **DECOR**, a method for jointly learning the structure \\(B\\) and correlated noise \\(\\Omega\\) in linear–Gaussian SEMs. It combines a NOTEARS-style smooth acyclicity constraint for \\(B\\) with sparse estimation of \\(\\Omega\\) (or \\(\\Theta=\\Omega^{-1}\\)) via graphical lasso, and introduces a **bow-free** principle to avoid having both a directed edge and a bidirected (error-correlation) edge between the same pair. The authors prove identifiability under bow-free graphs and a uniform eigenvalue lower bound on \\(\\Omega\\), and present simulation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Directly addresses confounding without relying on “deconfound-then-DAG” pipelines or strong pervasiveness assumptions.\n\n2. Provides an identifiability result (injectivity of $ \\((B,\\Omega)\\mapsto\\Sigma\\)$) under interpretable structural and spectral conditions.\n\n3. Some experiments have demonstrated the effectiveness of the method to a certain extent."}, "weaknesses": {"value": "1. **No real-data results in the manuscript:** This paper claims both synthetic and real data evaluations, but shows only simulations; external validity remains unclear.\n\n2. **Evaluation of \\(\\Omega/\\Theta\\) is missing:** Reported metrics assess only \\(B\\) (nSHD/TPR/F1). Since DECOR jointly estimates \\(\\Omega\\) (equivalently \\(\\Theta=\\Omega^{-1}\\)), please add support recovery and estimation-error metrics for \\(\\Theta\\).\n\n3. **Post-hoc bow alignment introduces hyper-parameter sensitivity:** Bow-free reconciliation uses post-hoc thresholds $ \\((\\tau_B,\\tau_{\\Omega})\\)$ and and a tie-breaking parameter \\(c\\) without sensitivity analysis; performance could hinge on these choices.\n\n4. **Baselines and fairness:** Baselines omit identifiable non-Gaussian or weakly nonlinear methods in linearized regimes and do not document sparsity alignment across methods, which can bias nSHD/F1."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sgyw7gdfj0", "forum": "Ja0jG1Pvtg", "replyto": "Ja0jG1Pvtg", "signatures": ["ICLR.cc/2026/Conference/Submission22961/Reviewer_a4R4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22961/Reviewer_a4R4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790086515, "cdate": 1761790086515, "tmdate": 1762942452362, "mdate": 1762942452362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structure learning method for linear gaussian SEMs in the presence of hidden variables. The paper proves the identifiability of such structures without assuming pervasive confounding. Next, the paper proposes a continuous optimization method to estimate such structures. Finally, the paper empirically compares with other baselines on some synthetic data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proves the identifiability of Linear Gaussian SEMs without assuming pervasive confounding\n2. The assumptions are clearly stated and explained\n3. The paper proposes a scalable method using continuous optimization\n4. The paper is generally well-written"}, "weaknesses": {"value": "1. What is the motivation for studying structure learning of linear gaussian SEMs, and the motivation to study them under these assumptions in particular. Are there any real datasets or tasks where such structure learning could be useful? Several different settings and assumptions can be studied so I wonder what is the motivation for this particular setting.\n\n2. The experiments should consider cases with non-equal noise variances [1]. Prior work has shown structure learning may be possible through just looking at the marginal variances of the variables if equal noise variance is considered.\n\n3. The experiments should consider larger graphs with several hundred variables.\n\n4. The figures in the appendix show that other baselines beat or match DECOR. Why is that the case? The synthetic data in the experiments is generated according to the assumptions of this paper so I would expect DECOR to considerably outperform other baselines.\n\n5. The experiments do not consider any real dataset or task.\n\n6. Some missing references to continuous optimization methods for structure learning - [2] [3]\n\n[1]Ng, Ignavier, Biwei Huang, and Kun Zhang. \"Structure learning with continuous optimization: A sober look and beyond.\" Causal Learning and Reasoning. PMLR, 2024.\n\n[2] Bhattacharya, Rohit, et al. \"Differentiable causal discovery under unmeasured confounding.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n\n[3] Prashant, Parjanya Prajakta, et al. \"Differentiable Causal Discovery for Latent Hierarchical Causal Models.\" International Conference on Learning Representations, 2025"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Eem2omJCYd", "forum": "Ja0jG1Pvtg", "replyto": "Ja0jG1Pvtg", "signatures": ["ICLR.cc/2026/Conference/Submission22961/Reviewer_NQPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22961/Reviewer_NQPw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971904784, "cdate": 1761971904784, "tmdate": 1762942452058, "mdate": 1762942452058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DECOR, a continuous optimization framework for linear Gaussian SEMs with latent confounding. Theoretically, DECOR proves global identifiability under two simple sufficient conditions: bow-freeness and a uniform eigenvalue margin, with the map (B,Ω)→Σ being injective. Practically, the method alternates between a NOTEARS-style acyclicity-constrained update for and a convex update for the noise covariance, followed by bow-free reconciliation.\nExperiments cover synthetic regimes varying confounding density, DAG density, latent rank, and high-dimensional ratios, showing competitive or superior recovery over NOTEARS, GOLEM, GES, DeCAMFounder, and related baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In general the theory and identifiability proof is clean and optimization splits are standard. Furthermore we enumerate the following strengths:\n\n1) Simple, checkable identifiability conditions, with (B,Ω)→Σ injectivity under mild conditions. \n2) Integrates deconfounding and DAG estimation in a single likelihood, no factor pre-step.\n3) Strong gains in sparse-confounding regime; stable across wide factor loadings.\n4) Broad synthetic evaluation covering multiple confounding and dimensional regimes."}, "weaknesses": {"value": "1) No real-world evaluation: although the abstract claims “synthetic and real benchmarks,” all experiments are synthetic; no results on standard datasets (e.g., Sachs, DREAM) appear in the paper or appendix.\n2) Scope limitation: restricted to linear Gaussian SEMs; unclear how the method extends to nonlinear or heteroscedastic settings. No finite-sample error bounds.\n3) Bow enforcement is threshold-heuristic; could drop weak true edges.\n4) Misses some recent baselines that also handle confounding and bow-free graphs (Ancestral GFlowNets [1], DCD [2],  N-ADMG [3]).\n\n[1] Expert-Aided Causal Discovery of Ancestral Graphs https://arxiv.org/pdf/2309.12032\n[2] Differentiable Causal Discovery Under Unmeasured Confounding https://proceedings.mlr.press/v130/bhattacharya21a/bhattacharya21a.pdf\n[3] Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning https://arxiv.org/pdf/2303.12703"}, "questions": {"value": "1) Can the authors provide or plan real-data validation to substantiate the “real benchmark” claim?\n2) What are finite-sample consistency properties under approximate bow-freeness?\n3) Does alternating convex optimization guarantee recovery of the globally identifiable pair, or can local minima persist?\n4) How does DECOR relate to adaptive graph-sampling approaches (e.g., Ancestral GFlowNets)? Could generative amortization replace heuristic refinement?\n5) Can you provide empirical runtime vs. GOLEM or DeCAMFounder for p=100,q=10?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s6bAM40lRV", "forum": "Ja0jG1Pvtg", "replyto": "Ja0jG1Pvtg", "signatures": ["ICLR.cc/2026/Conference/Submission22961/Reviewer_1gu8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22961/Reviewer_1gu8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762471659965, "cdate": 1762471659965, "tmdate": 1762942451795, "mdate": 1762942451795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}