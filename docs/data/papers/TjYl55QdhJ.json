{"id": "TjYl55QdhJ", "number": 19028, "cdate": 1758292838214, "mdate": 1763356699820, "content": {"title": "How do Human Processes AI-generated Hallucination Contents: a Neuroimaging Study", "abstract": "Hallucinations produced by multi-modal large language models (MLLMs) pose considerable risks as it remains unclear to what extent humans can accurately recognize them.\nTo address this issue, this paper explores humans' neural responses to such hallucinated content across varying time scales.\nWe record EEG from 27 participants while they are viewing contents generated by a multi-modal large language models that either include hallucination words or not, and judge whether each description matched an image.\nThe collected EEG data is analyzed based on averaged event related potentials~(ERP) on hallucination vs non-hallucination words.\nResults suggest that multiple cognitive processes, e.g., semantic integration, inferential processing, memory retrieval, and cognitive load, are engaged during humans' recognition of hallucination content.\nHowever, when hallucinations are not recognized by human participants, the brain treats them no differently from non-hallucination content.\nThis indicates that humans already treat such hallucinations the same as non-hallucination content at a subconscious level.\nFurthermore, we conduct a prediction experiment that uses the collected EEG to detect hallucination contents. \nThis indicates that we can detect whether a user has been deceived by hallucinations generated by MLLMs with their brain activities.", "tldr": "", "keywords": ["hallucinations", "brain signals", "neuroscience", "multimodal large language model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68240fa230b136a08388f9162ad3062509ad7da8.pdf", "supplementary_material": "/attachment/32aaa22533565c796da7dd0eb3869f047498f874.pdf"}, "replies": [{"content": {"summary": {"value": "This paper investigates the neural mechanisms underlying human processing of hallucinated content about multimodal large language models. The authors conducted EEG experiments with 27 participants and analyzed event-related potentials while participants read image descriptions that either contained or did not contain hallucinated words. Additionally, the paper explores the use of EEG signals to train models for predicting whether a text contains hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces cognitive neuroscience methods into MLLM hallucination research and proposes EEG as an implicit feedback signal for hallucination detection or user trust modeling, which has potential practical value."}, "weaknesses": {"value": "Only 27 participants were included, lacking ecological validity in real-world scenarios. The model analysis is limited, and the EEG prediction experiments do not employ advanced neural network architectures. The analysis of hallucination types is insufficient; although entity, relation, and attribute hallucinations are distinguished, fine-grained comparisons are missing, such as the impact of domain knowledge on hallucinations."}, "questions": {"value": "1.Are the data general or do they include content requiring background knowledge?\n\n2.Do participants with different background knowledge show differences in responses to the same hallucinations? If so, at which stage do these differences appear?\n\n3.Do different types of hallucinations (entity/relation/attribute) show differences in ERP patterns?\n\n4.How consistent are participants’ responses, and does repeated exposure to the same stimuli affect their judgments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Gc6h2r20wA", "forum": "TjYl55QdhJ", "replyto": "TjYl55QdhJ", "signatures": ["ICLR.cc/2026/Conference/Submission19028/Reviewer_tiAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19028/Reviewer_tiAT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914763676, "cdate": 1761914763676, "tmdate": 1762931070792, "mdate": 1762931070792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the neural responses of humans to hallucinated content generated by multi-modal large language models (MLLMs). By recording EEG data from 27 participants, the study examines how human brain activity differs when processing hallucinated versus non-hallucinated content. The results suggest that multiple cognitive processes, including semantic integration, inferential processing, and memory retrieval, are involved in recognizing hallucinations. Additionally, the study demonstrates that EEG signals can predict whether content contains hallucinations, with reliable results when participants correctly identify hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The study provides valuable insights into how the human brain processes hallucinated content, contributing to a deeper understanding of human-AI interaction.\n\n2) The use of EEG data allows for the real-time detection of hallucinations, presenting a novel and non-invasive method to study the cognitive effects of AI-generated content.\n\n3) The research shows significant improvements in the model’s ability to detect hallucinations, providing potential applications for improving AI systems' reliability."}, "weaknesses": {"value": "1) The study is limited by its sample size (27 participants), which may not be representative enough to generalize findings to a broader population.\n\n2) The method relies heavily on participants' ability to accurately recognize hallucinations, which could vary significantly across individuals and affect the reliability of the results."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UmDmojER7y", "forum": "TjYl55QdhJ", "replyto": "TjYl55QdhJ", "signatures": ["ICLR.cc/2026/Conference/Submission19028/Reviewer_5ErB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19028/Reviewer_5ErB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997115407, "cdate": 1761997115407, "tmdate": 1762931070392, "mdate": 1762931070392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “How Do Humans Process AI-Generated Hallucination Contents: A Neuroimaging Study” explores how human brains process hallucinated versus non-hallucinated content produced by multimodal large language models (MLLMs). Using EEG recordings from 27 participants engaged in a multimodal image–text matching task, the authors investigate whether distinct neural patterns emerge when humans recognize or fail to recognize hallucinated content. Each trial involved viewing an image and then a sentence generated by Qwen2.5-VL-3B that either accurately described or hallucinated aspects of the image, followed by a yes/no judgment of correctness. EEG data were analyzed via event-related potentials (ERPs) at different latencies (N100, P200, N400, P600). Results show that hallucination words elicit larger ERP amplitudes—especially in N100, P200, N400, and P600—indicating heightened attention, semantic integration effort, and reanalysis processes. However, when hallucinations are not detected (“HalluWrong” cases), no ERP differences are observed, suggesting an absence of neural anomaly response. Additionally, EEG-based machine learning models (SVM, MLP, attention-based) can predict hallucination presence with high accuracy (AUC ≈ 0.94–0.98 within-subject) but only when participants consciously recognize them. The authors interpret these findings as evidence that hallucinations bypass automatic neural error monitoring when undetected, and that EEG may serve as an implicit signal for identifying deceptive AI content."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The study makes a novel interdisciplinary contribution, combining AI hallucination research with human cognitive neuroscience. It bridges the gap between computational detection of hallucinations and human neural processing, offering a fresh empirical perspective on how users engage with AI-generated misinformation."}, "weaknesses": {"value": "Despite its originality, the paper’s empirical rigor and interpretive scope have limitations. First, the sample size (n = 27) and limited number of stimuli per condition constrain statistical power, especially for high-dimensional EEG data analyzed across multiple regions and time windows. The ERP analyses rely on repeated-measures ANOVA across many electrodes and latencies but do not report corrections for multiple comparisons (e.g., Bonferroni or FDR), increasing the risk of false positives. No effect sizes or confidence intervals accompany the p-values, and claims of “significant differences” in Table 1 lack magnitude context. The behavioral results (Table 1) also show higher accuracy for hallucination trials, which is counterintuitive and not discussed—raising potential confounds related to task design or salience bias. The HalluWrong condition, key to RQ2, may suffer from low trial counts, undermining reliability of the “no effect” conclusion. The classification task lacks a clear baseline (e.g., random or non-neural features) and does not report significance testing of AUCs against chance levels. Furthermore, while the authors interpret ERP differences as reflecting “neural deception,” this framing risks overstating causal inference—absence of ERP effects could reflect signal noise or attentional variability, not necessarily cognitive “bypass.” Finally, while Section 6 acknowledges some limitations, it underplays ecological validity concerns: the controlled laboratory paradigm with static images differs from real-world interactions with AI systems."}, "questions": {"value": "Statistical robustness should be strengthened by including effect sizes (η²) and multiple-comparison correction. The paper would benefit from reporting the number of HalluWrong trials per participant and the distribution of EEG epochs per condition. Clarify whether the reported “neural silence” was confirmed through equivalence testing or absence of significant differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yF74iEPsrW", "forum": "TjYl55QdhJ", "replyto": "TjYl55QdhJ", "signatures": ["ICLR.cc/2026/Conference/Submission19028/Reviewer_hPvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19028/Reviewer_hPvT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240095686, "cdate": 1762240095686, "tmdate": 1762931069833, "mdate": 1762931069833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}