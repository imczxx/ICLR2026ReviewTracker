{"id": "f12Lo7ZUX5", "number": 22152, "cdate": 1758326813845, "mdate": 1759896883413, "content": {"title": "Reverse Distillation: Disentangling and Scaling Protein Language Model Representations", "abstract": "Unlike the foundation model scaling laws seen in natural language processing and computer vision, biological foundation models scale relatively poorly. For example, the ESM-2 family of protein language models plateaus at 650M-3B parameters on ProteinGym benchmarks. We address this limitation by introducing Reverse Distillation, a principled framework that decomposes large protein language model representations into orthogonal subspaces guided by smaller models of the same family. We hypothesize that this decomposition matches the natural hierarchy of protein properties, where broad features like secondary structure are robustly captured by compact, smaller models while the residual capacity of larger models specializes in protein-family specific functions. Our method is theoretically grounded and enables monotonic scaling---larger reverse-distilled models consistently outperform their smaller counterparts, overcoming the scaling plateau. Moreover, on ProteinGym benchmarks, reverse-distilled ESM-2 variants broadly outperform their respective baseline models at the same embedding dimensionality. Our approach offers a generalizable framework for disentangling hierarchical feature spaces in foundation model embeddings, with potential applications across biology and other domains where scaling challenges persist.", "tldr": "Protein language models plateau. Reverse Distillation decomposes them via smaller models, improving scalability.", "keywords": ["Protein language models", "model scaling", "Representation learning", "Subspace decomposition", "interpretability", "Model distillation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d810aa1e645e33e6c28754665caac692af30cc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tries to investigate why larger Protein Language Models such as ESM-2 fail to exhibit the expected scaling gains.\nThe authors attribute this to the entanglement of general and specialized representations within large models, which increases the variance of linear probes.\nTo address the above issue, this paper proposes Reverse Distillation, a linear subspace decomposition method that uses smaller models to define a general feature subspace and extracts orthogonal residuals from larger models to represent specialized knowledge.\nExtensive experiments on ProteinGym and BioMap show that RD consistently improves predictive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tries to tackle an important issue in large Protein Language Models (PLMs): the unexpected degradation of scaling behavior\n2. The proposed Reverse Distillation method is computationally lightweight and purely linear, involving only least-squares fitting and SVD decomposition\n3. Despite the method’s simplicity, RD exhibits stable and monotonic performance gains across multiple datasets and scaling levels."}, "weaknesses": {"value": "1. The paper builds its entire motivation on the “general vs. specialized representation” hypothesis but does not provide a quantitative or qualitative analysis to validate it\n2. The proposed Optimal Constrained Approximation theorem only guarantees minimal reconstruction error under a prefix constraint: a standard property of linear least squares combined with SVD. However, this result does not theoretically justify why the assumed decomposition is needed or effective.\n3. Since RD performs a chain-wise representation enhancement, a straightforward baseline naturally arises: direct representation fusion across the same model chain. It remains unclear whether RD’s improvement comes from its “distillation mechanism” or simply from aggregating multi-scale features. A comparison against naive fusion, or fusion with simple KD objectives, is essential to establish the method’s actual contribution.\n4. The experiments demonstrate improvement within the ESM-2 family, but it is uncertain whether the observed scaling restoration generalizes to other architectures."}, "questions": {"value": "All my concerns about this paper are stated in the weakness section. Please refer to the weakness section for rebuttal/discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Pci3sPTUz", "forum": "f12Lo7ZUX5", "replyto": "f12Lo7ZUX5", "signatures": ["ICLR.cc/2026/Conference/Submission22152/Reviewer_xk7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22152/Reviewer_xk7u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928345567, "cdate": 1761928345567, "tmdate": 1762942092098, "mdate": 1762942092098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the well-known \"counterintuitive scaling\" problem in Protein Language Models (PLMs) like ESM-2, where larger models often perform worse than medium-sized models on downstream benchmarks.1 The authors hypothesize this is due to \"feature entanglement,\" where larger models mix \"universal\" features (from small models) with \"specialized\" features, and this mixture acts as noise for standard linear probes.2 The authors propose \"Reverse Distillation\" (RD), a novel and elegant post-hoc framework. Instead of compressing, RD uses a smaller model's representation ($H_r$) as a basis and decomposes a larger model's representation ($H_p$) into an orthogonal combination $[H_r, H_{res}]$, where $H_{res}$ captures the new, orthogonal information from the larger model.2 The method is theoretically grounded (Theorem 1) 2 and empirically shown to restore monotonic scaling (i.e., the rd.3B model consistently beats the rd.650M model) on ProteinGym and BioMap benchmarks.2"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a critical, well-documented problem (PLM scaling failure ) with a highly novel solution. The idea of using smaller models as a basis for post-hoc orthogonal decomposition is elegant and new.\n2.  The experiments persuasively demonstrate that RD works. It not only improves baseline performance (e.g., rd.3B > 3B) but, more importantly, it restores monotonic scaling (rd.3B > rd.650M wins 96.4% of the time, vs. 53.6% for the baseline).\n3.The BioMap experiment (Table 4) provides strong evidence for the \"feature entanglement\" hypothesis. RD specifically fixes the scaling failure on \"universal\" features (like secondary structure) that the paper hypothesized were \"drowned out\" in larger models.\n4. The method is post-hoc, requiring no model retraining. The \"Chained\" version provides a practical, novel way to create Matryoshka-style nested embeddings from an existing model family."}, "weaknesses": {"value": "The paper's primary weakness is the exclusion of the ESM-2 15B model.2 The most severe example of scaling failure is the performance degradation from 3B to 15B.2 The paper only demonstrates fixing the 650M-to-3B plateau. Without testing the 15B model, the central claim of \"solving\" the scaling paradox is incomplete.\nThe core idea of using orthogonal subspaces to separate/disentangle knowledge, while novel in this application, is conceptually similar to methods in continual learning (e.g., O-LoRA), which should be cited."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SRgG85Lyek", "forum": "f12Lo7ZUX5", "replyto": "f12Lo7ZUX5", "signatures": ["ICLR.cc/2026/Conference/Submission22152/Reviewer_nDBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22152/Reviewer_nDBC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968052238, "cdate": 1761968052238, "tmdate": 1762942091873, "mdate": 1762942091873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves PLMs through a model distillation process which decomposes large models into smaller sub-models with disentangled residuals. The resulting embeddings also enjoy the Matryoshka property which allows for slices of embeddings to remain informative. These models also recover better scaling properties, allowing for more efficient parameter use. Benchmarking was done on ProteinGym and BioMap, showing good predictive performance as well as scaling with model size."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper is quite strong in my opinion. The proposed methods are a clear improvement on current approaches and is a valuable contribution to the protein representation field."}, "weaknesses": {"value": "* Additional inference time though not prohibitive could still limit adoption."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5e8djWvgT3", "forum": "f12Lo7ZUX5", "replyto": "f12Lo7ZUX5", "signatures": ["ICLR.cc/2026/Conference/Submission22152/Reviewer_akEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22152/Reviewer_akEp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970589220, "cdate": 1761970589220, "tmdate": 1762942091415, "mdate": 1762942091415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Initial Action Plan for Addressing Review Comments"}, "comment": {"value": "Thank you for your thoughtful comments and feedback. We’re delighted that you find the problem \"critical\" and \"important\" and our approach \"novel and elegant\". We are sharing an initial action plan below as we work on additional experiments and updates to the text. Please let us know if anything critical is not addressed in this plan.\n\n1. **Extension to ESM-2 15B model.** We had not been able to finish this in time due to GPU compute constraints and are actively working on this.\n2. **Feature space investigation.** We are working on a clean set of experiments to investigate if smaller models (and our prefixes) are relatively better at universal tasks and the larger models at specialized tasks.\n3. **Clarification of the theoretical grounding of our approach.** We also agree that non-linear reductions could potentially be more powerful. While a full exploration of, say, a VAE-style approach would be better suited for future work, we’re working on some exploratory experiments.\n4. **Better situating our work in the literature.** We greatly appreciate the reviewers pointing out relevant literature, including methods for continual learning. We’re studying them and will expand and extend our literature discussion.\n5. **Comparison with additional baselines.** We are working on evaluating the characteristics and performance of a simple concatenation and downprojection of embeddings from multiple models, in order to demonstrate the value of our reverse distillation and prefix approach.\n6. Time permitting, we will also explore extensions to other PLM families."}}, "id": "Th2InW8TGr", "forum": "f12Lo7ZUX5", "replyto": "f12Lo7ZUX5", "signatures": ["ICLR.cc/2026/Conference/Submission22152/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22152/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22152/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763058638976, "cdate": 1763058638976, "tmdate": 1763058638976, "mdate": 1763058638976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a significant problem in biological foundation models: they scale poorly compared to models in natural language processing. Specifically, larger Protein Language Models (PLMs) in families like ESM-2 often underperform smaller ones on key benchmarks, a phenomenon known as non-monotonic scaling. The authors hypothesize this is because small models capture \"universal\" features (like secondary structure), while larger models add \"specialized\" features (like protein-family specific functions). When these features are entangled in a single representation, the specialized features can act as noise, degrading performance on tasks that rely on universal patterns. To solve this, the paper introduces a method that decomposes large protein language model representations into orthogonal subspaces guided by smaller models. On benchmarks like ProteinGym and BioMap, the reverse-distilled ESM-2 models (e.g., rd.650M) broadly outperform their corresponding baselines (e.g., 650M)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper establishes the central problem: PLMs \"scale relatively poorly\" , with the ESM-2 family's performance plateauing. The authors' core hypothesis is highly intuitive that this is due to larger models \"entangling\" universal (low-level) and specialized (high-level) features, which increases variance.\n\n2. This work introduces high-performing and efficient embeddings. The resulting models outperform baselines at the same size. They also feature a \"Matryoshka-style\" structure, which allows smaller prefixes of a single embedding to be used as valid, lower-dimensional representations, saving computation and storage.\n\n3. The experimental design is comprehensive. The authors test their method on standard, challenging benchmarks, including ProteinGym DMS and BioMap. The inclusion of practical analyses, such as an ablation on the training data size and a measurement of inference overhead, further strengthens the work's quality."}, "weaknesses": {"value": "1. The authors should at least attempt a reverse distillation of 3B $\\rightarrow$ 15B (or the full chain up to 15B). This experiment is critical. If rd.15B outperforms rd.3B, the paper's core thesis is validated. If rd.15B still underperforms, it would suggest the scaling problem is more complex than just feature entanglement, fundamentally weakening the paper's conclusion.\n2. This linear-only approach may be restrictive. The paper itself hypothesizes that larger models encode rarer, higher-order phenomena. These complex, higher-order features may not be neatly separable from the universal features via a simple linear projection. The authors' method might only be extracting the linearly predictable component, leaving a \"residual\" that is still a mix of true novel features and non-linear transformations of universal features."}, "questions": {"value": "1. The paper hypothesizes that $H_{r}$ captures \"universal\" features and $H_{res}$ captures \"specialized\" ones. Beyond downstream task performance, did you conduct any qualitative analysis to verify this? For example, could you use feature attribution or probing to show that $H_{res}$ contains information about specific protein-family motifs or epistatic interactions that are demonstrably absent when probing $H_{r}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x1NnOBwJYE", "forum": "f12Lo7ZUX5", "replyto": "f12Lo7ZUX5", "signatures": ["ICLR.cc/2026/Conference/Submission22152/Reviewer_XboP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22152/Reviewer_XboP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986999534, "cdate": 1761986999534, "tmdate": 1762942091146, "mdate": 1762942091146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}