{"id": "jruZzZJWGt", "number": 13448, "cdate": 1758218037161, "mdate": 1759897436952, "content": {"title": "DRef: A Benchmark with Diverse Referring Expressions for Object Comprehension of Vision-Language Models", "abstract": "Referring expression comprehension (REC) tasks challenge vision-language models (VLMs) to locate specific objects within images based on natural-language descriptions, typically by generating bounding boxes or segmentation masks. Existing REC benchmarks suffer from fundamental shortcomings: (1) their limited diversity of referring expressions per object makes it impossible to distinguish whether VLMs truly understand object semantics or simply memorize specific associations; (2) the evaluation metrics do not reveal whether a VLM is robust enough to face complex and diverse referring expressions. We address these issues with a novel benchmark and two innovative metrics. Our benchmark, \\textbf{D}iverse \\textbf{Ref}erring Expressions for Object Comprehension (\\benchmark), encompasses 10,963 meticulously crafted diverse referring expressions for 824 objects spanning 187 categories. Each referred object features an average of 8.3 distinct positive expressions alongside 5.0 negative expressions for non-existent objects. To evaluate model robustness to expression diversity, we propose two complementary metrics: (1) {\\metrichard}, which necessitates successful localization across all expressions referring to the same object; and (2) {\\metricconsistency}, which quantifies how VLMs generate consistent outputs for expressions describing the same object. Our evaluation reveals that state-of-the-art models struggle with consistent object comprehension.  The best model in our assessment, Qwen2.5-VL-72B, attains merely 27.7\\% on {\\metrichard} and identifies all negative expressions for only 10.1\\% of images. {\\benchmark} can serve as a rigorous evaluation suite for assessing the robustness of REC models under diverse expressions, and hopefully encourage efforts toward increasing the reliability of REC systems in real-world applications such as robots.", "tldr": "We propose a benchmark DRef and new metrics (Hard Pass Rate, Consistency) for vision language models to evaluate their robustness and consistency when presented with multiple referring expressions of the same object.", "keywords": ["Referring Expression Comprehension; Visual Grounding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f621da5e818dd7484b668938da9536d69a1ae45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DRef, a novel benchmark for evaluating the robustness of vision-language models (VLMs) in referring expression comprehension (REC). Unlike existing benchmarks that use limited, single expressions per object, DRef provides multiple diverse expressions  for the same object, along with negative expressions for non-existent objects. To rigorously evaluate model performance under this diversity, the authors propose two new metrics: Hard Pass Rate (requiring correct localization across *all* expressions for an object) and Mean Consistency Rate (measuring the stability of predictions across expressions)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The motivation is well-argued. The paper clearly identifies a fundamental flaw in existing REC benchmarks — their lack of linguistic diversity — and proposes a direct, impactful solution with DRef and its new metrics.\n\n2.  The proposed metrics (Hard Pass Rate and Mean Consistency Rate) are insightful. They provide a much more rigorous and realistic assessment of model robustness and consistency than traditional accuracy metrics, effectively exposing the limitations of current VLMs."}, "weaknesses": {"value": "1. The claim of being the **“first systematic benchmark specifically designed to evaluate comprehensive object understanding across diverse referring expressions”** (Section 1) is potentially overstated. While DRef is highly novel, prior work like SCALAR-VG[1], FineCops-Ref [2,4], MMR [3] and  also explore multi-expression or multi-granularity reasoning, albeit not with the same focus on robustness evaluation. A more nuanced discussion of related work would strengthen the paper.\n\n2. The **dataset size is relatively small** (824 objects, 10,963 expressions). While the quality and diversity are high, the scale is significantly smaller than benchmarks like RefCOCO (50k+ annotations). This raises questions about the statistical power of the results and the generalizability of the findings to larger, more diverse datasets.\n\n3. The **analysis of model failure cases, while insightful, is qualitative**. The comparison between Qwen2.5-VL-3B and 72B (Appendix A.3) provides excellent examples but lacks deeper, quantitative analysis (e.g., attention map visualization, error type categorization) to explain *why* the larger model is more susceptible to distraction.\n\n4. The **evaluation is limited to open-source models**. The results would be more impactful if they included comparisons with leading closed-source models (e.g., GPT-4V, Gemini 1.5 Pro) to provide a more complete picture of the state of the art.\n\n\n[1] Yang, Xiaoyu, et al. \"Enhancing visual grounding and generalization: A multi-task cycle training approach for vision-language models.\" arXiv 2023.\n\n[2] Liu, Junzhuo, et al. \"FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension.\" EMNLP. 2024.\n\n[3] Jang, Donggon, et al. \"MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation.\" ICLR 2025.\n\n[4] Yang, Xuzheng, et al. \"New Dataset and Methods for Fine-Grained Compositional Referring Expression Comprehension via Specialist-MLLM Collaboration.\" IEEE TPAMI 2025."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VrR1qMS3fe", "forum": "jruZzZJWGt", "replyto": "jruZzZJWGt", "signatures": ["ICLR.cc/2026/Conference/Submission13448/Reviewer_TExx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13448/Reviewer_TExx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761333884659, "cdate": 1761333884659, "tmdate": 1762924072842, "mdate": 1762924072842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce DRef, a new Referring Expression Comprehension (REC) benchmark that contains 10963 manually annotated expressions for 824 objects (187 categories) drawn from COCO-val-2017 and OpenImages-V7-test. Two new metrics are proposed:\nHard-Pass Rate (R^Hard): an object is counted correct only if all its expressions are localized (IoU ≥ 0.5).\nMean Consistency Rate (R^C): average pair-wise IoU among predicted boxes for different expressions of the same object, weighted by mean IoU to ground-truth. Experiments on some VLMs show a large gap between the conventional “single-expression” accuracy and R^Hard, revealing that current systems are far less robust to linguistic variation than previously thought."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. REC benchmark that explicitly ties multiple linguistically diverse expressions to the same object instance and enforces hard consensus at the object level.\n2. Careful two-pass annotation protocol, ambiguity checks, hierarchical tag set (9 positive + 1 negative), public release of data + code.\n3. R^Hard and R^C quantify complementary aspects and expose failure modes that single-expression mIoU hides."}, "weaknesses": {"value": "1. The scalability of the dataset is small, e.g., 824 objects is small compared with > 50k in RefCOCO; only two image sources.\n\n2. This paper only gives a conclusion and a benchmark that existing methods are far less robust to linguistic variation than previously thought. However, no solution is presented, which limits the contribution of this paper. In addition, only the evaluation data is provided. Even though we are aware of this conclusion, what can the community do to address such a problem?"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gm5Mki8nWh", "forum": "jruZzZJWGt", "replyto": "jruZzZJWGt", "signatures": ["ICLR.cc/2026/Conference/Submission13448/Reviewer_1Wzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13448/Reviewer_1Wzo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462073853, "cdate": 1761462073853, "tmdate": 1762924072520, "mdate": 1762924072520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Referring expression comprehension (REC) is a fundamental vision-language task focused on localizing specific objects based on natural language descriptions. However, existing REC benchmarks typically provide only a single referring expression per target object, making it difficult to assess the robustness of vision–language models (VLMs) to variations in semantic descriptions. To address this limitation, this paper introduces a new benchmark, Diverse Referring Expressions for Object Comprehension (DRef), in which each object is associated with multiple distinct referring expressions—on average 8.3 positive and 5 negative expressions per object. To evaluate model robustness under linguistic diversity, the authors propose two complementary metrics: (1) Hard Pass Rate, which measures the ability to consistently localize the correct object across all valid expressions, and (2) Mean Consistency Rate, which quantifies the prediction consistency when the referring expression changes. Extensive experiments conducted on this benchmark reveal that current state-of-the-art VLMs exhibit notable limitations in handling diverse referring expressions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Traditional REC benchmarks were proposed before the advancement happened in MLLM; the expression diversity is limited overall. Besides, there is no existing benchmark trying to test whether the VLMs can successfully locate the same object regardless of the referring expressions used. This dataset positions itself well to complement the current REC benchmarks. The proposed benchmark is also of relatively high quality. 1) It contains different referring expression types such as: position/ size/attribute/ interaction and negative objects. 2)  box and human corrected mask annotation are provided to facilitate the evaluation on both the detection and segmentation sides. 3) Quality control process is implemented to reduce the ambiguity and ensure the dataset quality. \n\nBased on the new proposal evaluation sets, this paper also proposes two complementary metrics, hard pass rate and mean consistency rate. Both metrics aim to quantify the robustness of the VLM to different referring expressions from different perspectives; they reflect an interesting new assessment of the model capacity.  Comprehensive benchmarking is conducted on different models."}, "weaknesses": {"value": "I appreciate the group-wise performance analysis based on the DRef tag space shown in Figure 6. The results clearly indicate that larger models exhibit improved robustness to variations in referring expressions. However, since only the top 10 most frequent tag combinations are presented, it would be informative to also identify which combinations lead to the most significant failures across models. For instance, Qwen2.5-VL-72B achieves a P50 of 77.4, yet its R50-HARD drops to roughly 27.7, suggesting that certain linguistic patterns or tag combinations pose substantial difficulties even for the strongest systems. Additional analysis on these failure modes would strengthen the insights.\n\nIt would also be valuable to examine whether robustness to expression diversity correlates with object-specific factors, such as object size, category, or higher-level semantic grouping. Such analysis could help clarify whether failures arise primarily from language ambiguity or from visual complexity.\n\nThe manuscript states that the benchmark contains 824 objects, and the appendix notes 824 images, implying that each image contains exactly one annotated object. If this is indeed the intended dataset design, it would be helpful to emphasize this point clearly in the main text, as readers might otherwise question. Furthermore, the authors may wish to discuss how the evaluation framework generalizes to scenarios involving multiple target objects within the same scene, and whether current models show differing behavior when handling single-target versus multi-target referring expressions."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HlNiRgPk5u", "forum": "jruZzZJWGt", "replyto": "jruZzZJWGt", "signatures": ["ICLR.cc/2026/Conference/Submission13448/Reviewer_9YEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13448/Reviewer_9YEC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631749343, "cdate": 1761631749343, "tmdate": 1762924071648, "mdate": 1762924071648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DRef, a referring expression benchmark that evaluates models at the object level rather than per-expression. Each object is annotated with many positive and negative expressions, and the paper proposes strict metrics (Hard Pass Rate, Mean Consistency Rate) to test whether a model can localize the same object under diverse linguistic descriptions. Experiments on strong VLMs show that current models look good on single-expression REC but collapse when forced to be consistent across all expressions for an object."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is more varied than standard REC sets in that each object is associated with several types of referring signals (location, relative, attribute, interaction, negatives), so it can surface some failure modes that single-expression datasets miss.\n2. The writing and organization are clear and easy to follow."}, "weaknesses": {"value": "1. The experimental coverage is limited: the current comparisons do not include representative LLaVA series models, frontier models (e.g., GPT-4o, Gemini) , or other grounding models (e.g., GLAMM).\n2. Prompt diversity is partly curator-driven and may be somewhat templated; it is unclear whether performance would hold under paraphrased, reordered, or more colloquial expressions.\n3. While the paper diagnoses a gap in current REC evaluation, it does not propose a method aimed at improving the newly introduced metrics. The paper would be stronger with a proposed mitigation method to show that the proposed benchmark is not only harder but also improvable.\n4. The benchmark can be viewed as a strong extension that emphasizes per-object multi-expression consistency, rather than a fundamentally new evaluation paradigm."}, "questions": {"value": "1. Have you tested paraphrased, reordered, or more colloquial variants of the same expressions to see whether Hard Pass and consistency remain stable?\n2. Since you have identified “same object, many expressions” as the key failure, have you tested any method that can actually improves Hard Pass?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "te1NP6W9v8", "forum": "jruZzZJWGt", "replyto": "jruZzZJWGt", "signatures": ["ICLR.cc/2026/Conference/Submission13448/Reviewer_xEkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13448/Reviewer_xEkX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889769772, "cdate": 1761889769772, "tmdate": 1762924070681, "mdate": 1762924070681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}