{"id": "PxYlYQ6orQ", "number": 21477, "cdate": 1758318005778, "mdate": 1759896919814, "content": {"title": "Nested Text Labelling Structures to Organize Knowledge in AI Applications for the Humanities and Social Sciences", "abstract": "In the humanities and social sciences, recent research indicates that prevailing text annotation models do not always effectively and fully convey the nuances of expert knowledge, which in turn hinders the advanced application of AI. The paper aims at identifying the most suitable data models for both human annotators and machine learning processes, and the key issue is to deal with the trade-off between convenience and expressiveness of the annotation model. Experience gained through a number of applied projects and research studies has shown that there cannot be a single simple answer; this is why we propose a multi-level approach to data models used for text annotation. This article delineates its conceptual and logical foundations, alongside associated tasks. The proposed framework comprises three nested data models, each distinguished by its level of complexity. Based on a relational representation of textual annotations, this framework offers the flexibility required for a variety of annotation scenarios. It supports named entity recognition, relation extraction, semantic analysis, co-reference resolution, frame semantics, multi-span matching, etc. --- at least 17 types of tasks whose inputs and outputs have fundamentally different structural complexity. The framework includes a core model, an extended set of entities, and their relations. The same dataset can be related to various tasks, even tasks of significantly different types. The framework is capable of handling multiple annotations, multi-span elements, optional tags, and contextual metadata. The broad applicability of our framework is supported by the survey of 21 datasets and related tasks found in more than a thousand publications. The proposed approach broadens the horizons of structured text annotation, promoting the standardization of content analysis methodologies and enabling solutions to a diverse range of natural language processing tasks.", "tldr": "In the humanities and social sciences, there cannot be a single simple answer, which text annotation model is always the best, so we propose a general purpose multilevel system of data models for text annotation.", "keywords": ["Generalized text markup", "Natural Language Processing", "Text annotation", "Data collection", "Automated content analysis", "Multi-fragments", "Multiple assessors", "Machine learning", "Artificial intelligence", "Language modeling", "Humanities knowledge", "Semantic analysis", "Named entity recognition", "Relation extraction", "Co-reference resolution"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f793e6782dac01263f79303ccbf75c6ef15db855.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper introduces a unified data model/schema for text annotations \n- The schema consists of three levels \n    - Span Level: Highlight text fragments and tag them (eg. NER, POS, etc) \n    - Element Level: Group multiple related spans into elements that represent relationships or structures (more complex tasks like Relation Extraction (RE), Coreference Resolution, and Frame Extraction) \n    - Extended level: Add metadata like who annotated it, comments explaining decisions, and context notes on spans/elements (eg. context-dependent annotation such as human values dataset) \n\n- Validation of the schema  \n    - The authors validate their framework's usefulness and broad applicability by demonstrating it can successfully model and categorize a wide range of existing, real-world annotation tasks and datasets. \n    - 21 diverse datasets (including CONLL 2012, ADE, and RuSentNE) were analyzed by reviewing \"over a thousand\" associated publications found via the Semantic Scholar API. \n    - Used LLM to extract what tasks each paper addresses (Identified 17 distinct task types) \n    - Map all these combinations of datasets and tasks to their proposed 3-level hierarchy (Table 1 of the paper)"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper's main contribution is a new hierarchical framework to standardize text annotation (more details in the summary above)  \n- The paper claims to solve the \"fundamental trade-off\" that is an existing  problem text annotation acorss disciplines. The nested levels allow an expert to choose the simplest model possible for their task, while still providing the power to handle complex nuances (like multi-span elements or contextual comments) when needed. \n- The paper provides complete formal specifications: ER diagrams, relational schemas, entity definitions which also handles edge cases such as multi-annotator, comments, context, etc."}, "weaknesses": {"value": "- The paper claims that the current models fail to capture \"nuances\" but doesn’t really discuss what specific nuances are lost? They just assert this without proof. And the entire premise of the paper is based on existence of the problem, without a sufficient evidence that the problem exits \n- There are claims that the schema is an attempt to solve \"convenience vs expressiveness\" trade-off but the paper never measures or provide evdience for how would it lead to better convenience (eg. user studies) or expressiveness (eg. ML experiments). In short there is no validation that the current schema would be useful. \n- The mapping of 21 datasets to the schema, shows that datasets can be mapped to the schema. The pipelines used LLM-as-judge to annotate the type of tasks in the paper, however, there is no validation that the LLM-as-judge actually works or any robustness checks, making the pipeline and subsequent results unreliable."}, "questions": {"value": "- I am unsure about the who exactly does the schema help. Is it for the annotators to better understand the LLM generated annotations or it for better annotations using LLMs?   \n- Why is a unified schema better (or more precisely why is having 3 levels better than having 1 flexible schema)? Different tasks might genuinely need different representations. Forcing everything into one framework could make things worse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTsE9z4EZF", "forum": "PxYlYQ6orQ", "replyto": "PxYlYQ6orQ", "signatures": ["ICLR.cc/2026/Conference/Submission21477/Reviewer_5jBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21477/Reviewer_5jBS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814012440, "cdate": 1761814012440, "tmdate": 1762941798088, "mdate": 1762941798088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for performing knowledge management extracted from the textual information via annotation models. These can be extracted with the help of the tasks of entity and relation extraction."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Since the motivation and research questions are not described, it is hard to see the strengths of the paper."}, "weaknesses": {"value": "--> There is no mention of humanities and social sciences in the rest of the paper other than the title.\n--> The introduction is missing references to support the claims.\n--> The exact research questions that the authors want to target are missing.\n--> There is no related work.\n--> After the generalized logical or relational model that is created with the NER and Relation extraction tasks, how is that useful in any of the downstream tasks?\n--> The paper is missing the evaluation of the proposed knowledge management framework.\n--> What is the broad impact of this work?\n\nThe paper is still in a premature stage and the contributions are not very clear."}, "questions": {"value": "--> What are the research questions that authors are targeting?\n--> After the generalized logical or relational model that is created with the NER and Relation extraction tasks, how is that useful in any of the downstream tasks?\n--> How this framework can be evaluated?\n--> What is the broad impact of this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "viEwxhE4M1", "forum": "PxYlYQ6orQ", "replyto": "PxYlYQ6orQ", "signatures": ["ICLR.cc/2026/Conference/Submission21477/Reviewer_cmqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21477/Reviewer_cmqx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090148636, "cdate": 1762090148636, "tmdate": 1762941797827, "mdate": 1762941797827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical data model with three levels of increasing complexity. The motivation is that existing text annotation models often fail to capture fine-grained expert knowledge, and there is a need to balance convenience for annotators with the expressive power of the annotation representation. The authors develop a semi-automated literature review pipeline by querying the Semantic Scholar API and extracting relevant task descriptions using a large language model. Through this process, they analyze 21 datasets and identify 17 distinct types of text-labeling tasks. Based on these findings, they design a nested text-labeling framework that organizes these tasks according to their structural complexity. The authors highlight the adaptability of the framework to datasets with varying levels of complexity, as well as its practical convenience for human annotators. Finally, they demonstrate how the framework can be implemented as relational database structures and propose a corresponding relational schema."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation for using a hierarchical model that introduces additional complexity only when necessary\n2. Good coverage of identified annotation task types across multiple datasets.\n3. Potential for unifying datasets and standardizing annotation practices\n4. Helpful explanation of functional dependencies and how the proposed framework maps to relational database schemas"}, "weaknesses": {"value": "1. **Missing references:** I am afraid but there are very few references provided. For example:\n   - Lines 43–44: \"In the humanities and social sciences, recent studies suggest that current text annotation models often fail to encapsulate the full nuance of expert knowledge, thereby limiting their utility for advanced AI applications.\" \n     This is your main motivation for proposing new data models, but no sources are cited to support this claim.\n   - Line 223: \"The data model described in this subsection is a traditional one and can be found in many different datasets.\"\n     Please cite which datasets and sources.\n   - Lines 80–81: \"Existing methods proved inadequate, primarily due to limitations in scalability and coverage.\"\n     Which methods?\n\n2. **Missing related work section:** You do not have a related work section. You do position the contributions in the context of prior literature. What is the closest related work? How do other approaches model annotation structures? You rarely mention related work and do not contrast your work with existing models.\n\n3. **Clearly state contributions:** I suggest including a dedicated paragraph that explicitly lists the paper’s contributions. For example, is the semi-automated literature review pipeline intended to be a main contribution as well?\n\n4. **Significance of contributions:** Beyond the general motivation for improved annotation models, several contribution claims need to be moderated or substantiated:\n   1. Lines 65–68 \"The employment of the proposed model hierarchy during development, coupled with training on diverse task types, is posited to yield a new generation of machine learning models. These models would be characterised by greater usage simplicity and a broader range of application.\" This is a very strong statement. For a your proposed work of better organising annotation structures this claim seems a bit far-fetched.\n   2. You emphasize convenience for annotators, but no empirical evaluation of annotation usability or efficiency is provided. When motivating your work with that and highlighting this in the conclusion section, please ensure you can substantiate that.\n   3. You state that the structure “facilitates the selection of an appropriate AI model for any specific dataset” (lines 142–143). This may be true in principle, but for ICLR, this contribution is relatively limited unless comparative evidence is provided showing measurable improvements over existing data models.\n\n5. **Validation:** The framework is derived from 17 identified tasks. You then conclude (lines 318–319) that the framework is widely applicable. To support this claim, I would argue to validate the system on tasks not used in the design process.\n\n6. **Limitations:** You do not clearly articulate the paper's limitations. Which types of tasks or annotation scenarios are not well supported by the framework, and why? I would expect a dedicated section or paragrpah listing limitations.\n\n7. **Semi-automated pipeline:** The pipeline uses an LLM, but the specific model is not identified. How do you ensure correct extraction? Was the output manually verified? This is particularly important since large models are known to struggle with “needle in the haystack” retrieval tasks (see: https://research.trychroma.com/context-rot).\n\n8. **Vague**: Lines 18-19: \"Experience gained through a number of applied projects and research studies has shown that there cannot be a single simple answer; [...]\" That is very vague. Are these your own or do you draw this from the literature.\n\n9. **Typos**: \n\t- \"Analisys\" in Figure 1\n\t- \"can label o and more\" in line 295"}, "questions": {"value": "- Extended Level is only covered by three publications. This seems quite low compared to the other levels?\n- Line 470: \"A collection of tasks [...]\" this refers to the 17 distinct tasks?\n- Where exactly do you refer to the AI applications in the Humanities and Social Sciences in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W3Qt0y5hG4", "forum": "PxYlYQ6orQ", "replyto": "PxYlYQ6orQ", "signatures": ["ICLR.cc/2026/Conference/Submission21477/Reviewer_owiv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21477/Reviewer_owiv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762511475097, "cdate": 1762511475097, "tmdate": 1762941797289, "mdate": 1762941797289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}