{"id": "uJgfj5EJ2W", "number": 20915, "cdate": 1758311648221, "mdate": 1763686741635, "content": {"title": "MultiBreak: A Scalable and Diverse Multi-turn Jailbreak Benchmark for Stress-testing LLM Safety", "abstract": "We present MultiBreak, a scalable and diverse multi-turn jailbreak benchmark to stress-test large language model (LLM) safety. Multi-turn jailbreaks mimic natural conversational settings, making them easier to bypass safety-aligned LLM than single-turn jailbreaks. Existing multi-turn benchmarks are limited in size or rely heavily on templates, which restrict their diversity and realism. To address this gap, we unify a wide range of harmful jailbreak intents, and introduce an active learning pipeline for expanding high-quality multi-turn adversarial prompts. In this pipeline, a jailbreak attack generator is iteratively fine-tuned to produce stronger attack candidates, guided by uncertainty-based refinement. Our MultiBreak includes 7,152 multi-turn adversarial prompts, spans 1,724 distinct harmful intents, and covers the most diverse set of topics to date. Empirical evaluation shows that our benchmark achieves up to a 54.1% and 30.8% higher attack success rate (ASR) than the second-best dataset on DeepSeek-R1-7B and GPT-4.1-mini, respectively. More importantly, stress-testing reveals that LLMs resist overt harms (e.g., harassment) more effectively than subtle harms (e.g., high-stakes advice), yet remain highly vulnerable to framing-based attacks. These findings highlight persistent vulnerabilities of LLMs under realistic adversarial settings and establish MultiBreak as a scalable resource for advancing LLM safety research.", "tldr": "We introduce MultiBreak, a scalable and diverse multi-turn jailbreak benchmark, built with active learning to stress-test LLM safety.", "keywords": ["jailbreak", "safety", "large language models", "LLM", "multi-turn", "benchmark", "dataset", "robustness", "vulnerability"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5adb9a801d8aa4b7b7306b5b05090f8126f2a1f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MultiBreak, a benchmark for multi-turn jailbreaking attacks against LLMs. It first merges and cleans existing single- and multi-turn jailbreak datasets, to obtain a large and diverse pool of malicious intents and prompts. Then, via an active learning framework, fine-tunes an LLM to generate a large number of multi-turn conversation for jailbreaking the victim models. Finally, another LLM is used to refine the adversarial prompts to increase effectiveness. In the evaluation on open- and close-source LLMs, MultiBreak achieves higher success rate than existing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework with the iterative addition of adversarial prompts and fine-tuning, followed by uncertainty-guided refinement, is reasonable and simple.\n\n- The resulting adversarial prompts seem effective than the baselines according to the results in Table 2."}, "weaknesses": {"value": "- The attack success rate is very different when evaluated with the two judges, and the difference is in opposite directions across models (i.e., it's not just that one judge is consistently more conservative). Since no evaluation of their alignment with human judgement is provided, it's unclear which one, if any, can be trusted.\n\n- The reported success rate for different benchmarks is not obvious to compare: besides the issue with the automated judges (see above), different benchmarks have different unsafe goals (intents), so it's not clear that they're equally difficult. In general, I think there's a bit of confusion between the attack goals (what's the unsafe or harmful behavior the attacker wants to obtain) and method (in this case, how to generate the multi-turn prompts). The goals should stay fixed, and then different methods to achieve such goals could be compared. However, the paper collects both from previous benchmarks, and adds new ones. I think the proposed approach to generate multi-turn jailbreaks is promising, but should be compared to other methods on the same set of goals.\n\n- Most of the conclusions in Sec. 4.4 are either known or expected."}, "questions": {"value": "- The set of adversarial prompts seems fixed, and then, even if a model is robust to those, other reformulations could still work. Can the LLM-based generator (and rewriter) be used to generate new prompts in an on-line fashion, in case the fixed set is not effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s3VXiNdCa2", "forum": "uJgfj5EJ2W", "replyto": "uJgfj5EJ2W", "signatures": ["ICLR.cc/2026/Conference/Submission20915/Reviewer_hddK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20915/Reviewer_hddK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752259868, "cdate": 1761752259868, "tmdate": 1762938652025, "mdate": 1762938652025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly proposes a new multi-turn jailbreak dataset called MultiBreak, which covers a larger sample size and a broader range of malicious intent. To ensure the high-quality of the data sample and increase the data size, the authors first collect and filter exsiting multi-turn and single-turn malicious samples, then use them to train specific generators to generate multi-turn jailbreak sampels. There are also three concerns to the generated data: the attack success rate, the faithfulness and uncertainty, which combine to ensure the high-quality of the generated data. The final dataset is used to the stress-testing of LLMs and once again demonstrates the insufficient defensive capabilities of LLMs against multi-turn dialogue attacks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The samples in the dataset are carefully examined. For collected data, deduplication is performed based on semantic similarity, and filtering is conducted based on ASR. For generated data, filtering is performed based on ASR, intent consistency, and uncertainty (cross-model attack performance).\n2. The ensemble approach is employed for ASR evaluation and multi-round jailbreak sample generation, which may increase the final quality of the data sample.\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "**1. There seems to be little novel scientific insight via the MultiBreak:**\n\nAlthough the process of creating the dataset is highly rigorous, there seems to be little new finding enabled by MultiBreak. For example, the main claim in contribution 3 that 'LLMs resist overt harms but remain weak to subtle harms' has already been widely known. The conclusion and findings via MultiBreak are more like summaries of the known vulnerabilities in LLM safety.\n\n**2. Lack of countermeasures to address the long-tail effect in the dataset:**\nThe lack of countermeasures to address the long-tail effect in the dataset may cause generators to overfit specific jailbreak attack topics during fine-tuning. This leads to generators gradually becoming capable of generating high-quality jailbreak attack prompts only for specific jailbreak topics. Combined with the existence of filtering mechanisms, this imbalance in sample quantities across different jailbreak topics may be further exacerbated. Therefore, the scale-up process may ultimately undermine the utility of the dataset."}, "questions": {"value": "See the weakness above and:\n1. This work is more like an **Engineering** work, but not **Research**. So I am uncertain whether it is suitable for publication as a research paper."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This paper constructs a new multi-turn jailbreak dataset but has Ethics Statement section."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GoW5LQhvf6", "forum": "uJgfj5EJ2W", "replyto": "uJgfj5EJ2W", "signatures": ["ICLR.cc/2026/Conference/Submission20915/Reviewer_qNt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20915/Reviewer_qNt2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841086873, "cdate": 1761841086873, "tmdate": 1762938632612, "mdate": 1762938632612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows a benchmark and dataset for multi-turn jailbreak attacks, by unifying harmful intents and introducing an active learning pipeline for better adversarial prompts. Jailbreak generator is iteratively trained for stronger attacks guided by uncertainty. 7k prompts with 1.7k intents are proposed. Experiments show the results under the current multiple LLMs with the findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The dataset's 26 categories are comprehensive and fine-grained, spanning 9 courses. The data generation pipeline is clear and makes sense to me, which is convincing of its high quality. Most of the experiment analysis is impressive and comprehensive."}, "weaknesses": {"value": "- During the iterative attack generator finetuning, once the new adversarial prompt is generated, it is unclear how the duplication can be checked again and if the diversity will be reduced.\n\n- Experiments should also include some existing multi-turn defense methods [1,2] or naive safety post-training methods as baselines to show that the new benchmark is challenging enough for current defense methods.\n\n- The experimental setup is not detailed, e.g., it is unclear what the temperature is for the @1 and @5 calculations, since it affects the number of trials significantly.\n\nReferences:\n\n[1] Hu et al. Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks, 2025\n\n[2] Lu et al. X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability, 2025"}, "questions": {"value": "See above. I would be happy to raise my rating if all my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YbUy4Kcihk", "forum": "uJgfj5EJ2W", "replyto": "uJgfj5EJ2W", "signatures": ["ICLR.cc/2026/Conference/Submission20915/Reviewer_Lngo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20915/Reviewer_Lngo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973898592, "cdate": 1761973898592, "tmdate": 1762938595758, "mdate": 1762938595758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MultiBreak, a multi-turn jailbreak benchmark containing 7,152 adversarial prompts across 1,724 unique harmful intents—significantly larger and more diverse than existing datasets. The authors employ an active learning pipeline that iteratively fine-tunes attack generators and applies uncertainty-guided rewriting to scale up high-quality adversarial examples. MultiBreak achieves substantially higher attack success rates than prior benchmarks (up to 54.1% improvement on DeepSeek-R1-7B), and reveals that LLMs are more vulnerable to subtle harms (e.g., unsafe medical advice) than overt harms (e.g., hate speech), with framing-based attacks proving especially effective at bypassing safety guardrails."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "With 7,152 prompts spanning 1,724 unique intents across 26 fine-grained categories, MultiBreak is significantly larger and more diverse  than prior multi-turn jailbreak benchmarks. \n\nFine-grained analysis reveals critical vulnerabilities—subtle harms (unsafe medical advice) are easier to bypass than overt harms (hate speech), framing-based attacks are most effective, and conversation strategy matters more than length."}, "weaknesses": {"value": "The generator pre-scripts all conversation turns (Turn 1, 2, 3, etc.) upfront without seeing actual victim responses, so Turn 2 cannot reference what the victim (which may vary) actually said in Response 1, breaking natural conversational flow.\n\nFor a safety benchmark, lack of human judgment on whether attacks are genuinely harmful (vs. triggering false positives) is a critical gap. \n\nSome of the previous multi-turn jailbreaking methods, such as X-Teaming (https://arxiv.org/abs/2504.13203\n) and ActorAttack (https://arxiv.org/abs/2410.10700\n), extend single-turn benchmarks (like HarmBench) into multi-turn. However, discussion/comparison with such methods is missing."}, "questions": {"value": "Did the authors analyze whether their pre-scripted multi-turn attacks maintain effectiveness and contextual coherence when evaluated on completely different model families (e.g., Claude/Grok series) beyond the tested victims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XZOycHs2JM", "forum": "uJgfj5EJ2W", "replyto": "uJgfj5EJ2W", "signatures": ["ICLR.cc/2026/Conference/Submission20915/Reviewer_BaJ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20915/Reviewer_BaJ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982209380, "cdate": 1761982209380, "tmdate": 1762938543370, "mdate": 1762938543370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive and thoughtful feedback. We especially appreciate *BaJ7, qNt2, and Lngo*’s **acknowledgment of the quality** of our benchmark, as well as *hddK and qNt2*’s comments on the **effectiveness of our pipeline**. We are also grateful for the high-quality comments from each reviewer, which helped us refine the paper and make it more comprehensive.\n\nWe have updated our submission (revised text shown in blue) to incorporate the following improvements: \n- clarification of the victim model temperature setup in Section 4.1\n- human judgement results in Appendix A.5\n- explanation of duplication handling in Section 3.3.2\n- expanded analysis of refusal patterns in Appendix A.4.1\n\nWhile we have addressed each concern in detail in the individual responses, we would like to offer one *general clarification*. Unless otherwise stated, we use gpt-4.1-mini (closed-source representative) and llama-3.1-8b-instruct (open-source representative) as our victim models in all attack and defense experiments. This ensures that the **results remain consistent with the main comparisons** presented in Table 2 of our paper."}}, "id": "4wAgPmWNcH", "forum": "uJgfj5EJ2W", "replyto": "uJgfj5EJ2W", "signatures": ["ICLR.cc/2026/Conference/Submission20915/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20915/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20915/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763686681257, "cdate": 1763686681257, "tmdate": 1763686681257, "mdate": 1763686681257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}