{"id": "dHz2LBCyTh", "number": 16110, "cdate": 1758260138575, "mdate": 1759897261394, "content": {"title": "Batch and Sequential Unlearning for Neural Networks", "abstract": "With the increasing deployment of machine learning models trained on personal data, machine unlearning has become crucial for data owners to exercise their \"right to be forgotten\" and protect their privacy. While model owners can retrain the models without the erased data to achieve this goal, this process is often prohibitively expensive. Previous works have shown that Newton's method can be applied to linear models to unlearn multiple data points in batch (batch unlearning) with minimal iterations. However, adapting this method to non-linear models, such as neural networks, poses significant challenges due to the presence of degenerate Hessians. This problem becomes more pronounced when unlearning is performed sequentially (sequential unlearning). Existing techniques that tried to tackle this degeneracy often 1) incur unlearning updates with excessively large norm that yield unsatisfactory unlearning performance and 2) may require manual tuning of regularization hyperparameters. In this work, we propose new unlearning algorithms that leverage cubic regularization for Newton's method to address both challenges. We discuss the theoretical benefits of our method and empirically show that our algorithms can efficiently achieve competitive performance in both batch and sequential unlearning on real-world datasets.", "tldr": "", "keywords": ["machine unlearning", "second-order unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bb28d224f6f971b9c0fe6710f10da8cd14e4bf2.pdf", "supplementary_material": "/attachment/193519b9e77bdbd9b8a7171d087e31cbbfb8e651.zip"}, "replies": [{"content": {"summary": {"value": "The paper identifies a key limitation in second-order unlearning methods like damped Newton, where Hessian degeneracy near local optima causes unstable and excessively large parameter updates that harm performance. To address this, the authors propose Cubic-Regularized Newton Unlearning (CuReNU) and its scalable variant StoCuReNU, which apply cubic regularization to automatically control the Hessian damping factor. This approach mitigates degeneracy, stabilizes update norms, and provides theoretical convergence guarantees to second-order stationary points."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Unlearning is an important topic and the authors clearly explain and evaluate their proposal."}, "weaknesses": {"value": "The evaluation is limited to small models or LoRA ona small LLM. Thus it is unclear how this will scale to large models where retraining is not realistic, and approximate unlearning is needed. Some parts of the evaluation technique are unclear."}, "questions": {"value": "I would have appreciated some analysis of the results in Figure 2. In particular, the experiment for unlearning on Llama-2 still has 40\\% accuracy $D_e$ regardless of whether StoCuReNU or retraining is used, which suggests that the samples being targeted for unlearning are not unique within the training set. As a result, it is not clear if the experiments can be conclusive in this case. \n\nI was also confused on how the authors obtained a retraining time measurement for Llama2-7B. If I understand this should be the time to retrain Llama2-7B from scratch minus the data in the dataset they want forgotten. However, it's more likely this is the time to fine-tune Llama2-7B on the dataset. Thus, it seems misleading to call this the retraining time for Llama2-7B since that is not how a fully exact retraining method would work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kj2RFLwuVP", "forum": "dHz2LBCyTh", "replyto": "dHz2LBCyTh", "signatures": ["ICLR.cc/2026/Conference/Submission16110/Reviewer_wc7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16110/Reviewer_wc7S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759159147, "cdate": 1761759159147, "tmdate": 1762926287451, "mdate": 1762926287451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates Newton-like unlearning algorithms to address the Hessian degeneracy challenge. It proposes two methods, CuReNU and StoCuReNU, based on cubic-regularized optimization and analyzes the convergence guarantee. StoCuReNU achieves performance comparable to state-of-the-art empirical unlearning methods across diverse settings. The authors demonstrate that StoCuReNU is scalable with comparable unlearning performance for various settings,including batchand sequential unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper investigates Newton-like unlearning algorithms to address the Hessian degeneracy challenge. It proposes two methods, CuReNU and StoCuReNU, based on cubic-regularized optimization. StoCuReNU achieves performance comparable to state-of-the-art empirical unlearning methods across diverse settings. Theoretic analysis of convergence guarantees seem solid."}, "weaknesses": {"value": "1. This is not the first work to propose second-order unlearning for neural networks, so the contributions appear limited. For example, Qiao et al. (2025) and Zhang et al. (2024b) also introduce second-order unlearning algorithms for non-convex objectives. The paper therefore needs stronger motivation and a clearer articulation of its novel contributions.\n\n2. The authors compare against only a subset of existing unlearning algorithms; for instance, Qiao et al. (2025) is not included. Moreover, in the experimental evaluations, the proposed method does not perform noticeably better than the baselines, which further limits the contributions of this work.\n\n3. The experiments use only five unlearning rounds, averaged over three random runs, which seems quite limited.\n\n4. Accuracy is not a very convincing metric to evaluate the unlearning performance, Other common evaluation methods, such as membership inference attacks (MIA), are not considered."}, "questions": {"value": "1. Since this is not the first work to propose second-order unlearning for neural networks, the authors should restate the central question more clearly: How can we unlearn neural networks effectively using second-order methods?\n\n2. StoCuReNU claims smaller space complexity than Qiao et al. (2025); how does its time complexity compare?\n\n3. Can lower-complexity approaches be used to approximate or invert the Hessian in CuReNU, such as Hessian‚Äìvector products or related techniques?\n\n4. Line 375: ‚ÄúTug-of-War (ToW) score that aggregates these gaps (smaller is better)‚Äù ‚Äî did you mean ‚Äúlarger is better‚Äù?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "75pSwlKN19", "forum": "dHz2LBCyTh", "replyto": "dHz2LBCyTh", "signatures": ["ICLR.cc/2026/Conference/Submission16110/Reviewer_znzs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16110/Reviewer_znzs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813473230, "cdate": 1761813473230, "tmdate": 1762926287026, "mdate": 1762926287026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the machine unlearning problem and proposes cubic regularized method to handle the potential problem of Hessian degeneracy. The proposed method CuReNU introduces damping to stabilize the Newton's method and have a systematic way to avoid stability issues. The authors further propose a stochastic version StoCuReNU method to bypass the $O(d^3)$ involved in matrix inversion using efficient Hessian Vector Product computations. The numerical experiments also supports the method favorably."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Machine unlearning is a very timely topic and Hessian degeneracy can be serious issue for computation stability. The adaptation of tools from classical nonconvex optimization (cubic regularization) into ML context is very well motivated. The usage of second order information is something the community has often overlooked and it is great to see bring brought back to the stage. The paper is technically sound, clearly presented."}, "weaknesses": {"value": "The CuReNU and StoCuReNU are both adapted from existing algorithm in different setting and hence the convergene and theoretical guarantees are inherited. The authors say that \" this adaptation is both necessary and non-trivial to address failure modes\". Please clarify further what exactly had to be modified.\n\nWhile the theory shows favorable memory usage, the empirical results in Table 4 show STOCURENU's practical peak memory can be higher than its baselines. Is this due to a large constant factor (e.g., loading the base model, LoRA adapters, and HVP buffers) 1 and that the $\\mathcal{O}(2d)$ benefit is about asymptotic scaling as the dataset size $n$ grows? Is the problem too small scale for the asymptotic to kick in?\n\nIn Appendix G, the authors test on overfitted models and note that while STOCURENU is effective, a performance gap remains in Membership Inference Attack (MIA) mitigation compared to the SOTA empirical method, SCRUB. Please discuss this further. Is this gap expected for other scenarios as well or in general there is a gap?"}, "questions": {"value": "In Appendix J.3.1 the authors  show that $T$ represents a direct trade-off between computational efficiency and unlearning effectiveness. Could you please provide a principled heuristic in selecting $T$? Or maybe an early stopping criterion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S423Jfp9xn", "forum": "dHz2LBCyTh", "replyto": "dHz2LBCyTh", "signatures": ["ICLR.cc/2026/Conference/Submission16110/Reviewer_9cot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16110/Reviewer_9cot"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951715207, "cdate": 1761951715207, "tmdate": 1762926286171, "mdate": 1762926286171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the challenge of machine unlearning i.e. removing the influence of specific training data from a trained neural network without retraining from scratch. The authors focus on improving second-order unlearning methods such as Newton Unlearning, which rely on Hessian information to approximate retraining. They identify a critical limitation: Hessian degeneracy (presence of many small, zero or negative eigenvalues) in trained neural networks, which leads to unstable or divergent updates during unlearning. To overcome this, the paper proposes two novel algorithms: CuReNU (Cubic-Regularized Newton‚Äôs Unlearning) ‚Äì uses cubic regularization to automatically determine an appropriate damping factor, thereby stabilizing updates and it's stochastic variant StoCuReNU (Stochastic CuReNU) ‚Äì a scalable, Hessian-free variant using Hessian-vector products (HVPs). Empirically, on FashionMNIST, CIFAR-10, AG-News, and TOFU datasets, CuReNU and StoCuReNU achieve competitive unlearning performance compared to state-of-the-art empirical methods (e.g., SCRUB, DELETE) in both batch and sequential unlearning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well-written, including notational consistency, and is very easy to understand.\n2. The problem formulation is very clean and I believe the authors point towards and study an important problem.\n3. The proposed CuReNU and StoCuReNU are derived from established optimization theory. The adaptation for unlearning is technically sound and mathematically well-justified.\n4. Experiments span both batch and sequential unlearning on diverse datasets (vision, text). Metrics (accuracy, JS divergence, ToW score, etc.) are carefully chosen and clearly reported.\n5. The figures and tables are clear and informative."}, "weaknesses": {"value": "1. The proposed methods are direct adaptations of known optimization techniques. The novelty lies primarily in contextual application rather than new algorithms.\n2. The main results focus on small to medium models (CNN, ResNet-18, LoRA-tuned LLaMA-2). Full-scale LLM unlearning remains untested.\n3. Ablation studies that isolate the impact of cubic regularization vs. stochasticity are missing.\n4. Discussion around more efficient Hessian vector product is missing.\n5. While there are a lot of results that are included, the proposed methods are not always a clear winner."}, "questions": {"value": "1. How sensitive are CuReNU and StoCuReNU to the cubic regularization coefficient ùêø?\n2. Can the authors provide empirical Hessian spectra for larger models (e.g., LLaMA-2 layers) to substantiate degeneracy at scale?\n3. How does the unlearning error accumulate across rounds? Is there a mechanism to ‚Äúrecalibrate‚Äù the model to prevent drift after many sequential unlearning steps?\n4. For large foundation models with mixed data sources, how feasible is StoCuReNU in practice?\n5. Can authors include runtime comparisons at least for a subset of experiments w.r.t. baselienes?\n\nAny discussion around these will help improve the paper over it's current state."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1cBLUxi6YP", "forum": "dHz2LBCyTh", "replyto": "dHz2LBCyTh", "signatures": ["ICLR.cc/2026/Conference/Submission16110/Reviewer_mMKS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16110/Reviewer_mMKS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033565131, "cdate": 1762033565131, "tmdate": 1762926285517, "mdate": 1762926285517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}