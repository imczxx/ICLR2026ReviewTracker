{"id": "c4D7NJGC6D", "number": 20756, "cdate": 1758309793275, "mdate": 1763648236749, "content": {"title": "Contextual Latent World Models for Offline Meta Reinforcement Learning", "abstract": "Offline meta-reinforcement learning seeks to overcome the challenges of poor generalization and expensive data collection by leveraging datasets for related tasks. Context encoding is a prevalent approach, where an encoder maps transition histories to a task representation. In parallel, latent world models -- which map observations into temporally consistent latent spaces -- advanced self-supervised representation learning for planning and policy optimization. In this work, we unify these directions by introducing contextual latent world models: world models conditioned on the task representation and trained jointly with the context encoder. Coupling task inference with predictive modeling yields task representations that capture variation factors across tasks and empirically improves generalization to out-of-distribution tasks in diverse benchmarks, including MuJoCo, Contextual-DeepMind Control suite, and Meta-World.", "tldr": "", "keywords": ["Meta Learning", "Offline Reinforcement Learning", "World Modeling", "Representation Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e3f53a88f5bf30f97470f89db8d6031f244e6d2.pdf", "supplementary_material": "/attachment/a143a4f4946a16c1c47355c73611635a456ff4cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces C-DCWM, an offline model-based meta-reinforcement learning algorithm. The method utilizes a context-based approach, training a task encoder with an InfoNCE loss. It incorporates the learned task representations into the Discrete Codebook World Model (DCWM), extending it to the multi-task setting. The paper demonstrates that C-DCWM achieves superior performance compared to existing offline meta-rl baselines on the MuJoCo, Contextual-DMC, and Meta-World benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The study of world models capable of generalizing across multiple tasks is of significant practical importance, as it promises to substantially advance the real-world application of reinforcement learning.\n2. The authors provide extensive experiments across multiple benchmarks (MuJoCo, Contextual-DMC, and Meta-World) , demonstrating the method's strong empirical performance against several baselines."}, "weaknesses": {"value": "1. The proposed method appears to be a relatively straightforward extension of the DCWM frameworkto the multi-task setting by conditioning it on a task representation. This potentially limits the technical novelty of this work.\n2. The evaluation of generalization is limited. The out-of-distribution experiments primarily focus on parametric variations within the same task families. A more challenging and practically relevant OOD evaluation, such as generalization to unseen tasks in Meta-World, is notably absent.\n3. The paper lacks a comparative analysis of the computational complexity or overhead (e.g., parameter count, training time) relative to the baseline methods."}, "questions": {"value": "1. The current experiments (e.g., in MuJoC) primarily demonstrate OOD generalization to **new parameters of the same task family**. Does the model also exhibit **cross-task** generalization? For example, if the model is trained on a diverse set of tasks from Meta-World (e.g., 'door-open', 'window-close' et al. without ''window-open'), could it generalize (few-shot or zero-shot) to a completely **unseen** task, such as 'window-open'?\n2. Model-based methods typically have a larger parameter count compared to model-free method. Can authors provide analysis (e.g., an ablation study or parameter comparison) to confirm that the superior performance of C-DCWM stems from the effectiveness of the contextual world model itself, and not merely from an increased number of trainable parameters compared to the baselines?  \n- I will raise my score if my concerns above are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IYdMIXgKFI", "forum": "c4D7NJGC6D", "replyto": "c4D7NJGC6D", "signatures": ["ICLR.cc/2026/Conference/Submission20756/Reviewer_iC8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20756/Reviewer_iC8p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760886059564, "cdate": 1760886059564, "tmdate": 1762934177393, "mdate": 1762934177393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Contextual Discrete Codebook World Models (C-DCWM) for offline meta-reinforcement learning. It jointly learns (i) a context encoder that infers a task representation z from short histories, and (ii) a latent world model conditioned on z, built with a finite scalar quantization (FSQ) module and trained via cross-entropy-based temporal consistency and contrastive InfoNCE objectives. The latent representations are then used to train policies and critics (IQL) in the quantized latent space. Experiments on MuJoCo, Contextual-DMC, and Meta-World benchmarks show improved in-distribution and out-of-distribution performance compared to several existing meta-RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of context-conditioned latent world models is clearly implemented and experimentally well-controlled.  \n\n2. Empirical performance across multiple offline meta-RL benchmarks is consistent, showing that classification-based temporal consistency can outperform standard regression losses."}, "weaknesses": {"value": "1. Limited novelty. The approach mainly combines existing ingredients, e.g., context encoders from meta-RL, discrete latent world models from Dreamer/TD-MPC, and contrastive representation learning, into a joint training scheme. While the integration is clean, the conceptual advance over prior work such as CSRO, UNICORN, and discrete-latent Dreamer variants is marginal. \n\n2. Overreliance on IQL head. The offline RL component is fixed to IQL; no evidence is provided that the representation benefits generalize across different offline learners (e.g., CQL, TD3+BC). \n\n3. Missing comparison to contemporary discrete-latent models. Direct comparison to recent planning-based or discrete-latent world models (e.g., Dreamer-V3-Discrete, TD-MPC2 variants) is absent."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qlXgSizm9r", "forum": "c4D7NJGC6D", "replyto": "c4D7NJGC6D", "signatures": ["ICLR.cc/2026/Conference/Submission20756/Reviewer_2Hv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20756/Reviewer_2Hv7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981764499, "cdate": 1761981764499, "tmdate": 1762934176528, "mdate": 1762934176528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their constructive feedback. Your comments directly helped us strengthen the manuscript, broaden our empirical evaluation, and clarify several methodological aspects. Below, we highlight the additional experiments and analyses we conducted in response to your suggestions.\n\n### Experiments and Additional Analysis\n1. **Generalization to new environments (thanks to Reviewer iC8p).**  \nReviewer iC8p’s comments encouraged us to expand our empirical study to more challenging cross-environment generalization settings. In response, we evaluated our method, C-DCWM, on the ML10 and ML45 settings in the Meta-World benchmark. These settings involve training on 10 or 45 environments and evaluating on five structurally related but unseen environments. C-DCWM, outperforms baselines on both training and test tasks across both benchmarks. Increasing the number of training environments slightly improves generalization; however, a notable gap remains between training and testing environments. These new results (Table 2 in the revised manuscript, Section 4.1) provide a clearer and more nuanced understanding of C-DCWM’s generalization capabilities.\n\n2. **Effect of model size and number of parameters (thanks to Reviewer iC8p).**  \nReviewer iC8p’s question about parameter count motivated us to perform a more extensive scaling analysis. We trained all methods across six different model sizes and compared performance. C-DCWM, which naturally uses more parameters due to its discrete latent model, shows more reliable performance scaling than the baselines (Figure 6, Section B.4). C-DCWM, with the default model size, outperforms baselines across all model sizes. However, at smaller parameter budgets, C-DCWM can underperform baselines because the policy and value networks must be reduced to accommodate the discrete world model. This experiment clarified that C-DCWM’s gains are not merely due to increased model capacity.\n\n3. **Offline RL backbone comparison (thanks to Reviewer 2Hv7).**  \nReviewer 2Hv7 raised an important concern regarding possible overreliance on Implicit Q-Learning (IQL). In response, we added experiments using CQL [1] and TD3+BC [2] as alternative offline RL backbones (Table 14, Section B.5). We find that CQL performs competitively with IQL and even outperforms it on certain environments, while TD3+BC performs worse overall. These results demonstrate that the benefits of C-DCWM extend beyond the choice of the offline RL component. We thank the reviewer for highlighting this point.\n\n4. **Comparison to DreamerV3 (thanks to Reviewer 2Hv7).**  \nIn response to Reviewer 2Hv7’s suggestion, we conducted a comparison between C-DCWM and DreamerV3 [3], a state-of-the-art model-based RL method with a discrete latent space. As shown in Table 15 (Section B.7), DreamerV3 struggles with zero-shot generalization in the offline meta-RL setting, particularly in environments requiring distinctly different optimal behaviors (e.g., forward vs. backward movement). This comparison, directly motivated by the reviewer’s feedback, helped contextualize C-DCWM within the world-model literature.\n\n5. **Computational complexity (thanks to Reviewer iC8p).**  \nIn response to the reviewer’s request, we added an analysis of computational cost (Figure 7, Section B.6). C-DCWM incurs a higher per-step training cost but converges in fewer iterations. At test time, inference is slightly slower due to the latent-space encoding. This analysis clarifies the trade-offs associated with discrete world models and strengthens the empirical discussion.\n\n### References\n[1] Kumar, Aviral, et al. “Conservative q-learning for offline reinforcement learning.” *Advances in Neural Information Processing Systems* 33 (2020).  \n[2] Fujimoto, Scott, and Shixiang Shane Gu. “A minimalist approach to offline reinforcement learning.” *Advances in Neural Information Processing Systems* 34 (2021).  \n[3] Hafner, Danijar, et al. “Mastering diverse control tasks through world models.” *Nature* (2025)."}}, "id": "fMcLG1IWah", "forum": "c4D7NJGC6D", "replyto": "c4D7NJGC6D", "signatures": ["ICLR.cc/2026/Conference/Submission20756/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20756/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20756/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763648252694, "cdate": 1763648252694, "tmdate": 1763648252694, "mdate": 1763648252694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel offline meta-RL method called Contextual Discrete Codebook World Models (C-DCWM). C-DCWM encodes offline datasets from different tasks into task related representations. The world model is then conditioned on different learned representations. Experiments demonstrate that jointly training the world model and the context encoder leads to improved generalization performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is clearly written and easy to follow.\n2.The idea of extending DCWM to a conditioned version for generalizing across different tasks, and jointly training the model to obtain better task representations, is novel and well-motivated.\n3.The authors have conducted comprehensive experiments and ablation studies to verify and analyze the effectiveness of C-DCWM."}, "weaknesses": {"value": "1. While the current experimental validation is primarily conducted on continuous control tasks, these environments typically feature consistent dynamics within each task, which likely simplifies the learning of a discrete codebook. To further substantiate the generalizability of the proposed method, it would be compelling to evaluate it on more heterogeneous domains, such as the Atari benchmark. In these environments, a single task (or game) often comprises distinct levels requiring diverse policies and decision-making skills, presenting a more significant challenge for codebook learning. Extending the analysis to such tasks would greatly strengthen the empirical evidence for the method's robustness and effectiveness.\n2. some figures and tables are placed far from where they are referenced. For example, Figure 1 is distant from Section 3, which makes it inconvenient to cross-reference the figure with the corresponding formulas and explanations."}, "questions": {"value": "1.The placement of the figures and tables could be reorganized for better readability. For instance, Figure 1 might be placed after current Figure 2.\n2. As I understand it, to generalize the policy to an out-of-distribution task, C-DCWM requires a dataset from that task to compute the task representation. However, this assumption may be unrealistic, as we may not always have access to a dataset for an unknown task. Would it be possible instead to compute $z$ in an online manner—for example, starting from an initial $z$ and updating the task representation after each interaction step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEEtd7PFZI", "forum": "c4D7NJGC6D", "replyto": "c4D7NJGC6D", "signatures": ["ICLR.cc/2026/Conference/Submission20756/Reviewer_62LT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20756/Reviewer_62LT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994655145, "cdate": 1761994655145, "tmdate": 1762934175970, "mdate": 1762934175970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}