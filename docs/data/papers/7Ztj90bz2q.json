{"id": "7Ztj90bz2q", "number": 20156, "cdate": 1758303127409, "mdate": 1759896998340, "content": {"title": "RISE : Regression Imbalance Handling using Switching Experts", "abstract": "Deep Imbalanced Regression (DIR) is challenging due to skewed label distributions and the need to preserve target continuity. Existing DIR methods rely on a single,monolithic model, yet empirical analysis shows that standard benchmarks exhibit strong distributional heterogeneity, exposing a core limitation of such approaches. We theoretically prove that this property creates an irreducible bias for any single model, leading to poor performance in data-scarce regions. This creates a core challenge for algorithmic fairness, as these regions often correspond to marginalized demographic groups. To address this, we propose RISE—Regression Imbalance handling via Switching Experts—a modular Mixture-of-Experts–inspired framework, theoretically motivated by our analysis. RISE employs a novel imbalance-aware algorithm to identify underperforming regions via validation loss and trains dedicated experts with targeted upsampling. As a complementary framework, RISE achieves new state-of-the-art performance while improving fairness, highlighting a\nprincipled new direction for imbalanced regression", "tldr": "", "keywords": ["Imbalanced Regression", "Minority labels", "Fairness", "Mixture of experts"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/922386c7d0aa30ee85c7018b581b47225638d4dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of Deep Imbalanced Regression (DIR). The authors observe that existing DIR methods often suffer from strong distributional heterogeneity. They theoretically demonstrate that such approaches are biased in data-scarce regions. To address this issue, the authors propose RISE (Regression Imbalance handling via Switching Experts), a modular framework inspired by the Mixture-of-Experts architecture. RISE identifies underperforming regions and achieves improved performance across diverse settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors claim to be the first to identify the distributional heterogeneity issue in standard DIR benchmarks.\n2. The paper includes theoretical analysis that strengthens the overall conclusions.\n3. The proposed method is a natural extension of the theoretical findings and demonstrates improved performance."}, "weaknesses": {"value": "1. I have some concerns regarding the novelty of the work. The relationship between RISE and prior methods is not clearly articulated, and it is unclear how RISE fundamentally differs from or improves upon existing approaches. Please see the questions below for more details.\n2. The writing could be improved for clarity and organization; see the questions below."}, "questions": {"value": "1. What is the relationship between this work and prior studies? Are there additional related works that should be discussed? How does this paper differ from previous approaches in terms of novelty and contribution?\n2. In Section 4.2, T2 performs significantly better, whereas in Section 4.3, I3 achieves the best results. Could the authors provide explanations or insights into these differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uMNyXNxF0u", "forum": "7Ztj90bz2q", "replyto": "7Ztj90bz2q", "signatures": ["ICLR.cc/2026/Conference/Submission20156/Reviewer_wPda"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20156/Reviewer_wPda"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803186890, "cdate": 1761803186890, "tmdate": 1762933597394, "mdate": 1762933597394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Stronger evidence of functional heterogeneity in DIR"}, "comment": {"value": "To provide stronger evidence that the head vs. tail regions in DIR Datasets A (Moschoglou et al., 2017) and Dataset B (Rothe et al.,2018) correspond to fundamentally different predictive functions, we conducted two additional experiments (Details added in revised version of the paper in Appendix G):\n\n### **1) Freeze-and-Probe: testing feature transferability**\n\nThe goal of this experiment is to isolate feature transferability as the only factor under study.  \nTo do so, we fix the entire ResNet--50 backbone in both models and train only a newly initialized linear layer on the scarce Tail-Train data (e.g., label values $<15$).  \nBy freezing all convolutional layers, we eliminate effects from forgetting, overfitting, or capacity differences, ensuring that any performance difference must arise solely from the quality of the underlying feature representation.\n\nBoth models are trained under identical conditions:\nidentical linear probe architecture, identical L2 regularization, identical optimization hyperparameters, and identical early stopping based on the Tail-Val set. The only difference is the source of the frozen backbone:\n\n* **Model~B (General-Feature Baseline)**: Frozen ImageNet-pretrained ResNet50 backbone. This represents strong, general-purpose features not biased toward any label region in our dataset.\n\n* **Model~A-Probe (Head-Feature Test):** ResNet50 backbone first fine-tuned only on the Head region (e.g., label values 20-40), then frozen.  This tests whether features specialized for the head region transfer effectively to the tail.\n\nWe train only the linear layer for both models using the same Tail-Train data and evaluate the best checkpoint (chosen via Tail-Val early stopping) on the Tail-Test set.\n\n**Table 1: Freeze-and-Probe: Tail Test MAE Comparison**\n\n| Dataset | ImageNet (B) | Head-pretrained (A-Probe) | Relative Drop |\n| :--- | :---: | :---: | :---: |\n| *Dataset A* | $3.18 \\pm 0.06$ | $4.75 \\pm 0.07$ | $33$% |\n| *Dataset B* | $2.22 \\pm 0.01$ | $2.95 \\pm 0.01$ | $24$% |\n\nOn both datasets, the Head-biased features (Model A-Probe) performed significantly worse than the generic ImageNet features (Model B). Since all other variables were held constant (scarce data, probe architecture, L2 regularization, and early stopping), this performance gap cannot be attributed to scarcity or overfitting.\nThis is a direct, un-confounded proof of **Negative Transfer**. It demonstrates that the features learned for the Head are not merely incomplete but are fundamentally incorrect and harmful for the Tail task.\n\n### **2) Gradient Cosine Similarity (GCS): measuring optimization conflict**\nWe further examined whether head and tail objectives impose conflicting gradient directions, which would imply incompatible predictive functions [1].\nFor completeness, we computed GCS under the standard regression loss (MSE). However, MSE gradients are mechanically confounded. (Refer Appendix G.2)\n\nExperimental Setup:\n\n* Following [2], we discretize the label range into 101 bins (one for each label 0-100) and replace the monolithic model's (with Resnet50 backbone) head with an $N=101$ logit output layer, using Cross Entropy Loss.  \n* The gradient is $\\nabla \\ell = (\\hat{\\mathbf{p}} - \\mathbf{p})$, where $\\hat{\\mathbf{p}}$ is the softmax output and $\\mathbf{p}$ is the one-hot target vector which has no residual-dependent sign flip. \n* The resulting normalised GCS, $\\cos(\\nabla \\ell_{head}, \\nabla \\ell_{tail})$ computed on balanced batch of head and tail data points directly measures true optimization alignment/conflict.\n\nWe compute GCS per-layer and for the final- fully connected layer (fc) by flattening the corresponding parameter gradients into vectors. We report epoch-wise means over multiple runs (10 random seeds) and over several balanced mini-batches.\n\n**Table 1: Average GCS Comparison**\n\n| Dataset | Epoch | layer1 | layer4 | fc |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dataset A** | 1 | -0.0147 | -0.0527 | -0.1799 |\n| | 10 | -0.0370 | -0.0707 | -0.5965 |\n| | 50 | +0.0276 | -0.1183 | -0.4788 |\n| | 100 | +0.0444 | -0.1914 | -0.4905 |\n| **Dataset B** | 1 | +0.0283 | -0.0404 | -0.1258 |\n| | 10 | +0.0132 | -0.0512 | -0.3165 |\n| | 50 | +0.0154 | -0.1081 | -0.3754 |\n| | 100 | +0.0121 | -0.1104 | -0.3408 |\n\nDeeper layers (layer4, fc) show persistently negative GCS (–0.18 to –0.60), while early layers remain near zero, reflecting generic vs. region-specific features. **Persistent negative alignment means updates that reduce head loss increase tail loss, revealing incompatible optimization directions.** Similar trends are also observed during training with MSE loss as shown in revised version of our paper in Appendix G.2. This confirms that a single shared model cannot optimize both regions without explicit parameter separation.\n\n[1] Wang et al., 2021 ICLR: Gradient Vaccine-Investigating and Improving Multi-task Optimization in Multilingual Models.\n\n[2] Niu et al., 2016 CVPR: Ordinal regression with multiple output cnn."}}, "id": "4tzuqQn0zH", "forum": "7Ztj90bz2q", "replyto": "7Ztj90bz2q", "signatures": ["ICLR.cc/2026/Conference/Submission20156/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20156/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20156/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763638596266, "cdate": 1763638596266, "tmdate": 1763640541554, "mdate": 1763640541554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the Deep Imbalanced Regression (DIR) problem, where target distributions are heavily skewed and standard models fail to generalize across head and tail regions. It introduces RISE, a modular Mixture-of-Experts framework that identifies regions of poor model performance using validation-loss statistics, trains specialized experts for these regions, and dynamically routes instances to the most suitable expert at inference. The approach is supported by solid theoretical analysis of bias–variance trade-offs and validated across multiple benchmarks with consistent performance gains. Overall, this is a well-written and well-structured paper with a clear problem definition, a sound hypothesis, and a technically coherent proposal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Paper is very well written. The problem statement and main hypothesis are well stated in the intro and sound. The experiments are guided by RQ stated at the beginning clearly setting the direction of this section and making the analysis serve the right purpose.\n- Authors tackle a problem a very relevant problem that is quite often neglected, as the imbalanced learning literature focuses more on classification than regression.\n- The paper makes several strong claims about heterogeneity in DIR, that any monolithic model in DIR suffers from an irreducible heterogeneity bias amplified by imbalance,  and RISE’s performance improvement over baselined and they are well analyzed and validated either theoretically or empirically.\n- Proposed MOE is tuned for imbalanced regression based on errors/model complementarity (monolithic predictor) rather than feature space regions which is interesting and well justified.\n- Ablation studies analyzing different training strategies (e.g., router training)."}, "weaknesses": {"value": "- Figure 3 would benefit from larger font sizes and reorganization. It is hard to read even with a large zoom (200% of original size) as the font size is too small.\n\n- The manuscript describes RISE as “an orthogonal meta-framework” but does not clarify what orthogonality refers to. A clearer justification or rewording (e.g., “modular” or “complementary”) would improve precision.\n\n- While the evaluation follows prior DIR work using MAE, MSE, and bMAE, it would be more informative to include imbalance-aware metrics such as SERA or Relevance-Weighted RMSE (RW-RMSE). These would better capture performance across skewed target regions and align with the paper’s focus on heterogeneity and fair regression. In particular, SERA is an interesting metric for measuring imbalanced regression problems as it is not based on target region discretization [1].\n\n- The analysis for RQ5 is unconvincing. The random sampling approach is poorly described and the baselines are not competitive. A meaningful comparison must include boosting methods as they share similarities with the proposal (fitting new models on the errors of the previous one). That could even help highlight the benefits of the proposed MoE scheme.\n\n- The paper uses fairness in a broad, informal sense (equal performance across label densities) rather than a principled fairness metric or definition. As such, its fairness claims seem overstated.\n\n- In my opinion, a third option for the router training (union of Train and Val) should be considered as it has been shown to be relevant in the training of other ensemble selection approaches (see) and would make the analysis complete.\n\n- No actual public code or dataset links are provided, so reproducibility depends on re-implementing from the description.\n\nRefs: \n\n[1] Ribeiro, Rita P., and Nuno Moniz. “Imbalanced Regression and Extreme Value Prediction.” Machine Learning, vol. 109, no. 9, 2020, pp. 1803-1835."}, "questions": {"value": "1) What exactly does “orthogonal” mean in describing RISE as a meta-framework? Does it refer to architectural independence, complementarity to existing DIR baselines, or some form of objective orthogonality?\n2) The manuscript refers to improved “fairness” primarily as balanced performance across label densities. Could the authors elaborate on whether this aligns with any established fairness frameworks or metrics, or clarify the intended interpretation of fairness in this context?\n3) Present results with other imbalanced regression metrics such as SERA and RW-RMSE or provide a proper justification why they are not suitable for the scenarios in this paper.\n4) Including a third router-training regime that uses the union of Train + Validation data could make the ablation study more complete.\n5) Consider a stronger ensembling approach for RQ5 (like a boosting) and provide more details on the sampling used.\n\nProper answer to these questions I would definitely raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fi2S5sbge9", "forum": "7Ztj90bz2q", "replyto": "7Ztj90bz2q", "signatures": ["ICLR.cc/2026/Conference/Submission20156/Reviewer_Grbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20156/Reviewer_Grbf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871752223, "cdate": 1761871752223, "tmdate": 1762933540294, "mdate": 1762933540294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose RISE, a modular Mixture-of-Experts (MoE)–style framework motivated by the empirical observation that standard imbalanced regression benchmarks exhibit strong distributional heterogeneity. The method first identifies underperforming label regions based on validation loss, then trains specialized experts with targeted upsampling.\nThe authors conduct experiments on three benchmark datasets and report notable improvements on Dataset A (AgeDB-DIR) when integrating RISE with existing baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of addressing DIR through the lens of distributional heterogeneity is interesting, and it is technically reasonable to apply a MoE framework based on validation loss. The experimental results on AgeDB-DIR (Dataset A) are impressive, showing substantial improvements when RISE is integrated with existing baselines, and supported by ablation studies."}, "weaknesses": {"value": "### 1. Distributional heterogeneity\nThe notion of \"distributional heterogeneity\" is defined through model performance rather than as an inherent property of the data. The regions identified as heterogeneous are simply those where the baseline model performs poorly, which could result from underfitting or insufficient representation in the training set rather than distinct conditional distributions. Such \"heterogeneity bias\" may also come from the few-shot regions being under-sampled from the true data distribution, rather than from intrinsic heterogeneity. Framing it as heterogeneous data regression thus feels overclaimed, as true heterogeneous-data modeling is a different problem scope.\n\nAlso, in Figure 2, though the opposite trend is observed on training vs. test data for Dataset A, the trend is nearly identical for Dataset B across both training and test sets. This contradicts the claim in lines 66–69 regarding the distinct conditional distributions and thus weakens the empirical evidence for distributional heterogeneity for DIR datasets.\n\n### 2. Flawed frequency-based approach\nI agree that frequency-based methods such as over- or under-sampling are often suboptimal, mainly because they can lead to overfitting on minority regions or underfitting on majority ones. However, the argument in the manuscript, based on Table 2, that \"model performance is not strictly inversely proportional to frequency\" is not convincing. The order of the errors still follows the same ordering as the sample frequencies, suggesting that frequency is a strong explanatory factor. The claim would be more credible if supported by a quantitative correlation plot or a continuous analysis across the entire label range, rather than only coarse adjacent bands without statistical validation or finer binning.\n\nAlso, RISE-train also incorporates targeted up-sampling seemingly not consistent with the claim here. \n\n### 3. Experiment\nThe authors claim SOTA performance for DIR, but as shown in Table 9 (Appendix), adding RISE does not consistently yield the best performance on Dataset B or STS-B. The improvements is significant only on Dataset A (AgeDB-DIR), while the gains on other datasets are marginal or almost same after applying RISE to other baselines. \n\nMoreover, as illustrated in Figure 6 of [1], Dataset A is relatively \"less imbalanced\" compared to the other benchmarks, which may partially explain why RISE performs particularly well on that dataset but not on more severely imbalanced ones.\n\n\n[1] Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression, ICML 2021"}, "questions": {"value": "- Please see my comments in Points 1 and 2 of the Weaknesses section above.\n\n- The best empirical number of experts is reported as three. Is this choice related to the three label regions (many-, medium-, and few-shot) defined in the experiments? Do the routers actually learn to assign samples from similar or adjacent label regions to the same expert, or is the assignment largely random?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeNoGHPUFo", "forum": "7Ztj90bz2q", "replyto": "7Ztj90bz2q", "signatures": ["ICLR.cc/2026/Conference/Submission20156/Reviewer_aGaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20156/Reviewer_aGaM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902962957, "cdate": 1761902962957, "tmdate": 1762933517081, "mdate": 1762933517081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RISE, a framework designed to tackle the problem of Deep Imbalanced Regression. The authors argue that the core challenge in DIR is not just an imbalance in label distribution, but the distributional heterogeneity, where different regions of the label space have fundamentally different relationships between inputs x and outputs y. The paper argues that a single  \"monolithic\" model is biased towards the dominant head regions and cannot capture these distinct functions.\nRISE addresses this by replacing the monolithic model with a Mixture-of-Experts. The paper use a validation loss of a pre-trained baseline model to automatically discover contiguous \"failure regions\" in the label space. It then trains a set of specialized experts. Finally, a gating network, trained on the validation set, learns to route new inputs to the most appropriate expert. The paper provides theoretical justification for this approach and demonstrates empirically that RISE improves performance over state-of-the-art DIR methods across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel problem formulation and motivation. \n- The presented method  is modular and model-agnostic, designed as a ‘meta-framework’ that can be built on top of existing DIR methods.\n- The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. The evidence for \"fundamentally different predictive functions\" is somewhat weak. The cosine similarities between weight vectors for \"many\" and \"medium\" regions are also very low, which dilutes the argument that the conflict is specifically a head-vs-tail problem. A more convincing experiment, such as pre-training on the \"many\" region and fine-tuning on the \"few\" region, would be needed to truly test feature transferability and the \"fundamental shift\" claim.\n\n2. The evidence presented in Figure 2 is debatable. **The stated values (e.g., 24.7 for \"few\" in Dataset A test) do not appear to visually match the bar heights. More importantly, the contrasting behavior between Dataset A (low test error few, high test error in many) and Dataset B (high test error in few than the test error in many) undermines the narrative about heteroscedasticity.** This reduces confidence in the paper's claims based on this figure. As Figure 2 is the main evidence for the claims, I suggest the authors to make it bigger and vectorized. \n\n3. The paper main  weakness is its experimental methodology. The router network and region-discovery mechanism are both trained on the held-out validation set. This provides the RISE framework with access to data that the baseline models (SRL, LDS+FDS, etc.) do not use for training, making the main performance comparison in Table 4 an unfair. A cleaner, more valid approach would require all methods to have access to the same information. For example, a portion of the training set should be held out to train the RISE router, ensuring a fair comparison on the common, unseen test set.\n\n4.   Theorem 1 correctly identifies a persistent bias, but in practice, for extremely scarce tail regions, high variance is often the dominant source of error (due to small n). Furthermore, related (and unreferenced) work has also discussed the bias-variance trade-off in imbalanced regression.\n\n5. The language in the introduction (e.g., \"mastering each sub-problem\") need to be weakened. Furthermore, the research gap identified for some prior work (\"lack mechanisms to detect minority regions or train dedicated experts\") is what the authors proposed not really a research gap."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRwFxcluWj", "forum": "7Ztj90bz2q", "replyto": "7Ztj90bz2q", "signatures": ["ICLR.cc/2026/Conference/Submission20156/Reviewer_mmVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20156/Reviewer_mmVS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992338533, "cdate": 1761992338533, "tmdate": 1762933502792, "mdate": 1762933502792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}