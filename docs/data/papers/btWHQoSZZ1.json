{"id": "btWHQoSZZ1", "number": 11633, "cdate": 1758202692727, "mdate": 1763377881540, "content": {"title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning", "abstract": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.", "tldr": "We present Rex-Thinker, an object referring model with grounded reasoning.", "keywords": ["Object Referring", "Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bda224459c49621204c7bbf99ea60652ce35756.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "An overly simplified summary of the paper: the authors propose using explicit chain-of-thought (CoT) reasoning with predefined steps (plan, action, summarize) for the object-referring task (i.e., detecting the exact object(s) referred to by a natural language question). To train models to follow these steps the authors use GPT-4o to annotate an existing object-referring dataset, HumanRef (Jiang et al. (2025b)), with their reasoning traces, creating the HumanRef-CoT dataset. They then use those annotations to post-train a model (SFT for cold-start, followed by GRPO) to produce the Rex-Thinker model. The evaluation is split into in-domain (HumanRef) and out-of-domain (RefCOCOg (Mao et al., 2016)) benchmarks: Rex-Thinker outperforms other models on the in-domain dataset and is comparable to them on the out-of-domain set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper presents an interesting idea and gives enough explanation and detail to follow it (the appendix is particularly helpful for things omitted in the main paper due to space). The evaluation covers both in- and out-of-domain datasets to demonstrate the model’s effectiveness, and the authors include additional experiments exploring different aspects of the approach."}, "weaknesses": {"value": "There are, however, three main issues that justify my score. \nFirst, the paper doesn’t convincingly demonstrate the quality of the generated dataset, HumanRef-CoT. There is no targeted evaluation of the quality or usefulness of the reasonings added on top of HumanRef — no human evaluation or deeper analysis — which is surprising since the dataset is presented as one of the paper’s main contributions. My concern is amplified by the authors’ own note (line 239) that GPT-4o sometimes produces wrong answers, which is problematic given that the ground truth is available in the input (Figure 2). \nSecond, the out-of-domain results for Rex-Thinker limit the generalizability of the proposed idea — the gains seem largely in-domain. \nThird, the evaluation could dig deeper into which instance types Rex-Thinker fails on and which it improves; for example, the “Interaction” column in Table 2 seems like a good candidate for further discussion. With the current results it’s hard to draw clear conclusions about the model’s strengths and limitations."}, "questions": {"value": "For the results in Tables 2 and 3, it’s unclear how the numbers for the other models were obtained. What exact setup was used for those baselines? For example, was SFT done using ground-truth reasoning or not? Please clarify the evaluation/setup for each compared model (I may have missed this in the appendix so please point me to the correct section if that is the case).\n\nMinor typo: line 332, “Blod” → “bold.”"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Given that one of the main contributions is a dataset, I was surprised that the paper doesn’t include any discussion of its ethical aspects. From what I understood, there don’t seem to be any guardrails against problematic or offensive language in the generated reasoning steps, which could make the dataset ethically questionable. To be clear, I didn’t find any evidence of such issues myself, but the absence of any discussion or acknowledgment of this risk in the paper is concerning.\nAlso there is no discussion on plans to make the dataset publicly available."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oNbGpdSTHr", "forum": "btWHQoSZZ1", "replyto": "btWHQoSZZ1", "signatures": ["ICLR.cc/2026/Conference/Submission11633/Reviewer_28Tw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11633/Reviewer_28Tw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431115324, "cdate": 1761431115324, "tmdate": 1762922703886, "mdate": 1762922703886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Rex-Thinker, a novel framework for the task of Referring Expression Comprehension (REC). Diverging from traditional methods that directly predict bounding boxes, the authors reformulate the task as an explicit and interpretable Chain-of-Thought (CoT) reasoning process. Rex-Thinker employs a symbiotic architecture that first utilizes an open-vocabulary object detector to generate candidate object proposals (\"box hints\"). Subsequently, a Multimodal Large Language Model (MLLM) performs step-by-step reasoning over these candidates to evaluate their alignment with the given language description."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors tackle the task of Grounded Object Referring from a novel perspective (Chain-of-Thought reasoning), providing a new, interpretable approach.\n2. The authors have constructed a high-quality dataset that can facilitate the development of the research community.\n3. The paper is well-written and clearly organized."}, "weaknesses": {"value": "1. The methodology seems largely built on recent \"R1-like RL\" and \"think-with-images\" paradigm, which lacks novelty.\n2. The paper lacks validation for the annotations generated by GPT-4o. Given that commercial models have been shown to have issues (e.g., hallucination), a manual review and evaluation of the annotated data is necessary to ensure its quality.\n3. The paper lacks comparisons with the recent \"Think-with-image\" paradigm, e.g., Deepeyes, Pixel-Reasoner, and GRIT. Considering that Rex-Thinker also employs a two-stage post-training paradigm, comparing it with these methods would be crucial for better clarifying the authors' contributions."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6omXBwWnF8", "forum": "btWHQoSZZ1", "replyto": "btWHQoSZZ1", "signatures": ["ICLR.cc/2026/Conference/Submission11633/Reviewer_72MZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11633/Reviewer_72MZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807273361, "cdate": 1761807273361, "tmdate": 1762922703353, "mdate": 1762922703353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies referring expression grounding with explicit, verifiable reasoning rather than direct box regression. The task is conducted as retrieval over candidate boxes from an open-vocabulary detector, followed by a plan–act–summarize chain of thought that can also abstain when the target is absent. Training is two-stage: supervised fine-tuning on curated CoT traces, then GRPO reinforcement with an F1-based detection reward, a format reward, and KL regularization. Experiments on HumanRef and out-of-domain RefCOCOg show consistent gains in precision, recall, and F1."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The plan–act–summarize CoT exposes intermediate reasoning tied to concrete boxes. This improves debuggability and reduces hallucination risk. It also enables a principled “no target” refusal. \nThis paper implements the SFT-then-RL framework on the REC with reasoning for MLLMs. \nThe reward combines F1 for grounded detection with a lightweight format constraint. This directly optimizes what the benchmark cares about."}, "weaknesses": {"value": "The paper does not report even small-scale human analysis/evaluation of the GPT-4o–generated chain-of-thought data. Quality control relies mainly on some rule-based functions like answer-conditioned prompts and automatic consistency filtering (keeping only samples whose final prediction matches ground truth). This may introduce bias in the framework and lacks inter-annotator checks as GPT-4o is not the most advanced model and the data is generated data. As a result, the reliability and transferability of these might be limited. \n\nIn another side, the contribution is incremental relative to prior retrieval-based referring and grounded CoT work[1,2]. The methodological addition is an F1-aligned RL reward with strict IoU matching. From the experiments the performance beyond existing approaches appears small margin. \n\n[1] ChatRex: Taming Multimodal LLM for Joint Perception and Understanding. [2] ARGUS: Vision-Centric Reasoning with Grounded Chain-of-Thought, CVPR 2025"}, "questions": {"value": "It might be better for the author to conduct a human analysis/evaluation of random samples of the GPT-4o–generated chains of thought."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XmBcotHYp", "forum": "btWHQoSZZ1", "replyto": "btWHQoSZZ1", "signatures": ["ICLR.cc/2026/Conference/Submission11633/Reviewer_Q9vM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11633/Reviewer_Q9vM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996955142, "cdate": 1761996955142, "tmdate": 1762922702952, "mdate": 1762922702952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to induce or enhance chain-of-thought reasoning of VLMs, with an application on Referring Expression Comprehension  (REC). The training recipe is standard practice, i.e., SFT+RL (GRPO)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using RL for Referring Expression Comprehension is under-explored, beyond early efforts ([a-b] in Weaknesses below).\n2. The improvements from SFT/RL post-training look encouraging, although not totally convincing (See Weaknesses 2 and 3)."}, "weaknesses": {"value": "1. Citations to previous REC + RL works are absent. For example, [a-b]. Instead, the paper only cites generic VLM works with RL, in which REC is only a subtask.\n2. In Table 3, Rex-Thinker-CoT and Rex-Thinker-GRPO perform worse than QwenVL-2.5-7B (the base model of Rex-Thinker), which seems to be a sign of catastrophic forgetting due to post-training. The authors should find a way to mitigate this.\n3. The main experimental results (Tables 2 and 4) are on one dataset only, the HumanRef. \n4. (Minor) \"Symbiotic approach\" is an unnatural framing. Such two-stage pipelines are commonly used, and people usually don't call them \"symbiotic approaches\".\n\n[a] iterative shrinking for referring expression grounding using deep reinforcement learning. CVPR 2021.\n\n[b] One for all: One-stage referring expression comprehension with dynamic reasoning. Neurocomputing 2023."}, "questions": {"value": "1.  In the example in figure 3, the detector detects \"person\", which is straightforward. Can the model detect targets with negations, e.g. \"non-persons\"? It seems not obvious and may be challenging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0kcF07R0q", "forum": "btWHQoSZZ1", "replyto": "btWHQoSZZ1", "signatures": ["ICLR.cc/2026/Conference/Submission11633/Reviewer_HDXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11633/Reviewer_HDXw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071469810, "cdate": 1762071469810, "tmdate": 1762922702636, "mdate": 1762922702636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}