{"id": "rcmfu1ydAf", "number": 21904, "cdate": 1758323384359, "mdate": 1759896897480, "content": {"title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Tasks", "abstract": "Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks.", "tldr": "We introduce HUME, a framework for measuring human performance on embedding tasks, providing baselines across 16 datasets to interpret model scores.", "keywords": ["Human Evaluation", "Embeddings", "MTEB", "Benchmarking", "NLP", "Multilingual"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2403691cbb3bc66568f19630be56ab46fef72a59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HUME, a framework for measuring human performance on text embedding tasks from MTEB. The authors evaluate human annotators across 16 datasets spanning four task categories: reranking, classification, clustering, and semantic textual similarity (STS)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The selection of 16 diverse datasets across multiple languages, domains, and task types is well-motivated and thorough. The inclusion of both high-resource and low-resource languages adds an important cross-lingual perspective."}, "weaknesses": {"value": "While the paper addresses an interesting question, I have concerns:\n1. **Necessity and Scope of the Study**\n\t- **Limited sample sizes**: With only 20-50 instances per task, can we really draw reliable conclusions about human performance on these benchmarks? \n\t- **Missing task categories**: The framework only covers 4 of MTEB's task types (reranking, classification, clustering, STS). What about retrieval, which is a very important task nowadays?\n2. **Methodological Concerns**\n\t- **Statistical significance**: The paper presents many performance comparisons but doesn't include significance testing. With small sample sizes, how confident can we be that observed differences are meaningful?"}, "questions": {"value": "**Unclear Motivation for Human-Model Comparison**\n\nI'm struggling to understand the fundamental premise of this paper. Why should we compare embedding models to human performance at all?\n\nThis isn't like evaluating generative models, where we compare LLMs to humans because we want human-level intelligence and knowledge. Embedding models serve a completely different purpose. We never use humans to generate embeddings or perform large-scale semantic search in practice. The goal is optimizing downstream task performance, not replicating human judgment. So what does human performance actually tell us about embedding quality?\n\nThe paper repeatedly interprets superior model performance as suspicious, suggesting models are \"exploiting patterns rather than achieving true semantic understanding.\" But why can't models just be legitimately better at these tasks? Machine learning is designed to find patterns beyond human perception. When a model outperforms humans at emotion classification, maybe it's actually better at detecting emotional patterns consistently, rather than exploiting flawed data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UpFkTUjF8x", "forum": "rcmfu1ydAf", "replyto": "rcmfu1ydAf", "signatures": ["ICLR.cc/2026/Conference/Submission21904/Reviewer_N1ED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21904/Reviewer_N1ED"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760751691260, "cdate": 1760751691260, "tmdate": 1762941975682, "mdate": 1762941975682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all four reviewers (FCbW, RywH, zWFZ, N1ED) for their time and for providing detailed, insightful, and constructive feedback on our submission. We are encouraged that reviewers found our core idea \"supper cool\" (RywH) and the problem \"timely and important\" (zWFZ).\nThe feedback has provided a clear and valuable path for strengthening our work. The most critical feedback converged on three themes, which we address first in a general response, followed by specific replies to each reviewer.\n\n\n1. On the Core Motivation: Calibration, Not a Ceiling (Response to FCbW, N1ED)\n\nSeveral reviewers (FCbW, N1ED) correctly questioned the motivation for comparing human and model performance, arguing that \"ground-truth\" labels already exist and that \"human-level\" performance is not necessarily the goal for embeddings.\nOur goal is benchmark calibration and diagnostics, not setting a performance \"ceiling.\" We are not using human performance as an upper bound, but as a tool to interpret benchmark scores and diagnose dataset quality. Our work questions the \"ground truth\" itself. As our results and low inter-annotator agreement (IAA) show, many labels in MTEB are ambiguous, subjective, or of poor quality.\nThe key finding is that when models achieve \"superhuman\" performance, it often occurs on datasets with low IAA. This suggests that models are not achieving true \"superhuman\" semantic understanding, but are instead exploiting label ambiguity or overfitting to artifacts in the benchmark's \"ground truth.\" Thus, HUME provides an essential diagnostic. It allows us to differentiate between (a) models truly excelling at a well-defined task and (b) models simply overfitting to a flawed dataset. In the final revision, we will add concrete examples to the main text where the model is 'correct' (i.e., matches the 'ground truth') but the label itself is clearly ambiguous, underscoring our point that models are often just fitting to dataset artifacts. We will stress that if we want to build better, more reliable evaluations, they cannot be based on such ambiguous examples.\n\n\n2. On the Omission of Retrieval Tasks (Response to FCbW, RywH, zWFZ, N1ED)\n\nAll four reviewers noted the exclusion of retrieval as a major limitation, given it is a primary application of embeddings.\nWe agree that retrieval is critical. However, a direct human evaluation of large-scale retrieval presents unique methodological challenges. Building a robust human evaluation framework for large-scale retrieval requires particular considerations that would constitute significant work on its own; a human would need to recall or read thousands of candidate documents per query.\nTo address this, our study includes Reranking tasks, which we posit as the necessary and human-evaluable proxy for retrieval. We ask humans to evaluate the relevance of the top-10 candidate documents for a query, which is directly human-evaluable and conceptually equivalent in embedding-space behavior. This design follows prior work (e.g., MTEB Arena human evaluation) and ensures comparability with model reranking tasks. We will explicitly state this reasoning in Section 3.2, emphasizing that our reranking evaluation serves as the human-accessible proxy for retrieval.\n\n3. On Methodological Robustness (Small Samples & Annotators) (Response to FCbW, RywH, zWFZ, N1ED)\n\nAll reviewers rightly raised concerns about the small sample sizes (20-50 examples) and limited annotator pool (1-2 per task), which impact statistical robustness.\nWe acknowledge this limitation. This study was intentionally designed as a breadth-first framework. Our primary goal was to establish the first human baselines across a wide variety of 16 MTEB tasks. Crucially, our pool is diverse in terms of linguistic and cultural background, covering five distinct languages (English, Arabic, Russian, Norwegian Bokmål, and Danish). We argue this diversity is particularly important for a multilingual benchmark, which was necessary to uncover the cross-task patterns (like the IAA-vs-superhuman-performance link) that form our central contribution.\nWe agree that more annotations would be ideal. However, our current \"broad-and-shallow\" design (with over 1,500 individual annotations) is what enables us to make our central claim about dataset quality.\nTo show the uncertainty range and address concerns about statistical robustness, we will add 95% confidence intervals and statistical significance testing to our main result tables. Inter-annotator agreement metrics (Fleiss’ κ, correlation-based measures for STS, etc.) are already reported in Appendix D.\nFinally, we are preparing a public call for additional annotators to expand linguistic and demographic diversity in the next HUME leaderboard release, which will strengthen statistical power, generalizability and diversity."}}, "id": "lE6n8LMhLD", "forum": "rcmfu1ydAf", "replyto": "rcmfu1ydAf", "signatures": ["ICLR.cc/2026/Conference/Submission21904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21904/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21904/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763473457757, "cdate": 1763473457757, "tmdate": 1763473457757, "mdate": 1763473457757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HUME, a framework for measuring human performance on text embedding tasks and comparing it directly to state-of-the-art embedding models. It evaluates human annotators across 16 MTEB datasets spanning classification, clustering, reranking, and semantic textual similarity in multiple languages. Results show that human performance is competitive but not dominant, often ranking around the upper-middle of model performance, with notable advantages in certain multilingual tasks. The authors analyze task difficulty, dataset quality, and human agreement, highlighting that low human performance often reflects dataset ambiguity rather than human limitations. The paper concludes with implications for benchmark design, cultural and linguistic gaps in current models, and recommendations for more reliable evaluation practices."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a timely and important problem by grounding text embedding evaluation in human performance. The motivation is well articulated, and the direction is practical and relevant for improving how benchmark results are interpreted.\n\n2. The experimental design is generally solid, with clear task selection and consistent evaluation protocols. While some aspects could be expanded (see weaknesses), it establishes a strong foundation for systematic human–model comparison across languages and task types.\n\n3. The discussion and implications are well developed, offering clear insights into dataset quality, evaluation reliability, and multilingual gaps. The proposed future suggestions are concrete and meaningful, adding to the significance and forward-looking value of the work."}, "weaknesses": {"value": "1. The study’s reliance on only two annotators for most tasks (and, in some multilingual settings, just one) severely limits the robustness and representativeness of the claimed “human” performance. With such a small pool, the results risk reflecting individual annotator biases rather than general human ability, making the observed human–model gap less reliable as an empirical reference point. In addition, the number of annotated examples per dataset—ranging only from 30 to 50 items—is far too small relative to the scale of the original benchmarks. This raises concerns about statistical power and generalizability: subtle effects may be missed, while task-specific variability could be overstated or underrepresented in the final conclusions.\n\n2. Although the paper identifies several dataset quality issues, it does not sufficiently analyze their underlying causes from the perspective of model training. In particular, there is no supporting evidence or exploration of factors such as training data distributions, domain coverage, or cultural mismatches that might explain the observed human–model performance differences. This lack of analytical depth weakens the explanatory power of the findings, especially in multilingual settings where humans outperform models in some tasks but not others.\n\n3. The paper directly compares human annotators and embedding models but lacks a key experiment: whether large language models can follow the same annotation protocol. Evaluating LLMs under the same instructions would not only provide a useful reference baseline but also offer practical value by reducing the cost of future human evaluations. This experiment could significantly strengthen the paper’s claims regarding the human–model gap.\n\n4. While the paper covers classification, clustering, STS, and reranking tasks, it notably omits retrieval tasks, which are arguably the most central and widely used application of text embeddings in real-world systems. Excluding retrieval significantly limits the practical relevance and completeness of the study’s conclusions.\n\n5. The paper provides insufficient clarity on whether the human labeling instructions were fully aligned with the original dataset annotation schemes. Any divergence in labeling guidelines could introduce inconsistencies and confound the comparison between human annotators and model performance, raising questions about the validity of the measured human–model gap."}, "questions": {"value": "1. (W1) Could the authors provide justification for using such a small number of annotators and annotated samples? Do they have any evidence or pilot studies suggesting that this limited pool and sample size are sufficient to produce stable and representative estimates of “human” performance?\n\n2. (W2) Can the authors provide a more detailed analysis of the underlying factors contributing to the observed human–model performance differences, such as training data coverage, domain bias, or cultural and linguistic variability, especially in multilingual tasks?\n\n3. (W3) Would it be feasible to evaluate LLMs by instructing them to follow the same annotation protocol as human annotators? If so, how might such an experiment serve as a complementary or cost-efficient reference point in future work?\n\n4. (W4) Why were retrieval tasks excluded from the study, and how might including them—given their centrality to text embedding applications—affect the overall findings and implications?\n\n5. (W5) How closely were the human labeling instructions matched to the original dataset annotation schemes, and could any inconsistencies between the two affect the validity of the human–model comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VGmHYbDsgE", "forum": "rcmfu1ydAf", "replyto": "rcmfu1ydAf", "signatures": ["ICLR.cc/2026/Conference/Submission21904/Reviewer_zWFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21904/Reviewer_zWFZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761038220173, "cdate": 1761038220173, "tmdate": 1762941975475, "mdate": 1762941975475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HUME, a framework for measuring human-level performance on text embedding benchmarks. They compared human performance with 13 embedding models on 4 major categories of tasks (classification, clustering, semantic textual similarity, and reranking) in MTEB and analyzed the consistency of human annotators as the entry point, putting forward suggestions for the development of future evaluation benchmarks.\nThe main contributions include: (1) Proposing a framework for evaluating human embedding capabilities, which is currently relatively lacking; (2) The assumption that tasks with low human consistency may reflect the ambiguity of data or definitions rather than the true \"superhuman\" model capabilities offers a new guidance for the future development of benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Supper cool idea!\n1. The paper is original in reframing benchmark evaluation: a human evaluation framework that quantifies human-level performance across text-embedding tasks and relates it to model results.\n2. The study is of good methodological quality, covering sixteen datasets and four task categories with transparent protocols and clear reporting.\n3. The paper’s significance lies in revealing that many “superhuman” model claims arise in tasks with low human agreement, highlighting the need for benchmark reform and more interpretable evaluation standards."}, "weaknesses": {"value": "The small and homogeneous annotator pool (all male, limited in number, with single annotators for some languages) limits generalizability and the evaluation sets are small and lack confidence interval reporting, reducing statistical robustness.\n\nThere is a lack of a more direct and powerful analysis of the performance attribution of the superhuman model (directly attributing low human consistency to data/task design and quality issues is not rigorous enough). \n\nThe retrieval is the most widely used embeddinng application, which is not included in this work. I'm very curious and excited about how well this framework could guide the retriver evaluation.\n\nAdditionally, the proposed “agreement-weighted evaluation” idea is conceptually interesting but underdeveloped."}, "questions": {"value": "How do you ensure that the low agreement among your annotators truly reflects data ambiguity, rather than annotation fatigue or lack of clear instructions? It is possible to supplement the analysis of model bad cases and whether the model as a whole leans towards certain labels.\n\nCould you share a concrete formula for your proposed “agreement-weighted evaluation”? It is suggested that several feasible agreement-based weighted evaluation methods be proposed."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Is there a description of the annotator’s compensation, protection and psychological safety, etc. details?"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EOolwF2ByZ", "forum": "rcmfu1ydAf", "replyto": "rcmfu1ydAf", "signatures": ["ICLR.cc/2026/Conference/Submission21904/Reviewer_RywH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21904/Reviewer_RywH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708648929, "cdate": 1761708648929, "tmdate": 1762941975248, "mdate": 1762941975248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HUMA, a human performance evaluation on the MTEB benchmark. Results show that humans achieve an average score of 77.6%, compared to 80.1% for the best embedding model. Further analysis reveals that text embedding models underperform humans on low-resource languages, and that some datasets exhibit low quality or label ambiguity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper presents a comprehensive human performance evaluation of MTEB, covering reranking, classification, clustering, and STS datasets.\n\n(2) Based on the human performance results and inter-annotator agreement analysis, the authors identify several problematic datasets containing labeling ambiguities (e.g., the emotion classification dataset), which may lead to unrealistic model evaluations.."}, "weaknesses": {"value": "(1) The motivation of conducting a comprehensive human performance evaluation of MTEB is not very convincing to me. The two possible reasons I can get are (1) estimating an uppper bound of dataset performance, similar to how human performance on GLUE was previously used as a target to achieve. But this is not the case here since the paper shows that current models already surpass human performance; and (2) as the paper points out, identifying problematic datasets where humans perform poorly or where inter-annotator agreement is low. However, this second motivation seems somewhat weak to me.\n\n(2) The retrieval task, which is widely used in real-world applications, is not included in the human performance evaluation.\n\n(3) Most tasks includes only two annotations, and for each task there are only 20-50 instances annotated. This raise concerns on the reliablity of the evaluation.\n\n(4) Comparing the performance of native low-resource language speakers with text embedding models primarily trained on English data may not be a fair comparison."}, "questions": {"value": "(1) In L99-102, the paper states that \"Human evaluation is well established in NLP, especially for generative tasks like machine translation, summarization, and dialogue.  In contrast, embedding-based tasks have relied almost exclusively on automated metrics, with little attention to human baselines.\". This motivation is not convincing to me. For these generation tasks, automatic evaluation metrics are often unreliable, which requires human evaluation. However, for embedding-based tasks that have well-defined ground truth labels, human evaluation may not be that necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qokMkHzIrI", "forum": "rcmfu1ydAf", "replyto": "rcmfu1ydAf", "signatures": ["ICLR.cc/2026/Conference/Submission21904/Reviewer_FCbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21904/Reviewer_FCbW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991541909, "cdate": 1761991541909, "tmdate": 1762941974904, "mdate": 1762941974904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}