{"id": "4t51qDeEq7", "number": 14138, "cdate": 1758229076821, "mdate": 1759897388061, "content": {"title": "Mitigating Hallucinations in Large Language Models via Hybrid Reinforcement Learning", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing by producing text that is coherent, contextually relevant, and often indistinguishable from human writing. However, a major challenge persists: hallucinations—outputs that are linguistically fluent but factually inaccurate or irrelevant—pose significant risks in domains requiring high precision, such as healthcare, law, and finance. In this study, we introduce a Hybrid Reinforcement Learning (HRL) framework that strategically combines Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF). By harmonizing the reliability of human oversight with the scalability of AI-based evaluation, HRL enhances factual accuracy while maintaining text fluency. Experiments on standard benchmarks, including TruthfulQA and MMLU, demonstrate substantial reductions in hallucination rates and marked improvements in factual correctness compared to prior approaches. This framework provides a robust, scalable pathway toward deploying LLMs more reliably in high-stakes applications.", "tldr": "We propose a Hybrid Reinforcement Learning framework that dynamically combines human and AI feedback to significantly reduce hallucinations in large language models while maintaining text quality and scalability.", "keywords": ["Large Language Models", "Hallucination Mitigation", "Reinforcement Learning from Human Feedback", "Reinforcement Learning from AI Feedback", "Hybrid Reinforcement Learning", "Natural Language Processing", "Factual Accuracy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4073f4b2c7534acdb244e1ed9ac04971634cf61.pdf", "supplementary_material": "/attachment/941c71ef34c7829e7580cac93e2fb46ea846a095.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a hybrid reinforcement learning (HRL) framework that integrates Reinforcement Learning from Human Feedback (RLHF) and Reinforcement learning from AI Feedback (RLAIF) using a dynamic weighting mechanism to mitigate hallucinations. They propose some techniques for improving HRL training, including uncertainty masking, progressive curriculum learning and periodic calibration of the AI feedback generator. Their experiments on TruthfulQA and MMLU demonstrate that the proposed approach can improve factual accuracy while maintaining coherence. They conduct an ablation study to investigate the impact of the dynamic weighting factor in the hybrid reward and impact of different training techniques on factual accuracy. Their analysis shows that the optimal performance is achieved with a balanced combination of human feedback and AI feedback. Also, HRL achieves a higher cumulative reward with fewer training steps."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This paper addresses the critical problem of mitigating factual hallucinations in LLMs\n- The core idea of dynamically combining human and AI feedback in RL training is conceptually simple and sound, which strikes a better balance between the high quality of human feedback and the scalability of AI feedback\n- Their experiments show that HRL improves factual accuracy and reduces hallucination rate compared with RLHF, RLAIF and static hybrid baselines"}, "weaknesses": {"value": "- The human feedback used in HRL is generated by a simulator rather than real human annotations.\n- Some details of the proposed method and the experiment setup are not clearly explained , e.g. it is not clear what context and time dependent features they used to compute the dynamic weight, what uncertainty estimation methods they used for uncertainty masking, how the hallucination rate is measured, which raises concerns about its reproducibility\n- Qualitative analysis is not presented in Section 4.5\n- Lack of significance testing of the results in Table 1 and Table 2"}, "questions": {"value": "- The formatting of the figures is not good enough. Figures 1 and 3 include an additional caption within the graph, which is confusing. The layout should be more compact.\n- The authors claim that HRL is efficient with minimal computational overheads, but the dynamic weighting mechinism can actually introduce extra computation at each training step compared with the baselines"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "28pkoj9Lfw", "forum": "4t51qDeEq7", "replyto": "4t51qDeEq7", "signatures": ["ICLR.cc/2026/Conference/Submission14138/Reviewer_fUBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14138/Reviewer_fUBF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748710126, "cdate": 1761748710126, "tmdate": 1762924602433, "mdate": 1762924602433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Hybrid Reinforcement Learning framework to mitigate hallucinations in large language models by dynamically combining human feedback and AI feedback. The core contribution is an adaptive weighting mechanism α(c,t) that balances these feedback sources based on context and training progression, trained using PPO optimization. The authors report improvements over baselines on TruthfulQA and MMLU benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The problem is well-motivated—hallucination is a critical issue for LLM deployment. The idea of combining human and AI feedback is reasonable, though not entirely novel. The key contribution is making the mixing weight α learnable and context-dependent, which is a logical extension over static hybrid approaches."}, "weaknesses": {"value": "The exposition is poor and lacks critical methodological details. For example, what are the exact features included in φ(c,t)? The paper mentions \"context features\" and \"temporal information\" but provides no specification. How is \"periodic recalibration\" implemented?  what is the algorithm? The optimization procedure for the weighting parameters w_α is not described—are they trained jointly with the policy network or separately?\n\nThe experimental setup is fundamentally flawed. The main selling point of the paper is to adaptively incorporate human feedback, yet no actual human feedback is used in training. Line 213 states: \"Human feedback uses simulation with expertise 0.85, agreement κ > 0.7, incorporating realistic noise and variability patterns.\" How is this simulated? Using what model? On what data? What does \"expertise 0.85\" mean operationally? How are \"realistic noise and variability patterns\" generated? Without real human annotators, the claims about RLHF integration are not validated.\n\nLine 207 mentions using \"LLaMA-2 7B/13B as primary models with DistilGPT-2 as fallback when computational resources are limited.\" This is confusing. The paper proposes a data mixing strategy—why are different base models involved? How does model selection relate to the core contribution?\n\nLine 213 states \"AI feedback leverages trained DeBERTa-NLI and DialoGPT models with uncertainty estimation and confidence thresholds.\" How is uncertainty estimation performed? What are the specific confidence thresholds? Why these models? How were they trained?\n\nLine 214 mentions \"Evaluation employs trained NLI models for factual accuracy assessment with automatic heuristic fallbacks.\" Which NLI models? What are these automatic heuristics? How do they ensure robustness?\n\nThe presentation quality is poor. The paper uses excessive bullet points rather than cohesive prose. Figure 1 showing training/validation convergence dominates the page but provides minimal insight"}, "questions": {"value": "I would like to know the details of the method and experimental setup, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJWTHeXeJJ", "forum": "4t51qDeEq7", "replyto": "4t51qDeEq7", "signatures": ["ICLR.cc/2026/Conference/Submission14138/Reviewer_mASg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14138/Reviewer_mASg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951642517, "cdate": 1761951642517, "tmdate": 1762924601969, "mdate": 1762924601969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hybrid reinforcement learning (HRL) framework that combines human and AI feedback to mitigate hallucinations in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a hybrid reinforcement learning (HRL) framework that combines human and AI feedback to mitigate hallucinations in LLMs, which is a good thing."}, "weaknesses": {"value": "The contribution and novelty of the method are quite limited. The motivation is unclear. The discussion on related works is not comprehensive."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yT3hBUaplK", "forum": "4t51qDeEq7", "replyto": "4t51qDeEq7", "signatures": ["ICLR.cc/2026/Conference/Submission14138/Reviewer_BRQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14138/Reviewer_BRQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979363371, "cdate": 1761979363371, "tmdate": 1762924601130, "mdate": 1762924601130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a framework that adaptively combines RLHF and RLAIF to mitigate hallucination. The method is evaluated on TruthfulQA and MMLU, compared against baselines including SFT, RLHF, RLAIF, and Static Hybrid."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper propose a framework that adaptively combines RLHF and RLAIF."}, "weaknesses": {"value": "1. The paper appears to be an incomplete version, with fewer than eight pages and overall poor presentation. The description of the method and its corresponding modules is ambiguous and lacks many crucial details. As a result, the proposed approach is difficult to follow, making it challenging to provide meaningful feedback. Including more detailed introduction and clarifying the experimental setup would be necessary for proper evaluation.\n2. I do not see any details regarding the implementation of the human feedback and AI feedback modules. Is the human feedback obtained from actual human annotations or generated by an AI simulator? If it is from human annotations, what is the source dataset? If it is simulated, what model is used? Additionally, what is the format of the feedback (textual or numerical rewards)? If both the human and AI feedback are generated by models and share the same format, what are the concrete differences between the two modules?\n3. The training details are insufficiently explained, making the work difficult to replicate. For example, the authors only mention that “learning rates are method-specific with batch sizes ranging from 4–32 depending on computational constraints,” without providing concrete hyperparameter values in appendix.\n4. The evaluation procedure is also unclear. It is not specified how metrics such as Hallucination Rate, Coherence Score, and Helpfulness are computed. If the authors use an LLM-as-a-judge setting, the implementation details (e.g., judge model, prompt template) should be provided. If human annotation is used instead, the paper should include details about the annotation process or guidelines, as well as the quality control procedures to ensure consistency and reliability.\n5. Too many important details are missing throughout the paper, so I will not list them one by one here.\n6. Only conduct experiements on two benchmarks and one model series.\n7. The figures (e.g., Figure 1) contain very limited information but occupy a large portion of the paper, which affects presentation quality."}, "questions": {"value": "See above section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3gMTOsCOeZ", "forum": "4t51qDeEq7", "replyto": "4t51qDeEq7", "signatures": ["ICLR.cc/2026/Conference/Submission14138/Reviewer_yrFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14138/Reviewer_yrFH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980300124, "cdate": 1761980300124, "tmdate": 1762924600585, "mdate": 1762924600585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}