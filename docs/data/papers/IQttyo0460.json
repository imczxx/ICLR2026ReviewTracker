{"id": "IQttyo0460", "number": 7059, "cdate": 1758006416574, "mdate": 1759897874711, "content": {"title": "E²LoRA: Efficient and Effective Low-Rank Adaptation with Entropy-Guided Adaptive Sharing", "abstract": "As large pre-trained models rapidly scale, Parameter-Efficient Fine-Tuning (PEFT) through methods like Low-Rank Adaptation (LoRA) becomes increasingly crucial.  While LoRA has emerged as a cornerstone of PEFT, excelling at preserving performance with minimal additional parameters, exploring parameter-sharing mechanisms of LoRA remains critical to pushing efficiency boundaries. However, existing naive LoRA sharing methods often degrade performance due to sacrificed representational diversity and weakened model expressiveness. To overcome this issue, we conduct an in-depth analysis of pre-trained models using gradient-based proxy entropy, and uncover two critical, previously overlooked properties: Local Similarity and Layer-wise Information Heterogeneity. Building on these insights, we propose E²LoRA, a novel dual-adaptive sharing framework. It enables adaptive sharing interval partitioning, guided by inter-layer proxy entropy similarity, and adaptive rank allocation, informed by layer-wise absolute proxy entropy. This unique design leverages inherently informative properties of pre-trained models to significantly reduce parameter redundancy while maintaining or enhancing expressiveness. Comprehensive evaluations across diverse tasks, modalities, and models consistently demonstrate that E²LoRA achieves an excellent balance of efficiency and effectiveness, consistently matching or surpassing baselines with approximately 50% fewer trainable parameters.", "tldr": "", "keywords": ["LoRA", "PEFT"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f44d2ad18977e7a693bb1deb9b7524346c0fa54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LoRA and other PEFT methods are efficient, but pushing further with cross-layer sharing or lower ranks often hurts representational capacity and degrades performance. E2LoRA addresses this by analyzing gradients with a proxy-entropy lens and drawing two key insights: local similarity that adjacent layers tend to carry more similar information and are thus suitable for blockwise sharing; and layer-wise heterogeneity, different layers (or intervals) carry very different amounts of information and should receive differentiated capacity (rank). Building on this, E2LoRA proposes a dual-adaptive framework: LSS (Local Similarity Sharing) uses relative mutual information with adaptive thresholds to greedily partition adjacent, information-similar layers into shared intervals and restrict sharing within each interval; HRA (Heterogeneity-aware Rank Allocation) then uses the maximum proxy entropy within each interval to allocate ranks under a fixed overall budget, granting higher ranks to more informative intervals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The “local similarity + layer-wise heterogeneity” perspective operationalized via gradient proxy-entropy and relative mutual information offers a clear mechanism and transfers across backbones and tasks.\n\n2. The dual-adaptive scheme (partition + rank allocation) is nearly plug-and-play with existing LoRA/ShareLoRA pipelines, reducing trainable parameters under a fixed budget while largely preserving (or slightly improving) performance."}, "weaknesses": {"value": "1. While the paper targets both efficiency and effectiveness, the reported gains over baselines are not striking. Many tables show modest uplifts at comparable or lower trainable-parameter budgets.\n\n2. Several LoRA-family methods in 2025 already achieve very low trainable-parameter regimes (e.g., COLM-2025 LoRI). The latest baseline here appears to be DoRA (Feb 2024). Given the pace of PEFT, comparisons should include newer 2024–2025 lines (ICLR/ACL/ICML 2025) to establish contemporaneous relevance.\n\n3. Related work coverage is incomplete for the entropy/gradient/shared-layer thread. The method is positioned as entropy/gradient-guided sharing, yet closely related recent works are missing or under-discussed (e.g., BSLoRA, ICML 2025; LoRA-GA, NeurIPS 2024; LoRA-One, ICML 2025). A more thorough placement of these lines is needed (the 3 papers I mentioned are just an example; the related work is too weak ). \n\n4. RoBERTa-base and ViT-B are relatively small by today’s standards. Although Llama-3.1-8B is included, a stronger LLM sweep (more family/bases, more diverse instruction/reasoning/long-context/code tasks) is important to substantiate generality.\n\n5. Stability/robustness of the entropy-RMI partitioning is under-analyzed. \n\n\n\nIn summary, the paper targets peft via entropy/gradient-guided layer sharing and adaptive rank allocation. The direction is reasonable, the current presentation feels under-polished: the motivation is not sharply articulated, the method reads incremental relative to recent PEFT progress, and the writing/positioning leaves key claims under-substantiated. The paper doesn’t provide a new conceptual lens on PEFT; it reads more as an engineering tweak than a step that changes how we understand parameter sharing."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JWr7hm0bur", "forum": "IQttyo0460", "replyto": "IQttyo0460", "signatures": ["ICLR.cc/2026/Conference/Submission7059/Reviewer_CKkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7059/Reviewer_CKkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396712311, "cdate": 1761396712311, "tmdate": 1762919250796, "mdate": 1762919250796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E²LoRA, a parameter-efficient fine-tuning method based on LoRA. It first analyzes pre-trained models using a gradient-based proxy entropy, identifying \"Local Similarity\" (adjacent layers share information) and \"Layer-wise Information Heterogeneity\" (layers contain different amounts of information). Based on these insights, E²LoRA introduces a dual-adaptive framework involving: 1) Adaptive Sharing Interval Partitioning, grouping similar adjacent layers to share LoRA adapters based on inter-layer entropy similarity , and 2) Adaptive Rank Allocation, assigning LoRA ranks to these shared intervals based on the absolute proxy entropy of the layers within them. The goal is to significantly reduce trainable parameters (claiming approx. 50% reduction) while maintaining or improving performance compared to baseline LoRA and naive sharing methods. Experiments are conducted across NLU, NLG, and image classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed dual-adaptive mechanism, which adjusts both the sharing scope (intervals) and the capacity (rank) based on properties derived from the model itself, is conceptually appealing.\n\n\n2. The paper generally presents its motivation and method clearly. Figure 2 provides a good overview of the framework.\n\n3. The initial analysis using gradient-based proxy entropy to identify Local Similarity and Layer-wise Information Heterogeneity  provides a potentially valuable and principled motivation for designing adaptive parameter sharing strategies, moving beyond naive global or fixed-block sharing. This perspective is somewhat original."}, "weaknesses": {"value": "1. Complexity vs. Benefit: The proposed E²LoRA framework adds significant complexity compared to standard LoRA or simple sharing schemes. It requires an initial phase of gradient computation on a data subset , proxy entropy calculation , Relative Mutual Information (RMI) matrix computation , adaptive thresholding and partitioning , and finally rank allocation based on interval entropy. While the paper claims parameter reduction, the reported performance gains over baselines are marginal (sometimes slightly lower or equivalent on average). This raises questions about whether the added complexity is justified by the benefits\n\n2. Proxy Entropy Robustness: The method heavily relies on gradient-based proxy entropy calculated on a \"mini-subset of the training dataset\". The stability and reliability of this proxy measure are questionable. How sensitive is it to the choice and size of this subset? Does this entropy accurately capture the necessary information across different tasks, modalities, and model states? The paper lacks analysis on the robustness of this core component. Basing adaptive decisions on potentially unstable proxies undermines the method's soundness.\n\n3. Insufficient Comparisons: The comparisons, especially regarding efficiency, could be stronger. For instance, while E²LoRA uses fewer parameters than standard LoRA (e.g., 0.16M vs 0.30M in Table 1 ), the performance difference isn't compelling. A more rigorous comparison would involve evaluating standard LoRA with a reduced rank to match E²LoRA's parameter count (or vice-versa, as done briefly in Table 5  but only for one model/task) across all experiments to isolate the benefit of the adaptive strategy itself versus just using fewer parameters. Comparisons against methods like VeRA, which achieve even lower parameter counts, also show mixed results, with E²LoRA sometimes underperforming despite using more parameters (though comparisons here are complicated by different base ranks).\n\n4. Hyperparameter Sensitivity: The adaptive partitioning uses layer-specific thresholds $\\tau_m$ derived from average RMI 20, and the rank allocation depends on a base rank $r_0$ 21and the total budget calculation22. The sensitivity to these aspects (especially $r_0$ and the implicit assumptions in the thresholding) is not adequately explored."}, "questions": {"value": "1. Could the authors provide details on the computational overhead (time and memory) introduced by the initial gradient computation, entropy/RMI calculation, and partitioning/allocation steps? How does this compare to the overall fine-tuning time?\n\n2. How robust is the proxy entropy measure and the resulting partitioning/ranking to the choice and size of the initial data subset used for gradient calculation? Have the authors experimented with different subsets?\n\n3. Could the authors provide more extensive comparisons where baseline LoRA (and potentially other methods like AdaLoRA, DoRA) are configured to have the exact same number of trainable parameters as E²LoRA across the main benchmark tables (Tables 1, 2, 3)? This would allow for a fairer assessment of the proposed adaptive strategy's effectiveness versus simply reducing rank.\n\n4. How sensitive is the adaptive partitioning algorithm (Algorithm 2 ) to noise or minor variations in the RMI matrix? Does the greedy approach guarantee a near-optimal partitioning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I9EF8aM5I7", "forum": "IQttyo0460", "replyto": "IQttyo0460", "signatures": ["ICLR.cc/2026/Conference/Submission7059/Reviewer_LoWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7059/Reviewer_LoWT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563549694, "cdate": 1761563549694, "tmdate": 1762919250372, "mdate": 1762919250372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E2LoRA, a dual-adaptive Low-Rank Adaptation framework that improves both efficiency and effectiveness via entropy-guided adaptive sharing and rank allocation. \n\nUsing a gradient-based proxy entropy analysis, the authors identify two properties in pretrained models: Local Similarity and Layer-wise Information Heterogeneity. The authors designed mechanisms that dynamically group similar layers and assign ranks proportionally to each interval’s information entropy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel entropy-based view of LoRA sharing and rank allocation.\n2.  Well-motivated and empirically supported through cross-modal experiments.\n3. Simple, modular design compatible with existing LoRA and ShareLoRA."}, "weaknesses": {"value": "1. The theoretical justification of the proposed proxy entropy metric remains largely heuristic and lacks a rigorous connection to established information-theoretic principles.\n2. The empirical analysis across different model architectures and scales is limited, leaving the generality of the proposed insights insufficiently validated.\n3. Certain methodological details, especially regarding entropy computation and notation, require clearer explanations to improve reproducibility.\n4. The performance improvements over strong efficiency-oriented baselines such as LoRA-FA and VeRA are relatively modest in the RoBERTa base and CLIP ViT experiments, which weakens the practical significance of the gains."}, "questions": {"value": "1. Will all models share those properties mentioned in the introduction (e.g., Larger Models (>13B), MoE Models, different series of models(Mistral, QWen, etc))?\n2. Can the proposed adaptive rank allocation be combined with other LoRA variants, such as AdaLoRA, DoRA, and HiRA, and would they be complementary?\n3. Could the proxy entropy approach be extended to other PEFT frameworks beyond LoRA, such as prefix-tuning or adapters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jgib4TYatT", "forum": "IQttyo0460", "replyto": "IQttyo0460", "signatures": ["ICLR.cc/2026/Conference/Submission7059/Reviewer_wUSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7059/Reviewer_wUSM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844084776, "cdate": 1761844084776, "tmdate": 1762919249759, "mdate": 1762919249759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel PEFT method $E^2LoRA$ to decide parameter-sharing groups across layers and allocates LoRA ranks according to a proxy-entropy measure. The approach first estimates layer-wise information and relative mutual information on a small batch of data, then proportionally distributes a global rank budget based on the estimated importance of each group. Experiments across multiple tasks demonstrate competitive or improved performance with reduced trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a simple yet effective framework to jointly decide rank allocation and parameter sharing. The method is plug-and-play and can be applied to both vanilla LoRA and other LoRA variants, showing practical usefulness.\n- This paper provides extensive experiments across multiple modalities and model scales, together with ablations that support the reported stability and generalization."}, "weaknesses": {"value": "The comparison baselines omit several recent rank-allocation and sharing approaches (e.g., [1-5]), and the baselines used across tasks are not fully consistent, some methods appear only in selected experiments. This makes it a little difficult to characterize the contribution and relative advantage of the $E^2LoRA$.\n\n> [1] ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models\n>\n> [2] DyLoRA: Parameter-Efficient Tuning of Pretrained Models using Dynamic Search-Free Low Rank Adaptation\n>\n> [3] IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning\n>\n> [4] RA-LoRA: Rank-Adaptive Parameter-Efficient Fine-Tuning for Accurate 2-bit Quantized Large Language Models\n>\n> [5] HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning"}, "questions": {"value": "- What is the exact mini-subset size used to compute gradients for proxy-entropy estimation? Reporting this and adding a small ablation would strengthen the method's practicality.\n- In Line 228, the interval grouping uses a greedy approach. Computing the optimal intervals should be computationally cheap; have you compared greedy vs. optimal?\n- Is there any theoretical justification for using standard deviation as proxy entropy? A simple counterexample exists where a low-rank gradient matrix has higher variance than a higher-rank one, yet this metric may suggest the opposite.\n- In Table 2, the parameter counts for $E^2LoRA$ on GSM8K and HumanEval appear identical. Please verify. Similarly, Table 4 and Table 5 report seemingly inconsistent parameter numbers (3.66M, 3.61M)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GkNbfYqFEy", "forum": "IQttyo0460", "replyto": "IQttyo0460", "signatures": ["ICLR.cc/2026/Conference/Submission7059/Reviewer_VLBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7059/Reviewer_VLBt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881111049, "cdate": 1761881111049, "tmdate": 1762919249416, "mdate": 1762919249416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}