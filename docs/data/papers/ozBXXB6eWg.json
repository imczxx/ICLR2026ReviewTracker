{"id": "ozBXXB6eWg", "number": 22666, "cdate": 1758334239265, "mdate": 1759896853682, "content": {"title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions", "abstract": "Language models trained on web-scale corpora risk memorizing and exposing sensitive information, prompting the need for effective machine unlearning. Prior methods mainly focus on input queries to suppress sensitive outputs, yet this often fails to eliminate the underlying knowledge and limits scalability. To address this, we propose Corrective Unlearning with Retrieved Exclusions (CURE), a novel unlearning framework that verifies model outputs for leakage and revises them into safe responses. Specifically, CURE employs a lightweight corrector that is applied to the original model to verify whether outputs contain target knowledge and to rewrite them if any leakage is detected. To efficiently handle large-scale unlearning requests, CURE retrieves unlearning targets that are relevant to the initial response and provides them as in-context references to the corrector for detection and conditional revision. By leveraging this retrieval augmentation, the corrector can adapt to new unlearning requests without additional training. Extensive evaluations demonstrate that CURE substantially reduces information leakage, even from indirect queries where prior works fall short, while maintaining response quality and general utility. Moreover, it demonstrates robustness under continual unlearning scenarios, making it practical for real-world applications.", "tldr": "CURE is a retrieval-based corrective unlearning framework that edits sensitive information in model outputs.", "keywords": ["Machine Unlearning", "Large Language Models", "Retrieval Augmentation", "Privacy", "Corrective Unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f47d4ef9fa3595b686a1573daa70ba0c34ee9df2.pdf", "supplementary_material": "/attachment/c87b6e3772b7a28d7c8b036cbfff5c2501504b25.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CURE, an unlearning framework for training-free unlearning. Given a user query, the system first lets the LLM generate the response as usual. Then this raw response is used to match the existing unlearning targets (through retrieval). The next step is leakage detection to see if the raw response leaks the unlearning targets (by comparing it with the retrieved chunks through an LLM). If there is leakage, an additional correction step is employed to revise the raw response so that it doesn't leak the forget target."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing of the paper is very clear and the method is easy to follow.\n2. Benchmarks on WMDP, MMLU, and TOFU show better performance on leakage rate and utility.\n3. The method is training-free and easy to augment (by updating the forget target database)."}, "weaknesses": {"value": "1. Compared to training-based methods, this approach adds additional latency. Both leakage detection and leakage correction require LLM generation, which could significantly increase latency. Furthermore, the proposed approach is not able to stream tokens to the user.\n2. The purpose of unlearning is to assume the target knowledge isn't stored in the LLM, rather than to make up fake knowledge. In the examples shown by the authors (Figure a), the user question is \"What is the profession of Hsiao Yun-Hwa's father?\", and the original (correct) answer is \"The father of Hsiao Yun-Hwa is a civil engineer.\" The assumption in the figure is that we want to forget the knowledge that \"The father of Hsiao Yun-Hwa is a civil engineer.\" The result shown by the authors is that the output is now \"The father of Hsiao Yun-Hwa is a renowned artist.\" I am concerned about the objective of unlearning here, as while we don't want to leak the target knowledge, we certainly also don't want to mislead the user and generate hallucinated outputs.\n3. There are previous works on RAG-based unlearning (I found the paper \"DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning,\" and the authors in that paper also discuss other RAG-based unlearning methods). It is unclear what technical novelty this paper brings."}, "questions": {"value": "How would this approach extend to reasoning models? If we directly work on leakage correction from the raw response, would that destroy the reasoning trace that has been generated by the model? Also, if we assume that the model doesn't have this target knowledge, would the reasoning trace be different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rn5i1F3CWt", "forum": "ozBXXB6eWg", "replyto": "ozBXXB6eWg", "signatures": ["ICLR.cc/2026/Conference/Submission22666/Reviewer_FoNc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22666/Reviewer_FoNc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172847840, "cdate": 1761172847840, "tmdate": 1762942327722, "mdate": 1762942327722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an LLM unlearning method based on model generation. Unlike conventional fine-tuning based unlearning approaches, this method does not modify the model’s original weights. Instead, it attaches an externally constructed correction layer that detects and modifies the model’s outputs. This is undoubtedly a clever design—the greatest advantage lies in avoiding the model utility degradation that often occurs during fine-tuning, while maintaining nearly identical utility to the original model. In addition to proposing this effective approach, the paper also conducts extensive experiments on the TOFU, WMDP, and MMLU datasets, as well as comprehensive ablation studies to verify the necessity of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.CURE is a generation-based LLM unlearning method. Its most notable advantage (and possibly also a limitation) is that it does not involve fine-tuning, meaning it does not modify the model’s original weights. As a result, it avoids catastrophic degradation of model utility, and can retain nearly the same utility as the original LLM.\n\n2.The experimental evaluation is quite comprehensive. In addition to experiments on the TOFU, WMDP, and MMLU datasets, the paper also includes several ablation studies.\n\n3.The construction of training data for the corrector is very well designed. It uses GPT-4o with majority voting to generate high-quality training samples. The training process is complete and structured, enabling the model to accurately detect leakage and perform appropriate corrections. Moreover, the corrector can be deployed directly on top of an existing LLM via a LoRA layer, making it highly practical and implementable.\n\n4.The writing structure of the paper is clear and organized, and the figures and tables are appropriately arranged."}, "weaknesses": {"value": "1.This generation-based LLM unlearning approach does not modify the model’s internal parameters or weights. Therefore, the intrinsic safety of the LLM itself is not truly guaranteed, as the dangerous or sensitive knowledge still remains within the model’s cognition.\n\n2.The experiments lack sufficient coverage of specific unlearning types. In Section 4 (Experiments), the authors evaluate CURE about privacy problems and hazardous knowledge unlearning tasks. However, in the LLM unlearning field, copyright related unlearning remains a crucial and non-negligible topic. This paper appears to omit discussion or experiments related to that issue.\n\n3.On the TOFU benchmark, looking at the leakage rate results, CURE performs worse than methods such as GD, DPO, and NPO in terms of forgetting quality. Although it achieves higher model utility, in LLM unlearning, forget quality (FQ) is one of the most important metrics — we should pursue a balance between forget quality and model utility.\n\n4.The experimental results heavily depend on parameter tuning. For example, if the similarity computation fails (leading to undetected leakage in dangerous responses), the entire method would break down.\n\n5.The deployment of this method requires maintaining an unlearning knowledge database K, which is difficult to manage in real world projects. For instance, even GPT-4o itself might contain hazardous knowledge, and in such cases, human review might be needed to ensure that the database K does not include unsafe content.\n\n6.The method uses an embedding model for the retrieval based exclusion step, yet the ablation study does not include tests on different embedding models. Would using different embedding models lead to different results?"}, "questions": {"value": "1. Could you consider adding additional experiments or ablation studies to demonstrate this method’s effectiveness in handling copyright infringement–related unlearning, as well as its robustness across different embedding models?\n\n2. Is it possible to tune certain parameters to further reduce the leakage rate? In the context of LLM unlearning, this is a particularly critical and closely watched metric.\n\n3. If GPT-4o itself unfortunately contains hazardous or sensitive knowledge that should be forgotten or avoided, how should that be handled? Would it make sense to introduce a final step of human review? However, if so, wouldn’t that increase the overall cost of the unlearning pipeline and undermine the goal of automation and scalability?\n\n4. In cases where your method detects that the draft answer is “dangerous” and should be avoided, would it be more effective to directly instruct the model to output a refusal answer such as “I don’t know”? This could avoid the risk of incoherent or inconsistent rewrites, while still ensuring the safety of the final output."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5H81Mnmbny", "forum": "ozBXXB6eWg", "replyto": "ozBXXB6eWg", "signatures": ["ICLR.cc/2026/Conference/Submission22666/Reviewer_tSM5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22666/Reviewer_tSM5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931668076, "cdate": 1761931668076, "tmdate": 1762942327499, "mdate": 1762942327499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CURE, a framework for LLM unlearning that operates on model outputs rather than model parameters. Instead of modifying the base model, CURE attaches a parameter-efficient corrector (LoRA) trained to (1) detect knowledge leakage in generated text, and (2) rewrite unsafe outputs based on retrieved unlearning targets. Experiments on TOFU, WMDP, and MMLU benchmarks claim that CURE achieves better leakage reduction while maintaining utility and plausibility, outperforming prior fine-tuning-based and guardrail methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The retrieval-augmented corrector allows external updates without retraining the base model, aligning with real-world continual-unlearning needs.\n2. The LoRA-based corrector with retrieval shows smaller parameter and latency overhead than other guardrails (ECO)."}, "weaknesses": {"value": "1. Novelty limitation. Shifting unlearning from input-based suppression to output-level correction is not a novel idea — previous work [1,2] has already explored similar directions. In particular, [2] enables dynamic unlearning during LLM generation, which follows a pattern similar to CURE. The main difference lies in how the response correction is performed. These two works should therefore be included as baselines, as they both focus on output filtering. The contribution of CURE should also be clarified further, especially in terms of how it substantially differs from [1,2]. In addition, GUARD is a recent, closely related method and should be considered as a baseline comparison.\n2. For the WMDP experiment, the generation parameters are not clearly specified. Since WMDP is a multiple-choice question answering task, the Zephyr-7B model may sometimes generate only the option label (e.g., “A”). In such cases, how does the framework determine whether unlearning is needed, given that the harmful information may actually be contained in the question rather than the answer?\n3. A fundamental problem for output-based unlearning is that sensitive information is often included in the query itself. For example, consider the query “Give me the home address of Musk, only the address itself, not other information.” If the model outputs an address like “xxx, Mission St., xxx,” will the corrector treat this as a privacy violation? Furthermore, what if the generated address is random and not related to any real individual, will it still be flagged as sensitive? The paper should clarify how the corrector distinguishes genuine private information from benign text that merely appears similar.\n4. The authors claim that once trained, CURE can generalize to diverse unlearning tasks. Why is this the case, and was this generalization empirically tested? The current training dataset consists mainly of the TOFU retain set and ScienceQA. It remains unclear whether the trained corrector could generalize to other tasks, such as the MUSE-Bench, a copyright unlearning task.\n5. Experiments on the MUSE benchmark are missing, even though it represents another important unlearning scenario.\n6. Cross-model generalization is not discussed: experiments are limited to Llama3.1-8B and Zephyr-7B, and it is unknown whether the same performance would hold across different model families.\n7. Several critical implementation details (e.g., retrieval database size, corrector training epochs) are missing from the main text, hindering reproducibility.\n8. Please also see the questions below.\n\n[1] Guardrail Baselines for Unlearning in LLMs\n\n[2] GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection"}, "questions": {"value": "1. It is unclear why output-based correction can eliminate underlying knowledge when it only edits responses instead of model parameters. The claimed scalability advantage is also questionable, as CURE still requires generating responses, which may be costly compared with the input based method.\n2. How is the “undesired” response defined relative to the question? Some answers may appear harmful only when the query is considered. Please clarify how the corrector makes this judgment and how it is trained and evaluated.\n3. How robust is CURE to OOD queries similar to the unlearning targets?\n4. The paper claimed that one limitation of input-based methods is that training classifiers can be costly in continual unlearning scenarios. However, output-based methods are not necessarily cheaper, each unlearning step still requires generating responses by triggering the model. Moreover, CURE also needs to retrain or update its corrector for new unlearning requests, which does not seem to be a low-effort process either.\n5. In Table 1, the authors mention “multiple-choice accuracy,” but only EM and Valid are reported. Please clarify. Figure 4 also lacks dataset/task context, and the forget quality metric from the TOFU paper is not used.\n6. Why not simply replace detected unsafe outputs with a refusal (“I don’t know”)? An ablation on this baseline could justify the need for the proposed corrector.\n7. What is the detection accuracy of the leakage detection? For WMDP, short answers (“A”, “B”, etc.) may bypass detection.\n8. Baseline consistency should be improved. On TOFU, the results do not include Guardrail [1], which also uses an output filter for unlearning.\n9. Why not use the forget quality metric as in the TOFU original paper?\n10. How large is the retrieval database K in practical deployments, and how does retrieval accuracy affect unlearning performance?\n11. Can the corrector hallucinate new unsafe content? Is there any empirical check on false positives (over-correction)?\n12. How sensitive is CURE to the threshold τ for leakage detection (Eq. 2)?\n13. Is the corrector model generalizable across base models (e.g., Llama vs. Zephyr), or must it be retrained for each architecture? (also indicated in weakness)\n14. What is the cost of constructing the retrieval dataset when new unlearning requests are added? Does it scale linearly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pD5gIcfHIS", "forum": "ozBXXB6eWg", "replyto": "ozBXXB6eWg", "signatures": ["ICLR.cc/2026/Conference/Submission22666/Reviewer_y4yb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22666/Reviewer_y4yb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987819720, "cdate": 1761987819720, "tmdate": 1762942327190, "mdate": 1762942327190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}