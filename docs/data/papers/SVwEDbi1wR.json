{"id": "SVwEDbi1wR", "number": 13566, "cdate": 1758219277835, "mdate": 1759897428043, "content": {"title": "Who Routes the Router: Rethinking the Evaluation of LLM Routing Systems", "abstract": "The growing ecosystem of Large Language Models (LLMs) with diverse capabilities and costs has motivated the need for LLM routing systems that dynamically select the most appropriate model for each query. Evaluating these routing systems is important yet inherently challenging due to the complex interplay of multiple factors: the selection of representative input queries, the composition of the model pool, and the definition of comprehensive evaluation metrics for optimal routing decisions. Through extensive analysis of existing benchmarks, we identify critical limitations that may lead to incomplete results and/or misleading conclusions about router performance: \n(1) limited task diversity, (2) imbalanced model pools, and (3) oversimplified evaluation methodologies. To address these limitations, we propose a novel evaluation framework that incorporates diverse task distributions (33,337 queries across 68 categories), a balanced model pool of 85 models with complementary model strengths, and multi-faceted metrics that reflect real-world deployment scenarios. \nWe implement this framework as an open-source benchmark, enabling researchers to rigorously assess routing strategies under realistic conditions. The code and dataset are shared anonymously at: https://anonymous.4open.science/r/rethinking-routing-evaluation-DE30", "tldr": "We present an open, reproducible evaluation framework that addresses key limitations in current router evaluations, such as limited task diversity, imbalanced model pools, and oversimplified metrics.", "keywords": ["LLM router", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dcc0168efd4a72f4e10c2950ee90fdf42d03167a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present flaws (redundant tasks, redundant and dominant models, lack of multi-faceted evaluation) with current router evaluation practices. They then present a remastered evaluation of the methods in embedLLM and some binary routing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Each of the identified issues with LLM evaluation are (to me) valid problems with how we measure router performance** \n\n* Many Routing datasets are simple blends of general reasoning, knowledge and math benchmarks which leaves out many important applications.\n* Often routers are trained on datasets where one model can dominate. For example in CARROT, o3 mini is by far the best model at most tasks.\n* As far as I am aware, routing benchmarks have not looked into issues such as OOD generalization.\n\n**The remastered llm evaluation addresses some of these issues**\n\n* The authors subsample EmbedLLM tasks to highlight those that benefit from non-generalist models (the authors also remove duplicate LLM queries)\n* The authors inject pseudo-specialist models\n* The authors hold out certain categories of query to test for ood generalization"}, "weaknesses": {"value": "* The set of ``remastered\" evaluations feels limited. A re-analysis of EmbedLLM is completed, as is a redone analysis of binary routing. It would be interesting to see how other predictive routing evaluations such as those in CARROT or routerbench are effected by the identified flaws. CARROT in particular has its own data set, which may have more variety than that in embedLLM.\n* Certain ``solutions\" to the proposed flaws feel limited. For example, the injected specialist models are just normal models with artificially boosted scores, not models trained for specific domains that are then incorporated in the evaluation.\n* Most critically, its not clear to me how much the proposed changes affect evaluation results. The main text of the paper should include an analysis of how each the changes to embedLLM (question diversity, expert ) effect the performance of each routing method (and in particular the ordering of each method)."}, "questions": {"value": "* How does the analysis extend to other predictive multi llm routers?\n* How do the proposed changes effect the original embedLLM results? Which of the faults is most important to reduce?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I7W5nS836I", "forum": "SVwEDbi1wR", "replyto": "SVwEDbi1wR", "signatures": ["ICLR.cc/2026/Conference/Submission13566/Reviewer_E1wn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13566/Reviewer_E1wn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006704435, "cdate": 1761006704435, "tmdate": 1762924165674, "mdate": 1762924165674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies an interesting problem: how to choose the proper LLM to use given a user query. Although there has been a line of research and analysis of existing benchmarks, this paper identifies some critical limitations that may lead to incomplete results and misleading conclusions, such as the existing benchmarks have limited task coverage or skewed on some specific tasks, and the model pools are too large and imbalanced. To address this issue, this work proposes a new evaluation framework. Empirical results have been provided to showcase the limitations of existing evaluation strategies and the advantages of the new benchmarks proposed in this work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work is overall well written and easy to follow.\n2. The empirical results are quite complete and coherent. \n3. This work studies an important problem: how to fairly evaluate the existing LLM routers, and I think this line of research is important."}, "weaknesses": {"value": "I do not find any major weaknesses of this work, while I am not very familiar with existing literature so I may refer to other reviewers' opinions.\n\n1. I feel it might be better to also consider the influence of the embedding models on the final performance of the routers. \n2. It is also quite surprising that the kNN-based methods outperform the trained MLPs, especially on OOD tasks. I suspect this is because the retraining dataset is relatively limited, preventing the MLPs from fully converging. To verify this hypothesis, it might be useful to include a simple parametric baseline such as linear regression as a router. If the linear model were to outperform the MLPs, it would suggest that the training dataset is indeed insufficient for proper convergence. Could you please elaborate on this issue?\n3. The pseudo model is not very realistic, and it is better to use some fine-tuned specific models instead for each small task."}, "questions": {"value": "1. I had a hard time understanding Figure 5 (a), as the X-axis is the model parameter. Can you explain that to me? And how model parameters affect your models' accuracy, as shown in the curves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XeXmSprbL7", "forum": "SVwEDbi1wR", "replyto": "SVwEDbi1wR", "signatures": ["ICLR.cc/2026/Conference/Submission13566/Reviewer_3m3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13566/Reviewer_3m3M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546172564, "cdate": 1761546172564, "tmdate": 1762924165185, "mdate": 1762924165185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the limitation of previous LLM routing benchmarks from three perspectives: (1) limited task diversity, (2) imbalanced model pools, and (3) oversimplified evaluation methodologies. Given that, this paper proposes a novel evaluation framework that incorporates diverse task distributions, a balanced model pool of 85 models with complementary model strengths, and multi-faceted metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. How to properly evaluate LLM routing approaches is an important topic in current research on LLM serving systems.\n2. This paper proposes a novel benchmark with 33k queries across 68 categories, which serves as a concrete foundation for effective LLM evaluation.\n3. This paper illustrates the technical developments with sufficient justification."}, "weaknesses": {"value": "1. Some advanced routing work is neither compared nor discussed. For example,\n    1. Ding, Dujian, et al. \"BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute.\" Forty-second International Conference on Machine Learning.\n2. This paper primarily leverages binary label & BARTscore as the quality metrics for LLM responses, which seems limited for open-ended conversation — the mainstream LLM service scenarios.\n3. In the evaluation section, the performance results are primarily reported on Llama2 models (fig 3 & 6) which are often considered as outdated models given the presence of Llama3 herd and Llama4 family."}, "questions": {"value": "1. Majority of the evaluation examples are just one-turn query. However, the daily LLM usage typically happens with multi-turn conversation. Can the proposed benchmark to evaluate LLM routing performance in the multi-turn conversation scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nQCZ2LKiCk", "forum": "SVwEDbi1wR", "replyto": "SVwEDbi1wR", "signatures": ["ICLR.cc/2026/Conference/Submission13566/Reviewer_9rhj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13566/Reviewer_9rhj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172711554, "cdate": 1762172711554, "tmdate": 1762924164164, "mdate": 1762924164164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses fundamental flaws in current benchmarks for evaluating systems that dynamically route queries to the most suitable large language model (LLM). Existing frameworks often suffer from limited task diversity, imbalanced model pools, and oversimplified metrics, leading to misleading conclusions about router performance. To overcome these issues, the authors propose RouterBench+, a comprehensive evaluation framework featuring (1) a diverse and realistic task distribution of 33,337 queries across 68 categories, (2) a balanced pool of 85 models with complementary strengths, and (3) modified metrics that better capture the cost-performance trade-offs and out-of-distribution robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper accurately identifies important flaws in existing router evaluation pipelines. Lack of task diversity, dominance by a single model, and poor assessment of OOD performance are all major issues that make existing evaluation pipelines unsuitable for real-world scenarios.\n\n2. The solutions are clearly and systematically presented, with experiments and plots illustrating their efficacy, and overall, the paper is easy to follow."}, "weaknesses": {"value": "1. The removal of duplicate queries leads to very small improvements for learning based routers and causes a drop in performance in for clustering-based routers. Thus, the claim in lines 268-269 that \"duplicate queries with conflicting labels can mislead routers\" is not well substantiated by the results.\n\n2. The role of the pseuod-specialist models in Section 4.3 is not clear. If they are not meant for deployment, how does adding them provide any benefit? If a router is selected based on its performance in the presence of pseuod-specialist models, but does not have access to those models when it is deployed, how will it be able to replicate that performance?\n\n3. It is not clear how the binary routing evaluation paradigm in Fig 3 is helpful in evaluating routers in multi-LLM settings.\n\n4. It is not clear how the benchmark incorporates cost-performance trade-offs, latency constraints and reliability, as claimed in line 420"}, "questions": {"value": "1. Please give some examples of common-sense and domain-specific tasks to better illustrate the difference between the two.\n\n2. How is the generalist model and the set $\\mathcal{M}_{\\text{non-gen}} $ chosen from a given set of models? \n\n3. The expression in line 237 suggests that the score is the difference between specialist and generalist accuracy but the caption of figure 2 says it is the difference between the specialist and the heuristic router. Which is it?\n\n4. What is the dimensionality of the embeddings $\\mathbf{e}_i$, $\\mathbf{e}_j$ in line 263? A high dimensionality may lead to inconsistent results due to the curse of dimensionality.\n\n5. How will sub-sampling tasks in EmbedLLM help (Section 5.1) if the dataset has very few specialist tasks to begin with?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S6bQQLvYZt", "forum": "SVwEDbi1wR", "replyto": "SVwEDbi1wR", "signatures": ["ICLR.cc/2026/Conference/Submission13566/Reviewer_gUhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13566/Reviewer_gUhH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240130829, "cdate": 1762240130829, "tmdate": 1762924163431, "mdate": 1762924163431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}