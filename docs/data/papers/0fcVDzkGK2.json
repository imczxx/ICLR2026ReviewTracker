{"id": "0fcVDzkGK2", "number": 10873, "cdate": 1758183743243, "mdate": 1759897623266, "content": {"title": "DIVIDE-AND-DENOISE: A GAME THEORETIC METHOD FOR FAIRLY COMPOSING DIFFUSION MODELS", "abstract": "The widespread availability of large-scale pre-trained generative models raises a\nquestion: how can we best leverage them beyond their original training distribu-\ntions? Two strategies provide partial answers. Composition combines multiple\ndiffusion models, typically through linear averaging of their predictions, to pro-\nduce out-of-distribution samples. Guidance steers a single model by biasing its\ngeneration with rewards or classifier scores. We unify these perspectives with\nDivide-and-Denoise, a game-theoretic approach to compositional sampling from\nmultiple pre-trained diffusion models, coordinated through an allocation flow. At\neach denoising step, we alternate between (i) partitioning the sample into regions\nassigned to distinct models for denoising (composition) and (ii) aligning the sam-\nple with this division (guidance). The partition is determined by solving a fair al-\nlocation problem under a shared alignment objective. We evaluate our method on\ntext-to-image generation. Using models conditioned on different prompts, Divide-\nand-Denoise reliably generates images that capture the semantics of each prompt,\neven surpassing joint-prompt conditioning. On the GenEval benchmark, it further\noutperforms energy-based composition and joint prompting baselines, resolving\ncommon issues such as missing objects and attribute mismatches.", "tldr": "a game-theoretic approach to compositional sampling from multiple pre-trained diffusion models", "keywords": ["Diffusion Models", "Fair Composition", "Game-Theoretic", "Text-to-Image"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9322bfe71458447dedd651465f55a6419253caea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a game-theoretic approach to combine multiple different text-to-image diffusion models. A crucial constraint is that the models must have the same latent dimensions. Ensuring fairness when dividing the noise maps among the models prevents the collapse into a single concept and only generating this concept. The experimental results show that the approach doesn't suffer from collapse to a single concept and can generate all the objects present in the prompt using different models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of fusing different models to generate an image is very intriguing.\n- The paper is well written, and even the mathematical details are easy to understand."}, "weaknesses": {"value": "- The figures in the paper have low resolutions. The text in the images is not readable.\n- A few more example images would be nice to illustrate what makes this approach better than other approaches.\n- The evaluation is not very thorough. For example for the generation of multiple objects and the attribute allocation only figure 3 is shown as evidence.\n- It is not clear why the prompts used in Section 4.3 are out-of-distribution.\n\nMinor:\n- In line 404, 412 and 413 the citations seem to be missing.\n- The figure number in line 423 is not correct\n- In line 466 the table number is not correct"}, "questions": {"value": "Q1: I might have missed it, but how is the fairness ensured when dividing the pixels to the models?  \nQ2: Why do the pixels have to be distributed to a fixed model? Wouldn't it also be possible, especially when two areas overlap, to average over the noise maps of multiple models?  \nQ3: How does VQA measure the compositional correctness? If I am not mistaken, an image can be composed in different ways, while the VQA can still be correct.  \nQ4: Why are there values missing for VQA in table 1?  \nQ5: Why are the prompts used in Section 4.3 OOD? What does it mean if there are \"conflicts between individual prompts\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hY8jwanFEY", "forum": "0fcVDzkGK2", "replyto": "0fcVDzkGK2", "signatures": ["ICLR.cc/2026/Conference/Submission10873/Reviewer_9aeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10873/Reviewer_9aeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842651969, "cdate": 1761842651969, "tmdate": 1762922087474, "mdate": 1762922087474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Divide-and-Denoise” proposes a game-theoretic framework for compositional sampling from multiple pre-trained diffusion models. Rather than directly averaging denoising predictions (as in MultiDiffusion or joint-prompt methods), the authors formulate the problem as a fair division game, where latent coordinates are “goods” and each diffusion model acts as a “player.” This elegant formulation allows the model to dynamically allocate spatial responsibility among different diffusion processes in a principled, temporally coherent, and fairness-aware way.\n\nThe method alternates between two tightly coupled updates at each diffusion step:\n\t1.\tCompositional denoising, which generates a latent proposal based on soft region assignments Q_t;\n\t2.\tDynamic allocation, which optimizes Q_t via a bilevel optimization that enforces fairness, smoothness, and attention alignment across time.\n\nA key novelty is the introduction of the alignment score derived from cross-attention maps, which measures semantic consistency between denoised regions and textual prompts. The paper also introduces a “fictitious player” to handle unassigned or background regions, ensuring that all latent coordinates are properly modeled. Theoretical analysis leads to a closed-form softmax-like solution for Q_t (Theorem 2), while alternating optimization jointly refines both the denoising kernel and spatial allocation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Conceptual originality: The use of game theory and fair division in diffusion model coordination is highly innovative and goes beyond heuristic compositional fusion.\n\n(2) Theoretical rigor: The bilevel formulation, connection to entropy-regularized MDPs, and derivations (Theorems 1–2) are mathematically sound and clearly motivated.\n\n(3) Strong empirical performance: On multi-object and attribute-binding tasks, Divide-and-Denoise significantly reduces object overlap and color confusion, outperforming joint-prompt and MultiDiffusion baselines."}, "weaknesses": {"value": "(1) Computational overhead: Alternating updates for Q_t and p_t^c introduce nontrivial cost during inference.\n\n(2) Dependence on cross-attention quality: The allocation accuracy relies heavily on stable and interpretable attention maps.\n\n(3) Limited evaluation scope: Current experiments are restricted to text-to-image synthesis; demonstrating broader modality coverage would further strengthen the claim of generality.\n\n**Important** (4 )Many figures are blurry, making them nearly unreadable. Several references are missing or incorrectly formatted, which severely reduces the paper’s professionalism and readability."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GG0KrGioVr", "forum": "0fcVDzkGK2", "replyto": "0fcVDzkGK2", "signatures": ["ICLR.cc/2026/Conference/Submission10873/Reviewer_roNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10873/Reviewer_roNH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896523421, "cdate": 1761896523421, "tmdate": 1762922087106, "mdate": 1762922087106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle compositional generation with diffusion models by introducing \"Divide-and-Denoise\", a game theoretic sampling procedure that composes multiple pretrained diffusion model \"player\" models via fair division of the latent space at every denoising step. The method alternates between (i) an allocation step that infers soft segmentations by solving a fairness-constrained optimization using utilities derived from cross-attention maps, and (ii) a denoising step whose optimal Gaussian kernel has a mean that combines per-model updates masked by the allocation plus a guidance term driven by an alignment score; a fictitious background player and a KL term encourage sensible coverage and temporal smoothness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- ***Interesting & principled idea***: Recasts compositional generation as a fair-division game over soft region allocations, using cross-attention."}, "weaknesses": {"value": "- ***Writing quality***: The paper appears incompletely prepared at submission time. In the experiments section there are placeholder “?” citations, tables that overflow horizontally, and tables with missing entries. The manuscript also exceeds the 9-page limit, suggesting the writing and formatting were not finalized. These presentation issues significantly hinder readability and raise concerns about diligence in preparing the submission.\n\n- ***Experimental setups***: The Joint Prompt setup appears to be an extremely weak baseline. With such a simple enumeration-style prompt, the model has a high probability of failure. Instead, the authors should compare results when using a language model to generate natural prompts containing multiple objects. In the same vein, averaging is also far too simple as a baseline. It seems strange to expect that averaging score values from different conditions would work well.\n\n- ***Prompt division***: This paper focuses on effectively dividing and combining generation from multiple players, yet it doesn't address how to divide the conditions among them. For example, if there's a long prompt, there is a need to determine how to distribute its contents to each player. With the current approach, I have serious doubts about whether this can work for scenarios with complex multiple relations.\n\n- ***The title of Section 4.3***: I don't understand why this is considered out-of-distribution at all. Wouldn't \"conflict prompt\" be more appropriate?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5zG3M4XOSY", "forum": "0fcVDzkGK2", "replyto": "0fcVDzkGK2", "signatures": ["ICLR.cc/2026/Conference/Submission10873/Reviewer_nmYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10873/Reviewer_nmYV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944145088, "cdate": 1761944145088, "tmdate": 1762922086775, "mdate": 1762922086775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}