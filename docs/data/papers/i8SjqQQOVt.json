{"id": "i8SjqQQOVt", "number": 6530, "cdate": 1757987925417, "mdate": 1759897909560, "content": {"title": "RACER: Retrieval-Augmented Contextual Rapid Speculative Decoding", "abstract": "Autoregressive decoding in Large Language Models (LLMs) generates one token per step, causing high inference latency. Speculative decoding (SD) mitigates this through a guess‑and‑verify strategy, but existing training-free variants face trade‑offs: retrieval‑based drafts break when no exact match exists, while logits‑based drafts lack structural guidance. We propose **RACER** (**R**etrieval‑**A**ugmented **C**ont**e**xtual **R**apid Speculative Decoding), a lightweight and training‑free framework that integrates retrieved exact patterns with logit‑driven future cues. This unification supplies both reliable anchors and flexible extrapolation, yielding richer speculative drafts. Experiments on Spec-Bench, HumanEval, and MGSM demonstrate that RACER consistently accelerates inference, achieving a speedup of $2.2{\\sim}2.8\\times$ compared to autoregressive decoding, and outperforms prior training-free methods, offering a scalable, plug-and-play solution for efficient LLM decoding. Our source code is available at [this anonymous repository](https://anonymous.4open.science/r/racer_anonymous-9464).", "tldr": "RACER is a lightweight, training-free speculative decoding method that combines retrieval anchors with logit cues to accelerate LLM inference by 2.2–2.8× while outperforming prior methods.", "keywords": ["Large Language Models", "Speculative Decoding", "Inference Acceleration", "Training-Free Optimization"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e5ad0407d6035d0f19459bea08ca20d80064a71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RACER (Retrieval-Augmented Contextual Rapid Speculative Decoding), a lightweight\nand training-free framework that integrates retrieved exact patterns with logit-driven future cues. Experiments on Spec-Bench, HumanEval, and MGSM demonstrate that RACER consistently accelerates inference and can achieve 2.2~2.8x speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is technically sound and easy to understand.\n\nThe experimental results show the effectiveness of the proposed method."}, "weaknesses": {"value": "The paper focuses on generating tree structure and improve the overall MAT to speedup the large language model.\n\nHowever, they only conduct experiments with HuggingFace Transformers framework. Here comes a problem that the method may not have such speedup on the popular inference framework such as vLLM. In fact, HuggingFace Transformers framework does not optimize the speed of LLMs very well, which makes the ratio of the latency of tree generation process smaller. When using vLLM framework where the operations in LLMs are optimized very well, the tree generation process will take more time and reduce the speedup.\n\nThe author should verify their method on such inference frameworks to show that their method is actually useful in reality."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUnK3uuPvw", "forum": "i8SjqQQOVt", "replyto": "i8SjqQQOVt", "signatures": ["ICLR.cc/2026/Conference/Submission6530/Reviewer_M1gZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6530/Reviewer_M1gZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602094190, "cdate": 1760602094190, "tmdate": 1762918885817, "mdate": 1762918885817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RACER, a training-free decoding acceleration framework that combines two types of speculative signals: (1) a logits tree constructed using a copy-logits strategy to extrapolate high-probability tokens, and (2) a retrieval tree maintained by an AC automaton to leverage repeated patterns in the context. These two sources are integrated into a unified speculative draft, which is then verified using standard speculative decoding. The paper reports empirical results on several benchmarks, showing that RACER achieves a 2.2-2.8x speedup over standard autoregressive decoding and outperforms other training-free baselines such as TokenRecycling and LogitSpec."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and practical problem: accelerating LLM decoding without additional training. This is highly relevant for both research and deployment.\n\n2. The proposed method integrates retrieval-based and logits-based drafting in a unified framework, which is conceptually simple and compatible with existing speculative decoding systems.\n\n3. The method achieves consistent speedups across tasks and shows robustness with respect to hyperparameters such as draft size, retrieval tree capacity, and n-gram depth."}, "weaknesses": {"value": "1. **The paper is poorly written.** For example, the main contributions are the use of copy-logit for building the logits tree and the use of an AC automaton for maintaining the retrieval tree. However, the entire article uses only one sentence to explain “copy-logit”, and that sentence is not understandable (see Weakness 2). In addition, the acronym “LRU” is explained only the third time it appears, which is not reader-friendly. Moreover, **there are many very short paragraphs with only 2 or 3 lines** (e.g., lines 162–167 and 216–222), which makes the paper look unprofessional. There is also a missing period at the end of line 185.\n\n2. The definitions of the last-logit and copy-logit strategies are unclear. In particular, the meaning of “the same token” in line 125 is ambiguous; the paper should specify what the token is “the same as”. It is strongly recommended to explain these concepts more clearly. In Figure 1, the differences between white and green circles, as well as between solid and dashed arrows, should be explicitly explained in the figure caption.\n\n3. The node index definition in lines 165–167 is non-standard. Using proper mathematical symbols would make it clearer and more precise.\n\n4. The choice of the MGSM-ZH dataset is somewhat questionable. From Table 1, it seems this dataset was chosen mainly because EAGLE-3 performs poorly on it, allowing RACER to surpass EAGLE-3 on average. However, the reproduced EAGLE-3 model was not trained on Chinese data, making this comparison problematic. If my guess is true, I don't think there's any need to be so secretive. It's normal for retrieval-based methods to not outperform EAGLE-3, and it would be strange if they could.\n\n5. The novelty is somewhat weak. The authors claim three contributions. The copy-logit strategy is not clearly explained. The retrieval tree with AC automaton is acceptable, but the integration is rather trivial."}, "questions": {"value": "1. How do copy-logit and last-logit work? In Figure 1, what is the difference between the white circles and the green ones? What is the difference between a solid arrow and a dashed arrow?\n\n2. What is b in Equation 3? Is it the same as k in a k-ary tree?\n\n3. Why is the experiment done on MGSM-ZH, which is not popular in reasoning tasks? It is advised to do the experiments on more popular datasets, such as the original dataset of MGSM-ZH, GSM8K, or some other popular datasets such as AIME and AMC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X5zJrLDH0h", "forum": "i8SjqQQOVt", "replyto": "i8SjqQQOVt", "signatures": ["ICLR.cc/2026/Conference/Submission6530/Reviewer_JwCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6530/Reviewer_JwCs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960607688, "cdate": 1760960607688, "tmdate": 1762918885445, "mdate": 1762918885445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RACER, a training‑free framework that integrates retrieval-based speculative decoding (SD) with logit‑based drafts. Experiments on Spec-Bench, HumanEval, and MGSM demonstrate that RACER consistently accelerates inference, achieving a speedup of compared to autoregressive decoding, and outperforms prior training-free methods, offering a scalable, plug-and-play solution for efficient LLM decoding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This work presents a valuable exploration to integrate retrieval-based SD methods with logit-based drafting. It provides an insightful idea that SD should not only leverage retrieval contents, which provide seen information through exact pattern matches, but also logit-based contents, which supply unseen information for future tokens.\n2. The proposed method is assessed on a wide range of text generation benchmarks, including Spec-Bench, HumanEval, and MGSM. The evaluated models cover the Vicuna series and Qwen3 series.\n3. RACER consistently accelerates inference, achieving a speedup of 2.2x-2.8x compared to autoregressive decoding, and outperforms prior training-free methods, such as token recycling, logitspec, PLD, and REST."}, "weaknesses": {"value": "1. **Comparison with LogitSpec**: A similar idea of combining retrieval-based SD with logit-based drafts has been proposed in LogitSpec [1], which appears to be the most closely related work to RACER. A more thorough comparison between LogitSpec and RACER is needed to better highlight this paper’s contributions.\n2. **Writing Refinement**: The manuscript would benefit from significant improvements in writing clarity. For example, Section 3.1 fails to clearly explain the “copy-logit strategy.” Although I carefully reviewed Lines 124–126 and Figure 1, the explanation lacks sufficient detail. Given that the copy-logit strategy is a central contribution of the logit-based drafts, the lack of a clear and detailed description creates confusion for the reader.\n3. **Insufficient Methodological Details**: (1) In Figure 1, the relationship between frequency (y-axis) and the Mean Accepted Tokens (MAT) is unclear. (2) The term “failure link” in Lines 213–214 is not well defined. (3) Section 3.2 omits several critical details. For instance, what specific retrieval method is employed? Additionally, the update rule described in Lines 216–219 is presented too briefly, without adequate explanation.\n4. **Lack of Experimental Clarity**: It is unclear what the maximum number of draft tokens is for each SD step. Among these draft tokens, what is the proportion of retrieval tokens versus logit tokens? It is also confusing that Line 425 states the draft size ranges from 16 to 64, while the maximum breadth of the logit tree is 8, and the retrieval tree contains up to 10,000 nodes with an n-gram length of 10. The relationships among these parameters should be clearly clarified.\n5. **Missing Baseline Comparison**: Why is RACER not compared against EAGLE-3 using the Vicuna series? \n6. Table 2 illustrates that the logit-based drafting contributes more to the efficiency of RACER, while the improvement from the retrieval part seems to be much smaller. The contributions and necessity of each designed module should be further analyzed."}, "questions": {"value": "1. The font size in Figures 1 and 2 is too small and difficult to read.\n2. The phrase “P50 and P85 quantiles” in Lines 148–149 could be revised to “50th and 85th percentiles” for clarity and consistency.\n3. A grammar issue: the paragraph spanning Lines 183–186 ends without a period.\n\n\n[1] LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation. Liu et al. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P21rM1VwCs", "forum": "i8SjqQQOVt", "replyto": "i8SjqQQOVt", "signatures": ["ICLR.cc/2026/Conference/Submission6530/Reviewer_k6pL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6530/Reviewer_k6pL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923319178, "cdate": 1761923319178, "tmdate": 1762918885084, "mdate": 1762918885084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper unifies logit-based and retrieval-based training-free speculative decoding within a single framework. Experimental results demonstrate superior performance compared to either approach used independently."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper unifies two subcategories of training-free speculative decoding, achieving a better trade-off between acceptance rate and inference efficiency.\n2.\tThe methodology and presentation are clear and easy to follow, making the approach straightforward to reproduce."}, "weaknesses": {"value": "1.\tThe experiments primarily focus on a batch size of 1, which limits the generality of the throughput evaluation and may not reflect performance in higher-throughput settings.\n2.\tThe main contribution appears to be the integration of two existing methods, offering limited novelty and conceptual insight beyond their combination."}, "questions": {"value": "Throughput scaling: Could you report results for batch sizes > 1 to assess how throughput and latency scale beyond the single-batch setting?\n\nModel-size ablation: The current results suggest the method may benefit more on larger models. Since most evaluations are on small–to–medium scales, could you include an ablation across model sizes to quantify how effectiveness changes with model capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d4Ml00KXFc", "forum": "i8SjqQQOVt", "replyto": "i8SjqQQOVt", "signatures": ["ICLR.cc/2026/Conference/Submission6530/Reviewer_9qLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6530/Reviewer_9qLi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762265331288, "cdate": 1762265331288, "tmdate": 1762918884616, "mdate": 1762918884616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}