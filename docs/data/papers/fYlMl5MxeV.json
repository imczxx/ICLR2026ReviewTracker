{"id": "fYlMl5MxeV", "number": 16307, "cdate": 1758262961677, "mdate": 1763723998466, "content": {"title": "Calibration-Free Defense Against Backdoor Attacks in the Wild", "abstract": "The widespread adoption of pre-trained neural networks from unverified sources has heightened concerns about backdoor attacks. These attacks cause networks to misbehave on inputs containing specific triggers while maintaining normal performance otherwise. Existing methods typically rely on pruning, operating under the assumption that backdoors are encoded in a small set of specific neurons. This approach, however, is ineffective on large-scale models where phenomena like polysemanticity make isolating malicious neurons without harming model performance difficult. Furthermore, pruning-based methods are impractical as they require unavailable calibration data to determine critical thresholds, limiting their deployment in real-world scenarios. We introduce Calibration-free Model Purification (CMP), a novel, completely data-free defense that avoids pruning entirely. CMP leverages a self-distillation framework guided by our discovery of a systematic \"prediction skew\" as the fundamental mechanism for backdoor transfer during knowledge distillation. It employs a dual-filtering system that counteracts this skew, preventing the student model from inheriting the teacher's malicious behavior. On the challenging ImageNet dataset, CMP reduces attack success rates to near-zero across diverse attacks while preserving clean accuracy, outperforming existing methods. Our work presents the first scalable, threshold-free defense, offering a practical solution for real-world AI security.", "tldr": "We propose CMP, where CMP is the first backdoor defense method to be: 1. Truly Data-Free without threshold. 2. The only algorithm which works in Practical Scale.", "keywords": ["Backdoor Defense", "Backdoor Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bda8e63868b5dd85bde0dd24a0063a5820e5ce5f.pdf", "supplementary_material": "/attachment/47b3cb787bbc729049478df8f9fd37ca8d1bc886.pdf"}, "replies": [{"content": {"summary": {"value": "Exisiting backdoor defense methods rely on pruning with the assumption that backdoors are encoded in a small set of specific neurons. This paper argues against this assumption for tis ineffectiveness on large-scale models and proposes Calibration-free Model Purification. It avoids purning entirely and leverages a self-distillation framework guided by the discovery of a systematic \"prediction skew\". A dual filtering system is employed. CMP reduces attack success rates on diverse attacks while preserving clean accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation of the work is very interesting. The authors identify two fundamental limitations from exisiting defenses, including assumption on backdoors encoded in a small subset of identifiable neurons and unavailability due to requring training resources form threshold calibration. Then a calibration-free model purification method is proposed. I think these observations are helpful to this area.\n- The proposed CMP method employs two filters. They are reasonable and easy to implement. Across different backdoor attacks, the proposed method shows consistent better performances.\n- The authors also show experiments on OOD data. This shows the generalizability of the proposed method."}, "weaknesses": {"value": "- I did not find major weaknesses of this paper. Overall, I think the paper is well-motivated and presented."}, "questions": {"value": "- In Figure 1, should the label for Teacher model in 2. Mismatch Filter be the Target Label?\n- In Sec. 3.2, it mentions augmentations cause benign images to exhibit trigger-like characteristics. Which augmentation operators are evaluated? Does this conclusion apply to a set of augmentations or some specific augmetation operators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AlNjJM3Lac", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Reviewer_NQnR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Reviewer_NQnR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761017630470, "cdate": 1761017630470, "tmdate": 1762926448126, "mdate": 1762926448126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On the theoretical justification of the probability prediction skew"}, "comment": {"value": "**[C1] On the theoretical justification of the probability prediction skew**\n\n### Response to Reviewer regarding the intuition of Prediction Skew\n\nWe appreciate the reviewer's suggestion to provide an intuitive explanation for the **prediction skew** phenomenon. We address this concern from an information-theoretic perspective, specifically utilizing **Rate-Distortion Theory**.\n\n1. Theoretical Premise: Simplicity Bias (Occam's Razor)\n\nWe assume that neural networks, during training (e.g., via SGD), implicitly minimize the description length of the learned solution (Minimum Description Length principle). In Rate-Distortion terms, the model seeks to minimize the \"Rate\" (information stored in weights) required to achieve a low \"Distortion\" (successful attack).\n\n2. The Cost of Learning Subtle Triggers\n\nIn advanced backdoor attacks, adversaries aim to embed triggers $\\tau$ that are subtle (e.g., invisible patterns or dynamic warping).\n\n- **High Complexity:** Distinguishing these subtle triggers from benign noise requires defining a highly complex decision boundary.\n    \n- **Information Cost:** Let $I(X; T)$ be the mutual information required for the model to map the triggered input to the target label $y_t$. For a subtle trigger with a weak signal, $I(X; T)$ is significantly large because the \"bit cost\" to distinguish the trigger is high.\n    \n\n3. Prediction Skew as an Optimization Shortcut\n\nTo minimize the total information cost while maintaining the attack success rate, the model exploits the prediction skew. We can decompose the total information required as follows:\n\n$$\\text{Total Information} \\approx R_{\\text{prior}} + I(X; T \\mid \\text{skewed prior})$$\n\n- **$R_{\\text{prior}} \\approx 0$ (Low Cost):** Encoding a global bias (skew) towards the target class $y_t$ is computationally \"cheap.\" It essentially requires adjusting a single bias parameter in the final layer, meaning the information cost $R_{\\text{prior}}$ is negligible.\n    \n- **$I(X; T \\mid \\text{skewed prior}) \\ll I(X; T)$ (Reduced Conditional Cost):** Once the prior probability for the target class is artificially inflated (skewed), the model requires significantly less evidence from the input $X$ to classify it as $y_t$. The decision boundary lowers, allowing even weak/subtle triggers to cross the threshold easily.\n    \n\n4. Conclusion\n\nConsequently, the model naturally converges to a state exhibiting prediction skew ($\\pi_{y_t}^{poisoned} \\gg \\pi_{y_t}^{clean}$), as this represents the most information-efficient path (Global Minimum in complexity) to satisfy the backdoor objective."}}, "id": "cAayNpftnG", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722350088, "cdate": 1763722350088, "tmdate": 1763722350088, "mdate": 1763722350088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a data-free backdoor defense method, CMP, which utilizes a self-distillation framework to prevent a student model from learning the malicious behaviors of a teacher model. The results indicate that CMP is effective for backdoor defense. However, the experimental evaluation is incomplete: the main performance evaluation was conducted solely on ImageNet, whereas the analysis for the key motivation in the main text used CIFAR-10. Additionally, the paper lacks a discussion on adaptive attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1，\tThe knowledge distillation process can cause a student model to inherit the backdoor behaviors of its teacher. Addressing this issue, this work proposes a more robust and secure distillation framework.\n2，\tThis work utilizes \"Model Inversion\" to synthesize data from the model itself, requiring no additional clean datasets."}, "weaknesses": {"value": "1，\tThe notion that backdoors can transfer from a teacher to a student model via knowledge distillation has been discussed in prior works [1] [2]. However, in the \"RELATED WORKS\" section, the authors claim to have \"uncovered an unprecedented mechanism that allows backdoors to transfer.\" It is recommended that the authors review and clarify their novelty claim here.\n2，\tThis work finds that data augmentation can cause benign images to exhibit trigger-like characteristics. Although the authors demonstrate this phenomenon in Table 2, they do not provide a more detailed analysis or discussion. It is suggested to add visualizations or other more interpretable analyses.\n3，\tIn its related work on 'Backdoor Defense', this paper discusses pruning-based defenses more extensively. However, the core method of this work is self-distillation. It is recommended that the authors include more summary of similar methods [1-6].\n4，\tWhat are the time and computational costs of model inversion, and what is the visualization of the synthetic image samples? \n5，\tIn Section 3.2, the authors design an experiment on the CIFAR-10 dataset to demonstrate the \"prediction skew\" phenomenon. However, in the experimental evaluation in Section 5, the authors do not provide the defense performance results of their method on CIFAR-10."}, "questions": {"value": "1，\tThe paper states in Section 5.1 that the CMP method synthesizes 1000 images per class, which implies that for the ImageNet-1K dataset, approximately 1,000,000 synthetic images need to be generated. What is the time cost of this process?\n2，\tThe paper lacks a discussion on adaptive attacks. If an attacker anticipates that the defender will use distillation for purification, they could design a novel backdoor to bypass this defense. Notably, [7] proposed Anti-Distillation Backdoor Attacks (ADBA), which are specifically designed to survive the knowledge distillation process and successfully transfer from the teacher model to the student.\n\nThe citation of weakness and questions are listed below.\n[1] Yao Z, Zhang H, Guo Y, et al. Reverse backdoor distillation: Towards online backdoor attack detection for deep neural network models[J]. IEEE Transactions on Dependable and Secure Computing, 2024, 21(6): 5098-5111.\n[2] Hong J, Zeng Y, Yu S, et al. Revisiting data-free knowledge distillation with poisoned teachers[C]//International Conference on Machine Learning. PMLR, 2023: 13199-13212.\n[3] Yoshida K, Fujino T. Disabling backdoor and identifying poison data by using knowledge distillation in backdoor attacks on deep neural networks[C]//Proceedings of the 13th ACM workshop on artificial intelligence and security. 2020: 117-127.\n[4] Hu C, Teng X, Xing W, et al. Distill To Detect: Amplifying Anomalies in Backdoor Models through Knowledge Distillation[C]//ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025: 1-5.\n[5] Bie R, Jiang J, Xie H, et al. Mitigating backdoor attacks in pre-trained encoders via self-supervised knowledge distillation[J]. IEEE Transactions on Services Computing, 2024, 17(5): 2613-2625.\n[6] Li X, Gao Y, Liu M, et al. A New Data-Free Backdoor Removal Method via Adversarial Self-Knowledge Distillation[J]. IEEE Internet of Things Journal, 2024.\n[7] Ge Y, Wang Q, Zheng B, et al. Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 826-834."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iSYZW0Ulh2", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Reviewer_ua98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Reviewer_ua98"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390935307, "cdate": 1761390935307, "tmdate": 1762926447612, "mdate": 1762926447612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response regarding the Mechanism of Skew Filter"}, "comment": {"value": "**[C2] Response regarding the Mechanism of Skew Filter**\n\n1. Our Strategy: Dual Defense Mechanism\n\nThe Skew Filter serves a dual purpose: it simultaneously eliminates the learned prior bias ($R_{\\text{prior}}$) and blocks the direct injection of backdoor features.\n\n- **Restoring the Information Barrier:** By removing the skewed prior, we force the information cost required to trigger the backdoor, $I(X; Y_{backdoor})$, back to its original high state.\n    \n- **Blocking the Strongest Signal:** It physically blocks the channel where the most potent backdoor signal resides. This effectively creates a \"barrier,\" preventing the student from learning subtle backdoors during distillation.\n    \n\n2. Why the Second-Highest Logit? (Signal Strength Analysis)\n\nThe rationale for targeting the second-highest logit stems from the necessity of signal strength for a successful attack.\n\n- **Exclusion of Top-1:** First, the **Mismatch Filter** ensures that no samples explicitly misclassified as the target class enter the distillation process1. Thus, the remaining backdoor signal is **not** in the Top-1 prediction (which corresponds to the correct ground-truth label).\n    \n- **Concentration of Malicious Signal:** For a backdoor trigger to be effective, its signal must be strong enough to potentially override the semantic class information. In a correctly classified sample (where the trigger is present but \"stealthy\"), this high-magnitude signal cannot vanish; it must reside in the **strongest non-target logit**.\n    \n- **Conclusion:** Therefore, the second-highest logit is the only candidate capable of carrying a signal strong enough to function as a backdoor. Targeting the Top-1 and Top-2 logits addresses the vast majority of the threat surface.\n    \n\n3. The Role of Detach: Implicit Regularization\n\nOur design choice employs detach (stopping gradients) rather than hard masking (zeroing out). This acts as a form of implicit regularization leveraging the zero-sum dynamics of the Softmax function:\n\n- **Blocking the Inducement:** Consider the competition between the Top-1 (clean) and Top-2 (target) classes. The poisoned teacher sends two simultaneous signals: (A) \"Maximize the correct class (Top-1)\" and (B) \"Maintain high probability for the target class (Top-2).\" By detaching the second-highest logit, we block the gradient flow for signal (B)2.\n    \n- **Implicit Suppression:** Consequently, the student is no longer encouraged to inflate the second-highest class. As the optimizer maximizes the probability of the Top-1 class (driven by clean features), the probability of the second-highest class is **implicitly suppressed** because the \"force\" (teacher's skew) holding it up has been removed.\n    \n- **Preserving Semantic Integrity (Why not zero out?):** Forcing the logit to zero implies \"this class is impossible,\" which destroys valid semantic relationships (**Dark Knowledge**) between classes (e.g., a dog resembling a wolf). Detach avoids this destruction.\n    \n\n4. Why Only the Second-Highest? (Minimal Intervention)\n\nOur goal is to cleanse the backdoor while maximizing the preservation of clean accuracy. This presents a trade-off between filtering poison-candidates and retaining information quantity.\n\n- **Diminishing Returns:** Extending the filter to the third-highest or lower logits might remove marginal amounts of residual skew poisoning, but it would also discard valid \"dark knowledge\" essential for student generalization.\n    \n- **Optimal Trade-off:** Targeting only the second-highest logit is the **minimal effective intervention**. It removes the logit with the highest probability of being a backdoor carrier while preserving the rich semantic information contained in the rest of the distribution."}}, "id": "oZKcXeI0uK", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722470933, "cdate": 1763722470933, "tmdate": 1763722470933, "mdate": 1763722470933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CMP (Calibration-Free Model Purification) — a data-free, threshold-free defense against neural backdoor attacks. The key insight is that backdoor persistence during knowledge distillation arises not merely from specific neurons but from a systemic prediction skew, where the poisoned teacher model over-assigns probability mass to the target class even on benign inputs. CMP adopts a self-distillation framework using synthetic images generated via model inversion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-organized and clearly written.\n- The paper introduces a new causal explanation of backdoor transfer through prediction skew, shifting the perspective from neuron-level pruning to distribution-level bias correction.\n- The dual-filter design is conceptually novel and operationally elegant, effectively removing backdoors without any calibration data."}, "weaknesses": {"value": "- The “prediction skew” phenomenon, though empirically compelling, lacks a formal definition or an information-theoretic grounding. No bound is provided linking skew magnitude to ASR reduction. A KL-based or mutual information analysis would strengthen the theoretical foundation.\n- Experiments are restricted to moderate-size architectures (ResNet-18, small ViT/DeiT). The paper claims scalability to larger models but does not report training cost, memory, or wall-time for full ImageNet distillation. This is critical for real-world viability.\n- The attacker is assumed fixed; CMP’s robustness against adaptive threats is not explored."}, "questions": {"value": "- Why is detaching the second-highest logit the most effective choice?\n- How sensitive is CMP to the quality or diversity of DeepInversion-generated samples? Does the defense remain stable if inversion produces low-quality or partially mode-collapsed data?\n- Please report distillation runtime, GPU hours, and resource overhead relative to standard fine-pruning or NAD."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N / A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uQl8o5TTlH", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Reviewer_LcoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Reviewer_LcoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406035770, "cdate": 1761406035770, "tmdate": 1762926447167, "mdate": 1762926447167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On the qualitative results of model inversion based synthetic images, and how augmentations incur backdoor behavior"}, "comment": {"value": "**[C3] On the qualitative results of model inversion based synthetic images, and how augmentations incur backdoor behavior**\n\nTo address the reviewers' requests for visualizations  and deeper analysis of how augmentations induce backdoor behavior , we have included additional visualizations in the Appendix.\n\n**1. Qualitative Analysis of Synthetic Images**\nWe provide a visualization of the synthetic images generated via Model Inversion. These images are generated by optimizing the input to match the batch normalization statistics stored in the teacher model. As shown in the figure, these images effectively capture the feature statistics required for distillation, despite containing high-frequency patterns typical of DeepInversion methods.\n\n**2. Mechanism of Augmentation-Induced Backdoor Activation**\nWe visualize the phenomenon where standard data augmentations inadvertently activate backdoor behaviors, as analyzed in Table 2.\n* **Observation:** We specifically visualize the effect of RandomResizedCrop. The figure demonstrates that cropping specific regions of a benign synthetic image can result in a pattern that causes the poisoned teacher to output a high response for the target class (mimicking trigger features).\n* **Consequence:** If these augmented samples are blindly used for distillation, the student model learns to associate these benign features with the target label, transferring the backdoor.\n* **Justification for Mismatch Filter:** This qualitative analysis empirically justifies our **Mismatch Filter**. By removing samples where the teacher's prediction shifts after augmentation, we effectively filter out these accidental trigger activations."}}, "id": "Heahq7QOrg", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722520110, "cdate": 1763722520110, "tmdate": 1763722520110, "mdate": 1763722520110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Calibration-Free Model Purification (CMP), a fully data-free and threshold-free defense mechanism against backdoor attacks in deep neural networks. CMP leverages a self-distillation framework with two novel components — the mismatch filter and skew filter — to prevent the transfer of backdoor behaviors from a poisoned teacher to a clean student model. It operates without requiring calibration data or pruning, achieving robust results on ImageNet-scale experiments across diverse attacks. The authors claim CMP is the first practical, scalable, and data-free defense adaptable to both CNNs and transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of removing the backdoor without calibration data is novel and practically important. The paper contributes new insights into the mechanisms of backdoor transfer during knowledge distillation. \n- The method is well-motivated with analysis experiments in section 3.2, which also makes the presentation better for understanding.\n- The extensive experiments verified the effectiveness of CMP. Its architecture-agnostic design strengthens its broader impact."}, "weaknesses": {"value": "- While the prediction skew is identified as the key phenomenon, the paper offers limited formal or mathematical justification for why the dual-filtering scheme guarantees elimination of such skew under different attack types. Especially for the skew filter, which relies heavily on the assumption of the second-largest probability class as the backdoor class, it lacks empirical evidence.\n- Despite the synthesized data is from the previous technique (i.e., model inversion), it lacks visualization or explanation of how different they look compared to the real image. And more experiments are needed to validate the effectiveness of CMP using different model inversion techniques, illustrating the influence of the synthesized data.\n- The required LLM statement is not included in both the main text and appendix."}, "questions": {"value": "- The appendix is submitted separately, making it hard to find. As the author guide (https://iclr.cc/Conferences/2026/AuthorGuide) encourages submitting a single file (paper + supplementary text), can you concatenate them?\n-  Why do the OTBR (fixed) and (oracle) version performs the same in Table 4? Does it indicate that the calibration data is not important for an effective OTBR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E5ybhHSOAt", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Reviewer_g5od"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Reviewer_g5od"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635784173, "cdate": 1761635784173, "tmdate": 1762926446700, "mdate": 1762926446700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On the computation costs of generating synthetic images and knowledge distillation"}, "comment": {"value": "**[C4] On the computation costs of generating synthetic images and knowledge distillation**\n\nTo address concerns regarding the computational overhead of our method, we provide a detailed breakdown of the costs associated with synthetic image generation and knowledge distillation, demonstrating the scalability of CMP.\n\n**1. Cost of Model Inversion (Synthetic Data Generation)**\nWhile generating synthetic data via model inversion might initially appear computationally expensive, the process is highly efficient due to its parallel nature.\n* **Parallelization:** The generation process for each class is independent, allowing for \"embarrassingly parallel\" execution.\n* **Hardware Efficiency:** Unlike model training, the inversion process does not require heavy GPU resources and can be effectively parallelized across CPUs. In our experiments, generating the full synthetic dataset (1,000 images per class) takes approximately **8 hours using 10 CPUs**.\n* **Contribution Recap:** As stated in line 198-200, our contribution does not rely on the inversion itself, rather, our contribution is to propose generalized As demonstrated in Section 4.2 and Table 8 of the main text, CMP is flexible and can utilize Out-of-Distribution (OOD) data (e.g., COCO) instead of synthetic images. This completely eliminates the generation cost while maintaining robust defense performance.\n\n**2. Cost of Knowledge Distillation**\nThe distillation phase is comparable to standard model training but can be significantly optimized.\n* **Training Time:** For the full ImageNet-1K setting using 1,000 synthetic images per class, our purification process takes approximately **3 days using 4 NVIDIA RTX 3090 GPUs**.\n* **Scalability via IPC Reduction:** We further demonstrate that CMP does not require a large number of synthetic images to be effective. We evaluated the performance with reduced Images Per Class (IPC) counts (50, 100, 200, 500). As shown in the table below, we achieve comparable defense performance (low ASR) and clean accuracy even with significantly reduced data, drastically cutting down the computational time.\n\n**Table: Performance of CMP across different Images Per Class (IPC) settings**\n\n| IPC | Clean ACC (%) | ASR (%) | Est. Training Time |\n| :--- | :---: | :---: | :---: |\n| **50** | *48.9* | *0.11* | *11 Hours 47 Minutes* |\n| **100** | *53.6* | *0.11* | *18 Hours 40 Minutes* |\n| **200** | *56.3* | *0.09* | *32 Hours 52 Minutes* |\n| **500** | *65.0* | *0.08* | *46 Hours 37 Minutes* |\n| **1000 (Default)** | 66.71 | 0.06 | ~72 Hours |"}}, "id": "USPJtXqwg3", "forum": "fYlMl5MxeV", "replyto": "fYlMl5MxeV", "signatures": ["ICLR.cc/2026/Conference/Submission16307/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16307/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16307/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722560550, "cdate": 1763722560550, "tmdate": 1763722560550, "mdate": 1763722560550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}