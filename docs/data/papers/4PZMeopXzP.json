{"id": "4PZMeopXzP", "number": 9900, "cdate": 1758147726568, "mdate": 1763631468330, "content": {"title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning", "abstract": "Benchmarks for competition-style reasoning have advanced evaluation in mathematics and programming, yet physics remains comparatively underexplored. Most existing physics benchmarks evaluate only final answers, which fail to capture reasoning processes, while recent stepwise methods rely on heuristic LLM-as-judge scoring or restrictive linear assumptions, limiting reliability and diagnostic validity.\nWe introduce PRISM-Physics, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. \nWe prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human experts' scoring. \nExperiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities.", "tldr": "We present PRISM-Physics, a benchmark and a process-level evaluation framework that encodes physics solutions as DAGs and employs rule-based symbolic equivalence checking for reliable, fine-grained scoring.", "keywords": ["Physics Reasoning", "Process-Level Evaluation", "Symbolic Equivalence", "Scientific Problem Solving"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfc9bbe84c44b9a4afababd8a15c77768a3d9a8e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PRISM-PHYSICS, a benchmark and a process-level evaluation framework that encodes physics solutions as DAGs and employs rule-based symbolic equivalence checking for reliable, fine-grained scoring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A large-scale benchmark of competition-level physics problems with carefully curated, DAG-structured solutions. \n2. A DAG-based scoring policy that explicitly models causal dependencies among formulas, enabling fine-grained and interpretable process-level evaluation.\n3. A fully rule-based symbolic formula equivalence checker to reliably validate diverse mathematical expressions, ensuring consistent comparison across alternative formulations and eliminating reliance on heuristic LLM-as-judge scoring."}, "weaknesses": {"value": "1. My main concern is that Figures 1 through 5 are very unclear, and even when enlarged twice, they are still hard to read. These figures should ideally be the most direct representation of the data analysis in this study, PRISM-PHYSICS. I hope the authors can improve the clarity of these figures.\n2. Figure 1 appears on page 2, but there is no corresponding content on the first two pages. Is its placement here inappropriate? Additionally, Figure 1 is not referenced anywhere in the text. The same issue applies to Figure 2.\n3. I feel that the originality of the article is somewhat limited. Could the authors provide an explanation of the connection between the challenges presented in this study and the research methods? At the moment, the challenges and methods do not seem to align well.\n4. Regarding the analysis of the experimental section, I hope the authors can summarize it more clearly. The current summary of the experiments is not very clear.\n5. How was the step level notation done? I would appreciate clarification on this point."}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gKfT4qqLPF", "forum": "4PZMeopXzP", "replyto": "4PZMeopXzP", "signatures": ["ICLR.cc/2026/Conference/Submission9900/Reviewer_4yRy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9900/Reviewer_4yRy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359121975, "cdate": 1761359121975, "tmdate": 1762921361609, "mdate": 1762921361609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM-PHYSICS, a framework for evaluating physics problems at the process level. Solutions are represented as directed acyclic graphs (DAGs) to capture dependencies between steps. The key innovation is the ancestor-closure scoring, which allocates partial credit based on intermediate steps. A rule-based symbolic equivalence checker ensures accurate comparison of formulas. Experimental results show that PRISM-PHYSICS provides detailed and reliable evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- PRISM-PHYSICS provides a large-scale, competition-level benchmark with carefully curated, DAG-structured solutions to complex physics problems.\n- A fully rule-based symbolic equivalence checker ensures consistent validation of diverse mathematical expressions, eliminating reliance on heuristic LLM scoring and offering a more reliable comparison across alternative formulations.\n- The ancestor-closure scoring policy allows for partial credit on intermediate steps, offering a more nuanced and fair assessment of student reasoning."}, "weaknesses": {"value": "- Does the system account for context-dependent variations in formulas? (e.g., solving a problem from both kinematics and dynamics perspectives, or analyzing it through momentum and energy considerations)\n- In physics problems, certain expressions may be contextually equivalent, but the strict analysis in this algorithm might overlook such context-dependent equivalence.   Does the current framework account for these context-sensitive variations?\n- If skips over intermediate, simpler steps during the solution process, would this result in incorrect evaluation by the proposed method?  \n- The summary of the experiment is not clear enough. It is hoped that the author will have a more clear and organized discussion of the experimental results."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EKzfqWWsYD", "forum": "4PZMeopXzP", "replyto": "4PZMeopXzP", "signatures": ["ICLR.cc/2026/Conference/Submission9900/Reviewer_PRSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9900/Reviewer_PRSU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640894109, "cdate": 1761640894109, "tmdate": 1762921361258, "mdate": 1762921361258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM-Physics, a large-scale physics reasoning benchmark with a proposed DAG-based evaluation protocol that addresses the limitations of existing LLM-as-judge scoring methods. The evaluation framework includes a fully rule-based symbolic formula equivalence checker to ensure consistent validation across diverse mathematical formulations, thereby eliminating reliance on subjective judgments. In the experiments, the paper investigates a diverse set of leading LLMs on PRISM-Physics and demonstrates the superiority of the proposed evaluation protocol compared to the LLM-as-judge method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using DAG to judge the correctness of the final answer and intermediate steps is reasonable, and I agree that using the LLM-as-judge method to evaluate the correctness of physics problems is challenging and prone to errors.\n2. The theoretical analysis part of the paper is solid.\n3. The experiment is comprehensive and convincing."}, "weaknesses": {"value": "1. My main concern is that, although PRISM-Physics can conduct rule-based judgments to determine the correctness of the final answer and intermediate steps using a DAG, the construction of the DAG still heavily relies on LLM-based extraction and rewriting. Compared to the existing LLM-as-judge method, the uncertainty introduced by LLMs seems to have merely shifted from the judgment stage to the preprocessing stage.\n2. Another concern lies in the scalability and additional computational cost of the proposed evaluation protocol. Compared to existing benchmarks, PRISM-Physics requires an annotated DAG in addition to the final answer for each question in order to perform a more rigorous evaluation. Thus, the scalability of the proposed protocol appears limited. If we aim to extend this rigorous protocol to other existing benchmarks, what additional requirements would those questions need to meet? Furthermore, if we intend to construct a DAG for a new physics problem, how much extra computational cost would this introduce in the preprocessing stage?\n3. Typo: in Line 362: \"zero-shot **COTzheg** prompts\". In Table 1, it would be better to retain the same number of digits after the decimal point and to bold the best results.\n4. The results in Figure 8 (Appendix E.2) are difficult to understand. The authors should at least explain the meaning of each rectangle in the text and clarify whether the difference shown represents \"multimodal – text\" or \"text – multimodal\"."}, "questions": {"value": "See Weaknesses 1, 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "umUVtNSp82", "forum": "4PZMeopXzP", "replyto": "4PZMeopXzP", "signatures": ["ICLR.cc/2026/Conference/Submission9900/Reviewer_4ABk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9900/Reviewer_4ABk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790096247, "cdate": 1761790096247, "tmdate": 1762921360718, "mdate": 1762921360718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM-Physics, a physics benchmark with particular focus on process-level evaluation. The authors model reasoning process as a Directed Acyclic Graph, where each node represents a formula in the resoning chain, and edges denote logical dependencies. They then propose to score a student answer by counting the ancestor closure of student's nodes within that of reference answer. Besides the scoring scheme, the authors established a way to reliably transform natural language reasoning into DAG with normalized formulas, allowing for robust formula matching during evaluation. Lastly, they curated PRISM-Physics, contributing to process-level evaluation of physics problem solving."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and very detailed.\n2. Experiments and analysis are comprehensive and insightful, covering a wide range of LLMs and problem types.\n3. The proposed DAG-based scoring scheme is novel and captures certain logical dependencies in the reasoning path."}, "weaknesses": {"value": "1. The Ancestor Closure Scoring Policy seemed too forgiving for rigorous process-level evaluation. If I understand correctly, this scoring scheme overlooks skipped steps in the students reasoning path, as long as a targeted formula is attained downstream. Neither does this scheme check for validity of derivation, since it does not check whether assumptions leading to the formula are correct.\n2. Following point 2, it would be nice to verify how well the scoring scheme aligns with true logical evaluation.\n3. While the DAG-based scoring system does not require a student's answer to have the same sequential steps, it also ignores the structure in the student's answer. If I understand correctly, the student's answer is extracted as a bag of formulas with no logical dependencies extracted from context; only the reference answer is represented as DAG with logical dependency. Again, this seemed quite forgiving."}, "questions": {"value": "1. Since you assume all ancestor nodes in the reference DAG are scored, have you checked how often a student answer actually misses those ancestor nodes? Would missing these nodes break the logical soundness of the student's answer?\n2. In Section 6.4, did human experts score with Ancestor Closure Scoring as well? If yes, have you tried alternative evaluation methods (that is based on true logic instead of formula matching) to see how Ancestor Closure Scoring aligns with that scheme?\n3. In Appendix E.1, did you use 8K context and zero temperature for evaluation of reasoning LLMs?\n4. Have you tried extracting the student's answer as a DAG as well, and compare it to the reference DAG for a more rigorous scoring?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03me9o7w3B", "forum": "4PZMeopXzP", "replyto": "4PZMeopXzP", "signatures": ["ICLR.cc/2026/Conference/Submission9900/Reviewer_Fpc6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9900/Reviewer_Fpc6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944336369, "cdate": 1761944336369, "tmdate": 1762921359729, "mdate": 1762921359729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}