{"id": "OOYO9x9gpH", "number": 6718, "cdate": 1757993357411, "mdate": 1759897898948, "content": {"title": "LU-500: A Logo Benchmark for Concept Unlearning", "abstract": "Current concept unlearning approaches for copyright have achieved notable progress in handling styles or portrait-like representations. However, the task of unlearning company logos remains largely unexplored. This challenge stems from logos’ simplicity, omnipresence, and strong associations with branded products, often occupying minimal space within an image. To bridge this gap, we introduce LU-500, a comprehensive benchmark for logo unlearning, consisting of 10 prompts to generate images of logos from Fortune Global 500 companies. Our benchmark features two tracks: LUex-500, with explicit prompts, and LUim-500, requiring implicit reasoning to address real-world scenarios like standard usage and adversarial attacks. We further propose five novel, multi-grained evaluation metrics, ranging from local logo regions to global image attributes and spanning both pixel and latent spaces, enabling a robust quantitative analysis of complex visual scenes. Experimental results reveal that existing inference-time unlearning techniques, such as NP, SLD, SEGA, and fine-tuning-based methods like ESD and Forget-Me-Not, all fall short in logo unlearning. To investigate this limitation, we propose a prompt-based baseline using large language models, which demonstrates significant improvements, highlighting the potential of unlearning in semantic space. Additionally, we analyze the correlation between the unlearning performance of an image and its characteristics such as logo area, location, and fractal dimension. We find that SSIM might be a profit control for logo unlearning.", "tldr": "", "keywords": ["Benchmark", "Concept Unlearn"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/388a3ba58a0d34f1c9508fb1a88f01b40f1fd1fb.pdf", "supplementary_material": "/attachment/e932704d3a699ad0027e3397786464263ab39235.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes LU-500, a benchmark for “logo unlearning” in text-to-image models, built from 9,584 prompts across Fortune Global 500 brands with explicit (LUex-500) and implicit (LUim-500) tracks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The benchmark design is clear. LU-500 isolates logo unlearning with two realistic prompting modes and a sizable, vetted prompt set."}, "weaknesses": {"value": "1. I think this work essentially belong to the “prompt engineering” , not only LU-500 built from some prompts, but also the ProLU are three prompt-based LLM agents. Unfortunately, I do not see any algorithmic innovation in this work.\n\n2. This work heavily relies on GPT-4o to built both the benchmark and the ProLU agents, yet Appendix A claims LLMs were used only to polish writing—that’s ridiculous\n\n3. The author claims to “propose” 5 metrics as core contribution in the introduction, but CLIPScore and SSIM are common metrics and there is nothing new."}, "questions": {"value": "See the weakness part of my review"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "T8swwwCtj5", "forum": "OOYO9x9gpH", "replyto": "OOYO9x9gpH", "signatures": ["ICLR.cc/2026/Conference/Submission6718/Reviewer_v8Vf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6718/Reviewer_v8Vf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663404966, "cdate": 1761663404966, "tmdate": 1762919009272, "mdate": 1762919009272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a logo unlearning benchmark, LU-500 and also 5 metrics designed for quantitative evaluation of logo unlearning efficacy. Furthermore, a prompt-based unlearning method, ProLU, has been provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated contribution. This paper fills a gap: prior benchmarks largely focus on natural images and stylistic concepts. Logos are intellectual property that diffusion model often memorize and highly relevant for both safety and legal compliance.\n2. Proposed benchmark.The proposed LU-500 contains 500 logos across 10 commercial categories.\n3. Four quantitative metrics are proposed for logo unlearning efficacy evaluation."}, "weaknesses": {"value": "1. More strong concept unlearning baselines (e.g., [1] ) should be involved for benchmark evaluation. \n2. Limited benchmark scale and diversity. Despite its value, 500 logos remain small relative to the variety of commercial marks. Many logos share similar geometric primitives, which might cause evaluation saturation.\n3. Ambiguous boundary between memorization and semantic retention. Some metrics (e.g., CLIP-based LS) may not effectively differentiate between semantic similarity (“a bitten apple”) and literal logo reconstruction (“Apple Inc.” logo). A clearer delineation between concept-level leakage and pixel-level memorization would improve interpretability.\n\n[1] Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models, NeurIPS 2024"}, "questions": {"value": "Check the above weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f16jun7i0e", "forum": "OOYO9x9gpH", "replyto": "OOYO9x9gpH", "signatures": ["ICLR.cc/2026/Conference/Submission6718/Reviewer_xpRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6718/Reviewer_xpRC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781315320, "cdate": 1761781315320, "tmdate": 1762919008944, "mdate": 1762919008944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for logo unlearning. It focuses on evaluating inference time unlearning methods on this benchmark, and proposes a baseline unlearning method based on prompting editing. Through experiments, it shows that existing inference-time unlearning methods are not effective in unlearning logos."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of this paper is generally clear. And it provides an interesting correlation analysis between unlearning performance and various logo characteristics (area, location, edge density, etc.)."}, "weaknesses": {"value": "This paper has the following major limitations:\n- The scope of copyright protection is limited. The exclusive focus on logos is too narrow for meaningful copyright protection. Other crucial copyrighted elements may include characters, protected artworks and patterns, and so on. The methods may not generalize to other types of copyrighted content\n- The sole focus on inference-time unlearning methods is a major limitation because it does not represent the full spectrum of unlearning approaches. There is no explanation for why other unlearning approaches wouldn’t work. Fine-tuning methods and model manipulation unlearning should be compared with even if they might be more computationally demanding.\n- The proposed baseline shows fundamental flaws. As we can see from Figure 6 that residual logos remain clear and brand identities are recognizable even if logos are partially removed. And it may not work if implicit brand indicators beyond logos are presented in the prompt. \n- The current metric does not guarantee complete information removal. It does not test against adversarial attempts to recover logos.\n\nThe front page image needs some work, the layout is cluttered and does not clearly show the main information."}, "questions": {"value": "Why focus exclusively on logos rather than a broader range of copyrighted visual content? Have you tested whether your benchmark and methods generalize to other types of copyrighted material (e.g., characters, artistic styles, patented designs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TS6GT3T7iD", "forum": "OOYO9x9gpH", "replyto": "OOYO9x9gpH", "signatures": ["ICLR.cc/2026/Conference/Submission6718/Reviewer_UoF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6718/Reviewer_UoF3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969639896, "cdate": 1761969639896, "tmdate": 1762919008321, "mdate": 1762919008321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces LU-500, a new benchmark designed to evaluate concept unlearning methods for company logos within text-to-image diffusion models.\nThe dataset contains prompts derived from Fortune Global 500 companies and is divided into explicit (LUex-500) and implicit (LUim-500) tracks.\nThe authors propose five quantitative metrics (CLIPScore, LogoScore, LogoSSIM, ImageScore, ImageSSIM) to assess both local logo removal and global image preservation across pixel and latent spaces.\nExperiments compare inference-time unlearning methods (NP, SLD, SEGA) and fine-tuning approaches (ESD, Forget-Me-Not) on Stable Diffusion 3 Medium.\nAll baselines perform poorly, motivating the authors’ prompt-based baseline, ProLU, which edits prompts through a three-agent pipeline (Remover, Reflector, Checker). ProLU achieves stronger logo removal but somewhat weaker background preservation.\nThe paper also performs a correlation analysis between unlearning effectiveness and image characteristics (area, location, fractal dimension) and finds only weak relationships."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- New benchmark focusing on logo unlearning. This is a neglected but socially relevant copyright-protection task.\n- The work is clearly written and easy to follow. \n- The five proposed metrics systematically separate local logo removal from global fidelity, going beyond binary success rates."}, "weaknesses": {"value": "- LU-500 focuses only on Fortune 500 logos; small-brand or non-Latin logos are not covered.\n- Reliance on CLIP and SSIM metrics raises concerns about semantic leakage or bias: low CLIPScore may not perfectly reflect successful logo removal. Human evaluation or perceptual studies would strengthen claims of “logo removal.”\n- The benchmark and metrics are valuable, but ProLU mainly repurposes LLM-based prompt rewriting without clear algorithmic innovation beyond dataset design."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2CsuA4AO6E", "forum": "OOYO9x9gpH", "replyto": "OOYO9x9gpH", "signatures": ["ICLR.cc/2026/Conference/Submission6718/Reviewer_bZCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6718/Reviewer_bZCH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224265366, "cdate": 1762224265366, "tmdate": 1762919007882, "mdate": 1762919007882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}