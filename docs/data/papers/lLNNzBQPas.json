{"id": "lLNNzBQPas", "number": 3093, "cdate": 1757330987128, "mdate": 1759898109562, "content": {"title": "Interleaving Reasoning for Better Text-to-Image Generation", "abstract": "Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o.\nMotivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve text-to-image (T2I) generation. \nWe introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. \nTo train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. \nWe curate IRGL-300K, a 300K-scale dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking–image trajectories. \nStarting from a unified foundation model that natively emits interleaved text–image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking–image trajectory data. \nExtensive experiments show SoTA performance, yielding absolute gains of 5–10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. \nAs an early exploration, our results demonstrate that interleaving reasoning is a powerful paradigm for advancing T2I.", "tldr": "", "keywords": ["Interleaving Reasoning", "Text-to-Image Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d55f4bd9b32a543095b2a911e124531e42b233d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel **Interleaving Reasoning Generation (IRG)** framework for text-to-image (T2I) generation. The key idea is to alternate between text-based reasoning and image synthesis, enabling the model to progressively refine image quality and semantics. To support this, the authors curate a **large-scale IRGL-300K dataset** comprising six decomposed learning modes designed to teach both textual reasoning and full image generation trajectories. Experimental results on **5** benchmarks, including **GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN**, show consistent improvements over existing T2I models or reasoning T2I approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-structured and clearly presented, with thoughtful visual aids such as color-coded pipeline diagrams (e.g., Fig. 2) that make complex processes easy to follow.  \n2. It addresses an **important and timely challenge** in multimodal generation to bridge reasoning and image synthesis, and provides not only a methodological contribution but also a valuable 300k-sample dataset, which could benefit future research.  \n3. The experimental validation is strong, demonstrating clear and consistent gains across multiple benchmarks, indicating that the proposed approach is both effective and generalizable."}, "weaknesses": {"value": "1. **Lack of ablation on pipeline design rationale**. While the paper offers a detailed description of the six learning modes and two-stage training, it does not empirically justify whether each component is necessary. For example, it remains unclear if *Initial Thinking Understanding Learning* is crucial, or if training could skip directly to *Initial Thinking Generation Learning* since in Eq. 3, $T_{in}$ will not consider any visual input to generate $T\\_{out}^{(1)}$. An ablation isolating these stages would help clarify the necessity and impact of each design choice.  \n2. **Limited qualitative demonstrations**.  The paper would benefit from showing more examples of intermediate reasoning outputs (e.g., textual thoughts and intermediate images). Figure 4 showcases final results, but visualizing reasoning steps, similar to the teaser in Fig. 1, would provide clearer insight into how the reasoning process contributes to image refinement.  \n3. **Insufficient comparison to closely related works**. The paper overlooks recent open-source approaches with similar reasoning-generation mechanisms, such as *Self-Correcting with LLM* [1] and *CoT-based Image Generation* [2]. Including these in the comparison, even qualitatively or in terms of design philosophy, would strengthen the paper’s contextual positioning.\n4. **Limited exploration of higher reasoning steps**. The paper only discusses cases where $n \\le 2$. Extending the study to higher reasoning depths could further emphasize the strength of the proposed *interleaving* reasoning design. For instance, even if trained with $n=2$, the model could use a sliding window of size 2 to perform continuous reasoning, achieving $ n=3, 4$, or even $5$. This would provide deeper insights into how well the model generalizes to multi-step reasoning scenarios.\n\n---\n\n**Overall:**  \nThis is a promising and well-written paper tackling an important topic in T2I reasoning. The contributions are solid, but the lack of discussion and empirical validation of the training pipeline design slightly weakens the argument for its necessity. The current score is 4, which could be raised if the paper includes clearer ablations and more intermediate reasoning visualizations.\n\n---\n\n[1] Wu, T. H. et al. *Self-Correcting LLM-Controlled Diffusion Models.* CVPR 2024.  \n[2] Guo, Z. et al. *Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.* CVPR 2025."}, "questions": {"value": "1. Why can the proposed training pipeline validate the process? (weakness 1) \n2. Can the model generalize to a higher reasoning step with a fixed window size? (weakness 4)\n3. Given Eq. 4, what will the $T_q$ look like? \n4. What is the inference time comparison compared to other approaches? Since this approach requires extra reasoning, will this double the inference time? \n---\n **(suggestion)**: It can increase the readability to add a prefix in Eq. 1. For example, put \"image understanding\" for the first line and \"T2I with CoT\" for the second line."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gA9kQzq4of", "forum": "lLNNzBQPas", "replyto": "lLNNzBQPas", "signatures": ["ICLR.cc/2026/Conference/Submission3093/Reviewer_PpTp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3093/Reviewer_PpTp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760707775422, "cdate": 1760707775422, "tmdate": 1762916548193, "mdate": 1762916548193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis to improve T2I generation quality. The key idea is: (1) generate initial thinking and an initial image, (2) reflect on the initial image to identify improvements, and (3) generate a refined image. To train IRG, the authors propose IRGL-300K, a dataset with six decomposed learning modes covering text-based thinking and full thinking-image trajectories. A two-stage training pipeline is employed: Stage 1 builds reasoning and reflection capabilities across all six tasks, while Stage 2 fine-tunes the complete IRG pipeline. Experiments show improvements of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Intuitive Framework**: The paper's core contribution, the Interleaving Reasoning Generation (IRG) framework, is conceptually elegant and highly intuitive. The \"think-generate-reflect-refine\" process mirrors human creative strategies, making the approach easy to understand and appreciate. \n\n**Methodological Depth**: The creation of the IRGL-300K dataset and the design of the two-stage, six-mode training process demonstrate a deep, systematic approach to solving a challenging training problem."}, "weaknesses": {"value": "The paper presents an interesting idea but is undermined by significant methodological flaws, an absence of critical analysis, and unconvincing experimental results. The core weaknesses fall into two main categories: the questionable contribution of the proposed method and the unaddressed practical costs.\n\n**Unjustified Complexity and Missing Cost-Benefit Analysis**\n\nThe primary weakness is the paper's failure to justify the substantial increase in complexity and cost. The Interleaving Reasoning Generation (IRG) framework introduces significant overhead at both training and inference stages, yet the benefits are not clearly demonstrated.\nCritically Absent Efficiency Analysis: The proposed 2-turn method involves a 5-step sequential pipeline (initial thought → initial image → encode → reflection → refined image). This inherently introduces significant latency compared to single-step generation. The paper completely omits any discussion of inference efficiency, including critical metrics like inference time, memory consumption, or throughput. Without this analysis, it is impossible to assess the practical viability of the method.\n\n\n**Questionable Marginal Utility**\n\nThe paper's own results cast doubt on the core value proposition of the second \"improving\" turn.\nEvidence of Negative Returns: According to Table 4, single-turn generation from the IRG-trained model outperforms the 2-turn generation on the WISE benchmark (0.79 vs 0.77). This suggests that the reflection and refinement step can actually degrade image quality. The paper fails to investigate or explain these failure cases, leaving a critical gap in understanding the method's behavior."}, "questions": {"value": "The paper's core idea is intuitive, but the proposed solution is exceptionally heavyweight, involving significant costs for both data creation and inference. Given this high investment, the actual benefits need to be clearly demonstrated, but the evidence presented is unconvincing and contradictory. Specifically, the results in Table 4 are confusing, as automated benchmarks show little to no improvement from the second reasoning turn. Furthermore, in the qualitative examples (Figure 4), the refined image can lose important details. The central issue is whether the method's substantial costs are justified by a reliable and significant improvement in image quality. The current results fail to make a convincing case. I hope the authors can provide clarification on these critical points.\n\n**Training & Data Cost**: What were the resource costs (e.g., API expenses, GPU hours) to create the IRGL-300K dataset and train the model?\n\n**Inference Cost**: How much slower is the 2-turn generation process compared to a single turn? Can you provide concrete numbers for inference time and memory usage?\n\n**Evaluation Contradiction**: MLLM ranking prefers 2-turn generation, but benchmark scores in Table 4 do not. Have you conducted human evaluations to confirm that the second turn offers a genuine improvement and rule out potential MLLM bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zxHTm2kxgc", "forum": "lLNNzBQPas", "replyto": "lLNNzBQPas", "signatures": ["ICLR.cc/2026/Conference/Submission3093/Reviewer_bEWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3093/Reviewer_bEWK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817621501, "cdate": 1761817621501, "tmdate": 1762916547978, "mdate": 1762916547978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Interleaving Reasoning Generation (IRG), a framework that improve T2I generaton by interleaving reasoning.\n\n\nThe key idea is to let the model first \"think\" (generate textual reasoning), then generate an image conditioned on that reasoning, and finally \"reflect\" on the generated image to refine it. \nThe framework differs from previous \"multi-turn post-hoc self-refinement / editing\" methods, IRG integrates reasoning and refinement in a single end-to-end pipeline via the supervision of \"text–thinking-image–thinking–image\" trajectory.\nTo support this paradigm, this paper introduces IRGL-300K, a large-scale dataset with six decomposed learning modes designed to train both text-based reasoning and full image generation capabilities.\n\n\nComprehensive experiments demonstrate strong improvements (5–10 points) across multiple benchmarks (GenEval, WISE, TIIF, GenAI-Bench, OneIG-EN).\n\n\nThe approach is conceptually interesting, technically solid, and empirically effective."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Interleaving Reasoning Generation (IRG), which differs from previous  \"multi-turn post-hoc self-refinement / editing\" methods. Instead of iterative post-hoc image editing, IRG integrates reasoning and refinement in a single end-to-end pipeline via \"text–thinking-image–thinking–image\" supervision. This unified formulation is both elegant and innovative.\n\n2. The approach is not limited to improving semantic alignment (as most prior reflection-based methods do) but also targets to improve visual quality, including texture rendering, fine-grained detail, and shadow realism.\n\n3. The IRG pipeline achieves consistent gains of 5–10 points on five diverse text-to-image benchmarks (GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN), demonstrating strong generalization and scalability.\n\n4. The authors recognize that standard benchmarks are often insensitive to fine-grained visual improvements and supplement them with multi-MLLM evaluator comparisons, providing convincing qualitative and quantitative validation."}, "weaknesses": {"value": "Seen in Questions"}, "questions": {"value": "My questions focus on clarity and rationale. \n\n1. Lines 288–290 \n> The main goal of this stage is to strengthen the text-based reasoning capability, while incorporating full thinking–image trajectories to avoid degrading the core generative performance.\n\nIt is unclear why \"incorporating full thinking–image trajectories\" is necessary during the reasoning-focused stage.\nWas this motivated by empirical observation (e.g., observed degradation of image generation without such data)?\nOr is it based on the general expectation that further fine-tuning of MLLMs may cause forgetting of generative capabilities?\n\n2. Lines 199–200\n> The model exchanges and exploits multiple segments of interleaved text–image representations, a process we term Interleaving Reasoning Generation (IRG).\n\nThe meaning of \"exchanges and exploits multiple segments of interleaved text–image representations\" is not clear.\nDoes this refer to multi-modal token interleaving within the transformer architecture,\nor to multi-step reasoning across modalities (e.g., text → image → text → image)? \nThis statement could be misleading and might need clarification.\n\n3. Section 2.2.2. \n\nThis section is quite dense and terminology-heavy.\nIt would be valuable to explain the thought process that led to the definition of the six decomposed learning modes.\nWas this decomposition empirically derived or conceptually motivated?\nA clear justification would significantly improve readability and insight.\n\nAdditionally, according to the mapping pattern (Eq. 4 →Eq.  7, Eq. 5 → Eq. 8, Eq. 6 → Eq. 9),\nwhy does Eq. 7 not include T^{(1)}_{out} as an input, while Eq. 8 and Eq. 9 do?\nThis asymmetry deserves clarification.\n\n4. Section 2.2.4. \n\nYou propose two complementary CFG-conditioning schemes (image-based and reflection-based).\nWhy not also include the traditional prompt-only CFG (conditioning on T_{in}) as a baseline?\nWas this excluded for theoretical or empirical reasons (e.g., redundancy, instability, or negligible impact)?\nDiscussing this design choice would improve completeness.\n\n5. Figure 2 is difficult to understand."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hqlrD12MoQ", "forum": "lLNNzBQPas", "replyto": "lLNNzBQPas", "signatures": ["ICLR.cc/2026/Conference/Submission3093/Reviewer_5XBw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3093/Reviewer_5XBw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950023120, "cdate": 1761950023120, "tmdate": 1762916547590, "mdate": 1762916547590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced Interleaving Reasoning Generation (IRG). The main idea is to reason with both text and image during text-to-image, to produce better images in the end. Specifically, the process in this paper has 4 stages: (1) Given an input text prompt, generate textural \"thinking\" (2) after thinking, generate an initial image (3) generate the second textural \"thinking\" to refine the image (4) generate the final image based on the previous steps.\n\nThe training process includes 6 learning modes, each targeting different sub-steps above. The authors also created IRGL-300K, a training dataset for this kind of interleaved reasoning generation. The primary source of the images are from GPT-4o. The authors also used Qwen2.5-VL to generate the text data.\n\nThe authors conduct experiment on Bagel, showing that it improves the results on GenEval, WISE, TIIF, and GenAI-Bench. The authors also conduct ablation studies on various components in the training process."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of let unified models to use interleaved text and image reasoning steps for text-to-image generation makes a lot of sense. It utilizes unified models' power, and goes beyond bagel's COT + image generation scheme.\n\n2. Experiments show that the authors' approach works, and can give improvement compared to the original Bagel.\n\n3. The authors conduct experiments on a variety of tasks, and the ablations answer many important research questions."}, "weaknesses": {"value": "1. The method is heavily dependent on distilling GPT-4o images. The dataset for \"initial learning stage\" is adapted from a GPT-4o generated dataset. The second stage is created through GPT-4o and Bagel generated images. It remains an open question that if this method works without a strong text-to-image model to distill with. For example, if the authors can observe similar gains through data generated by another open text-to-image model? Or even from Bagel's own generations? That would make the paper much stronger.\n\n2. There is no human evaluation in this paper. All evaluations are model based, and some are MLLM-as-a-judge results. It would be benefitial to show some human results on a subset of the testing prompts, to confirm that this method indeed improves model performance.\n\n3. There is no discussion on the compute / inference cost of this method. With more stages and many CFG steps, there will be a lot of extra compute, and they worth discussion.\n\n4. It would be beneficial to show some more analysis on the final model. For example, what is the lengths of the thinking steps? Are there any failure examples that the second image is not improving the first image?"}, "questions": {"value": "Most of my questions have been discussed in the weakness section\n\n1. Can this method work without using GPT-4o distilled data? For example, using Qwen-Image. How will the performance change.\n\n2. Is it possible that with Bagel's self-generated data, and some real images, we can see such gains? \n\n3. More analysis on the reasoning steps would be very helpful. Like more analysis on the reasoning lengths, and some failure case examples.\n\n4. What is the inference cost of the method? Is it possible that in some cases, we can just finish generation in the first stage, to save some cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UXt4UnEWu7", "forum": "lLNNzBQPas", "replyto": "lLNNzBQPas", "signatures": ["ICLR.cc/2026/Conference/Submission3093/Reviewer_JwV8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3093/Reviewer_JwV8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245437525, "cdate": 1762245437525, "tmdate": 1762916547085, "mdate": 1762916547085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}