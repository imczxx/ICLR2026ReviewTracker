{"id": "LrXGarwVO8", "number": 14225, "cdate": 1758230616104, "mdate": 1759897382711, "content": {"title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment", "abstract": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present \\textit{MobileGUI-RL}, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.", "tldr": "", "keywords": ["GUI Agent", "Reinforcement Learning", "Multimodality"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/667ff7645e24db5262391324ff823174966dce79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper *MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment* introduces **MobileGUI-RL**, a scalable framework for training mobile GUI agents through online reinforcement learning (RL) rather than traditional offline imitation learning. The approach integrates a **powerful VLM-based unified evaluator** (e.g., Qwen2.5-VL-72B) to automatically assess success via binary visual feedback, eliminating the need for hand-crafted rewards. It also builds a diverse curriculum of mobile interaction tasks using **self-exploration** and **text-based world model filtering**, enabling the agent to learn across dynamic, realistic GUI environments.\n\nTo optimize policy learning, the authors propose **MobGRPO**, a trajectory-aware variant of GRPO that distributes a trajectory-level advantage across steps and employs a **multi-component reward** combining success, efficiency, and termination penalties. Experiments on AndroidWorld and Android-in-the-Wild benchmarks show consistent performance gains, achieving state-of-the-art results while requiring significantly fewer training tasks. The work demonstrates that online RL with automated curriculum generation and trajectory-level optimization can substantially improve generalization and robustness of mobile GUI agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel online reinforcement learning framework for mobile GUI agents that departs from conventional offline imitation learning paradigms. Its integration of a *vision-language model (VLM) as a unified evaluator* for automatic reward generation, coupled with *self-exploration–based task synthesis* and *text-based world model filtering*.\n\n2. The proposed *MobGRPO* algorithm introduces a trajectory-aware modification to GRPO with a multi-component reward structure that balances task success, efficiency, and stability. The design addresses key challenges in long-horizon, sparse-reward environments and is validated through comprehensive experiments and ablations showing consistent and interpretable improvements over strong baselines.\n\n3. By demonstrating that online RL can outperform much larger models trained with offline data (e.g., surpassing GPT-4o and 72B VLMs using a 32B model), the work provides a strong case for efficient and scalable training of interactive agents."}, "weaknesses": {"value": "1. Training Data Source:\nIn Section 4.1 Experiments Setting, the paper mentions 436 curated GUI navigation tasks. What is the construction method and coverage scope of these tasks?\n\n2. Unified Evaluator:\nPlease describe the specific metrics of the evaluator, such as its correlation with SR (Success Rate) or its human verification accuracy. These details are crucial for assessing the actual contribution of the evaluator.\n\n3. Online Sampling Efficiency:\nSince online sampling for mobile agents is a computationally intensive engineering challenge, please explain the sampling efficiency and implementation details of the online learning system used in this work.\n\n4. Reward Calculation:\nThe reward formulation adjusts for both short successful and failed trajectories. However, has the paper considered that excessive encouragement of shorter successful paths might reduce the policy’s exploration capability and thus harm performance on out-of-distribution (OOD) tasks?"}, "questions": {"value": "The questions are already included within the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kR1c4rDLEm", "forum": "LrXGarwVO8", "replyto": "LrXGarwVO8", "signatures": ["ICLR.cc/2026/Conference/Submission14225/Reviewer_ojrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14225/Reviewer_ojrd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761114660154, "cdate": 1761114660154, "tmdate": 1762924681374, "mdate": 1762924681374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MobileGUI-RL, a new paradigm for training GUI agents in online environments. MobileGUI-RL consists of three key components:\n\n1. **Self-Exploration**: Utilizes GPT-4o to generate 1251 tasks through autonomous exploration.\n2. **Task Filtering**: Employs a text-based world model to filter these tasks, resulting in 436 high-quality tasks.\n3. **Decay Reward**: Uses Qwen2.5-VL-72B as a verifier to assign binary rewards (1 or 0). To further refine the learning signal, the binary reward is transformed into a length-aware reward that encourages shorter successful trajectories and penalizes shorter unsuccessful ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel training framework for agents in online mobile environments.\n2. The data processing pipeline is well-designed.\n3. The 7B and 32B models achieve state-of-the-art performance on several online benchmarks."}, "weaknesses": {"value": "1. **Writing and Organization:** The writing needs further polishing. For example:\n   - \"Qwen 2.5 VL 72B\", \"MobileGUI 7B\" and \"MobileGUI 32B\" should be correctly formatted as \"Qwen2.5-VL-72B.\" \"MobileGUI-7B\" and \"MobileGUI-32B\" respectively. \n   - Sentences like \"More details on input construction are in appendix X\" should specify the actual appendix location.\n   - Statements such as \"More detailed action definitions are provided in the appendix section\" should clearly indicate the appendix section or number.\n   - I recommend refining Section 3 (Method) for better readability. For instance, provide clearer and more concise definitions for \"Curriculum Learning.\" The section \"MULTI-COMPONENT REWARD DESIGN\" might be presented before \"TRAJECTORY-AWARE POLICY OPTIMIZATION\" for better logical flow. Consider improving ambiguous terminologies \"Differentiating Successful Trajectories,\" \"Penalizing Premature Termination,\" and \"Handling Degenerate Batches\". They read like AI-generated translations.\n   - The sentence \"Our agent, MobileGUI, is built upon the Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-32B-Instruct large multi-modal model.\" is confusing; the word order and phrasing should be improved for clarity.\n\n2. **Training Details:** More comprehensive training details are needed, such as:\n   - Additional information about the online tasks used during training, including task instructions, more examples, and an analysis of training task distribution. The lack of detailed training information limits reproducibility.\n   - More details in the appendix on **Curriculum Learning**, **Task-Filter**, and **Self-Exploration** procedures.\n\n3. **Experimental Support:** The study would be strengthened by additional experiments, for example:\n   - Ablation studies on data size.\n   - Model ablations for components such as the verifier and task generator.\n\n4. **Broader Benchmarks:** It is recommended to evaluate on more static benchmarks, such as AndroidControl and GUI Odyssey, if possible.\n\n5. **Related Works:** Please include a broader discussion of related work, especially regarding task generation and length-aware reward computation."}, "questions": {"value": "1. Could the authors clarify the differences between ARPO and MobileGUI-RL?\n\n2. **Please address the concerns mentioned above.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "44yfUrd4qu", "forum": "LrXGarwVO8", "replyto": "LrXGarwVO8", "signatures": ["ICLR.cc/2026/Conference/Submission14225/Reviewer_QBaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14225/Reviewer_QBaE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209645806, "cdate": 1761209645806, "tmdate": 1762924680342, "mdate": 1762924680342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MobileGUI-RL, a reinforcement learning framework for training GUI agents directly in online, interactive environments. MobileGUI-RL enables continuous real-time interaction, self-exploration, and policy updating, allowing agents to generalize to dynamic mobile applications. The framework combines a synthetic task generation pipeline, which uses self-exploration and a text-based world model for task filtering, with an adapted GRPO algorithm, which integrates trajectory-aware advantages and multi-component rewards to improve both task success and efficiency. Experimental results on three mobile-agent benchmarks (AndroidWorld, AITW-Gen, and AITW-Web) show that MobileGUI-RL significantly improves performance over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a new approach to training GUI agents in online environments using reinforcement learning, which addresses the shortcomings of previous offline methods.\n- This paper designs a scalable training infrastructure with batched virtual execution on multiple Android emulators, enabling high-throughput, asynchronous data collection. This design improves both sample efficiency and policy robustness, which is an important technical strength for large-scale deployment.\n- Extensive experiments across three online benchmarks (AndroidWorld, AITW-Gen, and AITW-Web) demonstrate consistent and substantial performance gains."}, "weaknesses": {"value": "- The study contains several details not fully disclosed. Please refer to the question section for specifics.\n\n- The reproducibility of the results is relatively low and requires further enhancement of the experimental repeatability.\n\n- There is a lack of more ablation studies. Please refer to the question section for details."}, "questions": {"value": "1. What is the accuracy when using Qwen2.5 VL 72B as the evaluator?\n2. If the state includes screenshots with a resolution of 1080×2400, how can the context length problem be avoided in multi-turn interactions?\n3. What is $\\pi_{explore}$? How does it perform random walk? How are unexpired UI elements counted, and how are element-type UI elements handled?\n4. What are $\\pi_{base}$ and $\\pi_{world}$? How does $\\pi_{world}$ ensure no hallucinations over time?\n5. The task filtering and the curriculum design heavily depend on $\\pi_{world}$, which relies on the performance of $\\pi_{base}$ and the ability of $\\pi_{world}$ to accurately predict the next state. Can you elaborate on this point?\n6. RL training uses two models, 7B and 32B. Are these models trained simultaneously? The author mentions that the rollout is a combination of samples from both models. How does RL training work in this case?\n7. The author designs many reward components, and the effectiveness of each component requires further experimental analysis.\n9. According to the author's ablation results, applying vanilla GRPO directly to the Qwen-2.5-VL-7B model (without the author's three methods) only yields about a 1% improvement in accuracy, which seems to be below expectations. Could you provide a detailed explanation and analysis of the reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJgu9KG8vT", "forum": "LrXGarwVO8", "replyto": "LrXGarwVO8", "signatures": ["ICLR.cc/2026/Conference/Submission14225/Reviewer_HP2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14225/Reviewer_HP2q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292042939, "cdate": 1761292042939, "tmdate": 1762924679882, "mdate": 1762924679882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce a framework for training visual language models (VLMs) that can produce effective sequence of actions on a mobile phone's GUI to complete user-provided task. The authors first argue that previous methods designed for this task are trained on offline data of specific environments which hinders their performance in an unseen environment. They propose a training framework that identifies various possible tasks on a mobile, designs a curriculum based on the complexity of those tasks, models the rewards to avoid late successes and early failures, and trains a modification of popular GRPO algorithm. Authors compare their framework with previous methods and show consistent improvement. They also present an ablation study on high-level choices used in the framework to provide evidence for importance of individual choices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) I like how the authors designed this entire framework: the automatic curriculum design ad reward design. \n2) I can see the potential of someone with physical disabilities use voice-assistance to control their mobile phone as long as safety is ensured. The algorithm proposed in this paper can be highly useful to personalize an AI model for individual users.\n3) The paper shows that the proposed framework achieves significant improvements over prior techniques in terms of success rates and sample complexity."}, "weaknesses": {"value": "**Major (my reason for not providing higher score)**\n1) No examples of the inference of the trained system. The example shown in the appendix in the section D does not mention the details of the user request or explain whether the trajectory generated is of a trained AI model. Also, the paper does not provide any details about the identified tasks through their method of synthetic task generation.\n2) There is no comparison of proposed trajectory-aware MobGRPO against standard GRPO. If I understand correctly, the proposed algorithm uses length of the observations in the batch of data to normalize the total per-token loss. There are a few other algorithmic variations like Dr. GRPO [1] which can also be used for the comparison study.\n3) Tables 2 and 3 report average success rates of different models attempting the GUI control task. How are these rates computed? How many number of attempts were allowed? Is there an analysis on success rate and attempts? Also, if it is possible, I would like to see the standard error across the test examples as it would provide me sufficient information to judge the statistical significance of these results.\n4) The figure 2 shows that there is instability in learning with the curriculum proposed in this work. In the description, authors suggest this instability is not reflective of the model's true task proficiency. A more convincing argument would be to show how exactly the true performance of the model changes as the training progresses, i.e., having a validation curve would immensely clarify any doubts. It would be a good addition to have training curves for different variations of the algorithm.\n5) Why do you call the metric \"impossible task ratio\"? Shouldn't the curriculum keep this metric growing steadily instead of sudden increase? If the tasks are impossible, why are they shown to the model in the first place? Is their forgetting of the earlier skills, if completely new task that does not use any previously learnt technique appears as part of the curriculum?\n\n\t\n**Minor:**\n1) Line 81: \"..reward-driven learning difficult..\" I think authors intend to say success criterion based training is difficult. The current phrasing makes it sound as if authors are proposing a training mechanism that does not depend on rewards.\n2) Line 90 says \"four mobile agent benchmarks\", however the tables only depict 3. Am I misunderstanding something?\n3) Line 144: The novelty of the parallel environment is contestable. Creating virtual environments in reinforcement learning is a standard workflow in expediting training. In addition, I do not think your current setting requires a parallel env in the first place because the main bottleneck is LLM inference, not the phone emulator.\n\n\n**General comment:**\nIf the major weaknesses highlighted above along with the minor ones and ethical concerns are satisfactorily addressed, I am willing to increase my rating of this work."}, "questions": {"value": "I have asked my questions in the weakness section above."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I want to highlight, in general, it is possible that without appropriate safety measures, AI model can take control of the personal device to harm human interests. I suggest authors add their perspective on why developing such AI agents is still necessary for the current times. I also suggest them to comment on the possibility of safety violations induced by RL training of the AI model."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YNeoldZu4v", "forum": "LrXGarwVO8", "replyto": "LrXGarwVO8", "signatures": ["ICLR.cc/2026/Conference/Submission14225/Reviewer_h578"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14225/Reviewer_h578"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835916610, "cdate": 1761835916610, "tmdate": 1762924679205, "mdate": 1762924679205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MobileGUI-RL, a scalable framework for training vision-based mobile GUI agents using online RL rather than traditional offline approches. The framework features a synthetic task creation pipeline leveraging self-exploration and task filtering via a text-based world model, and adapts GRPO to GUI navigation with trajectory-aware advantage estimation and composite rewards. Experiments on AndroidWorld, Android-in-the-Wild (General tasks and webshop) with Qwen2.5-VL-7b/32b as training base model, with further ablation studies examining the contributions of curriculum learning and reward design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe experiment results show improvements for both Qwen2.5-VL 7b/32b.\n-\tThe paper shifts to online RL with mentioning scalable data collection environments. \n-\tThe case study offers an intuitive illustration of the agent fixed the failure that the pre-trained LLMs made."}, "weaknesses": {"value": "-\tSeveral directly relevant recent works, such as DigiRL and DistRL are not compared, despite clear overlap in online RL settings and robust GUI agents. This absence undermines the claims of uniqueness and makes positioning statements in Section 2 unsupported. \n-\tInsufficient details on key components are missing. For example, (1) the scalable environment is not well described. How to ‘align compute-intensive environment simulation with CPU and model training with GPUs’? (2) The text-based world model is not precisely described. How is the textual state structured? How to deal with compounding errors? (3) the reward design details, such as decay parameter $\\lambda$, clipping factors, penalty constant, are not empirically justified. (4) It is not clear how the tasks are created in curriculum. \n-\tThe paper over-claims a few contributions including task self-exploration (initially mentioned in voyager [G.Wang 2023], and similar to task generations in OS-Genesis, GUI-Xplore, ScreenAgent), GRPO modification with trajectory-level and token-level information (already used in DAPO)."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ktvJp5410i", "forum": "LrXGarwVO8", "replyto": "LrXGarwVO8", "signatures": ["ICLR.cc/2026/Conference/Submission14225/Reviewer_Qt87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14225/Reviewer_Qt87"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901682003, "cdate": 1761901682003, "tmdate": 1762924678492, "mdate": 1762924678492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}