{"id": "fYoFjkLkR2", "number": 13958, "cdate": 1758225868645, "mdate": 1759897399999, "content": {"title": "The surprising strength of weak classifiers for two-sample testing", "abstract": "The two-sample testing problem, a fundamental task in statistics and machine learning, seeks to determine whether two sets of samples, drawn from underlying distributions $p$ and $q$, are in fact identically distributed (i.e.~whether $p=q$). A popular and intuitive approach is the classifier two-sample test (C2ST), where a classifier is trained to distinguish between samples from $p$ and $q$. Yet despite simplicity of the C2ST, its reliability hinges on access to a near-Bayes-optimal classifier, a requirement that is rarely met and difficult to verify. This raises a major open question: can a weak classifier still be useful for two-sample testing? We show that the answer is a definitive yes. Building on the work of Hu & Lei (2024), we analyze a conformal variant of the C2ST that converts the scores from any trained classifier---even if weak, biased, or overfit---into exact, finite-sample p-values. We establish two key theoretical properties of the conformal C2ST: (i) finite-sample Type-I error control, and (ii) non-trivial power that degrades gently in tandem with the error of the trained classifier. The upshot is that even poorly performing classifiers can yield powerful and reliable two-sample tests. This general framework finds a powerful application in Bayesian inference, particularly for validating Neural Posterior Estimation (NPE) models, where the task of comparing a learned posterior approximation $q(\\theta \\mid y)$ to the true posterior $p(\\theta \\mid y)$ can be framed as a two-sample test. Empirically, the Conformal C2ST outperforms classical discriminative tests across a wide range of benchmarks for this task. Our results establish the conformal C2ST as a practical, theoretically grounded diagnostic tool.", "tldr": "", "keywords": ["Neural posterior estimation", "statistical testing", "simulation based inference"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c583d96f869dd5c06e87e3461537b55541d0c9ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Conformal C2ST, which turns scores from any classifier into finite sample valid p values through conformal calibration, giving a two sample test that controls type I error without asymptotics and remains powerful even with weak or biased classifiers. The theory shows validity and robustness based on rank information, and the method includes a single calibration multiple statistic to reuse data. Experiments with controlled shifts and image data show stronger and more stable power than standard classifier based tests"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces Conformal C2ST, turning scores from any classifier, including weak or biased ones, into finite-sample valid p values via conformal calibration, yielding a broadly applicable two-sample test.\n2. Provides finite-sample type-I error control without asymptotics and argues robustness by relying on rank information rather than raw accuracy, maintaining meaningful power even when the classifier is imperfect.\n3. Includes a practical single-calibration “multiple” statistic for data reuse and shows stronger, more stable power than standard classifier-based tests on controlled shifts and image data."}, "weaknesses": {"value": "1. The paper would benefit from a clearer placement of Conformal C2ST relative to standard two-sample tests (e.g., MMD, energy distance, and classifier C2ST without conformalization) and closely related conformal or permutation approaches. Please spell out what is inherited, what is new, and where the performance gains come from.\n2. Several closely related classifier-based two-sample tests appear to be missing. Kim et al. [1] develop theory for classifier-based testing, including consistency even when Bayes accuracy is near one-half, which speaks directly to the paper’s motivating question that \"This raises a major open question: can a weak classifier still be useful for two-sample testing?\". Kübler et al. [2] and Cheng and Cloninger [3] study tests based on logit scores; the current draft mainly compares to accuracy-based variants. Liu et al. [4] analyze links between classifier-based and kernel tests. It would help to cite these works, explain the connections, and, where feasible, include targeted comparisons.\n3. While the theory ensures finite-sample type-I control, guidance on power is limited, especially the relationship between classifier performance and test power, which is central to the question of whether weak classifiers are useful. If formal analysis is challenging, additional experiments that systematically vary classifier strength, calibration size, and class balance would make the story more convincing. The toy example in the introduction could also be strengthened.\n4. Since the method can use any classifier score, practical guidance would be valuable. Consider ablations comparing common choices (logit, calibrated probability, density-ratio proxy, representation distance) and a note on sensitivity to monotone transformations. A short decision guide would help practitioners.\n5. The paper notes that a fresh calibration set is needed for each test point. This can be a significant drawback in standard settings of two-sample testing; it would be helpful to discuss when this method is preferable, possible workarounds. Please also elaborate on the assumption that \"the same marginal distribution of $y$ for both joint distributions.\"\n6. Since the approach builds on that conformal framework, the novelty may be limited unless the differences and advantages are made explicit. The draft states that Hu & Lei rely on a nearly Bayes optimal classifier for density-ratio modeling; I could not find this requirement stated as such in their paper. It would help to clarify the exact assumption in Hu & Lei (with citation to the relevant section), explain why it may fail in practice, and show precisely how the proposed method avoids or relaxes it.\n7. Please add comparisons with state-of-the-art classifier-based two-sample tests (for example, logit-score variants such as AutoML C2ST). \n\n[1] Kim, I., Ramdas, A., Singh, A., & Wasserman, L. (2021). CLASSIFICATION ACCURACY AS A PROXY FOR TWO-SAMPLE TESTING. The Annals of Statistics, 49(1), 411-434.\n\n[2] Kübler, J. M., Stimper, V., Buchholz, S., Muandet, K., & Schölkopf, B. (2022). Automl two-sample test. Advances in Neural Information Processing Systems, 35, 15929-15941.\n\n[3] Cheng, X., & Cloninger, A. (2022). Classification logit two-sample testing by neural networks for differentiating near manifold densities. IEEE Transactions on Information Theory, 68(10), 6631-6662.\n\n[4] Liu, F., Xu, W., Lu, J., Zhang, G., Gretton, A., & Sutherland, D. J. (2020, November). Learning deep kernels for non-parametric two-sample tests. In International conference on machine learning (pp. 6316-6326). PMLR."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oqnQ9iYmGT", "forum": "fYoFjkLkR2", "replyto": "fYoFjkLkR2", "signatures": ["ICLR.cc/2026/Conference/Submission13958/Reviewer_wM8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13958/Reviewer_wM8H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460957329, "cdate": 1761460957329, "tmdate": 1762924461486, "mdate": 1762924461486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method that is complementary to the conventional classifier two-sample test (C2ST). Whereas the original C2ST relies on the classification error to decide between $H_0$ and $H_1$, the proposed method is instead based on a ranking statistic, which reflects how likely a classifier is to rank positive samples ahead of negative ones. The authors construct such a ranking function using an approximated density ratio and provide theoretical guarantees on Type~I error via conformal inference. In addition, they present evidence that, under certain conditions, a classifier can preserve good ranking performance even when its decision boundary shifts, resulting in higher testing power than the original C2ST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work makes an important contribution complementary to the original classifier two-sample test. The key intuition is that even a weak classifier can still achieve a reasonably high AUC, which is critical to the soundness of the proposed method.\n\n2. The presentation of the work is clear and flows well."}, "weaknesses": {"value": "1. The main weakness of the proposed method is that it requires sampling a large number of points to construct the calibration set. Although Section 2.4 introduces an alternative (“conformal multiple”) that utilizes only a single calibration set, the experiments (line 422) indicate that a calibration set of size 1000 is repeatedly drawn when applying the proposed method. This leads to an unfair comparison, since line 414 states that only 1000 data points are used by the baseline to construct a testing function. Furthermore, although the conformal multiple variant appears in the experimental figures, the paper does not specify the size of the calibration set used for this alternative, leaving it unclear whether the same number of points is used to construct the testing function across all methods. \n\n2. A particularly important yet simple baseline that is missing is to apply Platt scaling to calibrate the classifier and then directly use its output probabilities for ranking for the proposed method."}, "questions": {"value": "1. Can you elaborate on NPE with real-world examples? In what scenarios can one sample $\\theta$ from the true $p(\\theta)$?\n2. Assumption 1 reads somewhat artificial. Is there a comparable assumption in prior work?\n\n3. Why is $\\xi$ taken to be uniformly distributed on $[0,1]$? Intuitively, I would make $\\xi$ as a Bernoulli variable to break ties. Does this choice affect testing power?\n\nAdvice and typos\n1. I do not think Assumption 2 is truly an assumption; it would be more appropriate to treat it as an input to Theorem 1, emphasizing that the testing power depends on the approximation error of density-ratio. For instance, see section 5.4.4 in [1].\n2. Missing $\\tilde{X} \\sim q$ on line~249.\n3. $\\hat{T}\\to\\infty$ on line~333 appears to be a typo.\n\n[1] Li, Weizhi, et al. \"Active Sequential Two-Sample Testing.\" Transactions on Machine Learning Research 2024 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fmbbErbxtv", "forum": "fYoFjkLkR2", "replyto": "fYoFjkLkR2", "signatures": ["ICLR.cc/2026/Conference/Submission13958/Reviewer_j5Zc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13958/Reviewer_j5Zc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714500634, "cdate": 1761714500634, "tmdate": 1762924460896, "mdate": 1762924460896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a conformal variant of the classifier two-sample testing (C2ST), which aims to address the problem that too weak or poorly trained classifier cannot distinguish two samples, which leads to a low test power under alternative hypothesis. \n\nThe paper's main contribution is to provide two theoretical contributions of proposed conformal C2ST, 1) finite-example Type-I error control and 2) significant power that degrades gracefully with classifier quality, so even underpowered classifiers still can yield a useful test. These can make the conformal C2ST become a more practical framework or tool."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared to the original C2ST, the proposed conformal C2ST is indeed more powerful and is well-motivated to address the significant  limitations of C2ST. \n2. Theoretically prove the type-I error control and ties power to ranking quality. \n3. Empirically prove that conformal C2ST outperforms C2ST."}, "weaknesses": {"value": "1. Even though the paper focuses on increase the power of C2ST, however, C2ST currently is not state-of-the-art enough in the field of two-sample testing. There is no other empirical comparisions of power with other two-sample testing methods, e.g., MMD-based methods, MMD-Agg, MMD-FUSE, MMD-DUAL. Those methods does not need to train a classifier, but has state-of-the-art performance. \n2. Will the power reduce from the split process of calibration samples influence the performance of training, since less samples are used for training the classifier. How to ensure that conformal C2ST always outperform C2ST?\n3. Is there any time-complexity analysis of testing phase, since conformal prediction normal time-consuming than the simple inference?"}, "questions": {"value": "Mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eeANXWxsz1", "forum": "fYoFjkLkR2", "replyto": "fYoFjkLkR2", "signatures": ["ICLR.cc/2026/Conference/Submission13958/Reviewer_qyQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13958/Reviewer_qyQ7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992074352, "cdate": 1761992074352, "tmdate": 1762924460468, "mdate": 1762924460468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies two sample testing when the classifier you train is not very good. The authors take a classifier two sample test and wrap it in a simple conformal step. For each point from the candidate distribution they rank the classifier score against scores from a small calibration set drawn from the reference distribution to get a p value. They then test whether these p values look uniform, for example with a KS test. This gives exact finite sample type I error control for any fixed scoring function. The main claim is that the test also keeps meaningful power even when the classifier is weak, because power only needs a useful ranking signal rather than near Bayes accuracy. The paper proves validity, connects power to AUC and total variation, and gives a robustness theorem that shows power degrades gently with an error bound that scales like epsilon to the two thirds. The method is used to validate neural posterior estimators and also to detect image corruptions, where it outperforms standard C2ST and several recent baselines in many settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Framing C2ST inside a conformal uniformity test is a clear and useful twist. The paper’s focus is on the practical regime where the classifier is weak or even biased, and it shows this can still work.\n\nValidity is clean. Under the null, the conformal p values are exactly uniform for any fixed score and any calibration size. The test aggregates many such p values with a KS statistic. On power, the paper ties the expected p value to AUC and to total variation, then proves a robustness theorem that controls the gap between oracle and plug in scores by the L2 error of the density ratio with an epsilon to the two thirds rate.\n\nTwo sample testing is a core tool for checking generative modeling and for simulation based inference. In practice people often have only small or under trained classifiers."}, "weaknesses": {"value": "The paper uses the KS test on the empirical CDF of p values. KS is simple, but there are other choices like higher criticism or Cramer von Mises that can dominate in some alternatives.\n\nThe uniform test samples a fresh calibration batch from the reference for every test point. The paper argues this is often cheap in NPE, but not always. A more detailed accounting of time and memory cost versus the single calibration multiple test would help adopters make the right choice.\n\nBaselines include SBC, TARP, discriminative calibration, and vanilla C2ST. That is helpful. Still, L C2ST and other recent classifier based diagnostics could strengthen the story, even if they lack finite sample guarantees. Also, it would be good to show one clear case where the conformal approach loses to a strong alternative, so readers see the tradeoffs.\n\nThe experiments are strong, but the link to a full NPE workflow could be tighter. For example, how to pick m and nq given a fixed simulation budget, and how often to re train the classifier during model development."}, "questions": {"value": "Have you tried alternative uniformity tests beyond KS, such as higher criticism that can be powerful for sparse departures, or Anderson Darling that weights the tails.\n\nCan you design a hybrid that uses a small pool of calibration sets and reuses them across test points with a block bootstrap correction, to split the difference between the uniform and multiple tests. Any theory or early evidence here. \n\nSince only the ranking matters, have you tried training a ranker or a small correctness predictor on top of classifier features. Does this improve AUC and power while keeping the same guarantees. \n\n\nIn the rotation toy example, power drops to the level of the type I error when the boundary is orthogonal to the signal. Could you add one more real example where ranking is truly uninformative so readers know what red flags to watch for. \n\nFor the posterior testing use case, would you recommend reporting a single p value, or a power curve across m or across corruption level gamma like in the controlled suite. Some guidance on the most informative diagnostic report would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "en0eRr1Ihw", "forum": "fYoFjkLkR2", "replyto": "fYoFjkLkR2", "signatures": ["ICLR.cc/2026/Conference/Submission13958/Reviewer_VoCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13958/Reviewer_VoCK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023641779, "cdate": 1762023641779, "tmdate": 1762924459933, "mdate": 1762924459933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}