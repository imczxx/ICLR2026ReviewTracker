{"id": "ZBbKLvH0w4", "number": 21465, "cdate": 1758317908569, "mdate": 1759896920666, "content": {"title": "In-Context Compositional Q-Learning for Offline  Reinforcement Learning", "abstract": "Accurately estimating the Q-function is a central challenge in offline reinforcement learning. However, existing approaches often rely on a single global Q-function, which struggles to capture the compositional nature of tasks involving diverse subtasks. We propose In-context Compositional Q-Learning ($\\texttt{ICQL}$), the first offline RL framework that formulates Q-learning as a contextual inference problem, using linear Transformers to adaptively infer local Q-functions from retrieved transitions without explicit subtask labels. Theoretically, we show that under two assumptions—linear approximability of the local Q-function and accurate weight inference from retrieved context—$\\texttt{ICQL}$ achieves bounded Q-function approximation error, and supports near-optimal policy extraction. Empirically, $\\texttt{ICQL}$ substantially improves performance in offline settings: improving performance in Kitchen tasks by up to 29. 46\\%, and in Gym and Adroit tasks by up to 6\\%. These results highlight the underexplored potential of in-context learning for robust and compositional value estimation, positioning $\\texttt{ICQL}$ as a principled and effective framework for offline RL.", "tldr": "", "keywords": ["In-context Learning", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8fa40374f27bbf7ade18a4a71db8b84e2445976.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ICQL , a novel offline RL framework that formulates Q-learning as a contextual reasoning problem. The core idea is to use a linear Transformer to learn local Q-functions from retrieved similar transition data, rather than fitting a single global Q-function. The authors argue that many tasks exhibit compositional structures and local value patterns, and they provide theoretical guarantees on bounded approximation error and near-optimal policy extraction. Experiments on the D4RL benchmark demonstrate performance improvements of 8.6% on Gym, 6.3% on Adroit, and 16.4% on Kitchen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Viewing value learning in offline RL as a In-contextual learning problem is highly innovative and well-motivated by the observation that the state space exhibits local compositional structure. The paper provides a formal analysis of bounded approximation error (Theorem 3.5) and connects retrieval quality to performance through a coverage assumption. Moreover, it successfully integrates with existing methods such as IQL and TD3+BC, demonstrating broad applicability of the proposed framework."}, "weaknesses": {"value": "1.The local linear Q-function approximation (Definition 3.1) is a standard practice in the literature. The paper’s main theoretical contribution lies in connecting this formulation to in-context learning, but the underlying assumptions—particularly Assumption 3.3 on set coverage—are quite strong and may not hold in practical settings.\n\n2.Retrieving k-nearest neighbors for each query and processing them with a multi-layer Transformer introduces significant computational overhead compared to standard methods. However, the paper does not provide any runtime analysis or comparison to quantify this cost.\n\n3.Weak justification for using linear Transformers: Why specifically choose linear attention? Although the paper claims to reveal the “underexplored potential of linear attention,” it provides no comparison with standard self-attention, leaving the rationale insufficiently supported.\n\n4.Limitations of the experimental evaluation: The experiments are conducted only on the D4RL benchmark, which is not sufficiently comprehensive. More importantly, the paper does not compare against recent state-of-the-art offline reinforcement learning methods, limiting the strength of its empirical validation."}, "questions": {"value": "1.Has the paper conducted any comparison between linear Transformers and standard self-attention? Which specific properties of linear attention are deemed essential or indispensable for the proposed method’s effectiveness?\n\n2.Assumption 3.3 requires knowledge of the coverage parameter σ, but it remains unclear how this parameter is determined in practice. How is σ set or estimated empirically, and how well does the theoretical analysis based on this assumption correlate with actual performance?\n\n3.Why does ICQL perform catastrophically on the Hammer-Human task (−49.4%)? Is this performance drop solely due to dataset characteristics, or does it reveal a fundamental limitation of the proposed method? Clarifying this distinction would help assess the robustness and generality of ICQL.\n\n4.The paper states that, \"After training, the extracted policy can be evaluated on its own without extra retrieval process or\ncontextual inference.\" However, since the policy is trained using context-dependent Q-values, how does it function without access to retrieval during evaluation? Does this setup introduce a train–test mismatch or affect the consistency between training and inference behaviors?\n\n5.Why does the paper not include comparisons with more recent approaches (e.g., [1, 2, 3, 4]) or other modern offline RL algorithms? Such comparisons are essential to demonstrate the competitiveness and relevance of the proposed method in the current research landscape.\n\n[1]Tarasov, D., Kurenkov, V., Nikulin, A., & Kolesnikov, S. Revisiting the minimalist approach to offline reinforcement learning. NIPS， 2023.\n\n[2]Mao Y, Wang Q, Qu Y, et al. Doubly mild generalization for offline reinforcement learning. NIPS, 2024.\n\n[3]Park S, Li Q, Levine S. Flow q-learning. ICML, 2025.\n\n[4]Li Q, Zhou Z, Levine S. Reinforcement learning with action chunking. arXiv preprint arXiv:2507.07969, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JedAhRtOFm", "forum": "ZBbKLvH0w4", "replyto": "ZBbKLvH0w4", "signatures": ["ICLR.cc/2026/Conference/Submission21465/Reviewer_MZEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21465/Reviewer_MZEj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210743773, "cdate": 1761210743773, "tmdate": 1762941792296, "mdate": 1762941792296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an offline RL algorithm called ICQL which solves offline Q-learning as a contextual inference problem. It retrieves local transitions from the dataset and fits a local linear Q-function via a linear-attention critic. The authors theoretically prove the bounded Q-approximation error and near-optimality guarantee for the greedy policy. In the experiments, ICQL outperforms most of the offline tasks in D4RL against offline RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide a clear formulation of their method and theoretical justificaiton of Q-approximation error bound and performance gap bound. The theoretical contribution is solid. The idea is simple but quite interesting. Capturing the local structure is often easier than considering the global pattern. The authors gives a feasible approach to this direction.\n1. The experiments on the well-known D4RL benchmark are comprehensive. They demonstrate a clear comparison against popular offline RL algorithms.\n1. The supplementary material gives sufficient information for theory verificaiton and experiment verification. They are very helpful for follow-up research."}, "weaknesses": {"value": "1. In offline RL, the coverage ratio $\\sigma$ is an important factor influencing algorithm performance. In section 4.3.3, the authors only provide an indirect ablation to analyze the effect of $\\sigma$. A direct analysis of the coverage of ideal local transition set would make the work more practical.\n1. I notice some failure cases. While I often have a good tolerance on failure cases in novel methods, the performance on Hammer-Human-v1 looks too bad. A deepr analysis of this environment would be helpful to clarify the method's scope of applicability.\n1. The Figure 3 does not make sense to me. \n    1. Does SAC work as an oracle Q value estimator in this figure? If so, since SAC has two Q networks, which one do the authors use? Additionally, do all the three methods share the same policy network to ensure fairness? I also suggest approximating Q values via Monte Carlo rollouts using the same policy as the oracle, which is less biased by the training algorithms.\n    1. The Q values should be normalized to the same scale across three plots so it can provide a uniformed color scheme to readers. The current plots are hard to analyze.\n    1. I suggest replacing the tSNE plots with scatter plots that directly show the correlation of the estimated Q values and oracle Q values.\nMinor issue\n1. In line 377, it should be \"Figure. 3\" instead of a stand alone \"3\"."}, "questions": {"value": "1. The Definition 3.2 is also not clear to me\n    1. In Equation 4, why is it $-||s_{query} - s_i||^2$? If I understand correctly, it should be $k$ nearest states to $s_{query}$, right?\n    1. $d_{min}^s$ is the minimal distance to $s_{query}$ in $\\overline{\\Omega}^k_{s_{query}}$. Shouldn't $\\Omega_{s_{query}}^{d_{min}}$ be empty since there is no closer states in this set?\n1. The algorithm is still a bit unclear to me. Should the local weight vector $w_s^*$ be trained for every $s_{query}$? If so, I recommend the authors make a comprehensive analysis in training cost as it will take enormous computational resource.\n1. Why does the algorithm fail on Hammer-Human-v1, could the authors make further justificaiton on this failure case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oZovmxaLW3", "forum": "ZBbKLvH0w4", "replyto": "ZBbKLvH0w4", "signatures": ["ICLR.cc/2026/Conference/Submission21465/Reviewer_srbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21465/Reviewer_srbX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891394400, "cdate": 1761891394400, "tmdate": 1762941791925, "mdate": 1762941791925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces In-Context Compositional Q-Learning (ICQL) for offline RL: a transformer retrieves top-k similar transitions and fits a local linear Q for each query, instead of learning a single global critic. Claims: (i) with bounded features and adequate retrieval coverage, the greedy policy is near-optimal (performance-gap bound); (ii) on D4RL (Mujoco/Adroit/Kitchen), ICQL yields consistent gains—largest on compositional tasks—supported by ablations on context length, transformer depth, and retrieval strategy. The work positions ICQL as retrieval-conditioned value learning (not return-conditioned action modeling), aiming to better exploit task locality than standard offline RL critics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear idea & scope: reframes Q-learning as local, retrieval-conditioned regression—intuitive and timely.\nTheory with knobs: bound ties performance to local linearity and retrieval coverage, matching design levers.\nEmpirically persuasive where it should be: biggest wins on multi-stage “Kitchen,” aligned with the compositionality thesis.\nAblations with signal: context length / retrieval choices matter in the expected directions."}, "weaknesses": {"value": "Baseline gap: no direct comparison to retrieval-augmented sequence models (e.g., Retrieval-Augmented Decision Transformer: External Memory for In-context RL). Both ICQL and retrieval-augmented Decision Transformer methods use external memory to condition decisions in offline RL\n\nCompute transparency: training-time cost/latency vs. IQL/CQL/TD3+BC not reported.\n\nRetrieval proxy: L2 proximity in state space may misalign with Q-similarity; robustness to metric choice is under-explored."}, "questions": {"value": "\"We observe that, for each RL control task, the state space can be inherently divided into multiple sub-tasks\": what kind of sub-tasks, please add some background description? Different parts of the state space correspond to different “phases”\n\nFigure1: I don't see obvious difference between QueryB-R1,2,3. \n\nHave you tried learned value-aware similarity (contrastive encoders predicting Q-proximity) or state–action NN retrieval, and how does that affect the theory’s coverage term?\n\nCan you report FLOPs/memory vs. baselines and show compute–performance trade-offs for context length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "djNcs96nYp", "forum": "ZBbKLvH0w4", "replyto": "ZBbKLvH0w4", "signatures": ["ICLR.cc/2026/Conference/Submission21465/Reviewer_qZiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21465/Reviewer_qZiC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934952594, "cdate": 1761934952594, "tmdate": 1762941791680, "mdate": 1762941791680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ICQL, an offline RL method that reframes Q-learning as contextual inference. For each query state-action it retrieves a small, state-similar set of transitions from the offline dataset and uses a linear transformer to infer a local linear Q-function, then plugs this into an IQL-style training pipeline. The motivation is that offline datasets often have locally coherent regions of value, but adjacent regions can differ sharply. A single global critic tends to smooth these away, whereas ICQL tries to learn \"per-query\" value estimates from retrieved context. On D4RL benchmarks, ICQL often outperforms strong offline RL baselines and the ablations support the claim that retrieval/local context actually matters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Treats offline Q-learning as \"retrieve local neighbors, do in-context TD, act\", which matches the empirical observation that value structure is local and nonuniform across the dataset.\n- ICQL often improves over IQL/CQL-style baselines on MuJoCo.\n- Varying context length / $k$ and retrieval strategy shows performance is sensitive to the retrieved neighborhood, which supports the paper's core claim that which transitions you condition on matters.\n- The paper states assumptions (local linear approximability, coverage of the local set) and derives a performance bound."}, "weaknesses": {"value": "- ICQL requires a retrieval step for every training query. For large offline buffers this $k$-NN step can dominate wall-clock, but the paper does not report complexity, index structure (exact vs. ANN), or training time vs. IQL/CQL. This is a real deployment concern the paper currently ignores.\n\n- The method’s notion of a \"nearby transition\" is Euclidean distance in the (low-dim) state space. This is reasonable for MuJoCo/Adroit/Kitchen but does not address high-dimensional observations (images) where $L_2$ is brittle and the coverage assumption becomes much harder to satisfy. A learned or task-aware retriever is not explored, only mentioned.\n\n- The theory leans on \"within a small neighborhood the Q-function is linearly approximable\". This is a strong assumption for tasks with multimodal returns, contact-rich transitions, or poorly aligned state features. The paper does not analyze what happens when the neighborhood is not well modeled by a single linear head.\n\n- The current wording about \"nearby clusters presenting as noise\" is hard to follow and mixes an empirical visualization (clusters in the dataset) with the formal local-set definition.\n\n- There are experiments varying context length / $k$, and they do show non-monotonic behavior, but the paper avoids discussing the $d,\\bar d$ definitions it introduces earlier and effectively hides the choice inside $k$-NN. A clearer statement of how performance scales with $k$ (and how they pick it) would help.\n\n- The paper instantiates the method with a linear transformer/head but does not compare against a slightly more expressive local model (e.g., a small MLP on the retrieved set). As a result, it is unclear whether \"in-context linear-TD\" is essential, or just convenient.\n\n- There are typos and phrasing that make already technical sections (the definitions and the \"in-context TD\" explanation) harder to read."}, "questions": {"value": "- What happens in ICQL when the retrieved neighborhood is not well modeled by a single linear head (e.g. contact-rich or multimodal regions)?\n\n- Could the authors clarify what is meant by \"nearby clusters presenting as noise\" and how this maps to the formal local-set definition?\n\n- The authors introduce $(d,\\bar d)$ for local sets, but the experiments effectively tune $k$ in $k$-NN. How should we relate the theory parameters to the practical $k$?\n\n- Is the choice of a linear transformer/head essential for stability/theory, or would a slightly more expressive local model (small MLP over the retrieved set) also work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dRMq0Dt0iQ", "forum": "ZBbKLvH0w4", "replyto": "ZBbKLvH0w4", "signatures": ["ICLR.cc/2026/Conference/Submission21465/Reviewer_YKZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21465/Reviewer_YKZA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967434184, "cdate": 1761967434184, "tmdate": 1762941791405, "mdate": 1762941791405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}