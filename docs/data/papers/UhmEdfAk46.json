{"id": "UhmEdfAk46", "number": 8936, "cdate": 1758103097106, "mdate": 1763737358816, "content": {"title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce HoE (Hierarchical Mixture-of-Experts), a lightweight, parameter-efficient, and plug-and-play approach that eliminates the need for model retraining, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, HoE consists of three hierarchical components: LoRA Experts, Router Experts and Weighting Router, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate HoE across various tasks on 16 objectives and 200 different preferences among 8 benchmarks, demonstrating superior performance over 15 recent baselines.", "tldr": "HoE (hierarchical Mixture-of-Experts) is a Multi-objective Alignment approach that enabling LLMs to adapt across the entire Pareto frontier with minimal resources.", "keywords": ["large language model", "multi-objective", "mixture-of-expert", "model fusion"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da14a6879b6a9a88be3c16b0a0a206f158619f82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HoE, a hierarchical mixture-of-experts framework for multi-objective alignment of large language models, enabling them to adapt to diverse and conflicting human preferences without costly retraining. HoE integrates LoRA experts, router experts, and preference routing to efficiently cover the Pareto frontier, providing scalable and fine-grained control over model behaviour."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper propose a novel alignment approach named HOE. There are some strengths:\n* **Methodology**: This paper introduces a hierarchical expert-model framework (HOE) to handle multi-objective alignment, and incorporates Pareto-optimality concepts to provide a theoretical grounding for the approach.\n* **Scalable and extensible**: leverages model fusion techniques and a lightweight routing module to enable efficient training with lower resource costs.\n* **Experiments**: The evaluation covers multiple datasets and baseline methods, yielding a comprehensive analysis of the results."}, "weaknesses": {"value": "However, there are some weakness of this paper:\n* **Methodology**: multi-objective LoRA experts are expected to learn different preferences, and the experimental results also validate it. However, the design of multi-objective router expert seems to be redundant. The ablation study only discusses a single router and the role and necessity of a multi-expert router are not clearly demonstrated.\n* **Reproducibility**: The results appear to rely on the pretrained model, yet the manuscript does not specify how the pre-trained model was chosen. The method mainly fine-tunes the routing layer while freezing LoRA parameters, but there is no detail on where the LoRA parameters come from or whether they need to be trained. Taken together, these points suggest certain reproducibility gaps in the code and experimental setup.\n* **Evaluation**: The evaluation in this paper primarily relies on reward-model scores, which may limit the ability to capture objective, user-centred aspects of quality and user needs. It would be more persuasive if the authors could explain the rationale behind the chosen metrics."}, "questions": {"value": "* Q1: The HOE integrates the PPO paradigm to optimise models. I wondering that if the reward model in PPO is the same with the reward model in evaluation?\n* Q2: In Line 1233, there may be citation error \"(??)\". Please correct it.\n* Q3: The method mainly fine-tunes the routing layer while freezing LoRA parameters, but there is no detail on where the LoRA parameters come from or whether they need to be trained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4eEuOcMD94", "forum": "UhmEdfAk46", "replyto": "UhmEdfAk46", "signatures": ["ICLR.cc/2026/Conference/Submission8936/Reviewer_tavC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8936/Reviewer_tavC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737572173, "cdate": 1761737572173, "tmdate": 1762920680002, "mdate": 1762920680002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the important and challenging problem of multi-objective alignment for LLMs. The core idea of using a hierarchical, decomposition-based MoE framework (HOE) is novel and parameter-efficient. The paper's primary strength lies in its comprehensive and very strong empirical results, demonstrating state-of-the-art performance across numerous benchmarks by dominating 15 baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The core architectural idea of a hierarchical Mixture-of-Experts for MOA, inspired by decomposition methods, is novel. \n\n2. HOE achieves state-of-the-art performance, consistently dominating the Pareto frontiers of 15 competitive baselines (including RS, MOD, and RiC) in 2-objective settings. This is a very strong empirical contribution.\n\n3. The paper features high-quality ablation studies that provide clear insights into the model's components."}, "weaknesses": {"value": "1. The paper's central claim is the achievement of \"optimal Pareto frontiers\" and \"superior Pareto-optimal results\". However, the paper provides no evidence that the proposed HOE method actually converges to the true, global Pareto optimal frontier. The theoretical analysis in Appendix G relies on strong assumptions, such as the convexity of the objective functions (Assumption G.1), which are well-known to not hold in the non-convex landscape of LLM optimization. While the use of Tchebycheff (TCH) scalarization is appropriate (as it can find non-convex frontiers), it does not guarantee that the frontier it finds is the optimal one. Therefore, all of the paper's claims of \"optimality\" are purely empirical and lack the foundational theoretical guarantees that the term \"Pareto optimal\" implies.\n\n2. The framework's design is confusing, as it introduces two distinct types of multi-objective components: \"Multi-Objective LoRA Experts\"  and \"Router Experts\". Both components appear to serve the exact same purpose: covering intermediate points on the Pareto frontier. The paper fails to provide a clear justification for why both are necessary.\n\n3. This paper lacks of criticial details. A core component, the Merge function used to create multi-objective experts ($\\tau_{\\lambda}=Merge(...) $), is never defined in the paper.  The \"task-SVD\" compression process is also vague. The example in Appendix E.1  suggests it may involve manual, per-objective hyperparameter tuning, which would severely undermine the \"plug-and-play\" and \"lightweight\" claims. Furthermore, the mathematical formulation for the router optimization is inconsistent between the main text and the appendix, confusing any attempt at re-implementation. \n\n4. There are several issues for the Proof in App. G as well. \n\nFirst, the proof's strongest claims of convergence rely entirely on Assumption G.1 (Convexity). The parameters $\\theta$ being optimized belong to the Router Experts, which are neural network layers within a Transformer. The optimization landscape for LLMs and deep neural networks is well-known to be highly non-convex.\n\nSecond, even if local stationary point convergence is established, a local stationary point is not, by any means, equivalent to a Pareto optimal solution.\n\nThird, Lines 1480-1494 are very confusing. For example: â€œIf none of these conditions hold but assumptions 1, 2, 3, and 4 remain valid...\"\n\n5. Please clarify some mathematical formulations. The main text in Section 3.2 (Eq. (6)-(8)) introduces an Online Mirror Descent (OMD) method for the Tchebycheff (TCH) objective . However, Appendix E.3 (Eq. (12)-(18)) shows a different-looking formulation that is supposedly based on the same TCH and OMD principles. The relationship between Eq. (8)  and Eq. (18)  is unclear, even though both appear to describe the same PPO gradient"}, "questions": {"value": "Please see the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uceGcZuss2", "forum": "UhmEdfAk46", "replyto": "UhmEdfAk46", "signatures": ["ICLR.cc/2026/Conference/Submission8936/Reviewer_V6P4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8936/Reviewer_V6P4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859715134, "cdate": 1761859715134, "tmdate": 1762920679570, "mdate": 1762920679570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors studied the problem of multi-objective alignment problem in LLMs. To address the problem, they decompose the alignment problem into a number of single-preference subproblems, each of which handled by specialized experts. They combined LoRA experts, router experts, and preference routing to address the problem in their hierarchical MOE framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow.\nA number of different NLP tasks were taken to evaluate the performance of the proposed framework.\nA number of datasets were used to conduct the experiments."}, "weaknesses": {"value": "See the below questions."}, "questions": {"value": "Motivations of why the authors combine LoRA experts, router experts, and preference for the multi-objective alignment problem are not convincing, as there are many methods/strategies to address such tasks. \n\nBecause the proposed framework is combined by LoRA experts, router experts, and preference, there are a large number of parameters applied in the framework, although the authors tried to reduce the size of the LLM by their lightweight, parameter-efficient, and plug-and-play approach. In fact, such combination may not be appropriate to address the multi-objective alignment ask due to the size of the LLMs after the combination.\n\nNot sure if there are other ways to eliminate the need to train the proposed model beside the propose HoE? \n\nAre there any additional baselines that were published in this year to be taken as baselines in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eDcmEQpSO2", "forum": "UhmEdfAk46", "replyto": "UhmEdfAk46", "signatures": ["ICLR.cc/2026/Conference/Submission8936/Reviewer_cfcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8936/Reviewer_cfcq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762270215514, "cdate": 1762270215514, "tmdate": 1762920679085, "mdate": 1762920679085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}