{"id": "ZGWwV34Vmu", "number": 783, "cdate": 1756817986962, "mdate": 1759898242167, "content": {"title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks", "abstract": "Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning.", "tldr": "BridgeV2W bridges pretrained video generation models to embodied world models via embodiment masks that align actions with pixel spaces, while ensuring viewpoint robustness and embodiment-agnostic architectures.", "keywords": ["World Models", "Robotic Manipulation", "Action-Conditioned Video Prediction"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6245a9adac6f8addc16c7928b883739ebda7527.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "BridgeV2W proposes a unified framework that bridges pretrained video generation models with embodied world models (EWMs) for robotic applications. The key innovation is transforming coordinate-space actions into pixel-aligned embodiment masks rendered from URDF and camera parameters. These masks are then injected into pretrained video diffusion models using a ControlNet-style conditioning pathway. It also introduces a flow-based motion loss to emphasize dynamic, task-relevant motion regions over static backgrounds. BridgeV2W achieves state-of-the-art video generation results on the DROID (single-arm) and AgiBot-G1 (dual-arm) datasets, showing robustness to unseen viewpoints and scenes, and unification across embodiments. It also demonstrates potential in downstream robotics tasks, such as policy evaluation and goal-conditioned planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The embodiment mask design elegantly bridges the gap between coordinate-space actions and pixel-space video prediction.\n2. Consistent improvements across PSNR, SSIM, LPIPS, and especially FVD and Mask-IoU metrics on both datasets. Notable robustness in unseen-view and unseen-scene settings (Table 1).\n3. The introduced flow-based motion loss is interesting, as it encourages learning from dynamic, task-relevant regions.\n4. Demonstrates practical use for real-world policy evaluation and goal-conditioned planning, beyond just simple video generation evaluation."}, "weaknesses": {"value": "1. The approach assumes access to precise URDFs and camera parameters, which may not hold for in-the-wild or human video data (although segmentation-based alternatives are mentioned).\n2. The goal-conditioned manipulation tasks show modest performance (13/40 successes vs. 17/40 from VLA baselines), indicating that planning still struggles with complex motion or rotation-heavy actions.\n3. How sensitive is BridgeV2W to inaccurate URDFs or camera calibration errors? Would learned or self-calibrated projection functions suffice?\n4. How might BridgeV2W integrate with modern VLA frameworks (like π₀ or OpenVLA) for closed-loop planning instead of offline CEM optimization?\n5. Visual Action Prompts (ICCV’25) [1] presents a similar concept by projecting complex 3D dynamics into 2D action prompts, which makes the novelty of this paper appear limited.\n\n[1] Precise Action-to-Video Generation Through Visual Action Prompts. Wang etal. ICCV 2025"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ROfPnwRXLC", "forum": "ZGWwV34Vmu", "replyto": "ZGWwV34Vmu", "signatures": ["ICLR.cc/2026/Conference/Submission783/Reviewer_MKGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission783/Reviewer_MKGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884336067, "cdate": 1761884336067, "tmdate": 1762915603605, "mdate": 1762915603605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel method to mitigate three key gaps in world modelling, namely, Action-Video Gap, Sensitivity, and Architecture Across Embodiments. \n\nIn this paper, the world model basically takes the following two as its inputs:\\\na) an initial frame as current state S;\\\nb) an input action sequence A in forms of either Cartesian end effector or joint motion;\\\nand predicts the future frames, which is seen as the influence of the action to the embodied world.\n\nThe method basically leverages video generation as the base model, while incorporate comprehensive techniques such as a) Embodiment Masks, b) ControlNet-Style Conditioning, and c) Flow-Based Motion Loss. \n\nThe strong empirical results and divese downstream applications shows the method is a promising step towards its goal: Bridging Video Generation Models to Embodied World Models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High originality: Rather than treating robot actions as abstract coordinate vectors (e.g., end-effector poses), the authors propose rendering them as pixel-aligned embodiment masks using readily available URDF models and camera parameters. This insight effectively reconciles the semantic and representational mismatch between low-dimensional control signals and high-dimensional video generation models.\n\n2. Rigorous and thorough: Experiments span two diverse robotic platforms (single-arm DROID and dual-arm AgiBot-G1), with careful evaluation under in-domain, unseen-viewpoint, and unseen-scene conditions. \n\n3. Broad significance: For both robotics and generative modeling communities."}, "weaknesses": {"value": "1. Dependence on Precise Camera Calibration and URDF. \n\nThe core embodiment mask generation pipeline assumes access to accurate camera intrinsics/extrinsics and a complete URDF model. While common in controlled lab settings (e.g., DROID, AgiBot-G1), this requirement severely limits applicability in real-world or human-in-the-loop scenarios where:\n\n- Camera calibration may drift or be unavailable (e.g., mobile phones, uncalibrated webcams),\n- URDFs may be missing (e.g., legacy industrial arms, soft robots, or human demonstrators).\n\nAlthough the paper mentions using GroundedSAM to extract masks from video in URDF-free settings (Sec 3.2), this is only briefly noted and not evaluated experimentally.\n\n2. Limited Evaluation of Long-Horizon Coherence and Error Accumulation\n\nThe model predicts 25-frame videos (~2.5s at 10 FPS), which is sufficient for short-horizon tasks but does not assess compounding errors in longer rollouts—a critical flaw for world models used in MPC or policy evaluation over extended horizons. \n\nMoreover, the dynamics-consistency loss (Eq. 4) uses only up to K=4 latent-frame offsets, which may not capture long-range dependencies.\n\n3. Downstream Planning Performance Lags Behind VLA Baselines\n\nIn Table 5 (and corrected Table 8 in Appendix), BridgeV2W underperforms strong VLA policies (e.g., π0, SpatialVLA) on all tasks, especially those requiring precise rotation (e.g., flip cup: 0/10 vs. OpenVLA-OFT’s 2/10). This raises questions about its practical utility as a planner.\n\nThe paper attributes this to “harder search over rotational DOFs,” but does not explore whether the world model itself misrepresents rotational dynamics (e.g., due to coarse mask rendering or diffusion artifacts)."}, "questions": {"value": "The paper mentions (Sec 3.2) that \n\n> embodiment masks can be extracted via segmentation tools like GroundedSAM in settings without URDF or camera calibration (e.g., human–robot videos). However, this pathway is not evaluated experimentally. \n\nHave the authors tested autoregressive multi-step rollouts (e.g., chaining predictions over 5+ steps)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mOYu6JqrVI", "forum": "ZGWwV34Vmu", "replyto": "ZGWwV34Vmu", "signatures": ["ICLR.cc/2026/Conference/Submission783/Reviewer_dagg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission783/Reviewer_dagg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909229770, "cdate": 1761909229770, "tmdate": 1762915603313, "mdate": 1762915603313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BridgeV2W converts coordinate-space actions into pixel-aligned embodiment masks that let pretrained video generators model robot behavior. Using the robot's URDF and known camera intrinsics/extrinsics, it renders per-view masks and injects them into a CogVideoX-5B-I2V backbone through a ControlNet-style branch to predict future frames from an initial image. A flow-based motion loss emphasizes dynamic, task-relevant regions. Trained on DROID and AgiBot-G1, the framework reportedly improves temporal realism, perceptual quality, and action-video alignment across in-domain, unseen-viewpoint, and unseen-scene settings compared other embodied world model baselines such as IRASim, Cosmos, and EVAC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1:** Clear motivation and observation: large-scale pretrained video generation models suffer from three key limitations and if the action representation is transformed into a pixel-aligned mask that reflects the embodiments's actual motion, these limitations can be substantially mitigated. URDF and camera intrinsic and extrinsics provide a solid approach to tackle this.\n\n**S2:** Motion-centric training objective: the paper adds a flow-based motion loss that emphasizes dynamic task-relevan regions on top of diffusion and latent dynamics-consistency objectives.\n\n**S3:** Reproducibility details: the architecture choice (CogVideoX-5B-I2V), training resolution/horizon, clip sampling, and extensive hyperparameters are documented."}, "weaknesses": {"value": "**W1:** Mask-IoU evaluates alignment between segments of generated and ground‑truth frames. But because BridgeV2W is conditioned on URDF-rendered masks, the metric remains highly correlated with the conditioning signal and may not accurately model motion or contact.\n\n\n**W2:** The experiments labeled as \"unseen camera viewpoint\" give the method ground-truth camera intrinsics/extrinsics at test time and use the URDF to project a per-view robot mask that is injected into the video generator. This solves the geometric part of cross-view prediction outside the model and makes the task closer to appearance completion conditioned on an oracle silhouette. Baseline world moedls do not receive an equivalent calibration/mask signal are at a structural disadvantage."}, "questions": {"value": "**Q1:** The baselines do not get an equivalent mask/geometry channel. So, why is mask IoU a fair metric? Does it make sense to include it in the results tables?\n\n**Q2:** If  the same per-view mask or equivalent calibration features would be provided to baselines, could they utilize these and yield better results?\n\n**Q3:** In Line 441, you report that BridgeV2W is sometimes \"optimistic\". Are there any options to avoid generating successful rollouts when action errors are modest?\n\n**Q4:** In Line 464, you mention that the reason why BridgeV2W works poorly in substantial rotation is due to the harder search over rotational degrees of freedom. While I can understand that this is harder problem, I suspect that this might be related to \"Weakness 1 (W1)\", as a silhouette-driven conditioning signal is less informative for certain rotations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UgZ2NDA4tb", "forum": "ZGWwV34Vmu", "replyto": "ZGWwV34Vmu", "signatures": ["ICLR.cc/2026/Conference/Submission783/Reviewer_ykWF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission783/Reviewer_ykWF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762553461339, "cdate": 1762553461339, "tmdate": 1762915603089, "mdate": 1762915603089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/2)"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. We are encouraged that they consistently recognized the strengths of our work: the clear motivation and effectiveness of pixel-aligned embodiment masks in mitigating key limitations of previous embodied world models (R. `ykWF`), the originality and rigor of our cross-embodiment design and evaluations (R. `dagg`), and the solid empirical gains across video quality, action–video alignment, and downstream tasks (R. `MKGQ`). We also appreciate the positive remarks on our motion-centric training objective and the thorough reproducibility details provided.\n\nFor clarity and to avoid repetition, we begin by consolidating the two major concerns that were commonly raised across reviewers.\n\n> **Concern 1: Reliance on Camera Calibration and URDF**\n\nWe appreciate the reviewers for raising this important point. BridgeV2W requires camera intrinsics/extrinsics and a URDF **only at inference time**, as these are needed to calculate the pixel-aligned embodiment masks (“calc masks”) used for action conditioning. This requirement does not extend to training: most training data could come from uncalibrated videos by extracting segmentation-derived masks (“seg masks”) with tools such as GroundedSAM. In practice, we find that incorporating a small fraction of calc-mask robot data during training is sufficient to anchor the model to the target embodiment, while the bulk of supervision could come from segmentation-only human or robot videos. This design enables BridgeV2W to naturally **scale to large unstructured datasets where camera parameters and URDFs are unavailable**.\n\nTo validate this capability, we conducted additional experiments on the Ego4D FHO subset [1], which provides diverse egocentric human–hand videos without any calibration or kinematic model. We extracted hand masks using GroundedSAM and combined them with AgiBot-G1 robot data under several training configurations. As shown in the table, training with segmentation-derived masks on the robot data (100% G1 seg) still offers usable supervisory signals, though it differs from the precise calc masks used at inference time. Crucially, introducing large-scale Ego4D segmentation masks and mixing in only a small portion of G1 calc-mask data nearly recovers the performance of full calc-mask training, indicating that human-centric segmentation masks contribute rich motion priors, while a limited amount of calibrated robot data is sufficient to align with the robot’s embodiment. This mixed configuration provides an effective and scalable way to leverage uncalibrated datasets while requiring only minimal calibrated supervision.\n\nRegarding inference-time requirements: although BridgeV2W needs approximate camera parameters and a URDF model to render embodiment masks, this setting is realistic and lightweight for modern robotic systems, where such metadata is routinely maintained for control, teleoperation, or simulation. Importantly, the geometry used in our experiments is not all obtained via ideal checkerboard calibration. For instance, many of the camera intrinsics/extrinsics parameters in the DROID dataset’s come from CtRNet-X [2], a learned 2D–3D alignment method that introduces estimation noise. Yet, BridgeV2W performs strongly across all benchmarks. This shows that our method operates reliably under approximate, readily obtainable calibration. Furthermore, recent methods such as URDFormer [3] can recover articulated URDF models directly from RGB images when a URDF is unavailable.\n\nOverall, BridgeV2W leverages broad, uncalibrated video sources during training while requiring only lightweight geometric information at inference time. Under these practical and easily satisfied conditions, the model consistently achieves strong performance across all benchmarks, demonstrating both scalability during training and practicality during deployment.\n\n| Data Source                                                  | PSNR ↑    | SSIM ↑    | LPIPS ↓   | FVD ↓     | Mask-IoU ↑ |\n| ------------------------------------------------------------ | --------- | --------- | --------- | --------- | ---------- |\n| 100% G1 **calc mask**           | 24.49     | **0.868** | **0.102** | 129.5     | **58.3**   |\n| 100% G1 **seg mask**             | 22.87     | 0.822     | 0.129     | 191.6     | 53.9       |\n| 30% G1 **calc mask** + Ego4D **seg mask**                    | 24.28     | 0.850     | 0.118     | 133.9     | 57.2       |\n| 70% G1 **seg mask** + 30% G1 **calc mask** + Ego4D **seg mask** | **24.58** | 0.863     | 0.108     | **118.5** | 58.1       |"}}, "id": "F327FwpaCO", "forum": "ZGWwV34Vmu", "replyto": "ZGWwV34Vmu", "signatures": ["ICLR.cc/2026/Conference/Submission783/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission783/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission783/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763628999871, "cdate": 1763628999871, "tmdate": 1763628999871, "mdate": 1763628999871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}