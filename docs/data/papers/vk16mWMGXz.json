{"id": "vk16mWMGXz", "number": 21074, "cdate": 1758313457633, "mdate": 1759896943460, "content": {"title": "Multi-modal Data Mixtures for Vision-Language Model Training", "abstract": "Vision-Language models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. This paper introduces MMix, a principled framework for automatically determining multi-modal data mixtures for VLM training. We formulate this task as a modality-aware alignment maximization over domains, deriving multi-modal alignment scores from the dual solution through inter-modal coupling variables. Our method is crucially designed to handle domains with missing modalities, allowing for the systematic integration of language-only domains. In experiments on both 0.5B and 7B VLMs, MMix boosts accuracies on diverse evaluation benchmarks with marginal computational cost. Remarkably, it matches the expert-tuned performance 1.28$\\times$ faster in image-text tuning and extends to more complex multi-modal video scenarios outperforming uniform weights performance with only 33\\% steps.", "tldr": "We introduce a systematic data-mixing framework for vision-language models by deriving multi-modal alignment scores that can further handle missing modalities.", "keywords": ["Data mixture", "Vision-language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b66ede02fec14d7daa0467b336cce4ef8325928.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "As described in Introduction, the central question in this paper is to find out how to determine the optimal proportions of various domains for training multi-modal models. To this question, the authors propose to solve some optimization problem for finding an aligning vector $w^v$ for each modality $v$, and use it to compute scores $S^v_i$ for each modality $v$ and domain $i$, and finally compute the domain weight $p_i$ by softmax of the scores $\\sum_v S^v_i$. In experiments, they show that the proposed method (marginally) outperforms the baselines of uniform or manual weighting, in multiple settings with modalities like texts, images, and videos."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall well-structured and easy to follow.\n- The experimental design covered three modals, beyond the image-text pair.\n- The proposed method seems to be simple to implement, and computationally feasible under a limited number of domains.\n- Experimental results show marginal improvements compared to the baseline methods."}, "weaknesses": {"value": "- It is overall unclear why the proposed method should work well. Particularly, it is unclear 1) why the exponential distribution with scores is expected to provide appropriate weights for each domain, 2) why we should compute these scores by the inner product between the domain's centroid $x_i$ and the aligned vector $w$, and 3) why the optimization problem eq.3 is appropriate to find aligning vectors for computing such scores. Or, another question is: 4) how the optimality of the eq.3 or eq.4 leads to the \"optimal proportions\" between domains (beyond subjective explanations).\n- It is unclear how much the \"modality-aware\" weighting contributes to the improvement in accuracy by the proposed method, which is claimed as a core contribution of this work.\n- The accuracy gain seems marginal even with additional weighting compared to the naive baselines. I think the proposed method may have almost no significance in practice, rather than collecting additional data or scaling up models."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2ATUGjiFUz", "forum": "vk16mWMGXz", "replyto": "vk16mWMGXz", "signatures": ["ICLR.cc/2026/Conference/Submission21074/Reviewer_5Vc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21074/Reviewer_5Vc9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842792325, "cdate": 1761842792325, "tmdate": 1762940642815, "mdate": 1762940642815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the multiple domain dataset mixing problem used to train vision-language models (VLMs), which integrate images and language by aligning image features with language tokens. While research exists on determining the domain mixing ratio for text-modal LLMs, VLMs have continued to rely on heuristics to determine resampling weights. The paper formulates this ratio determination as a problem of maximizing alignment with general structures across modalities and domains in latent space. This score is computed by solving a linear equation with regularization over the feature space of VLM hidden states. Furthermore, the paper extends this formulation to handle data with missing modalities. Experiments demonstrate that the proposed method efficiently improves performance compared to uniform weights or weights based on human heuristics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** This paper likely formulates the resampling weight determination problem for datasets in VLMs for the first time and proposes an algorithm to solve it.\n- **S2.** The algorithm proposed in this paper can simultaneously handle both multi-modal and uni-modal data, featuring a simple structure that solves closed-form solutions using regularized linear equations.\n- **S3.** Experiments conducted on relatively lightweight models such as 0.5B and 7B suggest that the proposed method reliably improves the weights by uniform sampling and human heuristic."}, "weaknesses": {"value": "- **W1.** The relationship between alignment of general structures in latent space and generalization performance has not been theoretically explained. This suggests that maximizing alignment of general structures, upon which the proposed method relies, may not lead to improved generalization performance of VLMs.\n- **W2.** The paper refers to the proposed method as an “automatic data mixing strategy,” but in reality, it requires human-defined domains, and there is insufficient discussion on how these domain definitions should be specified. For example, datasets like TextVQA [a] mix multiple domains such as OCR, document reading comprehension, and mathematical reasoning, and it remains unclear how to handle such composite capabilities.\n- **W3.** The proposed method relies on the latent state representations of existing VLMs. This means it determines the dataset mixing ratio by measuring alignment at the pretraining stage. In other words, the alignment state may fluctuate across iterations or epochs, and weights computed at pretraining may become suboptimal at the midpoint of training. Furthermore, if the performance of VLMs at the pre-training stage is insufficient, they may fail to recognize domains and thus be unable to perform effective feature extraction.\n- **W4.** The paper has not verified scalability for models larger than 7B. However, since this verification requires substantial computational resources, it is not essential for verifying the research question. On the other hand, it is highly important for practical applications to investigate generalization across models. To further strengthen this claim within the accessible computational environment of the paper, ideas include computing weights using a 7B model and comparing the differences with the 0.5B model's weights. Additionally, it might be beneficial to confirm the generalizability of VLMs by using others besides LLaVA, such as Qwen2.5-VL [b] or InternVL3.5 [c].\n- **W5.** The proposed algorithm requires $O(k^3)$ computational complexity for a domain size $k$, which could become a bottleneck if we anticipate further increases in the number of composite domains, as discussed in W2.\n\n[a] Singh, Amanpreet, et al. \"Towards vqa models that can read.\" CVPR 2019.\n\n[b] Bai, Shuai, et al. \"Qwen2. 5-vl technical report.\" arXiv preprint arXiv:2502.13923 (2025).\n\n[c] Wang, Weiyun, et al. \"Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency.\" arXiv preprint arXiv:2508.18265 (2025)."}, "questions": {"value": "Please response the concerns raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zn9dzyfFE9", "forum": "vk16mWMGXz", "replyto": "vk16mWMGXz", "signatures": ["ICLR.cc/2026/Conference/Submission21074/Reviewer_HbVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21074/Reviewer_HbVQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977121389, "cdate": 1761977121389, "tmdate": 1762940642230, "mdate": 1762940642230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework, named MMix, for automatically determining multimodal data mixtures for VLM training. The determination problem of data mixture is formulated as a modality-aware alignment maximization across domains. The multi-modal alignment scores obtained from the dual solution through inter-modal coupling variables. In the proposed framework, missing modalities can also be handled easily, allowing for the systematic integration of heterogeneous multi-modal data. In experiments, 0.5B and 7B VLMs are applied to various benchmarks. Then, it has been shown that MMix can achieve better accuracies with marginal computational cost."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The idea of deriving mixture weights from the dual solution of an alignment-maximization problem is a novel idea. And the detailed and careful derivation is provided in Appendix. It provides a theoretical pathway for extending data-mixing methods developed for LLMs to the multimodal domain.\n\nThis method proposes a way to handle a missing-modality problem naturally in the equation.\n\nThe computational cost is drastically reduced. This fact shows the practicability of the proposed method.\n\nIn 4.2, it has been demonstrated that MMix scales to more complex multi-modal settings such as VideoQA.\n\nFrom the multiple aspects listed above, the proposed method is practical and useful as well as being supported by theoretical guarantees.\n\nAblation studies in B.6 also facilitate better understanding of the proposed method."}, "weaknesses": {"value": "The experimental validation is done only with LLaVA-OneVision families. The authors may want to conduct more extensive experiments using other fundamental models to show their generality. Discussion on whether the weights can also generally be transferred to other fundamental models would also be helpful.\n\nThis paper can be reorganized and revised to facilitate better understanding of possible readers. For instance, the flowchart in Figure 1 and the subsections in section 3 do not align each other, which may confuse possible readers. \n\nSome minor modification proposals (no need to reply)\n- Table captions are sometimes confusing. For instance, that for Table 2 does not mention that the weights are transferred from 0.5B to larger 7B model (such description is given in the manuscript but not in the caption). Therefore, readers may be confused with weights dedicatedly designed for the larger model.\n- The authors might want to conduct statistical significance test to claim the “best” performance. I am OK even if there is no significance."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nOdsG9xO3h", "forum": "vk16mWMGXz", "replyto": "vk16mWMGXz", "signatures": ["ICLR.cc/2026/Conference/Submission21074/Reviewer_W7md"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21074/Reviewer_W7md"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004388429, "cdate": 1762004388429, "tmdate": 1762940641586, "mdate": 1762940641586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automated multimodal data mixing method, MMix. Its core innovation lies in quantifying the importance of each domain in training through \"modality-aware domain alignment scores\", and dynamically adjusting the sampling weights accordingly. The theoretical basis of this method is established on the framework of maximizing domain alignment and coupling shared latent variables, formally deriving an unsupervised and scalable weight distribution mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper focuses on how to automatically allocate sampling weights for multi-domain and multi-modal data in VLM training without relying on manual parameter tuning or expensive grid search. It is clearly pointed out that there are problems such as modal deficiency, domain heterogeneity, and the non-scalability of manual parameter adjustment in VLM training. The problem settings are close to the actual needs of the industry."}, "weaknesses": {"value": "- The paper assumes that all domains should be aligned in a shared and unified direction (i.e., the projection vector w), and weights are allocated by maximizing the alignment score. This assumption ignores the heterogeneity among domains. For instance, the semantic spaces of the Math/Reasoning and OCR domains may not be in the same direction at all. Forcing alignment could lead to excessive compression or misleading weighting. This assumption is similar to the idea of single-view PCA, but in multimodal tasks, different domains may correspond to multiple independent semantic subspaces rather than a globally shared direction.\n\n- The kernel matrix is simply summed by elements $K_{MM} = \\sum_v K^{[v]}$. This linear superposition method ignores the scale differences and semantic inconsistencies between modes. For instance, the embedding dimension or norm of an image modality may be much larger than that of a text, resulting in its dominant kernel matrix.\n\n- The VLM models used in the experiment are all out-of-dated. LLaVA is a 2023 model. It is recommended to use newer LMs for the experiment, which will be more convincing."}, "questions": {"value": "Please respond to the theoretical issues listed in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DBd0ai4lwY", "forum": "vk16mWMGXz", "replyto": "vk16mWMGXz", "signatures": ["ICLR.cc/2026/Conference/Submission21074/Reviewer_SyKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21074/Reviewer_SyKh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087030773, "cdate": 1762087030773, "tmdate": 1762940641006, "mdate": 1762940641006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}