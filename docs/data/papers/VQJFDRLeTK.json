{"id": "VQJFDRLeTK", "number": 416, "cdate": 1756738737018, "mdate": 1759898262146, "content": {"title": "Efficient Edge Test-Time Adaptation via Latent Feature Coordinate Correction", "abstract": "Edge devices face significant challenges due to limited computational resources and distribution shifts, making efficient and adaptable machine learning essential. Existing test-time adaptation (TTA) methods often rely on gradient-based optimization or batch processing, which are inherently unsuitable for resource-constrained edge scenarios due to their reliance on backpropagation and high computational demands. Gradient-free alternatives address these issues but often suffer from limited learning capacity, lack flexibility, or impose architectural constraints. To overcome these limitations, we propose a novel single-instance TTA method tailored for edge devices (TED), which employs forward-only coordinate optimization in the principal subspace of latent using the covariance matrix adaptation evolution strategy (CMA-ES). By updating a compact low-dimensional vector, TED not only enhances output confidence but also aligns the latent representation closer to the source latent distribution within the latent principal subspace. This is achieved without backpropagation, keeping the model parameters frozen, and enabling efficient, forgetting-free adaptation with minimal memory and computational overhead. Experiments on image classification and keyword spotting tasks across the ImageNet and Google Speech Commands series datasets demonstrate that TED achieves state-of-the-art performance while $\\textit{reducing computational complexity by up to 63 times}$, offering a practical and scalable solution for real-world edge applications. Furthermore, we successfully $\\textit{deployed TED on the ZYNQ-7020 platform}$, demonstrating its feasibility and effectiveness for resource-constrained edge devices in real-world deployments.", "tldr": "An efficient single-instance TTA method for edge devices, leveraging forward-only optimization in the latent principal subspace.", "keywords": ["Test-time Adaptation", "Edge Devices", "Forward-Only", "Latent Feature"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83859d8d30f08c8cb451af27613931fbd56a27cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper highlights the importance of computational efficiency in test-time adaptation (TTA) and proposes an efficient framework called TED (**T**TA for **E**dge **D**evices). In brief, TED first precomputes basis vectors from source data during the source training phase. At test time, the encoded representation of a target sample is optimized through the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [a] to align it with the source distribution, and the resulting projected representation is then used for the final prediction. The proposed approach is extensively evaluated across multiple datasets, tasks, devices, and network architectures, demonstrating its scalability.\n\n[a] The CMA Evolution Strategy: A Tutorial. arXiv 2016"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The method is novel and described clearly. Figure 2 effectively summarizes the overall procedure.\n2. The effectiveness of the proposed method is shown in both performance and resource efficiency across various architectures and edge devices. It is surprising that TED outperforms T3A with about one-third of the time cost\n3. The experiments cover multiple benchmarks, including image classification and speech recognition tasks."}, "weaknesses": {"value": "1. Although the authors emphasize TTA efficiency, the discussion overlooks recent studies with similar goals. The related work (eg, T3A, MEMO, and SAR) mainly covers older methods. Adding comparisons or discussions (eg, Line 59) with recent efficiency-focused works [1–4], and including time and memory results beyond T3A, MEMO, and SAR, would strengthen the paper.\n2. The proposed method shows strong results, but it is questionable whether this advantage mainly comes from the single-instance setup, where the baselines naturally struggle. Although SAR and MEMO follow the same condition, they are relatively simple and outdated. It would be valuable to evaluate TED in batch adaptation settings and in comparison with existing works such as T3A, MEMO, and [1–4]. (In fact, the single-instance setup in this paper cannot be considered fully practical, similar to the “multiple images in a batch” setup, since the inputs are not streaming or continuous [5].)\n3. All six subplots in Figure 3 convey the same message: \"the adapted features are closer to the original features than the corrupted ones\". Since this information is redundant, it might be better to keep only Figure 3(f). In addition, Table 8 in the supplementary material would fit better in the main paper.\n\n[1] MECTA: Memory-Economic Continual Test-Time Model Adaptation ICLR 2023\n\n[2] EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization CVPR 2023\n\n[3] BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation ICML 2024\n\n[4] SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity CVPR 2025\n\n[5] NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation NIPS 2022"}, "questions": {"value": "1. Please refer to the weaknesses above.\n2. The name “TED” does not clearly represent the proposed approach. Revising it to better align with the proposed approach could be considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D2jwe0Fd0m", "forum": "VQJFDRLeTK", "replyto": "VQJFDRLeTK", "signatures": ["ICLR.cc/2026/Conference/Submission416/Reviewer_JVp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission416/Reviewer_JVp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690304700, "cdate": 1761690304700, "tmdate": 1762915515372, "mdate": 1762915515372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper Summary: This paper proposes TED (Test-time adaptation for Edge Devices), a novel single-instance test-time adaptation method designed for resource-constrained edge devices. The method performs forward-only coordinate optimization in the principal subspace of latent features using CMA-ES (Covariance Matrix Adaptation Evolution Strategy). By updating only a compact low-dimensional vector representing coordinates in the source latent principal subspace, TED achieves efficient adaptation without backpropagation while keeping model parameters frozen. The authors validate their approach on image classification (ImageNet series) and keyword spotting (GSC-C) tasks, demonstrating up to 63× reduction in computational complexity while achieving state-of-the-art performance in single-instance TTA scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Paper Strengths: (1) The problem formulation is well-motivated and addresses a critical gap in TTA for edge devices, where gradient-based and batch-dependent methods are impractical due to computational and memory constraints.\n(2) The core idea of correcting coordinates in the latent principal subspace is elegant and theoretically grounded. The mathematical framework (Equations 2-7) clearly demonstrates how the proposed method reformulates TTA as a coordinate correction problem.\n(3) The method is architecture-agnostic and demonstrates strong versatility across different backbone networks and diverse tasks (image classification and keyword spotting)."}, "weaknesses": {"value": "Paper Weaknesses: (1) The overall novelty appears limited. While the overall framework is well-designed, the individual components show limited innovation. The use of PCA-based subspace projection is standard in dimensionality reduction, and CMA-ES is an existing optimization algorithm. The main contribution appears to be the combination of these techniques rather than fundamental algorithmic innovation.\n(2) In Section 3.2 and Appendix A, the authors claim that minimizing Shannon entropy drives OOD features closer to the source domain in Vk. However, this connection relies on strong assumptions that may not hold in practice, especially under severe distribution shifts. The paper does not empirically validate these assumptions or provide theoretical guarantees.\n(3) The paper does not adequately explain why CMA-ES is the optimal choice for this problem. While CMA-ES is gradient-free, other evolutionary algorithms or simpler random search methods might be more efficient for low-dimensional optimization. No comparison with alternative gradient-free optimizers is provided.\n(4) As shown in Table 1, TED shows minimal improvements for certain corruption types. The paper acknowledges this limitation only briefly in Section 4.3 but does not provide sufficient analysis of when and why the method fails.\n(5) Although the experimental results are generally honest, the datasets used are limited. It is recommended that the authors conduct further validation on more challenging datasets (e.g., CIFAR10-C, CIFAR100-C and DomainNet) to enhance the generalizability and persuasiveness of the conclusions.\n(6) The paper does not include comparisons with recent state-of-the-art methods such as [1] and [2], which achieve good performance. As these approaches are both effective and relatively simple, their omission makes it difficult to assess the novelty and practical advantage of the proposed approach. \n[1] A Versatile Framework for Continual Test-Time Domain Adaptation: Balancing Discriminability and Generalizability, CVPR 2024 \n[2] Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction, WACV 2024"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WkMn9sgItc", "forum": "VQJFDRLeTK", "replyto": "VQJFDRLeTK", "signatures": ["ICLR.cc/2026/Conference/Submission416/Reviewer_QpSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission416/Reviewer_QpSX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733357228, "cdate": 1761733357228, "tmdate": 1762915515213, "mdate": 1762915515213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets single-instance test-time adaptation (TTA) on edge devices. It leverages source-domain latent statistics computed offline, and at test time keeps model parameters frozen while forward-only optimizing a low-dimensional coordinate to align the target latent toward the source distribution. The authors report accuracy/efficiency gains over prior TTA methods on image classification and keyword spotting, and support practicality with on-device experiments using the quantized variant (QTED)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method is simple, facilitating implementation and portability, and achieves computational efficiency by updating only the k-dimensional adaptation variable at test time rather than the model parameters.\n\n- The approach is architecture-agnostic, applying to diverse backbones (e.g., CNNs, Transformers, RNN/LSTM encoders) as long as a latent representation is available."}, "weaknesses": {"value": "- Practicality limited by dependence on source statistics. The paper’s central contribution hinges on source-domain latent statistics. Appendix C.3 shows clear sensitivity to the number of source samples used to compute those statistics (e.g., 5k is insufficient, and Table 1 relies on a full 50k). Moreover, obtaining the statistics requires forwarding the entire source dataset through the encoder to build the latent mean and subspace, which is often infeasible in real deployments where source data cannot be accessed or processed post-training. Without these statistics, the method cannot run, which substantially undermines practical applicability.\n\n- The core contribution is test-time latent-space modification guided by source statistics, which is not new, and the optimization relies on an existing algorithm (CMA-ES). Moreover, the on-device deployment appears to hinge primarily on reducing parameter bit-width (quantization). Overall, the contribution reads as incremental rather than a substantive advance.\n\n- Potentially unfair comparisons. Many baselines (e.g., T3A/CoTTA/SAR/MEMO/FOA) do not use source data or source statistics, whereas the proposed method starts with source statistics. Comparing across these different information regimes risks unfair conclusions. It would also help to compare against work that introduces single-instance backpropagation-free TTA via normalization and to explain the differences from [r1].\n\n- Quantization fairness and presentation. Appendix A discusses quantization (QTED), yet the main text does not foreground that quantization is required for the Zynq-7020 deployment, which can make it appear that the method is edge-deployable without quantization. Since quantization is not the paper’s contribution, claiming on-device evidence without evaluating baselines under matched quantization/precision and identical budgets is unfair.\n\nReferences:\n[r1] Mirza, M. J., Micorek, J., Possegger, H., & Bischof, H. (2022). The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)."}, "questions": {"value": "Q1. To mitigate concerns about reliance on source statistics, can you provide sensitivity analyses over the number of source samples N and the subspace dimension k, and also justify the use of source statistics in your problem setting with a clear rationale and deployment assumptions?\n\nQ2. On quantization, can you show matched-precision comparisons where all baselines are quantized and deployed on the same edge device with the same bit-width and compute/memory budgets as QTED?\n\nQ3. On technical novelty, can you explicitly state what is novel beyond latent-space modification with source statistics and the use of CMA-ES, and provide supporting analysis or evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BpYshMeVjN", "forum": "VQJFDRLeTK", "replyto": "VQJFDRLeTK", "signatures": ["ICLR.cc/2026/Conference/Submission416/Reviewer_qiYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission416/Reviewer_qiYF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877250398, "cdate": 1761877250398, "tmdate": 1762915515059, "mdate": 1762915515059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TED (Test-time adaptation for Edge Devices), a gradient-free single-instance test-time adaptation method. TED performs adaptation by correcting latent feature coordinates within a principal component subspace computed from the source domain. The method uses CMA-ES optimization to minimize prediction entropy without requiring backpropagation, aiming to achieve efficient and lightweight adaptation suitable for edge devices. Experiments are conducted on several ImageNet variants and keyword spotting tasks under a single-image test-time adaptation setting, showing moderate accuracy gains compared to other gradient-free baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear and intuitive formulation of latent-space adaptation based on principal component correction.\n2. The proposed method is simple and broadly applicable, requiring no architectural modifications and minimal additional computation.\n3. The integration of PCA-based latent subspace modeling with a backpropagation-free optimization framework is methodologically sound and represents a reasonable design choice for efficient test-time adaptation."}, "weaknesses": {"value": "1. I have concerns regarding the single-image setting adopted in this paper. In realistic deployment scenarios, it is highly unlikely that a model would only ever process one test sample at a time without any opportunity for continuous adaptation. Even under the most restrictive conditions, a model can accumulate several steps before resetting, which would allow existing methods such as SAR[1] to significantly improve their stability and learning efficiency. The authors should further justify the motivation and practical relevance of this assumption.\n2. Compared with the strict single-image setting, adopting a batch size = 1 constraint during continuous adaptation, together with label shift scenarios, would more closely reflect the characteristics of edge devices and real-world streaming environments. The paper should include corresponding experiments under these more realistic settings to evaluate TED’s robustness and effectiveness in continual adaptation.\n3. In contrast to FOA[2], which only requires 32 source-domain samples to compute the necessary statistics, TED needs up to 5000 source-domain samples to reach 55.94% accuracy (compared to 55.03% for No Adapt). Accessing source-domain data at such a scale is prohibitively expensive and nearly unacceptable for practical applications of test-time adaptation.\n4. The paper lacks comparisons with several single-instance normalization calibration or lightweight TTA methods, such as IABN[A], TTN[B], and ZOA[C]. Including these baselines would provide a more comprehensive evaluation and clarify the relative advantages of TED in limited-data or resource-constrained scenarios.\n5. The experiments claiming the effectiveness of TED across diverse network architectures and edge devices only compare against the No Adapt baseline, without including other representative TTA methods.\n6. Both the main text and the appendix omit important hyperparameter and tuning details for all baseline methods, including learning rates, update steps, and regularization weights. The absence of this information severely limits reproducibility and transparency. A complete configuration table should be provided in the supplementary material.\n\n[1] Towards stable test-time adaptation in dynamic wild world. ICLR 2023.\n\n[2] Test-Time Model Adaptation with Only Forward Passes. ICML 2024.\n\n[A] NOTE: Robust Continual Test-time Adaptation  Against Temporal Correlation. NIPS 2022.\n\n[B] TTN: A Domain-Shift Aware  Batch Normalization in Test-Time Adaptation. ICLR 2023.\n\n[C] Test-Time Model Adaptation for Quantized Neural Networks. ACMMM 2025."}, "questions": {"value": "1. In all reported settings, TED resets the model immediately after adapting to a single test sample. I would like to know whether TED itself possesses the capability for continuous or streaming adaptation.\n2. In the ablation study, some larger combinations of (e.g. k8n10) show a noticeable drop in performance (50.01%). Intuitively, increasing k or n within a reasonable range should improve performance—why does it instead lead to a significant degradation in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0voDlOpRwx", "forum": "VQJFDRLeTK", "replyto": "VQJFDRLeTK", "signatures": ["ICLR.cc/2026/Conference/Submission416/Reviewer_8Tmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission416/Reviewer_8Tmh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941702548, "cdate": 1761941702548, "tmdate": 1762915514494, "mdate": 1762915514494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}