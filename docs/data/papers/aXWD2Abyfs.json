{"id": "aXWD2Abyfs", "number": 17862, "cdate": 1758281366852, "mdate": 1759897149403, "content": {"title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models", "abstract": "Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g., grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass, forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability to end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advanced performance on complex visual reasoning benchmarks, including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA.", "tldr": "", "keywords": ["Visual Reasoning", "Large Multimodal Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a637d1ce2258a5c1daf997be72a0e26eb84e216b.pdf", "supplementary_material": "/attachment/aa0b5bdc39bd5b2c02439457f832b75fb28fc822.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified visual reasoning mechanism for Large Multimodal Models (LMMs) that follows a human-like \"understand-think-answer\" process. Unlike existing approaches that either use shortcut learning (directly predicting answers) or rely on external tools/multiple forward passes, the proposed method enables models to: (1) analyze questions and gather relevant visual information using intrinsic capabilities like grounding and captioning, (2) engage in self-prompted reasoning based on gathered cues, and (3) generate final answers—all in a single forward pass. The authors introduce a semi-automatic expert-supervised data engine to curate 334K visual reasoning samples and train Griffon-R, which achieves strong performance on visual reasoning benchmarks (VSR: 70.9%, CLEVR: 63.7%) and general multimodal benchmarks (MMBench: 79.0%, ScienceQA: 87.0%)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated approach: The \"understand-think-answer\" paradigm is intuitive and addresses real limitations of current LMMs in compositional reasoning. The motivation is clearly articulated with concrete examples (Figure 1).\n\nEfficiency advantage: Completing all reasoning steps in a single forward pass without external tools is more efficient than toolkit-based methods. Table 4 shows 13x speedup over SEAL while maintaining competitive accuracy.\n\nStrong empirical results: Griffon-R achieves state-of-the-art or competitive performance across multiple benchmarks, particularly excelling on CLEVR (63.7% vs. 55.8% for the second-best) and showing improvements over both standard LMMs and enhanced reasoning methods.\n\nComprehensive evaluation: The paper evaluates on both specialized visual reasoning benchmarks and general multimodal tasks, demonstrating that the method doesn't sacrifice general capabilities for reasoning performance.\n\nPractical data generation pipeline: The semi-automatic annotation approach combining AI expert (Qwen2-VL-72B) and human curation is a pragmatic contribution, though it needs more detail."}, "weaknesses": {"value": "1. Limited Technical Novelty\n\nThe core contribution appears to be more about structured data annotation than a novel reasoning mechanism:\n\nThe \"understanding\" step simply prompts the model to use existing capabilities (grounding, captioning, OCR) it already possesses\n\nThe \"thinking\" step is self-prompting based on context, which is well-explored in CoT literature\n\nThe main innovation is in training data format rather than architecture or fundamental mechanisms\n\n2. Insufficient Detail on Data Annotation Process\n\nCritical aspects of the data engine are under-specified:\n\nWhat are the exact prompts used to instruct the AI expert? (Referenced Appendix A not provided)\n\nWhat percentage of annotations required human correction or removal?\n\nHow much human effort was needed? This is crucial for assessing scalability\n\nWhat quality control metrics were used?\n\nThe 334K dataset seems relatively small for modern LMM training—how does scale affect performance?\n\n3. Weak Mechanism Validation\n\nThe ablation studies don't convincingly validate the three-stage design:\n\nTable 4 compares against only one baseline (SEAL) on one benchmark (V-Star) with modest improvement (+1.3 points)\n\nNo ablation showing the necessity of all three stages (understand/think/answer). What if understanding+answer suffice? Or just think+answer?\n\nTable 5 shows annotations help but doesn't decompose which aspects (understanding annotations vs. thinking annotations) matter most\n\n4. Incomplete Analysis of Understanding Quality\n\nTable 3 uses RefCOCO as a proxy for understanding quality, but this only measures visual grounding, not comprehensive understanding\n\nNo analysis of: How often does the understanding stage gather correct/relevant information? How do understanding errors propagate?\n\nMissing failure case analysis—when does the mechanism break down?\n\nNo error breakdown by reasoning type or task complexity\n\n5. Unfair or Incomplete Comparisons\n\nModels compared have different parameter counts (7B, 9B, 13B, 17B) making direct comparison difficult\n\nSome baselines are older (e.g., LLaVA-1.5 from 2023); missing comparisons with more recent strong models\n\nTable 1 shows missing results for several method-benchmark combinations (many \"-\" entries)\n\nNo comparison with recent visual reasoning methods like GPT-4V with structured prompting\n\n6. Scalability and Generalization Concerns\n\nRequires expensive, carefully curated three-stage annotations—can this scale to millions of samples?\n\nTested only on Griffon architecture with Gemma9B—does it generalize to other LMM architectures?\n\nCan the model learn to generate intermediate steps without explicit supervision (e.g., via RL or distillation)?"}, "questions": {"value": "Can you provide complete ablations removing each stage independently to validate the three-stage design?\n\nWhat percentage of the 334K annotations required human intervention, and what was the inter-annotator agreement?\n\nDoes this approach generalize to other LMM architectures beyond Griffon? Have you tested on LLaVA or Qwen-VL bases?\n\nCan you provide detailed error analysis showing when understanding/thinking/answering fails?\n\nWhat are the exact inference-time prompts for understanding and thinking stages?\n\nHave you explored learning to generate intermediate steps without explicit annotation (e.g., through reinforcement learning or self-improvement)?\n\nHow does performance scale with the amount of annotated training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jkyHRuFHMI", "forum": "aXWD2Abyfs", "replyto": "aXWD2Abyfs", "signatures": ["ICLR.cc/2026/Conference/Submission17862/Reviewer_EA7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17862/Reviewer_EA7o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760758019240, "cdate": 1760758019240, "tmdate": 1762927692291, "mdate": 1762927692291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Griffon-R, a large multimodal model for visual reasoning based on a unified Understand–Think–Answer (UTA) process.\nInstead of relying on external tools or multi-step reasoning, Griffon-R performs end-to-end reasoning in a single forward pass, mimicking how humans perceive, think, and respond.\nTrained on a 334K expert-curated dataset, it achieves strong results on benchmarks like VSR, CLEVR, and MMBench, outperforming prior LMMs while being much faster.\nThe work offers a concise, efficient, and interpretable approach to multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work pushes forward end-to-end compositional reasoning, a core obstacle in vision-language integration. It establishes a strong empirical and conceptual foundation for future intrinsically capable multimodal reasoning models, potentially influencing both academic and applied LMM research.\n2. The paper provides a high-quality dataset (334K samples) created through a semi-automatic expert-supervised process — a valuable contribution for the community."}, "weaknesses": {"value": "1. The paper lacks a formal theoretical analysis and ablation experiments quantifying the specific contribution of each UTA phase.\n2. Generalization to out-of-domain or noisy visual data remains unclear.\n3. Many references are incorrectly formatted or missing spaces between citations and text, which affects readability and professionalism."}, "questions": {"value": "1. How is the “self-prompt” mechanism implemented internally? Is it learned implicitly through training on annotated reasoning sequences, or guided by fixed templates during inference?\n2. Will the curated 334K dataset or annotation engine be publicly released? This is crucial for reproducibility and comparison.\n3. The paper emphasizes traceable reasoning. Could the authors quantify this with metrics like faithfulness or rationalization alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yjd5DyPlNo", "forum": "aXWD2Abyfs", "replyto": "aXWD2Abyfs", "signatures": ["ICLR.cc/2026/Conference/Submission17862/Reviewer_ngzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17862/Reviewer_ngzi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725261880, "cdate": 1761725261880, "tmdate": 1762927691886, "mdate": 1762927691886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a unified visual reasoning CoT format and demonstrates a data curation pipeline. Training VLM with the collected data results in enhanced performances on multiple multimodal understanding and reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides enough details to help the readers understand the full story. The experimental results cover multiple domains."}, "weaknesses": {"value": "1. No novel methodology or insights proposed: the unified framework features no significant difference compared to widely-used CoTs for VLMs. The motivation of unifying visual reasoning format for different tasks is unclear, as special tasks (math, coding, ...) need special CoT formats, and advanced LLMs can discover new formats via RL.\n2. The compared models and benchmarks are severely outdated for a work in 2025. For example,they used Qwen-2-vl for data annotation, while Qwen2.5-vl is released in 2025.02. Very few commonly-used VLM benchmarks after 2024 is evaluated.\n3. The paper writing is not in good style: the citation formats have no parentheses; appendix is in supplementary instead of the main pdf."}, "questions": {"value": "Most details are clear, but the paper may benefit from updating the models and benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gh4TEerE0Z", "forum": "aXWD2Abyfs", "replyto": "aXWD2Abyfs", "signatures": ["ICLR.cc/2026/Conference/Submission17862/Reviewer_yews"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17862/Reviewer_yews"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842155469, "cdate": 1761842155469, "tmdate": 1762927691419, "mdate": 1762927691419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors fine-tune a Gemma model to get their proposed Griffon-R model, using data that has been labeled by experts/AI. They find the fine-tuned model performs well on data similar to the labeled data. They do this using a proposed prompt engineering approach, which they call an \"understand-think-answer\" process."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The results in Table 2 were strong."}, "weaknesses": {"value": "- The \"Understand-Think-Answer\" approach is a relatively common prompt engineering approach and is not novel nor a significant research contribution. \n- The authors add a new dataset, test their fine-tuned model on data similar to their new dataset, and report strong results. This likely results in overfitting to their fine-tuned dataset and forgetting of old information, which should be tested for.\n- The benchmark comparisons made are unfair as they compare a model with more data that has been labeled then the existing models. They should compare the proposed model with other models having seen the same data.\n- Incorporating AI into the annotation results in noisier labeled data.\n- The authors abstain from making it clear the proposed griffon model is a fine-tuned variant of Gemini."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "11Fgy6F388", "forum": "aXWD2Abyfs", "replyto": "aXWD2Abyfs", "signatures": ["ICLR.cc/2026/Conference/Submission17862/Reviewer_wtzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17862/Reviewer_wtzv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877347189, "cdate": 1761877347189, "tmdate": 1762927690534, "mdate": 1762927690534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}