{"id": "oKy58UqRLW", "number": 6939, "cdate": 1758002897140, "mdate": 1759897882793, "content": {"title": "An Efficient Global-Local Feature Extraction Architecture for 3D Point Clouds", "abstract": "Accurate 3D object detection and segmentation from LiDAR point clouds require both global context and fine-grained local features. Sparse convolutions capture local geometry efficiently but have limited receptive fields, while transformers model long-range context at high memory and runtime costs and often miss fine detail. We introduce Dilated Uniform Attention with 3D Sparse Convolution (DUA-SConv), a building block that integrates attention and sparse convolution in a complementary way. Each block applies self-attention over a uniformly dilated neighborhood spanning a large, fixed region to provide coarse global context, followed by sparse convolution to recover fine-grained local features. Stacked DUA-SConv blocks form a compact backbone that achieves high accuracy in 3D detection and segmentation with low runtime and parameter count.", "tldr": "", "keywords": ["Detection", "Lidar", "Point-Clouds"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5da9a7da0d97d7ef77389c6b43a796bd11d4cc58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "DUA-SConv is a hybrid 3D backbone that combines attention and sparse convolution to balance global context and local detail in LiDAR-based 3D detection and segmentation. It introduces Dilated Uniform Attention to capture wide-range contextual information efficiently, followed by sparse convolution for precise local feature recovery. By stacking these lightweight DUA-SConv blocks, the model achieves high accuracy with reduced computational cost and parameter count."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**[S1] Clear motivation and supporting evidence. **\n\nThe paper provides a clear motivation by emphasizing the importance of modeling long-range receptive fields in 3D point processing. Their discussion on the limitations of conventional convolutions effectively supports the need for their proposed approach. The theoretical justification is coherent, linking the receptive field expansion to improved context aggregation. Empirical results further reinforce their argument, showing noticeable improvements in capturing global scene structures. Overall, their claim regarding the long-range receptive field is convincing and well-supported by both analysis and experiments.\n\n**[S2] Effective performance on nuScene dataset.**\n\nThe proposed model demonstrates strong performance on the **nuScenes** dataset, effectively handling large-scale outdoor environments. Its ability to process sparse and wide-range point distributions highlights the robustness of the convolution design. Quantitative results show clear improvements over previous baselines, indicating consistent advantages in outdoor scenarios. The authors also emphasize that their method maintains efficiency while preserving accuracy across long-range interactions. Together, these results confirm that the model is particularly well-suited for outdoor perception tasks where spatial coverage is critical."}, "weaknesses": {"value": "**[W1] Missing PTv3 and Sonata baselines on nuScenes**\n\nThe authors claim that their proposed convolution demonstrates strong capability in modeling long-range interactions. While the presented results are promising, this claim would be more convincing if the authors included comparisons against **PTv3** and **Sonata** on outdoor datasets such as **nuScenes**. Additionally, **Sonata** is missing from the evaluations on **S3DIS** and **ScanNet**, despite being published at **CVPR 2025**. Including it would provide a fairer and more comprehensive comparison. \n\n**[W2] Missing qualitative results**\n\nAlthough the proposed method achieves competitive quantitative performance, the paper lacks **qualitative results** or detailed visual analysis. The authors should include qualitative comparisons with prior methods on datasets such as **nuScenes** and **ScanNet**, which would help illustrate the qualitative advantages and better support their quantitative claims.\n\n**[W3] Slightly worse performance on SemSeg datasets**\n\nThe proposed method underperforms **PTv3** on indoor datasets, indicating potential limitations in handling short-range geometric structures. This suggests that the model may not generalize well to indoor or densely cluttered environments. To provide a more comprehensive understanding, the authors could also include experiments on **classification** or other **long-range understanding** tasks to validate the effectiveness of their proposed convolution. Moreover,"}, "questions": {"value": "- In **Figure 4**, the groups appear duplicated when N=64. Is there a specific reason why **azimuth** and **elevation** are partitioned with overlaps rather than being uniquely divided? Also, could the authors clarify what the **index values** in the figure represent?\n- How were **azimuth** and **elevation** values computed for **ScanNet**? Depending on the normalization procedure of point clouds, these values may vary significantly. Please clarify the computation method and normalization scheme used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IcLvxE8LSQ", "forum": "oKy58UqRLW", "replyto": "oKy58UqRLW", "signatures": ["ICLR.cc/2026/Conference/Submission6939/Reviewer_RTGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6939/Reviewer_RTGD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761098675462, "cdate": 1761098675462, "tmdate": 1762919172381, "mdate": 1762919172381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an efficient transformer design that unifies global-local LiDAR perception through uniform grouping, localized attention, and implicit relative positional encoding—achieving strong accuracy with scalable computation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated with a clear explanation of the underlying challenge in LiDAR perception. The authors use visualizations effectively to illustrate the density imbalance across different ranges and to show why uniform grouping and local-global modeling are necessary.\n2. The method achieves state-of-the-art results on both NuScenes and Waymo Open Dataset"}, "weaknesses": {"value": "1.Although the paper lists three main contributions, most of them boil down to the introduction of the DUA (Dilated Uniform Attention) module. The overall methodological novelty feels incremental, as the proposed framework mainly adapts existing attention mechanisms to LiDAR range representations rather than introducing a fundamentally new idea.\n\n2.The DUA module itself is not highly innovative, it essentially performs standard attention operations on the range image domain, similar to what has been explored in prior transformer-based LiDAR perception works. The conceptual leap from previous designs is therefore relatively small.\n\n3.The model requires transformations between the range image and sparse point cloud spaces, along with additional grouping operations. These steps likely introduce notable latency. The reported 117 ms inference time is considerably slower than recent efficient LiDAR transformers such as HEDNet and ScatterFormer, while the accuracy improvement is relatively modest, raising concerns about the overall efficiency–performance trade-off.\n\nHe, C., Li, R., Zhang, G., & Zhang, L. (2024). ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention. In Proceedings of ECCV 2024"}, "questions": {"value": "1. Since the model focuses on efficient global-local feature aggregation, it would be interesting to see whether the approach generalizes beyond detection to dense prediction tasks such as semantic or instance segmentation.\n2. I’m curious how much latency is introduced when transforming the point cloud into the range-view representation and performing the grouping operations, before converting it back to the sparse point cloud space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ekYtLQ1xoW", "forum": "oKy58UqRLW", "replyto": "oKy58UqRLW", "signatures": ["ICLR.cc/2026/Conference/Submission6939/Reviewer_AzB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6939/Reviewer_AzB2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622278068, "cdate": 1761622278068, "tmdate": 1762919172005, "mdate": 1762919172005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DUA-SConv, an efficient hybrid backbone for 3D point cloud processing that addresses the challenge of capturing both global context and local detail. The core contribution is a novel \"Uniform Dilated Grouping\" (UDG) mechanism that applies range-dependent dilation to compensate for the non-uniform density of LiDAR data. This allows an efficient serialized transformer to learn coarse global context from large, consistent spatial regions. This global context is then refined by a 3D sparse convolution to capture fine-grained local features. Experiments demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe Uniform Dilated Grouping (UDG) mechanism is a novel and technically sound method for \"equalizing\" point cloud density before applying attention. This directly addresses a key limitation of prior window-based transformers, whose receptive fields are spatially inconsistent (Fig. 1b). The complementary design, using attention for coarse-global context and sparse convolution for fine-local refinement, is well-motivated and elegant.\n2.\tExtensive experimentation and ablation studies validate the effectiveness of the proposed method. \n3.\tThis paper is written and  organized well."}, "weaknesses": {"value": "1.\tThe paper lacks intuitive visualizations of the UDG mechanism in action. While Figure 4 shows the indices of the groups, it does not provide a qualitative visualization of what a \"dilated group\" of points actually looks like in a real point cloud, especially when contrasted with a \"naive\" group. Adding such a visualization would significantly help readers understand the practical effect of UDG.\n2.\tThe key components of the LR-DUT module, such as point serialization and the K/Q-only positional encoding, appear heavily adopted from Point Transformer V3.  The core innovation is clearly the Uniform Dilated Grouping (UDG). It would be more precise to frame the main contribution as the novel integration of UDG with an existing serialized attention framework, rather than implying the entire LR-DUT block is a novel invention.\n3.\tIt will be more convincing to add more advanced method UniMamba[1] in Tab.1.\n\n[1] UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection. CVPR 2025"}, "questions": {"value": "Refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vNeJuwFETp", "forum": "oKy58UqRLW", "replyto": "oKy58UqRLW", "signatures": ["ICLR.cc/2026/Conference/Submission6939/Reviewer_mcbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6939/Reviewer_mcbN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724736223, "cdate": 1761724736223, "tmdate": 1762919171644, "mdate": 1762919171644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an architecture that integrates the local feature extraction capability of 3D sparse convolutions with the long-range contextual modeling of dilated attention. This combination enables more effective feature learning from point clouds, which often suffer from uneven density compared to other data modalities. The method builds upon established techniques (3D sparse convolutions, point cloud serialization, transformers, and positional encoding) and introduces Uniform Dilated Grouping (UDG) strategy which forms the foundation of the proposed DUA-SConv module, the core component of the architecture. Experimental results on popular benchmarks demonstrate that the proposed network outperforms other models of similar size and achieves competitive performance compared to larger architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is overall clear, well-structured, and easy to follow. \n \nThe motivation for integrating modules that capture local geometric details (via 3D sparse convolutions) with those that model long-range context (via dilated transformers) is well-justified. \n \nThe proposed UDG effectively partitions point clouds into groups of approximately uniform spatial size and density which is an interesting and practical solution to the varying point density problem of LiDAR data. The authors combine this component with well-established and effective techniques in a structured and coherent manner to construct the overall architecture. \n \nResults on many popular benchmarks are reported."}, "weaknesses": {"value": "The proposed method has some similarity to the Neurips 2024 paper \"LION: Linear Group RNN for 3D Object Detection in Point Clouds\", which applies linear RNN operators on grouped features within a window-based framework. The current paper seems to extend this learning paradigm to window transformers and the 3D Sparse Convolution is very similar to the 3D sub-manifold convolution of LION in capturing local information. This undermines the novelty, especially as performance improvements are also limited. Can the authors provide a more thorough comparison with LION?\n\nSince the paper primarily emphasizes efficiency, it would benefit from additional comparisons of FLOPs, memory consumption, and runtime against Transformer-based models (e.g., PTv3, SphereFormer), Mamba-based models (e.g., Voxel Mamba) and LION to better justify the efficiency–performance trade-offs. The authors should include FLOPs, memory, and runtime comparisons, along with results from scaled-up versions of the proposed model. This will further strengthen the paper. \n\nAlthough the proposed architecture has the advantage of a smaller model size (i.e., fewer parameters), it does not appear to achieve state-of-the-art performance. While achieving SOTA results is not strictly necessary, an analysis of scaling strategies and an evaluation of a larger version of the model compared to current SOTA methods would provide valuable insight into the design’s potential capabilities. \n\nThe Waymo Level 1 results are not reported. Can authors include this and keep result reporting consistent with Voxel Mamba, SAFDNet, LION as well as the UniMamba & FSHNet papers cited below. \n\nComparisons do not include the most recent methods. The following papers perform better than the current method. Can you provide comparisons to these baselines: \n\n[1] S Liu et al. \"FSHNet: Fully Sparse Hybrid Network for 3D Object Detection.\" CVPR 2025. \n\n[2] X Jin et al. \"UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection.\" CVPR 2025. \n\n[3] Z Liu et al. \"LION: Linear Group RNN for 3D Object Detection in Point Clouds.\" NeurIPS 2024. \n\nWhy does adding positional encoding to K, Q, and V lead to lower performance compared to adding it only to K and Q (as shown in Table 4)? \n\nThere are a few minor writing issues, though they do not affect the overall understanding of the paper. In particular, the terms “dilation” and “dilution” should be used consistently (e.g., the caption of Figure 3 should use “dilation factor” to match the terminology in the main text). Additionally, the column names in Table 5 are missing and should be included for clarity."}, "questions": {"value": "Please see the Weaknesses section. I am open to changing my rating if the authors can address my comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MPWIXUwHj4", "forum": "oKy58UqRLW", "replyto": "oKy58UqRLW", "signatures": ["ICLR.cc/2026/Conference/Submission6939/Reviewer_nrxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6939/Reviewer_nrxi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875904859, "cdate": 1761875904859, "tmdate": 1762919171303, "mdate": 1762919171303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}