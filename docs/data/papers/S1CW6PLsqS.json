{"id": "S1CW6PLsqS", "number": 7917, "cdate": 1758043167662, "mdate": 1759897822620, "content": {"title": "SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization", "abstract": "Contrastive language-audio pretraining, which aims to unify multimodal representations in a shared embedding space, serves as a cornerstone for building a wide range of applications, from cross-modal retrieval to cutting-edge multimodal large language models. However, we find that the perpendicular component of the pushing force from negative samples in contrastive learning is a double-edged sword: it contains rich supplementary information from negative samples, yet its unconstrained nature causes optimization trajectory drift and training instability. To address this, we propose Support Vector Regularization (SVR), a method that introduces an auxiliary support vector to control this perpendicular component, aiming to harness its rich information while mitigating the associated trajectory drift. The efficacy of SVR is critically governed by its semantic radius, for which we explore two unsupervised modeling strategies: direct parameterization and an adaptive radius predictor module enhanced with constraints to improve its predicting accuracy. Extensive experimental results demonstrate that our method surpasses widely used baselines like InfoNCE and SigLIP loss across classification, monolingual retrieval, and multilingual retrieval on standard audio-text datasets. Both the theoretical analysis and the experimental results on optimizing trajectory drift validate the correctness and effectiveness of our SVR method.", "tldr": "", "keywords": ["contrastive learning; audio-text retrieval"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6100ed0cb8b6d22b0d81efdd8198eddb62b8070.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies contrastive language–audio pretraining (CLAP) and identifies a phenomenon called optimization trajectory drift, caused by the perpendicular component of the negative-sample pushing force in contrastive learning. The authors propose Support Vector Regularization (SVR), which introduces auxiliary support vectors and a semantic radius to selectively suppress this perpendicular component while preserving useful negative-sample information. They present both static and dynamic strategies for modeling the semantic radius. Experiments across monolingual and multilingual audio–text retrieval, as well as zero-shot classification, show consistent improvements over InfoNCE and SigLIP with negligible overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work offers a fresh angle by decomposing contrastive gradients into pulling and perpendicular pushing components, which provides new insights into training instability in CLAP.\n2. The SVR formulation is presented with solid mathematical justification, and the derivations clearly show how trajectory drift is effectively suppressed.\n3. Experiments across retrieval, classification, and multilingual tasks, along with ablations, demonstrate consistent improvements while keeping the computational cost minimal."}, "weaknesses": {"value": "1. While the paper offers a new analytical angle, the proposed SVR is similar to a regularization tweak of InfoNCE and conceptually close to earlier margin-based or support-vector–style methods.\n\n2. The dynamic semantic radius predictor is unsupervised and fragile under noise. Its meaning is not clearly explained, and why it shrinks during training or how it reflects semantic difficulty is described more heuristically than rigorously.\n\n3. The baseline setup makes comparisons harder to follow. In Tables 1 and 2, many models are cited only by reference instead of name. Some strong recent baselines are also missing, including DS-CLAP (Liu et al., 2024), Cacophony (Zhu et al., 2024), and ATRI (Yin et al., 2025).\n\n4. The experiments are limited to AudioCaps and Clotho, with multilingual results relying on machine translation. Compared with recent work that uses larger datasets such as AudioSet, WavCaps, and LAION-Audio and covers broader tasks like captioning or grounding, the generalization claims are less convincing.\n\n\n[a] Liu et al., 2024, DSCLAP: Domain-specific contrastive language-audio pre-training\n\n[b] Zhu et al., 2024, Cacophony: An improved contrastive audio-text model\n\n[c] Yin et al., 2025, ATRI: Mitigating multilingual audio-text retrieval inconsistencies by reducing data distribution errors"}, "questions": {"value": "1. The dynamic semantic radius relies on an unsupervised predictor. How robust is it under noisy embeddings (e.g., low-resource or weak encoders)? Can the authors show sensitivity or variance analysis to demonstrate stability?\n\n2. The radius shrinks during training, but the explanation is heuristic. Can the authors provide clearer evidence that its values correlate with semantic difficulty (e.g., hard-negative density or retrieval errors)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iTqOHiwsno", "forum": "S1CW6PLsqS", "replyto": "S1CW6PLsqS", "signatures": ["ICLR.cc/2026/Conference/Submission7917/Reviewer_EVJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7917/Reviewer_EVJV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730754475, "cdate": 1761730754475, "tmdate": 1762919941172, "mdate": 1762919941172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SupCLAP with a Support Vector Regularization (SVR) that shifts the text embedding towards its positive audio.They add an extra contrastive term, claiming to suppress the perpendicular component of the negative “pushing force” and reduce optimization trajectory drift.  And this will help improve performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper gives a novel regularization term on the InfoNCE loss,  their experiments show that this regularization indeed bring benefits. They also provide their intuition in section 2.  And The writing of this paper is good."}, "weaknesses": {"value": "1. In the section 2, the authors discuss the optimization trajectory drift. However, the causal link “lower perpendicular component → better performance” is not theoretically or empirically established. So this section is not very convincing to me. The author may provide theoretical or empirical evidence.\n2. For the InfoNCE loss, we originally want the text embedding being away from the negative sample. So why we need to reduce the perpendicular component of the negative “pushing force”? \n3. Moreover, the proposed Support-Vector Regularization (SVR) seems closely related to simply re-scaling or re-weighting the contrastive gradients. It would strengthen the paper if the authors compared their method to simpler baselines—e.g., emphasizing the parallel component only or giving positive sample gradient more weights—to demonstrate that the observed improvement is not merely a side-effect of gradient magnitude control."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1AE6PAHFfA", "forum": "S1CW6PLsqS", "replyto": "S1CW6PLsqS", "signatures": ["ICLR.cc/2026/Conference/Submission7917/Reviewer_pCqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7917/Reviewer_pCqQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766188631, "cdate": 1761766188631, "tmdate": 1762919940544, "mdate": 1762919940544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SupCLAP, a method for regularizing the contrastive learning optimization process of standard objectives (InfoNCE, SigLIP) to improve performance on downstream retrieval and 0-shot classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Motivation is reasonably strong, and mathematical rigor to the proposed method is appreciated.\n- Despite the overall mathematical maturity of SupCLAP, the proposed method is still intuitive and straightforward, aiding the clarity of the paper.\n- The experimental results, while generally modest, show clear gains to the using the proposed method relative to the baselines."}, "weaknesses": {"value": "In general, there is not much massively wrong with the present manuscript, my two biggest concerns are:\n- It would be useful to report confidence intervals for the retrieval / classification experiments, as the results are generally modest, and it is hard to tell whether this gain in performance is truly statistically significant.\n- Overall, there is not much clear evidence as to *why* SupCLAP seems to work better empirically. While there is ample connection to *in theory* what might be going on during optimization, this is never attempted to be empirically confirmed. It would be very useful to analyze parts of the optimization process for SupCLAP (vs InfoNCE or SigLIP) such as gradient norm, gradient variance, or simply convergence speed to see whether the surmised theoretical reasons for its performance actually translate in practice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E88kTX51MR", "forum": "S1CW6PLsqS", "replyto": "S1CW6PLsqS", "signatures": ["ICLR.cc/2026/Conference/Submission7917/Reviewer_nh4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7917/Reviewer_nh4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772646685, "cdate": 1761772646685, "tmdate": 1762919940047, "mdate": 1762919940047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of optimization trajectory drift during contrastive learning due to the perpendicular component of pushing force from negative samples. It highlights how this aspect can lead to instability in training and proposes a novel solution called Support Vector Regularization (SVR).\n\nSVR introduces an auxiliary support vector that allows for better control of negative sample influence, leveraging their informative content while mitigating trajectory drift. The effectiveness of SVR is critically tied to the selection of the semantic radius, and the paper explores two unsupervised modeling strategies: static direct parameterization and an adaptive radius predictor module with constraints.\n\nExtensive experimental results demonstrate that SupCLAP significantly outperforms various baseline methods across tasks such as classification and both monolingual and multilingual retrieval, validating its potential in stabilizing optimization paths and improving model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses Optimization Trajectory Drift: The proposed Support Vector Regularization (SVR) method effectively controls optimization trajectory drift caused by the influence of negative samples, enhancing training stability.\n\nPerformance Improvement: SupCLAP significantly outperforms various existing baselines across tasks such as classification and monolingual/multilingual retrieval, demonstrating its superior performance in multimodal learning.\n\nExploration of Unsupervised Modeling Strategies: The study explores both static parameterization and adaptive radius prediction as unsupervised modeling strategies, providing new methodological insights for future research."}, "weaknesses": {"value": "Limited Generalizability Beyond Benchmarks: While the performance in the tested tasks is impressive, the study may have limitations in generalizability. The results may not translate well to all real-world applications, particularly those outside the tested domains or with different data characteristics."}, "questions": {"value": "Given that the SVR method introduces additional parameters that require careful tuning, how does this impact the overall efficiency of the method during training and implementation? Specifically, what are the trade-offs between the improved performance and the potential increases in optimization time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7l3lpX6Hlr", "forum": "S1CW6PLsqS", "replyto": "S1CW6PLsqS", "signatures": ["ICLR.cc/2026/Conference/Submission7917/Reviewer_xE2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7917/Reviewer_xE2L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995698736, "cdate": 1761995698736, "tmdate": 1762919939621, "mdate": 1762919939621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}