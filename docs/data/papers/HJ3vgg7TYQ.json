{"id": "HJ3vgg7TYQ", "number": 17162, "cdate": 1758272915215, "mdate": 1759897193155, "content": {"title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning", "abstract": "Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results. Code: \\url{https://anonymous.4open.science/r/RePrompt-CD21}.", "tldr": "", "keywords": ["Reasoning; T2I; RL"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50e2fe1b43c03d20c6867a73e6f5ba04405f58bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RePrompt, a prompt enhancement framework for text-to-image (T2I) generation models that introduces explicit reasoning into the prompt generation process and optimizes based on image-level outcomes using reinforcement learning. The reward model adopts an ensemble reward that evaluates three dimensions: human preference, visual realism, and semantic alignment. On the GenEval benchmark, significant improvements in spatial layout fidelity and compositional generalization are observed, and on T2I-Compbench, particularly notable score improvements in spatial compositions are confirmed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method fixes the image generation model and optimizes only the language model policy, making it applicable to any existing T2I backbone. Since the reward model depends only on the prompt-image output pair and not on any specific T2I architecture, it generalizes naturally across different generation backbones and unseen prompt distributions.\n\n- Substantial improvements are observed in compositional understanding, particularly in spatial position (Position). In the Position category of GenEval, remarkable relative improvements are achieved compared to the Qwen2.5 3B baseline: FLUX (+77.1%), SD3 (+78.8%), and Pixart-Σ (+122.2%). Overall GenEval scores also consistently improve across each backbone (+11.8%, +10.3%, +6.9%).\n\n- In terms of inference latency, the method is significantly faster (30s per image) compared to Idea2Img (140s per image) and PARM++ (110s per image), while also achieving the highest accuracy (0.76), demonstrating practical advantages.\n\n- In qualitative evaluation, concrete examples are presented, showing that for prompts such as \"a fire hydrant with a tennis racket\" and \"a photo of a dog above a cow,\" the method avoids object fusion and misplacement observed in baseline models and faithfully reproduces the intended spatial composition."}, "weaknesses": {"value": "- **Limited scope of evaluation**: The main evaluation relies solely on two automatic evaluation benchmarks: GenEval and T2I-Compbench. Human evaluation is not included, so the practical utility aspects such as \"whether actual users find the results convincing,\" \"whether generated prompts are readable,\" and \"whether reasoning explanations are appropriate\" depend solely on automatic metrics.\n\n- **Insufficient comparison with closely related methods**: Prior work on prompt enhancement mentions iterative refinement approaches and single-pass LLM-based enhancement methods, and quantitative comparisons with Promptist, PAG, GPT4, Deepseek-r1, and Qwen2.5 are provided in tables. However, detailed analysis from the perspective of RL-based prompt optimization is insufficient. Therefore, the theoretical and experimental justification for \"why CoT (Chain-of-Thought) with RL is superior to conventional prompt optimization\" is somewhat weak.\n\n- **Lack of CoT quality evaluation**: While the method generates reasoning traces that simulate visual implications of prompts—much like how humans mentally visualize a scene—and this structured, logic-driven process anticipates potential errors during prompt construction, direct evaluation of the reasoning text itself in terms of correctness, consistency, and conciseness is not performed. The claim that \"reasoning is effective\" is made indirectly through final image scores, but it is not clearly separated whether \"performance improved because of structuring\" or \"merely because detection-friendly phrases were added.\"\n\n- **Insufficient discussion of safety and robustness**: The design optimizes prompt generation through reinforcement learning, but there is no discussion of risks where RL might learn extreme descriptive expressions, overly detailed specifications, or expressions that deceive the reward model, nor mechanisms to detect or suppress such behaviors. Although the Broader Impact section mentions risks of generating misleading content and bias propagation, and recommends pairing the method with content moderation filters and fairness-aware training objectives, concrete countermeasures or experimental validation are not included."}, "questions": {"value": "- Do you plan to conduct human evaluation? Evaluating the quality of generated images and readability of prompts from the perspective of actual users would demonstrate practical utility that cannot be captured by automatic metrics alone.# Review"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LtOGkMZipM", "forum": "HJ3vgg7TYQ", "replyto": "HJ3vgg7TYQ", "signatures": ["ICLR.cc/2026/Conference/Submission17162/Reviewer_bm45"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17162/Reviewer_bm45"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892760410, "cdate": 1761892760410, "tmdate": 1762927147132, "mdate": 1762927147132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes reasoning-augmented framework for text-to-image generation. Unlike previous methods that rely on rewriting or heuristic feedback loops, RePrompt trains an auxiliary LLM via reinforcement learning (RL) that generates both reasoning traces and refined prompt to further prompt a frozen text-to-image model. The proposed method has been extensively evaluated on GenEval and T2I-Compbench datasets, achieving consistent improvements across backbones (FLUX, SD3, PixArt)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of combing LLM reasoning and image-level feedback is novel and promising. \n\n- The reward is also well designed. First of all, the visual-reasoning reward acts as a bridge to connect image reward (human preference alignment) with semantic grounding (VLM reward). Second, it allows the reward to depend only on the behavior of input and output, enabling model-agnostic characteristic of RePrompt across different T2I backbones.\n\n- The ablations and theoretical analysis (in Appendix B), together with the empirical results, all justify the design of RL in this paper, especially on GRPO optimization, and reasoning traces which acts as variance-reduction condition. \n\n- The paper is well presented and the reproducibility is good."}, "weaknesses": {"value": "- The evaluation benchmarks are only object-centric datasets. The performance on open-world prompt is not verified. It is better to show several examples on this scenario.\n\n- No failure cases in the visualization. What is the model behavior on rare, free-form prompts not covered in GenEval? For example, \"a photo of a cat working in an office\""}, "questions": {"value": "- Is human-in-the-loop verification needed to ensure that the reward aligns well the perceptual alignment?\n\n- What is the impact of reasoning length on the generation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8oJPXTTQ9D", "forum": "HJ3vgg7TYQ", "replyto": "HJ3vgg7TYQ", "signatures": ["ICLR.cc/2026/Conference/Submission17162/Reviewer_k9Mw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17162/Reviewer_k9Mw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906082469, "cdate": 1761906082469, "tmdate": 1762927146685, "mdate": 1762927146685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RePrompt, a novel framework that uses Reinforcement Learning (RL) to train a Large Language Model (LLM) to enhance text-to-image (T2I) prompts. It indicates that the generation of a structured, self-reflective \"reasoning trace\" alongside the enhanced prompt helps ground the prompt in visual semantics and improve compositional accuracy. The method is trained end-to-end using a tailored ensemble reward model that assesses image quality, semantic alignment, and prompt structure. Experiments on GenEval and T2I-Compbench show improvements, especially in spatial understanding, over strong LLM-enhanced baselines across multiple T2I backbones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of explicit, structured reasoning with RL for prompt enhancement is a well-motivated approach. It effectively bridges the gap between linguistic fluency and visual plausibility that plagues LLM-based prompters.\n2. The paper demonstrates consistent performance gains across three different diffusion-based T2I models (FLUX, SD3, Pixart-Σ). The improvements in challenging areas like spatial reasoning are compelling.\n3. The framework is designed to be T2I model-agnostic, requiring no retraining of the image generator. The reported inference latency (30s) is significantly lower than iterative optimization baselines, making it more practical."}, "weaknesses": {"value": "1. The training and evaluation prompts are heavily focused on object-centric, compositional generation (training prompts sourced from GenEval-like templates). This raises a concern about potential overfitting to the specific categories and styles of the benchmarks used.\n\n2.  It is unclear how RePrompt would perform on more diverse, stylized, imaginative, or long-form narrative prompts that are common in real-world use. \n\n3. While the paper shows generalization across diffusion-based models, its performance on architecturally different T2I models (e.g., autoregressive or unified multimodal models) remains unverified. Furthermore, the claim of being \"model-agnostic\" is slightly tempered by the finding that the policy is \"individualized to each T2I backbone.\" This suggests that to achieve optimal performance on a new T2I model, one might need to retrain the RePrompt LLM via RL, which is computationally expensive and reduces plug-and-play utility.\n\n4. The fundamental problem might be mitigated by future, more capable T2I models that inherently possess better compositional reasoning. If such models emerge, the value of an add-on module that requires significant RL training could diminish. \n\n5. The instructions provided to the LLM-enhancer baselines (Qwen2.5, GPT-4, etc.) are not included. The performance of these baselines is highly sensitive to how they are prompted, providing this information would increase the transparency and reproducibility of this work.\n\n6. What are the common failure cases for RePrompt? For instance, does the reasoning trace ever lead to over-specification or introduce its own hallucinations? Examples of such failures would help users understand the limitations of the current approach."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wkJRCPI27F", "forum": "HJ3vgg7TYQ", "replyto": "HJ3vgg7TYQ", "signatures": ["ICLR.cc/2026/Conference/Submission17162/Reviewer_mYzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17162/Reviewer_mYzy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991595200, "cdate": 1761991595200, "tmdate": 1762927146374, "mdate": 1762927146374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a new method for automated prompt engineering for text-to-image generation. In particular, they use the standard RL training method for LLM and train an LLM to perform this specific task. They compare their method with several baselines and show improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. Table 2 shows great transferability of the prompts among different text-to-image models.\n3. The ablation study conducted in this paper is very thorough."}, "weaknesses": {"value": "1. My main concern about this paper is regarding its novelty. The training procedure of their model is a very standard RL recipe for LLM, and using LLM as an automated prompt generator for text-to-image generation is not a new idea either (e.g. Hao et al. (2023); Mo et al. (2024); Yeh et al. (2024); Ma ̃nas et al. (2024); Yun et al. (2025); Cao et al. (2023); Qin et al. (2024); Yang et al. (2024d); Wu et al. (2024); Wang et al. (2024) in the paper). It seems like this paper would be better suited for other venues like TMLR.\n2. The authors claim that prior LLM prompt generation methods “frequently generate prompts that produce images with semantically inconsistent or visually implausible content, such as conflicting object placements or unrealistic interactions, because the underlying LLMs lack grounding in physical reality and do not incorporate feedback from downstream visual task” “ with limited generalization”. However, neither of the claims are supported by their experiments. Specifically on generalizability, the model that the authors propose is trained on a dataset curated by following the prompt construction in the GenEval benchmark and evaluated on the same benchmark plus another small benchmark with only 300 test examples. It is unclear to me why these experiments can warrant claims w.r.t. better generalizability.\n3. The authors use GPT-4V, a deprecated model in the GPT family for comparison, not only that it is impossible to replicate the result, it also renders the comparison a bit outdated. It would be better if the authors can compare their method with newer GPT models (would be even better to include comparison with a standard model and a reasoning model).\n4. The authors have also only finetuned from Qwen models and it would be nice to show results from other model families.\n5. The authors can consider adding discussions and/or comparisons to the following papers:\n\nYeh et al. TIPO: Text to Image with Text Presampling for Prompt Optimization. 2024.\n\nLu et al. Language models as black-box optimizers for vision-language models. 2024.\n\nHe et al. Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation. 2024."}, "questions": {"value": "Have the authors trained from instruct models as opposed to base models? How much can this method improve the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RDM9cuRzNf", "forum": "HJ3vgg7TYQ", "replyto": "HJ3vgg7TYQ", "signatures": ["ICLR.cc/2026/Conference/Submission17162/Reviewer_VzC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17162/Reviewer_VzC3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023552254, "cdate": 1762023552254, "tmdate": 1762927145918, "mdate": 1762927145918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}