{"id": "FOgOAyKkIs", "number": 15764, "cdate": 1758255039100, "mdate": 1763747134403, "content": {"title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation", "abstract": "Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features.", "tldr": "", "keywords": ["Depth Estimation", "Data Generation", "Reinforcement Learning", "Foundation Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1365a2119b2f7a576cf1fcb94dc76d7bc7c78d4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces BRIDGE for monocular depth estimation that pairs a reinforcement learning optimised depth-to-image generator with a hybrid supervision strategy. The generator produces approximately twenty million realistic RGB images directly from source depth maps in synthetic datasets (Hypersim, TartanAir), using reward-gradient training that balances depth consistency from depth inversion similarity with a CLIP based aesthetic score. This makes it so each RGB image is aligned with reliable depth. Training then fuses teacher pseudo labels with similarity guided masks based on ORB feature registration and SSIM to inject high precision ground truth depth where the generated RGB matches the original scene. The monocular depth model uses a DINOv2 Giant encoder and a DPT head with scale and shift invariant and gradient matching losses and a separate scale head for metric depth. In zero shot evaluations BRIDGE reports state of the art results on NYU v2, ScanNet, ETH three D, and Sintel, and competitive results on KITTI. Ablations attribute the gains to both the reinforcement learning depth to image data and the hybrid supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Monocular depth estimation is foundational in computer vision and this paper addresses a well-known bottleneck: scarcity and quality of depth-annotated data. By generating ~20mil RGB-D image pairs using their engine, the proposed method promises to resolve this major constraint. In doing so they claim improved generalisation across indoor/outdoor, zero-shot settings and better fine-detail capture.\n\n- The paper first trains a depth-to-image generator via RL that translates source depth maps into realistic RGB images while preserving geometric structure (Sec. 2.1). Then it uses a hybrid supervision method with teacher model pseudo-labels for large scale and similarity-guided high precision ground truth for fine regions (Sec. 2.2). This is novel in that many prior works either rely purely on synthetic data or pseudo-labels but do not explicitly build a large RL-optimised generation engine that maps depth to image at this scale.\n\n- The papers evaluates the method on five mainstream benchmarks—KITTI, NYUv2, ScanNet, ETH3D, Sintel in zero-shot settings and also for metric depth transfer. They show SOTA or near-SOTA results. For relative depth estimation they report the best performance on most of the datasets (Table 1) comparing to strong recent baselines like Depth Anything V2. \n\n- The pipeline is clearly presented with synthetic teacher model plus D2I RL generator, similarity-masking and hybrid supervision and training a student depth model. The paepr gives enough implementation details (Sec. 3) to reproduce many elements. The structuring of contributions is explicit, which helps understand what was done at each step.\n\n- The paper uses the DINOv2-Giant encoder together with a DPT head (Sec. 2.3) for depth estimation. In the proposed model selection ablation, the paper justifies the use of DINOv2-Giant for the generated large-scale data. This indicates performance due to a strong choice for architecture/representation and not purely data-scale.\n\n- The D2I generator’s reward is explicitly designed to enforce geometrical accuracy (via depth map inversion similarity) and aesthetic quality. Additionally, they use registration (ORB features) followed by SSIM comparisons to mask and filter high-precision regions (Sec. 2.2). This focus on geometric alignment is important because many synthetic/generative data approaches have issues with domain or structural correctness."}, "weaknesses": {"value": "- While BRIDGE improves empirical performance, the main novelty lies in using reinforcement learning to guide depth-to-image generation and combining pseudo-labels with high-precision masks. The final depth model relies on an existing DINOv2-Giant encoder and DPT head, so the conceptual advance is largely in pipeline engineering rather than a new principle for depth reasoning. This is not a major issue as the contributions are still useful.\n\n- The paper briefly describes the reward structure of the depth-to-image generator but does not analyse how different reward terms contribute to the final RGB–D alignment or depth quality. There are no ablation experiments isolating the CLIP reward, the depth consistency term or the reinforcement learning optimiser itself. Essentially, it is unclear whether RL is essential or whether similar results could be achieved with standard diffusion or adversarial training."}, "questions": {"value": "The paper mentions that the depth-to-image model is trained via a reward-gradient-driven direct optimisation process to ensure both geometric accuracy and visual realism. Could the authors clarify how this reinforcement learning component differs in practice from standard diffusion fine-tuning or reward-weighted loss formulations? Specifically, how important is the RL itself to achieving the reported geometric consistency and did the authors observe performance degradation if trained without the reward gradient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lMmmAC71k1", "forum": "FOgOAyKkIs", "replyto": "FOgOAyKkIs", "signatures": ["ICLR.cc/2026/Conference/Submission15764/Reviewer_BMGy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15764/Reviewer_BMGy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505193884, "cdate": 1761505193884, "tmdate": 1762925996646, "mdate": 1762925996646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank reviewers oh7b, tqP2, Jp7h, and BMGy for their time and effort in reviewing our paper and providing insightful feedback. We are grateful for the positive comments on our novel data generation paradigm (oh7b, Jp7h, BMGy), strong performance (oh7b, tqP2), and robust generalization (oh7b). Your comments have been instrumental in improving the quality and completeness of our work.\nRegarding the reviewers' common concerns about the novelty in relation to prior work, the specifics of our RL-D2I training, and the cost-effectiveness of our data generation strategy, we have provided extensive clarifications and new experimental results. We have added a detailed comparison with related works and a new ablation study to demonstrate the performance gains from our data generation pipeline.\nWe have uploaded a revised draft of our paper. The main revisions are summarized as follows:\n* **Expanded the related works section to include a detailed comparison with Atlantis [1]**, clarifying the key differences and highlighting the novelty of our RL-based approach.\n* **Added a new visualization comparing Depth SFT and RL-based optimization** to intuitively demonstrate the superiority of our RL-based approach in maintaining geometric consistency.\nWe sincerely value your reviews and believe they have greatly contributed to making the new revision more comprehensive and solid. We have also posted individual responses to each of your comments below.\n\n[1] Zhang et al., Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion."}}, "id": "wBvkAZ7znP", "forum": "FOgOAyKkIs", "replyto": "FOgOAyKkIs", "signatures": ["ICLR.cc/2026/Conference/Submission15764/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15764/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15764/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742985913, "cdate": 1763742985913, "tmdate": 1763742985913, "mdate": 1763742985913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considered the oldest pain-point in MDE: lack of large-scale, high-quality RGB-D data. It turns the problem upside-down: instead of collecting more real RGB-D pairs, it trains an RL-tuned diffusion model that converts existing synthetic depth maps into 20 M photo-real RGB images whose geometry is provably consistent with the input depth. We can roughly say it is a data augmentation approach. \nA hybrid supervision scheme then fuses (1) teacher-model pseudo-labels on the new RGB with (2) high-precision synthetic depth wherever the generated and original RGB are sufficiently similar (SSIM > 0.85). (3) A vanilla DINOv2-Giant + DPT head is finally trained on this 20 M set. The resulting model outperforms the Depth-Anything-V2 (62 M images) on every indoor benchmark and matches or surpasses the best metric-depth specialists on KITTI/NYU while using 3× fewer data. Extensive ablations, metric-depth fine-tuning and conditional-synthesis sanity checks are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The use of RL to optimize the D2I generation process is novel and effective (according to the reported results). The RL formulation ensures that the generated images are not only visually realistic but also geometrically consistent with the source depth maps. This approach significantly mitigates common issues like geometric artefacts and structural distortions found in traditional D2I models.\n2) The hybrid supervision strategy integrates teacher model pseudo-labels with high-precision ground truth depth. This dual supervision ensures that the model benefits from both the broad coverage of pseudo-labels and the precision of ground truth labels, leading to improved accuracy and robustness.\n3) The use of similarity detection methods (SSIM and gradient analysis) to create masks for high-precision regions is reasonable. This approach ensures that the model receives reliable supervision during training, enhancing the overall quality and utilization efficiency of the labels."}, "weaknesses": {"value": "1）The choice of SSIM threshold (0.85) and other hyperparameters for mask generation is arbitrary. A sensitivity analysis to show how these thresholds affect the final performance would be valuable. \n2)  The paper claims to generate a diverse dataset, but it does not provide quantitative metrics or a valid discussion to support this claim. We believe that the scale of data augmentation will definitely generate more diversity but is it too costly? We spent too much but earn very little. What is the contribution of this 'diversity' to the final performance? More discussions are needed. \n3) The paper uses a fixed MiDaS-small model for depth inversion in the RL reward function. This method may introduce biases if the MiDaS model fails on certain types of images. Exploring alternative depth inversion methods or robustness checks would be beneficial."}, "questions": {"value": "1) Following the weakness discussed above, what are the limitations of using MiDaS-small for depth inversion in the RL reward function? \n2) The model's performance on the KITTI dataset is not optimal. Any investigation into these issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SxdkbQ1atu", "forum": "FOgOAyKkIs", "replyto": "FOgOAyKkIs", "signatures": ["ICLR.cc/2026/Conference/Submission15764/Reviewer_Jp7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15764/Reviewer_Jp7h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796105496, "cdate": 1761796105496, "tmdate": 1762925996284, "mdate": 1762925996284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper designs a pipeline for training depth estimation model together with synthesizing images from depth map. The pipeline contains several stages: image generation from depth maps, pseudo label generation, model training. The experiments show that the trained model achieves SOTA performance in comparison with previous methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The data synthesis for depth estimation model training is useful. \n\n- The performance is good.\n\n- The paper is well-written."}, "weaknesses": {"value": "- The main contribution is providing a data synthesis pipeline for depth estimation model training, which has very limited novelty.\n\n- The depth-to-image strategy for data synthesis is used before [1] that is not been discussed.\n\n- Given a depth map, the generated RGB image has a very similar structure. Does this lead to data redundancy? In other words, for a fixed number of depth maps, is it always better to generate more RGB images? Could the data redundancy from generating too many images lead to limited performance gains or even overfitting?\n\n- How the proposed  mask generation strategy impacts the performance.\n\n\n[1] Zhang et al., Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dQJOHKiPnm", "forum": "FOgOAyKkIs", "replyto": "FOgOAyKkIs", "signatures": ["ICLR.cc/2026/Conference/Submission15764/Reviewer_tqP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15764/Reviewer_tqP2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903411466, "cdate": 1761903411466, "tmdate": 1762925995649, "mdate": 1762925995649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BRIDGE, a framework for MDE that tackles the challenges of data scarcity and quality limitations. It achieves this by leveraging a Reinforcement Learning (RL)-optimized Depth-to-Image (D2I) generation engine. BRIDGE is capable of synthesizing over 20 million highly realistic and geometrically accurate RGB images, each paired with its ground truth depth, all derived from diverse source depth maps. A pivotal contribution lies in its hybrid supervision strategy, which combines high-precision ground truth depth (meticulously filtered by similarity detection) with pseudo-labels generated by a teacher model. The authors demonstrate that training an MDE model on this generated dataset, utilizing their hybrid supervision, allows BRIDGE to achieve state-of-the-art performance across various benchmarks (NYUv2, ScanNet, ETH3D, Sintel, DA2K). Remarkably, it does so while using significantly less training data than some existing formidable methods. The paper particularly highlights BRIDGE's exceptional capability in capturing fine-grained details, maintaining robustness in complex scenes, and exhibiting strong zero-shot generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Data Generation Paradigm: The RL-optimized Depth-to-Image (D2I) generation engine is a highly original and effective solution to data scarcity. By using RL to optimize for both visual realism and geometric consistency (via depth loss and aesthetic reward), the method generates a massive and high-quality synthetic RGB-D dataset. \n\nEffective Hybrid Supervision Strategy: The proposed hybrid supervision, combining teacher pseudo-labels for broad coverage and similarity-guided high-precision ground truth for fine-tuning, is a strong methodological contribution. It intelligently balances the quantity provided by pseudo-labeling with the quality of true annotations, leading to more robust and accurate models. \n\n Robustness and Generalization: The model demonstrates excellent zero-shot generalization to diverse and complex \"in-the-wild\" scenes, including challenging elements like reflective surfaces and transparent objects. This is crucial for practical MDE applications."}, "weaknesses": {"value": "Training Details for RL-D2I: While the RL-D2I generation engine is the core innovation, the paper's detailed description of its training process is somewhat lacking. For example, a deeper elaboration could be provided, such as mentioning the training duration, an overview of the GPU resources used, and the final improvement in reward results. \n\nSpecifics of the Scale Head: The paper mentions a specialized scale head for zero-shot metric depth estimation, stating that it is decoupled and trained separately. To facilitate a more comprehensive understanding of the metric depth estimation capabilities, it would be very helpful if further details regarding the architecture of this scale head and its specific training procedure could be provided."}, "questions": {"value": "Supplementary Notes on RL-D2I Model Training: Could authors supplement the approximate training duration, the type and number of GPU resources used, and the changes in reward results for the RL-optimized D2I model training? \n\nScale Head Architecture and Training: Could the authors provide a brief description of the architecture of the specialized scale head? Furthermore, since it's trained separately, what is the specific loss function used for its training, and on which specific dataset(s) is it optimized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rRBVmtIvyO", "forum": "FOgOAyKkIs", "replyto": "FOgOAyKkIs", "signatures": ["ICLR.cc/2026/Conference/Submission15764/Reviewer_oh7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15764/Reviewer_oh7b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906817274, "cdate": 1761906817274, "tmdate": 1762925995260, "mdate": 1762925995260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}