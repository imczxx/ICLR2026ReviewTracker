{"id": "9chqEmpIZT", "number": 1328, "cdate": 1756871380131, "mdate": 1763734056587, "content": {"title": "Mitigating Reward Hacking in LLM-based Recommendation: A Preference Optimization Approach", "abstract": "Post-training adaptation has become the central paradigm for leveraging large language models (LLMs) in recommendation. While recent preference optimization methods, such as Direct Preference Optimization (DPO), enhance pairwise preference discrimination, they remain vulnerable to \\emph{reward hacking}: models exploit imperfections in reward signals, leading to inflated training metrics without genuine recommendation gains.\nWe provide a theoretical analysis of this phenomenon from a gradient perspective and formalize the concept of the \\emph{$\\varepsilon$-insensitive region}, where pairwise updates exert negligible influence on the relative ordering between positives and unsampled negatives. We further show under the Bradley–Terry model that such regions can occupy a substantial portion of the preference distribution, inevitably causing misaligned ranking.\nTo address this issue, we propose \\textbf{Si}mulated Preference Optimization for \\textbf{R}eward-hacking m\\textbf{i}tigation using Pseudo-negatives (\\textbf{\\our{}}). Our framework introduces pseudo-negative samples to enrich contrastive signals and reduce the prevalence of $\\varepsilon$-insensitive regions.\nExtensive experiments on three public benchmarks—LastFM, Goodreads, and Steam—demonstrate that \\our{} consistently improves ranking quality and effectively mitigates reward hacking, providing both theoretical and practical insights for advancing LLM-based recommendation. Our code is available at\n\\url{https://anonymous.4open.science/r/C557-id}", "tldr": "We propose SIRIUS, a pseudo-negative sampling method that mitigates reward hacking and improves LLM-based recommendation.", "keywords": ["Large Language Models", "Sequential Recommendation", "Preference Optimization", "Reward Hacking"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2ccf126b05a638dd9fcaf93eac7a9a7df20900c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on preference learning for LLM-based recommendation and proposes a method to prevent reward hacking in preference learning. The authors analyze gradient sensitivity between response pairs for pairwise preference models including the Bradley–Terry model. Specifically, they analyzed how the gradient for a pair $(i,k)$ behaves when viewed from a pair $(i, j)$. They characterize the size of the region in which the gradient $\\partial p^{ik}/ \\partial p^{ij}$ and fall within an epsilon-sensitivity band. Building on this insight, they propose a preference-learning method that leverages unsampled negatives and empirically validate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clearly motivated research question.\n- The theoretical analysis provides a solid motivation for the approach.\n- Empirical results corroborate the method’s training effects (via reward margin and over-optimization analyses).\n- Performance is demonstrated on sequential recommendation tasks."}, "weaknesses": {"value": "Although the idea and the results are interesting, I believe there are shortcomings in the precision of the methodological description and in parts of its validation. The reasons are as follows.\n\n\n\n\n- **Discussion of the epsilon-sensitive region (theory and experiments):** The claim that the vanishing-gradient region occupies a large fraction appears somewhat arbitrary, as it seems heavily dependent on the choice of epsilon. Figure 3(a) suggests that the gradient is not exactly zero in most cases; if epsilon is taken to be very small, the vanishing-gradient region may almost disappear. The paper lacks a clear perspective on which epsilon values are meaningful for training.  \n  In actual training, what magnitude of epsilon is necessary, and can the authors theoretically or empirically quantify the proportion of the space that fails to meet such an epsilon?\n\n- **Details of SIRIUS:** I could not fully grasp the details of SIRIUS even after reading the appendix. Could you provide pseudo-code? The authors states: \n\n> In practice, the pseudo-negative is not sampled from the data distribution but introduced as a virtual item to reduce the risk of vanishing gradients for unsampled pairs.\n\nIf one were to sample pseudo-negatives from the data distribution, what would that procedure look like in practice? Conversely, when introducing a virtual item, how exactly is training carried out? Does SIRIUS effectively admit two versions? If so, what are the pros and cons, and how do their performances differ?\n\n- **Validation of Proposition 3:** Regarding Proposition 3, do the values relating epsilon and the area align with empirical observations? Can this be validated on the LastFM, Goodreads, and Steam datasets used in the experiments?\n\nMinor comments:\n\n- **Fitting of SimPO + SIRIUS in Figure 4:** The epoch-wise training curve does not appear to have entered a decreasing phase, suggesting that fitting with R(d) may not be appropriate. Is it possible that the decline would occur at later epochs?\n- **Empirical analysis of reward margin:** In Figure 3(b)–(d), how is the reward margin computed? I would like to see the reward margin over epochs plotted separately for sampled pairs and unsampled pairs. Is such a plot feasible?"}, "questions": {"value": "See weaknesses.\n\nI will reconsider my score if the authors address these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D2gsLsMS85", "forum": "9chqEmpIZT", "replyto": "9chqEmpIZT", "signatures": ["ICLR.cc/2026/Conference/Submission1328/Reviewer_ZsNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1328/Reviewer_ZsNJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554469146, "cdate": 1761554469146, "tmdate": 1762915737963, "mdate": 1762915737963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies reward hacking in preference-optimized LLM recommenders by formalizing ε-insensitive regions where gradient updates on sampled pairs negligibly affect unsampled item rankings. Theorem 1 proves their existence for triplets of items in pairwise preference models; under Bradley-Terry, 63.8% of pairs occupy this region at ε=0.99 (Proposition 3). The proposed method SIRIUS introduces a pseudo-negative item with fixed reward $r^0_u=0$ to provide optimization anchors. Experiments on LastFM, Goodreads, and Steam datasets show significant HitRatio@1 improvements over DPO, SimPO, and S-DPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**: Formalizes reward hacking via ε-insensitive regions (Definition 2), providing geometric perspective on gradient saturation in preference optimization.\n\n**Quality**: Theorem 1's existence proof is mathematically sound (for the triplet setting). Proposition 3 quantifies occupancy rate under Bradley-Terry model as a function $\\epsilon$. \n\n**Clarity**: Well-motivated and clear problem definition (reward hacking in DPO-like recommendation). Figure 1 effectively visualizes the problem. Mathematical notation is consistent.\n\n**Significance**: Addresses practical issue in LLM-based recommendation with computationally efficient solution."}, "weaknesses": {"value": "**Limited theoretical scope**: The analysis and Theorem 1 seems to apply only to triplet cases and provide existence results without generalization to larger item sets or constructive proofs.\n\n**Loose theory-implementation link**: The implementation fixes the pseudo-negative reward $r^0_u = 0$ without verifying that it satisfies the theoretical condition, leaving a gap between the formal analysis and practical algorithm."}, "questions": {"value": "- Theorem 1 and 2 are written as if they hold for any pairwise preference model, but the proofs seem to rely on triplet (i, j, k) constructions. Are these results theoretically limited to three-item cases, or can they be generalized to larger item sets?\n\n- For scenarios with a large number of candidate items, what does the current theory suggest about the prevalence or effect of ε-insensitive regions? Is the phenomenon expected to scale or saturate with item count?\n\n- How should the pseudo-negative sample be constructed in practice? The paper states $r^0_u=0$ but seems not to describe the implementation. Additionally, what are the consequences if the reward distribution is not  centered near zero or if $r^0$ deviates significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7iCLUd8X8", "forum": "9chqEmpIZT", "replyto": "9chqEmpIZT", "signatures": ["ICLR.cc/2026/Conference/Submission1328/Reviewer_8t7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1328/Reviewer_8t7L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720507198, "cdate": 1761720507198, "tmdate": 1762915737818, "mdate": 1762915737818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically investigates reward hacking that occurs when applying preference optimization methods to LLM recommendations. It introduces the concept of an $\\epsilon$-insensitive region, where gradients from observed pairs have negligible effect, and shows that such a region exists in the Bradley–Terry model. To mitigate this, the authors propose SIRIUS, which provides pseudo-negative samples to reduce the $\\epsilon$-insensitive region, and demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The studied problem is important and the paper is easy to folllow.\n- The explanation of reward hacking via an ε-insensitive region, formulated through a gradient-based perspective, is novel, and the authors provide an illustration of it in the Bradley–Terry model.\n- By introducing pseudo-negative samples, SIRIUS shows its effectinvess to reduce ε-insensitive region."}, "weaknesses": {"value": "- Choice of pseudo-negative sample: The experiments seem to use a fixed reward of 0. While showing its effectiveness empirically, is the method sensitive to this hyperparameter? Providing a justification for this choice and including an ablation study would be helpful.\n- Missing directly related baselines: There are no baselines directly related to reward hacking. It would be useful to compare SIRIUS with reward-hacking mitigation methods from LLM alignment adapted to the LLM recommendation task.\n- Limited experiments: Experiments are limited to a Llama 2–7B. Including state-of-the-art models would better demonstrate the method’s effectiveness.\n- Broader application: The ε-insensitive region is not specific to LLM recommendation and also arises in LLM human-value alignment, but the paper does not discuss this. Since human-value alignment also has many unsampled responses, SIRIUS could plausibly apply there too though its effectiveness may be lower than in the recommendation setting."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vW2UytyEdo", "forum": "9chqEmpIZT", "replyto": "9chqEmpIZT", "signatures": ["ICLR.cc/2026/Conference/Submission1328/Reviewer_pJGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1328/Reviewer_pJGd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866543078, "cdate": 1761866543078, "tmdate": 1762915737694, "mdate": 1762915737694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates reward hacking in pairwise preference-optimization for LLM-based recommenders (e.g., DPO/SimPO under a Bradley–Terry modeling lens). It argues that pairwise training induces an $\\varepsilon$-insensitive region in which updates on sampled positive/negative pairs propagate weakly to the relative ordering against most unsampled items. To mitigate this, the authors propose SIRIUS: for each training instance, add a pseudo-negative “anchor” with a fixed reward, thereby anchoring the absolute scale of the positive item rather than only its pairwise margin. Experiments on LastFM, Goodreads, and Steam show consistent improvements over DPO-style baselines and diagnostics (e.g., “golden reward vs. KL” curves) suggesting delayed/mitigated reward hacking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the problem choice timely and practically important. The $\\varepsilon$-insensitive perspective is a neat, clarifying way to explain why pairwise objectives can keep increasing margins without improving ranking quality. The theoretical steps build logically: propositions delineate where gradients do and don’t flow; the insensitivity region formalizes when those gradients are effectively negligible; the anchor analysis explains the mechanism by which absolute calibration of the positive can improve many unsampled comparisons. The method itself is simple and cheap to adopt. The writing is clear, figures are informative, and the empirical signal is coherent across datasets, with especially useful diagnostics that visualize “over-optimization.”"}, "weaknesses": {"value": "1) In appendix B3, $y=p^{ij}$ seems to be a typo. This is one example that I found, but I did not get a chance to proofread all the math in the appendices. As this paper has a heavy theoretical component, I highly recommend proofreading all the math.\n2) Theorem 2 explicitly assumes an upper bound on reward values (good). What would strengthen the paper is connecting that assumption to the *implementation details*: (i) specify how boundedness is enforced in experiments (e.g., reward clipping, temperature calibration, or operating directly in a bounded Bradley–Terry score space); (ii) make the exact choice of the fixed anchor reward $r_0$ explicit (value, units/scale, and whether it is per-dataset or global); (iii) clarify the sign/scale convention that guarantees when needed that $p_{i0} < 0.5$ (or relax that requirement in the supporting argument). A short sensitivity study varying $r_0$ would make the empirical story more robust.\n3) Gradient story: the text sometimes reads as if SIRIUS updates unsampled negatives directly; more precisely, it anchors the absolute scale of the positive ($r_i$), which indirectly changes many pairwise orderings. Tightening this language would improve clarity.  \n4) Evaluation breadth: results emphasize HR@1 with 20 sampled negatives per test case. This setup favors methods that “make the positive pop,” which SIRIUS is designed to do. I would be more convinced by adding NDCG/Recall@k and larger candidate pools (e.g., 100 or full catalog) to test calibration beyond top-1 precision.  \n5) Ablations and variance: the paper can benefit from probing sensitivity to the anchor (vary $r_0$ and any anchor loss weight), comparing against multi-negative sampling at matched compute, and including error bars/CI across seeds. Also, ensure baseline stabilization heuristics and number of epochs are harmonized or justified across methods."}, "questions": {"value": "Every item raised in the Weaknesses section can be viewed as a question for the authors.\nI may well be mistaken on several of these points, and I would sincerely appreciate clarification or correction wherever appropriate.\nIf the authors can address or resolve even part of these concerns—whether by showing that I misunderstood something or by providing additional detail—it would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gFUDvtdUwy", "forum": "9chqEmpIZT", "replyto": "9chqEmpIZT", "signatures": ["ICLR.cc/2026/Conference/Submission1328/Reviewer_1bGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1328/Reviewer_1bGP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993545460, "cdate": 1761993545460, "tmdate": 1762915737582, "mdate": 1762915737582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}