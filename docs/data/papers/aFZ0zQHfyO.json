{"id": "aFZ0zQHfyO", "number": 24196, "cdate": 1758353942374, "mdate": 1763385636080, "content": {"title": "Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models", "abstract": "Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT) paradigm, have shown remarkable reasoning capabilities. Intuitively, different problems often require varying depths of reasoning. While some methods can determine whether to reason for a given problem, they typically lack a fine-grained mechanism to modulate how much to reason. This often results in a ``one-size-fits-all'' reasoning depth, which generates redundant overthinking for simple questions while failing to allocate sufficient thought to complex ones. In this paper, we conduct an in-depth analysis of LALMs and find that an effective and efficient LALM should reason smartly by adapting its reasoning depth to the problem's complexity. To achieve this, we propose a difficulty-adaptive reasoning method for LALMs. Specifically, we propose a reward function that dynamically links reasoning length to the model's perceived problem difficulty. This reward encourages shorter, concise reasoning for easy tasks and more elaborate, in-depth reasoning for complex ones. Extensive experiments demonstrate that our method is both effective and efficient, simultaneously improving task performance and significantly reducing the average reasoning length. Further analysis on reasoning structure paradigm offers valuable insights for future work.", "tldr": "", "keywords": ["Large Audio Language Model", "Reinforcement Learning", "Difficulty-aware Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f718ba277b63d3caed6c7474c591a17c5f833307.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Think Smart, Not Hard, a reinforcement-learning framework that enables difficulty-adaptive reasoning for Large Audio Language Models (LALMs). The key insight is that current LALMs, trained via SFT or GRPO, apply uniform reasoning depth regardless of question difficulty—causing redundant “overthinking” for easy tasks and shallow reasoning for complex ones.\nTo address this, the authors propose two difficulty-adaptive, length-based reward functions: (1) Group Ratio Difficulty Reward (GRDR), which estimates difficulty from the correctness ratio of sampled rollouts, and (2) Group Audio-Attention Difficulty Reward (GA2DR), which derives difficulty from attention-entropy over audio tokens. These rewards dynamically link reasoning length to model-perceived difficulty, encouraging concise reasoning for simple inputs and deeper reasoning for challenging ones. Experiments demonstrate consistent gains over standard GRPO and truncation-based baselines, achieving shorter reasoning traces without sacrificing accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper identifies an important inefficiency in current LALMs by showing that they apply uniform reasoning depth across different difficulty levels. It provides a clear motivation and strong empirical support for introducing difficulty-adaptive reasoning.\n\n(2) The proposed GRDR and GA2DR rewards are simple and effective, allowing adaptive reasoning without external supervision. \n\n(3) Experiments on MMAU with supporting AirBench results demonstrate consistent gains over GRPO and truncation baselines."}, "weaknesses": {"value": "(1) The paper introduces exponential difficulty–length scaling within GRPO without analyzing its impact on policy-gradient variance or monotonic-improvement conditions. In the absence of bounded-gradient or convergence guarantees, objective stability remains unverified.\n\n(2) GA2DR uses batch-normalized attention entropy as a continuous difficulty signal (Eq. 5), but provides no calibration or correlation against human difficulty labels. Despite normalization, the signal may still conflate input noise or sequence length with true task complexity.\n\n(3) Because GRDR derives difficulty from on-policy rollout correctness, the reward baseline drifts as training evolves, creating a feedback loop that can distort credit assignment and destabilize late-stage optimization.\n\n(4) The Cold-Start variant shows degraded outcomes, yet the paper omits optimization diagnostics (e.g., KL, gradient-norm, reward-variance trajectories), leaving claims of adaptive-reward stability anecdotal.\n\n(5) Reasoning length is used as a proxy for efficiency, but no inference-time, FLOP, or latency measurements are reported, so computational gains are not established beyond token-level proxies.\n\n(6) Model-perspective difficulty labels are produced by the same model family used for training (Qwen variants among the labelers and learners), risking family-bias. No cross-family labeling or out-of-domain validation is provided to demonstrate generalization.\n\n(7) The paper includes a single-task qualitative example highlighting redundancy vs. core reasoning, but lacks a systematic failure-mode analysis across tasks/difficulties, so it remains unclear whether shorter traces consistently reflect better reasoning rather than premature truncation."}, "questions": {"value": "(1) How stable are the GRDR and GA2DR difficulty signals throughout training? Please provide curves of γ variance and reward distribution across epochs to verify that the adaptive scaling converges rather than oscillates or collapses.\n\n(2) GA2DR assumes that attention entropy reflects reasoning difficulty, yet this metric could be sensitive to batch normalization, padding, or input noise. Have you examined its noise robustness or its correlation with human-annotated or dataset-defined difficulty levels on MMAU?\n\n(3) Could the authors quantify the relationship between reasoning length and task accuracy, for instance by plotting accuracy/reward versus token length? \n\n(4) Does the proposed difficulty-adaptive training generalize beyond audio–text reasoning to text-only or audio–visual tasks? \n\n(5) Since the model-perspective difficulty labels are produced by the same family of backbones (Qwen2-Audio, Qwen2.5-Omni, Kimi-Audio, Gemini2.5-Pro) used for training, could there be self-consistency or family bias? Have you tested cross-model validation using an unseen architecture to confirm that the learned adaptivity generalizes?\n\n(6) What is the per-batch computational overhead of calculating attention-entropy–based difficulty, and can it scale to larger multimodal corpora or longer sequences without becoming a bottleneck?\n\n(7) The method emphasizes shorter reasoning chains, but is there evidence that reasoning quality remains unchanged? Have you conducted any human or LLM-as-a-judge evaluations of coherence, factual correctness, or interpretability to ensure that brevity does not harm reasoning fidelity?\n\n(8) The Cold-Start SFT→GRPO variant exhibits degraded performance, but no diagnostics are shown. Could the authors provide gradient-norm, reward-variance, or KL divergence curves to clarify why Cold-Start fails to stabilize and whether this reflects optimization instability or data mismatch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lI6omi5X8Y", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Reviewer_gZvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Reviewer_gZvn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761001367892, "cdate": 1761001367892, "tmdate": 1762942989304, "mdate": 1762942989304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to address the \"one-size-fits-all\" issue in LALMs reasoning, analyzing performance differences between SFT/GRPO and the impact of explicit/implicit prompts. It proposes two adaptive reward functions: GRDR (discrete difficulty) and GA²DR (continuous difficulty). Experiments on the MMAU benchmark verify that the method improves performance while shortening reasoning length, and also refines an ideal reasoning paradigm and model training recommendations, providing a solution for LALMs reasoning optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It identifies the pain point of difficulty adaptation in LALMs reasoning, fills gaps in SFT/GRPO comparison and prompt effect analysis.\n- The method aligns with audio modal characteristics; the two difficulty definitions cover different scenarios, featuring flexible design.\n- Experiments use multiple baselines and multi-dimensional verification, and introduce model-perspective difficulty labeling, ensuring reliable results."}, "weaknesses": {"value": "- GRDR uses a fixed group size G=8 without verifying other values, and GA²DR lacks basis for layer selection, leading to insufficient robustness in difficulty definition.\n- The hyperparameter values of the reward function have no basis and lack sensitivity analysis, affecting reproducibility.\n- There are inconsistencies in formula and figure formatting. such as no periods or commas are added at the end of formulas. Figure 1 lacks a legend, which makes it impossible to identify the meaning of different curves, colors, or markers in the figure."}, "questions": {"value": "1. (Line 192) What is the basis for choosing G=8 in GRDR? How will performance change if G is set to 4 or 16?  \n2. (Line 211) Why does GA²DR use the last layer to calculate attention entropy? Will switching to an intermediate layer cause significant differences?  \n3. (Line 246) What is the basis for setting $k_{easy}$ and $k_{hard}$ in the reward function? How will performance change after adjustment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ubC6ahjYPR", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Reviewer_2MYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Reviewer_2MYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667248839, "cdate": 1761667248839, "tmdate": 1762942988638, "mdate": 1762942988638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how Large Audio‑Language Models (LALMs) should modulate how much they reason, rather than simply deciding whether to reason. Building on analyses that compare SFT vs. GRPO and explicit vs. implicit CoT prompting, the authors propose difficulty‑adaptive reward shaping that links reasoning length to problem difficulty. They introduce two difficulty estimators computed from the model’s perspective: GRDR (Group Ratio Difficulty Reward), which labels a question as easy/medium/hard based on the share of correct rollouts in a group and GA$^2$DR (Group Audio‑Attention Difficulty Reward), which maps the entropy of last‑token attention over audio tokens to a continuous difficulty $\\gamma \\in [0,1]$. A negative‑exponential length reward promotes short, concise CoT on easy items and longer, exploratory CoT on hard ones. Experiments on MMAU‑test‑mini with Qwen2‑Audio‑7B and Qwen2.5‑Omni‑7B (LoRA + GRPO, explicit CoT prompt) show modest average accuracy gains over GRPO+TR and notably bigger gains on hard subsets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is clear and easily understandable\n- Meaningful improvements on hard items with small overall gain\n- Good ablation on explicit CoT helps on hard items."}, "weaknesses": {"value": "- The paper has limited evaluation scope. Results focus on MMAU‑test‑mini with single‑run ACC. There are no standard errors or full‑benchmark experiments.\n- The paper doesn't report compute/training cost: GRDR needs grouped rollouts; GA$^2$DR requires attention extraction. The paper does not quantify added RL cost vs. GRPO/TR.\n- The hyperparameters are not ablated. For instance, among slopes ($k_{easy}$ and $k_{hard}$), Group sizer $G$ and $l_{min}$; only $l_{min}$ is ablated.\n- GA$^2$DR hinges on last‑token attention over audio tokens, however, the scheme is not tested across across different layers/heads. Why the last year only?\n-  Fig. 2 shows log‑length trends but not absolute token reductions on the full test. The log trend hides effective sizes and makes it hard to tell how much shorter the reasoning actually got.\n- The paper measures all lengths with the Qwen2-Audio tokenizer, even when the model being trained/evaluated is Qwen2.5-Omni. This might lead to discrepancies if two models are using different tokenizers."}, "questions": {"value": "- What is the training overhead of GRDR/GA$^2$DR relative to GRPO and TR (same batch size, same G)?\n- For GA$^2$DR, does using different layers/heads (or pre‑softmax attention) change the difficulty estimates and outcomes?\n- Do results hold on the full MMAU or other audio‑reasoning datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BdkuIUBjou", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Reviewer_vJnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Reviewer_vJnZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681191951, "cdate": 1761681191951, "tmdate": 1762942987653, "mdate": 1762942987653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the inefficiency of LALMs using a uniform reasoning depth for all tasks. The authors propose a \"difficulty-adaptive\" method, using a new RL reward function that links reasoning length to the model's perceived difficulty. This encourages concise reasoning for simple tasks and in-depth reasoning for complex ones, ultimately improving performance while significantly shortening the reasoning length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It addresses the critical and practical problem of high computational cost and inefficiency in CoT reasoning, achieving the important goal of \"better performance with less computation.\"\n2. The core idea of using \"model-perspective difficulty\" (instead of static human labels) is intelligent. The proposed metrics, like $GA^{2}DR$ (based on audio attention entropy), are novel and well-justified.\n3. The experimental results are strong. The method not only improves accuracy on hard problems but also significantly reduces the average reasoning length, effectively validating the \"think smart\" concept."}, "weaknesses": {"value": "1. The experimental validation relies almost entirely on the MMAU-test-mini benchmark. While the analysis on this benchmark is deep (e.g., broken down by difficulty), this raises questions about generalizability.\n2. The proposed difficulty metrics (GRDR and $GA^{2}DR$) appear computationally expensive during training. GRDR requires $G=8$ full rollouts per sample just to determine its difficulty level1. $GA^{2}DR$ requires extra computation for attention entropy and relies on batch normalization 2222. This overhead could make the already complex RL training process significantly slower and more costly, a trade-off that is not discussed in detail."}, "questions": {"value": "1. Regarding the GRDR metric , how sensitive are the final performance and efficiency gains to the specific thresholds you chose (e.g., 3 and 6)? Would relaxing the definition of \"easy\" from 6 correct rollouts to 5, for example, significantly alter the results?\n2. In Section 4.3, you analyze and identify an ideal reasoning paradigm: extracting conditions (captioning), followed by analysis, and then the answer. However, your reward function (Equation 6) primarily optimizes for length and correctness. How does this reward mechanism ensure that as the model shortens its reasoning, it preserves this critical logical structure rather than simply deleting random thought steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HxpbtfNgqt", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Reviewer_8amV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Reviewer_8amV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765516242, "cdate": 1761765516242, "tmdate": 1762942987005, "mdate": 1762942987005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a difficulty-adaptive reasoning framework for LALMs, introducing two self-perspective difficulty estimators and length-negative-exponential rewards (GRDR and GA²DR). The goal is to encourage concise reasoning for easy questions and deeper reasoning for hard ones, thereby improving efficiency without sacrificing accuracy. The authors conduct experiments on the MMAU benchmark, demonstrating that their approach reduces reasoning length while maintaining strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is compelling, addressing a critical limitation of current LALMs—lack of adaptive reasoning. The proposed rewards (GRDR and GA²DR) show practical benefits, significantly reducing reasoning length on the MMAU-test-mini benchmark while preserving accuracy(From Table 4). The framework’s ability to dynamically adjust reasoning depth based on question difficulty is a promising direction for improving LALM efficiency."}, "weaknesses": {"value": "1、Contribution 1 from authors only reports aggregate accuracy on MMAU-Test-Mini split by difficulty.  The authors should perform finer-grained statistical analyses to validate the claim that “models under-/over-reason on different difficulties”.\n2、Contribution 2 from authors introduces GRDR and GA²DR, but their relationship is unclear.  GA²DR appears to be a stand-alone criterion; its connection with GRDR is never formally discussed.  Besides, GRDR itself seems questionable: Table 2 shows that models perform best on medium-difficulty questions, while easy and hard accuracies are almost equal.  Under GRDR’s definition, should the original three difficulty buckets be re-labeled?  If so, how does the new labeling relate to the original one, and why is it more reasonable?\n3、How are the “groups” in GRDR constructed? \n4、What is the theoretical or empirical link between GRDR and GA²DR?  Table 3 treats them as independent ablations.  If they are orthogonal, why must both be proposed?  Moreover, the performance gaps in Table 3 are within one standard deviation, so improvements may simply come from random-seed variation rather than the rewards themselves.\n5、Experiments are restricted to MMAU-test-mini. No results on other audio-reasoning benchmarks or cross-domain transfer are provided, limiting generalizability.\n6、Line 361 claims “Specifically, it achieves short reasoning for simple questions and long reasoning for difficult ones, while overall significantly reducing reasoning length.”  But Figure 2 only shows that GRDR/GA²DR shorten the average length; it gives no evidence that the model produces longer chains for hard questions than for easy ones."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XIhj5Vsxqi", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Reviewer_73Us"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Reviewer_73Us"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902054801, "cdate": 1761902054801, "tmdate": 1762942986285, "mdate": 1762942986285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# General Response\n\n## Updates in this edition of the paper\n\n### Added more experiments: \n1. We add the results of our methods on two new benchmarks, presented in Table 4 and Table 5 of Section 4, and Table 11 and Table 12 in Appendix A.4. (suggested by Reviewer 73Us, 8amV and vJnZ)\n2. We include additional ablation studies and experiments with different k-value settings, reported in Section 4.3. (suggested by Reviewer vJnZ and 2MYJ)\n\n### Improved the clarity:\n\n1. In Appendix A.1, we add Figure 4 to illustrate the task definition.\n2. We refine several unclear descriptions and update some ambiguous formulas and figures. (suggested by Reviewer 2MYJ)\n3. In Table 13 of Appendix A.5, we provide the detailed reasoning lengths of all models on MMAU-Test-Mini. (suggested by Reviewer 73Us)\n4. In Figures 7 to 10 of Appendix A.6, we add the variation plots of reasoning length and accuracy for $GRDR$ and $GA^2DR$ on MMAU-Test-Mini. (suggested by Reviewer gZvn)\n5. In Figures 11 and 12 of Appendix A.7, we present the training curves of $GRDR$ and $GA^2DR$. (suggested by Reviewer gZvn)\n6. In Appendix A.10, we add two additional example analyses, Figure 14 and Figure 15. (suggested by Reviewer gZvn)"}}, "id": "TevR4YloQ0", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763384859317, "cdate": 1763384859317, "tmdate": 1763384859317, "mdate": 1763384859317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# General Response\n## Commom Problem 1: About the definitions and relationship of $GRDR$&$GA^2DR$ (To Reviewer 73Us and 8amV)\nIn training, our difficulty estimation comes from two perspectives. $GRDR$ is outcome-oriented, where difficulty is determined by the correctness ratio of rollout responses—a higher ratio indicates an easier question. Specifically, it divides the eight rollout responses (corresponding to nine possible cases) evenly into three difficulty levels. $GA^2DR$ is process-oriented, where difficulty is measured using audio attention entropy—a more dispersed attention distribution corresponds to higher entropy and greater difficulty. Experimental results show that both methods achieve strong performance.\n## Common Problem 2: About more evaluations on other benchmarks (To Reviewer 73Us, 8amV and vJnZ)\nFor ease of reading, we present the results on the two newly added benchmarks below.\nWe update our paper with two new benchmarks: MMAU-v0515 and MMAR. MMAU-v0515 refines several Q&A pairs and audio samples, while MMAR focuses on complex, cross-lingual reasoning tasks with more diverse and challenging data. Across both benchmarks, our methods maintain leading performance, further demonstrating their strong generalization ability.\n| Models (On MMAU-Test-Mini-v0515) | Sound | Music | Speech | Avg       |\n| -------------------------------- | ----- | ----- | ------ | --------- |\n| Qwen2.5-Omni-7B                  | 74.17 | 65.26 | 61.56  | 67.00     |\n| GRPO                             | 84.08 | 69.46 | 74.17  | 76.30     |\n| $\\quad$+ TR                             | 83.78 | 70.65 | 74.47  | 76.30     |\n| $\\quad$+ GRDR                           | 83.48 | 70.35 | 75.97  | 76.60     |\n| $\\qquad$+ $l_{min}=0.1$                  | 83.18 | 70.95 | 76.27  | 76.80     |\n| $\\quad$+ $GA^2DR$                       | 83.18 | 71.55 | 75.67  | 76.80     |\n| $\\qquad$+ $l_{min}=0.1$                  | 81.08 | 72.15 | 77.77  | **77.00** |\n\n| Models (On MMAR) | Sound | Music | Speech | Avg       |\n| ---------------- | ----- | ----- | ------ | --------- |\n| Qwen2.5-Omni-7B  | 58.79 | 40.78 | 59.86  | 56.70     |\n| GRPO             | 60.00 | 48.05 | 62.24  | 59.90     |\n| $\\quad$+ TR             | 64.84 | 49.51 | 63.94  | 61.90     |\n| $\\quad$+ GRDR           | 61.21 | 51.94 | 65.30  | 61.20     |\n| $\\qquad$+ $l_{min}=0.1$  | 63.63 | 52.91 | 65.98  | 63.00     |\n| $\\quad$+ $GA^2DR$       | 64.84 | 54.85 | 65.30  | 62.90     |\n| $\\qquad$+ $l_{min}=0.1$  | 64.84 | 53.39 | 63.60  | **63.00** |"}}, "id": "tggJokuKIY", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763385341538, "cdate": 1763385341538, "tmdate": 1763385341538, "mdate": 1763385341538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# General Response\n## Common Problem 3: About several hyps-parameters (G, k, $l_{min}$, Attention-layers / heads)\n\n1. **About the G in $GRDR$**: In $GRDR$, representing the number of responses rolled out for a single example. Following prior works (R1-AQA and Audio-Thinker), we set G = 8.\n\n2. **About the k in the length-based reward**: We perform an ablation study by removing the dynamic $k(\\gamma)$ and using fixed k values to validate our method. Different k settings are tested across three benchmarks, with results reported in Section 4.3 and partial data shown below. Here \"*\" represent the model-perspective difficulty label. From the data, we can see that while the length-based reward is less effective than GRDR, it still outperforms direct GRPO and TR. Different k-value settings show minor overall differences but fluctuate notably across difficulty levels, indicating that using a fixed k for all problems causes imbalance. These results further demonstrate the effectiveness of our method.\n\n   | Models (On MMAU-Test-Mini) | Easy  | Easy* | Medium | Medium* | Hard  | Hard* | Avg          |\n   | -------------------------- | ----- | ----- | ------ | ------- | ----- | ----- | ------------ |\n   | GRPO                       | 59.69 | 93.92 | 78.23  | 59.81   | 60.77 | 27.41 | 69.40 (-0.8) |\n   | $\\quad$+ TR                       | 58.14 | 93.73 | 78.43  | 57.47   | 57.75 | 25.86 | 68.40 (-1.8) |\n   | $\\quad$+GRDR (ours)               | 60.07 | 93.16 | 80.00  | 58.87   | 59.91 | 32.81 | 70.20        |\n   | $\\quad$+ Length-based Rewardf (k=2)                        | 59.69 | 94.49 | 77.84  | 58.41   | 61.63 | 27.41 | 69.40 (-0.8) |\n   | $\\quad$+ Length-based Rewardf (k=6)                        | 56.20 | 93.92 | 79.21  | 57.00   | 61.63 | 28.95 | 69.20 (-1.0) |\n   | $\\quad$+ Length-based Rewardf (k=10)                       | 58.91 | 93.92 | 79.60  | 56.07   | 59.48 | 31.27 | 69.60 (-0.6) |\n\n3. **About the selection of the attention layer and heads**: In our preliminary analysis, we examined the entropy values across different layers and found that the differences were minimal. Considering that deeper layers contain richer semantic information and better reflect the model’s final problem-solving state, we therefore select the last layer. For attention heads, since different heads contribute differently and using a single head introduces uncertainty, we take the average across all heads within the same layer.\n\n## Common Problem 4: About the computational cost of our methods\n\nWith the same batch size and G settings, direct GRPO, GRPO with TR, and GRPO with our rewards show no notable difference in training time or memory usage. Our methods add no extra computation since they reuse rollout accuracies and attention values from the forward pass. All methods train at about 0.0125 steps per second (≈80 seconds per step), and full training on FS takes roughly 3.6 days on three A100-40G GPUs (two for forward passes, one for vLLM rollout). In addition, when handling long sequences, the time cost of generating rollout responses is far greater than that of our difficulty computation, so our methods do not become a bottleneck."}}, "id": "sUIpsDyfBS", "forum": "aFZ0zQHfyO", "replyto": "aFZ0zQHfyO", "signatures": ["ICLR.cc/2026/Conference/Submission24196/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24196/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission24196/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763385526293, "cdate": 1763385526293, "tmdate": 1763385996966, "mdate": 1763385996966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}