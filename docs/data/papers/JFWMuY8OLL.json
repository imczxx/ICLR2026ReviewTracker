{"id": "JFWMuY8OLL", "number": 8450, "cdate": 1758084366420, "mdate": 1759897783092, "content": {"title": "Deepfake Detection through Color-Based Spatial-Temporal Feature Mapping with Biometric Information", "abstract": "The detection of deepfakes continues to grapple with challenges arising from the rapid evolution of generative models and the intricate characteristics of real-world data. Current detection frameworks frequently exhibit overfitting to particular artifacts, which constrains their effectiveness against novel manipulation techniques. While many models demonstrate high accuracy on standardized benchmark datasets, their performance often deteriorates when confronted with authentic deepfake instances. This study investigated the integration of biometric data, explicitly addressing the limitations of deepfake generation in mirroring the subtle biometric variations present in human faces. By segmenting facial regions into mesh representations, we analyzed the correlation between RGB features and biometric signals, particularly focusing on heart rate data. This approach enabled the development of Color-Based Spatial-Temporal (CST) feature maps, which provide a more nuanced depiction of the interactions between visual attributes and biometric inputs. The goal of this study was to propose a novel feature map and evaluate its performance. We assessed the effectiveness of these biosignal feature maps in conjunction with established detection models on the FaceForensics++ and Celeb-DF datasets. The incorporation of these feature maps resulted in remarkable outcomes, achieving nearly 99\\% accuracy (ACC) and an area under the curve (AUC) nearing 1. Importantly, cross-validation demonstrated significant improvements in both ACC and AUC with the addition of these feature maps. Transitioning to a transfer learning framework, while retaining the biosignal feature maps, yielded further enhancements in performance metrics. These findings underscore the considerable value of integrating biometric information to bolster deepfake detection capabilities, often surpassing the results of prior research while remaining anchored in fundamental learning principles. The model exhibited consistent performance across diverse cross-testing scenarios, highlighting its robustness and adaptability.", "tldr": "", "keywords": ["deepfake detection; Color spatial temporal feature map"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a79d437842a249306b23c58a01d546bff57fc61e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a color Spatial-Temporal (CST) map for deepfake detection. Instead of classifying individual frames, the method\nconverts a short face video into a single image that encodes how per-patch RGB values change over time. A CNN (mainly Xception) is trained on these CST images for real/fake classification. The paper reports in-dataset and cross-manipulation results on FF++ (c23) and cross-dataset performance on Celeb-DF (v1). A transfer-learning variant initializes a frame-based model from the CST-trained network."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s main strength is its use of physiology-linked temporal color rhythms, which are expected to provide generalizable cues than frame-level artefacts. It packages these dynamics into a compact CST image, making training and inference fast with standard CNNs."}, "weaknesses": {"value": "(1) The novelty of the paper is limited. The core idea, using heartbeat/physiology-like color changes over time to spot fake, has been\ndone before in rPPG-style detectors. Studies like FakeCatcher (Ciftci et al., 2019), DeepRhythm (Qi et al., ACM MM 2020) already explored such concepts earlier.\n\n(2) The study evaluates only on FF++ (c23) and Celeb-DF v1, both relatively established/older benchmarks; there is no evidence on more recent or harder settings (e.g., DFDC, heavy compression, diffusion fakes), means the strong generalization narrative is under-supported.\n\n(3) The approach appears architecture-dependent: CST helps CNNs but hurts ViT (Table 3), and cross-dataset results on CDFv1 do not beat the best baseline (SPSL) (Table 5). This weakens claims of broad applicability.\n\n(4) The experimental validation skipped basic sanity checks, like trying different grid sizes, different clip lengths, various lighting conditions, strong video compression, or hiding parts of the face. So, it is hard to decipher the robustness of the proposed approach under real-world artefacts, leaving doubts whether the model is truly using heartbeat-like signals or just leaning on easy, dataset-specific quirks. \n\n(5) The conclusion mentions analyzing heart rate data, but the method only builds CST from RGB and never extracts rPPG. Please either add an explicit analysis showing how CST tracks heart rate, or the claim should be “color dynamics correlated with physiology”."}, "questions": {"value": "(1) Is the train-validation-test split strictly video and identity-disjoint? How is per-video vs per-frame sampling handled?\n\n(2) It would be great to have results on broader datasets (CDF-v2, WildDeepfake, FF++ c40/raw) and under different constraints like lighting flicker, heavy compression/noise, skin-tone & makeup.\n\n(3) Do the authors explicitly extract heart rate (or similar)? If not, provide spectral evidence or rephrase the claim.\n\n(4) It is not clear why ViT underperform, and is the proposed approach sensitive to the base architecture? Any experiments with light temporal models, 3D CNNs, or hybrid CNN on CST?\n\n(5) Please address the concerns mentioned in the weakness section as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7KPSoRc6zm", "forum": "JFWMuY8OLL", "replyto": "JFWMuY8OLL", "signatures": ["ICLR.cc/2026/Conference/Submission8450/Reviewer_EFWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8450/Reviewer_EFWB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555120866, "cdate": 1761555120866, "tmdate": 1762920337070, "mdate": 1762920337070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a deepfake detection approach that integrates biometric information such as heart rate signals extracted from facial color variations, into a new feature representation called CST feature map."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written\n2. The performance of intra-data set FF++ is impressive"}, "weaknesses": {"value": "1. The paper doesn't properly do cross-dataset evaluation. The results on CDFv2, DFDC, DFDCP, KoDF, DF-Platter. The current results shown doesn't indicate that the model is generalizable and will work across a variety of datasets. \n2. The authors don't compare their performance of several recent methods. \nCVPR 24: https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_LAA-Net_Localized_Artifact_Attention_Network_for_Quality-Agnostic_and_Generalizable_Deepfake_CVPR_2024_paper.pdf\nECCV 24: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06913.pdf\nNeurIPS24: https://openreview.net/pdf?id=otZPBS0un6\nAAAI 25: https://arxiv.org/pdf/2501.04376\n\n3. It is not clear to as to how CST feature maps can capture physiological features and heart rate. Can you justify ?"}, "questions": {"value": "I have mentioned the questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WbFaPjtSGx", "forum": "JFWMuY8OLL", "replyto": "JFWMuY8OLL", "signatures": ["ICLR.cc/2026/Conference/Submission8450/Reviewer_pRDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8450/Reviewer_pRDX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881175989, "cdate": 1761881175989, "tmdate": 1762920336616, "mdate": 1762920336616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study enhances deepfake detection by integrating biometric signals, especially heart rate, with facial RGB features to create Color-Based Spatial-Temporal (CST) feature maps. Tested on the FaceForensics++ and Celeb-DF datasets, the approach achieved nearly 99% accuracy and demonstrated strong robustness, outperforming traditional models and showing superior adaptability through transfer learning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an innovative Color-Based Spatial-Temporal (CST) feature map combining RGB and biometric signals to capture subtle physiological inconsistencies.\n2. Results demonstrate near-perfect accuracy and AUC across datasets, with consistent cross-domain robustness and clear interpretability through visual analyses."}, "weaknesses": {"value": "The paper does not evaluate the proposed method on the latest deepfake generation systems such as Sora or Veo, which limits its validation against state-of-the-art video synthesis techniques and raises questions about its real-world robustness."}, "questions": {"value": "1. Could the authors explain why the method was not evaluated on deepfakes generated by diffusion-based models (e.g., Stable Diffusion, Sora, Veo)? Since these systems now dominate realistic video synthesis, such experiments are crucial for demonstrating generalization to current-generation forgeries.\n\n2. The CST feature map is said to encode biometric information such as heart rate, but the paper does not show explicit quantitative or visual evidence of this relationship. Could the authors clarify how these RGB-based temporal features were verified to correspond to genuine physiological signals rather than generic color information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7bBFTJAD5y", "forum": "JFWMuY8OLL", "replyto": "JFWMuY8OLL", "signatures": ["ICLR.cc/2026/Conference/Submission8450/Reviewer_9fi1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8450/Reviewer_9fi1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903090841, "cdate": 1761903090841, "tmdate": 1762920336236, "mdate": 1762920336236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}