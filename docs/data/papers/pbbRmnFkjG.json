{"id": "pbbRmnFkjG", "number": 9764, "cdate": 1758138975945, "mdate": 1759897699724, "content": {"title": "Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry", "abstract": "DINOv2 sees the world well enough to guide robots and segment images, but we still do not know what it sees. We conduct the first comprehensive analysis of DINOv2’s representational structure using overcomplete dictionary learning, extracting over 32,000 visual concepts in what constitutes the largest interpretability demonstration for any vision foundation model to date. This method provides the backbone of our study, which unfolds in three parts. \n\nIn the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits “Elsewhere” concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies exclusively on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular cue families matching visual neuroscience principles. \n\nTurning to concept geometry and statistics, we find the learned dictionary deviates from ideal near-orthogonal (Grassmannian) structure, exhibiting higher coherence than random baselines. Concept atoms are not aligned with the neuron basis, confirming distributed encoding. We discover antipodal concept pairs that encode opposite semantics (e.g., “white shirt” vs “black shirt”), creating signed semantic axes. Separately, we identify concepts that activate exclusively on register tokens, revealing these encode global scene properties like motion blur and illumination. Across layers, positional information collapses toward a 2D sheet, yet within single images token geometry remains smooth and clustered even after position is removed, putting into question a purely sparse-coding view of representation. \n\nTo resolve this paradox, we advance a different view: tokens are formed by combining convex mixtures of a few archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). Multi-head attention directly implements this construction, with activations behaving like sums of convex regions. In this picture, concepts are expressed by proximity to landmarks and by regions—not by unbounded linear directions. We call this the Minkowski Representation Hypothesis (MRH), and we examine its empirical signals and consequences for how we study, steer, and interpret vision-transformer representations.", "tldr": "DINOv2’s internal features specialize by task,. Concepts are distr but distributed, antipodal, position-compressed landmarks, motivating the Minkowski Representation Hypothesis: activations as convex-region sums, not sparse codes", "keywords": ["computer vision", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b7eadf33432153dd370082095f6f93da2df9478.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Using a stable sparse autoencoder, the paper builds a large concept dictionary for DINOv2 and shows task-specific feature usage. Analyses of concept statistics and geometry reveal distributed, partially sparse structure that only partially matches the Linear Representation Hypothesis. Finally, the authors propose the Minkowski Representation Hypothesis: token embeddings lie in sums of convex regions (aligned with multi-head attention’s convex mixing), which have implications in mechanistic interpretability such as model steering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Clarity:\n- The paper follows a mostly logical narrative structure: Testing LRH with Stable SAE -> Empirical findings on concept statistics and geometry -> MRH\n- Once released, the interactive concept explorer can make the work much easier to visualize.\n\nQuality:\n- The methodology is rigorous, relying on quantitative measures for its major claims (singular-value spectra, coherence vs Grassmannian/random baselines, etc).\n- The convex-hull constraint on the SAE makes it more reproducible by keeping concepts in-distribution. \n\nOriginality:\n- While concepts such as superposition and polysemanticity have been widely explored in mechanistic interpretability, the idea of convex regions is quite novel in this area, to the best of my knowledge."}, "weaknesses": {"value": "l161: Because the stable SAE constrains dictionary atoms to the convex hull of activations, convexity is guaranteed for the learned concepts but not necessarily for the native representation. Maybe explicitly separate SAE-induced claims from “model-native” claims, and add ablations with unconstrained SAE, showing the same qualitative geometry without the convex prior.\n\nl064: The importance definition is clear, but the figure mapping isn’t. Maybe specify exactly what dot size and color encode in Figure 1. Adding a concise legend in the main text, a one-line formula linking $\\phi$ to the plotted quantity would make the visualization reproducible and easier to see.\n\nl259: UMAP is fine for intuition, but it shouldn’t be used to make claims. Maybe caution that UMAP as visualization only, add a brief hyper-parameter sensitivity (n_neighbors, min_dist, seed) in the appendix, and keep the main analysis on PCA/spectral metrics.\n\nl293: \"dense Sun et al. (2025) activations\" -> \"dense activations Sun et al. (2025)\""}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QpghnXDpj6", "forum": "pbbRmnFkjG", "replyto": "pbbRmnFkjG", "signatures": ["ICLR.cc/2026/Conference/Submission9764/Reviewer_vpmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9764/Reviewer_vpmQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760930471889, "cdate": 1760930471889, "tmdate": 1762921256557, "mdate": 1762921256557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a 32k-concept dictionary extracted from DINOv2 using Sparse Autoencoders (SAEs) and presents an interactive web-based visualization platform. The authors analyze how downstream tasks selectively recruit concepts from this dictionary, finding that classification relies on \"Elsewhere\" concepts implementing object negation, segmentation uses boundary detectors forming coherent subspaces, and depth estimation draws on three families of monocular cues (projective geometry, shadows, and frequency transitions). The work proposes the Minkowski Representation Hypothesis (MRH) as an alternative to the Linear Representation Hypothesis (LRH), suggesting that token embeddings behave as sums of convex regions around archetypal landmarks rather than as sparse combinations of near-orthogonal directions. However, the authors acknowledge that MRH remains a working hypothesis requiring further empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "**Clear introduction and methodological foundation**: The paper provides adequate background on ViTs, DINOv2, vision explainability, SAE adoption, and task-specific learned concepts, making the work accessible to the broader audience.\n\n**Interpretability contributions**: The interpretation sections are particularly well-executed, and the release of a web-based visualization platform for navigating DINOv2's 32k-concept dictionary represents a valuable resource for the community.\n\n**Technical rigor**: The submission is mostly clear, technically correct, and results appear reproducible given the extended mathematical explanations, despite code not being provided.\n\n**Novel theoretical perspective**: The paper proposes MRH as an interpretability framework for understanding ViTs, suggesting future research should examine these models through this geometric lens."}, "weaknesses": {"value": "**Limited scope of downstream task exploration**: Section 3 examines only three downstream tasks, while DINOv2 supports many more applications. For classification, only the \"elsewhere\" concept is explored in detail, limiting the generalizability of findings.\n\n**Insufficient empirical foundation for MRH**: The authors propose MRH based on intuitions from task-specific geometrical organization but acknowledge it still needs rigorous proof (line 445). Given the limited number of tasks and task-specific concepts explored (see point above), the theoretical proposition rests on unstable empirical grounds.\n\n**Underdeveloped discussion of implications**: The paper lacks sufficient explanation of what consequences adopting the MRH concept has for interpreting ViTs and what concrete benefits this perspective provides to the research community. The discussion section should match the verbosity and clarity of the introduction.\n\n**Minor:**\n- Misleading characterization of SAE contribution: The authors state they \"operationalize\" LRH using SAE, when they actually adopt an SAE previously introduced by Fel et al. (2025). This overstates their methodological contribution.\n- Excessive reliance on appendix: Figures in the appendix are cited frequently in the main text (e.g., \"Different tasks recruit different concepts\" in Sec. 3, \"Empirical evidences\" in Sec. 6), resulting in sections that are difficult to follow without constant back-and-forth navigation.\n- Citations should stay in brackets, use \\citet.\n- TL;DR is broken."}, "questions": {"value": "- Sec. 3: what's the reason for isolating the top 100 most task-aligned concepts per-head when analyzing their similarities?\n- Sec. 3: how do you defined the top concepts for classification and how important or frequent is the \"elsewhere\" concept? why only this concept is analyzed for the classification task?\n- Sec. 3: you cite Fig.10 (right) in appendix but the same can be seen in Fig. 2\n- Is \"the largest interactive interpretability demo\" (line 112) an advantage? isn't complexity a problem for interpretability?\n- What's the x-axis of Fig.11 left?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wthlnE2UJ0", "forum": "pbbRmnFkjG", "replyto": "pbbRmnFkjG", "signatures": ["ICLR.cc/2026/Conference/Submission9764/Reviewer_PH2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9764/Reviewer_PH2n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555616683, "cdate": 1761555616683, "tmdate": 1762921256210, "mdate": 1762921256210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors analyze the internal representations of the DinoV2 model to provide interpretability insights into how the model performs tasks. They begin by characterizing tokens using a sparse autoencoder to create a concept dictionary. They then investigate how different tasks use this dictionary to accomplish their distinct goals and highlight their distinct differences. For example, they find that classification uses elsewhere concepts while segmentation uses boundary detectors. Secondly, the authors find that the representations are partially dense demonstrating behavior that may be inconsistent with sparse coding. Finally, the authors propose a new hypothesis called the Minkowski Representation Hypothesis which they suggest as an alternative to the LRH. They provide initial evidence for the hypothesis, then leave room for investigations in the future."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper demonstrates clear novelty and depth while providing a useful tool for other researchers to investigate further. These qualities constitute a clear accept."}, "weaknesses": {"value": "The analysis in this paper is mostly descriptive as opposed to describing why different tasks within DINOv2 have different representational sparseness etc."}, "questions": {"value": "One weakness of this paper is that it primarily is descriptive as opposed to providing an understanding of why each task has such different utilization of concepts. Can the authors describe why they believe this asymmetry is happening? Is it relative complexity between the task types etc?\n\nThe authors suggest that Dinov2’s representations follow their proposed Minkowski Representation Hypothesis. One thing that isn’t clear is whether the authors believe that other networks should follow this hypothesis. It seems to stem very naturally from attention layers. Do the authors expect other networks to follow this hypothesis? If so, when do they expect it to emerge over LRH or do they consider it an alternative hypothesis to LRH in general?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "irpm15UdKb", "forum": "pbbRmnFkjG", "replyto": "pbbRmnFkjG", "signatures": ["ICLR.cc/2026/Conference/Submission9764/Reviewer_F23h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9764/Reviewer_F23h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957077259, "cdate": 1761957077259, "tmdate": 1762921255853, "mdate": 1762921255853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies what DINOv2 “sees” by operationalizing the Linear Representation Hypothesis (LRH) with a sparse autoencoder to extract a 32k-concept dictionary from ViT activations, then analyzing how downstream tasks recruit these concepts and what the concepts’ geometry suggests. Empirically, different tasks use distinct, low-dimensional subsets: classification leans on off-object “Elsewhere” concepts, segmentation concentrates on boundary detectors, and depth estimation draws on monocular cue families. they also run geometric diagnostics and find some deviations from the LRH like structured redundancy. Motivated by this, the authors propose the Minkowski Representation Hypothesis (MRH): token embeddings lie in Minkowski sums of convex polytopes spanned by archetypal landmarks, a structure they argue multi-head attention realizes via convex combinations per head and summation across heads. They provide preliminary qualitative/quantitative signals consistent with MRH - e.g. token embeddings smoothly interpolate between landmark-like prototypes instead of varying along linear feature axes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is ambitious in scope, undertaking one of the first large-scale interpretability analyses of a state-of-the-art vision foundation model through SAEs. \n- Goes beyond static visualization to quantify how distinct downstream tasks (classification, segmentation, depth) selectively recruit different subsets of the concept space, revealing functional specialization. e.g. the paper identifies interpretable and generalizable phenomena—e.g., “Elsewhere” (off-object) concepts, border detectors, and monocular depth cues—illustrating that meaningful, task-specific structure emerges spontaneously in DINO.\n- Moves beyond the traditional “sparse direction” view of representations to analyze the geometric structure (anisotropy, coherence, spectra) of learned concepts\n- The work also makes a creative theoretical leap—proposing the Minkowski Representation Hypothesis to reinterpret transformer geometry in terms of convex archetypal regions, an idea that elegantly ties attention mechanics to cognitive theories of conceptual spaces."}, "weaknesses": {"value": "- The paper attempts to do too much—spanning SAE implementation, large-scale task analysis, and a new geometric theory (MRH)—without a unifying throughline. The connection between these parts often feels narrative rather than logically necessary.\n- The evidence for the Minkowski Representation Hypothesis is largely qualitative and circumstantial (e.g., UMAPs, smooth PCA maps, block structures). Stronger quantitative tests or falsifiable predictions are needed to substantiate the claim.\n- The observed deviations from LRH (anisotropy, coherence, low-dimensional task subspaces) could be explained by simpler mechanisms such as structured sparsity or normalization effects—yet the paper moves quickly to a new geometric framework without ruling these out.\n- Key hyperparameters (e.g., SAE sparsity level, dataset etc.) and their sensitivity are not well explored; it is unclear how robust the reported concept structure for the 3 tasks is to these design choices.\n-  While task specialization is interesting, it remains correlational; it’s unclear whether manipulating the discovered concepts truly affects task behavior. I think some causal manipulation could be interesting too."}, "questions": {"value": "- How do you quantitatively assess convex regions or Minkowski sums beyond analogy to attention mechanics?\n- How sensitive are the learned concept dictionaries to SAE initialization and regularization strength?\n- How do the discovered “Elsewhere” concepts improve human interpretability or model steering compared to prior SAE- or NMF-based approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cZutpRaT19", "forum": "pbbRmnFkjG", "replyto": "pbbRmnFkjG", "signatures": ["ICLR.cc/2026/Conference/Submission9764/Reviewer_nELv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9764/Reviewer_nELv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236693670, "cdate": 1762236693670, "tmdate": 1762921255375, "mdate": 1762921255375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}