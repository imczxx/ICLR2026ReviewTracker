{"id": "a3CUE06G5Y", "number": 19964, "cdate": 1758301013289, "mdate": 1763434776960, "content": {"title": "Learning Efficient and Interpretable Multi-Agent Communication", "abstract": "Effective communication is crucial for multi-agent cooperation in partially observable environments. However, a fundamental trilemma exists among task performance, communication efficiency, and human interpretability. To resolve this, we propose a multi-agent communication framework via $\\textbf{G}$rounding $\\textbf{L}$anguage and $\\textbf{C}$ontrastive learning (GLC) to learns  efficient and interpretable communication protocols. Specifically, GLC employs an autoencoder to learn discretized and compressed communication symbols, ensuring high communication efficiency. These symbols are then semantically aligned with human concepts using data generated by a Large Language Model (LLM), making them human-interpretable. Furthermore, a contrastive learning objective is introduced to ensure consistency and mutual intelligibility among all agents, thereby securing high task utility. GLC dynamically balances these objectives by the Information Bottleneck principle. Extensive experiments show that GLC outperforms state-of-the-art methods across multiple benchmarks, delivering superior task performance, higher communication efficiency, and enhanced human interpretability.", "tldr": "", "keywords": ["Multi-agent communication", "reinforcement learning", "contrastive learning", "language grounding"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4759d61c32eefaebb782b634a88f7809c9242916.pdf", "supplementary_material": "/attachment/a57f7d936431ffafbc00911759a5009f21c45888.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes GLC, a multi-agent communication framework that supposedly addresses the \"trilemma\" between task performance, communication efficiency, and human interpretability. They use discrete autoencoders, LLM-based language grounding, and contrastive learning. Tested on several environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The trilemma framing is reasonable, I guess. It's true that most methods don't try to optimize all three objectives at once.\nThe communication efficiency numbers in Table 1 look good - achieving 300x compression over continuous methods is noteworthy.\nFigure 4 showing the t-SNE clustering is interesting, though I have concerns about how cherry-picked these examples might be (see below)."}, "weaknesses": {"value": "This method is way too complicated. There are four different loss terms that need to be carefully balanced, three hyperparametersthat seem to require extensive tuning, moreover different scheduling strategies for each one. The paper says λ_A is \"task-dependent\" (so you need to tune it separately for each new task), λ_R uses a linear annealing schedule, and λ_C is fixed. This is a hyperparameter nightmare. \n\nHow is a practitioner supposed to know what to use for a new environment?\n\nThe whole \"dynamic balancing\" thing in Section 4.5 feels like post-hoc rationalization for hyperparameter tuning. They claim different environments \"impose distinct pressures\" that their method \"naturally adapts\" to, but really they're just tuning different weights for different tasks. That's not adaptive, that's just... tuning.\n\nThe Information Bottleneck connection is handwavy. They claim the framework is \"grounded in the IB principle\" but Equation 6 just shows a rough correspondence between loss terms and IB components. The \"detailed derivation\" in Appendix A.2 is literally 2 paragraphs and doesn't actually derive anything rigorously. This feels like slapping \"information theoretic\" labels on things to sound fancy rather than actually using IB theory to guide the design.\nAlso, why does reconstruction loss approximate I(C;O)? They're reconstructing an intermediate feature vector h, not the observation O. This seems like a pretty important distinction that's glossed over.\n\nExperimental problems:\nThe environments are tiny. A 5x5 grid with 3 agents is not realistic. Even their \"scalability\" test is just a 10x10 grid. How is this supposed to work with real multi-robot systems that might have dozens of agents in complex continuous spaces?\n\nThe comparison to LangGround seems unfair. LangGround uses 8192-bit continuous vectors while GLC uses 32-bit discrete symbols. Of course GLC is more efficient! But LangGround could also quantize its messages. A fair comparison would use the same vocabulary size. Actually looking at Table 1, aeComm uses 24 bits and performs comparably to GLC in terms of steps (5.4 vs 4.5 in ppv1), and VQ-VIB also uses discrete communication. So the efficiency advantage isn't as clear as they claim.\n\nThe baselines are pretty old - IC3Net (2019), aeComm (2021), VQ-VIB (2022). Where are more recent methods? Multi-agent RL is a fast moving field. While I am no expert, it is surprising. \n\n I'm confused about the interpretability claims in general. The agents communicate using discrete symbols (32 bits = presumably 2^32 possible messages?). How do you translate these to natural language for a human? The paper doesn't really explain this. They mention \"cosine similarity matching against a predefined phrase dataset D\" in the appendix but that sounds like nearest-neighbor retrieval which could easily fail for novel messages.\n\nSection 4.5 is confusing and doesn't clearly explain WHY each hyperparameter needs a different strategy\nFigure 2 is way too cluttered, I can barely understand what's happening\nLots of important details are hidden in the appendix (like how the text interface works)\nThe \"dual-environment framework\" mentioned in Section 4.2 is never formalized\nSome claims are overstated - they say the method \"dynamically balances\" objectives but really you just set different weights for different tasks"}, "questions": {"value": "How do you actually show a human what the agents are communicating in real-time? The nearest neighbor lookup to dataset D seems brittle.\n\nWhy does GLC+LLM underperform pure LLM? Doesn't this contradict your interpretability claims?\n\nTables 7-9 show hyperparameter sensitivity but don't provide any principled way to choose values for new domains. How is this usable?\n\nThe IB principle is about trading off compression and task-relevant information. How do you decide how much to weight each one? The current approach seems arbitrary.\n\nYou claim the method \"naturally adapts\" to different task pressures but you're manually setting different weights for each environment. That's not adaptation, that's human tuning. Am I misunderstanding something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "voP2g2sCJV", "forum": "a3CUE06G5Y", "replyto": "a3CUE06G5Y", "signatures": ["ICLR.cc/2026/Conference/Submission19964/Reviewer_YdVf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19964/Reviewer_YdVf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843124763, "cdate": 1761843124763, "tmdate": 1762999988703, "mdate": 1762999988703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a trilemma in multi-agent cooperation, which are **task performance** that agents need to coordinate well to succeed, **communication efficiency** that messages between agents should be compact, **human interpretability** that messages should align with human-understandable concepts. It introduces a new framework, Grounding Language and Contrastive Learning (GLC), to balance the three objectives simultaneously. At the core, GLC has an autoencoder to learn discrete symbols, which ensures highly efficient communication. GLC grounds language by aligning these discrete symbols with human concepts using data generated by a large language model, which makers the symbols interpretable by humans. Additionally, GLC has a contrastive learning objective to ensure consistency and mutual intelligibility among all agents, which ensures high task utility. For experiments, GLC show strong performance in multiple metrics over multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is well motivated by simultaneously tackling three main problems (i.e. task utility, communication efficiency, human interpretability) in multi-agent cooperation games, with prior works only tackle one or two dimensions.\n2. Use of large language models (LLM) -generated message embeddings avoids human labeling, and effectively aligns the symbol to humans since LLMs are known for the capability of producing human-like language. This process can also be considered as teaching, essentially from LLM to the MARL agent.\n3. Paper is well-written with nice illustrative figures. Experimental results are strong compared with a large set of baselines and ablations."}, "weaknesses": {"value": "1. Since the language grounding is through a cosine similarity loss between the embeddings, therefore it is only interpretable at semantic level, but not at syntax level (i.e. compositionality, etc.). However, human-interpretable languages should consider both levels."}, "questions": {"value": "1. GLC communicates at 32 bits per step, despite it's very efficient, but have you evaluated what will be the efficiency for the LLMs? One property of human language or human-like language is not very efficient as our language often contain words / tokens that are not semantically meaningful. Therefore, communication efficiency lowers the interpretability or resemblance to human language?\n2. What will be scalability of this method since the tasks being evaluated are mainly those simple multi-agent games for emergent communication? For example, whether it is applicable to real-world tasks or whether it can continuously improve existing language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wyS4fIkJlH", "forum": "a3CUE06G5Y", "replyto": "a3CUE06G5Y", "signatures": ["ICLR.cc/2026/Conference/Submission19964/Reviewer_NCwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19964/Reviewer_NCwj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845793654, "cdate": 1761845793654, "tmdate": 1762999988725, "mdate": 1762999988725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the GLC framework addresses a foundational challenge in MARL, reconciling high task performance, communication efficiency and human interpretability. GLC integrates a discrete autoencoder, to compress inter-agent communication channels for efficiency, an LLM to ensure messages are interepretable while a contrastive learning objective is used for inter-agent semantic consistency across diverse agents. The authors evaluate their framework in common MARL benchmarks - USAR and Predator Prey. The authors demonstrate superior performance to other communication framework baselines, but at a much higher communication efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. GLC's loss function is decomposed into tractable surrogate rewards - task reward, human interpretability and inter-agent consistency and it integrates it along with a compression term (reconstruction loss) for the autoencoder. The use of explicit IB objectives with LLM integration provides a robust framework for future research.\n2.  Online querying of the LLM, and not just as a reference for post-hoc or expert trajectories, allows agents to adapt to evolving team topologies or task variations, as demonstrated by the strong zero-shot generalization."}, "weaknesses": {"value": "1. Computational and practical implications of an online LLM querying are not explored or evaluated, as scalability concerns both in computation costs and as agent populations increases seems to be concerning.\n2. Longer horizon tasks are not evaluated where an LLM might experience a shift.\n3. Traffic Junction, StarCraft II, SMAC would be better benchmarks, however, the training time needs to be reasonable for that. A deeper discussion on computational viability of the framework would be helpful for the community."}, "questions": {"value": "The decoding mechanism for latent state to LLMs depend on similarity based approaches and maybe fail to capture deeper relations. Do you have insights on how the framework would perform on more complex tasks with more agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j9CCtMfskX", "forum": "a3CUE06G5Y", "replyto": "a3CUE06G5Y", "signatures": ["ICLR.cc/2026/Conference/Submission19964/Reviewer_WbEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19964/Reviewer_WbEU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102137359, "cdate": 1762102137359, "tmdate": 1762999989010, "mdate": 1762999989010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multiagent communication framework, Grounding Language and Contrastive learning (GLC), to learn an efficient and interpretable protocol. It includes three components: 1) an autoencoder that compress message to discrete symbols, 2) a contrastive learning objective that aligns messages from other agents in the same time window and pull away messages from other trajectories in the same batch, and 3) a dual LLM agent that enables alignment with natural language. The experiments with Predator-Prey and USAR  environments show that GLC performs better than prior methods and the learned embeddings are interpretable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a unified framework to learn communication protocols that are both practically efficient and semantically meaningful.\n- The proposed method has alignment with human language while being efficient.\n- The system incorporates a mechanism for dynamically adjusting the weights of various loss terms based on different situations.\n- Extensive experiments are presented to demonstrate the utility, interpretability, and efficiency of the proposed method."}, "weaknesses": {"value": "- The experiment tasks and baselines are not clearly introduced. It is unclear the communication goal of the tasks and what each baseline represents. \n- It is unclear why contrastive learning to align messages in a temporal window can be useful in complex collaboration settings, agents need to split a task. In this case, align the same language will lead to all agents performing the same task.\n- The language example in Fig. 4 shows that the language structure is simple, e.g., going to a certain cell. It is unclear how well the learned protocol can generalize to a map with a different size. \n- The interpretation metrics rely on cosine similarity and BLEU. However, these measurement can have high scores but with key information being different. It will be a better evaluation if it can evaluate the semantic equivalence rather than these scores."}, "questions": {"value": "- The author uses LLM agent as a human proxy to show the generalization to unseen agents. Is it possible for real human to collaborate with GLC and what’s the performance of that?\n- Though the size of the environment is scaled, the exact number of agents used in that environment is unclear.\n- How does GLC compare with LLM agent with compressed messages? This baseline should give good efficiency and high interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CNPHCUs4gm", "forum": "a3CUE06G5Y", "replyto": "a3CUE06G5Y", "signatures": ["ICLR.cc/2026/Conference/Submission19964/Reviewer_wBBP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19964/Reviewer_wBBP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762405017304, "cdate": 1762405017304, "tmdate": 1762999988828, "mdate": 1762999988828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their time and thoughtful feedback, which has been invaluable in helping us improve this work. We are particularly grateful for your recognition of GLC's novelty and methodological soundness in addressing efficient and interpretable communication for MARL agents.\n\nA common question raised by multiple reviewers concerns the scalability of our approach. In response, we have performed additional experiments demonstrating GLC's strong performance in larger-scale environments. These new results are presented in $\\textbf{Appendix A.3.7}$, and we have added a new Appendix B that provides extended discussion on scalability and broader implications. For the convenience of all reviewers, we include $\\textbf{Appendix B}$ directly in this rebuttal document.\n\nFor the specific questions and suggestions from each reviewer, we provide individual point-by-point responses in their respective sections below. We believe these clarifications and additional results adequately address all raised concerns and further strengthen our contribution.\n\n$\\textbf{Appendix B Discussion on Scalability and Future Work}$\n\nOur selection of the Predator-Prey and USAR environments was strategic, as they serve as established and computationally tractable testbeds that effectively capture the core challenges of the efficiency-utility-interpretability trilemma under study. These environments allowed for the extensive ablation studies and convergence analyses necessary to validate GLC's core contributions within practical resource constraints. We acknowledge that evaluation on larger-scale benchmarks like the StarCraft Multi-Agent Challenge (SMAC) or real-world robotic simulators represents a valuable direction for future work, and we confirm that the GLC framework is environment-agnostic and readily generalizable to such scenarios.\n\nThe GLC architecture is inherently designed for scalability through several core principles. The discrete autoencoder ensures bandwidth-efficient communication that is invariant to environment size or agent population. Furthermore, the contrastive learning objective maintains semantic consistency and protocol coherence across large agent populations by structuring the communication space based on functional context. The dynamic balancing mechanism, guided by the Information Bottleneck principle, allows the system to adaptively prioritize different objectives—such as compression or semantic richness—depending on the task's complexity and scale. Our scalability experiments in $\\textbf{Appendix A.3.7}$, conducted on enlarged grid worlds with increased agent and prey populations, empirically validate that GLC maintains robust performance and communication efficiency as the problem scale expands. To further demonstrate GLC's generalization ability, we plan to test it in more complex embodied settings such as ALFWorld (multi-step reasoning with natural language) and RoCoBench (grounded multi-agent collaboration). Success in these domains would strongly validate GLC's practicality for real-world human-AI collaboration under longer horizons and physical constraints.\n\nIn terms of computational viability, we emphasize that GLC's design is highly efficient and practical for real-world deployment. The use of the LLM is strictly confined to a one-time, offline phase for generating a static dataset of expert trajectories. During the central training and deployment phases, no LLM queries are made, eliminating any ongoing computational overhead, latency, or cost associated with large model inference. This makes GLC particularly suitable for bandwidth-constrained applications like robotic swarms or autonomous vehicle networks, where both interpretability and low communication latency are critical.}\n\nGLC creates a synergistic relationship with LLMs rather than seeking to replace them. While LLMs serve as general-purpose knowledge bases and a source of human-aligned semantic grounding, GLC learns task-specific, highly efficient communication protocols. Our ad-hoc teamwork experiments demonstrate that these two paradigms can interoperate effectively, with GLC agents successfully collaborating in mixed teams with LLM agents. This shows that GLC's protocols are not only efficient but also semantically accessible to external human-like intelligences, bridging the gap between opaque RL protocols and verbose natural language.}\n\nLooking ahead, our future work will explicitly explore GLC's application in more complex and demanding domains. This includes application to extended multi-agent benchmarks like SMAC, investigation into distributed training strategies to handle increased environmental complexity, and deeper analysis of how the emergent communication vocabulary and its syntactic structure evolve with task difficulty. We are confident that the GLC framework provides a solid and scalable foundation for these future research directions toward practical and interpretable multi-agent systems."}}, "id": "8qvs3rJHgd", "forum": "a3CUE06G5Y", "replyto": "a3CUE06G5Y", "signatures": ["ICLR.cc/2026/Conference/Submission19964/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19964/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission19964/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763435674904, "cdate": 1763435674904, "tmdate": 1763435674904, "mdate": 1763435674904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}