{"id": "CELYMnherN", "number": 12335, "cdate": 1758207141773, "mdate": 1759897516181, "content": {"title": "A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components", "abstract": "We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.", "tldr": "We present a novel generalized expectation-maximization algorithm for federated clustering problems where the total number of unique clusters across clients is unknown.", "keywords": ["clustering", "gaussian mixture model", "federated learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8895cddb139e2cc59742d0f44bbdb7adbfd8839c.pdf", "supplementary_material": "/attachment/204915f99a07374deb760809102c54aa703571e5.zip"}, "replies": [{"content": {"summary": {"value": "The authors discuss clustering in the federated setting. Data is distributed across clients, who share cluster centroids but follow different cluster weights, specifically, no client has data from all clusters. . The effective novelty of the paper is the ability of the proposed FedGEM to automatically infer the number of clusters across the federation. \nThe authors discuss convergence properties of FedGEM in the general case and for the special case of GMMs. The paper concludes with a variety of empirical studies, specifically showing that FedGEM works even if \"real\" data is not well-separated, thereby invalidating a core prerequisite for the theoretical analyses (concavity)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper appears to be thorough and well-executed. The authors spend time analyzing a lot of aspects of their method in an extensive Appendix. The methods limitations are well-covered along with its strengths.\nThe problem of federated clustering is highly relevant, the method is original and automatic cluster count detection unlocks a new capability. \nNote that I did not check derivations."}, "weaknesses": {"value": "I do not follow the paper's motivation about OEM fault detection. How exactly does this relate to (federated) clustering? The authors claim the dimensionality of the data poses a problem - however all of their experiments are well within the range of internet-transferable sizes. (The largest dataset across clients comes in at ~17mb). \nAlthough the paper describes some limitations of the proposed method, I see a few more that I think need to be addressed:\n- The entire analysis and experimental result assumes full client participation. A realistic federation might only have a subset of clients participate at every round. The server-side aggregation would require rethinking\n- The final aggregation radius needs to be determined through cross-validation. This is highly impractical in the federated setting as it would involve running the entire algorithm end-to-end multiple times. Furthermore, the optimal radius depends on the data geometry, scaling the feature space would require a new radius.\n- It is unclear how anisotropic covariances would change the nature of aggregation radii\n- Cross-validation incurs a huge price for DP guarantees\n\nmissing communication cost quantification. The authors note that AFCL requires clients sharing arrays of the same size as the local data. That price might be worth wile to pay (in terms of communication cost) if the total number of communication rounds is smaller due to a faster convergence rate. Especially as the number of clusters per round is variable, per-iteration costs of transmitting centroids could be high."}, "questions": {"value": "If the authors could discuss the limitations I believe to be missing in the paper and also discuss communication costs, I'll consider raising my score!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ao0HyuasLo", "forum": "CELYMnherN", "replyto": "CELYMnherN", "signatures": ["ICLR.cc/2026/Conference/Submission12335/Reviewer_vQdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12335/Reviewer_vQdW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864411304, "cdate": 1761864411304, "tmdate": 1762923258154, "mdate": 1762923258154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents a federated generalized expectation-maximization (FedGEM) algorithm for clustering in a setting where clients have heterogeneous data and the total number of clusters across the clients is unknown. FedGEM involves clients performing local EM steps to identify local cluster centers and then constructing uncertainty sets around these centers. These sets (centers and radiuses) are communicated to a server, which infers overlaps between local clusters to collaboratively train shared cluster parameters and estimate the possible total number of clusters. The authors provide a theoretical analysis of the convergence of the FedGEM and its ability to correctly infer the cluster count. The empirical evaluation demonstrates that FedGEM achieves state-of-the-art performance, outperforming the mentioned baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Sound Validation: The extensive experimental results demonstrate the effectiveness of the FedGEM. The results in Table 1 show that FedGEM outperforms AFCL, the only other federated baseline that operates without knowing K. Also, FedGEM is competitive with federated methods that are given the true K in advance. The sensitivity study further indicates the robustness of FedGEM, showing strong performance even when theoretical assumptions like well-separated clusters are violated.\n- Theoretical Analysis: This submission is supported by a theoretical analysis. The authors provide probabilistic convergence guarantees for the general FedGEM algorithm under standard assumptions (Theorems 1, 2, and 5). Also, this submission provides a detailed analysis for the isotropic GMM setting, where they prove the First-Order Stability (FOS) condition for multi-component GMMs (Theorem 6)."}, "weaknesses": {"value": "- Generality: The implementation, theoretical verification, and experiments are based on an isotropic GMM. However, its performance on real-world data with arbitrarily shaped clusters might be limited.\n- Computational Complexity: The server is required to identify cluster overlaps involves pairwise comparisons between all local components from all clients. This is a computational complexity that scales quadratically with the total number of local components across the network. Though the scalability study in the appendix shows good performance, this could become a bottleneck in FL settings with thousands of clients.\n- Presentation and Grammar Issues: 1) grammar issue: line 96-97; 2) the names of the mentioned methods should be in the same font if the authors would like to emphasize"}, "questions": {"value": "- How to set `final_aggregation_radius`? Could you provide more practical heuristics for setting this value? How sensitive are the ARI and the estimated K to this hyperparameter?\n- Others, please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pSOSS3jJGO", "forum": "CELYMnherN", "replyto": "CELYMnherN", "signatures": ["ICLR.cc/2026/Conference/Submission12335/Reviewer_BFt3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12335/Reviewer_BFt3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923920983, "cdate": 1761923920983, "tmdate": 1762923257850, "mdate": 1762923257850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FedGEM, a federated generalized EM algorithm for mixture models with an unknown global number of components (\\\\(K^\\*\\\\)). Each client runs a local GEM on an isotropic GMM with fixed mixture weights and covariances, updating only component means and computing uncertainty radii. The server merges overlapping components based on these uncertainty sets and performs constrained parameter updates. Theoretical results establish finite-sample and population convergence, as well as a sufficient condition for correct (\\\\(K^\\*\\\\)) recovery. Experiments on synthetic and benchmark datasets show competitive clustering accuracy and moderate scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "# originality\nFederated EM with unknown global K via uncertainty-set–based merging at the server is an interesting angle relative to standard federated clustering/EM, and distinct from k-means-style approaches\n\n# quality\n\nThe paper presents local convergence results for the GEM variant, as well as finite-sample versus population map deviation bounds under the stated assumptions.\n\nthe radius subproblem has a stated unique solution and a low-complexity solver with a convergence and complexity discussion\n\n# clarity\n\nProblem setup and the server pseudo-code for super-clustering/aggregation are explicit\n\nAssumptions are listed and some are justified in an appendix\n\n# significance\n\nUnknown K across many clients arises in FL. The approach could be a useful building block if the server-side merging is robust and the hyperparameters are chosen well. Empirical results indicate competitive performance and a reasonable scaling trend"}, "weaknesses": {"value": "# originality\n\nThe method is positioned generally (\"mixture models with unknown K\"), but the analysis and implementation hinge on isotropic Gaussians with fixed weights; this narrows the contribution relative to the stated ambition. Extending to anisotropic covariances or learning pi would be more compelling.\n\n# quality\n\nweights fixed, covariance fixed to identity, Kg known locally. the paper also does not study the effects of mispecified pi.\n\nThe overlap-detection step entails pairwise checks over all client components, i.e., quadratic in the total number of components. the paper relies on empirical timing rather than giving a tight complexity analysis for this stage. This should also be discussed as a limitation.\n\nit is not analyzed whether a client (or all clients) stuck at a sharp local maximizer will \"snap back\" after the server’s within-set update. \n\n\n\n# clarity\n\nAssumption 6 (supremum variable vs. the conditioning argument of M / $\\hat{M}$\nThe role and definition of the strong concavity parameter ​$\\lambda_g$ are not introduced where first used (Assumption 3)\n\n# significance\n\nFinal aggregation radius $\\epsilon^{final}$ is a user hyperparameter. While a sufficient condition wrt Rmin is given, Rmin is not known. The paper should indicate a protocol for choosing this hyperparameter\n\nNo discussion of communication/computation trade-offs vs. alternative K estimation strategies (e.g., centralized model selection surrogates, Bayesian nonparametrics) in FL."}, "questions": {"value": "See weaknesses.\n\nOther questions:\n\nAnisotropic models: Do you foresee obstacles to extending the convergence and finite-sample analysis to full-covariance Gaussians (or even tied/diagonal covariances)? What breaks in the proofs?\n\nIs fixing pi essential for your bounds, or could pi be updated (with constraints) without derailing the strong concavity/FOS arguments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "54B9KgW7vX", "forum": "CELYMnherN", "replyto": "CELYMnherN", "signatures": ["ICLR.cc/2026/Conference/Submission12335/Reviewer_iREx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12335/Reviewer_iREx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998395412, "cdate": 1761998395412, "tmdate": 1762923257575, "mdate": 1762923257575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. The algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. \n\nThis paper performs a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, this paper studys the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. This paper also performs various numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The structure of the paper is relatively clear. \n2. The theoretical derivation and proof in the paper are quite thorough."}, "weaknesses": {"value": "There are only two compared algorithms that do not assume prior knowledge of K. Among them, one is the algorithm proposed in 1974, and the other is AFCL, which uses entirely different datasets from those in this paper. These two aspects result in flaws in the experimental design, making it difficult to truly reflect the algorithm's performance."}, "questions": {"value": "As above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J5t4zWwnc0", "forum": "CELYMnherN", "replyto": "CELYMnherN", "signatures": ["ICLR.cc/2026/Conference/Submission12335/Reviewer_aUfD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12335/Reviewer_aUfD"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission12335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762505488711, "cdate": 1762505488711, "tmdate": 1762923257260, "mdate": 1762923257260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}