{"id": "yBAp76dQMf", "number": 22308, "cdate": 1758329473242, "mdate": 1763703311529, "content": {"title": "FrameOracle: Learning What to See and How Much to See in Videos", "abstract": "Vision-language models (VLMs) have advanced video understanding, but their performance is limited by the number of input frames they can process. Existing frame sampling strategies, such as uniform or fixed-budget selection, often fail to adapt to variations in information density or task complexity, resulting in inefficiency and information loss. To address this, we present **FrameOracle**, a lightweight and plug-and-play module that predicts both (1) which frames are most relevant to a given query and (2) how many frames are needed. FrameOracle is trained using a four-stage curriculum, with the first three stages relying on weak proxy signals such as cross-modal similarity. In the final stage, it leverages stronger supervision from a new dataset we introduce, **FrameOracle-41K**, the first large-scale VideoQA collection to provide keyframe annotations specifying the minimal set of frames required to answer each question. Extensive experiments across five VLMs and six benchmarks demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4 frames without any loss in accuracy. When starting from 64-frame candidates, it reduces the input to an average of 13.9 frames while improving accuracy by 1.4\\%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable video understanding.", "tldr": "FrameOracle is a lightweight, plug-and-play module that jointly predicts which frames and how many to keep for efficient video understanding.", "keywords": ["Video Understanding", "Adaptive Frame Sampling", "Vision-Language Model", "Video Large Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e276750702186e3768d29a9454e002846683b51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses a key challenge in video understanding with vision-language models (VLMs):\nHow to efficiently select the most relevant frames from a video to answer a given query, while also determining how many frames are actually needed. To answer these questions the authors propose a 'plug-and-play' frame selection (named FrameOracle) to select which and how many frames to use for a given query. In addition, the paper also contributes a dataset named FrameOracle-41K which contains information regarding the important frames for each query inside the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 - Adaptive frame selection method that selects how many frames and which frames to use, an extension of what current SOTA models are doing.\n2 - New dataset: the dataset is a significant contribution of this work, despite the little attention given to it.\n3 - The authors have considered many different datasets which increase the reliability of the method in the considered settings."}, "weaknesses": {"value": "After carefully reading the paper I have the following doubts which I categorize as weaknesses:\n\n1 - The adaptive frame selection setting: While the authors claim adaptive frame selection, they are in fact doing adaptive frame selection from a fixed pool of frames. This is not aligned with the nature of videos which can be in variable sizes. This becomes more critical when you apply the method to long video understanding, depending on the quantity of information the video contains and the dynamic, using 16 or 64 frames (uniformly sampled before the frame selection mechanism) is not enough let alone when reduced to ~10 frames. While this is mentioned as a limitation from the authors, I fail to recognize an important contribution from the method if the frame selection cannot operate on variable sequence length (or even fixed but complete) since it is always bounded to the correctness of the uniform sampling.\n\n2 - Plug and play: The method is not actually plug and play since the query is encoded from the vlm tokenizer, which means for every VLM with different tokenizer, you have to train a separate model. It would have been plug and play if you would have used the for example the SigLIP language encoder.\n\n3 - Feature fusion: the cross-modal fusion is an integral part of your technical contributions, but it is unclear how you do it. Do you concatenate the tokens?\n\n4 - Transformer encoder layer: Features are fused and then sent to the transformer encoder layer. How does the transformer process the tokens, is it a global attention (i.e. all tokens from all frames), is it spatial and then temporal, or is it only spatial? This is related to weakness 3 also. I guess the architecture part is a bit undermined in this work. The reason why I stress this point is because the frame selection is mainly a mechanism to reduce computation while keeping or improving the accuracy. In terms of memory, for an LLM/VLM the most expensive operation is the self-attention (let's consider a plain self-attention) due to quadratic memory scaling. Now, if the encoder layer is using self-attention among all the tokens, it means the transformer encoder has memory requirements similar to those of the LLM/VLM during computations (the difference would be the number of heads) with exception to the system prompt tokens. So, while reducing latency, the frame selector has big memory requirements. I suggest this point is clarified.\n\n5 - Why not use the visual tokenizer of the VLM directly, the method is not plug-and-play anyway. The visual tokenizer of the VLM is already align with language (so no stage-1 training), and can possibly ease the 'which frames to use' problem. (Note, this will not affect my evaluation negatively, is more for my own curiosity.)\n\n6 - The 4 stage training: What would happen if the training would consist only of stage-1 and 4? An experiment would be interesting. Additionally, stage-2 and -3 freeze and unfreeze the Rank head and K head. If you train the model (even with a low learning rate) in stage-3 while you freeze Rank head, the performance of Rank head will decrease. Now how big of the problem this is depends on the frames given in input and the video composition and it would be problematic for long videos and scenarios with high sensitivity on the frame choice. The approach is not validated rigorously to have a conclusion on this matter.\n\n7 - Validation: While the paper validates across many datasets and compares with previous works in different datasets, I think it is evaluated in a very shallow way without depth. The ablations are not very comprehensive, just the 4 stages. No insights on the visual backbone choice or text tokenizer, and many more (see above).\n\n8 - Dataset: The dataset, in my understanding would be the bigger contribution to this work but it is clearly overlooked and very little analysis and experiments are done with it. \n\n9 - While the work considers only frame selection mechanism, entering in the long video world, I guess is fair to consider and compare against works that apply to long video understanding (is not necessary for the proposed method to surpass those works, but to have an idea how it actually helps when compared to methods designed for long videos). \nYou can have a look at: \"Moviechat+: Question-aware sparse memory for long video question answering.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2025), which is training free but uses a memory layer (plug-and-play) to compress frames rather than select them, or \"ReWind: Understanding Long Videos with Instructed Learnable Memory.\" Proceedings of the Computer Vision and Pattern Recognition Conference (2025). This is more a suggestion to see how selecting compares to compression. \n\n\nGiven these concerns, I will suggest a weak reject. The work has no significant technical contributions, it is incremental. In addition, the theoretical contributions are not significant to compensate the technical ones. The dataset is the only significant contribution but is clearly not the main focus of the paper. I am open to improve my score if my concerns are addresses or my interpretations are deemed as not correct."}, "questions": {"value": "Check the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f51pfrbpkR", "forum": "yBAp76dQMf", "replyto": "yBAp76dQMf", "signatures": ["ICLR.cc/2026/Conference/Submission22308/Reviewer_p9ow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22308/Reviewer_p9ow"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760505939977, "cdate": 1760505939977, "tmdate": 1762942162919, "mdate": 1762942162919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a key challenge in video understanding with vision-language models (VLMs):\nHow to efficiently select the most relevant frames from a video to answer a given query, while also determining how many frames are actually needed. To answer these questions the authors propose a 'plug-and-play' frame selection (named FrameOracle) to select which and how many frames to use for a given query. In addition, the paper also contributes a dataset named FrameOracle-41K which contains information regarding the important frames for each query inside the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 - Adaptive frame selection method that selects how many frames and which frames to use, an extension of what current SOTA models are doing.\n2 - New dataset: the dataset is a significant contribution of this work, despite the little attention given to it.\n3 - The authors have considered many different datasets which increase the reliability of the method in the considered settings."}, "weaknesses": {"value": "After carefully reading the paper I have the following doubts which I categorize as weaknesses:\n\n1 - The adaptive frame selection setting: While the authors claim adaptive frame selection, they are in fact doing adaptive frame selection from a fixed pool of frames. This is not aligned with the nature of videos which can be in variable sizes. This becomes more critical when you apply the method to long video understanding, depending on the quantity of information the video contains and the dynamic, using 16 or 64 frames (uniformly sampled before the frame selection mechanism) is not enough let alone when reduced to ~10 frames. While this is mentioned as a limitation from the authors, I fail to recognize an important contribution from the method if the frame selection cannot operate on variable sequence length (or even fixed but complete) since it is always bounded to the correctness of the uniform sampling.\n\n2 - Plug and play: The method is not actually plug and play since the query is encoded from the vlm tokenizer, which means for every VLM with different tokenizer, you have to train a separate model. It would have been plug and play if you would have used the for example the SigLIP language encoder.\n\n3 - Feature fusion: the cross-modal fusion is an integral part of your technical contributions, but it is unclear how you do it. Do you concatenate the tokens?\n\n4 - Transformer encoder layer: Features are fused and then sent to the transformer encoder layer. How does the transformer process the tokens, is it a global attention (i.e. all tokens from all frames), is it spatial and then temporal, or is it only spatial? This is related to weakness 3 also. I guess the architecture part is a bit undermined in this work. The reason why I stress this point is because the frame selection is mainly a mechanism to reduce computation while keeping or improving the accuracy. In terms of memory, for an LLM/VLM the most expensive operation is the self-attention (let's consider a plain self-attention) due to quadratic memory scaling. Now, if the encoder layer is using self-attention among all the tokens, it means the transformer encoder has memory requirements similar to those of the LLM/VLM during computations (the difference would be the number of heads) with exception to the system prompt tokens. So, while reducing latency, the frame selector has big memory requirements. I suggest this point is clarified.\n\n5 - Why not use the visual tokenizer of the VLM directly, the method is not plug-and-play anyway. The visual tokenizer of the VLM is already align with language (so no stage-1 training), and can possibly ease the 'which frames to use' problem. (Note, this will not affect my evaluation negatively, is more for my own curiosity.)\n\n6 - The 4 stage training: What would happen if the training would consist only of stage-1 and 4? An experiment would be interesting. Additionally, stage-2 and -3 freeze and unfreeze the Rank head and K head. If you train the model (even with a low learning rate) in stage-3 while you freeze Rank head, the performance of Rank head will decrease. Now how big of the problem this is depends on the frames given in input and the video composition and it would be problematic for long videos and scenarios with high sensitivity on the frame choice. The approach is not validated rigorously to have a conclusion on this matter.\n\n7 - Validation: While the paper validates across many datasets and compares with previous works in different datasets, I think it is evaluated in a very shallow way without depth. The ablations are not very comprehensive, just the 4 stages. No insights on the visual backbone choice or text tokenizer, and many more (see above).\n\n8 - Dataset: The dataset, in my understanding would be the bigger contribution to this work but it is clearly overlooked and very little analysis and experiments are done with it. \n\n9 - While the work considers only frame selection mechanism, entering in the long video world, I guess is fair to consider and compare against works that apply to long video understanding (is not necessary for the proposed method to surpass those works, but to have an idea how it actually helps when compared to methods designed for long videos). \nYou can have a look at: \"Moviechat+: Question-aware sparse memory for long video question answering.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2025), which is training free but uses a memory layer (plug-and-play) to compress frames rather than select them, or \"ReWind: Understanding Long Videos with Instructed Learnable Memory.\" Proceedings of the Computer Vision and Pattern Recognition Conference (2025). This is more a suggestion to see how selecting compares to compression. \n\n\nGiven these concerns, I will suggest a weak reject. The work has no significant technical contributions, it is incremental. In addition, the theoretical contributions are not significant to compensate the technical ones. The dataset is the only significant contribution but is clearly not the main focus of the paper. I am open to improve my score if my concerns are addresses or my interpretations are deemed as not correct."}, "questions": {"value": "Check the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f51pfrbpkR", "forum": "yBAp76dQMf", "replyto": "yBAp76dQMf", "signatures": ["ICLR.cc/2026/Conference/Submission22308/Reviewer_p9ow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22308/Reviewer_p9ow"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760505939977, "cdate": 1760505939977, "tmdate": 1763704979758, "mdate": 1763704979758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lightweight and plug-and-play module capable of dynamically selecting a variable number of frames based on the difficulty of each question.\n\nIn addition, the authors introduce a curriculum-based training strategy to effectively train this frame selection module.\n\nThe paper also designs a data generation pipeline that provides the minimal set of frames required to answer each question, forming the first VideoQA dataset with such keyframe annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is detailed and clearly presented, with strong motivation and solid overall design.\n\n2. A novel module is introduced to jointly predict the number of frames to select and frame-level importance scores, along with a carefully designed training paradigm. The method is validated across multiple backbones, showing good generalization.\n\n3. The authors build a new data generation pipeline, providing the first VideoQA dataset with minimal keyframe annotations, which is a valuable contribution to the community."}, "weaknesses": {"value": "1. The proposed multi-head design for predicting both frame count and frame importance is reasonable, but it depends on the backbone’s global reasoning capability. The paper should clarify whether different backbones lead to significantly different results.\n\n2. It would be helpful to compare against a strategy that fixes or predicts a total information budget (instead of a frame count) as the selection target — would such a formulation be more reasonable?\n\n3. The paper adopts a curriculum learning scheme with four progressive training stages, and the ablation study supports each stage’s usefulness. However, is such staged training truly necessary? Could joint training achieve similar results? A comparison would make the claim more convincing.\n\n4. Since the selected frames are all highly important, do they sometimes concentrate around similar patterns, causing redundancy in visual information? A discussion or visualization could help clarify this."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rMCC76JW8u", "forum": "yBAp76dQMf", "replyto": "yBAp76dQMf", "signatures": ["ICLR.cc/2026/Conference/Submission22308/Reviewer_CCS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22308/Reviewer_CCS3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901920088, "cdate": 1761901920088, "tmdate": 1762942162430, "mdate": 1762942162430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FrameOracle, a lightweight and plug-and-play frame selection module that dynamically determines both the most relevant frames and the number of frames required for a given video understanding task. To support its training, the authors also present FrameOracle-41K, the first large-scale VideoQA dataset annotated with keyframes that specify the minimal frame subset needed to answer each question. The proposed approach is evaluated across six benchmarks and compared against five vision-language models (VLMs), demonstrating strong efficiency gains while preserving task accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces FrameOracle-41K, a large-scale dataset specifically created for keyframe selection in VideoQA, with annotations indicating the minimal set of frames required to answer each question.\n\n- The proposed method improves computational efficiency by reducing the number of processed frames, while still maintaining comparable or better accuracy than full-frame baselines.\n\n- FrameOracle outperforms existing keyframe selection methods, showing better frame relevance and stronger downstream task performance across multiple benchmarks."}, "weaknesses": {"value": "- The data generation process heavily relies on another agent model for producing keyframe annotations, which raises concerns about potential bias, annotation noise, and the dependency of the dataset quality on the agent’s capabilities.\n\n- The data generation pipeline appears relatively simple and lacks clear novelty. How does it differ from existing data generation approaches, and what unique contributions does it offer?\n\n- The training process consists of four distinct stages, which adds considerable complexity to the pipeline and may hinder scalability and ease of adoption in practical settings."}, "questions": {"value": "How does a simple baseline, such as uniform sampling, perform in comparison? For instance, when FrameOracle reduces the number of frames from 16 to 10.4 on average, how does uniform sampling of 10 frames from the original 16-frame sequence compare in terms of performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TKzpQHecDY", "forum": "yBAp76dQMf", "replyto": "yBAp76dQMf", "signatures": ["ICLR.cc/2026/Conference/Submission22308/Reviewer_otKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22308/Reviewer_otKT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093837705, "cdate": 1762093837705, "tmdate": 1762942161990, "mdate": 1762942161990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose FrameOracle, a lightweight, plug-and-play selector for video-VLMs that predicts both which frames are relevant to a given query and how many frames are actually needed, tackling the inefficiency of uniform or fixed-budget sampling. It’s trained via a four-stage curriculum that begins with weak proxy signals and culminates in supervised fine-tuning on a new dataset, FrameOracle-41K, which supplies keyframe annotations specifying the minimal sufficient frames per question. Across five VLMs and six benchmarks, FrameOracle cuts 16-frame inputs to ~10.4 with no accuracy loss and trims 64 candidates to ~13.9 while improving accuracy by ~1.4%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Performance Gains\n\nFrameOracle reduces frame usage while maintaining or improving accuracy across six diverse benchmarks and five different video-language models\n\n\n- Plug-and-Play Generalization without Co-Training\n\nUnlike most keyframe selection methods, FrameOracle operates independently of the base VLM, requiring no co-training or model-specific tuning — showing strong transferability and making it highly practical for real-world deployment\n\n- A Novel Dataset (FrameOracle-41K)\n\nThe paper contributes a large, purpose-built dataset with keyframe supervision, enabling both training for adaptive frame selection"}, "weaknesses": {"value": "- Marginal Gains at Larger Compute Budgets\n\nWhen starting from a large candidate pool (e.g., 64 frames), efficiency gains diminish, achieving only modest FLOP and latency reductions, which limits its benefit for already optimized pipelines\n\n- Performance on Fine-Grained Temporal Tasks\n\nFrameOracle underperforms heuristic methods like KFC on datasets such as MLVU, which require precise temporal reasoning and multi-event grounding."}, "questions": {"value": "Can you show more examples from the dataset itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5FpowMQEn6", "forum": "yBAp76dQMf", "replyto": "yBAp76dQMf", "signatures": ["ICLR.cc/2026/Conference/Submission22308/Reviewer_m9jf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22308/Reviewer_m9jf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182030030, "cdate": 1762182030030, "tmdate": 1762942161640, "mdate": 1762942161640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}