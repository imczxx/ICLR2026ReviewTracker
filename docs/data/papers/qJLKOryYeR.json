{"id": "qJLKOryYeR", "number": 19335, "cdate": 1758295429850, "mdate": 1759897044812, "content": {"title": "Off-Policy Token Clipped Supervised Fine-Tuning Yields a Robust Cold-Start", "abstract": "Supervised Fine-Tuning (SFT) is a critical step for adapting Large Language Models (LLMs) to specialized domains, often serving as a cold-start for subsequent reinforcement learning (RL). However, SFT's tendency to memorize a small set of expert data for a downstream task can impair generalization and lead to catastrophic forgetting of prior knowledge, undermining the promise of effective RL. In this paper, we demonstrate that this degradation primarily results from tokens in the expert data to which the base model assigns low probability. Specifically, we frame these as 'off-policy' tokens, as they represent a significant deviation from the model's current prior knowledge. Due to the nature of the log-likelihood objective, these off-policy tokens produce larger gradient magnitudes, destabilizing the training process. \nTo investigate this phenomenon, we adopt a well-established clipping strategy from reinforcement learning, which is widely used to manage off-policy data in an on-policy manner. Applying this strategy to SFT moderates the learning process by constraining gradient updates from off-policy tokens, creating a more on-policy-like training dynamic. Through extensive experiments on the agentic benchmarks ALFWorld and ScienceWorld, we discover that this clipped approach, compared to standard SFT, reduces forgetting on out-of-distribution tasks by 11.54\\% and boosts final RL performance by 6.70\\%. Furthermore, latent-space analysis validates our initial claim, showing that applying the off-policy token clipped strategy results in less model's internal representational drift than standard SFT and is thus key to preserving prior knowledge.", "tldr": "We introduce a novel supervised fine-tuning strategy inspired by trust region methods in RL, which enables a more stable, on-policy-like training dynamic and preserves pre-existing knowledge for subsequent RL.", "keywords": ["Large Language Models", "Supervised Fine-Tuning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e1f500102c896d91983fda6d9062289e3b2f470.pdf", "supplementary_material": "/attachment/a437b5ab312e07b6eaf81180931ad2139d16d436.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a primary cause of **catastrophic forgetting** during the Supervised Fine-Tuning (SFT) of Large Language Models (LLMs). The authors argue that SFT, often used as a \"cold-start\" for Reinforcement Learning (RL), is destabilized by what they term **\"off-policy\" tokens**—tokens within the expert data to which the base model assigns a very low probability. They demonstrate that the standard log-likelihood objective assigns disproportionately **large gradient magnitudes** to these tokens, causing significant representational drift and the erosion of pre-trained knowledge, particularly in the initial stages of training.\n\nTo address this instability, the paper proposes **Off-Policy Token-Clipped SFT (OPC-SFT)**. This method adapts the clipping mechanism from the Proximal Policy Optimization (PPO) algorithm, a trust region method used in RL. OPC-SFT moderates the learning process by constraining gradient updates from off-policy tokens. It achieves this by clipping the token-level probability ratio to a bounded interval. The primary contributions include identifying this \"off-policy\" token problem and proposing the OPC-SFT solution. Experiments on the ALFWorld and Science World agentic benchmarks show that this approach **reduces OOD forgetting by 11.54%** and **improves final RL performance by 6.70%**. This is supported by latent-space analysis, which shows that OPC-SFT results in less representational drift."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper is its original, theoretically grounded approach to addressing catastrophic forgetting during Supervised Fine-Tuning (SFT). The authors offer a well-supported diagnosis: low-probability “off-policy tokens” in expert data produce disproportionately large gradients, which destabilize the model’s internal representations. The evidence presented for this claim is convincing.\n\nThe solution, Off-Policy Token-Clipped SFT (**OPC-SFT**), is an adaptation of the PPO clipping mechanism from Reinforcement Learning (RL) to the supervised setting. While not particularly novel, the clipping immediately has practical significance, demonstrating a substantial average 11.54% reduction in out-of-distribution forgetting and a significant 6.70% increase in final RL agent performance across challenging agentic benchmarks, such as ALFWorld and Science World (for Qwen2.5 and Llama 3). \n\nFurthermore, the paper is commended for its high quality and clarity in technical validation. The methodology is rigorously tested against strong baselines (including DFT and NEFTune) across three different model backbones, ensuring the generality of the results. The inclusion of comprehensive diagnostic analyses—specifically the latent-space shift visualization via PCA—provides clear, mechanistic evidence that OPC-SFT successfully preserves the model's prior knowledge by limiting representational drift. The presentation of the research is exceptionally clear, logically progressing from problem identification (large gradient norms in early SFT) to the derived solution ($\\mathcal{L}_{OPC-SFT}$), making the core concepts and the derivation of the novel loss function highly accessible to readers."}, "weaknesses": {"value": "The paper's primary weakness lies in the justification and novelty of its core mechanism. The authors are transparent in citing concurrent work (Zhu et al., 2025) that also explores applying a PPO-style clipping mechanism to SFT. This significantly narrows the paper's novel contribution to its analysis—framing the problem as one of \"off-policy tokens\" causing catastrophic forgetting—rather than a novel algorithmic solution. Furthermore, the adaptation from the PPO objective to SFT involves a critical simplification that is not adequately justified: setting the advantage function $\\hat{A}=1$ (Eq. 8-9). This assumes every token in the expert data is equally and maximally \"good.\" This is a strong and likely incorrect assumption; it conflates mundane tokens (e.g., \"the,\" \"is\") with critical reasoning tokens and ignores any potential sub-optimality in the expert data. The paper would be much stronger if it explored or justified this choice, for example by ablating against alternative, non-uniform advantage weightings (e.g., weighting by token surprisal, or weighting down tokens the base model already knows well)."}, "questions": {"value": "The adaptation from the PPO objective to the proposed $\\mathcal{L}_{OPC-SFT}$ (Eq. 9) hinges on the critical simplification of setting the advantage $\\hat{A}=1$. This assumes all tokens in the expert data are equally and maximally optimal, which seems unlikely (e.g., it equates mundane tokens like \"the\" with critical, task-specific tokens). Could the authors please provide a more detailed justification for this choice? Furthermore, have you experimented with alternative, non-uniform advantage weightings? For example, weighting tokens by their surprisal (negative log-probability) under the base model, or down-weighting tokens the model already assigns high probability to. This would help clarify if this simple, uniform weighting is a robust choice or a missed opportunity for a more nuanced credit assignment.\n\nThe empirical evidence shows clear success on agentic tasks (ALFWorld, ScienceWorld), which the paper effectively argues are highly \"off-policy.\" However, the gains are admittedly \"modest\" on math tasks, which are shown to be more \"on-policy\" (Fig. 4). This suggests the method's utility might be a niche solution for specific domain adaptation rather than a general-purpose SFT improvement. To substantiate the paper's broader claims, could the authors provide any results or analysis on how OPC-SFT performs on standard, large-scale instruction-tuning benchmarks (e.g., a subset of OpenOrca or Alpaca)? This would help clarify if the method provides benefits when the fine-tuning data is more diverse and \"closer-to-policy.\"\n\nThe stability and performance of trust-region methods heavily depend on how the reference policy ($\\pi_{\\theta_{old}}$) is managed. The paper states this is \"periodically refreshed\" (Sec 3.3) but provides no details on this crucial hyperparameter. Could the authors please specify what update frequency was used for $\\pi_{\\theta_{old}}$ in the experiments (e.g., updated every N steps, every epoch)? More importantly, could you provide an ablation study or analysis showing how sensitive the model's performance is to this update frequency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1hk6K8hCzl", "forum": "qJLKOryYeR", "replyto": "qJLKOryYeR", "signatures": ["ICLR.cc/2026/Conference/Submission19335/Reviewer_N85X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19335/Reviewer_N85X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588419218, "cdate": 1761588419218, "tmdate": 1762931276249, "mdate": 1762931276249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work identifies a limitation in the standard SFT-before-RL training paradigm:  overfitting and catastrophic forgetting caused by large gradients from \"off-policy\" (low-probability) tokens in SFT data. \nTo mitigate this, the authors propose OPC-SFT, a token-level clipping mechanism inspired by the clipping operation in PPO-style RL objectives. \nExperiments on the ALFWorld and ScienceWorld benchmarks suggest that OPC-SFT reduces forgetting on out-of-distribution tasks and improves subsequent RL performance, compared to standard SFT and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Provides a clear empirical analysis of the identified problem, supported by relevant evidence.\n* Proposes a simple, practical method that is easy to implement.\n* Experimental evaluation spans diverse tasks and models, showing consistent improvements.\n* Intermediate diagnostic metrics (e.g., principal components, gradient norm distributions) yield additional insights, e.g., explain why agentic tasks are more affected than math/reasoning tasks."}, "weaknesses": {"value": "**Issue 1: Limited clarity on the rationale behind the proposed method.**  \n- There is a discrepency between the analysis and the proposed clipping mechanism.  \n  - The analysis defines *off-policy tokens* as those with small model probability $\\pi_{\\theta}$, which seems to align more with the DFT method that weights token loss by token probability.  \n  - The proposed OPC-SFT method instead clips tokens with a large probability ratio $\\pi_{\\theta} / \\pi_{\\text{old}} > 1 + \\epsilon$, i.e., tokens whose probability increased substantially after a few policy update steps.  \n- In Section 3.3, the explanation connecting OPC-SFT to PPO’s clipping mechanism is tenuous and potentially misleading. One major discrepency is that PPO’s rationale fundamentally relies on an on-policy sampling assumption and the policy gradient theorem, which are totally irrelevant for SFT.  \n- The final loss in Eq. (9) for OPC-SFT essentially applies $\\min\\\\{r(\\theta), 1 + \\epsilon\\\\}$, meaning *a token's gradient is clipped if and only if its probability ratio exceeds $1 + \\epsilon$*. That's all. An intuitive, accurate and straightforward explanation that could save pages of analysis in Section 3.2, 3.3 and Appendix A. I would suggest that the authors consider reframing and clearly articulating the actual rationale behind this choice.  \n- The argument in Lines 205–209 is unconvincing: a small denominator in Eq. (5) (which can be rewritten as $\\nabla_{\\theta} \\log \\pi_{\\theta}$) does not inherently imply a large gradient norm.\n\n\n\n**Issue 2: Insufficient analysis and discussion of critical hyperparameters.**  \n- The refresh period for $\\pi_{\\text{old}}$ is an important hyperparameter, yet it is barely discussed (only briefly mentioned in Line 245).  \n  - If the period is 1, clipping is never triggered, and the proposed OPC-SFT method reduces to standard SFT.  \n  - If the period is too large, clipping can occur for nearly all tokens, potentially halting learning.  \n- A detailed analysis or ablation study on the effect of this parameter, alongside $\\epsilon$, would significantly strengthen the paper."}, "questions": {"value": "- Line 74: How exactly are the 11.54% and 6.70% numbers calculated?\n- Figure 1 (c): What does “episode” refer to here? Should it be “epoch”?\n- Table 1: Why does the Qwen 1.5B model outperform the 7B model significantly? Similar question about Table 3.\n- References to Tab. B.1 (Line 761) and Tab. B.2 (Line 793) are unclear, maybe typos?\n- Line 1204: The rollout temperature for RL is set to 0.7, smaller than the commonly used value of 1. Is there a specific motivation for this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YLT8ydspDg", "forum": "qJLKOryYeR", "replyto": "qJLKOryYeR", "signatures": ["ICLR.cc/2026/Conference/Submission19335/Reviewer_VFg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19335/Reviewer_VFg9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968221094, "cdate": 1761968221094, "tmdate": 1762931275882, "mdate": 1762931275882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates catastrophic forgetting in LLM models, and particularly why SFT can lead to a higher degree of forgetting compared to RLFT. The work argues that this is mostly caused by \"off-policy\" token in SFT which leads to large magnitude gradients and therefore forgetting. To resolve this problem the authors propose to borrow the clipping machinery used by the PPO algorithm, and show improved memory retention and act as a better cold-start for the RL finetuning stage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a meaningful problem in the finetuning pipeline of LLMs, and provides a simple yet effective solution of importing clipping gradient strategy from PPO. The authors show empirically the efficacy of their approach. As far as I can tell, the proposed solution, relying on clipping is novel and interesting."}, "weaknesses": {"value": "While the paper is generally clear and easy to read there are a few issues around contextualizing the work that I would like to point out. \nFirst I'm not convinced by the terminology used, namely that of \"off-policy tokens\" jointly with supervised learning. I understand that you can view SFT as imitation learning, I just find the abuse of language excessive, and potentially leading to confusions later. In my opinion I would call either both stages RL finetuning stages, just that the first one is off-policy imitation learning one. Or call the first stage supervised finetuning, but then use the language of probabilistic modelling to describe what you are doing, not that of RL, which is sufficiently expressive. This is also potentially the paper to be more explicit or formal about the proposed method. \n\nThe other weakness that I find in the text is that it does not connect at all with continual learning, a subfield that has studied catastrophic forgetting extensively, bot the causes of its emergence in neural networks as well as providing several potential solutions. This disconnect to a large body of work seems concerning. I would suggest at least citing some survey (they are several) in this space and acknowledging the existence of the field. \n\nGoing into the specifics. The paper claims that large magnitude gradients resulting from low probability tokens during the SFT finetuning are the cause of forgetting. While I do not completely disagree with the hypothesis, I feel like further evidence could be useful. Is the claim that from an optimization point of view this \"erratic\" gradients (line 208) are making optimization unstable (e.g. we are in a high curvature region). And that is why they are disruptive? \nTo reframe my question maybe. I do not think the norm of the gradient is the problem (or at least the cause). The actual problem is that when learning something new, not having access to the previous task that we are attempting not to forget, the best course of action is to be conservative and ask to stay as close as possible to the previous policy (under some metric of choice). Which is exactly what PPO does as well (stay close to pi_old). So is not the dynamics of the learning process (i.e. the magnitude of the gradient) that is the problem, is that we are balancing how much we want to learn the new task vs attempt to preserve knowledge by being very conservative and saying do not change anything. \n\nI'm bringing this up because the dynamics and justifications in PPO are a bit different, and the clipping (or more formally the KL term that the clipping is meant to approximate) is a term to induce stability in training and make sure PPO does not diverge. However SFT does not exhibit any kind of divergence behavior (even if gradients are large), therefore I believe the motivation has to be different. I find the current framing to rely on this idea of unstable learning in SFT which I believe to be misleading. \n\nFollowing of this, I feel like equation (6) is not sufficiently justified. I know the authors point to concurrent work, but by naming the work concurrent it means that the formulation is novel as with respect to this paper, and therefore it should be justified (rather than rely on the other paper as justification). I feel this is where the SFT nomenclature becomes fuzzy. How does equation 6 relate to equation 1. How can we understand this from the MLE point of view? Should we still call this SFT then? Or maybe if not equation 6 then equation 9. Would it had been better to derive this from a constraint optimization perspective, similar to PPO via a KL term on the difference between the old and new distribution?  Also the constraint (trust region view) would then directly connect with many CL algorithms (at least regularization based ones), which take the form of a constraint optimization that minimizes the loss on the current task under a KL constraint with respect to the old task, with the difference from many of these works that here the KL constraint is being approximate via a clipping strategy akin to PPO. \n\nOf course the authors do not need to fully agree with my perspective, but I find the derivation in section 3.3 very hand-wavy, relying on similarities rather than a more formal derivation. Which can lead to question of what is the objective in (9) actually minimizes ? What will it converge to? etc. \n\nEmpirical results look good, but I was wondering if it would make sense to have a baseline that involves replay during the SFT phase? Or argument against it in the text."}, "questions": {"value": "1. Can you justify the semantics of objective in equation 9 as a supervised objective ? What would be the correct way of interpreting it?\n2. Can you provide some more insights of whether is the gradient magnitude that is problematic or something different? Why would then approaches like imposing gradient clipping (used for e.g. often when you dealing with exploding gradients), or decreasing learning rate be a solution? \n3. Can you provide any vanilla standard continual learning baseline for the experiments (e.g. replay as the most simple one to implement)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not see reasons for an ethics review."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mPPJsXB6kv", "forum": "qJLKOryYeR", "replyto": "qJLKOryYeR", "signatures": ["ICLR.cc/2026/Conference/Submission19335/Reviewer_tn6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19335/Reviewer_tn6h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088858202, "cdate": 1762088858202, "tmdate": 1762931275255, "mdate": 1762931275255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the effect of Supervised Finetuning (SFT) procedure in memorization (overfitting) and catastrophic forgetting of the pretraining set. The hypothesis of the authors is that supervised training is dominated by a few tokens that are \"off-policy\" (i.e that had low likelihood under the current generative model). Fitting these tokens impact most the forgetting, typically during the first epoch.\n\nThis probes author to reformulate supervised learning as a special kind of reinforcement learning (in the REINFORCE/PPO-style) with constant advantage A=1. This allows to re-use the clipping trick of PPO on the likelihood ratio between the current policy $\\theta$ and $\\theta_{old}$. \n\nMultiples experiments are conducted on benchmarks like \u0000ALFworld, LiveCodeBench, or MATH500. Other OOD experiments and ablations can be found in appendix"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strength of the method is clearly its simplicity. This is a straightforward modification of existing SFT pipelines. \n\nThe numerical results are strong compared to the SFT baseline and the more complex competitor approaches. Three strong backbones of small size (Qwen2.5-7B, Qwen2.5-1.5B and Llama3.2-3B) are finetuned. Their limited capacity make them susceptible to forgetting.   \n\nThe scientific methodology brings strong evidence of usefulness (e.g Fig. 2, Fig. 3). \n\nOverall, simplicity and extensiveness of experiments, make a strong case for this paper's acceptance."}, "weaknesses": {"value": "### Source of the idea\n\nThe idea of reformulating supervised learning as a form of RL has been done in the past. At the end, PPO is just a pretext and a possible interpretation of simply clipping a likelihood ratio. I think it would be better to present it as a form of importance sampling with clipping.\nMore context on this would be welcome, such as the discussion of the competitor's work [which is acknowledged by authors]\n\nWenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, and Pengfei Liu. Proximal super-\nvised fine-tuning. CoRR, abs/2508.17784, 2025.\n\n### Clipping ratio\n\nCan you report the percentage of clipped ratios as function of time (and epsilon)? I expect this probability to increase until it reaches a plateau (possibly 100%).   It is important to measure it, as every clipped gradient stops bringing signal (wasted token). This is an important measure of the efficiency of the method.  That might have consequences in the data-scarce regime: for very small sets that are OOD, I'm scared that this would stop training completely."}, "questions": {"value": "### Reference policy\n\nWould there be a reason to use anything else than $\\pi_0$ as a reference policy $\\pi_{\\theta}$: e.g, updating it after the end of the first epoch. I'm worried that, by training for too long, the current iterate $\\pi$ would become too far from $\\pi_{\\theta}$, which would trigger clipping repeatedly until no gradient is flowing. \n\n### Baseline\n\nAnother baselines comes to mind, related to token probabilities: KL divergence used as regularization between $\\pi_{\\theta}$ and $\\pi_0$. This is regularization approach, whereas PPO can be more understood as a form of constraint like trust region methods. What do you think about this alternative?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u3LO2YwGop", "forum": "qJLKOryYeR", "replyto": "qJLKOryYeR", "signatures": ["ICLR.cc/2026/Conference/Submission19335/Reviewer_GBro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19335/Reviewer_GBro"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762394899170, "cdate": 1762394899170, "tmdate": 1762931274820, "mdate": 1762931274820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}