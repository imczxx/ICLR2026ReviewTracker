{"id": "BzNf90Csfa", "number": 2120, "cdate": 1756992498456, "mdate": 1763722142365, "content": {"title": "Curation Leaks: Membership Inference Attacks against Data Curation for Machine Learning", "abstract": "In machine learning, data curation is used to select the most valuable data for\nimproving both model accuracy and computational efficiency. Recently, curation\nhas also been explored as a solution for private machine learning: rather than\ntraining directly on sensitive data, which is known to leak information through\nmodel predictions, the private data is used only to guide the selection of useful\npublic data. The resulting model is then trained solely on curated public data.\nIt is tempting to assume that such a model is privacy-preserving because it has\nnever seen the private data. Yet, we show that without further protection curation\npipelines can still leak private information. Specifically, we introduce novel attacks\nagainst popular curation methods, targeting every major step: the computation of\ncuration scores, the selection of the curated subset, and the final trained model.\nWe demonstrate that each stage reveals information about the private dataset,\nand that even models trained exclusively on curated public data leak membership\ninformation about the private data that guided curation. These findings highlight the\ninherent privacy risks in data curation that were previously overlooked, and suggest\nthat (1) in the context of curation, privacy analysis must extend beyond the training\nprocedure to include the data selection process, and (2) true privacy-preserving\ncuration will require new methods with formal privacy guarantees.", "tldr": "", "keywords": ["machine learning", "privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8130e0457d702c6d599fc09bbc0c7babc9d38060.pdf", "supplementary_material": "/attachment/1db4dd8e08458acc2effd903f1049c1dbcc8c032.zip"}, "replies": [{"content": {"summary": {"value": "This paper uncovers a novel and important privacy vulnerability in machine learning pipelines: membership leakage during data curation. The authors demonstrate both theoretically and empirically that data curation, even before model training, can expose sensitive information about which samples were part of the raw dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a new angle on privacy leakage. Most membership inference works focus on model outputs; This work focus shifts to data preprocessing and curation.\n2. The problem is well motivated and clearly defined.\n3. The empirical results are convincing and well-presented."}, "weaknesses": {"value": "1. While the proposed curation leak is novel and conceptually important, its real-world applicability is limited by the assumption that the adversary can access or infer the curation outcome. In many production pipelines, the raw uncurated data or discarded samples are not observable.\n2. The paper briefly mentions DP sampling and randomization but does not propose new defense mechanisms.\n3. Ablation studies (e.g., effect of dataset size or embedding dimension) are not included."}, "questions": {"value": "1.Clarify the assumption\n2.Add necessary ablation studies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UPmf4hesud", "forum": "BzNf90Csfa", "replyto": "BzNf90Csfa", "signatures": ["ICLR.cc/2026/Conference/Submission2120/Reviewer_hymw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2120/Reviewer_hymw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695688223, "cdate": 1761695688223, "tmdate": 1762916038610, "mdate": 1762916038610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that “privacy via curation” (only using sensitive data to select public data and then training solely on the public subset) is not automatically private. They design membership inference attacks against: (i) curation scores; (ii) the selected public subset; and (iii) the final model (via a small number of crafted public “fingerprint” samples).The results show that image-embedding/nearest-neighbor style curation is much more vulnerable; TRAK-style gradient-averaged curation is more robust but still leaky for small target sets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem itself seems interesting."}, "weaknesses": {"value": "1. I’m familiar with LiRA and recognize you use an online variant, but the three attack surfaces are hard to follow because the threat model isn’t explicitly stated up front. Please spell out—on one page—(i) the adversary’s goal (membership in the private target set used for curation vs. classic training-set membership), (ii) what the adversary can observe at each stage (scores, selection mask, final model), and (iii) what the adversary can do (e.g., can they inject public items?). A single “who-sees-what” diagram for the 3 stages would save readers a lot of guesswork. Also, there are many typos in the main text and appendix; a thorough copy edit would help.\n\n2. The paper does not convincingly explain why leakage at the scoring and subset-selection stages matters when, in many realistic deployments, only the final model is exposed. In such cases, standard membership inference on the trained model already demonstrates risk. The authors should clarify concrete scenarios (and access assumptions) where Sections 3.1 and 3.2 introduce new or additional threat surface beyond what model-only exposure already entails.\n\n3. The end-to-end attack hinges on the ability to insert crafted samples into the public pool. This reads as logically circular: relying on an existing injection threat to establish a new leakage threat. The paper should justify the realism and scope of this assumption (e.g., where such insertion is feasible, at what rates, and under what defenses or deduplication) and disentangle the novelty of the proposed leakage from the prerequisite capability.\n\n4. The underlying question is important: if a model is trained on curated public data, where the curation used a private target set, does the deployed model leak membership about that private set? However, the current presentation makes the answer hard to find. A concise figure (or two) that lays out the threat model for all three stages—attacker view, defender assets, and leakage channel—would greatly improve readability. With a clearly defined problem and threat model, the empirical results would be far more persuasive; without that foundation, even sophisticated methods risk failing to convince readers."}, "questions": {"value": "Please address the “Weaknesses” above in your rebuttal—especially by (1) stating the problem crisply, (2) explaining why it matters in realistic deployments, and (3) laying out clear threat models for all three attack stages. If those pieces are clarified and make sense, I’m inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FMrZNvYmdT", "forum": "BzNf90Csfa", "replyto": "BzNf90Csfa", "signatures": ["ICLR.cc/2026/Conference/Submission2120/Reviewer_zhSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2120/Reviewer_zhSF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916300559, "cdate": 1761916300559, "tmdate": 1762916038379, "mdate": 1762916038379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate the privacy risks of data curation and show effective membership inference attacks on all the major steps of data curation. This includes similarity score evaluation of the data points in the public dataset (which is the easiest), and the construction of the curated public dataset (which is harder), and the training of the ML model on the curated dataset. The paper builds upon popular and well-known membership inference attacks and provided versions that works regardless of the data curation method used. Furthormore, custom attack methods are designed for image similarity score-based curation and TRAK-based curation. Experiments show that while data curation does not train any model on the private dataset, but all its major steps still leak the membership of the private domain dataset, and it is worse for smaller domain dataset. This reveals new privacy risks of data curation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper reveals new privacy risks of data curation via membership inference attacks.\n2. Systematic review of MIAs in all steps in the data curation pipeline, and attacks are proposed and validated for all of them.\n3. Existing MIAs work for agnostic with modifications, but the paper also proposes custom attacks are proposed to target concrete data curation methods, including TRAK and image-based data curation."}, "weaknesses": {"value": "1. Choice of parameters are not clear or discussed in the main text.\n2. Computational complexity of the attacks is not discussed.\n3. Limited experiments for end-to-end model MIAs.\n\nPlease see the questions listed in the section below."}, "questions": {"value": "1. Some important parameter and implementation details are not explained. For example, how many shadow models $m$ are used in the attacks? What are the parameters in the data curation procedure and what is the setting of the model training procedure? When training the model, is regularization used? If so, how heavy is that?\n2. The paper does not discuss the effects of $m$ on the attack performance. I would like to see the relation between the compute spent by the attacker and the attack effectiveness.\n3. For end-to-end model MIAs, all results are when the selected target subset is 1% of the target set. I understand that model training is time consuming, but I would like to see the attack performance for more TPRs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YVmvJLeSXU", "forum": "BzNf90Csfa", "replyto": "BzNf90Csfa", "signatures": ["ICLR.cc/2026/Conference/Submission2120/Reviewer_AAT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2120/Reviewer_AAT3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974421776, "cdate": 1761974421776, "tmdate": 1762916038126, "mdate": 1762916038126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first comprehensive privacy analysis of data curation pipelines in machine learning. The authors challenge the common assumption that training models on curated public data inherently protects private target datasets used for curation. Through systematic membership inference attacks across three pipeline stages (curation scores, selected subsets, and final models), the paper demonstrates that each stage leaks information about the private target set."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This is the first systematic analysis of privacy leakage in data curation pipelines, addressing a genuine blind spot in the community. it systematically evaluates privacy leakage across three critical stages of the curation pipeline (curation scores, selected data subsets, and the final trained models) using diverse datasets and curation methods.\n\n- The analysis of influence sparsity in image-based curation provides valuable insights into why certain methods are more vulnerable.\n\n- The paper delivers actionable insights: simple max-based curation is fundamentally insecure, while average-based curation (like TRAK) provides innate robustness except in the small-dataset regime ."}, "weaknesses": {"value": "- The threat model is not clearly defined. The adversary's assumed capabilities and knowledge are not clearly stated upfront and appear to change depending on the attack. The adversary knowledge could be escalates to extreme, white-box levels. For the end-to-end TRAK attack, the adversary is assumed to have profound knowledge of the curation mechanism, including the model architecture, the ability to compute gradients, and the need to calculate the Gram matrix $G$. This level of white-box access is not clearly stated or justified.\n\n- The paper's core attack methodology is its adaptation of LiRA. This adaptation is (a) confusingly explained and (b) methodologically questionable. Running an attack algorithm on subsets of the private data to build in/out distributions  is not standard and is not justified as a sound method for simulating the true 'in' and 'out' worlds.\n\n- The paper's strongest end-to-end attack is weakened by its reliance on a proxy metric. The authors state the full end-to-end attack is 'computationally intractable'. Their solution is to 'assume that the adversary can measure whether $f \\in \\tilde{D}$.' This assumption effectively equates selection with detection, completely ignoring the possibility that ML training dynamics (e.g., SGD noise, and the influence of thousands of other samples) could 'drown out' the fingerprint’s signal, making it undetectable in the final model even if it had been selected."}, "questions": {"value": "- Can you please provide a single, clear definition of the threat model?\n\n- Can you please provide a more detailed justification for your 'shadow' methodology? Specifically, why is re-running the curation algorithm on random subsets of the private data $\\mathcal{T}$  a sound way to construct the in/out distributions for a LiRA attack? This seems to be a custom algorithm that borrows the LiRA name, and its statistical validity is not obvious.\n\n- Can you provide any evidence that this assumption of selection equals detection holds? For example, can you show that a fingerprint sample, when selected, always creates a detectable signal in the final model, even when trained as part of a large curated set?\n\n- Could you please analyze the query budget/cost or computational cost of the attack? How would it affect the scalability of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ERgO81lQ51", "forum": "BzNf90Csfa", "replyto": "BzNf90Csfa", "signatures": ["ICLR.cc/2026/Conference/Submission2120/Reviewer_Ju1r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2120/Reviewer_Ju1r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183949351, "cdate": 1762183949351, "tmdate": 1762916037918, "mdate": 1762916037918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We Thank the Reviewers for the Feedback, Summary of Changes"}, "comment": {"value": "We are grateful to all reviewers for their constructive feedback, which greatly helped us  improve our submission. We thank the reviewers for noting our work as “addressing a genuine blind spot in the community” (Reviewer Ju1r), “a new angle on privacy leakage” (Reviewer hymw) and revealing “new privacy risks of data curation” (Reviewer AAT3). We are further happy that they recognize our work as “well motivated and clearly defined” (Reviewer hymw) and “interesting” (Reviewer zhSF) and our empirical evaluation as “convincing and well-presented” (Reviewer hymw) and delivering “valuable” and \"actionable insights” (Reviewer Ju1r). We hope that our work can contribute to the community and inform about a “novel and conceptually important” (Reviewer hymw) aspect of privacy leakage.\n\nBelow, we present additions to the paper and highlights of the rebuttal that we believe are valuable to share with all reviewers. Following this, we will provide individual responses addressing each Reviewer's comments in detail. A revised version of the paper is uploaded at the same time.\n\n- **We clarified the threat model** and motivated it further based on real examples of released curation scores, released curated datasets, curation as a service, and data access challenges. We added these aspects to the introduction and include a new Section 3.1 dedicated to clarifying the adversary goal and capabilities, and how they vary for attacks on each curation stage.\n\n- **We added a new attack for TRAK scores** based on formulating the membership inference problem as a least squares regression. We show that an adversary can recover membership information for larger target datasets (\\~10,000 samples) than shown in the initial submission.\n\n- **We show the robustness of our end-to-end leakage findings** by including additional experiments on the detectability of fingerprints in the training data. We detail how those measurements can be obtained and ablate over the training dataset sizes, showing that the inclusion of fingerprints leads to measurable signals. This holds for both types of fingerprints we employed, i.e., images with *wrong* captions (e.g., an image of an airplane with the caption “*ratatouille*”) in Image-Based curation and images with benign captions with *added wrong information* (e.g., an image of an airplane with the caption “an image of an airplane *and ratatouille*”) for TRAK. \n\n- **We extend the paper to include defenses and leverage and evaluate mitigation strategies based on Differential Privacy.** We evaluate attack performance for various strengths of privacy guarantees. We find that these drastically reduce the attack efficacy, supporting the conclusion of our paper, that curation should incorporate protective measures.\n\n- **We add additional ablations**. We show that both for attacking Image- and TRAK-Based curation, there is a sweet spot where adding or removing dimensions reduces attack success. We show that for LiRA-style attacks, adding more shadow models helps to varying degrees for the different datasets. We ablate over target dataset sizes, showing that the nearest neighbor mechanism of image-based curation consistently exposes samples across all sizes. TRAK-based curation, with its’ averaging, exposes *all* samples at small sizes, but the averaging equally makes it hard to attack *any* sample for large dataset sizes."}}, "id": "pU26WeFA4O", "forum": "BzNf90Csfa", "replyto": "BzNf90Csfa", "signatures": ["ICLR.cc/2026/Conference/Submission2120/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2120/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission2120/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724149757, "cdate": 1763724149757, "tmdate": 1763724149757, "mdate": 1763724149757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}