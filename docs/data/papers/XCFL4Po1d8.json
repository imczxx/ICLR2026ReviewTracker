{"id": "XCFL4Po1d8", "number": 18375, "cdate": 1758286960786, "mdate": 1763617759417, "content": {"title": "Best-of-three-worlds Analysis for Dueling Bandits with Borda Winner", "abstract": "The dueling bandits (DB) problem addresses online learning from relative preferences, where the learner queries pairs of arms and receives binary win-loss feedback. Most existing work focuses on designing algorithms for specific stochastic or adversarial environments. Recently, a unified algorithm has been proposed that achieves convergence across all settings. However, this approach relies on the existence of a Condorcet winner, which is often not achievable, particularly when the preference matrix changes in the adversarial setting. Aiming for a more general Borda winner objective, there currently exists no unified framework that simultaneously achieves optimal regret across these environments.\nIn this paper, we explore how the follow-the-regularized-leader (FTRL) algorithm can be employed to achieve this objective. We propose a hybrid negative entropy regularizer and demonstrate that it enables us to achieve $\\tilde{O}(K^{1/3} T^{2/3})$ regret in the adversarial setting, ${O}({K \\log^2 T}/{\\Delta_{\\min}^2})$ regret in the stochastic setting, and $O({K \\log^2 T }/{\\Delta_{\\min}^2} + ({C^2 K \\log^2 T }/{\\Delta_{\\min}^2})^{1/3})$ regret in the corrupted setting, where $K$ is the arm set size, $T$ is the horizon, $\\Delta_{\\min}$ is the minimum gap between the optimal and sub-optimal arms, and $C$ is the corruption level. These results align with the state-of-the-art in individual settings, while eliminating the need to assume a specific environment type. We also present experimental results demonstrating the advantages of our algorithm over baseline methods across different environments.", "tldr": "", "keywords": ["dueling bandits; borda winner; best of three worlds; FTRL"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9237dfe04eee479f91f284dc719b2ed64f0311cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work establishes the first best-of-three-worlds algorithm for dueling bandits under the Borda winner objective, developing a single FTRL-based algorithm which achieves order-optimal regret in both stochastic and adversarial settings, as well as in settings with a corruption budget. Previously, such a result was shown for dueling bandits with Condorcet winner and regret notion, but the regret scales differently in such a setting and so the Borda result requires new innovations, namely a hybrid negative entropy regularizer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper shows the first best-of-three-worlds regret bound for Borda dueling bandits.\n* The writing is generally clear and presentation is easy to follow.\n* There are also experiments to validate theoretical results, i.e. superiority of their FTRL algorithm over other approaches for the various settings."}, "weaknesses": {"value": "I think there could be more discussion of related works to better highlight the technical novelty of this work. In particular, there are other works [1]-[3] on getting best-of-both or best-of-three-worlds results in settings where the regret scales like $T^{2/3}$ that should be definitely be mentioned. My understanding is that the hybrid regularizer used in this work is required to adapt the self-bounding analysis for this type of regret scaling rather than the $\\sqrt{T}$ rate seen in Condorcet dueling bandits. My main concern is whether this Borda dueling bandit problem can in fact be instantiated as a partial monitoring problem and inherit the results of [2]. For instance, it is known that Condorcet dueling bandits can be viewed as a partial monitoring problem in some instances (see mentions of dueling bandits in \"Information Directed Sampling for Linear Partial Monitoring\" Kirschner et al., 2020) and also in some generalized versions of the Borda problem (see Remark 3 in the cited paper Suk and Agarwal., 2024), or whether the adaptations required in the FTRL self-bounding analysis are already implicit in the works [1]-[3].\n\n[1] Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs, Ito et al., NeurIPS 2022.\n\n[2] Best-of-Both-Worlds Algorithms for Partial Monitoring. Tsuchiya et al., ALT 2023.\n\n[3] Simultaneously Learning Stochastic and Adversarial Bandits with General Graph Feedback. Kong et al., ICML 2022."}, "questions": {"value": "Please see my main question above in \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o0IZ67fSis", "forum": "XCFL4Po1d8", "replyto": "XCFL4Po1d8", "signatures": ["ICLR.cc/2026/Conference/Submission18375/Reviewer_zTzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18375/Reviewer_zTzf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952397328, "cdate": 1761952397328, "tmdate": 1762928082891, "mdate": 1762928082891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a relaxed comparator for setting up the regret where a uniformly winning arm is not achievable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The relaxed regret benchmark makes sense and the regret bounds derived therein is meaningful.\n- The best-of-both-worlds results are of relevance when the underlying environment is uncertain."}, "weaknesses": {"value": "- While it makes sense to assume the environment is static and is either stochastic, corrupted, or adversarial, I wonder if it makes more sense to assume that the environment may switch among these three regimes as the number of rounds increases. This is for example considered in Cortes, C., DeSalvo, G., Gentile, C., Mohri, M., & Yang, S. (2018, July). Online learning with abstention. In International conference on machine learning (pp. 1059-1067). PMLR.\n- One of the motivation to consider dueling bandits is RLHF. I wonder how would the borda winner setting be better elaborated in such settings? And how are computational efficiency be guaranteed beyond being a convex optimization problem, that is, is there any scalable method to use for solving FTRL problems in huge scale?"}, "questions": {"value": "See the previous section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JMqZn9uBGF", "forum": "XCFL4Po1d8", "replyto": "XCFL4Po1d8", "signatures": ["ICLR.cc/2026/Conference/Submission18375/Reviewer_57Uf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18375/Reviewer_57Uf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997814944, "cdate": 1761997814944, "tmdate": 1762928082548, "mdate": 1762928082548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the \"best-of-three-worlds\" problem in dueling bandits, where the goal is to attain regret bounds in adversarial, stochastic and corrupted problem simultaneously. In particular, they show such guarantees in terms of the Borda regret, which is the cumulative difference in Borda score between the played arms and hte Borda-optimal pair of arms. The proposed algorithm for this task estimates the pairwise probabilities using so-called importance weighted estimators, and then runs a regularized-follow-the-leader (RFTL) algorithm using these estimators. The output of RFTL is then mixed with a uniform distribution to get the policy for a given round."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To my knowledge, the best-of-three-worlds with Borda score is a new contribution.\n- The regret bounds for the adversarial and stochastic settings match the optimal regret in those settings up to log factors. (Per the lower bounds in Saha et al., 2021).\n- The paper gave a clear explanation of how the analysis of best-of-three worlds with Cordocet regret does not transfer to Borda regret, and therefore they require a new algorithm/analysis approach."}, "weaknesses": {"value": "- Although the best-of-three worlds guarantees are new for this setting, the actual individual guarantees do not necessarily improve on those that have been attained previously. In particular, the guarantees on the corrupted setting are $1/\\Delta^2$, while prior work (Agarwal et al, 2021) showed $1/\\Delta$ dependence. This renders the claim on line 99 incorrect, that the guarantees match the state-of-the-art for the corresponding setting.\n- I found the presentation to somewhat confusing at places, which I have detailed in the Questions box."}, "questions": {"value": "- Should the $R_T$ in (1) be $R_T^b$? The environment definitions that follow use $w(i)$, which makes it seem that it is in reference to the shifted regret $R_T^b$.\n- In line 203, it appears that the expectation is over the $w$, but the expectation is written as $E_{s \\sim D}$. I found this to be confusing, and would suggest that the authors modify for clarity.\n- Equation (2) uses $u_K$, but it hasn't been defined yet.\n- In line 242, it says that $x_t$ and $y_t$ are sampled from $d_t$, but the Algorithm 1 shows them being sampled from $\\pi_t$.\n- I would suggest adding an intuitive explanation for the algorithm step sizes in (5)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fZEd97p5zF", "forum": "XCFL4Po1d8", "replyto": "XCFL4Po1d8", "signatures": ["ICLR.cc/2026/Conference/Submission18375/Reviewer_bvPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18375/Reviewer_bvPj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762550843202, "cdate": 1762550843202, "tmdate": 1762928082206, "mdate": 1762928082206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}