{"id": "fVJEWdwvLO", "number": 4905, "cdate": 1757795971250, "mdate": 1759898005874, "content": {"title": "Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning", "abstract": "Prototypical self-supervised learning methods consistently suffer from partial prototype collapse, where multiple prototypes converge to nearly identical representations. This undermines their central purpose—providing diverse and informative targets to guide encoders toward rich representations—and has led practitioners to over-parameterize prototype sets or add ad-hoc regularizers, which mitigate symptoms rather than address the root cause. We empirically trace the collapse to the joint optimization of encoders and prototypes, which encourages a type of shortcut learning: early in training prototypes drift toward redundant representations that minimize loss without necessarily enhancing representation diversity. To break the joint optimization, we introduce a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Concretely, we model prototypes as a Gaussian mixture updated with an online EM-style procedure, independent of the encoder's loss. This simple yet principled decoupling eliminates prototype collapse without explicit regularization and yields consistently diverse prototypes and stronger downstream performance.", "tldr": "We trace partial prototype collapse in prototypical SSL to joint encoder–prototype optimization and propose an EM-based decoupling strategy that prevents collapse, preserving prototype diversity and improving downstream performance.", "keywords": ["Self-Supervised Learning", "Representation Learning", "Computer Vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a32625a35cebf6d0f1b1ea9314f06b5c444c4aac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies partial prototype collapse in prototypical self-supervised learning (SSL), where many prototypes become almost the same. The authors show that this happens because encoders and prototypes are trained together under the same loss, which encourages shortcut learning. To fix this, they propose a fully decoupled training strategy: prototypes are learned as Gaussian mixture components using an online EM update, separated from encoder optimization. The method removes prototype collapse and improves prototype diversity, k-NN accuracy, and robustness to long-tailed datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explains well why prototype collapse matters and gives strong evidence that this issue is general across many SSL methods, not only DINO.\n- The proposed decoupling is easy to understand, no need new hyperparameters, and is supported by strong experiments.\n- The authors test several popular baselines (DINO, CARP, iBOT, CAPI) and evaluate both balanced and long-tailed datasets. The analysis of prototype uniqueness and training dynamics is very complete.\n- The paper is well structured and the figures help a lot to understand the argument."}, "weaknesses": {"value": "The reviewer is happy with the paper overall. Here is a few area that could be improved:\n- The paper mostly gives empirical evidence, but not a full theoretical explanation of why joint optimization causes collapse. IMO, a simple gradient or convergence analysis would make the argument stronger.\n- The improvements in accuracy are clear but not very large (around +1–2%). The contribution is more diagnostic than algorithmic. \n- The method is mainly tested on CARP. It would be good to see results on other recent large-scale SSL frameworks."}, "questions": {"value": "This paper reminds me of non-parametric or memory-based approaches in contrastive learning—such as MoCo, which keeps a memory queue of past embeddings, or OBoW and SwAV, which use fixed or slowly updated codebooks as pseudo-prototypes.\n\nCould the authors clarify whether their decoupled update is conceptually related to these methods?\nAnd is the proposed online EM procedure similar to maintaining a dynamic memory bank of cluster centers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7LQbYK6hZW", "forum": "fVJEWdwvLO", "replyto": "fVJEWdwvLO", "signatures": ["ICLR.cc/2026/Conference/Submission4905/Reviewer_k3fq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4905/Reviewer_k3fq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749342410, "cdate": 1761749342410, "tmdate": 1762917753280, "mdate": 1762917753280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article is on the prototype collapse problem for prototypical self-supervised learning algorithms. In such algorithms, representations are organized around certain prototype points, where the prototype points are also trained. A major issue is that during training, these prototypes can merge or collapse into a smaller set. The article offers an empirically evidenced hypothesis for the cause of this problem as the joint estimation of the encoder parameters and prototype points based on the same loss. Then it proposes a solution where different loss functions are defined for the encoder and prototype points, with the prototype loss based on Gaussian mixture models. The article presents various numerical experiments to support its analytical hypothesis and the proposed solution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The article offers a practically valuable observation about the potential cause of the prototype collapse problem in relation to the joint optimization of encoder parameters and prototype points.\n\n- It also provides a loss separation approach (for encoder and prototype points) that is demonstrated to be successful in avoiding the collapse problem.\n\n- A clear measure of collapse is also introduced in the article."}, "weaknesses": {"value": "The main weakness of the article is that it is entirely based on an empirical approach, lacking a satisfactory analytical foundation for both the cause and the proposed solution of the prototype collapse problem. Although the article is useful in terms of practical implementations, a stronger theoretical grounding is needed to make its arguments more convincing and provide deeper insights for a potential ICLR submission."}, "questions": {"value": "- Line 96: Provide the full form for EMA-updated (Exponential Moving Average).\n\n- Line 99: Not a proper matrix multiplication (is the transpose of C necessary?).\n\n- Definition 2.1: Is there a constraint $\\|v_m\\|=1$?\n\n- Is this a logical necessity: “If partial prototype collapse truly stems from the joint optimization of prototypes and encoders, then separating their updates should prevent the collapse”?\n\n- Is it possible to provide some analytical guarantee that the use of separate losses avoids prototype collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uu3jXq1y5n", "forum": "fVJEWdwvLO", "replyto": "fVJEWdwvLO", "signatures": ["ICLR.cc/2026/Conference/Submission4905/Reviewer_qYSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4905/Reviewer_qYSn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765885164, "cdate": 1761765885164, "tmdate": 1762917752715, "mdate": 1762917752715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Prototypical self-supervised representation learning (SSL) methods such as DINO suffer from a partial prototype collapse issue, which heavily limits the diversity of the learned prototypes. This paper argues that this issue is caused by the joint optimization of the encoders and the prototypes. Inspired by partially decoupled training in CAPI [1], this paper proposes a fully decoupled training method. Through empirical experiments on the CARP [2] SSL method, the paper shows that this eliminates prototype collapse. By using diverse prototypes, they claim to learn improved representations, as reflected in improved downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Prototypical SSL methods such as DINO have grown to become widely used and better understanding the partial prototype collapse issue sheds light on the inner workings of these methods and provides insights on how to further improve them.\n- The development of the fully decoupled training methodology by using an online Gaussian Mixture Model to update the prototypes is interesting. This is a valuable contribution as this involves specific choice of techniques such as responsibility based forgetting to avoid collapse and other techniques to further improve performance.\n- Decoupled training of the encoders and the clustering model/prototypes offers an interesting pathway to simplify and improve prototypical SSL pre-training like DINO."}, "weaknesses": {"value": "**Major:**\n- The partial prototype collapse occurs in many prototypical learning methods. The decoupled training is presented as a more general contribution, but the paper only experiments with CARP. So, it is unclear if the proposed method is indeed general.\n- In Table 3, there is a direct comparison between CARP and CARP+Decoupling only for the Resnet-50 model. Can the authors produce similar results for the other backbones (ViT-S, ViT-B)? Otherwise, it is hard to identify the impact of the proposed decoupled training method.\n- One prior approach to prevent partial prototype collapse is the KoLeo-Proto (KP) regularization [3]. But the paper does not directly compare against this. For example, this can be clarified in Table 2 by adding the results for DINO+Decoupling and CARP+KP. That can show both the generality of the decoupling approach and comparison with KP regularization, at least in the long-tail context.\n- Since the paper mainly experiments with the CARP method, it is also worth comparing CARP+Decoupling with CARP+KP in some of the ImageNet experiments in Table 3.\n- The best possible linear probing accuracy achievable using the learned representations is a useful indicator of the representation quality. The hyperparameter choice is indeed important but running this is trivial and not expensive.  See section B.3 in DINOv2 [4]. The forward pass through the model is done once and several linear classifiers with different hyperparameters are trained simultaneously. For a publicly available implementation, I refer the authors to the iBOT repo (https://github.com/bytedance/ibot/).\n- The paper shows improved prototype diversity but does not sufficiently demonstrate how this translates to better representations. An important aspect of SSL is the transferability of learned representations. Currently, no transfer learning experiments are included. [3] found that the transfer learning performance was similar or worse when pre-training on ImageNet with KP regularization. Is this also the case with Decoupling? In that sense, it is especially important to evaluate this (for say, CARP vs CARP+Decoupling and if possible CARP+KP).\n- The motivation for choosing CARP in section 4.1 is not clear. It is stated that CARP exhibits a higher degree of collapse and hence, a bigger challenge. But in section 2.2 (line 153) it is stated that \"CARP achieves substantially higher prototype diversity than DINO\" and in section 4.3 (line 368) it is stated that \"CARP is far less prone to prototype collapse than the baseline CARL\". These statements are conflicting. Wouldn't it make sense to experiment with DINO instead if the idea is to choose the method that exhibits the highest amount of collapse/challenge (see Table 1)?\n\n**Minor:**\n- On one hand, this simplifies some complexities in training prototypical SSL methods by not requiring centering, sharpening. But this introduces other techniques and hyperparameter choices in the online GMM step. Since the responsibility based forgetting is important to prevent collapse, how sensitive is the training to the choice of the forgetting factor $\\eta$? An ablation experiment on this choice can be useful. Also, what is the value of $\\eta$ in the paper?\n\n[1] Darcet et al. “Cluster and Predict Latent Patches for Improved Masked Image Modeling.” TMLR 2025.\n\n[2] Silva et al. “Representation learning via consistent assignment of views over random partitions.” *NeurIPS 2023*.\n\n[3] Govindarajan et al. “On partial prototype collapse in the dino family of self-supervised methods.” BMVC 2024.\n\n[4] Oquab et al. “Dinov2: Learning robust visual features without supervision.” TMLR 2024."}, "questions": {"value": "- How much time and memory overhead does the online GMM add, including all the proposed techniques in Table A.1?\n- Is the online GMM setup (techniques and related hyperparameters) the same for both ImageNet and iNeturalist2018 pre-training? Is it necessary to adapt them to different pre-training datasets?\n- In Table 2, what is the backbone used for all the models?\n- The kNN performance reported in Figure 3b does not match with the results reported in Table 3. Can the authors clarify if the results in Figure 3b are after 100 epochs (corresponding to Fugure 4) and what backbone model is used here?\n- CARP uses prototypes that are divided into P blocks of size N_B each. The paper specifies the total number of prototypes. But how many partitions are used? Just to clarify, is the online GMM applied to each block separately or is it applied to the complete set of prototypes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VTxQXmF4Fa", "forum": "fVJEWdwvLO", "replyto": "fVJEWdwvLO", "signatures": ["ICLR.cc/2026/Conference/Submission4905/Reviewer_7bAf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4905/Reviewer_7bAf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998520818, "cdate": 1761998520818, "tmdate": 1762917751839, "mdate": 1762917751839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores the pervasiveness of a type of shortcut learning in self-supervised methods called partial protytype collapse. The authors show not all methods exhibit equal susceptibility to partial protytpe collapse, noting that methods that have decoupled training objectives for prototypes suffer much less from this shortcut. Motivated by this finding the authors proposes learning prototypes using an EM approach that's separate from the main self-supervisd training objective."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- findings in Section 2 that not all methods suffer from partial prototype collapse is novel and nicely motivates the decoupled training method proposed.\n- the authors connect this collapse to recent findings regarding the muted effect of data scaling in SSL methods (lines 388). This is a fundamental open question that is carefully studied here through the lens of prototypes. \n- examining training dynamics of the prototype collapse in Section 4.3 is an insightful approach that goes beyond simply examining the final representation. This section offers several nice insights such as the fact that the collapse emerges early on in training. \n- authors include a nice comparison for KNN and linear probing for both iNaturalist and ImageNet, showing decoupling outperforms baselines. \n- authors include hyperparameters and training details in the appendix"}, "weaknesses": {"value": "- define prototype in the introduction, and provide a clear accessible explanation of the partial prototype collapse you're aiming to solve.\n- as described in section 2.1 the setting you're working with is focused on image domain, make sure this is clearly defined early on in the paper so readers have the right expectation of the setting you're exploring. \n- similarly, technial terms such as KP regularization also require even a brief explanation to make this work accessible to broader audience. The authors assume too much prior knoweldge in my opinion. \n- what limitations does using a Gaussian Mixture Model impose on the learned algorithm? A discussion and exploration of the limitations this imposes is missing.\n- the authors discuss the merits of CARP w/ Decoupling in Section 4.4 with respect to head and tail classes, but offer a discussion or explanation of why CARP w/ decoupling underperforms DINO + KP for medium tailed classes as shown in Table 2. \n- I'd be curious to see scaling trends for CARP w/ decoupling given the discussion of scale in Section 4.4. Have the authors experimented with this?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tbpW5jge4h", "forum": "fVJEWdwvLO", "replyto": "fVJEWdwvLO", "signatures": ["ICLR.cc/2026/Conference/Submission4905/Reviewer_Lrqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4905/Reviewer_Lrqz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007056400, "cdate": 1762007056400, "tmdate": 1762917751413, "mdate": 1762917751413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}