{"id": "gE17TwVMNh", "number": 15905, "cdate": 1758256907134, "mdate": 1759897274145, "content": {"title": "Dynamic Reflections: Probing Video Representations with Text Alignment", "abstract": "The alignment of representations from different modalities has recently been shown to provide insights on the structural properties of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of \\textit{video} data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that state-of-the-art video encoders (e.g., VideoMAEv2) achieve significantly stronger alignment with text than the best image encoders (e.g., DINOv2), suggesting that motion and temporal context provide important cues for relating complex dynamic scenes to their semantic descriptions. Second, we show that alignment quality is highly sensitive to the richness of the provided annotations; using multiple, diverse captions for a single video yields substantial gains over a single caption. Jointly, these two observations suggest that limited cross-modal alignment observed in previous approaches is to a significant extent due to impoverished representations of \\textit{both} visual (static images vs. videos) and text (single caption vs. a collection) data given at test time. Furthermore, we also investigate the correlation between semantic alignment and performance on non-semantic downstream tasks, providing initial evidence that strong semantic grounding may be linked to \\textit{general-purpose} video representation and understanding. Ultimately, our work introduces video-text alignment as an informative way to probe the representation power of different encoders for spatio-temporal data.", "tldr": "Our study of video-text representation alignment demonstrates that alignment is dramatically improved by using richer test-time data, such as multiple video frames and diverse captions.", "keywords": ["Platonic Representation hypothesis", "video understanding", "video-text alignment"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9affa70247982d3848aae5275cb1aa8ca601f3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends the static cross-modal alignment evaluation scheme to the dynamic temporal domain via the Mutual k-NN metric. It explores the laws of video-text alignment using the VATEX and PVD datasets; validates the correlation between alignment and downstream tasks using Kinetics-400 and SSv2; and verifies the sensitivity of alignment to temporal information using VideoComp. Its core contributions are: 1. Proposes an alignment evaluation scheme tailored for the temporal domain; 2. Confirms that multiple video frames and multiple captions during testing can enhance alignment performance; 3. Proposes the Test-time Scaling Laws formulation, which can guide data collection and model capability comparison; 4. Preliminarily verifies that video-text alignment can serve as a zero-shot metric, enabling replacement of expensive cross-modal decoder-based evaluation without additional training; 5. Verifies the temporal sensitivity of alignment using the Test of Time and VideoComp datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.For the first time, static alignment evaluation strategies are extended to the dynamic temporal domain, enabling alignment assessment between video and text.\n2.It clarifies the law that \"multiple frames/multiple captions can improve alignment but exhibit a saturation effect,\" providing a directly reusable quantitative basis for \"data collection scale optimization\" and \"rapid model selection\" in practical applications, thus avoiding redundant resource investment.\n3.Covering multiple downstream tasks, the zero-shot evaluation metric features a low-cost advantage: it enables the assessment of model representation quality without the need to train additional expensive cross-modal decoders, significantly simplifying the evaluation process and reducing computational and annotation costs, which adapts to the demand for efficient model screening in production.\n4.It empirically verifies temporal sensitivity, pointing out directions for the optimization of video models."}, "weaknesses": {"value": "1.The core theory relies solely on experimental observations and lacks solid support from theoretical derivation.\n2.The Test-time Scaling Laws are essentially derived from data fitting, lack theoretical support, and their generalizability across scenarios remains limited.\n3.The downstream tasks covered in the preliminary exploration of the zero-shot metric are incomplete. In practical scenarios, more focus is placed on the actual performance of downstream tasks, and relying solely on relative scores may not meet the practical needs of video model selection.\n4.The temporal analysis lacks validation on large datasets, casting doubt on the applicability of its conclusions."}, "questions": {"value": "Reference Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XBmFdwlGM6", "forum": "gE17TwVMNh", "replyto": "gE17TwVMNh", "signatures": ["ICLR.cc/2026/Conference/Submission15905/Reviewer_ZGK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15905/Reviewer_ZGK1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760950810527, "cdate": 1760950810527, "tmdate": 1762926121851, "mdate": 1762926121851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale empirical study on the emergent alignment between unimodal video and text representations, extending the Platonic Representation Hypothesis to the temporal domain. The authors' central thesis is that the alignment is not a static property but is highly dependent on the amount of information provided at test-time. This observation is formalized through a proposed \"test-time scaling law,\" a parametric model that achieves a high degree of fit with the empirical data. Finally, the authors show that this alignment score strongly correlates with downstream performance on a variety of video understanding tasks, proposing it as a computationally efficient, zero-shot proxy for evaluating video model capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tLarge-Scale Empirical Study: The primary strength of this work lies in its experimental rigor. The authors conduct a comprehensive analysis across a vast suite of 63 vision models and 30 language models on multiple datasets.\n\n2.\tCritical Baseline: The paper's systematic use of a powerful image-encoder-plus-frame-averaging baseline is a significant contribution. The fact that this simple baseline outperforms many purpose-built video models is a critical finding for the community.\n\n3.\tNovel Phenomenon: The introduction of the test-time scaling law (Eq. 2) is a valuable contribution. The high coefficient of determination (R² > 0.98) indicates its descriptive power, and the interpretability of its parameters provides a principled way to compare how different architectures leverage temporal information."}, "weaknesses": {"value": "1.\tThe Scaling Law: I am concerning about the scaling law proposed in the paper. What is the true significance of scaling data to boost alignment scores? Isn't the high alignment achieved this way just an artificial way to minimize error? Fundamentally, isn't this just about providing more information to reduce the randomness of the MkNN metric and make the metric itself more robust? But if a model is incapable of producing a robust, comprehensive, and unambiguous representation from a single piece of data, then it naturally deserves a lower alignment score. Isn't that precisely what's supposed to happen? Why do we need to artificially intervene to change this score at all?\n\n2.\tInsufficient Control for Confounding Variables: The paper's core claim is that video-text alignment is a predictive proxy for downstream performance. However, the analysis does not control for obvious confounding variables like model scale, pre-training data volume, and architecture. It is unclear whether alignment is a true predictive cause or simply another correlated effect of a \"better model.\" The study would be significantly stronger if it could demonstrate this correlation holds even when controlling for these factors (e.g., within a single model family of varying sizes).\n\n3.\tThe statement of \"General-Purpose\" Applicability: The claim that the alignment metric serves as a probe for \"general-purpose video representation and understanding\" is not fully substantiated. The reported weak correlation with the point tracking task is a direct counterexample, suggesting the metric's predictive power may be limited to a specific class of tasks. \n\n4.\tLacking theoretical analysis on Scaling Law: The proposed scaling law is presented as an empirical fit. While the fit is excellent, the paper offers no theoretical justification for its specific mathematical form. Without a basis in information theory, learning theory, or another principled framework, the law remains an observation specific to this experimental setup, and its generalizability to other datasets, modalities, or alignment metrics is not guaranteed."}, "questions": {"value": "1.\tWhat is the true significance of scaling data to boost alignment scores?\n\n2.\tIsn't the high alignment achieved this way just an artificial way to minimize error? Fundamentally, isn't this just about providing more information to reduce the randomness of the MkNN metric and make the metric itself more robust? \n\n3.\tIf a model is incapable of producing a robust, comprehensive, and unambiguous representation from a single piece of data, then it naturally deserves a lower alignment score. Isn't that precisely what's supposed to happen? Why do we need to artificially intervene to change this score at all?\n\n4.\tWhether alignment is a true predictive cause or simply another correlated effect of a \"better model.\"? \n\n5.\tLacking theoretical analysis on Scaling Law: The is presented as an empirical fit. While the fit is excellent, Is there any theoretical justification/guarantee for its specific mathematical form of the proposed scaling law. \n\nOther questions please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QNcdMHDcwj", "forum": "gE17TwVMNh", "replyto": "gE17TwVMNh", "signatures": ["ICLR.cc/2026/Conference/Submission15905/Reviewer_h9ZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15905/Reviewer_h9ZS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555403399, "cdate": 1761555403399, "tmdate": 1762926120336, "mdate": 1762926120336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates video-text representation alignment, focusing on modern video and language encoders. It demonstrates how cross-modal alignment depends on the richness of visual and textual data provided at test time. The study introduces test-time scaling laws, showing how adjustments to the number of frames and captions can improve alignment scores. The paper further explores the relationship between alignment quality and downstream task performance, including temporal reasoning and general video understanding. The findings suggest that alignment could serve as a valuable zero-shot metric for evaluating video models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**Comprehensive Approach**: The paper provides the first comprehensive study of video-text representation alignment, extending the Platonic Representation Hypothesis to the temporal domain, making it a significant contribution.\n\n**Correlation with Downstream Tasks**: The correlation between alignment scores and performance on semantic and non-semantic tasks demonstrates the practical value of alignment as a metric."}, "weaknesses": {"value": "The idea of probing visual representation with video-text alignment is not such convincing. This evaluation is fair for models proposed on cross-modal tasks, but visual ability is not only cross-modal alignment.   \nFor example, in tasks such as video object detection and video object tracking, the vision model only need to detect pixel-level difference in the picture, without the need to be aware of textual semantics.   \nThe DINO-series [1], SAM-seris [2], I-JEPA [3] and V-JEPA [4] are some evidence that model can excel in visual tasks without the need of textual semantic. \n\n[1] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P. and Joulin, A., 2021. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9650-9660).    \n[2] Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. \"Segment anything.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4015-4026. 2023.  \n[3] Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y. and Ballas, N., 2023. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 15619-15629).   \n[4] Assran, Mido, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi et al. \"V-jepa 2: Self-supervised video models enable understanding, prediction and planning.\" arXiv preprint arXiv:2506.09985 (2025)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1HxCv1IiWR", "forum": "gE17TwVMNh", "replyto": "gE17TwVMNh", "signatures": ["ICLR.cc/2026/Conference/Submission15905/Reviewer_AP4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15905/Reviewer_AP4E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555412430, "cdate": 1761555412430, "tmdate": 1762926117582, "mdate": 1762926117582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work conducts a study of video-text representation alignment, probing the capabilities of modern video and language encoders.The authors propose a systematic probing framework using mutual k-NN alignment, extending previous static image–text  to videos. The study is comprehensive especially in video-text alignment study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, The study of the video-text alignment is meaningful and important.\n2, Provides comprehensive experiments, which requires a lot of hardwork.\n3, the idea of test-time scaling seems sound\n4, Obervation provided in L182-186,L484-485 is informative."}, "weaknesses": {"value": "1, Limited novelty in methodology.\nThe approach seems  a empirical report, might not  meet ICLR’s innovation threshold.\n2, abstract is different from the paper, could be misleading.\n3, the improvement over previous image based methods seems limited."}, "questions": {"value": "1, What's the main difference or challenge in adapting mutual k-NN from  Huh et al. (2024) to the video domain? It seems the novelty is incremental. \n2, L484-485, what could the potential reason? This question may help with future directions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bCVJnTmUZK", "forum": "gE17TwVMNh", "replyto": "gE17TwVMNh", "signatures": ["ICLR.cc/2026/Conference/Submission15905/Reviewer_aU2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15905/Reviewer_aU2m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926106765, "cdate": 1761926106765, "tmdate": 1762926117143, "mdate": 1762926117143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}