{"id": "bYA07SdHoS", "number": 6810, "cdate": 1757996715280, "mdate": 1759897892336, "content": {"title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts", "abstract": "Jailbreaking commercial black-box models is one of the most challenging and serious security threats today. Existing attacks achieve certain success on non-reasoning models but perform limitedly on the latest reasoning models. We discover that carefully crafted developer messages can markedly boost jailbreak effectiveness. Building on this, we propose two developer-role-based attacks: D-Attack, which enhances contextual simulation, and DH-CoT, which strengthens attacks with deceptive chain-of-thought. In experiments, we further diccover that current red-teaming datasets often contain samples unsuited for measuring attack gains: prompts that fail to trigger defenses, prompts where malicious content is not the sole valid output, and benign prompts. Such data hinders accurate measurement of the true improvement brought by an attack method. To address this, we introduce MDH, a Malicious content Detection approach combining LLM-based screening with Human verification to balance accuracy and cost, with which we clean data and build the RTA dataset series. Experiments demonstrate that MDH reliably filters low-quality samples and that developer messages significantly improve jailbreak attack success. Codes, datasets, and other results are in appendix.", "tldr": "We propose two text jailbreak attacks against commercial black-box LLMs and a malicious content detection method, and apply the latter to red team dataset cleaning and jailbreak response detection.", "keywords": ["Jailbreak Attack", "Malicious Content Detection", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5340dea7dd8ff01eb11cb29d61bf58767bd1c1e9.pdf", "supplementary_material": "/attachment/bbef7f697f4e43cc774a04d4b1e091a7c88a5b29.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces two developer-role-based jailbreak attacks, D-Attack and DH-CoT, and highlights how developer messages can substantially improve jailbreak effectiveness. It also proposes MDH, a detection method for filtering low-quality malicious samples, and builds the RTA dataset to enable more accurate evaluation. Overall, the work contributes new attack strategies and dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The fine-grained classification of harmful prompts is useful to filter out low-quality samples.\n\n- The paper presents an extensive empirical evaluation of attacks across different OpenAI models."}, "weaknesses": {"value": "1. Overall, the writing could be improved for clarity and readability.\n- The paper introduces a large number of non-standard acronyms, which makes it difficult for readers to follow without frequent back-and-forth checking.\n- Important details, such as the significant difference between the system role and the developer role, would be better presented clearly in the main text rather than placed in a lengthy appendix.\n- The overall flow could also be improved, especially in the methodology section (e.g., the descriptions of D-Attack and DH-CoT), which is currently hard to follow.\n\n2. The MDH framework appears to rely on a fairly standard majority voting process among multiple judge LLMs combined with human annotation. The novelty therefore seems primarily limited to the more fine-grained classification of attack prompt types. It would be helpful if the authors could elaborate on what specific design choices make the proposed malicious content detection framework go beyond an incremental contribution.\n\n3. The motivation for developing the new attack method is not sufficiently articulated. In particular, the distinctions between the system role and the developer role (referenced in Lines 38 and 207) are not clearly explained in the main text. It is also unclear why these distinctions would necessitate a different attack strategy. Although Appendix E discusses this issue, the justification remains unconvincing, especially since the precise differences between the two roles are not publicly available. I encourage the authors to elaborate on these points in the main text, as the proposed attack method currently appears to lack strong motivation.\n\n4. Why does Table 9 only compare against template-based jailbreak methods? Could other attacks (e.g., methods from HarmBench https://www.harmbench.org/results ) also be applied to this task? If so, why were they excluded? Please elaborate.\n\n5. No statistical uncertainty is reported for the results, which limits the reader’s ability to evaluate the significance and reliability of the findings."}, "questions": {"value": "Please refer to **Weaknesses** for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMzDkHgMz9", "forum": "bYA07SdHoS", "replyto": "bYA07SdHoS", "signatures": ["ICLR.cc/2026/Conference/Submission6810/Reviewer_pcEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6810/Reviewer_pcEj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761194203406, "cdate": 1761194203406, "tmdate": 1762919078125, "mdate": 1762919078125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two things: two novel jailbreak attacks based on the developer role called D-Attach and DH-CoT, as well as a technique called Malicious content Detection with Human verification (or MDH) to screen for explicitly malicious prompts and filter out benign prompts, non-harmful prompts, and non-triggering harmful response prompts. They apply MDH to 5 commonly used datasets to create the RTA series with 1155 explicitly malicious prompts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work identifies and mitigates weaknesses in existing jailbreak datasets and creates a filtered dataset to mitigate them.\n2. This work points out important behavioral differences between the system and the developer role, such as showing that prompts that are often rejected in the system role can succeed in the developer role (line 813-814)"}, "weaknesses": {"value": "1. The mini test set created for Judger selection only contains 10 prompts, which is extremely small and unreliable (line 156).\n2. Limited technical novelty of attacks: both the D-Attack and D-HCoT attacks are a combination of known prompting-based attack vectors, like persona-based attacks (Cognitive hacking or COG in [1]) [1,2], instruction-based attacks (Direct Instruction or INSTR in [1]), few-shot hacking (Few Shot Hacking or FSH in [1]) [1,2], and H-CoT [3]. \n3. Writing is confusing to follow, and uses a lot of unnecessary acronyms and complicated exposition. \n4. All attacks are specific to OpenAI models because they rely on the developer role, limiting the applicability of the findings and making them less impactful.\n5. Marginal gains attained by DH-CoT over H-CoT (Table 9), especially for GPT-3.5, GPT-4.1, and GPT-4o.\n\n[1]: Rao, Abhinav, et al. \"Tricking llms into disobedience: Formalizing, analyzing, and detecting jailbreaks.\" arXiv preprint arXiv:2305.14965 (2023).  \n[2]: Schulhoff, Sander, et al. \"Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition.\" Association for Computational Linguistics (ACL), 2023.  \n[3]: Kuo, Martin, et al. \"H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking.\" arXiv preprint arXiv:2502.12893 (2025)."}, "questions": {"value": "Why should we care about these findings when they are specific to the developer role, which is just a design choice for OpenAI models and doesn't tell us anything general about the safety of LLMs?\nHow do you make sure the MDH data generation doesn't end up advantaging your proposed attacks in terms of ASR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sRYmdKJItM", "forum": "bYA07SdHoS", "replyto": "bYA07SdHoS", "signatures": ["ICLR.cc/2026/Conference/Submission6810/Reviewer_26Eb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6810/Reviewer_26Eb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980633900, "cdate": 1761980633900, "tmdate": 1762919077790, "mdate": 1762919077790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to instruct the attacker LLM with developer-role messages, which improve jailbreak success on target commercial-scale target models. Authors also release a cleaned, attack-oriented RTA dataset for more comprehensive evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written with a good intuition. The proposed prompting trick is well-justified by such an intuition.\n2. The research topic of stress-testing LLMs and robustifying them is important and timely."}, "weaknesses": {"value": "1. The storyline of designing the jailbreak message is built upon the multi-role configuration of the OpenAI model APIs, which limits its applicability when using other attacker models. While I appreciate that the authors have vaguely pointed out such a limitation in the appendix, proposing a potential solution or designing fixes to enhance transferability would be more convincing.\n2. Please fix salient typos such as \"diccover\" in the abstract. While minor typos would not affect the judgment, too frequent observation may harm the reading experience."}, "questions": {"value": "The weakness is listed above. My initial rating is 4 for this paper, and I look forward to the interaction with the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V3mhRicucQ", "forum": "bYA07SdHoS", "replyto": "bYA07SdHoS", "signatures": ["ICLR.cc/2026/Conference/Submission6810/Reviewer_k4HS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6810/Reviewer_k4HS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027583057, "cdate": 1762027583057, "tmdate": 1762919077461, "mdate": 1762919077461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses critical vulnerabilities in commercial large language models (LLMs), particularly OpenAI's API, by proposing two novel developer-role-based jailbreak attacks: D-Attack, which enhances contextual simulation through structured malicious developer messages, and DH-CoT, which augments deceptive chain-of-thought prompting with aligned educational framing to bypass safeguards in reasoning-optimized models such as o3 and o4. \n\nMeanwhile, it recognizes deficiencies in existing red-teaming datasets—such as the inclusion of benign prompts (BP), non-obvious harmful prompts (NHP), and non-triggering harmful-response prompts (NTP) that obscure attack efficacy evaluations. To find high-quality red-team data, the authors introduce MDH, a hybrid malicious content detection framework combining LLM-based multi-round voting with selective human verification to clean datasets. The refined datasets are called the RTA series.\n\nThrough experiments on refined RTA datasets derived from sources like SafeBench and BeaverTails, the methods demonstrate substantial ASR improvements (e.g., up to 40% on advanced models), while MDH achieves over 95% detection accuracy with minimal human effort (4–8% review rate), underscoring the need for precise, attack-oriented evaluation benchmarks in LLM security research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces innovative, transferable black-box attacks (D-Attach, DH-CoT) leveraging the emergent Developer role, achieving high efficacy against state-of-the-art reasoning models like o3 and o4, with demonstrated cross-provider transferability to Gemini, Claude, and Deepseek.\n- Proposes MDH as a scalable, cost-effective detection framework that balances automation and human oversight, enabling the creation of high-quality RTA datasets.\n- Provides a novel prompt taxonomy (BP, NHP, NTP, EHP) to critique and refine red-teaming datasets, enhancing the precision of jailbreak evaluations and contributing practical tools (codes, datasets) in appendices for reproducibility."}, "weaknesses": {"value": "- Primarily evaluates on OpenAI models due to Developer role dependency, potentially limiting immediate applicability to non-OpenAI APIs without adaptation, though transferability is noted.\n- Relies on manual annotation for ground truth in MDH evaluations (e.g., 10–22% of samples), which, while minimized, introduces subjectivity and scalability challenges for larger datasets.\n- Experiments for DH-CoT are constrained to the MaliciousEducator dataset for fair comparison, restricting broader validation across diverse prompt types and potentially underrepresenting variability in attack performance."}, "questions": {"value": "1. This paper mainly studies the developer role in OpenAI's API. However, the experiments showed the empirical results for other models without the “developer\" role (using the \"system\" role instead). Is it possible to gain a universal understanding in the conclusion that does not bind to the \"developer\" role, which might persist after one or two system updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0IaMg5i5go", "forum": "bYA07SdHoS", "replyto": "bYA07SdHoS", "signatures": ["ICLR.cc/2026/Conference/Submission6810/Reviewer_5wDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6810/Reviewer_5wDj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098263484, "cdate": 1762098263484, "tmdate": 1762919077128, "mdate": 1762919077128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}