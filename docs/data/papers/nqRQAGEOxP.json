{"id": "nqRQAGEOxP", "number": 16375, "cdate": 1758263872846, "mdate": 1759897244808, "content": {"title": "DIME: Tackling Density Imbalance for High-Performance and Low-Latency Event-Based Object Detection", "abstract": "Event-based object detection takes advantages of the high temporal resolution and dynamic range of event cameras, offering significant benefits in scenarios involving fast motion and challenging lighting conditions. Typically, event streams are first converted into frame sequences through frame-based representations, followed by spatiotemporal feature fusion, similar to video processing. However, video-based processing methods overlook the sparse and non-uniform nature of event streams, making them inadequate for meeting the effectiveness and low-latency processing demands. To address these challenges, we rebuild the spatiotemporal dependency model of event stream by focusing on three key aspects: First, we design a spatiotemporal linear attention to direct build dependencies at patch-level while maintaining spatial parallelism; Second, we incorporate a frame-level temporal decay and spatial position encoding mechanism into the linear attention, which adaptively adjusts the internal state of the network based on the frame information; Third, we propose a structure-level local and global linear attention architecture, which extract event features based on our linear model at different granularities. Our model achieves SOTA performance on Gen1 and 1Mpx datasets, firstly surpassing 50\\% mAP on 1Mpx with a compact size, while reducing parameters by 3.2× and runtime by 5.1× compared to similar-performing methods, and outperforming lightweight models by +4.3\\% mAP.", "tldr": "", "keywords": ["event; object detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbcb8ec874899ed345635282e7934990cf873edd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DIME, a novel method addressing the uneven information density in event frames for event-based object detection. DIME is designed at three levels: (i) a spatiotemporal linear attention mechanism at the patch level to directly model dependencies and avoid anchor-block bias; (ii) a frame-level scoring and temporal decay encoding to enhance adaptive memory of informative regions; and (iii) a local–global hybrid architecture for progressive multi-scale feature extraction. The results show that DIME achieves SOTA performance on the Gen1 and 1Mpx datasets, with notably fewer parameters and faster inference than existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is thoroughly validated across multiple datasets and compared against a wide range of baselines, demonstrating clear overall advantages in accuracy, parameter efficiency, and inference speed.\n\n2. The lightweight DIME-S significantly reduces model parameters while maintaining high performance, making it well-suited for real-time deployment.\n\n3. The manuscript is clearly written with detailed figures and tables, a complete appendix."}, "weaknesses": {"value": "1. The motivation of this paper is to achieve low-latency and high-performance object detection. From my perspective, this goal could be more effectively addressed through asynchronous fusion of both frames and events, rather than using only a single event modality. Nevertheless, the focus on event-based object detection alone is still a valuable direction. If low latency is the primary objective, approaches such as sparse convolution or graph-based methods might be more appropriate. The idea of addressing density imbalance as the central motivation is not entirely convincing to me. Could the authors provide a clearer explanation of how this directly contributes to latency reduction or performance improvement?\n\n2. In Section 3.1, the authors describe aspects of event density. Are these currently treated as hyperparameters? Could the authors include experiments comparing different fixed temporal window lengths or event counts, and contrast them with the proposed method to better demonstrate its advantage?\n\nOverall, this work represents an incremental improvement in event-based object detection, rather than a major breakthrough. I did not find strong contributions toward areas such as asynchronous event-based detection, event-based multimodal LLMs for detection, or event-based stereo 3D detection. I encourage the authors to carefully consider feedback from other reviewers as well. For now, I rate this paper a 4, and will re-evaluate after the rebuttal and further discussion."}, "questions": {"value": "Please provide responses to each of the comments regarding the weaknesses. \n\nIn addition, how could the tracking density imbalance technique be applied solely to event representations, and ideally extended to a joint representation of events and frames? This might make the approach even more useful for the field."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vGg1JoqHXS", "forum": "nqRQAGEOxP", "replyto": "nqRQAGEOxP", "signatures": ["ICLR.cc/2026/Conference/Submission16375/Reviewer_h62Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16375/Reviewer_h62Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818588403, "cdate": 1761818588403, "tmdate": 1762926499301, "mdate": 1762926499301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles event-based object detection and aims to mitigate event-density imbalance across timestamps and spatial locations with a new method, DIME. DIME consists of a patch-level spatial-temporal attention strategy and a temporal decay method to enhance important patches and frames during training. Experiments on the Gen1 and 1Mps datasets demonstrate the state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem that this paper aims to address sounds interesting, the asynchronousness and sparsity nature of event data inherently leads to the information imbalance in different patches and frames, usually presenting a significant challenge in handling blank patches and achieving optimal efficacy.\n- This paper is well-written, with the motivation and proposed method well-structured.\n- Experiments on well-known benchmarks demonstrate a state-of-the-art performance and a good balance between accuracy and speed."}, "weaknesses": {"value": "- The problem statement is compelling, while the resulting solution seems disconnected from this challenge: \n   - It is hard to understand why introducing the linear attention across different patches would mitigate the issue from information imbalance in different event data patches. It is not clearly explained what issue the information imbalance actually introduces. Basically, the proposed spatial-temporal attention is a kind of generic strategy that enhances patches beneficial for the final learning objective, instead of balancing the “imbalance” across patches.\n   - On the other hand, the proposed temporal decay is designed as a weighted sum of motion sharpness and the entropy. In the appendix, it can be seen that this metric is composed of the information of rapidly moving objects and the overall structural information. It seems that this strategy may further exacerbate the imbalance across patches and frames, since frames with more objects and higher structural complexities will be enhanced while sparse frames will be further marginalized, right?\n- The section on “causal real-time inference in event object detection” is vague. Real-time performance is always a challenge in a broad range of object detection tasks. Given the limited information provided in this section, it is hard to see what is uniquely causal real-time inference here, why video-based methods would struggle to adapt to events, or how DIME’s design specifically addresses that challenge.\n- Following the previous point, this paper claims that video-based methods introduce significant computational overhead in event-based object detection, while relevant experimental comparisons and discussions are missing.\n- The implemented baselines are lower than the original performance, the performance of EGSST and SMamba in Figure 1 and Table 1 is much lower than the best results in their original paper. It is not clear whether experiments are performed fairly.\n- Given the different performance in the ablation results and main results, it seems that different backbones are used. The reasons for the lack of improvement on Maxvit may require further discussion and analysis.\n- Reproducibility statement is missing."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WANZNmfZiS", "forum": "nqRQAGEOxP", "replyto": "nqRQAGEOxP", "signatures": ["ICLR.cc/2026/Conference/Submission16375/Reviewer_PbB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16375/Reviewer_PbB3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831680947, "cdate": 1761831680947, "tmdate": 1762926498796, "mdate": 1762926498796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of spatiotemporal modeling degradation caused by uneven event-frame density in event cameras. The authors propose a novel framework named DIME for event-based object detection, which introduces improvements from three complementary levels. At the patch level, a temporal linear attention mechanism is designed to directly establish cross-spatiotemporal dependencies while avoiding the anchor-patch issue. At the frame level, a frame scoring–driven temporal decay and spatial position encoding scheme is used to enhance the network’s dynamic memory and representation capacity. At the structural level, a local–global hybrid architecture is proposed to enable multi-scale feature extraction. Overall, the framework is conceptually sound and experimentally validated on both the Gen1 and 1Mpx datasets, demonstrating a good balance between accuracy and real-time performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The introduction of linear attention with spatial positional encoding effectively captures global dependencies while maintaining local consistency.\n\nThe proposed local–global hybrid spatiotemporal modeling (TLA + STLA) achieves a strong trade-off between representation capability and computational efficiency.\n\nThe experiments are extensive, including results on major datasets and ablation studies covering temporal decay, spatial encoding, and hierarchical modeling."}, "weaknesses": {"value": "The paper claims that linear attention mitigates the problem of uneven event density, but there is no theoretical explanation or visualization to support this claim. A feature distribution or activation map analysis would help verify the mechanism.\n\nEquation (10) introduces a parameter β (set to 0.5 in the supplementary), but the paper does not discuss how β is chosen or how sensitive the model is to this parameter. Adding a sensitivity or ablation study would strengthen the empirical justification.\n\nAlthough the paper repeatedly states that video-based models such as RVT fail to meet low-latency requirements, the reported inference time for RVT appears shorter than that of the proposed method. This discrepancy should be clarified, and the advantages of DIME beyond latency should be further emphasized.\n\nThe innovation boundary is somewhat unclear. The main novelty seems to rely on replacing RNN modules with linear attention. The paper would benefit from a clearer articulation of the conceptual difference and its impact on spatiotemporal modeling.\n\nFinally, the results of EGSST and SMamba in Table 1 (marked with *) are lower than those reported in their original papers. The authors should clarify whether these numbers come from reimplementation under different conditions or if the original results should be used for a fair comparison."}, "questions": {"value": "What is the essential distinction between DIME and video-based models such as RVT? Beyond replacing RNNs with linear attention, does DIME introduce a fundamentally different modeling perspective?\n\nHow does linear attention outperform RNNs and state-space models (SSMs) in the context of event stream modeling? Can the authors provide quantitative or qualitative evidence (e.g., sparsity modeling or gradient stability)?\n\nCould the authors provide additional visualization or statistical analysis under different event densities to better demonstrate DIME’s robustness to uneven event distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MZp3vlbSHK", "forum": "nqRQAGEOxP", "replyto": "nqRQAGEOxP", "signatures": ["ICLR.cc/2026/Conference/Submission16375/Reviewer_x4Gy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16375/Reviewer_x4Gy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925701441, "cdate": 1761925701441, "tmdate": 1762926498432, "mdate": 1762926498432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper titled “Dime: Tackling Density Imbalance for High-Performance and Low-Latency Event-Based Object Detection” tackled uneven spatiotemporal information flow in event cameras visa three modules: spatiotemporal dependency through token flattening, temporal decay and spatial position encoding for adaptively adjusting frame-memory, and token mixing for local and global feature extraction. It reports event-based object detection results. The work clearly frames the problem statement, shows effective spatiotemporal coupling but limited comparison to state of the art. Issues such as comparison with event-by-event processing methods, corrupted figures, overemphasis on video-baselines, limited evidence/visualization for uneven information distribution, very small gains from the temporal decay modeling, missing information on dataset rates and interpretability need to be addressed in the revision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper articulates the problem statement of handling unevenly distributed spatiotemporal data from event cameras and shows ways to approach the issues through experiments using the three proposed modules. \n2.\tThe experiments section shows results comparing to state of the art event camera based detection models.\n3.\tThe proposed coupling of spatio-temporal events through token flattening, while already utilized in literature, is used well in the final architecture.\n4.\tThe authors introduced the notion of using a frame-score to find a temporal decay parameter for fusing time information, and used temporal and spatiotemporal token mixing to relate object level and global features, which is an innovative contribution."}, "weaknesses": {"value": "1.\tSome of the figure labels in the paper has become corrupted and needs to be addressed.\n\n2.\tThe related works and comparison sections need improvement. There are very limited discussions and comparisons with event-by-event processing models that deal with the asynchronous nature of the data, and hence, the naturally considers the uneven event distribution. Examples includes methods based on neuromorphic computing, event-by-event processing with memory-augmented transformers (Kamal, et. al, ICLR'2023, Kamal et. al,. ECCV 2024, Hamaguchi, et. al. CVPR, to name a few), and graph based methods (AEGNN, CVPR'22).  Current literature review focuses more on the limitations of models that perform event aggregation followed by adaptation of image/video based methods.  It is important that authors clearly present their work in the context of asynchronous event-by-event processing based methods. The quantitative comparison of computation, latency, and model size will also be presented with these methods. \n\n3.\tThe challenges in event-based object detection section discusses the issue about uneven distribution of information density, but this distribution has not been backed by sufficient data to prove the importance of this problem with event camera. The work would benefit from a visualization showing how the uneven nature of the event distribution causes problems in the traditional detection models.\n\n4.\tThe methods section needs further explanation on the overview of how spatio-temporal tokens are interacting within the proposed architecture through additional visualization.\n\n5.\tIn the results section, the effect of the temporal decay is not apparent and seems to contribute very little to the overall performance, questioning the need for the added complexity of using this module."}, "questions": {"value": "1. In figure 1-c, what is the runtime for? Is it for event frames with fixed time-lengths, or is it for variable length events? Also, what device is the timing characterization done on? It needs to be disclosed on the performance plot.\n\n2. What are key intuitive differences the proposed approach compared to the existing approaches that natively processes asynchronous event-streams (instead of performing event aggregation)? It is not clear why authors assumed event-aggregation as at the baseline methods and demonstrated advancements over that. \n\n3. How does the proposed approach compare with prior asynchronous and event-by-event processing methods? Please provide comparison off accuracy, parameters, complexity, and latency. \n\n4. What are the data/event rates of the two mentioned datasets? Please add these for better understanding of the nature of the event data.\n\n5.\tIn Table 1- could the authors specify whether the runtime is per a fixed event frame, or per event? It is important for the readers to understand the actual data rate when comparing runtime. (for example, 10ms inference time in a 30 fps video would make sense, but 10ms inference time per event in a 1000 fps event rate would not be acceptable.)\n\n6.\tFrom the ablation studies in Table-3 and Table-4, the effects of temporal decay and spatio-temporal coupling are not apparent on the uneven information distribution of event cameras. Could authors visualize the intermediate features  study to highlight how the model accumulates uneven information density for different samples (low speed vs high speed)\n\n\n7. In Figure-4 it is shown that detection decisions are generated on each time step t, t-1… Could the authors describe the advantage of generating detections in each time step compared to taking the decision only at the final time-step after accumulating over all the frames in a sequence?\n\n8.\tIn section 4.2 – “while frames with less information obtain lower scores, leading to weaker decay and a stronger reliance on historical information” – do the authors mean that a new frame with less information content contributes less to the whole sequence and the historical information contributes more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WGjCQCPgOA", "forum": "nqRQAGEOxP", "replyto": "nqRQAGEOxP", "signatures": ["ICLR.cc/2026/Conference/Submission16375/Reviewer_7tHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16375/Reviewer_7tHn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186300480, "cdate": 1762186300480, "tmdate": 1762926497985, "mdate": 1762926497985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}