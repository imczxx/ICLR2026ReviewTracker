{"id": "5og80LMVxG", "number": 19510, "cdate": 1758296877744, "mdate": 1759897035291, "content": {"title": "Latent Wavelet Diffusion For Ultra High-Resolution Image Synthesis", "abstract": "High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present $\\textit{Latent Wavelet Diffusion (LWD)}$, a lightweight training framework that significantly improves detail and texture fidelity in ultra-high-resolution (2K-4K) image synthesis. LWD introduces a novel, frequency-aware masking strategy derived from wavelet energy maps, which dynamically focuses the training process on detail-rich regions of the latent space. This is complemented by a scale-consistent VAE objective to ensure high spectral fidelity. The primary advantage of our approach is its efficiency: LWD requires no architectural modifications and adds zero additional cost during inference, making it a practical solution for scaling existing models. Across multiple strong baselines, LWD consistently improves perceptual quality and FID scores, demonstrating the power of signal-driven supervision as a principled and efficient path toward high-resolution generative modeling.", "tldr": "We enhance Ultra High-Resolution image generation by decomposing latent features into wavelet subbands, allowing the model to focus on frequency-specific refinement during diffusion.", "keywords": ["Generative Models", "Diffusion Models", "Wavelet", "Ultra High-Resolution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d0bdc0433b569f9f6f73c763c952061fa62be36.pdf", "supplementary_material": "/attachment/66201a717f3bfc3012b0777c1b363d4a732a132f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Latent Wavelet Diffusion (LWD), a novel training framework designed to enhance the quality of ultra-high-resolution (UHR) image synthesis (2K-4K) from latent diffusion models. The authors identify a key limitation in existing methods: a uniform training process that fails to distinguish between low-detail and high-detail regions, leading to wasted computation and a loss of fine-grained texture.\n\nLWD addresses this by introducing a signal-driven, frequency-aware supervision strategy. The framework consists of two main stages. First, a pre-trained Variational Autoencoder (VAE) is fine-tuned with a scale-consistent spectral objective to produce a more stable and spectrally regular latent space. Second, during the fine-tuning of a latent diffusion model, LWD computes wavelet energy maps from the latent codes at each step. These maps are used to create a time-dependent spatial mask, which dynamically modulates the training loss to focus more intensely on detail-rich (high-frequency) regions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "LWD is a training-only strategy, requiring no architectural changes to the diffusion model and adding zero computational overhead during inference."}, "weaknesses": {"value": "1 The paper called the proposed VAE is a spectrally-aware VAE, where no spectral objective is involved. In my opinion, it is somewhat overclaimed.\n\n2 Figure 3 is hard to read. For example, what does DCT mean?\n\n3 The title is Latent Wavelet Diffusion. However, in the paper, the wavelet only proposes a mask for RGB-based diffusion training. I do not \nagree that this model is a wavelet diffusion model, and worry about the contribution&novelty of this paper.\n\n4 The improvements in Table 1&2 are limited."}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CA3SigW5hL", "forum": "5og80LMVxG", "replyto": "5og80LMVxG", "signatures": ["ICLR.cc/2026/Conference/Submission19510/Reviewer_Zh5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19510/Reviewer_Zh5f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637622324, "cdate": 1761637622324, "tmdate": 1762931407317, "mdate": 1762931407317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Latent Wavelet Diffusion (LWD), a method aimed at improving ultra-high-resolution image synthesis (2K-4K) using latent diffusion models. The authors propose a frequency-aware masking strategy based on wavelet energy maps, which helps focus the training process on detail-rich regions in the latent space. The approach also incorporates a scale-consistent VAE objective to ensure high spectral fidelity. The key advantage of LWD is that it enhances image quality and detail without requiring architectural modifications or additional inference costs. The paper provides experimental results across multiple datasets and compares LWD with several state-of-the-art models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is notably efficient, both in training and inference, as it does not require any changes to the underlying model architecture or introduce additional computational cost during inference.\n2. The approach results in noticeable improvements in key image fidelity metrics such as FID and LPIPS, with a demonstrated enhancement in texture and detail preservation.\n3. The frequency-aware saliency map and time-dependent masking strategy offer a interpretable way to focus model attention on high-frequency regions."}, "weaknesses": {"value": "1. While the method shows improvements in several fidelity metrics, there is a noticeable performance drop on certain tasks, such as PickScore and HPSv2.1. These drops suggest potential degradation in text-image alignment, which could impact the quality of the generated images, as seen in the generated Eiffel Tower in Figure 4. The seasonal inconsistency in the image (the LWD + URAE version of the Eiffel Tower showing a different season than the original) further indicates this issue.\n2.  The paper discusses the VAE fine-tuning with scale-consistency loss, but the contribution of this step to the overall performance is not adequately ablated or isolated. Without a clear comparison, it is difficult to assess how much this component adds to the method’s success.\n3. The paper introduces a saliency map generated using wavelet decomposition, but it does not provide an ablation study to verify whether this saliency map is actually contributing to the improvements in performance. An experiment comparing results with and without the saliency map would help clarify whether this component is functioning as intended and contributing meaningfully to the enhancement of fine details."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4cgM83v1gb", "forum": "5og80LMVxG", "replyto": "5og80LMVxG", "signatures": ["ICLR.cc/2026/Conference/Submission19510/Reviewer_sQLN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19510/Reviewer_sQLN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827839507, "cdate": 1761827839507, "tmdate": 1762931406901, "mdate": 1762931406901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Wavelet Diffusion (LWD), a frequency-aware modification of diffusion training that enables existing architectures to generate high-resolution samples without architectural changes or excessive sampling cost. The authors use a multiscale VAE training objective together with a wavelet-based diffusion training loss, extending both the latent space and the diffusion process to better address the unique challenges of high-resolution generation. The VAE training ensures that the latent space exhibits well-structured frequency characteristics, while the frequency-aware loss encourages the model to allocate more training capacity to high-frequency regions of the input. Experiments demonstrate that LWD achieves higher-quality generation at high resolutions compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper leverages existing architectures for high-resolution generation with minimal training overhead. Given the importance of high-resolution generation in many applications, the proposed method could have significant practical impact.\n\n- The use of wavelets to modulate the loss function during diffusion model training is, in my opinion, both novel and interesting.\n\n- The method also fine-tunes the VAE component, which appears crucial for maintaining a well-structured latent space at higher resolutions.\n\n- The experimental results demonstrate the benefits of the proposed approach, at least to some extent.\n\n- Overall, the paper is well written and easy to follow."}, "weaknesses": {"value": "- The quantitative and qualitative results presented in the paper are somewhat confusing, in my opinion. While the qualitative results clearly show advantages of LWD + URAE, the quantitative metrics—particularly HPSv2, which I believe aligns better with human perception than FID—indicate worse performance. Conducting a user study and providing more qualitative comparisons between LWD and existing baselines would strengthen the paper’s contributions.\n\n- The claim regarding identical inference time is somewhat misleading. The inference cost of diffusion models also depends on the input resolution. As discussed in the HiDiffusion paper, generating high-resolution images with models like SDXL is increasingly challenging due to the computational demands at higher resolutions. The authors should clarify this point and perform a more balanced evaluation that demonstrates the inference-time limitations of running large models (e.g., Flux) at higher resolutions.\n\n- The proposed multiscale VAE loss is not entirely novel, as similar formulations have been introduced in EQ-VAE and other prior works. The authors should revise the claim suggesting this is their contribution.\n\n**Minor**:\n- There appears to be a typo in Equation (1)."}, "questions": {"value": "1. Can you explain Figure 3? To me, SD3-Med has the closest spectrum to the RGB image as far as the plot shows. How does SE tuning help in this case?\n\n2. Can you report HPS scores for Table 2? It might be more reliable compared to other metrics such as FID for text-to-image generation\n\n3. Have you tried training the model only on low-frequencies first, and then shifting the attention more and more toward high-frequency details as the training progresses? The current version has high-frequency training for all steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "1. Could you clarify Figure 3? From the plot, it appears that SD3-Med has the spectrum most similar to the RGB image. How does SE tuning provide additional benefit in this case?\n\n2. Could you also report HPS scores for Table 2? This metric may be more reliable than FID for evaluating text-to-image generation quality.\n\n3. Have you considered a progressive frequency training strategy—starting with low-frequency components and gradually shifting focus toward high-frequency details as training progresses? The current version seems to include high-frequency pixels throughout all steps."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vpw3AFo85t", "forum": "5og80LMVxG", "replyto": "5og80LMVxG", "signatures": ["ICLR.cc/2026/Conference/Submission19510/Reviewer_QbLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19510/Reviewer_QbLK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922344447, "cdate": 1761922344447, "tmdate": 1762931406522, "mdate": 1762931406522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to mask the diffusion loss to favor areas where details lie.\nA masking weight matrix $A_\\text{wavelet}$ is initially computed from the input image using a 2x2 wavelet decomposition (only Haar wavelets appears to be mentioned).\nA mask $M_t$ is computed at every training by setting $1$ at every location where $A_\\text{wavelet} + l \\geq t/T$, here $t$ is current training step out of a total of $T$.\nA masked loss is then computed by multiplying the score error with $M_t$, effectively zero-ing gradients in low-detail areas.\n\nIn addition the paper makes use of existing methods for scale-consistency VAE fine-tuning to further improve results and help the conditioning of the latents provided to their core method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is simple and easy to implement, it's reminding me of past ideas for super-resolution where people used edge detection maps as an extra signal for learning. \n2. The paper is well written and easy to follow.\n3. The method does not incur any extra training costs or inference cost other than compute the wavelet matrix which is insignificant."}, "weaknesses": {"value": "1. Notations can be confusing: in some settings $t$ is the current training step (eq.6) while in other settings $t$ is the noise level / diffusion time step conditioner (eq 4, 5). \n2. What happens with other wavelets, or even DCT or FFT which can fulfill similar roles as Haar wavelets? (you mention in the appendix that Haar are the best suited, yet I'd still want experimental confirmation).\n3. Your method has two component (1) scale-consistency VAE fine-tuning (from existing works) and (2) frequential-energy masked loss, what's the effect on the evaluation metrics of each component?\n4. The $l$ ablation and other ablations such as scale-consistency should be in the main paper, not in the appendix.\n5. Some big images such as Fig.1 could be moved to the appendix to make space for scientific content like the aforementioned the ablations and wavelets comparisons that are currently missing.\n6. Experimental results feel inconclusive: sometimes there is improvement, sometimes degradation. There are lot of metrics being reported in tables 1 and 2, it's unclear whether some are more important than others. Just to explain further my point of view: 2.5 FID points reduction from 35.25 to 32.88 does not feel significant because the FID is still very far from 0."}, "questions": {"value": "1. From Fig.2 I understand the map gets normalized globally, can that impact areas of details with less contrast?\n2. The mask as described progressively vanishes, having more and more 0 as training progresses. What mechanism ensures that no (catastrophic) forgetting is happening for the masked-out points? Does it rely entirely on $l$ being large enough?\n3. In Eq.3 what's the purpose of $1/C$ if $E$ is min-max normalized after?\n4. Have you tried the RMS amplitude $\\sqrt{E(i,j)}$ instead of $E$, given your curriculum is a linear thresholding?\n5. In tables 1 and 2, when comparing to the baselines, do these baselines benefit from scale-consistency VAE fine-tuning?\n6. The scale-consistency VAE fine-tuning seems to originate from previous papers, so I assume it would make sense to make baselines benefit from it unless you consider the scale-consistency fine-tuning part to be your own contribution. Please clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m3wUaWedUV", "forum": "5og80LMVxG", "replyto": "5og80LMVxG", "signatures": ["ICLR.cc/2026/Conference/Submission19510/Reviewer_9QM1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19510/Reviewer_9QM1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951753859, "cdate": 1761951753859, "tmdate": 1762931406040, "mdate": 1762931406040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}