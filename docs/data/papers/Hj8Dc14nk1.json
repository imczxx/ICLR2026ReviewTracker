{"id": "Hj8Dc14nk1", "number": 15429, "cdate": 1758251254001, "mdate": 1763732143125, "content": {"title": "Children's Intelligence Tests Pose Challenges for MLLMs? KidGym: A 2D Grid-Based Reasoning Benchmark for MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) combine the linguistic strengths of LLMs with the ability to process multimodal data, enabling them to address a broader range of tasks. This progression highlights a shift from language-only reasoning to integrated vision–language reasoning in children development. Inspired by the Wechsler Intelligence Scales, we introduce KidGym, a comprehensive 2D grid-based benchmark for assessing five essential capabilities of MLLMs: execution, perception reasoning, learning, memory, and planning. The benchmark comprises 12 unique tasks, each targeting at least one core capability, specifically designed to gauge MLLMs' adaptability and developmental potential, mirroring the stages of children's cognitive growth. Additionally, our tasks encompass diverse scenarios and objects with randomly generated layouts, ensuring more accurate and robust evaluation of MLLM capabilities. KidGym is designed to be fully user-customizable and extensible, allowing researchers to create new evaluation scenarios and adjust difficulty levels to accommodate the rapidly growing MLLM community. Through evaluation of state-of-the-art MLLMs using KidGym, we identified significant insights into model capabilities and revealed important limitations in current status. We release our benchmark at: https://kidgym.github.io/KidGym-Website/.", "tldr": "", "keywords": ["Benchmark", "MLLM", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8315d9ce1e123ebec9adcf577c1dd2efca82e464.pdf", "supplementary_material": "/attachment/68581c2a28a0f1776dfa7588b68e9b9cc34e24da.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces KidGym, a benchmark that assesses five essential capabilities of MLLMs: execution, perception reasoning, learning, memory, and planning. The benchmark is cognitively inspired and uses games as benchmarking tools. The benchmark identifies key limitations of current MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark captures current limitations of MLLMs: namely, in reasoning over nonsemantic, abstract visual information; insensitivity to item quantity; composite capacity tasks involving the interaction of multiple rules; and weak performance in perception reasoning and planning tasks.\n2. The tasks are well designed and include extensive ablations of the environment that allow for more robust evaluation of capabilities. The authors have also made the benchmark customizable to accommodate more tasks, which is a good contribution for future evaluations. The benchmark also varies the level of difficulty for each task type in a nicely controlled way. \n3. The authors provide human baselines to compare with MLLLM performance. \n4. The authors also tested CoT and ICL methods as additional ablations. It is interesting that CoT improves performance for some models."}, "weaknesses": {"value": "1. Minor: Figure one shows the task types in KidGym. It would be helpful to also illustrate how each task type maps to the four capabilities that KidGym assesses\n2. It would be helpful to provide more context for the Wechsler Intelligence Scale. It is unclear where the tasks come from and how they are constructed. \n3. The amount of tasks (despite the ablations of environment and randomness) is quite small compared to other existing benchmarks\n4. The skills required for each task are largely based on intuitions. The mapping from task to skills would benefit from clearer explanations and justifications based on existing literature. \n5. The paper would benefit from more comprehensive error analysis of specific failure cases and whether any patterns can be surfaced from these cases (and providing concrete examples would help the reader get a better sense of the failure modes)."}, "questions": {"value": "1. In the abstract, the authors write that the progression “parallels the developmental trajectory from language acquisition to visual perception in children”. Is this true? Does visual perception occur after language acquisition, or do they develop concurrently?\n2. Table 1 is very helpful for illustrating the contributions of KidGym. However, the dimensions “Difficulty Level” and “Use Extensible” seem more trivial than aspects like “dynamic vs. static”, which seems to be the more important contribution of KidGym. I would suggest revisiting this table to better show what makes KidGym an important contribution and what distinguishes it from existing benchmarks for MLLMs.\n3. Based on the task description, the Sorting task sounds like a linguistic task. Why is it particularly helpful for evaluating MLLMs?\n4. Would be helpful to add the human baseline result to Table 3\n5. Table 3 can benefit from a clearer caption. For example, what does L stand for in the header? What’s 1, 2, and 3? What does the scale represent? Is the number accuracy?\n6. The observation that closed source models outperform open source model is less interesting than comparing general model capabilities, model size, and other factors. Why do you think these models perform better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "atO2aq7kzX", "forum": "Hj8Dc14nk1", "replyto": "Hj8Dc14nk1", "signatures": ["ICLR.cc/2026/Conference/Submission15429/Reviewer_fF2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15429/Reviewer_fF2z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761242267211, "cdate": 1761242267211, "tmdate": 1762925706762, "mdate": 1762925706762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KidGym, a benchmark that assesses five essential capabilities of MLLMs: execution, perception reasoning, learning, memory, and planning. The benchmark is cognitively inspired and uses games as benchmarking tools. The benchmark identifies key limitations of current MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark captures current limitations of MLLMs: namely, in reasoning over nonsemantic, abstract visual information; insensitivity to item quantity; composite capacity tasks involving the interaction of multiple rules; and weak performance in perception reasoning and planning tasks.\n2. The tasks are well designed and include extensive ablations of the environment that allow for more robust evaluation of capabilities. The authors have also made the benchmark customizable to accommodate more tasks, which is a good contribution for future evaluations. The benchmark also varies the level of difficulty for each task type in a nicely controlled way. \n3. The authors provide human baselines to compare with MLLLM performance. \n4. The authors also tested CoT and ICL methods as additional ablations. It is interesting that CoT improves performance for some models."}, "weaknesses": {"value": "1. Minor: Figure one shows the task types in KidGym. It would be helpful to also illustrate how each task type maps to the four capabilities that KidGym assesses\n2. It would be helpful to provide more context for the Wechsler Intelligence Scale. It is unclear where the tasks come from and how they are constructed. \n3. The amount of tasks (despite the ablations of environment and randomness) is quite small compared to other existing benchmarks\n4. The skills required for each task are largely based on intuitions. The mapping from task to skills would benefit from clearer explanations and justifications based on existing literature. \n5. The paper would benefit from more comprehensive error analysis of specific failure cases and whether any patterns can be surfaced from these cases (and providing concrete examples would help the reader get a better sense of the failure modes)."}, "questions": {"value": "1. In the abstract, the authors write that the progression “parallels the developmental trajectory from language acquisition to visual perception in children”. Is this true? Does visual perception occur after language acquisition, or do they develop concurrently?\n2. Table 1 is very helpful for illustrating the contributions of KidGym. However, the dimensions “Difficulty Level” and “Use Extensible” seem more trivial than aspects like “dynamic vs. static”, which seems to be the more important contribution of KidGym. I would suggest revisiting this table to better show what makes KidGym an important contribution and what distinguishes it from existing benchmarks for MLLMs.\n3. Based on the task description, the Sorting task sounds like a linguistic task. Why is it particularly helpful for evaluating MLLMs?\n4. Would be helpful to add the human baseline result to Table 3\n5. Table 3 can benefit from a clearer caption. For example, what does L stand for in the header? What’s 1, 2, and 3? What does the scale represent? Is the number accuracy?\n6. The observation that closed source models outperform open source model is less interesting than comparing general model capabilities, model size, and other factors. Why do you think these models perform better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "atO2aq7kzX", "forum": "Hj8Dc14nk1", "replyto": "Hj8Dc14nk1", "signatures": ["ICLR.cc/2026/Conference/Submission15429/Reviewer_fF2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15429/Reviewer_fF2z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761242267211, "cdate": 1761242267211, "tmdate": 1763756439522, "mdate": 1763756439522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KidGym, a 2D grid-based benchmark for multimodal LLMs (MLLMs), inspired by the Wechsler intelligence scales. It operationalizes five capabilities: execution, perceptual reasoning, learning, memory, and planning, via 12 tasks (6 single-capability and 6 composite), each with three difficulty levels and randomized layouts. Tasks include Classification, Filling, Puzzle, Selection, Decode, Maze, and composite variants like Maze, Decode, Sorting, Filling, Placement, Counting. The environment presents a scene map, hint bar, and backpack with labeled items and exposes high-level actions to avoid low-level control. The authors evaluate seven SOTA MLLMs (closed- and open-source) across 100 zero-shot rounds per task, plus CoT/ICL variants, and provide capability radar plots. Key findings: closed models outperform open models; tasks involving abstract visual patterns, quantities, and composite rules are consistently harder. The benchmark is positioned as customizable (Gym-based) and claims a public release."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear capability decomposition tied to a widely known cognitive framework (Wechsler), with explicit mapping of tasks to abilities. This makes the evaluation scope transparent and multifaceted.\n2. Interactive, dynamic evaluation rather than purely static VQA-style prompts. The environment design (randomized layouts, diverse scenes, identification labels, hint/backpack) is thoughtful and motivated by known pain points (context consistency, low-level action brittleness).\n3. Breadth of models and tasks. Seven representative models, three difficulty levels, and 12 tasks give a reasonably comprehensive picture.\n4. Empirically grounded insights that echo community experience: weaknesses with non-semantic abstract vision (Puzzle), quantity sensitivity (Counting), and multi-rule composition (composites like Maze) are convincingly demonstrated and discussed.\n5. Extensibility claim (Gym API, configurable scenes/difficulty) is attractive for community uptake."}, "weaknesses": {"value": "1. Metric and objective mismatch for “planning.” Maze/MA* are described as requiring “as few steps as possible” (§5.1, p. 6), but the reported results are success rates only (Table 3); there is no path-length/efficiency metric, nor time-to-solve, which weakens the planning claim. Without cost-sensitive metrics (or success under step budgets), planning is only partially measured.\n2. High-level actions may reduce ecological difficulty. By design, the agent executes macros like “pick up the basketball” rather than navigating. This meaningfully simplifies the control problem and can blur construct validity for execution and planning. An ablation showing how conclusions change under finer-grained actions would strengthen the case.\n3. Interpretation of Wechsler analogy may be overextended. The paper frames results as “children’s intelligence tests pose challenges” (title) and borrows the Wechsler taxonomy, but does not provide normative calibration or validated correspondences to psychometric constructs or score scaling. The framing risks being rhetorical rather than psychometrically grounded.\n4. Presentation quality / editing. There are numerous typos and grammatical issues (“capbilities,” “cusomization,” “commodate,” etc.), and a few imprecisions (e.g., “diamond is hidden among several treasure chests” but no precise termination rules)."}, "questions": {"value": "Please see the weaknesses. If the authors can address my concerns listed in the weaknesses,I'd like to raise my evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZAYbAJG0q8", "forum": "Hj8Dc14nk1", "replyto": "Hj8Dc14nk1", "signatures": ["ICLR.cc/2026/Conference/Submission15429/Reviewer_iozr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15429/Reviewer_iozr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851555956, "cdate": 1761851555956, "tmdate": 1762925705565, "mdate": 1762925705565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introducedsa 2D MLLM benchmark KidGym, incorporating abilities based on the Wechsler Intelligence Scale. The abilities include execution, perception reasoning, learning, memory, planning.\n\nThe KidGym benchmark consists of 12 unique tasks (e.g., Classification, Maze, Puzzle, Counting) , each with three difficulty levels. These tasks are designed to assess the five capabilities either individually (6 tasks) or in combination (6 tasks). The environments feature diverse semantic scenes and randomized layouts to ensure robustness and prevent memorization. The benchmark is built on the Gym API, making it extensible.\n\nThe authors evaluates seven state-of-the-art MLLMs on KidGym including three closed-source models (o3, GPT-4o , Gemini-2.5-flash, Claude-3.7-sonnet) and four open-sourced models (DeepSeekVL-2, QwenVL-2.5, and InternVL-3).\n\nTheir experiments reveal significant limitations in current models, particularly in: reasoning over non-semantic visual information (e.g., the Puzzle task), identifying the quantity of items (e.g., the Counting task), dealing with composite tasks that require multiple capabilities (e.g., Maze* vs. Maze)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The benchmark itself is robust. The inclusion of three difficulty levels (L1, L2, L3) effectively demonstrates performance scaling , and the use of randomized layouts prevents models from succeeding via memorization.\n\nKidGym is designed to be fully user-customizable and extensible, and it is built on the standard Gym API. This, along with its public release, makes it a valuable and practical tool for the MLLM community."}, "weaknesses": {"value": "“Single-capability” claim unsubstantiated. The paper asserts six tasks isolate individual abilities but provides no validation (e.g., construct ablations/lesioning). In practice these tasks are compound: Maze (labeled Planning) also needs Perception; Classification (Execution) also needs Perception + Learning. Low scores may reflect hidden prerequisites, not the target skill.\n\nWechsler Intelligence Scales inspiration is superficial. Tasks don’t map cleanly to established Wechsler subtests/constructs, and no psychometric evidence (reliability/validity) is given.\n\nRelated work gaps. Important recent capability benchmarks (e.g., MaRs-VQA in COLM 2025) are missing.\n\nLimited headroom amid fast model progress. Frontier models such as GPT-5 and Gemini 2.5 Pro already approach saturation on several tasks, risking short shelf-life of your benchmark."}, "questions": {"value": "See weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lwVldowMld", "forum": "Hj8Dc14nk1", "replyto": "Hj8Dc14nk1", "signatures": ["ICLR.cc/2026/Conference/Submission15429/Reviewer_nziT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15429/Reviewer_nziT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225004718, "cdate": 1762225004718, "tmdate": 1762925703713, "mdate": 1762925703713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark, KidGym, aimed at evaluating the cognitive capabilities of Multimodal Large Language Models (MLLMs). The inspiration comes from the Wechsler Intelligence Scale for Children. KidGym includes 12 2D grid-based tasks covering five core capabilities: execution, perception reasoning, memory, learning, and planning (Section 3). These tasks are divided into single-capability and composite-capability types, with three difficulty levels (L1–L3), and each round features randomized environments. KidGym is built on the Gym API and supports full customization. The authors evaluate 7 representative SOTA MLLMs (such as GPT-4o, o3, Gemini, QwenVL, InternVL, etc.) and point out that current models still have significant shortcomings in quantity perception, abstract graphic reasoning, and multi-rule composite tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors align the task design with the Wechsler intelligence testing system and the concept of Executive Function from child cognitive psychology (Appendix A.1, Section 3), providing a structurally grounded framework with human cognitive analogy, in line with the current trend in AI evaluation toward psychological testing methods.\n\n2. KidGym evaluates five core capabilities, with 12 tasks of varying difficulty, and distinguishes between single and composite abilities (Section 5), making it more systematic than existing MLLM benchmarks. For example, Table 1 clearly shows KidGym’s advantages in capability dimensions compared to other benchmarks.\n\n3. The benchmark is built using the Gym API, allowing users to create new task scenarios and rules, making it highly extensible.\n\n4. A systematic comparison was conducted on 7 mainstream MLLMs, including both closed-source models (such as o3, Claude, Gemini, etc.) and open-source models (such as QwenVL, InternVL, etc.), and multiple evaluation paradigms were introduced."}, "weaknesses": {"value": "1. Limited novelty of experimental findings: The core findings of the paper—that MLLMs perform poorly in abstract reasoning, precise counting, and multi-step planning—largely confirm known conclusions from other established benchmarks. For example, the difficulty of abstract visual reasoning has already been well documented in benchmarks like ARC-AGI-2 [1]; similarly, the poor performance in the Counting (CO) task is similar to that found in studies [2][3]. Therefore, although the KidGym framework is novel, its final experimental conclusions do not offer significant new insights into the limitations of MLLMs, but rather reaffirm them within a new task set.\n\n2. Limited diagnostic clarity of the benchmark: The paper distinguishes between “Single Capacity” and “Composite Capacity” tasks but does not provide a rigorous methodology for this classification. In Section 5.1, several tasks labeled as “single capacity” clearly involve multiple cognitive abilities according to their own descriptions, yet the paper offers no explanation as to why they are mapped to only one capacity in Table 2. For example, the Puzzle (PU) task, described as “assembling pieces to reconstruct an image,” can reasonably be considered to involve planning, but it is exclusively mapped to perception reasoning, without any explanation as to why other components are excluded. This missing explanation may make the capacity mapping appear somewhat arbitrary, forcing readers to guess the authors’ assumptions and weakening the benchmark’s claim to clearly isolate and measure specific cognitive abilities.\n\n3. Superficial connection between psychometric concepts and actual evaluation metrics: The paper claims to be inspired by the Wechsler Intelligence Scale (Appendix A.1), but the correspondence between these concepts and the actual implementation is weak. For example, the “Execution” ability is explicitly linked to the Processing Speed Index (PSI) in WPPSI-IV, which measures the speed and accuracy of completing simple tasks (Appendix A.2). This analogy seems more like a narrative framing rather than a strictly applied scientific principle.\n\nRef：\n[1]\tChollet F, Knoop M, Kamradt G, Landers B, Pinkard H. Arc-agi-2: A new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831. 2025 May 17.\n[2]\tLi J, Zhang X, Zou H, Guo Y, Xu R, Liu Y, Zhu C, He Y, Cui P. COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts. InProceedings of the Computer Vision and Pattern Recognition Conference 2025 (pp. 9186-9198).\n[3]\tAmini-Naieni N, Han T, Zisserman A. Countgd: Multi-modal open-world counting. Advances in Neural Information Processing Systems. 2024 Dec 16;37:48810-37."}, "questions": {"value": "Here are some concerns I would like to raise. Thanks.\n\n1. Has there been a deeper analysis of the error types made by the models? Besides success rate, were failed cases (e.g., recognition errors, logical confusion, memory lapses) analyzed? This information is crucial for understanding model behavior. Although Section 6.3 mentions that CoT improves Gemini's performance, is there a systematic analysis of the strengths of the three reasoning paradigms (Zero-shot, CoT, ICL) for each capability or task type?\n\n2. In the tasks, which plays a dominant role: visual or language information? Especially in tasks like Sorting or Decode, is the model's success primarily dependent on language understanding (rules, prompts), or on the parsing of visual images?\n\n3. How much does randomized layout affect evaluation results? In Section 4, it is mentioned that each round has randomized layouts. Has the variance in task results been evaluated to verify the robustness of the benchmark?\n\n4. Does model scale lead to linear performance improvement? Is there any observation that larger models perform better across the five capability dimensions? Is there a saturation point for capability scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DsLdJF9Bal", "forum": "Hj8Dc14nk1", "replyto": "Hj8Dc14nk1", "signatures": ["ICLR.cc/2026/Conference/Submission15429/Reviewer_dWpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15429/Reviewer_dWpT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762400002086, "cdate": 1762400002086, "tmdate": 1762925703249, "mdate": 1762925703249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}