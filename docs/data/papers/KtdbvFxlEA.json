{"id": "KtdbvFxlEA", "number": 9387, "cdate": 1758120806454, "mdate": 1759897727923, "content": {"title": "Unlocking Volition: Proactive Intention Decoding via Interpretable Graph Learning of Multi-Region ECoG", "abstract": "Current brain–machine interfaces (BMI), face fundamental limitations due to inherent latency from reliance on delayed motor cortical signals, and computational overhead, restricting their effectiveness in real-time applications such as rehabilitation therapy. Recent neuroscience indicates prefrontal and sensory cortical activities precede motor execution, offering an opportunity for proactive intent prediction. However, challenges remain in acquiring multi-region neural data, efficiently decoding high-dimensional signals, and ensuring model interpretability. To address these, we developed a high-density electrocorticography (ECoG)-based paradigm based on marmosets and introduced an information-bottleneck-driven graph transformer (ECoG-IBGT), reframing neural decoding as graph classification. Our method achieves 99.29\\% accuracy up to 400 ms before action onset with inherent interpretability, laying a foundation for reliable, low-latency BMIs. Code is available at  [*********************************URL Blinded for Review*********************************].", "tldr": "ECoG-IBGT leverages interpretable multi-region ECoG graph representations to anticipate intention (400 ms prior to onset), substantially reducing system-level latency in BMIs.", "keywords": ["NEUROSCIENCE; Cognitive science; ECoG;Non-human primates;Intention prediction"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bfbbc791b9c8584fe2e893712eb1ad6948841245.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors developed a high-density ECoG-based framework on marmosets and introduced an information-bottleneck-driven graph transformer for intention detection. Specifically, subgraphs are generated with mutual information estimation learning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "[1] A high-quality, multi-subject, multi-context ECoG dataset\\\n[2] Comprehensive experiments and evaluations"}, "weaknesses": {"value": "[1] English language – The authors should substantially improve the quality of the manuscript.\\\n[2] Graph transformer – The authors should cite the work when they mention it in Section 3.3.\\\n[3] Math notations – The authors should indicate all the notations in their equations. The authors should substantially improve the manuscript quality.\\\n[4] Motivation – The motivation for applying subgraphs is still unclear."}, "questions": {"value": "[1] Why did the authors apply a graph transformer instead of using the transformer directly?\\\n[2] Why did the authors generate subgraphs? Why don’t they just use the full graph for more training efficiency?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors have affirmed compliance with the ICLR Code of Ethics and applicable institutional/regulatory policies governing research integrity, privacy, and human/animal subjects."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kffzAnQwIe", "forum": "KtdbvFxlEA", "replyto": "KtdbvFxlEA", "signatures": ["ICLR.cc/2026/Conference/Submission9387/Reviewer_xABC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9387/Reviewer_xABC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422510526, "cdate": 1761422510526, "tmdate": 1762920998119, "mdate": 1762920998119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an information-bottleneck graph-transformer for proactive intention decoding from dual-region high-density ECoG. Short pre-onset (vs. rest) windows are converted into functional graphs; the model jointly learns compact node/edge subgraphs for classification and inspection. The writing is clear, and the idea targets lower-latency and more interpretable BMI pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Reframes proactive decoding as graph classification with a learned subgraph (masking) mechanism rather than post-hoc attribution.\n\n2. Quality: Sensible pipeline design with basic sanity/robustness checks around graph construction and perturbation of important connections.\n\n3. Clarity: The core components (graph building, IB/masking, encoder) are explained cleanly with helpful figures.\n\n4. Significance: If generalizable, anticipatory decoding with compact subgraphs could inform low-latency BMI design and yield testable neuro hypotheses."}, "weaknesses": {"value": "1. Main results rely on random within-session splits, which can overestimate performance when near-duplicate windows appear across train/val/test. Cross-session evaluation is not foregrounded, so the true ranking of methods (including simple baselines) under shift is unclear. This is especially important considering Supplementary materials provide cross-session metrics for the main model and they show near perfect 100% metrics for validation (same session), but 80% test metrics (differertn session). Combined with (most likely) a large enough model in terms of trainable parameters and 233 minutes of data across all sessions such overfitting might happen and within-session estimation with random splits makes it invisible (as test and train sampes are mixed and can easily be similar to each other)\n\n2. Learned masks and motif plots are primarily associational; stronger validity checks (stability across seeds/sessions, model-randomization, counterfactual edits) are needed.\n\n3. Runtimes are reported on high-end GPU and do not provide end-to-end CPU latency (including graph building) or an asynchronous detection analysis which is a key for practical BMI.\n\n4. The description around “windows,” graph instances, and “node features” can be misread; rest-window sampling and temporal separation need clearer, leakage-resistant definitions.\n\n5. The paper states code is available as supplementary, but the submission lacks an accessible anonymized repo/archive; this blocks verification."}, "questions": {"value": "1. Please report cross-session performance in the main text for all baselines (incl. EEGNet) with variance across seeds to establish robust ranking.\n\n2. Precisely define rest-window sampling and temporal separation; add blocked/time-shifted splits and leave-session-out/leave-animal-out protocols.\n\n3. Provide mask-stability across seeds/sessions, model-randomization tests, and counterfactual edge/node removals to support causal importance.\n\n4. Report full pipeline latency (acquisition, preprocessing, graph, inference) on CPU-class hardware and an asynchronous detection analysis (e.g., false positives per minute at a fixed TPR).\n\n5. Explicitly state how many graphs are produced per event, how node features are formed, and whether pre-vocal vs. rest graphs are paired or independent. And how many samples are available in total for training and testing splits\n\n6. Supply an accessible anonymized repo/supplement (configs, split scripts, seeds) to reproduce the main tables and cross-session results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHyqTxwhhY", "forum": "KtdbvFxlEA", "replyto": "KtdbvFxlEA", "signatures": ["ICLR.cc/2026/Conference/Submission9387/Reviewer_PwTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9387/Reviewer_PwTa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919657242, "cdate": 1761919657242, "tmdate": 1762920997719, "mdate": 1762920997719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ECoG-IBGT, an information-bottleneck driven graph transformer that converts multi-region ECoG windows into functional brain graphs. It learns a compact behavior-relevant subgraph via node/edge soft masks and connectivity loss, and classifies vocalization vs rest to achieve proactive intention decoding up to 400 ms before vocal onset. Experiments on a high-density dual-region ECoG dataset show very high predictive performance (99.29% accuracy) and interpretable subgraphs implicating fronto-auditory motifs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel framing that reformulates proactive intention decoding as a graph classification problem combined with information bottleneck–based subgraph learning, which aligns well with the characteristics of multi-region ECoG data. The model also emphasizes real-time applicability, reporting low inference latency and highlighting how the graph-based representation can improve efficiency compared to conventional temporal models.\n2. The approach achieves interpretability in a principled way through the use of joint node and edge soft masks, a connectivity loss, and an HSIC-based mutual information term, allowing the model to learn compact and behavior-relevant subgraphs rather than relying on post-hoc explanations.\n3. The dataset is of high quality, featuring dense dual-region ECoG recordings from freely behaving marmosets, which provides valuable experimental data for the field and demonstrates translational ambition toward brain–machine interfaces.\n4. The experiments are thorough, with comparisons against a wide range of baselines and comprehensive ablation studies that clearly demonstrate the contribution of each component in the proposed method."}, "weaknesses": {"value": "1. The dataset includes recordings from only two subjects, which limits the ability to generalize across individuals and raises the possibility that the model might capture subject-specific features related to electrode placement or physiology.\n2. The reported accuracy of 99.29% at 400 ms before vocal onset appears unusually high for anticipatory decoding and may indicate potential data leakage or overly favorable experimental design, particularly if training and test splits were not separated by session or if overlapping windows were used.\n3. The graph construction relies on Pearson correlation and a fixed top-10% edge retention threshold, which may be brittle and potentially encode label-related signal differences, yet the paper provides only limited sensitivity analysis of these design choices.\n4. The information bottleneck regularization weights are extremely small, and it remains unclear whether the IB loss meaningfully influences optimization or whether the model performance is dominated by the cross-entropy term.\n5. The statistical reporting is limited, with only means and standard deviations provided; additional per-subject results, confidence intervals, or p-values for baseline comparisons would strengthen claims of significance.\n6. The reproducibility of results could be constrained by data access limitations, since invasive ECoG recordings in primates often require controlled access. This makes it uncertain whether external researchers will be able to replicate the findings. In addition, the authors could release an anonymous code repository for review.\n7. Some of the biological interpretations may be overstated, as the identified subgraph motifs do not establish causal connectivity, and the electrode coverage is limited to A1 and PFC, leaving out motor areas that are important for volition studies."}, "questions": {"value": "1. Please clarify how the training, validation, and test splits were created. Were these splits stratified by session or randomized across trials? It would be helpful to report per-subject and per-session test performance to demonstrate generalization.\n2. Please describe the window length used to form each graph and indicate whether the windows overlap between trials. \n3. Please report performance separately for each subject to reveal whether the model’s effectiveness is consistent across individuals or dominated by a single subject’s data.\n4. Please provide more details on the perturbation experiments used to validate interpretability. How much performance degradation occurs when top-ranked edges or nodes are removed, and are the identified motifs stable across random seeds or training runs?\n\nOverall, my main concern is that the ultra high accuracy is due to data leakage or unfair experiment settings, since this often happens in this area."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper involves invasive ECoG recordings in non-human primates (marmosets). The authors explicitly state that all animal experiments were approved by their institutional ethics committee. This should be reviewed by the conference."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yCMLqsHTfn", "forum": "KtdbvFxlEA", "replyto": "KtdbvFxlEA", "signatures": ["ICLR.cc/2026/Conference/Submission9387/Reviewer_kzfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9387/Reviewer_kzfm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974078162, "cdate": 1761974078162, "tmdate": 1762920997161, "mdate": 1762920997161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}