{"id": "iOjGMr6Xc1", "number": 12835, "cdate": 1758210716977, "mdate": 1759897482237, "content": {"title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models", "abstract": "Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach, Open-ended Instruction Relabeling (OIR), that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environments, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance the instruction-following abilities through reinforcement learning.", "tldr": "", "keywords": ["Reinforcement Learning", "Instruction Following", "Relabeling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c8e2eec8c02f8f9119853b7a4e74570a2b56901.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Open-Ended Instruction Relabeling (OIR), a framework that leverages large language models (LLMs) to automatically generate new natural language instructions from agent trajectories in reinforcement learning (RL). The key idea is to use LLMs to retroactively relabel both successful and failed trajectories with meaningful instructions that reflect the subtasks the agent implicitly accomplished. This creates a richer and more diverse dataset of instruction–trajectory pairs without human annotations, mitigating sparse reward issues.\nThe method integrates LLM-based relabelling with a prioritised instruction replay buffer and an embedding-based reward function based on cosine similarity. Experiments on the Craftax benchmark demonstrate improved sample efficiency, task coverage, and generalisation to unseen instructions compared to several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear motivation and presentation:** The paper is well written and logically structured, with clear explanations of the method and experimental setup.\n* **Strong empirical results:** OIR achieves notable improvements in sample efficiency, number of tasks completed, and generalisation performance compared to strong baselines.\n* **Comprehensive evaluation:** The authors include detailed analyses across three axes—efficiency, generalisation, and diversity—and conduct ablations on threshold sensitivity, LLM backbone, and buffer sampling strategies.\n* **Technical soundness:** The formalisation of the instruction relabelling process and the integration with prioritised replay seem technically sound. The inclusion of algorithm pseudocode, hyperparameters, and open-source code improves reproducibility.\n* **Practical impact:** The approach offers a scalable way to bootstrap instruction-following agents without human labels, which could be useful for developing autonomous open-ended RL systems."}, "weaknesses": {"value": "* **Limited conceptual novelty:** While the combination of hindsight relabelling and LLM-based generation is interesting, the approach mainly extends existing ideas (HER, LLM-guided labelling, and semantic reward shaping) rather than introducing a fundamentally new principle. The paper’s main contribution lies in integration of existing ideas and empirical demonstration rather than theoretical innovation.\n* **Dependence on LLM quality:** The success of the approach depends heavily on the quality of LLM-generated instructions, as demonstrated in Figure 4. \n* **Single-domain evaluation:** All experiments are limited to the Craftax environment. While it serves as a good benchmark, evaluating OIR on a another domain would strengthen the generality claims.\n* **Auxilary formalism:** Equations (5,6) formalise the notion of a “good” relabelling but are not directly used in the algorithm or experiments; they serve more as conceptual motivation than operational definitions."}, "questions": {"value": "1. What are the reasons for Gemma3-1B-IT relabelling to work significantly worse than the Qwen models? Have you compared qualitatively the generated instructions by the two models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqm7siK51p", "forum": "iOjGMr6Xc1", "replyto": "iOjGMr6Xc1", "signatures": ["ICLR.cc/2026/Conference/Submission12835/Reviewer_Q6Uh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12835/Reviewer_Q6Uh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761054761738, "cdate": 1761054761738, "tmdate": 1762923634619, "mdate": 1762923634619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Open-ended Instruction Relabeling (OIR), which uses LLM-based semantic reasoning to improve training of instruction-following RL policies. The method converts trajectories into textual observations, prompts an LLM to propose instructions implicitly achieved in those trajectories, and then relabels data with these instructions. Rewards for generated instructions are computed via cosine similarity between instruction and state embeddings, with episode termination when the similarity crosses a threshold. Experiments on Craftax-Classic show improved efficiency and generalization over baselines built on PQN and ELLM."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written, well structured, and easy to follow.\n\n- The evaluation explicitly tests generalization to paraphrased and compositional instructions (simple and complex variants), not just the original instruction set."}, "weaknesses": {"value": "- The approach presumes environments that can provide or be mapped to textual observations to prompt the LLM. A clearer statement of the environment class (symbolic / text-describable state, discrete action space, sparse achievements) would help understand the limit of the contribution.\n- Results are reported on only one environment (Craftax-Classic), which limits claims of generality and leaves open whether gains depend on environment-specific engineering.\n- The comparison to only ELLM and PQN is insufficient to measure the method performance. Two categories of baseline are missing: (i) state-of-the-art Craftax agents (see [the leaderboard](https://github.com/MichaelTMatthews/Craftax)), and (ii) LLM/VLM-based methods (both pretrained and RL fine-tuned) such as :\n    - (2024) Tan, Weihao, et al. \"True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning.”\n    - (2023) Zitkovich, Brianna, et al. \"Rt-2: Vision-language-action models transfer web knowledge to robotic control.”\n    - (2022) Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.”\n- The related work should include prior efforts that use LLMs to *generate new instructions for RL training, which are closely related to this paper’s approach. Such as :\n    - (2024) Qi, Zehan, et al. \"Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning.\n    - (2023) Xu, Can, et al. \"Wizardlm: Empowering large language models to follow complex instructions.”"}, "questions": {"value": "- In §4.1 you note LLMs may propose inaccurate/misleading instructions. What fraction of generated instructions are flawed in practice, and how sensitive is training to this rate? Please detail the rule-based instruction filters you add and any safety checks to avoid harmful/detrimental instructions.\n- How many semantically unique instructions are generated during training, and does this number plateau? Can you report a diversity curve (e.g., unique instructions vs. steps) and relate it to performance?\n- What additional compute overhead does your method introduce compared to vanilla PQN?\n- Are the state/instruction embedding functions frozen or trained for Craftax? How reliably does cosine similarity track ground-truth success across varied generated instructions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FF2IMLQ7Fj", "forum": "iOjGMr6Xc1", "replyto": "iOjGMr6Xc1", "signatures": ["ICLR.cc/2026/Conference/Submission12835/Reviewer_dozw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12835/Reviewer_dozw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605577914, "cdate": 1761605577914, "tmdate": 1762923634378, "mdate": 1762923634378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Open-ended Instruction Relabeling (OIR), which leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Based on the general idea of hindsight relabeling, unsuccessful trajectories are relabeled using LLMs by identifying meaningful subtasks the agent has implicitly accomplished. This LLM-based labeling procedure enriches the agent's training data and substantially alleviate the reliance on human annotation. Experiments on Craftax environments demonstrate that OIR improves in sample efficiency, instruction coverage, and success rate compared to some baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and well-motivated. \n\n- The idea of using LLM to label trajectory in RL is well implemented. \n\n- The experimental results are well presented."}, "weaknesses": {"value": "- Leveraging Large Language Models to label data is a very general idea that has been investigated in various domains. This diminishes the novelty of the paper.\n\n- The benchmark is limited to Craftax. Therefore, it is hard to tell the generalization ability of OIR to other environments (especially larger game environments, such as Minecraft.)\n\n- The overall method of OIR looks ad hoc: the prompt, the relabeling of Failed Trajectories, reward definition, etc. Hence, it is necessary to test it on more benchmarks. (the previous weakness)\n\n- the performance of OIR is very sensitive to the cosine-similarity threshold $\\delta$ (Figure 4)\n\n- the effect of the instruction-buffer sampling strategy, i.e., prioritized instruction replay, seem very marginal compared to uniform sampling. (Figure 4)"}, "questions": {"value": "- Is OIR able to generalize to other game environments without much redesign of the different components in OIR? Can you provide some evidence?\n\n- Is there some guideline on how to set the cosine-similarity threshold $\\delta$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p0J6kTes3D", "forum": "iOjGMr6Xc1", "replyto": "iOjGMr6Xc1", "signatures": ["ICLR.cc/2026/Conference/Submission12835/Reviewer_gnxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12835/Reviewer_gnxZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794578042, "cdate": 1761794578042, "tmdate": 1762923634119, "mdate": 1762923634119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Open-ended Instruction Relabeling (OIR), a new method to improve instruction-following policies in reinforcement learning by reducing the need for human-labeled data. OIR uses Large Language Models (LLMs) to retrospectively analyze agent trajectories, including unsuccessful ones, and automatically generate open-ended instructions for subtasks the agent implicitly completed. Evaluated in Craftax environments, this approach enhances training data, improves sample efficiency, and results in a more capable, unified instruction-following policy compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1: The paper presents a novel paradigm for training open-ended instruction-following agents in a clear and logically coherent way. \n\n2: The experimental results clearly demonstrate that the proposed method outperforms the baselines on the majority of tasks. \n\n3: This method significantly improves the model's generalization capability."}, "weaknesses": {"value": "1: The experimental evaluation is conducted in only one environment. It is recommended to further validate the method in more environments, such as the vanilla MineCraft or Robotics.\n\n2: The paper lacks a detailed discussion of the observed performance degradation in the experiments. Is the trade-off between this decline in performance and the improvement in semantic representation truly justified?\n\n3: The scalability of the proposed method has not been discussed.\t\n\n4: Could the authors provide a more detailed analysis of the semantics generated by OIR for instructions that do not correspond to explicit environment achievements? Specifically, what behaviors do these semantics represent, and how does learning such semantics contribute to the performance or generalization ability of a multi-task agent?"}, "questions": {"value": "See in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bddgX0jxOi", "forum": "iOjGMr6Xc1", "replyto": "iOjGMr6Xc1", "signatures": ["ICLR.cc/2026/Conference/Submission12835/Reviewer_PujW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12835/Reviewer_PujW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892472507, "cdate": 1761892472507, "tmdate": 1762923633725, "mdate": 1762923633725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}