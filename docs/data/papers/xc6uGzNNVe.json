{"id": "xc6uGzNNVe", "number": 18613, "cdate": 1758289494362, "mdate": 1759897091784, "content": {"title": "Principal Spectral Regularization Makes Momentum Surpass Adam for LLM Training", "abstract": "Adam has been the most popular optimizer for training deep neural networks for nearly a decade. Recently, Muon, known for its momentum orthogonalization property, has emerged as a strong alternative to training large language models (LLMs). However, is orthogonalization over the whole learning space really necessary, especially given the high computational complexity of Newton-Schulz iteration in Muon? To the best of our knowledge, we are the first to report that Momentum with marginal spectral regularization on very few dimensions can surprisingly surpass Adam. In this work, we mainly made three contributions. First, from spectral visualizations of the LLM training dynamics and the optimization of the Styblinski-Tang function, we observe that the full orthogonalization of the matrix can be suboptimal in some cases. Second, we propose a novel principal spectral regularization (PSR) method that selectively penalizes only the dominant components with computational efficiency. Third, we show that the PSR approach enables SGD with momentum to surpass Adam in pretraining LLMs.", "tldr": "", "keywords": ["Spectral regularization", "Optimization", "Large Language Model (LLM)"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9433178113ffdd962d6798f30ce347f8f9d0c80c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a lightweight spectral method that allows standard momentum (SGD-M) to outperform Adam in large language model pretraining. The authors introduce Principal Spectral Regularization (PSR), which selectively penalizes only a few dominant spectral directions in the momentum, avoiding the heavy computation of full orthogonalization used by Muon. Experiments on LLaMA models show that PSR achieves similar or better performance than Adam and even approaches Muon’s efficiency, while using only about 2% of its additional FLOPs"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea is well motivated and solution is clear and straightforward"}, "weaknesses": {"value": "1. Figure 3 the experiments are not trained to Chinchilla optimal, only 10K steps where perplexity is relatively high. Recent paper [1] has shown a lot of the optimizer papers' claim or not necessarily solid under closer inspection.\n2. Figure 4 shows on 1.3B, PSR is strictly worse than running Muon.\n3. No wall-clock improvement shown in real training runs. The series of operations proposed is not necessarily faster than matrix multiplication on GPU.\n[1] Wen, Kaiyue, et al. \"Fantastic pretraining optimizers and where to find them.\" arXiv preprint arXiv:2509.02046 (2025)."}, "questions": {"value": "1. Has learning rate and other hyperparameters been swept? If it has, can you specify the methodology you use to decide them (3.0 × 10−4, and 0.1)?\n\n2. Setting the rescaling factor to 0.18 seems odd, since only the top directions are normalized, which means the overall update is in general smaller than Muon, where Muon has all eigenvalues set to 1? Shouldn't it be a larger scaling factor than 0.2 instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRfSDocroe", "forum": "xc6uGzNNVe", "replyto": "xc6uGzNNVe", "signatures": ["ICLR.cc/2026/Conference/Submission18613/Reviewer_yyW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18613/Reviewer_yyW5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537308574, "cdate": 1761537308574, "tmdate": 1762928328424, "mdate": 1762928328424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduced Principal Spectral Regularization (PSR). It changes the standard momentum updates to focus only on a few dominant spectral directions. The authors have shown that using PSR and SGD with momentum can have a better performance than Adam in LLM pretraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The strengths of this paper are summarized as follows:\n\n1. The authors have given strong empirical evidence, showing a consistent improvement over AdamW. The authors have tested the LLaMA architecture with multiple sizes, 350M, 1B, 3B, and 7B parameters. It covers both small and large models.\n\n2. This paper is easy to follow and easy to understand. LLM pretraining is an important area of research, and I believe that this paper will be interesting to the machine learning community."}, "weaknesses": {"value": "The weaknesses of this paper are summarized as follows:\n\n1. This paper lacks a theoretical analysis. The paper does not include a formal and rigorous convergence proof. The analysis is mainly empirical. It might not be so convincing whether or not the empirical performance can be generalized to other models.\n\n2. For the experimental results, this paper lacks evaluation on a more diverse architecture. This paper only focuses on the LLaMA model and the C4 dataset. Also, it would be better to include an ablation study on the number of spectral components per layer. \n\n3. Another minor comment is that the authors wrote “Fig. ??”. I think this should be a typo, and the author wants to refer to Figure 1?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfoNcpWYwO", "forum": "xc6uGzNNVe", "replyto": "xc6uGzNNVe", "signatures": ["ICLR.cc/2026/Conference/Submission18613/Reviewer_we5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18613/Reviewer_we5J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692995007, "cdate": 1761692995007, "tmdate": 1762928328042, "mdate": 1762928328042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes principal spectral regularization for optimizer updates, arguing that attenuating only the few strongest momentum directions can suffice for effective LLM training. It claims that full momentum orthogonalization, as in Muon, is often unnecessary and may be suboptimal for LLM pretraining given the computational cost of Newton–Schulz and potential amplification of tail noise. The evidence combines spectral analyses, a controlled function-optimization study, and LLM pretraining experiments. In these settings, momentum with principal spectral regularization matches or surpasses Adam while requiring substantially less compute than full orthogonalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important question with strong motivation. If gradients have a spiked-head, heavy-tail spectrum, then shrinking only the dominant directions is a natural way to improve stability without over-regularizing everything.\n\nAdam, Muon, and momentum are framed as choices about how much to flatten the update spectrum. Adam and Muon make updates more uniform across directions. Because LLM gradients and momenta show a spiked-head, heavy-tail spectrum, the idea is to regularize the spike and leave the tail. That balances exploration of rare directions with stability without paying the full cost of orthogonalizing the whole space. The proposed principal spectral regularization selects a small set of dominant directions and shrinks them. Conceptually, it keeps the informative tail intact while curbing outsized directions, which the paper argues is the sweet spot for LLM pretraining. It sits between “penalize only the very top” and “orthogonalize everything,” and in practice aims to capture much of Muon’s benefit at far lower cost."}, "weaknesses": {"value": "While the paper is interesting and offers useful insights, it has notable weaknesses. The positioning within the broader optimization literature is unclear at several points, and connections to related work and established insights are often underdeveloped.\n\n## Novelty\n### Spectral Distributions\nHeavy-tailed spectral distributions in deep networks are well documented. Multiple works report spiked-head, heavy-tail structure for gradients and Hessian surrogates, e.g., [1]. Other papers motivate low-rank or structured preconditioning consistent with that view, including [2], which motivates SOAP and shows that Shampoo’s preconditioner is effectively low rank. There is also theory showing that using leading eigenvectors can improve convergence when top eigenvalues dominate compared to unpreconditioned gradient steps [3]. \n\n\n### The claim that partial head shrinkage can beat both unregularized momentum and full uniformization is incremental and expected\n\nGiven this background, the claim that suppressing only the principal directions reads as incremental rather than surprising.\n\nMany popular optimizers can be viewed as preconditioned gradient methods that reduce spectral anisotropy, e.g. written as $P^{-1}g$ where $P$ approximates the Hessian. In the eigenbasis of $P$ \nhigh‑curvature directions are down‑weighted and low‑curvature directions are up‑weighted, which reduces anisotropy and pushes updates toward a more uniform spectrum.  This includes methods like K‑FAC, Shampoo, SOAP, Muon etc. Full‑matrix preconditioners explicitly rescale along all directions and approach a whitened or near‑uniform spectrum in their working basis.  Assuming heavy-tail distribution, the idea to tamp only the largest spectral components of momentum and keep the tail largely intact is an unsurprising middle ground between unregularized momentum and fully uniform updates.\n\nSo the high‑level claim — that partial head shrinkage can beat unregularized momentum and sometimes beat full uniformization — reads as an incremental twist on standard preconditioning rather than a surprising new phenomenon. The novelty is mainly in the specific, cheaper mechanism they propose for selecting and shrinking principal directions. \n\n## Limited theory about when head‑only shrinkage should win\n\nThe central claim that shrinking only the dominant directions improves optimization lacks formal support. The paper walks a line between two motivations for PSR, one about compute efficiency and one about optimization behavior. On compute, the method replaces Newton–Schulz which in theory cuts extra FLOPs. On optimization, the method aims to \"avoid amplifying noise from full uniformization\" by shrinking only the spiked head while keeping the heavy tail largely intact. I agree with the first motivation because the complexity accounting is explicit, but I am not convinced by the second motivation as a general rule. The importance of small eigen directions is an open question, since those directions may correspond to hard to learn skills that still move the loss, for example rare token handling, long range interactions, niche syntactic transforms, or calibration corrections. Second order methods purposely lift such directions by whitening or preconditioning, which increases their relative step size and can help learn these skills, while the proposed method leaves the tail unnormalized and could under weight these signals when they are not noise. The evidence offered suggests the mechanism but does not isolate it or directly measure noise amplification across regimes, which matters because this mechanism is exactly what differentiates PSR from second order approaches.\n\n\n\n## Heavy reliance on a toy problem to motivate the choice.\n\nThe Styblinski–Tang study is useful for illustrating the mechanism, but it is weak as a generalization argument. Section 3.2 constructs a power-law weighting over coordinates and adds Gaussian noise each step to emulate a spiked head with a noisy tail. In that setting, fully uniform updates will also uniformize the noise, while partially shrinking the head improves the signal-to-noise ratio. The observed sweet spot in (K) is exactly what a bias–variance tradeoff in an anisotropic, noisy problem would predict. This clarifies the intuition, but it is not evidence that the same tradeoff holds across deep networks and training regimes. A brief discussion of external validity and why the toy’s assumptions map to real LLM training would strengthen the paper.\n\n\n\n\n\n## Typos\nThe paper contains many notational inconsistencies and typos, which make it harder to read. For example, “Moun” instead of “Muon”, dimension symbols and errors in lines 86, 165, 284 and many more.\n\n\n[1] Zhao, Jiawei, et al. \"Galore: Memory-efficient llm training by gradient low-rank projection.\" arXiv preprint arXiv:2403.03507 (2024).\n[2] Morwani, Depen, et al. \"A New Perspective on Shampoo's Preconditioner.\" arXiv preprint arXiv:2406.17748 (2024).\n\n[3] Doikov, Nikita, Sebastian U. Stich, and Martin Jaggi. \"Spectral preconditioning for gradient methods on graded non-convex functions.\" arXiv preprint arXiv:2402.04843 (2024)."}, "questions": {"value": "Please address my concerns above.\n\nThe paper contrasts PSR primarily with Muon’s Newton–Schulz orthogonalization, but the paper would be stronger if it also compared against SOAP. Because the claim centers on outperforming full uniformization at lower cost, including baselines that operationalize “target the principal directions” would sharpen the argument. SOAP is especially relevant: it explicitly leverages an approximate eigenbasis and concentrates computation on the leading directions, aiming to capture most of the benefit without fully uniformizing the spectrum."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IqWZDVaxRE", "forum": "xc6uGzNNVe", "replyto": "xc6uGzNNVe", "signatures": ["ICLR.cc/2026/Conference/Submission18613/Reviewer_5xga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18613/Reviewer_5xga"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844956746, "cdate": 1761844956746, "tmdate": 1762928327581, "mdate": 1762928327581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes principal spectral regularization (PSR), where only a few dominant singular vectors and their corresponding eigenvalues are penalized. This can be viewed as a partial whitening operation compared to the full whitening from Muon. The author motivate this partial operation by optimizing a toy function and vary the whitening ratio to see the effect to convergence, and discovery that by only whitening a few dominant ones can maintain most of the gains from Muon. Based on this observation, the author proposes a scheme to efficiently perform partial whitening, and evaluate this approach using LLaMA and C4 dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of this paper is clear and intuitive. The proposed method seems to be valid, and the motivation is clear. Since the proposed approach aims to reduce the computational cost of Muon (is it really that expensive??), this may be of interests to practitioners with less compute power."}, "weaknesses": {"value": "One concern I had is the contribution of this work is kind of incremental. The partial whitening of dominant eigenvectors is not too hard to think of, since one of the intuition of Muon is that it treats each components of momentum equally, and thus may overcome some bad minima during the optimization. So it is not hard to notice that instead of boosting all singular values, one can also reduce the maginitude of the top singular ones, after normalization, this should have similar effect as Muon. The proposed partial whitening approach is also not completely novel, which is a variant of classic block subspace iteration algorithm to find low-rank top eigenvectors. Despite that, it is still valuable to see this is confirmed, but not sure if this is significant enough for a publication. \n\nSecond, is NS step of Muon that expensive? Kimi utilizes Muon to train 1T model, meaning that under the distributed setup, the computational cost of Muon is manageable. This is because one cannot just use the FLOPS to determine how costy it is in practice, since NS relies on matrix multiplication, which is highly optimised and potentially be distributed a well. But from some operations, like matrix decomposition (e.g. QR), despite it may have lower FLOPS for smaller matrix, I wonder is it possible to write a customized kernel that is as efficient as matrix multiplication? Current QR uses CuSolver, which is based on efficient HouseHolder reflector methods. And QR decomposition is not as easily distributed as NS, despite that distributed QR algorithm exists. Can you offer a wall-clock time comparison, for huge model/matrix?"}, "questions": {"value": "1. For empirical evaluations, it would be interesting to see the over-train setup (36B with 1.3B model is not overtrain, since it is only 1.4x compute optimal). Maybe 1.3B with over 2-3x compute optimal?\n\n2. Wall-clock time comparison with NS.\n\n3. From my past experience, LLaMA with C4 dataset can be strange sometimes, i.e. some tricks is useful for this setup but fails on the other. Can you also do a similar test with 1.3B NanoGPT, openwebtext dataset and similar context length and batch size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YJFGKuRLa", "forum": "xc6uGzNNVe", "replyto": "xc6uGzNNVe", "signatures": ["ICLR.cc/2026/Conference/Submission18613/Reviewer_q6g2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18613/Reviewer_q6g2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914103133, "cdate": 1761914103133, "tmdate": 1762928327161, "mdate": 1762928327161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper observes that momentum updates exhibit a spiked-head-long-tail spectral distribution, meaning only a few dominant singular directions strongly influence parameter updates. Based on this observation, the paper proposes Principal Spectral Regularization (PSR), which penalizes only the top-K dominant spectral components using a lightweight block Lanczos method, instead of performing full momentum orthogonalization as in Muon. Experiments show that PSR enables SGD with momentum to outperform AdamW and match Muon’s performance while requiring less than 2% of Muon’s computation cost. The contribution lies in demonstrating that full orthogonalization is unnecessary and that selectively regularizing dominant spectral directions yields better efficiency–performance trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Spectral analyses reveal a “spiked-head + long-tail” structure in momentum, helping explain why selective regularization works and providing interpretability.\n2. Instead of fully orthogonalizing momentum like Muon, the paper regularizes only the dominant singular directions, greatly improving efficiency with <2% of Muon’s compute cost.\n3. The method is tested both on a mathematical function optimization and on real LLM pretraining, demonstrating both theoretical soundness and practical effectiveness."}, "weaknesses": {"value": "0. Writing: The manuscript has several presentation issues that should be addressed before publication. For instance:\n\n    A. Multiple typos of the method name: PSR is incorrectly written as PRS in several places (e.g., Line 374/377 and Appendix).\n\n    B. Line 165, the reference to figure is not well posted in Latex.\n\n    C. Line 281, Algorithm 1, the mentioned function BlockBiDiagnal is not defined. Does it refer to the defined function BiDiagnal?\n\n    D. Section 3.2, the definition of d is inconsistent and confusing. Sometimes it refers to the dimension in Styblinski-Tang function (overlapping with n), as in Line 222 and the “d=1024” in the caption of Table 1. Sometimes it refers to the number of penalized dimension (overlapping with K) as in the first line of Table 1 and in the end of Table 1’s caption.\n\n    E. The caption of Table 3, “consistently” is misleading because AdamW performs better in some cases. Best/second-best highlighting is recommended for readability.\n\n1. The results in Table 2 are based on a single random matrix A. Running multiple trials and reporting the average with standard deviation would increase statistical reliability.\n2. The downstream verification of models is only based on LLaMA-1.3B. Evaluating additional model scales would strengthen the generality of the claim.\n3. Including a wall-clock time comparison would further substantiate the claim regarding PSR’s efficiency. In addition, Muon has a FLOP overhead less than 1% compared to the forward-backward phase, which implies that its computational overhead may not be the main bottleneck. Since PSR shows noticeably lower performance than Muon and the improvement in training efficiency is not clearly demonstrated, the practical impact of PSR may appear less compelling.\n4. Whether PSR is threshold-based or proportion-based (and the corresponding proportion) for LLM pretraining is not reported. Without this, results cannot be reproduced."}, "questions": {"value": "1. Issues mentioned in C and D of Weakness 0 need clarification.\n\n2. In Figure 3(c), some curves appear segmented in the initial stage, while others are smooth. This seems to be caused by reporting the early-stage results at a coarser interval. Could the authors clarify why the initial phase is plotted with a larger reporting interval and whether this affects the interpretation of convergence behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sud5IcJwUZ", "forum": "xc6uGzNNVe", "replyto": "xc6uGzNNVe", "signatures": ["ICLR.cc/2026/Conference/Submission18613/Reviewer_5gZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18613/Reviewer_5gZ8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917160846, "cdate": 1761917160846, "tmdate": 1762928326691, "mdate": 1762928326691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}