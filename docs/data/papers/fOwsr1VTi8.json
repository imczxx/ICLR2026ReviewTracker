{"id": "fOwsr1VTi8", "number": 22334, "cdate": 1758329719382, "mdate": 1759896871891, "content": {"title": "DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights", "abstract": "Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present \\ours, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by \\ours do not require fine-tuning to perform well and can scale to very large networks. We apply  Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient generation of diverse sets of large neural networks.", "tldr": "", "keywords": ["metanetworks", "deep weight space", "parameter space symmetry", "flow matching", "canonicalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a4fa252d1f8d914ebf06fd8d06584736848a3a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission presents a flow-based method for neural network weight generation. The authors identify generation quality, speed and diversity as key metrics to improve. Their proposed method extends existing work with a flow-based generation method, that demonstrates high quality of generated weights, on par with the original weights without the need for fine-tuning at considerably faster training and generation times. The authors also successfully make use of model canonicalization and PCA to reduce the complexity and size of the domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The topic of the submission is a highly relevant topic, fast and high fidelity generation remains unsolved. The application of flows on weights is well motivated as a match for high fidelity in high dimensions at relatively high speed. \n- The experimental evaluation show that the proposed method does indeed generate weights that perform very well, to the level of the original models. At the same time, training the flow models appears to be significantly faster than previous work. While these numbers are hidden in the appendix, they present a significant improvement in training speed (and potentially generation speed) over previous work. The experiment setup here is not entirely clear to me, but if this holds up in practice it seems a big deal for weight generation. For that reason I'm somewhat baffled the authors chose not to highlight it.\n- I particularly appreciated the evaluation on canonicalization, in which regimes it has an effect and where it seems to diminish. \n- The authors make an effort to review the related work and contextualize their method well."}, "weaknesses": {"value": "While I overall like the proposed method, and it seems to address limitations of previous work, mostly in quality and speed of generation, there are weaknesses that prevent me from scoring the submission higher at this point. I encourage the authors to address them, and am willing to improve my score. \n\n- Quality, model size and generation speed are relevant problems, and need to be considered in new weight generation methods. The experiments focus on evaluating original vs generated model performance as a proxy for quality, where previous works like D2NWG or RPG [2, 3] appear to perform similarly. The authors further argue with differences in the experimental setup or generation speed, but I could only find generation times for RPG and DeepFlowNets in the Appendix. This comparison appears to take RPG generation times and extrapolate by 100. I can't find any reference numbers to match the author's, so I would ask them to make their calculations transparent. I understand there is an argument to be made for parallelization, but I in general don't see the value of generating 100 models for the same task. If parallelization is the strength here, then I'd ask the authors to add the 1 model vs 100 model generation to extract that signal cleanly.  \n- As the authors note, experiment details matter for parameter synthesis and have made comparisons in previous work challenging. Therefore I would ask them to make their experimental setup clearer. On what and how many model checkpoints is DeepWeightFlow trained? What varies between the checkpoints in the dataset per experiment? Is there one global flow model, or one model per experiment? \n- Where and how is PCA used exactly? If using PCA is the main driver for training and generation speeds, I'd encourage the authors to run ablations so that readers understand where the speed improvements come from. If - as claimed - PCA provides a general compression of significant quality, then improvements should carry over to other methods. That said, previous work using auto encoders generally only achieved high compression ratios for small datasets or datasets with relatively low amount of diversity (shared seeds, etc), so I'm somewhat skeptical that linear compression can achieve as much. \n- The authors use the IoU to show diversity. However, it seems from their experiments that the generated models are substantially less diverse than the original models. While it's in the same diversity range as previous parameter generation methods [1], the experiments aren't directly comparable. Notably, the diversity trends from original to generated are reversed. This does not need to be an issue, but if the authors aim at diversity, this requires some discussion. Also, the authors chose to show diversity on small MLPs and MNIST. Since scalability is one of the main arguments, I encourage the authors to evaluate diversity on larger models and harder tasks.\n- Ultimately, the goal for weight generation has to be to go beyond the original models. This can mean generalization beyond the original data, but might also be a recombination of dataset/architecture/size/knowledge. Here, the contribution is rather narrow. Other work on weight generation has trained on diverse model datasets to cover a broader distribution or introduced generation conditioned on data [3, 4, 5]. I would encourage the authors to discuss how their model helps make progress towards real-world applications.\n- Minor issues in the related work: Permutation symmetries have been used as data augmentations for general in [6, 7]. Graph-based models are used successfully in the INR community, but [2,8] aren't graph auto encoders as far as I can tell. Neither seem to use any connectivity information.  \n- NIT: Commonly, hypernetworks mean generator-only models that are trained to synthesize the weights of a target model using target model signal. That seems different from the representation learning methods that operate on weights and only use weight signals. \n\n[1] Wang et al, 2024: Neural Network Diffusion.\n[2] Wang et al., 2025: Scaling Up Parameter Generation: A Recurrent Diffusion Approach.\n[3] Soro et al, 2024: Diffusion-Based Neural Network Weights Generation.  \n[4] Falk et al., 2025: The Impact of Model Zoo Size and Composition on Weight Space Learning.\n[5] Falk et al, 2025: Learning Model Representations Using Publicly Available Model Hubs.  \n[6] Schuerholt et al, 2021: Self-Supervised Representation Learning on Neural Network Weights. \n[7] Peebles et al, 2022: Learning to Learn with Generative Models of Neural Network Checkpoints. \n[8] Schuerholt et al, 2022: Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3C95xfoVZI", "forum": "fOwsr1VTi8", "replyto": "fOwsr1VTi8", "signatures": ["ICLR.cc/2026/Conference/Submission22334/Reviewer_xfHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22334/Reviewer_xfHk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647846653, "cdate": 1761647846653, "tmdate": 1762942173946, "mdate": 1762942173946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepWeightFlow, a flow-matching approach that generates complete neural-network weights. Using optional Incremental PCA for tractability, it scales from small models to architectures with roughly 10M parameters, delivering performance that matches or exceeds prior methods on the same benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\n\n* Direct Flow Matching in weight space (faster than diffusion).\n* Symmetry-aware via Git Re-Basin / TransFusion.\n* Lean scaling with (Incremental) PCA no learned autoencoder.\n\n**Quality**\n\n* Solid results across MLP/ResNet/ViT and datasets.\n* Clear ablations (capacity vs. canonicalization; source distribution).\n* Practical touches (BN stats recalibration) that boost reliability.\n\n**Clarity**\n\n* Well-structured, accessible explanations\n\n# Significance\n\n* Large efficiency gains: Cuts weight generation from hours to minutes, making high-quality ensemble creation computationally practical.\n* Better starting points: Provides strong initializations that accelerate fine-tuning and enhance transfer learning.\n* Scalable foundation:  A robust, scalable framework for generative modeling in weight space that can serve as a strong baseline for future,extensions."}, "weaknesses": {"value": "* **Unconditional generation limits controllability:**\n  The method’s unconditional design restricts its ability to generalize or transfer across datasets and architectures. While presented as “data-free,” this property also constrains the model’s flexibility\n\n* **Inability to jointly encode across datasets:**\n  Because the generator is unconditional, it cannot represent or sample from mixtures of weights trained on diverse datasets. Each dataset requires its own independently trained flow.\n\n* **Overstated scalability claims:**\n  The paper refers to networks of roughly *10M parameters* as “large-scale.” While this is nontrivial, it falls short of the scales (hundreds of millions to billions of parameters) typically considered large in contemporary neural network research, leaving true scalability uncertain.\n\n* **Unclear handling of label or output-size mismatches:**\n  When transferring to datasets with a different number of output classes, it is not specified how the method manages mismatched output dimensions or classifier heads. This omission raises questions about generalization to tasks with differing label structures."}, "questions": {"value": "For more question see weaknesses\n\n1.  **Transfer Learning Baseline:** Why does Table 5 not include the transfer performance of the original models from your training set as a baseline? This would clarify if DeepWeightFlow generates models with inherently better transferability.\n\n2.  **Scaling to Billion-Parameter Models:** How do you foresee the IPCA-based approach scaling to billion-parameter models? Is a single linear projection via PCA sufficient and computationally tractable for such a high-dimensional and complex weight space?\n\n3.  **Rationale for Unconditional Generation:** What was the motivation for focusing on unconditional generation? What are the main challenges you see in extending DeepWeightFlow to a conditional framework for sampling models with specific attributes?\n\n4.  **Generalization Beyond Classification:** Could you comment on the expected challenges of applying DeepWeightFlow to tasks beyond classification (e.g., in NLP or regression)? Would the current canonicalization methods be sufficient for the different symmetries in those domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e7mtPgLItu", "forum": "fOwsr1VTi8", "replyto": "fOwsr1VTi8", "signatures": ["ICLR.cc/2026/Conference/Submission22334/Reviewer_mm9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22334/Reviewer_mm9F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736191493, "cdate": 1761736191493, "tmdate": 1762942173521, "mdate": 1762942173521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an approach to generating complete sets of neural network parameters using an MLP generator trained via a flow matching objective. Training data was obtained purely by training many separate models on datasets like MNIST, CIFAR10, SVHN, etc. The proposed method is trained to generate weights for different architectures including MLPs, ResNets, and ViTs. \n\nSpecifically, this method uses incremental PCA to reduce the effective dimensionality of the target weight set, then performs flow matching in that space, followed by an inverse transformation (of the pca reduction) to go back to the original weight space. Permutation symmetry is addressed via cannonicalization using appraoches such as git re-basin and transfusion. \n\nThe approach is evaluated on the above datasets for in-distribution performance, as well as oof performance on CIFAR10 -> SVHN and STL-10. The diversity of generated samples is evaluated in terms of their predictive behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* I appreciate that the approach is a sensible way of handling the high dimensional weight space i.e. iterative pca -> flow matching. \n\n* I found the paper well-written and easy to follow\n\n* I am impressed that this work is able to handle complete weight sets even with more complicated architectures like ViTs, and those that include batchnorm. \n\n* The results of this work are compelling compared to other previous work like p-diff and flown."}, "weaknesses": {"value": "* I'm confused about the effect of cannonicalization. Ostensibly it makes training easier but the results shown in figure 2 don't seem to reveal any difference in final test accuracy of generated samples. What is the effect of not using the (computationally expensive) process)?\n\n* I think diversity could be addressed in more than just behavioral statistics. We might also want to know about the average L2 distance between generated and training networks. We might also want to see if for any (or the best) generated network, is there an exact or near-exact copy of it in the training corpus. These datasets are often quite small and memorization is common here."}, "questions": {"value": "1) How does this method scale in terms of absolute VRAM numbers? This is usually the limiting factor with these weight generation methods. At some point we need to do a matrix multiplication where one dimension is the full parameter vector. But with this incremental PCA approach scaling might be better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6WNgGD9oBp", "forum": "fOwsr1VTi8", "replyto": "fOwsr1VTi8", "signatures": ["ICLR.cc/2026/Conference/Submission22334/Reviewer_ZyLw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22334/Reviewer_ZyLw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962535630, "cdate": 1761962535630, "tmdate": 1762942173311, "mdate": 1762942173311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents a novel approach for weight generation capable of assembling full neural network models from a diverse set of neural network model checkpoints. This is accomplished using a flow matching approach, which is coined \"DeepWeightFlow\" by the authors. This approach works directly in the weight space and employs model alignment techniques such as Git Re-basin or TransFusion.\n\nExperimental results are reported on models trained on tabular and computer vision datasets. In-distribution evaluations are done against SOTA approaches, where each SOTA approach is trained on ResNet-18 models trained on CIFAR-10, aiming to sample complete ResNet-18 models for CIFAR-10. The same is done for ViT models trained on CIFAR-10 dataset. Out-of-distribution evaluations are done on models trained on multiple datasets, but only for the presented approach and not compared against SOTA approaches except FLoWN. Multiple ablation studies are reported to understand what impact different components of the presented approach have on sampling performance.\n\nI very much like the presented work and think that the in-distribution results are quite impressive, since it evaluates the generated model's performance without fine-tuning. This is very impressive given the diverse model collection used for training of the presented approach. This is particularly important since some SOTA approaches like RGP do generate weights that are similar to the very homogeneous model collection they have been trained on, and break when randomly initialised models are used for training. I truly appreciate the author's sensitivity to this detail in their experimental setup (line 200-202, line 248-251). \n\nRegardless of this, for this submission, I have some concerns given the experimental setup, which are (i) the limitation to only one computer vision dataset in the comparison against other SOTA approaches (Table 2) and missing broad evaluation of other SOTA approaches in their out-of-distribution evaluation (Table 5). I will outline all details in the strengths, weaknesses, and questions section below.\n\nOverall, I think this work provides a sufficient level of novelty and contribution to be accepted, but I would like to wait to have my questions and concerns addressed, i.e., I would be willing to increase my rating during the discussion with the authors."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1)**: The proposed method is able to generate neural networks that do not require any further fine-tuning. I think this is impressive, given the reported performance being close to the original performance of the model dataset. I am speaking specifically about the results in Table 2 and Table 3. I would have some questions regarding generalizability, though. These are outlined in the questions section.\n\n- **(S2)**: I very much appreciate the emphasis of this work on diversity in the model dataset being used for training. I think this is important for the entire model weight generation community. Speaking about this, I think that the results in Table 2 are a little bit misleading as the numbers reported by Wang et al 2025 (=95.1) have been generated by finetuned trajectories of the same seeded neural network model (this is already mentioned multiple times by the authors in the submission). Having said this, I think that the results of this work (=93.47) are more impressive since they are generated using a much more difficult setup, i.e., a more diverse model dataset for training, as the results from Wang et all 2025 (=95.1). I am not sure about the results of Soro et al (2025), to the best of my knowledg,e they only partially generate ResNet weights and do not fully generate a ResNet model. Finally, the results of Schuerholt et al (2024) also have some leakage of information as they use samples from the target model dataset to fit their KDE for sampling.\n\n- **(S3)**: I also appreciate additional results reported on the diversity of generated models and sampling efficiency. This provides some additional information contextualizing the proposed method.\n\n- **(S4)**: It is great to see a reproducibility statement in this work. The weight generation community needs more of this.\n\n- **(S5)**: This paper is a nice read since it is easy to read and follow."}, "weaknesses": {"value": "- **(W1)**: As mentioned above, in Table 2, I would like to see more comparisons beyond CIFAR-10 results. I think that might provide a more general impression of the proposed methods' performance.\n\n- **(W2)**: In the same sense, for Table 5, I would like to see comparisons against other SOTA methods. This would help to understand the proposed method's capabilities in the context of related work.\n\n- **(W3)**: Since the authors additionally claim that the proposed method is able to scale, I would love to see some scalability ablation results going beyond ViT-Small-192."}, "questions": {"value": "I have some questions:\n\n- **(Q1)**: This is a short one. Why does the paper mention that the ResNet-18 transfer learning experiments are done using PCA as preprocessing (Section 5.2), and for the in-domain experiments, this is not mentioned (Section 5.1)?\n\n- **(Q2)**: The proposed approach works directly in weight space or in PCA-reduced weight space (as far as I understood it). The paper also mentioned that sampling can be done \"without the requirement of additional conditioning during training or inference\" (line 308-309). How do you make sure the method does not overfit to the given training data? Or said the other way around: how homogeneous does your model dataset need to be to perform well?\n\n- **(Q3)**: In a similar spirit, I see that your model dataset is trained only with one specific set of hyperparameters, which is different to, e.g., the model zoos [1] used in some other work. Don't understand me wrong, I appreciate the focus of this work on diversity (wrt. seeds) of your model dataset, but I would be interested in your thoughts on choosing it this way. [1] Model Zoos: A Dataset of Diverse Populations of Neural Network Models, NeurIPS, 2022\n\n- **(Q4)**: What exactly is used for training the underlying FM backbone? The model dataset for training is outlined nicely in the appendix, but how many checkpoints and which checkpoints are you using for the training of your approach (maybe I missed it)\n\n- **(Q5)**: Table 9 in the appendix shows the number of parameters of the model datasets. Why is the ResNet-20 (0.27M parameter) smaller than the ResNet-18 ()11.2M parameter? I guess this is a typo, right?\n\n- **(Q6)**: I would be wondering if the proposed method is able to generalize beyond one image dataset, i.e, what would happen if the proposed method were trained on model datasets trained on CIFAR-10 and FashionMNIST and MNIST. Would this help in transfer learning scenarios?\n\n- **(Q7)**: Same question wrt.. the architecture. Is the proposed method able to generalize to multiple architectures? I mean not to train one FM model per neural network architecture, but to train the proposed methods with multiple architectures. Similarly, is the proposed method able to generate neural networks of different architectures with the same learned FM model?\n\n- **(Q8)**: I think that the result reported for Soro et al (2025) should be moved to partially generate ResNet model weights and not fully generated ResNet model weights. Please correct me if I am wrong.\n\n- **(Q9)**: Finally, you provide efficiency results for RPG but not for D2NWG. Both provide code for their method. Is there a reason why you do not provide numbers for D2NWG?\n\nAs I already said, I think this is fascinating, and I would be happy to increase my rating once I fully understand the generalizability capabilities of this approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P43gS53XAo", "forum": "fOwsr1VTi8", "replyto": "fOwsr1VTi8", "signatures": ["ICLR.cc/2026/Conference/Submission22334/Reviewer_8Vjc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22334/Reviewer_8Vjc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998295400, "cdate": 1761998295400, "tmdate": 1762942173039, "mdate": 1762942173039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}