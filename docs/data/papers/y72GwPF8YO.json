{"id": "y72GwPF8YO", "number": 5633, "cdate": 1757924277037, "mdate": 1759897964010, "content": {"title": "FineSplat: Fine-Grained 3D Open-Vocabulary Language Gaussian Splatting", "abstract": "Existing open-vocabulary scene understanding methods are primarily limited to coarse-grained understanding at the object category level, making them incapable of handling fine-grained queries. In this paper, we introduce a challenging task of fine-grained open-vocabulary scene understanding and propose a novel fine-grained 3D language gaussian splatting framework, FineSplat for short. Unlike prior methods that rely on the vision-language alignment model, such as CLIP, FineSplat models the feature field solely from textual captions, transforming the cross-modal feature matching challenge into a retrieval process between queries and captions. Specifically, we design the Fine-Grained Caption Generation (FGCG) strategy to obtain captions containing multi-dimensional fine-grained attributes. Then, the Fine-Grained Feature Field Modeling (FGFFM) strategy is introduced to encode generated fine-grained captions into object-level semantic features, which subsequently supervise the training of 3D Gaussian representations. Furthermore, we construct Fine-OVS, a benchmark to support research and evaluation of the fine-grained open-vocabulary scene understanding task. Extensive experiments conducted on the Fine-OVS demonstrate that our FineSplat framework significantly outperforms existing state-of-the-art methods.", "tldr": "", "keywords": ["Fine-Grained 3D Scene Understanding", "Language Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/025cc4b284e932ef577b3aaac3f9d2002768019e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a challenging task of fine-grained open-vocabulary scene understanding, which is of great significance for real-world interaction. A novel language gaussian splatting framework, FineSplat, is proposed to address this task. This paper also introduces a novel benchmark dataset, Fine-OVS, to address the gap in previous benchmarks where the test queries are limited to simple category-level labels. The authors conducted extensive qualitative and quantitative experiments to verify the effectiveness of the proposed method from multiple perspectives. Overall, this is an interesting paper that makes a meaningful contribution to the community."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well written and well organized, and the proposed FineSplat is easy to follow.\n2.This paper has a strong motivation and breaks through the CLIP-based paradigm by modeling the feature field using only fine-grained captions, which is a novel idea.\n3.To support research and evaluation on fine-grained scene understanding tasks, this paper constructs a novel benchmark, Fine-OVS, which includes 8 fine-grained attributes. \n4.Extensive quantitative and qualitative experimental results demonstrate that FineSplat exhibits stronger fine-grained understanding capabilities compared to baseline methods."}, "weaknesses": {"value": "1.The paper lacks ablation studies on Fine-Grained Feature Field Modeling. Specifically, if this process is treated as feature matching rather than text retrieval, how the choice of encoder affects performance remains unclear.\n2.In Figure 2, the two text encoders are shown using the same color. However, according to the paper’s description, they are two different encoders, so the visual representation in Figure 2 should be adjusted.\n3.The authors should clarify whether they plan to release the benchmark publicly. I believe this benchmark could significantly advance research in fine-grained scene understanding tasks."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DoQnQAQCuJ", "forum": "y72GwPF8YO", "replyto": "y72GwPF8YO", "signatures": ["ICLR.cc/2026/Conference/Submission5633/Reviewer_AMGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5633/Reviewer_AMGd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719511453, "cdate": 1761719511453, "tmdate": 1762918167851, "mdate": 1762918167851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for semantic 3D feature fields. Authors observes that CLIP often behaves like a bag-of-words under phrase/sentence-level queries and fails to capture rich compositional context. To address this, they propose an embedded 3DGS that ingests long, descriptive text features. The main idea is introduction of coarse-to-fine captioning pipeline that leverages a DAM and an MLLM to extract fine-grained textual signals, and they adjust the text encoder to handle unbalanced captions (mismatched information density between user queries and generated captions). To evaluate fine-caption-conditioned querying, they curate a new dataset for 3D localization from fine captions and show improvements over prior methods on tasks requiring detailed textual understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation : Identification of CLIP’s bag-of-words-like behavior under long queries and the proposal to inject richer text signals into 3DGS. The problem framing (descriptive queries vs. keyword queries) is persuasive and grounded in practical use cases.\n\n- The captioning pipeline and dataset can catalyze follow-up research on fine-grained 3D language grounding."}, "weaknesses": {"value": "**More Ablation experiments?** \n- It is unclear whether the method truly understands each fine-grained meaning; ablations are needed (e.g., Observation on if they can distinguish same category with different materials, or when spatial relations are identifiable). \n\n**Multi-view consistency**\n- Multi-view inconsistency remains a concern for text signals.\nPrior works note instability from view-dependent cues due to inconsistent SAM segmentation and occlusions. \nSuggest text multi-view bootstrapping experiments: does aggregating captions from different views converge to consistent 3D semantics?\n\n**Compatible with existing dataset**\n- Do results on existing datasets remain comparable? While expressiveness seems improved, it is unclear whether coarse information is still well represented.\n\n**About generalization study** \n- In the generalization study (Appendix E.4), comparisons should focus only on changed factors; since material accounts for only 29.2%, small overall changes may not indicate true generalization. How does performance differs using changing cases only."}, "questions": {"value": "- Is there an ablation without the unbalanced-query/caption mechanism?\n- Other questions are covered under Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AegaLNafLt", "forum": "y72GwPF8YO", "replyto": "y72GwPF8YO", "signatures": ["ICLR.cc/2026/Conference/Submission5633/Reviewer_uodS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5633/Reviewer_uodS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812286669, "cdate": 1761812286669, "tmdate": 1762918167618, "mdate": 1762918167618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify a limitation in current open-vocabulary 3D understanding methods, which are mostly limited to coarse, object-level recognition. They propose a new task called \"Fine-grained Open-vocabulary 3D Scene Understanding\" and a framework, FineSplat, to address it. The core idea of FineSplat is to avoid using vision-language models like CLIP for feature matching. Instead, it generates detailed textual captions for all objects in a scene and models the 3D feature field using only text-based embeddings. This reframes the problem from cross-modal (vision-text) matching to intra-modal (text-text) retrieval. The method involves a Fine-Grained Caption Generation (FGCG) strategy and a Fine-Grained Feature Field Modeling (FGFFM) strategy. The authors also introduce a new small-scale benchmark, Fine-OVS, to evaluate this fine-grained task"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Excellent Motivation:** The paper is well-motivated. It correctly identifies a key weakness in current 3D language-field methods (e.g., LangSplat, GAGS) that rely on CLIP: they struggle with fine-grained attribute binding and compositionality, often behaving like a \"bag-of-words\" . The examples in Figure 1 clearly illustrate this failure mode.\n2. **Novel Problem Formulation:** The core idea of reformulating the task from vision-text matching to text-text retrieval is clever. This directly sidesteps the known attribute-binding issues of models like CLIP and grounds the scene representation in a semantically richer (text-only) space.\n3. **New Benchmark Contribution:** The paper introduces Fine-OVS, a new benchmark specifically designed to evaluate this more challenging fine-grained task, which is a valuable contribution to the community."}, "weaknesses": {"value": "1. **Extremely Complex Pipeline:** The primary weakness of this paper is the immense complexity of the proposed pipeline, which feels more like a heavy engineering effort than a clean, scalable method. The \"Fine-Grained Caption Generation\" (FGCG) strategy alone (see Fig. 2) requires running multiple, large foundation models in sequence: (1) Run SAM to get masks ; (2) Run DAM (a caption model) on *every* mask ; (3) Run an MLLM (Qwen-VL) with complex multi-modal prompts (including blurred and highlighted images) to *refine* the captions. This multi-stage, computationally massive data-generation process seems impractical to scale.\n2. **Scene-Specific Components:** The method's scalability is severely limited by the fact that it requires training a *new, scene-specific* autoencoder for *every single scene*. This is also mentioned as a limitation by the authors. This means FineSplat is not a generalizable, \"train-once\" model, but rather a pipeline that must be partially re-trained for any new scene it encounters, which is a significant drawback.\n3. **Very Limited Evaluation:** The new Fine-OVS benchmark is extremely small, consisting of only **8 scenes**. While the method shows strong performance on this custom-built benchmark, this is not a comprehensive evaluation. It is unclear if this complex pipeline is feasible or effective on larger-scale datasets.\n4. **Poor Generalization to Standard Tasks:** The authors' own experiments on the standard (coarse-grained) LERF benchmark (Table 8) show that FineSplat performs *worse* than the baseline LangSplatV2 . This strongly suggests that the method has been over-specialized for its own narrow, fine-grained task and has lost the ability to perform well on general, coarse-grained queries. This supports the idea that this is a \"niche\" solution that does not advance general scene understanding."}, "questions": {"value": "1. Could the authors provide a full computational cost analysis for the *entire* pipeline? This should include the FGCG data generation (cost of running SAM, DAM, and MLLM on all views) and the FGFFM (cost of training the *per-scene*autoencoder). How many hours/VRAM does it take to process one scene from start to finish, compared to LangSplatV2?\n2. The poor performance on LERF (Table 8) is concerning. Does this imply a fundamental trade-off, where the model gains fine-grained accuracy by sacrificing coarse-grained accuracy? Can the model no longer reliably answer simple queries like \"find the mug\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vLo41gqllG", "forum": "y72GwPF8YO", "replyto": "y72GwPF8YO", "signatures": ["ICLR.cc/2026/Conference/Submission5633/Reviewer_v764"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5633/Reviewer_v764"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816497166, "cdate": 1761816497166, "tmdate": 1762918167416, "mdate": 1762918167416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}