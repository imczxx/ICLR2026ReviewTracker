{"id": "PNRGSW9jPz", "number": 24783, "cdate": 1758360319804, "mdate": 1763013630058, "content": {"title": "EvA: An Evidence-First Audio Understanding Paradigm for LALMs", "abstract": "While Large Audio Language Models (LALMs) have demonstrated remarkable capabilities in audio understanding tasks, their performance degrades sharply in complex acoustic scenes, revealing a fundamental limitation in their perceptual grounding. In this work, we first identify a critical failure mode that exposes this limitation: state-of-the-art LALMs paradoxically struggle more with simple evidence-extraction tasks than with complex reasoning ones. We diagnose this as a breakdown in acoustic evidence grounding, a problem rooted in systemic information loss during feature encoding and fusion. To address this, we introduce EvA (Evidence-First Audio), a new paradigm that prioritizes maximizing the fidelity of acoustic evidence. EvA's dual-encoder architecture combines Whisper with CED-Base, a ViT-based general audio encoder, and pioneers a structure-preserving, two-stage fusion process. First, it enriches evidence by hierarchically aggregating multi-level features from within the CED-Base encoder. Second, it integrates this representation with Whisper's output via a time-aligned, inject-and-add mechanism that guarantees perfect temporal integrity. To facilitate training for this paradigm, we co-develop EvA-Perception, a large-scale open-source dataset with high-temporal-precision annotations. Our resulting model establishes a new open-source state-of-the-art on multiple challenging benchmarks, including MMAU, MMAR, and MMSU. Crucially, EvA achieves its most significant gains on perception-heavy subsets, validating our hypothesis that addressing the evidence bottleneck is key to unlocking the next level of audio understanding.", "tldr": "", "keywords": ["Large Audio Language Model", "Audio Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7dfd122442c022c398559436dea15702857d4933.pdf", "supplementary_material": "/attachment/4ead05e91e6bb5dbdbb1a508c744393a562cbf4a.zip"}, "replies": [{"content": {"summary": {"value": "This paper diagnoses a critical \"evidence bottleneck\" in Large Audio Language Models, arguing that their primary limitation is poor perceptual grounding rather than flawed reasoning. To address this, the paper introduces Evidence-First Audio, a novel paradigm built on a dual-encoder architecture (Whisper and CED-Base) and a non-compressive, two-stage fusion mechanism that hierarchically aggregates multi-level acoustic features while preserveing temporal fidelity. The paper also develops EvA-Perception, a large-scale dataset with high-temporal-precision annotations to facilitate training. The resulting model achieves new state-of-the-art performance on the MMAU, MMAR, and MMSU benchmarks, with the most significant gains on perception-heavy tasks, thereby validating their evidence-first hypothesis. The EvA-Perception dataset and EvA model will be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.The paper presents an insightful diagnosis of a critical yet overlooked limitation in existing Large Audio Language Models. This diagnosis is well-supported by both theoretical arguments  and comprehensive experimental validation. \n\n2.To address this limitation, the paper introduces a novel dual-stream architecture and a purpose-built dataset, EvA-Perception, which collectively achieve state-of-the-art performance across multiple challenging benchmarks. \n\n3.The commitment to open-sourcing the models and the newly created dataset significantly enhances the reproducibility and potential impact of the work."}, "weaknesses": {"value": "1.The paper proposes a sophisticated architectural design within the CED-path, yet the ablation studies do not fully justify this complexity. More granular ablations would strengthen the paper's design claims and provide clearer insights for future work.\n\n2.While the paper is generally well-written, the introduction section could be improved. The core concept of \"evidence\" is central to the paper's thesis, yet it is used extensively without a concise, upfront definition, creating an initial barrier to understanding.\n\n3. The description of the fusion mechanism as \"lossless\" (L214, L477) appears to be an overstatement."}, "questions": {"value": "1.The term \"evidence\" is foundational to the paper's narrative and contributions. Could the authors provide a concrete definition of this concept?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "761AE3jtjF", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_j5Wj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_j5Wj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495900264, "cdate": 1761495900264, "tmdate": 1762943196751, "mdate": 1762943196751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the issue of information loss in evidence encoding for large audio language models. They approach it by combining multiple feature encoders, using hierarchical aggregation, time-aligned feature combination. They also contributes a dataset with high quality temporal annotations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I appreciate the effort went into building the model and dataset. I encourage the authors to keep improving the work through technical innovation"}, "weaknesses": {"value": "1. Author name is revealed as the folder name in supplementary material\n\n2. to show that model perception is lagging behind reasoning, instead of showing absolute scores of perception and reasoning as figure 1, one should show the gap between model and human performance, i.e. to show that the gap between model and human is bigger on perception\n\n3. the encoder \"CED-Base\" is never introduced in the paper - the full model name is never shown, the architecture is never explained, even the original paper is never cited anywhere. \n\n4. I strongly oppose using information theory to level up a paper when it doesn't actually add anything to the work. Section 3.1 and 3.2 are not rigorous and even flawed, not helpful in explaining their approach, and is a waste of space. What they are trying to say is that using more audio encoders can provide more information, which is commonsense and do not need any theoretical motivation. \n\nwhy the math is not rigorous or even flawed:\n\n4.1. Lack of a Well-Formed Probabilistic Model (whether implicit of not)\nAlthough Section~3 defines\n$$\nZ:\\text{ground-truth acoustic evidence}, \\quad\nX:\\text{raw waveform}, \\quad\nH:\\text{encoder hidden}, \\quad\nO:\\text{final representation}, \\quad\nY:\\text{output text},\n$$\nthe paper never specifies a joint distribution $p(Z,X)$.\nAre $Z$ latent causes that generate $X$, or deterministic annotations extracted from it?\nIf $Z=f(X)$, then $I(Z;X)=H(Z)$ trivially and the Data Processing Inequality (DPI) adds nothing.\nIf $Z$ is latent, its distribution must be defined before mutual information can be evaluated.\nThus, every $I(Z;\\cdot)$ term remains symbolic rather than quantitative.\n\n4.2. Misuse of the Data Processing Inequality\nThe paper treats\n$$\nZ \\rightarrow X \\xrightarrow{E} H \\xrightarrow{P} O \\xrightarrow{\\pi} Y\n$$\nas a Markov chain and directly applies\n$$\nI(Z;Y) \\le I(Z;O) \\le I(Z;H) \\le I(Z;X).\n$$\nHowever, $E, P,$ and $\\pi$ are deterministic neural networks whose parameters depend on the training data.\nOnce parameters are learned, the conditional independence required by DPI no longer holds.\n\n4.3. No Operational Link to Performance\nThe claimed ``information ceiling’’ never connects $I(Z;Y)$ to measurable task metrics such as WER or accuracy.\nA rigorous bound would invoke Fano’s inequality or rate–distortion theory to relate mutual information to the achievable Bayes risk.\n\n4.4. Tautological Lemma in Section 3.2\nThe so-called ``Complementary Evidence Advantage’’ states:\n$$\nI(Z;O_1, O_2)\n  = I(Z;O_1) + I(Z;O_2 \\mid O_1)\n  \\ge I(Z;O_1),\n$$\nwhich is merely the chain rule for mutual information.\nIt does not establish that a dual-path model can achieve higher $I(Z;Y)$;\nit only restates that adding variables cannot decrease mutual information.\n\n4.5. Unsupported ``Strict Superiority’’\nProposition 1 claims a strict gain if $I(Z;O_2 \\mid O_1) > 0$,\nbut this follows trivially from the lemma and is not empirically verified.\n\n4.6. Unproven ``$Z$-Sufficient Fusion’’\nThe authors later require a fusion function $F$ satisfying\n$$\nI(Z;F(O_1, O_2)) = I(Z;O_1, O_2),\n$$\nyet their proposed frequency-pooled, gated fusion is many-to-one and clearly non-invertible.\nNo argument or estimator is offered to demonstrate that it preserves $Z$-information.\n\n5. They claim to contribute a dataset, but there is no innovation in their approach because it just apply other (M)LLMs to extract and aggregate information (which is already used in many audio LLM works). Plus there is no example of the constructed dataset."}, "questions": {"value": "no questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mtPV7tPfj7", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_s3Ms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_s3Ms"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526006627, "cdate": 1761526006627, "tmdate": 1762943196548, "mdate": 1762943196548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EvA (Evidence-first Audio-language model), a novel architecture designed to enhance the evidence-extraction capability of audio-language reasoning models. Unlike traditional end-to-end LALMs that directly generate answers from audio embeddings, EvA explicitly divides the reasoning process into two stages: (1) Evidence-extraction encoding audio segments with dual-encoder to avoid being hampered by fusion strategy; (2) Answer Generation—where a large language model performs reasoning and generates responses based on the extracted evidence and textual context. The authors argue that reasoning based on verifiable acoustic evidence can reduce the model's \"hallucination\" phenomenon and improve interpretability.\nEvA is implemented as an extension of the Kimi-Audio-7B framework, incorporating a Time-Aware Alignment and Inject-and-Add Fusion mechanism. By fusing features from Whisper and CED while minimizing the loss of acoustic evidence during fusion, the model achieves enhanced performance. The paper also introduces a new dataset, EvA-Perception, which features high-temporal-precision annotations. Finally, the model achieves state-of-the-art results on multiple benchmarks including MMAU, MMAR, and MMSU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors claim that EvA effectively mitigates the “evidence bottleneck” by increasing the amount of acoustic information available to the LLM without retraining encoders.\n\nEvidence:\n- Empirical results (Table 2): EvA surpasses strong baselines such as Kimi-Audio, Qwen2.5-Omni, and R1-AQA across all benchmarks.\n- Ablation results (Table 3): show that adding the CED aggregator and alignment yields significant improvements in both AudioCaps CLAP metrics and benchmark perception accuracy.\n- Qualitative examples (Fig. 3): demonstrate that EvA-generated captions capture fine-grained temporal and tonal details better than baselines.\n- **Theoretical analysis** (Section 3): formally proves that dual-path fusion provides strictly higher information capacity than single-path models (via mutual information inequalities).\n\nOverall, this method has Excellent theoretical–empirical consistency, Clear diagnosis of a real architectural weakness in existing LALMs, Strong experimental gains without retraining encoders."}, "weaknesses": {"value": "1. Task coverage narrow (mainly perception, English-only).\n\n2. The authors' dual-encoder shares similar architectural ideas with that in SALOMNN. However, there is a lack of experimental comparison. While the experiments include comparisons between the dual-encoder and single-encoder, they fail to demonstrate that their proposed Aggregation and Fusion strategies outperform the Window-level Q-Former used in SALMONN.\n\n3. Meanwhile, there is a lack of theoretical proof regarding the advantages of their strategy over Q-Former."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IhoHBY37Hl", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_YYmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_YYmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883601329, "cdate": 1761883601329, "tmdate": 1762943196358, "mdate": 1762943196358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a promising dataset; however, it is unclear whether it will be publicly released. Since the main contribution lies in the dataset itself, the overall significance of the paper is limited. The experiments are not comprehensive, with insufficient results and analysis, and the LALM training approach is fairly standard with limited novelty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a valuable dataset that could benefit future research on LALM training and evaluation."}, "weaknesses": {"value": "The proposed weak-to-strong and mixed-to-strong strategies with SFT and GRPO are standard practices in current literature and therefore lack novelty.\n\nThe experimental evaluation is not comprehensive, as it includes only three benchmarks. Exploring additional LALM benchmarks would help strengthen your claims.\n\nThe paper contains some redundant analysis that could be streamlined. I recommend focusing on deeper technical insights and introducing more concrete methodological novelties to strengthen the contribution.\n\nThe overall writing could be improved, particularly by providing a clearer introduction and a more structured related work section.\n\nWhile the paper proposes an interesting dataset, the overall contribution is not substantial enough to meet the standards of ICLR. The methods and analyses presented are relatively incremental, and the paper would benefit from stronger technical innovations or deeper theoretical insights."}, "questions": {"value": "As this paper seeks to advance large audio-language models through a newly proposed dataset AudioMCQ, could the authors clarify how frequently the dataset has been utilized in the domain and provide evidence that it is well-defined and meaningful? Have you released the dataset and codes, or do you plan to make them publicly available?\n\nIs there any human annotation involved to verify the quality of the dataset? How do you ensure the correctness of the outputs generated by other models?\n\nCould the authors elaborate on the potential research impact of this dataset and how it advances the field?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "341wsiGYLb", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_GjKd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_GjKd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920385322, "cdate": 1761920385322, "tmdate": 1762943196167, "mdate": 1762943196167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a good motivation and an interesting problem formulation, but the proposed approach relies heavily on previous works and employs a relatively simple fusion method, which limits its novelty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem formulation for “evidence bottleneck” is very intriguing and inspiring for the poor acoustic grounding. The paper is well-written with clear motivations and methodology formulation. Prioritizing the perceptual front-end is a novel and reasonable approach to enhancing the audio understanding ability of LALMs."}, "weaknesses": {"value": "Since the work builds upon pretrained models such as Whisper, CED-Base, and Kimi-Audio-7B, the network design itself is not particularly innovative, as much of the credit should be attributed to these existing models. The main contribution lies in effectively combining and fusing them using LoRA. This direction is promising and could be further strengthened to reach the level of ICLR.\n\nThe Time-Aware Alignment and Inject-and-Add Fusion components are not novel, as similar operations are commonly used in speech and audio processing. Please clarify what specific novelty, if any, is introduced in these parts.\n\nAlthough GRPO is a known term, please provide its full name for clarity.\n\nThe experimental evaluation appears limited, given that it includes comparisons with only five LALMs and three benchmarks."}, "questions": {"value": "Regarding the frequency-domain knowledge extraction, what is the highest frequency captured? Does this differ between general audio and speech when extracting meaningful features? It would be helpful if the authors could compare these aspects in experiments and share their insights with the research community.\n\nAs this paper seeks to advance large audio-language models through a newly proposed dataset, could the authors clarify how frequently the dataset has been utilized in the domain and provide evidence that it is well-defined and meaningful? Have you released the dataset and codes, or do you plan to make them publicly available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "341wsiGYLb", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_GjKd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_GjKd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920385322, "cdate": 1761920385322, "tmdate": 1763043986948, "mdate": 1763043986948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical limitation in existing Large Audio Language Models (LALMs), which the authors term the evidence bottleneck. They show that the primary failure mode of state-of-the-art LALMs in complex acoustic scenes is not a deficiency in high-level reasoning, but rather a fundamental breakdown in perceptual grounding caused by information loss during audio encoding and fusion.To address this, the authors propose EvA, a new evidence-first paradigm. The core of EvA is a dual-encoder architecture that uses a speech-centric Whisper encoder and a generalist ViT-based audio encoder (CED-Base). The key novelty lies in its information-preserving fusion mechanism, which involves hierarchical aggregation of multi-level features from the generalist encoder and a time-aware inject-and-add fusion that aligns the generalist audio features to the Whisper timeline without temporal compression. To facilitate training, the authors also developed EvA-Perception, a new large-scale, open-source dataset with high-temporal-precision annotations designed to improve evidence-grounded training. The resulting EvA model is shown to set a new open-source state-of-the-art on the MMAU, MMAR, and MMSU benchmarks, with the most significant performance gains observed on perception-centric subsets, thereby validating the paper's central hypothesis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Clear Problem Formulation and Motivation: The paper effectively identifies a critical problem in existing LALMs, positing that performance limitations stem from incomplete acoustic evidence. This hypothesis is well-supported by both empirical data in Figure 1 and the theoretical argument in Lemma 1. This clear problem diagnosis naturally motivates the proposed method: augmenting the audio encoder with a CED module to provide richer acoustic information.\n2. Novel Architecture and Insight: The authors argue that previous information fusion methods are inherently lossy. In response, they propose a novel Aggregator designed to preserve information across different frequency bands and hierarchical layers. Furthermore, the inject-and-add strategy is a clever approach that effectively integrates semantic information while simplifying the training process.\n3. Strong and Well-Analyzed Empirical Results: The paper demonstrates significant performance improvements, achieving state-of-the-art results across three challenging and diverse benchmarks. Crucially, the authors go beyond reporting overall scores by breaking down performance into Perception and Reasoning categories. The results convincingly show that EvA yields the largest gains in the Perception category while also improving Reasoning performance, validating the core hypothesis of the paper.\n4. Significant Contribution to the Community: This work offers more than just a new perspective on audio representation; it also contributes several valuable datasets under the EvA. The creation of a large-scale, high-quality dataset with fine-grained temporal annotations is a substantial contribution in its own right and will be a valuable resource for future research."}, "weaknesses": {"value": "1. Potentially Unfair Baseline Comparison: The comparison in Table 2 may lack fairness for two reasons. First, it appears the baseline models were not fine-tuned on the newly introduced EvA datasets, making it difficult to disentangle the performance gains from the novel architecture versus the new training data. Second, the paper notes its model is English-only, whereas many of the baseline models are multilingual. This linguistic mismatch could be another source of unfairness in the comparison.\n2. Lack of Ablation Studies for the Aggregator Module: The paper's ablation studies primarily focus on the overall impact of the CED path. However, there are no detailed experiments to validate the specific design choices within the CED Aggregator itself. The individual contributions of the frequency-pooled gate and the cross-layer fusion mechanism have not been separately investigated, leaving the optimality of the Aggregator's design unsubstantiated."}, "questions": {"value": "1. What is the detailed rationale behind using a frequency-pooled gate? What are the specific challenges (e.g., computational complexity, feature space mismatch) associated with retaining and fusing the full, unpooled 2D time-frequency feature map?\n2. How significant is the empirical contribution of the cross-layer fusion mechanism? The paper hypothesizes that intermediate encoder layers provide richer, low-level information that is lost in the final layer. Is there direct empirical evidence from ablation studies to support this design choice and quantify its benefit over a simpler fusion that uses only the final layer's features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eZABnGUXYv", "forum": "PNRGSW9jPz", "replyto": "PNRGSW9jPz", "signatures": ["ICLR.cc/2026/Conference/Submission24783/Reviewer_3Jb1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24783/Reviewer_3Jb1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995784625, "cdate": 1761995784625, "tmdate": 1762943195956, "mdate": 1762943195956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}