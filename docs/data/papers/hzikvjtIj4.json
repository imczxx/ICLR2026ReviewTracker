{"id": "hzikvjtIj4", "number": 15632, "cdate": 1758253383079, "mdate": 1759897293445, "content": {"title": "AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism", "abstract": "Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing *asynchronous updates across both parallelism axes*, relaxing the co-location requirement at the expense of introducing *staleness* between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an *asynchronous sparse averaging* method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to *1B parameters*) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.", "tldr": "We introduce a fully asynchronous optimization method to address the communication overhead in both data and pipeline parallelisms, and demonstrate its efficacy in large scale language modelling tasks.", "keywords": ["Asynchronous Optimization", "Sparse Averaging", "Data and Pipeline Parallelism", "Decentralized Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3c70277784ee7f3c65a7fc7a80d5ae2263854a9.pdf", "supplementary_material": "/attachment/8319c200f4f8e18b425b50a3d0f20d1fb6abbc89.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new distributed training framework called AsyncMesh, which enables fully asynchronous optimization across both data parallel (DP) and pipeline parallel (PP) axes. To counteract the staleness introduced by asynchrony, the authors propose (1) a weight look-ahead mechanism for PP using Nesterov extrapolation and (2) an asynchronous sparse averaging scheme for DP with an exponential moving average (EMA)-based staleness correction. They theoretically prove convergence under standard assumptions and empirically show that AsyncMesh achieves performance comparable to fully synchronous training while drastically reducing communication overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Introduces AsyncMesh, that enables asynchronous updates across both data parallelism (DP) and pipeline parallelism (PP) to address this communication bottleneck.\n+ Combines Nesterov-based weight look-ahead for PP and Exponential Moving Average (EMA) correction for DP to counteract stale gradients and parameters effectively.\n+ Provides formal convergence guarantees for both asynchronous sparse averaging and delayed updates, extending existing results from stochastic approximation theory.\n+ Demonstrates performance parity with fully synchronous baselines on language models up to 1 billion parameters, showing scalability and robustness."}, "weaknesses": {"value": "- Theoretical convergence guarantees rely on homogeneous settings, which may not hold in practical heterogeneous or real-world decentralized systems.\n- Although sparse averaging reduces communication, it could slow convergence for extremely small subsets or large delays, as hinted in the theoretical analysis. No experiments have done on this.\n- The paper lacks direct comparisons with strong recent baselines such as DeepSpeed ZeRO, ZeRO++.\n- The effects of EMA decay rates, subset sizes, and delay parameters are not deeply analyzed.\n- Combining asynchronous DP and PP with custom staleness correction mechanisms may complicate integration into existing distributed training frameworks."}, "questions": {"value": "1. The theoretical convergence proofs assume identical hardware, learning rates, and i.i.d. data across replicas. Extend the theoretical framework or provide an empirical ablation to quantify performance under heterogeneity, such as uneven compute power, data imbalance, or non-uniform network latency.\n2. Could staleness correction via EMA still approximate global consensus effectively when replicas diverge in data distribution or update frequency? Discuss convergence guarantees in heterogeneous environments.\n3. The paper claims sparse averaging (e.g., 5%) maintains performance, but what happens at extreme sparsity (e.g., 1% or less) or high delay (τ > 50)? Is there a theoretical threshold or empirical tipping point where sparse averaging starts to degrade convergence or stability? Conduct controlled experiments varying both subset size (1–10%) and delay intervals (10–100 steps) to observe convergence degradation patterns. Include convergence curves and communication–accuracy trade-off plots to better illustrate the regime where AsyncMesh remains stable.\n3. Why were DeepSpeed ZeRO and ZeRO++ omitted from the baseline comparisons, given their dominance in large-scale model parallelism? How would AsyncMesh compare to ZeRO’s optimizer and gradient state partitioning in terms of both communication efficiency and memory footprint? Include at least one benchmark comparison (even partial) against ZeRO or ZeRO++ under similar mesh sizes to demonstrate where AsyncMesh provides unique advantages. Discuss interoperability potential — e.g., could AsyncMesh be layered on top of ZeRO’s optimizer partitioning?\n4. How sensitive is AsyncMesh’s convergence to the EMA coefficient schedule (λₜ)? Does the same schedule generalize across datasets, model sizes, or communication delays? Are there interactions between subset size, EMA decay, and delay that affect stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EraqW4Fpgr", "forum": "hzikvjtIj4", "replyto": "hzikvjtIj4", "signatures": ["ICLR.cc/2026/Conference/Submission15632/Reviewer_tRZk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15632/Reviewer_tRZk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503867337, "cdate": 1761503867337, "tmdate": 1762925894189, "mdate": 1762925894189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper try to incorporate both async communication in DP(data parallel) and PP(pipeline parallel).\n\nThe major contribution is to combine asyncPP and SPARTA in DP together and guarantee the loss curve can converge."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Theoretical analysis on AsyncPP and SPARTA in DP\n\n2. e2e experiments training and show loss curves"}, "weaknesses": {"value": "1. the major experimental model is a toy size of 160M, which cannot represent real world pre-training model patterns. In addition, it is just toy NanoGPT not a real GPT model. Furthermore, the model does not even have basic dropout layer, which make the loss curve comparison less convincing. \n\n2. the paper contribution is very minor, it just combined existed work AsyncPP and SPARTA in DP together and did a bit tuning. There is very little research novelty here.\n\n3. Whether the model can converge or not in such async model training case is heavily depend on delayed steps, the math part of this paper does not even discuss much about it thus making the whole proof less meaningful. \n\n4. the sec 5.4, 1B model itself is not a standard GPT model, for example, the embedding dim is very small as 2304, and 24 attention head is not standard gpt-3. In addition, the paper does not show any main stream and standard model size results, thus making the result less convincing. \n\n5. the authors lack of knowledge about SOTA data parallel framework, such as ZeRO or FSDP, which is the only DP paradigm used in real world. And people start only use them as DP starting from 2020. But there is no discussion on how to do async communication overhead in such schemes."}, "questions": {"value": "If t - $\\tau$ to t has a big delay, how should loss converge still hold? The proof does not make sense if there is a big gap between t - $\\tau$ to t. And there is no discussion on how to theoretically analyse and determine the biggest gap between t - $\\tau$ to t to make the loss curve difference minimal to fully synced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bpoy9ZR7Om", "forum": "hzikvjtIj4", "replyto": "hzikvjtIj4", "signatures": ["ICLR.cc/2026/Conference/Submission15632/Reviewer_yc1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15632/Reviewer_yc1x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612973023, "cdate": 1761612973023, "tmdate": 1762925893838, "mdate": 1762925893838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AsyncMesh, asynchronous staleness-aware training approach that combines an asynchronous sparse averaging method and an exponential moving average based correction mechanism. It also provides convergence guarantees for both sparse averaging and asynchronous updates and evaluates their methods using LLMs with up to 1B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. AsyncMesh explores the setup where both DP and PP are asynchronous.\n2. The paper designs an Exponential Moving Average (EMA) based correction mechanism that approximates the average staleness.\n3. The paper provides theoretical justification of convergence in the presence of staleness in a homogeneous setup where only a small subset of weights is communicated between DP replicas."}, "weaknesses": {"value": "1. The baseline for the evaluation is weak. The benchmark for this evaluation is weak. The evaluation only compares AsyncMesh with FullyAsync and DP. However, well-studied staleness-aware LLM training [1]  (with different degree of staleness) and also block coordinate descent with correction [2] was not included in the evaluation.\n2. The evaluation results did not show how much performance improvement sparse averaging could bring.\n\n[1] PipeDream: Generalized pipeline parallelism for DNN training\n[2] Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Correction"}, "questions": {"value": "Does the A00 machine used in the evaluation have NVLink?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f6yrPLLJBz", "forum": "hzikvjtIj4", "replyto": "hzikvjtIj4", "signatures": ["ICLR.cc/2026/Conference/Submission15632/Reviewer_uqSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15632/Reviewer_uqSG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996118637, "cdate": 1761996118637, "tmdate": 1762925893499, "mdate": 1762925893499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}