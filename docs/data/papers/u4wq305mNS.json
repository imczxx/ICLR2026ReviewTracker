{"id": "u4wq305mNS", "number": 22144, "cdate": 1758326717148, "mdate": 1759896883791, "content": {"title": "Beyond Parameters: Exploring Virtual Logic Depth for Scaling Laws", "abstract": "Scaling the size of large language models typically involves 3 dimensions: depth, width, and the number of parameters. In this work, we explore a 4th dimension: virtual logical depth (VLD), which allows increasing the effective algorithmic depth without changing the overall parameter count by reusing parameters within the model. While parameter reuse is not new, its role in scaling dynamics has remained underexplored. Unlike currently trending test-time methods, which mainly scale in token-wise, VLD alters the internal computation graph scaling during training, inference, or combination. We carefully design controlled experiments and have the following key insights on VLD scaling: 1. Knowledge capacity vs. parameters. At a fixed parameter count, VLD leaves knowledge capacity nearly unchanged (with only minor variance), while across models knowledge capacity scales with the number of parameters; 2. Reasoning vs. reuse. Properly implemented VLD substantially improves reasoning ability without increasing parameter count, decoupling reasoning from sheer model size. This provides a new possibility for scaling besides the current token-wise test-time scaling used by most reasoning models. 3. Robustness and generality. The trend of improved reasoning persist across architectures and configurations (e.g., different reuse schedules and step counts), indicating that VLD captures a general scaling behavior. These findings not only provide useful insights into the future model scaling strategies, but also introduce an even deeper question: Does super intelligence necessarily require ever-larger models, or could it have some trade-offs by re-using parameters and increasing virtual logic depth? We believe that there are many unknown dynamics within the model scaling that need exploration. Codes are available at https://anonymous.4open.science/r/virtual_logical_depth-8024/.", "tldr": "The 4th dimension for scaling model size,which significantly improves reasoning while forcing the knowledge capacity to be constant.", "keywords": ["Large Language Models", "Model Scaling", "Parameter Reuse"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e0632482520bd2a0d952d350bc4f070c692c249.pdf", "supplementary_material": "/attachment/90ea3c6a150dbf3617d71100902e892a573b957a.zip"}, "replies": [{"content": {"summary": {"value": "The paper “Beyond Parameters: Exploring Virtual Logic Depth for Scaling Laws” introduces Virtual Logical Depth (VLD) as a new scaling dimension for large language models, complementing the traditional three — depth, width, and parameter count.\nVLD increases a model’s effective algorithmic depth by reusing parameters within the computation graph, thereby extending the model’s reasoning process without adding new parameters."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Conceptual: Introduces Virtual Logical Depth (VLD) as a fourth dimension in LLM scaling laws.\n\nMethodological: Implements parameter and layer reuse to simulate increased logic depth without parameter growth.\n\nEmpirical: Demonstrates that VLD improves reasoning performance while keeping knowledge capacity and parameter count fixed."}, "weaknesses": {"value": "Unclear definition of “depth” and reasoning measurement.\nWhile the paper introduces Virtual Logical Depth (VLD) as a fourth scaling dimension, it does not provide a rigorous or operational definition of what constitutes “deeper reasoning.” The relationship between parameter reuse and actual logical depth remains conceptual rather than formally quantified. In particular, it is unclear how the number or schedule of parameter reuses maps to measurable increases in reasoning complexity.\n\nLack of mechanistic or interpretability analysis.\nThe paper focuses on aggregate performance and information-theoretic metrics but omits mechanistic visualization or circuit-level analysis. For example, it would be valuable to visualize how internal computation paths, attention patterns, or activation trajectories evolve as VLD increases. Without such interpretability results, the internal mechanisms underlying reasoning improvement remain opaque.\n\nLimited experimental scale and generality.\nMost experiments are conducted on relatively small-scale models (e.g., GPT-2 or similar transformer variants), which may not generalize to modern large-scale architectures. Since scaling dynamics often change non-linearly with model size, the conclusions about reasoning–scaling relationships under VLD should be treated as preliminary until validated on larger LLMs."}, "questions": {"value": "as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ju8OalYGN2", "forum": "u4wq305mNS", "replyto": "u4wq305mNS", "signatures": ["ICLR.cc/2026/Conference/Submission22144/Reviewer_JV56"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22144/Reviewer_JV56"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611518286, "cdate": 1761611518286, "tmdate": 1762942088457, "mdate": 1762942088457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose Virtual Logical Depth (VLD) as a new dimension for scaling, arguing that we need to look beyond the traditional metrics of depth, width, and parameter count. The core idea is to increase the model's effective algorithmic depth by reusing parameters in the computation graph. This effectively extends the reasoning process without actually increasing the model size."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper challenges the traditional scaling paradigm by introducing Virtual Logical Depth (VLD). It is shown that this conceptualization of a \"fourth dimension\" in scaling laws is a refreshing addition to the standard metrics of depth and width.\n\nInstead of simply making the model bigger, the authors implement a clever mechanism of parameter and layer reuse. This allows the model to simulate increased algorithmic depth without the computational cost of adding new parameters.\n\nThe experiments effectively demonstrate the trade-off: VLD significantly boosts reasoning performance while keeping the parameter count fixed. I particularly appreciated that this improvement didn't come at the cost of knowledge capacity."}, "weaknesses": {"value": "Unclear definition of “depth” and reasoning measurement. While the paper introduces Virtual Logical Depth (VLD) as a fourth scaling dimension and it is very interesting, it does not provide a clear or operational definition of what constitutes “deeper reasoning.” The relationship between parameter reuse and actual logical depth remains conceptual rather than formally quantified. In particular, it is unclear how the number or schedule of parameter reuses maps to measurable increases in reasoning complexity.\n\nLack of mechanistic or interpretability analysis: The paper focuses on aggregate performance and information-theoretic metrics but omits mechanistic visualization or circuit-level analysis. For example, it would be valuable to visualize how internal computation paths, attention patterns, or activation trajectories evolve as VLD increases. Without such interpretability results, the internal mechanisms underlying reasoning improvement remain opaque.\n\nLimited experimental scale and generality: Most experiments are conducted on relatively small-scale models (e.g., GPT-2 or similar transformer variants), which may not generalize to modern large-scale architectures. Since scaling dynamics often change non-linearly with model size, the conclusions about reasoning–scaling relationships under VLD should be treated as preliminary until validated on larger LLMs."}, "questions": {"value": "as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ju8OalYGN2", "forum": "u4wq305mNS", "replyto": "u4wq305mNS", "signatures": ["ICLR.cc/2026/Conference/Submission22144/Reviewer_JV56"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22144/Reviewer_JV56"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611518286, "cdate": 1761611518286, "tmdate": 1763664467661, "mdate": 1763664467661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Virtual Logical Depth (VLD) as a fourth scaling axis for Transformers: instead of adding parameters, the model repeats existing layers with shared weights to increase an effective depth, $D_{\\text {eff }}=D_{\\text {base }}+D_{\\text {VLD }}$. The manuscript claims that this \"vertical\" scaling substantially improves multistep reasoning while leaving \"knowledge capacity\" almost unchanged, measured via an entropy-based proxy $\\Delta H=H_1-H_2$ computed from softmax outputs on a synthetic random-sequence memorization task. Experiments cover small GPT-2 variants trained on synthetic iGSM math and a 3B LLaMA variant fine-tuned on a mixed SFT corpus; figures and tables report gains from layer-sharing patterns (sequence, cycle, inverse-cycle) and show occasional non-monotonicities at large effective depths."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "From an empirical perspective, the work isolates a simple, reproducible manipulation—weight sharing across depth—and documents consistent improvements on controlled synthetic tasks, with some transfer to real benchmarks. The write-up is generally clear and the measurement protocol is spelled out in enough detail to re-implement, including the entropy-based capacity metric and the iGSM setup. The authors are transparent about occasional plateaus and regressions at higher VLD factors, which is appreciated."}, "weaknesses": {"value": "Conceptual novelty is limited relative to prior work on depth-recurrence and cross-layer sharing. Universal Transformers introduced depth-time recurrence with tied parameters years ago, ALBERT formalized cross-layer sharing, and Takase & Kiyono precisely studied the same three tying patterns (sequence, cycle, reverse-cycle). The present paper mainly scales up the experiments without head-to-head, compute-matched comparisons against those baselines, making the “new scaling dimension” feel like a renamed synthesis rather than a new idea. \n\nCompute accounting is incomplete. VLD multiplies the number of layer applications; therefore training and inference FLOPs, latency, and throughput should be held constant or explicitly normalized in any claim that a 50M-parameter VLD model “beats” a larger non-VLD model. The paper shows accuracy gains but does not present FLOPs-matched or wall-clock-matched curves, and some gains could be explained by simply doing more computation per token. This undermines the main message about a distinct, more efficient scaling direction. \n\nThe capacity claim rests on a bespoke proxy (random-sequence memorization) rather than the now-standard “2 bits per parameter” factual-tuple capacity metric; there is no calibration that the proxy agrees with the canonical measure on the same models. Without such a cross-check, the statement that VLD “keeps capacity constant” is not convincing. \n\nEvaluation breadth is narrow for the strength of the claim. The 3B SFT experiment reports modest gains on Math500/AIME/GPQA/HumanEval/MBPP, but lacks confidence intervals, multi-seed variance, exact prompts/decoding, and, crucially, comparisons to test-time compute scaling baselines such as self-consistency or best-of-N selection, which are the natural “alternative path” the paper positions itself against."}, "questions": {"value": "1. Precise compute accounting would be appreciated. For the synthetic iGSM results, what are the per-example train/inference FLOPs and wall-clock for base vs. VLD models at each factor? Are throughput and memory comparable on the same hardware, and how do these costs trade off against accuracy?\n\n2. Regarding knowledge capacity, can the authors replicate the factual-tuple protocol that yields $\\approx 2$ bits/parameter and show that their $\\Delta H$ proxy correlates tightly with it on the very same checkpoints? If not, can they justify why a random-sequence memorization proxy is a valid surrogate for factual capacity?\n\n3. On novelty, the paper should position VLD concretely against Universal Transformers, ALBERT, Takase & Kiyono’s tying strategies, Reuse Transformers, and Dynamic Layer Tying. Which of these, if any, underperform VLD under the same parameter count and the same total FLOPs? Can the authors add those baselines with matched decoding and report paired comparisons?\n\n4. For real benchmarks, since the paper frames VLD as an alternative to inference-time scaling, can we see compute-matched comparisons to self-consistency and best-of-N methods on GSM8K, MATH, AIME, and GPQA-Diamond?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pYPtCIyG5a", "forum": "u4wq305mNS", "replyto": "u4wq305mNS", "signatures": ["ICLR.cc/2026/Conference/Submission22144/Reviewer_1ZQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22144/Reviewer_1ZQo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676419424, "cdate": 1761676419424, "tmdate": 1762942087817, "mdate": 1762942087817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Virtual Logical Depth (VLD) as a fourth dimension for scaling language models, distinct from the traditional dimensions of depth, width, and parameter count. VLD increases effective algorithmic depth through parameter reuse—specifically by repeating transformer layers with shared weights—without increasing the total parameter count. The authors investigate three reuse patterns (sequence, cycle, inverse cycle) and conduct controlled experiments measuring two key attributes: knowledge capacity (via information entropy absorption on random sequences) and reasoning capability (via synthetic math problems and real-world benchmarks). Their main findings demonstrate that VLD scaling maintains nearly constant knowledge capacity while significantly improving reasoning performance. Notably, smaller models with VLD can outperform larger standard models on reasoning tasks, suggesting a parameter-efficient alternative to conventional scaling. The work challenges the assumption that super-intelligence necessarily requires ever-larger models and proposes that strategic parameter reuse combined with increased computational depth may offer viable trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes several valuable contributions through rigorous empirical investigation. The systematic study of VLD as a scaling dimension fills an important gap, as parameter reuse dynamics have remained underexplored despite existing work on layer sharing. The experimental design is comprehensive and well-controlled, spanning both synthetic environments (random sequences for knowledge capacity, iGSM for controlled reasoning evaluation) and real-world benchmarks across multiple domains (mathematics, science, code generation). The knowledge capacity measurement using information entropy provides a theoretically grounded quantitative metric that enables clean separation of memorization from reasoning. The findings are robust, demonstrating consistency across different architectures (GPT-2, LLaMA), VLD patterns, model scales, and training regimes (pretraining and fine-tuning). Most importantly, the empirical discovery that reasoning and knowledge capacity can be decoupled through VLD is significant and challenges conventional scaling paradigms, with practical implications for building parameter-efficient models with enhanced reasoning capabilities."}, "weaknesses": {"value": "Despite its empirical contributions, the paper has several significant limitations. Most critically, it lacks theoretical or mechanistic explanation for why VLD improves reasoning while maintaining constant knowledge capacity—the work is primarily observational without providing insights into the underlying computational principles. The definitions of \"reasoning capability\" versus \"knowledge capacity\" could be more rigorous; measuring reasoning through benchmark accuracy may not capture the full complexity of reasoning processes. The observed non-monotonic scaling behavior, where performance occasionally degrades at higher VLD factors, is concerning and inadequately explained, suggesting potential optimization instabilities or fundamental limitations that practitioners would need to navigate. The experimental scope is limited to relatively small models (maximum 3B parameters) and specific task distributions, raising questions about whether findings generalize to frontier model scales and broader capabilities. Computational costs during training and inference are not discussed, despite increased depth potentially impacting efficiency. Finally, the paper positions VLD as an alternative to test-time compute scaling but provides no empirical comparison to increasingly popular methods like chain-of-thought reasoning or iterative refinement, leaving the relative merits unclear."}, "questions": {"value": "Can you provide any theoretical or mechanistic explanation for why VLD improves reasoning while keeping knowledge capacity constant? For instance, does the repeated processing through shared layers enable iterative refinement similar to recurrent computation, create implicit ensemble effects, or implement some form of algorithmic depth that's fundamentally different from parameter scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qc7lZm6he9", "forum": "u4wq305mNS", "replyto": "u4wq305mNS", "signatures": ["ICLR.cc/2026/Conference/Submission22144/Reviewer_TU6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22144/Reviewer_TU6T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984165487, "cdate": 1761984165487, "tmdate": 1762942087525, "mdate": 1762942087525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Virtual Logical Depth (VLD), which increases a model’s effective depth by reusing layer parameters without adding new ones. Experiments on GPT-2 models show that VLD boosts multi-step reasoning while leaving measured knowledge capacity nearly unchanged. A small validation on LLaMA-3.2-3B-Instruct after SFT also shows performance gains, suggesting potential benefits beyond synthetic tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, architecture-compatible idea that is easy to implement.\n- Clear synthetic improvements in reasoning at fixed parameter count.\n- Some external evidence from LLaMA-3B showing consistent gains."}, "weaknesses": {"value": "- Limited generality due to GPT-2-centric evidence. The main claims, including “reasoning increases while knowledge capacity stays constant”, are derived almost entirely from the results based on GPT-2-small-scale models, making it unclear whether the observations extend to modern architectures such as GPT-5, Gemini, or DeepSeek-R1.\n\n- LLaMA-3B SFT results cannot validate the core theory. SFT naturally improves downstream performance, so gains after fine-tuning do not strictly support the claimed scaling law, nor do they measure knowledge capacity on real models.\n\n- Unclear knowledge capacity definition. The paper’s definition of “knowledge capacity” relies solely on random-token memorization entropy, which may capture only a narrow slice of what large language models store. This metric may not reflect semantic knowledge, retrieval behavior, or long-context memory mechanisms, limiting the scope of the resulting conclusions."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6h5B0rgxUS", "forum": "u4wq305mNS", "replyto": "u4wq305mNS", "signatures": ["ICLR.cc/2026/Conference/Submission22144/Reviewer_xhss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22144/Reviewer_xhss"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762681303561, "cdate": 1762681303561, "tmdate": 1762942087097, "mdate": 1762942087097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}