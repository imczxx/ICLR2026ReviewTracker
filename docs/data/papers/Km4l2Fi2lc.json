{"id": "Km4l2Fi2lc", "number": 6379, "cdate": 1757976012847, "mdate": 1763096460385, "content": {"title": "Hallucination Mitigation in Large Vision-Language Models via Adaptive Multi-Subspace Projection", "abstract": "Recent advances in large vision-language models (LVLMs) have enabled powerful multimodal reasoning by integrating visual encoders with large language models (LLMs). However, their reliability is frequently undermined by hallucinations—generated text that inaccurately describes the visual input. Although fine-tuning can mitigate this, it is computationally expensive and demands large, curated datasets, making training-free alternatives more appealing. Among training-free strategies, model-editing is a more promising solution than decoding-based approaches. While decoding methods can adapt outputs per-input, they introduce substantial computational overhead and instability. Model-editing, by contrast, modifies the model's internal representations offline, offering a more efficient and stable framework. However, the effectiveness of current model-editing techniques is limited. Existing methods typically rely on a single, global subspace to correct errors. This static, one-size-fits-all approach treats all test samples identically, failing to capture the diverse modes of hallucination that vary from one input to another. To overcome this specific limitation, we propose a training-free hallucination mitigation framework that performs dynamic, per-instance suppression at test time. Our method advances the model-editing paradigm by first constructing a set of Disentangled Hallucination Subspaces, where each subspace isolates a distinct hallucination mode. Then, at inference, our model adaptively calculates weights to determine how a given input relates to each subspace. These weights guide a dynamically combined projection that selectively suppresses the most probable hallucination directions for that specific instance while preserving image-grounded semantics. Extensive experiments across multiple vision-language benchmarks and LVLMs families demonstrate consistent improvements, highlighting the robustness, generalizability, and efficiency of our approach.", "tldr": "Mitigating Hallucination in Large Vision-Language Models", "keywords": ["Language and Vision", "Hallucination"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/eb2fade90caa98a25be1a049845b6fdfa4aa17bd.pdf", "supplementary_material": "/attachment/e79adadc9091c778b22e4673ed148b9c01e2795f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free method to mitigate hallucinations in Large Vision-Language Models (LVLMs) through **adaptive multi-subspace projection**. The authors argue that existing model-editing approaches like Nullu use a single global subspace that fails to capture diverse hallucination patterns across different inputs. So, they construct multiple disentangled hallucination subspaces via K-means clustering and SVD, then adaptively weight these subspaces at test time based on input-specific hallucination signals derived from masked image perturbations. The method is evaluated on CHAIR and POPE benchmarks across three LVLM families (LLaVA-1.5, MiniGPT-4, mPLUG-Owl2), showing improvements over existing baselines including the recent Nullu method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a limitation of existing fixed model-editing methods: a single global subspace cannot adapt to the heterogeneous hallucination patterns that vary across different inputs. In my view, this observation is insightful and the proposed solution of using multiple subspaces with adaptive weighting represents a natural and promising direction for improvement.\n\n2. The inclusion of ablation studies on the number of subspaces, basis dimensions, and perturbation strategies demonstrates investigation of the method's behavior. The consistency of improvements across different settings is encouraging.\n\n3. The method maintains the training-free property which is valuable for practical deployment. Unlike fine-tuning approaches that require curated datasets and substantial computational resources, the proposed approach offers a reasonable middle ground by preprocessing subspaces offline and applying lightweight adaptive weighting at test time.\n\n4. The two-stage framework is well-designed and the mathematical formulation is generally clear."}, "weaknesses": {"value": "- In my opinion, the writing of the paper could be improved. The reported improvements over Nullu are relatively modest in limited statistical validation of improvements, and given the standard deviations shown some gains may not be significant. The paper would be much stronger with paired t-tests or similar statistical validation to confirm these improvements are reliable rather than random variation.\n\n-  While the paper claims efficiency advantages, no actual inference times, memory usage, or FLOPs are reported to validate this. Additionally, several key technical details lack clarity: **semantically salient regions** for masking (see in Equation 13) is never defined.\n\n- Table 3 reveals that increasing basis vectors improves hallucination metrics but causes substantial BLEU degradation, suggesting over-suppression of legitimate content. While the authors select **balanced** hyperparameters empirically, there is no principled guidance for navigating this trade-off in new settings, and no theoretical understanding of why it occurs."}, "questions": {"value": "1. Could you add statistical significance tests to validate that the improvements over Nullu are reliable rather than within-noise variation? This would substantially strengthen the empirical claims.\n\n2. How are **semantically salient regions** identified for the masking operation? Please provide implementation details or point to the specific saliency method used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JgZg4HIJlh", "forum": "Km4l2Fi2lc", "replyto": "Km4l2Fi2lc", "signatures": ["ICLR.cc/2026/Conference/Submission6379/Reviewer_c3Mr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6379/Reviewer_c3Mr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905516012, "cdate": 1761905516012, "tmdate": 1762918667014, "mdate": 1762918667014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "LqyXm5cK3O", "forum": "Km4l2Fi2lc", "replyto": "Km4l2Fi2lc", "signatures": ["ICLR.cc/2026/Conference/Submission6379/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6379/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763096458951, "cdate": 1763096458951, "tmdate": 1763096458951, "mdate": 1763096458951, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free framework to mitigate hallucinations in LVLMs. The method first constructs a set of disentangled hallucination subspaces via SVD and K-means. Then, at inference time, the model adaptively creates specific weights from the subspaces to alleviate the hallucination, via two forward queries of the original image and masked image."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The novelty is sound. The authors find an adaptive method to calculate specific weights for different queries, which is often neglected in other hallucination papers, as the hallucination type is different for different inputs. \n2. The general method builds up with the training-free methods, while adaptively creating specific weights from a clustered subspace, which seems to be superior to some other training-free methods. \n3. The method is well evaluated across different base models."}, "weaknesses": {"value": "1. The paper's Table 3 shows that as hallucination suppression increases (using more basis vectors), the BLEU score drops significantly. While BLEU is not a comprehensive metric for modern LVLMs, this still raises concerns about the degradation in general model performance. The evaluation is narrow on hallucination benchmarks and lacks testing on broader, general-purpose benchmarks (MMMU/VQAv2/...) to confirm that the model's abilities are not so compromised.\n\n2. One of the core claims is that it identifies distinct hallucination modes. However, the authors do not provide any qualitative analysis or evidence, or visualization to validate that these disentangled subspaces actually correspond to semantically different types of hallucinations.\n\n3. The method requires two forward passes at test time (one for the original image and one for the masked one) to get a diff. This will introduce extra computation. Moreover, it remains unclear why the specific difference signal serves as a proxy for the input-specific hallucination signal. Lastly, the authors seem to miss the exact strategy for masking. Is it random black masking or other strategies?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PiKGOxKFSV", "forum": "Km4l2Fi2lc", "replyto": "Km4l2Fi2lc", "signatures": ["ICLR.cc/2026/Conference/Submission6379/Reviewer_ftf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6379/Reviewer_ftf2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935313733, "cdate": 1761935313733, "tmdate": 1762918666692, "mdate": 1762918666692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free way to reduce hallucinations in large vision-language models (LVLMs) by editing their internal activations at test time instead of fine-tuning them. The key idea is to build multiple low-rank “hallucination subspaces,” each representing a different type of hallucination, by comparing model states from truthful vs. hallucinated captions. At inference, the model estimates which hallucination modes are most likely for the current input image, then dynamically projects its hidden representations away from those directions, suppressing ungrounded content while keeping image-relevant semantics. Experiments on benchmarks like CHAIR and POPE across models such as LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2 show that this adaptive multi-subspace projection reduces hallucinations more consistently than prior decoding-based or single-subspace editing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper models hallucination not as one global direction but as multiple disentangled subspaces, each tied to a different hallucination mode. At test time it adaptively weighs these subspaces for the current input and projects away the most risky directions, which leads to stronger hallucination suppression than fixed one-subspace editing.\n\n2. The ablation shows that different LVLM backbones prefer different numbers of subspaces (e.g., 7 for LLaVA-1.5, 11 for MiniGPT-4, 5 for mPLUG-Owl2). This suggests each model has its own “hallucination landscape,” rather than a universal structure. Making these subspaces interpretable in semantic terms (e.g., “spurious object insertion,” “wrong spatial relation”) would be a valuable next step."}, "weaknesses": {"value": "1. The method needs two forward passes at inference: it runs the LVLM on both the original image and a perturbed/masked version to estimate which hallucination modes are likely, then applies the adaptive projection. Prior fixed-edit approaches only require a single edited forward. Authors need to report a compute/runtime comparison against those baselines. \n\n2. The “contrastive dataset” used to build the hallucination subspaces is under-specified. The paper does not state where the images/captions come from, how large this dataset is. Without dataset source/scale, it is hard to judge fairness and reproducibility. \n\n3. The experiments are only on older/open LVLMs (LLaVA-1.5, MiniGPT-4, mPLUG-Owl2). There is no evidence that the approach still works on newer high-capability MLLMs (e.g., recent Qwen2.5-VL–series) ."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LLyPrzfWBz", "forum": "Km4l2Fi2lc", "replyto": "Km4l2Fi2lc", "signatures": ["ICLR.cc/2026/Conference/Submission6379/Reviewer_Wtxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6379/Reviewer_Wtxx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972417872, "cdate": 1761972417872, "tmdate": 1762918666222, "mdate": 1762918666222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}