{"id": "2YvbLQEdYt", "number": 14517, "cdate": 1758237822072, "mdate": 1759897365476, "content": {"title": "CyberGym: Evaluating AI Agents' Real-World Cybersecurity Capabilities at Scale", "abstract": "AI agents have significant potential to reshape cybersecurity, making a thorough assessment of their capabilities critical.\nHowever, existing evaluations fall short, because they are based on small-scale benchmarks and only measure static outcomes, failing to capture the full, dynamic range of real-world security challenges.\nTo address these limitations, we introduce CyberGym, a large-scale benchmark featuring 1,507 real-world vulnerabilities across 188 software projects.\nAdjustable to different vulnerability analysis settings, CyberGym primarily tasks agents with generating a proof-of-concept test that reproduces a vulnerability, given only its text description and the corresponding codebase.\nOur extensive evaluation highlights that CyberGym effectively differentiates agents' and models' cybersecurity capabilities.\nEven the top-performing combinations only achieve a ~20% success rate, demonstrating the overall difficulty of CyberGym. \nBeyond static benchmarking, we show that CyberGym leads to the discovery of 35 zero-day vulnerabilities and 17 historically incomplete patches.\nThese results underscore that CyberGym is not only a robust benchmark for measuring AI's progress in cybersecurity but also a platform for creating direct, real-world security impact.", "tldr": "", "keywords": ["Cybersecurity", "AI", "Agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e5bb70413346d206c0711060acdaa1509a5df69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CyberGym, a large-scale, execution-based benchmark for assessing AI agents’ cybersecurity skills. CyberGym aggregates 1,507 historical vulnerabilities from 188 OSS projects, packages each into a containerized environment, and defines a primary task: given a short natural-language description and the pre-patch codebase, an agent must generate a PoC that reproduces the vulnerability. Success is determined by execution with sanitizers: crash on pre-patch, no crash on post-patch. The benchmark also provides four difficulty levels by progressively adding auxiliary signals (description, stack trace, patch diff). A broad evaluation across 4 agent frameworks and 11 LLMs shows CyberGym is hard yet discriminative; the best combination reaches 22.0% success rate. The authors further demonstrate real-world impact, surfacing 17 incomplete patches and 35 zero-day vulnerabilities (10 unique from benchmark runs; 25 more from open-ended discovery across 431 projects), handled via responsible disclosure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper introduces a novel dataset collected at scale with 1,507 instances across 188 projects - a significant scale-up compared to prior benchmarks, while remaining grounded in real-world vulnerabilities; broad sanitizer crash types; substantial codebases.\n- The benchmark leverages execution-based metrics (pre- vs post-patch sanitizer) with reproducible containerization; experiment results reveal that the benchmark is difficult and discriminative.\n- Significant real-world impact: identification of 17 incomplete patches and 35 zero-days with responsible disclosure."}, "weaknesses": {"value": "- While the paper provides a valuable benchmark resource, the findings themselves contain limited novelty. Findings largely reaffirm known trends (thinking helps; richer inputs help; long PoCs are hard) without surfacing new mechanisms, patterns, or insights. Most of the discussions remain surface level, and the paper does not propose any novel algorithms or methodologies to improve performance.\n\n- The authors claim data contamination has limited impact on model performance but provide no formal justification. The paper provides neither details of the experiments nor full experimental results, reporting only a **coarse two-model split** by disclosure dates in which GPT-4.1 drops from 9.7% to 5.6% post-cutoff. Methodology details (split sizes, difficulty balance, statistics) are missing, so the claim is not convincingly supported.\n\n- Agent behavior analysis is shallow. The paper reports tool-usage stats and asserts “distinct patterns,” but lacks qualitative trajectory analyses, failure taxonomies, or PoC-type breakdowns to make those patterns actionable. Discussions are descriptive rather than diagnostic.\n\n- Cross-model/agent comparisons mix different “thinking”/tool budgets; results would be stronger with matched token/step/time budgets and variance estimates.\n\n- Commit-message rephrasing and filtering via GPT-4.1 could introduce noise/bias. Having human annotation/verification metrics would help."}, "questions": {"value": "- Can the authors provide failure modes of different agents at different difficulty levels? What do the agents spend the most time on (e.g., environment setup, reading code, crafting exploit, etc.)? Have the authors explored whether different environment or interface setups would lead to varying performance?\n\n- For contamination assessment, please provide: (a) counts per split (pre/post cutoff) for each model, (b) difficulty/PoC-length distributions per split, (c) statistical tests (e.g., CIs, effect sizes) for success-rate differences, and (d) results across more backbones. Do conclusions hold beyond Claude-3.7-Sonnet and GPT-4.1?\n\n- Current coverage skews to sanitizer-detectable C/C++ memory-safety issues. What is your roadmap for additional oracles (e.g., web/mobile, logic bugs) and languages? How would tasks/metrics adapt? In the interim, have you tried balanced resampling (by crash type/project) to test robustness of conclusions?\n\n- For GPT-4.1 rephrasing/filters, did you run expert audits? Please share sample sizes, agreement metrics, precision/recall (or error rates), and any corrections applied.\n\n- Can the authors provide matched-budget (tokens/steps/time) comparisons across models, and sensitivity of rankings under shared caps?\n\n- Have the authors considered reporting impact-weighted metrics (e.g., by crash type or CVSS proxy) so success reflects security significance, not just any crash?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rbWCSZJFWt", "forum": "2YvbLQEdYt", "replyto": "2YvbLQEdYt", "signatures": ["ICLR.cc/2026/Conference/Submission14517/Reviewer_7oMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14517/Reviewer_7oMB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760940657730, "cdate": 1760940657730, "tmdate": 1762924912395, "mdate": 1762924912395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CyberGym a large-scale benchmark designed to evaluate AI agents capabilities in real-world cybersecurity tasks. The core motivation is the inadequacy of current static, small-scale benchmarks, which fail to capture the dynamic, iterative nature of real-world security challenges. CyberGym addresses this by featuring 1,507 real-world vulnerabilities across 188 software projects, emphasizing a dynamic, interactive assessment where agents must iteratively generate and test exploits to achieve a successful outcome. \n\nThe authors demonstrate the utility of this benchmark by evaluating various current AI models(GPT, Claude, Gemini, DeepSeek and Qwen) on this challenging task set."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The use of 1,507 real-world vulnerabilities across 188 software projects is a substantial and necessary advancement over existing small-scale or synthetic benchmarks especially in the domain specific field of cubersecurity. \n\n2. The requirement for dynamic and iterative problem-solving accurately reflects the complex and exploratory nature of real-world vulnerability research.\n\n3. The intent to release a large, accessible dataset and environment is crucial for reproducibility in this field. The authors have done a good job committing to open-sourcing the dataset."}, "weaknesses": {"value": "1. The authors must detail the computational cost required to run the full benchmark suite for future research teams. \n\n2. The authors should have used a human cybersecurity analyst as a baseline reference to compare with the agents reference."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fkvCADgDCj", "forum": "2YvbLQEdYt", "replyto": "2YvbLQEdYt", "signatures": ["ICLR.cc/2026/Conference/Submission14517/Reviewer_Bi9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14517/Reviewer_Bi9L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624441276, "cdate": 1761624441276, "tmdate": 1762924911858, "mdate": 1762924911858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark (CyberGym) to evaluate the capabilities of LLM agents in reproducing exploits of previously known vulnerabilities. The authors curate 1507 vulnerabilities across 188 software projects sourced from Google’s OSS-Fuzz, and defined four tasks for each vulnerability with increasing level of difficulty based on the information availability. Interestingly, the paper shows that even though existing LLM agents struggle to accurately reproduce existing vulnerabilities (around 20%), they were still able to discover 35 zero-day vulnerabilities in the latest versions of the included projects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The thoroughness and rigor in collecting and filtering the dataset.\n    \n- The fact that the benchmark helped find 35 zero-day vulnerability is impressive.\n    \n- The paper includes useful and interesting sub-experiments and ablations, e.g., effect of thinking mode, and investigating data contamination."}, "weaknesses": {"value": "- One minor weakness of the paper is that it does not sufficiently motivate the need for difficulty levels 1, 2, and 3. It is unclear to me when it useful to reproduce an exploit when we already have the stack trace from running the ground truth exploit (level 2), or when we have the ground truth patch (level 3). You mention that level 3 is useful in one-day settings, but to my knowledge, one-day settings are when the vulnerability is discovered but still not patched.\n    \n- The authors mention that considering the union of outcomes across several LLMs greatly improves the results. However, you did not show results for combining the best performing LLMs and agents (e..g., combining Claude-4, Claude-3.7, and GPT4, with thinking mode on). This could help show the performance upper bound of current systems."}, "questions": {"value": "- What is the real-world benefit of benchmarking LLM agents when using difficulty levels 1, 2, and 3?\n    \n- Do your benchmark include a way to evaluate whether a patch is successful by ensuring that the original program functionality is maintained? If no, do you think adding such feature to benchmarks is challenging?\n    \n- How accessible is your benchmark? For instance, how much would it cost (dollars, or GPU hours) to evaluate a new LLM or agentic approach? Are there ways to mitigate the potentially high evaluation cost (e.g., provide a smaller subset of the benchmark)?\n    \n- You mention in Appendix E a case where one vulnerability was patched over several commits. Is this a failure of your benchmark, where you should have detected that and included this in the ground truth information of that vulnerability? Does this affect the accuracy of your benchmark?\n    \n- How do you plan to deal with the mentioned cases when the LLM defers actions and asks for user confirmation? This could mistakenly skew the results.\n    \n- In total, how many of the 1507 vulnerabilities were reproduced by at least one LLM? In other words, do all LLMs detect the same vulnerabilities, or does combining their outcomes bring more gains? (check weakness 2)\n    \n- Did you test running level 0 tasks (reproducing the vulnerability with no additional information) on known existing vulnerabilities? This could simulate the process of rediscovering zero-day vulnerabilities in historical data, especially when evaluating on vulnerabilities found after the knowledge cutoff of the LLM.\n    \n- CyberGym was able to discover zero-day vulnerabilities using the 759 PoCs/exploits obtained by reproducing existing vulnerabilities. I am confused as to how the resulting vulnerabilities are considered zero-day when their ground PoCs already exist?\n    \n\nMinor issues:\n\n- In lines 426 and 437, the referred results are in Appendix E not D."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GbJd4E6Qr3", "forum": "2YvbLQEdYt", "replyto": "2YvbLQEdYt", "signatures": ["ICLR.cc/2026/Conference/Submission14517/Reviewer_yga9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14517/Reviewer_yga9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675335103, "cdate": 1761675335103, "tmdate": 1762924911363, "mdate": 1762924911363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CyberGym is a benchmark for agents to discover vulnerabilities in OSS projects.\n- Sourced 1507 vulneabilities from OSS-Fuzz (spanning 2017-2025, a chunk beyond most frontier models' cutoff - data contamination assessment shows limited contamination)\n- Contains containerized pre-patch codebase, post-patch codebase, ground truth PoC (produced by OSS-Fuzz), and ground truth patch\n- Agent must produce a Proof-of-concept bash script that (1) Triggers sanitizer crash in pre-patched version (2) Do not trigger sanitizer crashes in post-patched versions.\n- The codebases have median of 1k files, and ground truth PoCs\n\nLevels 0-3 of information provided to the agent:\n- Prepatch codebase only (open-ended) < +Vul. Text description < +Ground truth PoC and Stack trace < Ground truth patch (diff)\n\nExperiments with OpenHands agent scaffolding.\n\nSome interesting results:\n- Specialized models with strong results on SWE bench may actually perform poorly on Cyberbench\n- Best results with Sonnet-4 and GPT-4.1\n-"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The authors seem to have taken strong steps towards reproducibility by providing the pre-patched program in a containerized environment.\nThe vulnerabilities are present in real open source projects, which makes it a very realistic benchmark.\nThe authors discover zero-day vulnerabilities in the Level 0 mode of the benchmark; this means the approach taken in the benchmark can be used by maintainers to discover new vulnerabilities."}, "weaknesses": {"value": "Most of the vulnerabilities seem to be related to C/C++ bad memory usage. While some of the projects are extremely popular, it should be made clear that it is really a subset of cyber-risks. \"Cybersecurity capabilities\" might be over-selling it a bit.\n\nSuccess of PoC is taken as crashing the sanitizer. I understand why the authors made this choice, but again, this is a narrow subset of vulnerabilities and I think the paper can be more explicit regarding the limitations of scope."}, "questions": {"value": "Which level is Figure 3? I assume it's level 1 from cross-referencing w/ figure 6 but it should be explicit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gJqn2jXX5U", "forum": "2YvbLQEdYt", "replyto": "2YvbLQEdYt", "signatures": ["ICLR.cc/2026/Conference/Submission14517/Reviewer_cQYX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14517/Reviewer_cQYX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860929686, "cdate": 1761860929686, "tmdate": 1762924910930, "mdate": 1762924910930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}