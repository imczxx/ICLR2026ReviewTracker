{"id": "d3CISVVO6v", "number": 8327, "cdate": 1758078779241, "mdate": 1759897791559, "content": {"title": "Multimodal Aligned Semantic Knowledge for Unpaired Image-text Matching", "abstract": "While existing approaches address unpaired image-text matching by constructing cross-modal aligned knowledge, they often fail to identify semantically corresponding visual representations for Out-of-Distribution (OOD) words. Moreover, the distributional variance of visual representations associated with different words varies significantly, which negatively impacts matching accuracy. To address these issues, we propose a novel method namely Multimodal Aligned Semantic Knowledge (MASK), which leverages word embeddings as bridges to associate words with their corresponding prototypes, thereby enabling semantic knowledge alignment between the image and text modalities. For OOD words, the representative prototypes are constructed by leveraging the semantic relationships encoded in word embeddings. Beyond that, we introduce a prototype consistency contrastive loss to structurally regularize the feature space, effectively mitigating the adverse effects of variance. Experimental results on the Flickr30K and MSCOCO datasets demonstrate that MASK achieves superior performance in unpaired matching.", "tldr": "We propose multimodal aligned semantic knowledge, which leverages word embeddings as bridges to associate words with prototypes, capturing semantic relationships between words and further utilizing information from OOD words.", "keywords": ["Unpaired Image-text Matching", "Out-of-Distribution Word", "Multimodal Aligned Semantic Knowledge", "Prototype"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cf85564ad2c9473b0e724e54b59dbfd096642b6.pdf", "supplementary_material": "/attachment/017a7e48b0d3d03a037925b9df3a461a2697282d.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses an underexplored issue in cross-modal matching tasks: image-text matching without pairing. The authors propose a new method called MASK, whose core is to construct a cross-modal aligned semantic knowledge base. The model uses word embeddings as a bridge to match words with their prototypes, and by introducing prototype consistency contrastive learning loss to regulate the feature space, it overcomes the shortcomings of existing methods in handling out-of-distribution words and variance of visual representation distribution. Experiments on the Flickr30K and MSCOCO datasets show that MASK achieves leading performance in the unmatched matching task and can effectively serve as a re-ranking module to enhance the performance of existing pre-trained multimodal models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe framework proposed in this paper is intuitive and effective. It utilizes the semantic relationships of word embeddings to construct prototypes for out-of-distribution (OOD) vocabulary and a prototype consistency contrastive learning loss, clearly pointing out the key weaknesses of existing methods and having a clear motivation.\n2.\tThis paper introduces external pre-trained word vectors as an auxiliary supervisory signal to establish an equivariant mapping that preserves the relationship between regional representations and word embeddings. This enables regional representations to effectively capture the semantic relationships between words.\n3.\tThe experimental results on Flickr30K and MSCOCO show that MASK outperforms existing models and knowledge-based methods significantly in the image-text matching task, without paired images, proving its effectiveness.\n4.\tMASK can acquire a kind of logical semantic knowledge based on conceptual prototypes through structured learning. Uses MASK as an reordering enhancement plugin for CLIP and ALBEF models, demonstrating its effectiveness in prototype consistency contrastive learning and semantic relationship alignment.\n5.\tThe appendix provides mathematical proofs for concepts such as peer-to-peer transformation mapping and the rationality of loss functions, enhancing the rigor and depth of the method. This is particularly valuable in applied research papers."}, "weaknesses": {"value": "1.\tThe explanation of the OOD vocabulary processing mechanism is unclear. This is one of the core innovations of this paper. However, the description in Section 3.4 is overly mathematical and lacks an intuitive, step-by-step concrete example. For instance, when encountering a word that does not exist in the knowledge base, how can we use the Glove word vectors to find similar known words and weight them to synthesize their visual prototypes? The current explanation remains at the formula level, which is less readable and reduces the comprehensibility of the method.\n2.\tTo construct the prototype of the OOD vocabulary, we need to sample m pairs of representations from the knowledge base. The paper does not explain at all how this sampling is carried out. Is it random sampling? Or is it Top-K sampling based on the semantic similarity with the OOD words? Different sampling strategies will greatly affect the quality of the final prototype, which is an important unspecified hyperparameter and must be clarified.\n3.\tThe paper did not analyze the overlap between the vocabulary in the test set and the VG's 12,385 concepts. If the vast majority of nouns in the test set are already present in the knowledge base, then the challenge and performance improvement attributed to OOD words might be overestimated. It is necessary to clearly define the specific definition and proportion of OOD in the experiments."}, "questions": {"value": "1.\tDoes the MASK method have good generalization ability on other types of datasets? For instance, how does the model perform on some datasets that contain more domain-specific words or more complex visual scenarios? Is there a plan to verify it on more diverse datasets?\n2.\tIf certain words are not included in the pre-trained word vectors, or if the semantics of the words have changed in a specific domain, how will the MASK method handle this situation? Has there been any consideration of reducing reliance on the pre-trained word vectors or introducing other methods to obtain the semantic information of the words?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eerKa14ehs", "forum": "d3CISVVO6v", "replyto": "d3CISVVO6v", "signatures": ["ICLR.cc/2026/Conference/Submission8327/Reviewer_EZrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8327/Reviewer_EZrZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468219813, "cdate": 1761468219813, "tmdate": 1762920252070, "mdate": 1762920252070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MASK (Multimodal Aligned Semantic Knowledge), a framework for unpaired image-text matching that leverages external semantic knowledge to bridge the gap between textual and visual modalities.\nInstead of relying on paired image-text data, the method constructs visual prototypes and uses pretrained word embeddings to generate prototypes for out-of-distribution (OOD) words through a weighted combination of existing ones.\nA prototype-consistency contrastive loss is further introduced to reduce intra-class variance and enhance alignment stability between textual and visual representations.\nExtensive experiments on Flickr30k and MSCOCO demonstrate competitive retrieval performance and validate the benefit of incorporating external knowledge.\nThe approach can also serve as a lightweight re-ranking module to enhance existing vision-language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel cross-modal semantic alignment method, MASK, which constructs representative prototypes for OOD words by exploiting the intrinsic relationships amongword embeddings.\n\n2. The paper provides solid comparisons with both model-based and knowledge-based baselines on Flickr30k and MSCOCO, together with ablation studies, sensitivity analysis, and visualization of learned representations.\n3. The proposed method can serve as a lightweight plug-in re-ranking module, enhancing existing vision-language models such as CLIP or ALBEF without retraining them end-to-end.\n\n4. This paper introduce a prototype consistency contrastive learning loss to structurally regularize the feature space, mitigating the adverse impact of distributional variance."}, "weaknesses": {"value": "1. Over-strong assumption and circular reasoning in the OOD prototype proof\nThe theoretical analysis in Appendix B relies on a strong linear/isometric assumption that equates textual and visual prototype spaces. Moreover, the proof assumes the existence of a relation-preserving mapping \nf,which is exactly what the model is trained to learn—thus creating a circular argument. Additional experiments or a more rigorous justification are needed to support this claim.\n\n2. High dependence on region proposals\nThe method heavily relies on the quality of the region proposals provided by upstream object detectors. The experiments show large performance gaps between different detectors (e.g., BUTD vs. DETR/DINO), indicating that the framework’s robustness and generalization depend strongly on detector performance.\n\n3. Insufficient analysis of pretrained word embeddings\nThe approach uses pretrained word vectors to construct OOD prototypes, but the effect of different embeddings (e.g., GloVe, word2vec, fastText) is not explored.  Since the semantic geometry of word embeddings directly affects OOD prototype generation, additional ablation or sensitivity analysis would strengthen the paper.\n\n4. Missing discussion on the parameter m for OOD prototype construction.\nIn Eq.(8), OOD prototypes are computed by weighting m known prototypes, but the paper does not explain how m is chosen or provide related experiments. The selection of m could significantly affect the semantic quality of constructed prototypes.\nIncluding an analysis or sensitivity study on this parameter would help clarify its impact and strengthen the conclusion."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y845fgheif", "forum": "d3CISVVO6v", "replyto": "d3CISVVO6v", "signatures": ["ICLR.cc/2026/Conference/Submission8327/Reviewer_VshV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8327/Reviewer_VshV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759159573, "cdate": 1761759159573, "tmdate": 1762920251572, "mdate": 1762920251572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MASK to address the issue of unpaired ITM by focusing on OOD words. \n(1) They use relationship among word embeddings to construct prototypes of OOD words. \n(2) A new consistency contrastive loss is proposed to make compact prototypes. \n(3) Pretrained word embeddings are used for relation-preserving equivalent mapping. \nAbound experiments are performed to illustrate the effectiveness and efficiency of MASK."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[1] MASK surpasses MACK[NeurIPS 22] and MACK++[TPAMI 24], which are the 1st work for unpaired ITM. So, MASK is the new SoTA work. \n\n[2] MASK made several smart designs(contributions): (1) Information retention loss: a self-supervised noise-adding objective, in which, \nFRM module will ONLY be used in knowledge construction phase, but NOT in infer phase. (2) External Textual knowledge(=GloVe) is used, which can be considered as auxiliary GT info, especially in Eq.(9) with the help of MTM module. (3) The semantic relationship between(among) words are captured(used/exploited) for enhancing generalization, which is established in Eq.(9) and Eq.(12)."}, "weaknesses": {"value": "[1] All of the weaknesses, please see the following Questions Part."}, "questions": {"value": "[1] In Table 7，it seems that the table title should NOT be “detectors” but “architecture”. Otherwise, it will be the same as the title of Table 9. \n※ p4 ln210: Lcl is defined as follows D: WHAT is D? Appendix D? \n※ p5 ln233: f should satisfy B: WHAT is B? B!=8? You mean Eq.(8)? \n\n[2] It will be better if the authors give an answer about Fig. 2 that \nWHETHER the (1) Pretrained F-RCNN and (2) Pretraind word vectors \nwill BP (which means: requires_grad == True)? \n※ I guess BOTH of them do NOT need grad to BP. \n※ Is there MORE module that also NOT need BP? F-RCNN? \n\n[3] OOD word/vocab is relative, NOT absolute. Because Glove (pretrained word emb) is likely to know so called “OOD word”. So, “OOD” is relative to CLIP/ALBEF (pretrained VLM) or F30k/COCO (dataset). \n※ In addition, in Fig. 2, “cat” seems to be a quite common seen word, NOT OOD. \n“otter[≈≈freshwater carnivorous mammal]/manga[≈≈comic]” may be more suitable. \nSo, a more SOUND example should be given, especially in REAL TEST VISUALIZATION. \nPlease give us some REAL test examples about your MASK with REAL OOD word. \ne.g. in T2I retrieval, a text query with OOD word makes original CLIP model search the GT image on Top 3, but after your MASK rerank, the GT image gets up to Top 1. \nYou can make VISUALIZATION about the Pos as well as Neg on I2T and T2I (F30k). \n\n[4] What about the Neg (Wrong) example visualization? Is there any OOD word? \nHow to EXPLAIN it? WHY it makes wrong? What TYPE of word it is? verb/prep or n./adj.？\n※ We know that, verb(v.), preposition(prep.) are harder than noun(n.), adjective(adj.). \nIs there any method to make v./prep. understanding more precise? \n※ How to make these MISTAKES to be alleviated? MORE effective methods? \n※ We AGGRE WITH your work, but we also wonder HOW it is WORKED indeed ! \n\n[5] Big Models (Large Language Models) are popular, and may have super high score now. \nWe want to know how many scores your MASK method can improve? \n※ Just Eval, DO NOT need any finetune/train! WHY NOT Keep pace with the times! \n※ SigLIP v1 can get 570.84 score on f30k 1K Test. We have already tested! \nsee: https://huggingface.co/google/siglip-so400m-patch14-384/tree/main\n※ Since Big Model can get 570+ score, CAN Big Model+MASK get 600 score (full score)? \n\n[6] More powerful region encoder, may lead to more score! \n※ F-RCNN ResNet 152 is more powerful than F-RCNN ResNet101. \n※ More SOTA image/region encoder for More Ablation studies? Swin Transformers? \n\n[7] What if w’ in the second term of Eq.(9) change into w? \n※ Because we think w is GT, but w’ not. Maybe GT is better. \n\n[8] p5 ln261: we first sample m paired ...\n※ How to sample? I don’t understand. In Appendix? \n※ I guess: using GloVe(=pre-trained word embeddings) to get Top m high score word."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RlV6mxiFm9", "forum": "d3CISVVO6v", "replyto": "d3CISVVO6v", "signatures": ["ICLR.cc/2026/Conference/Submission8327/Reviewer_PJM3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8327/Reviewer_PJM3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056259648, "cdate": 1762056259648, "tmdate": 1762920250410, "mdate": 1762920250410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}