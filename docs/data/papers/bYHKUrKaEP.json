{"id": "bYHKUrKaEP", "number": 22026, "cdate": 1758325053189, "mdate": 1763044321306, "content": {"title": "REAR: Retrieval-Augmented Egocentric Action Recognition", "abstract": "Egocentric Action Recognition (EAR) aims to identify fine-grained actions and interacted objects from first-person videos, forming a core task in egocentric video understanding. Despite recent progress, EAR remains challenged by limited data scale, annotation quality, and long-tailed class distributions. To address these issues, we propose REAR, a Retrieval-augmented framework for EAR that leverages external third-person (exocentric) videos as auxiliary knowledge---without requiring synchronized ego-exo pairs. REAR adopts a dual-branch architecture: one branch extracts egocentric representations, while the other retrieves semantically relevant exocentric features. These are fused via a cross-view integration module that performs staged refinement and attention-based alignment. To mitigate class imbalance, a class-adaptive selector dynamically adjusts retrieval depth based on class frequency, and independent classifiers are trained with logit-adjusted cross-entropy. Extensive experiments across three benchmarks demonstrate that REAR achieves state-of-the-art performance, with significant gains in object recognition and tail-class accuracy. Code will be released upon acceptance.", "tldr": "", "keywords": ["Egocentric Action Recognition", "Retrieval-Augmented", "Long-tail Recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9f810169d18e7a8abf761e769e5d5f14e6b64d4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "REAR is a retrieval-augmented egocentric action recognition framework that enriches ego features with top‑k exocentric videos retrieved via an EgoExoNCE‑trained cross‑view retriever. A dual‑branch shared encoder fuses retrieved exo features through similarity‑weighted aggregation and cross‑view attention, while a class‑adaptive selector increases k for tail classes and LACE aids long‑tail training. Across EPIC‑Kitchens‑100, EGTEA, and Charades‑Ego, REAR delivers consistent gains—especially on tail classes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Performance is strong across benchmarks, with consistent gains.\n- Particularly effective for long-tailed and noun recognition, a known bottleneck in egocentric understanding."}, "weaknesses": {"value": "- Academic novelty relative to EgoInstructor [1] is unclear. The core concepts—cross-view ego↔exo retrieval trained with EgoExoNCE and augmenting/fusing them for better downstream task—are inherited. REAR primarily swaps the downstream task from caption generation to closed-set action recognition and adds some engineering and modest architectural modification. Demonstrating transfer of retrieval augmentation to recognition is useful, but the conceptual advance beyond EgoInstructor appears incremental.\n\n- Closest related work is mispositioned. The manuscript identifies retrieval-augmented image recognition as the closest prior, but EgoInstructor (Retrieval-Augmented Egocentric Video Captioning) is the most directly related and should be foregrounded with a clear distinction of what is genuinely new.\n\n- The class-adaptive selector is heuristic.\n\n\n[1]: Xu, Jilan, et al. \"Retrieval-augmented egocentric video captioning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "The previous work, Egoinstructor, addresses the more challenging task of captioning, whereas this work focuses on closed-set action recognition, which is arguably an easier problem. I'm not convinced that this shift to a simpler task, on its own, constitutes a meaningful or novel contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5riuwoJS7", "forum": "bYHKUrKaEP", "replyto": "bYHKUrKaEP", "signatures": ["ICLR.cc/2026/Conference/Submission22026/Reviewer_ep8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22026/Reviewer_ep8L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873307011, "cdate": 1761873307011, "tmdate": 1762942026084, "mdate": 1762942026084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "bxdC6haQTW", "forum": "bYHKUrKaEP", "replyto": "bYHKUrKaEP", "signatures": ["ICLR.cc/2026/Conference/Submission22026/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22026/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763044320261, "cdate": 1763044320261, "tmdate": 1763044320261, "mdate": 1763044320261, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a two-stream architecture that implements a RAG framework for egocentric video action recognition. The contribution are on the retrieval mechanism and overall model design. Experiments show significant gains with including the RAG branch, and achieves sota results over recent baselines in four datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is one of the first works on RAG for video action recognition combining ego and exo view, which is an interesting research direction also for zero-shot. The model design is relatively simple and intuitive, and the components seem appropriately chosen and adapted to realize the framework. The paper is well written."}, "weaknesses": {"value": "The RAG integration and overall architecture design is effective but not particularly original in its components. It is combining cross-attention, softmax for weighting, and LACE loss adjustment.\n\nEquation 6 states that there are 3 different values for k while the MLP in 8 uses k stacked features as input. It is not detailed how the MLP deals with three different input lengths - using three MLPs with some weights sharing mechanism? Please clarify (see Questions section below).\n\nThere is no ablation or motivation on including $f^{\\text{exo}}_i$ also in $f'$ in Eq.10. Please clarify (see Questions section below).\n\nResources needed for training and inference are not specified. Please clarify (see Questions section below)."}, "questions": {"value": "Please clarify how the MLP in eq.8 deal with the three different input lengths - using three MLPs with some weights sharing mechanism?\n\nProvide an ablation and motivation on including $f^{\\text{exo}}_i$ also in $f'$ in Eq.10. \n\nProvide details on the resources needed for training and inference are not specified. It is stated that \"conducted on NVIDIA V100 GPUs\" - how many? For how long? What is inference latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yVul5B3PWF", "forum": "bYHKUrKaEP", "replyto": "bYHKUrKaEP", "signatures": ["ICLR.cc/2026/Conference/Submission22026/Reviewer_zKTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22026/Reviewer_zKTe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905164695, "cdate": 1761905164695, "tmdate": 1762942025751, "mdate": 1762942025751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of egocentric action recognition in which exocentric information is used in a retrieval augmented strategy. The paper proposes a new method which first finds the closest exocentric videos from a corpus for an inputted egocentric video (which represents a different dataset). These closest exocentric videos are then combined to help inference for the inputted egocentric video for the relevant actions/verb/noun classification. The method is tested across three egocentric datasets (EPIC-Kitchens, EGTEA Gaze+, Charades Ego) and YouCook2 as the exocentric gallery of videos. The results show that the method achieves the new state of the art results across all datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of using retrieval from the exocentric domain is a nice concept and is implemented in a way that is not over-complicated, makes sense, and isn't a naive fusion method.\n* The motivation of the method and the paper is strong and well formulated.\n* The paper writing is generally clear and easy to follow.\n* Results show the method's performance across the long-tail of actions with the method performing well on the tail classes."}, "weaknesses": {"value": "* $R$ is not specified in equation 2, I assume this is the retrieval module, but this does not use the visual encoder? Reading onwards it becomes a bit more clear that the cross-view retriever is different, but why is a separate model utilised in the cross-view retriever instead of the same video encoder which puts the videos into the same space anyway?\n* In the case in which the tail classes have less than 20 examples, are all examples still chosen? Given the size of the test set and the imbalanced nature of the dataset, is this a worry when performing inference?\n* It is hard to compare the results within Table 1 as all the models use different backbones and are thus incomparable. In this way, it is impossible to know whether REAR achieves state of the art performance because of the backbone of UniFormerV2 or because of the proposed aspects of the method. Including experiments with a ViT-L backbone which gives the strongest previous performance for Ego-Only and AVION or re-training AVION/Ego-Only with a UniFormerV2 backbone would help clarify this. Unfortunately, because of this, the results are inconclusive even with the positive increase in performance of REAR from baseline to cross-view.\n* The similarity calculation which uses the average of the video-video and video-text similarity calculation is defined but not ablated, it would be good to know how important this decision choice in the model's performance\n* Missing citation for Ego-Exo4D [1] as a dataset which includes both first person and third person videos within the related work\n* It would be good to know the impact of the video gallery, there are open questions regarding:\n  * How does the size of the gallery impact the method's performance?\n  * If an egocentric gallery was used, does this have a big impact on the method's performance, does this allow for better performance as the modalities are closer? Or is this worse because there is less complementary information from the exocentric perspective.\n  * How relevant do the videos within the gallery need to be? For example, if an exocentric dataset which has less relevance was used does this harm performance by a large amount?\n\n\n[1] Grauman, Kristen, et al. \"Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "1. Have fair comparisons between models with the same underlying backbone been investigated?\n2. Has an exploration into the gallery and how this affects the model performance been completed?\n3. Why is a different model used to find the relevant models in the exocentric modality?\n4. Was the average video-video and video-text similarity calculation ablated?\n5. In the case in which the tail classes have less than 20 examples, are all examples still chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Uocvd16kjO", "forum": "bYHKUrKaEP", "replyto": "bYHKUrKaEP", "signatures": ["ICLR.cc/2026/Conference/Submission22026/Reviewer_komg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22026/Reviewer_komg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923641252, "cdate": 1761923641252, "tmdate": 1762942025443, "mdate": 1762942025443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical problem of long-tail class distributions and data scarcity in Egocentric Action Recognition (EAR). The authors propose REAR, a novel retrieval-augmented framework that enhances egocentric video representations by leveraging knowledge from abundant, external third-person (exocentric) videos. A key innovation is that this approach does not require strictly paired or synchronized ego-exo data, making it more practical and scalable. Experiments on three benchmarks demonstrate that REAR achieves state-of-the-art performance, showing significant improvements, particularly in recognizing actions and objects belonging to the long tail."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is supported by a rigorous and well-designed experimental evaluation. The authors validate their proposed REAR framework across multiple standard egocentric action recognition benchmarks. The choice of baselines is comprehensive, covering a range of existing paradigms and ensuring a fair comparison.\n\n2. The authors creatively leverage retrieval-augmented knowledge from external, data-abundant exocentric videos to bolster the performance on rare, tail classes. This presents an inspiring and effective strategy for knowledge transfer that directly targets the data scarcity issue at the heart of the long-tail problem.\n\n3. The paper includes a set of ablation studies that systematically dissect the REAR framework and validate the contribution of each proposed component, to both overall performance and challenging tail classes."}, "weaknesses": {"value": "1. No resource-usage information is mentioned in the paper, which is also an important aspect for evaluating the practical applicability of the proposed method. I would suggest the authors to include the following details:\n\n    a) How many GPUs are used for training?\n\n    b) How does the training time compare with baseline methods?\n\n    c) Computation overhead comparison: the number of parameters, FLOPs.\n\n    d) Memory usage during both training and inference.\n\n    e) How does inference latency compare with baseline methods?\n\n2. In Table 4, it lacks the loss comparison under the 'Adp + SimAtt' setting. Will the LACE loss still excel at mitigating class imbalance problem under that setting?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ErrZdXAtCs", "forum": "bYHKUrKaEP", "replyto": "bYHKUrKaEP", "signatures": ["ICLR.cc/2026/Conference/Submission22026/Reviewer_kvqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22026/Reviewer_kvqx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973893774, "cdate": 1761973893774, "tmdate": 1762942024452, "mdate": 1762942024452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}