{"id": "qTFdf6BhMs", "number": 6107, "cdate": 1757952790475, "mdate": 1759897935130, "content": {"title": "Underwater Visual Geometry Estimation with Self-supervised Prototype-Graph Modulation", "abstract": "Underwater 3D reconstruction poses significant challenges due to the scarcity of large-scale labeled datasets and the lack of foundation models specifically designed for underwater scenarios. To overcome these limitations, we introduce SeaVGGT, a self-supervised framework for underwater geometric estimation that operates without reliance on annotated data or enhancement references. SeaVGGT exploits the fundamental physical principle that underwater image degradation inherently encodes scene depth, and captures this phenomenon through a graph of learnable prototypes. These prototypes encapsulate a diverse range of attenuation characteristics and are dynamically selected as context-aware conditions to modulate visual features in a depth-sensitive manner. The framework is trained in an end-to-end fashion using a set of physics-driven self-supervision losses, which enforces cyclic consistency between the original and reconstructed images based on the underwater imaging formation model. To robustly handle the variability of water types and environmental conditions, SeaVGGT adaptively refines prototype representations conditioned on the input image, thereby enabling strong generalization across diverse underwater domains. Extensive experiments on FLSea, USOD10K, and SQUID datasets demonstrate that SeaVGGT achieves a 13.47% reduction in RMSE under unseen water conditions compared to the VGGT baseline, underscoring its efficacy and broad applicability.", "tldr": "", "keywords": ["Underwater", "Depth Estimation", "Foundation Model", "3D Reconstruction"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a1287b8f1f58754c4439b304e2477f2f7b7799a.pdf", "supplementary_material": "/attachment/a4ee8f43132c6a981d4f15ad21b15b5dc859c5b2.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a self-supervised framework for underwater geometric estimation that does not rely on annotated data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors’ motivation is well-founded and aligns with the pressing needs of underwater environments.\nSeaVGGT leverages fundamental physical principles and utilizes underwater attenuation characteristics.\nThe proposed method adaptively refines prototype representations based on the input image, thereby achieving a certain level of generalization across various underwIn terms of both evaluation metrics and visual results.\nThe proposed method demonstrates significant performance advantages.  The authors also note that the model exhibits strong generalization ability.ater domains."}, "weaknesses": {"value": "The 13.47% mentioned in the abstract is unclear—does it represent the average improvement across all scenarios? It is recommended to provide a more detailed explanation here.\nThe paper does not provide a detailed analysis of the hyperparameters, such as the values of the loss term weights λ.\nIf the relative weighting of the loss terms is sensitive, such instability could significantly affect the training outcomes. Since the paper does not include any loss-weight sensitivity experiments, it is difficult to determine whether the observed performance gains truly stem from the effectiveness of the proposed method or merely result from an accidental optimum caused by specific hyperparameter settings."}, "questions": {"value": "Including a parameter analysis experiment would make me consider raising the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nes9CZuc1O", "forum": "qTFdf6BhMs", "replyto": "qTFdf6BhMs", "signatures": ["ICLR.cc/2026/Conference/Submission6107/Reviewer_6HLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6107/Reviewer_6HLZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842901744, "cdate": 1761842901744, "tmdate": 1762918467891, "mdate": 1762918467891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for reconstructing 3D geometry from single underwater images by finetuning a standard feedforward 3D reconstruction network, VGGT, on a curated set of underwater images to simultaneously estimate underwater scattering parameters and geometry."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method produces better quantitative results than previous baselines. The idea of learning how to recover water types without explicit supervision by using depth as a signal seems interesting and novel, or at least not widespread in the literature, and is a worthwhile contribution."}, "weaknesses": {"value": "Even though the model has better depth estimation metrics, it's not clear to me whether than translates into better qualitative results. In Figure 4, I only see the ground truth in two of the scenes, and it seems to me that the major errors in depth reconstruction involve the water background. Furthermore, there appear to be issues in the presented GT for the first of the two scenes, the last image of the first row. The background is portrayed as all on one depth plane, despite there clearly being scene content and variation. Even in the appendix results it's hard to tell what or where the improvement actually is, and the authors attempt at highlighting some regions in the background of the scene where there is little interesting content also does not help. I think the authors could improve the presentation of the qualitative results by zooming into interesting parts of the scenes where there models perform better, focusing on scene objects, rather than backgrounds. \n\nAdditionally, it's not clear why for this problem a rather complex GNN architecture was chosen to estimate the water parameters. The main idea appears to be using depth as supervision to learn how to recover scattering parameters in a self-supervised way, so any kind of regressor network could be used. The authors need either to ablate the design of the GNN more thoroughly than just the number of graph nodes used or to cite previous papers that show GNNs are useful for this kind of prototype discovery task through detailed analysis, because to my knowledge they are more typically based on metric learning and the like [1].\n\n[1] https://arxiv.org/abs/1703.05175"}, "questions": {"value": "- What is the reason for choosing a GNN architecture for the scattering parameter estimation module rather than a simpler architecture? \n\n- Does the method show improved qualitative results for objects in the scene, not just the water background?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L0SQXIiw2S", "forum": "qTFdf6BhMs", "replyto": "qTFdf6BhMs", "signatures": ["ICLR.cc/2026/Conference/Submission6107/Reviewer_95qr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6107/Reviewer_95qr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918748499, "cdate": 1761918748499, "tmdate": 1762918467109, "mdate": 1762918467109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced SeaVGGT, a self-supervised framework designed to address the domain shifts when applying the pre-trained VGGT model to underwater environments. The core of SeaVGGT is a \"prototype-graph modulation\" mechanism. The framework was trained in a self-supervised fashion, and its key was a physics-driven self-supervision loss. Extensive experiments on multiple real-world underwater data sets demonstrated that SeaVGGT significantly outperformed the VGGT baseline and other advanced depth estimation models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed \"prototype-guided token modulation\" only trained a lightweight modulation module, achieving remarkable improvements with minimal additional computational cost. This work utilized a self-supervised strategy to underwater environments, ensuring the feature modulation was driven by physics-guided cross-modal consistency towards physically plausible and geometrically accurate results, without reliance on annotated data. This was significant given the scarcity of large-scale labeled data sets in the underwater domain. The method achieved SOTA performance on three real-world underwater data sets, significantly outperforming the VGGT baseline and DepthAnything V2 across all metrics."}, "weaknesses": {"value": "The simplified underwater imaging model used in the paper simplified the physical process. It assumed to be constant background light B and a learnable scalar parameter β that models the attenuation coefficient. This simplification might limit the model's performance ceiling in more complex optical conditions.\nThe background loss relied on the predicted depth map D to select the top 0.1% of pixels with the largest depth values as a proxy ground truth for B. The paper also admitted to “mitigate potential inaccuracies of D under domain shift”. This self-supervised proxy ground-truth seems unstable: a poor depth map D might lead to a poor target for B, which could, in turn, impede the training of the B estimation network.\nThe MOTIVATION section reads more like a summary of the proposed method (e.g., revisiting the formulation of VGGT 54 and introducing the lightweight token modulation mechanism) rather than posing a problem, experimenting, or theoretically explaining the mechanism of the problem."}, "questions": {"value": "1. Can the authors discuss the potential limitations of using the simplified imaging model (i.e., the scalar parameter β)? Would adopting a more complex, physically realistic model that accounts for wavelength-dependent attenuation lead to performance improvements, or would it make the self-supervised learning harder to converge?\n2. As mentioned in the weaknesses, the background loss depends on an initial depth map D that has potential inaccuracies under domain shift to generate its own supervision signal. The ablation (Table 4) shows that background loss contributes, but could this unstable proxy signal occasionally has a negative impact on training? And what is the performance gap between the final performance and one affected by this negative impact, as this affects the practical evaluation of the method.\n3. The initialization of the prototype graph seems to rely on prior samples (Figure 7a). How would the model's performance be affected if the prototype colors Ai is randomly initialized?\n4. The graph edges of the prototype graph were established based on a fixed color distance threshold τ=0.3. How sensitive is the model to this hyperparameter τ? Did the authors attempt to use a dynamic or learnable graph structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VdZgzhXMyQ", "forum": "qTFdf6BhMs", "replyto": "qTFdf6BhMs", "signatures": ["ICLR.cc/2026/Conference/Submission6107/Reviewer_Wnbj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6107/Reviewer_Wnbj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928013290, "cdate": 1761928013290, "tmdate": 1762918466023, "mdate": 1762918466023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a modification of the VGGT depth estimation model for use with underwater images. The authors articulate a model for designing tokens that contain information about likely effects of the underwater light field, especially absorption and scattering by particulate matter. These tokens are then used to adapt a pretrained VGGT model. The full model is trained with a physics-guided self-supervision frame work where and image enhancement head lives along side the depth estimation head. The image enhancement is with an underwater imaging model that encodes an estimate of the attenuation coefficent. The reported results represent somewhat of an improvement over VGGT on its own."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The authors present an interesting modification to an existing depth estimation framework, introducing some of the physics of the underwater light field and using image enhancement/denoising in an effort to improve depth recovery.  \n- Quality: The experiments are somewhat minimal, but this is likely due to the lack of publicly available data with appropriate depth information. \n- Clarity: The paper is a bit scattershot, bouncing around between methods and discussion. Table and figure captions are minimal and can be difficult to parse. The authors spend a lot of time discussing the maths of their approach at the expense of describing the experiments and the datasets. It makes interpreting the improvements recognized by their methods somewhat difficult to see. \n- Significance: The authors are treating a significant problem in underwater depth estimation. It is a notoriously difficult area to work in."}, "weaknesses": {"value": "I find the experiments and results lacking in terms of explanation and contextualization. The improvements over VGGT are not striking on their own and require more information about the datasets to understand how compelling the results are. Likewise, the authors do not compare their results against any of the methods they list in the related work section. The main point of comparison for other underwater depth estimation models is UDepth, a model mentioned only in the results section without any information as to how it compares structurally with the proposed approach. The other two models used for contextualizing their improvements are depth estimation foundation models trained mostly on terrestrial images. Those three existing models are only tested on one of the three datasets selected for the experiments."}, "questions": {"value": "- Why are there bolded paragraphs of discussion in the middle of the methods (e.g. 216)? These sorts of statements should be presented along with some evidence of the performance improvements in the \n- Line 444: This doesn't seem like an ablation study. What element of the system is being manipulated to show a change in performance?\n- What are all the scenes in Table 1 and what dataset did they come from? It seems like FLSea, but that isn't mentioned until halfway through the caption. The scences are not mentioned at all in the text nor described in the caption. Why are the results so variable across them? Is there any structure in the scenes that may contribute to changes in performance? \n- What are we supposed to see in the 'qualitative comparison' presented in figure 4? In the bottom two rows, the inputs appear to be multiple images and there is no ground truth. What are the bounding boxes that show up in some of the images but not others?\n- Figure 7: what are the numbers in the color swatches in (b)? How do those swatches compare with the inputs?\n- Can you speculate as to what it is about 24 graph nodes that seems to be the sweet spot for the performance? It is interesting how consistent that is across the three datasets. \n- How was the stereo information used from the SQUID dataset? Were the images considered independently or were the pairs used together somehow? \n- Can you apply UDepth and DAv2 to the USOD10K and SQUID datasets? \n- Are there related underwater models (besides Yang et al 2024a and Zhang et al 2024b) among those mentioned in the related work section you could use to compare with your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MT0TbBmFK6", "forum": "qTFdf6BhMs", "replyto": "qTFdf6BhMs", "signatures": ["ICLR.cc/2026/Conference/Submission6107/Reviewer_pYbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6107/Reviewer_pYbY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002837341, "cdate": 1762002837341, "tmdate": 1762918464852, "mdate": 1762918464852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}