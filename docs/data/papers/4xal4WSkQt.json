{"id": "4xal4WSkQt", "number": 12372, "cdate": 1758207348506, "mdate": 1759897514249, "content": {"title": "Lightweight and Interpretable Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "abstract": "Unlike conventional \"black-box\" transformers with classical self-attention mechanism, we build a lightweight and interpretable transformer-like neural net by unrolling a mixed-graph-based optimization algorithm to forecast traffic with spatial and temporal dimensions.\nWe construct two graphs: an undirected graph $\\mathcal{G}^u$ capturing spatial correlations across geography, and a directed graph $\\mathcal{G}^d$ capturing sequential relationships over time. \nWe predict future samples of signal $\\mathbf{x}$, assuming it is ``smooth'' with respect to both $\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and $\\ell_1$-norm variational terms to quantify and promote signal smoothness (low-frequency reconstruction) on a directed graph.\nWe design an iterative algorithm based on alternating direction method of multipliers (ADMM), and unroll it into a feed-forward network for data-driven parameter learning. \nWe insert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$ that play the role of self-attention. \nExperiments show that our unrolled networks achieve competitive traffic forecast performance as state-of-the-art prediction schemes, while reducing parameter counts drastically.", "tldr": "We unroll a mixed-graph-based optimization algorithm into a lightweight and interpretable transformer for traffic forecasting, which achieves comparable performance to SOTA schemes with drastically reduced parameters.", "keywords": ["algorithm unrolling", "graph signal processing", "mixed graph", "traffic forecasting"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c787adee22d644d49bb1981f1128e2e6f8f0d82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a lightweight transformer-like architecture for traffic forecasting by unrolling a mixed-graph-based ADMM optimization algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses the important problem of reducing transformer parameters while maintaining performance\n- Mathematical framework grounded in optimization theory provides some theoretical foundation\n- Experiments across multiple traffic datasets (METR-LA, PEMS-BAY, PEMS03/04/07/08)\n- Attempts to bridge model-based and data-driven approaches through algorithm unrolling\n- Clear motivation for using both spatial and temporal graphs"}, "weaknesses": {"value": "- The novelty claims are exaggerated. Directed graph signal processing exists extensively in the literature. The statement \"for the first time\" regarding smooth signals on directed graphs is incorrect. References to prior work on directed graph Laplacians and spectral analysis are missing.\n\n- The symmetrization in Theorem 3.1 undermines the main contribution. By converting Ld_r via (Ld_r)^T Ld_r, the method essentially reduces to undirected graph processing, contradicting claims about novel directed graph handling.\n\n- Limited experimental superiority. Table 1 shows the proposed method is often outperformed by simpler baselines like STID and SimpleTM. The \"competitive\" framing hides that the method rarely achieves best performance.\n\n- The interpretability claim is weak. Knowing layers correspond to ADMM iterations doesn't provide actionable insights. What traffic patterns do learned graphs capture? How do practitioners use this \"interpretability\"?"}, "questions": {"value": "1. Can you provide citations for prior work on directed graph Laplacians and signal processing? How does your DGLR/DGTV differ from existing directed graph smoothness definitions?\n\n2. Why does symmetrizing via (Ld_r)^T Ld_r not reduce your approach to standard undirected graph methods? What specifically remains \"directed\" after this operation?\n\n3. Table 1 shows your method is often outperformed by simpler baselines (STID, SimpleTM). Why should practitioners choose your approach over these simpler alternatives?\n\n4. Can you provide comprehensive ablations showing: (a) undirected graph only, (b) directed graph only, (c) different numbers of unrolled layers, (d) DGLR vs DGTV?\n\n5. What is the actual performance-parameter tradeoff? Can you plot accuracy vs. parameters for all methods to show where your approach sits?\n\n6. How does \"interpretability\" help practitioners? Can you show examples of what the learned graphs reveal about traffic patterns that couldn't be discovered otherwise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wsVNGaMSG6", "forum": "4xal4WSkQt", "replyto": "4xal4WSkQt", "signatures": ["ICLR.cc/2026/Conference/Submission12372/Reviewer_HhZb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12372/Reviewer_HhZb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504933631, "cdate": 1761504933631, "tmdate": 1762923278565, "mdate": 1762923278565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Technical Clarification Questions on the Proposed Method"}, "comment": {"value": "I’ve been trying to understand several parts of the paper more thoroughly, and I may be missing something. I’d appreciate clarification on a few technical points:\n\n1. Since $\\(\\mathbf{L}_r^d\\)^ \\intercal\\mathbf{L}_r^d$ is symmetric and PSD, it seems to remove the antisymmetric components of $\\mathbf{L}_r^d$. Under that interpretation, the operator looks spectrally similar to an undirected Laplacian. Is there an aspect of Eq. (4) that is meant to retain directionality beyond the $\\ell_1$ term?\n\n2. In Fig. 2(c–d), I’m trying to understand how the provided example is intended to illustrate directional behavior. Right now, it looks to me like it mainly reflects how $\\(\\mathbf{L}_r^d\\)^ \\intercal\\mathbf{L}_r^d$ changes when $\\mathbf{L}_r^d$ is modified, which follows directly from the matrix product. Is there a particular directional effect the figure is meant to highlight?\n\n3. For Eq. (21), I’m curious about the motivation for using a Mahalanobis distance. Since $\\mathbf{M}$ is assumed symmetric PSD, it can be converted into a standard Euclidean distance via a linear transformation of the features. Given this equivalence, is there a specific reason why the Mahalanobis form is preferable in this setting?\n\n4. I’m still trying to pin down what the main conceptual novelty is. Recent work (e.g., Joshi, 2025) already frames transformers as GNNs on fully connected token graphs. Here, the experiments in Section 5.1 use a sparse neighborhood graph, and the directionality seems to be relaxed after the $\\(\\mathbf{L}_r^d\\)^ \\intercal\\mathbf{L}_r^d$ step. How should I think about the core contribution relative to existing transformer-as-GNN perspectives?\n\n5. The description of the model as “lightweight” also raises a question for me. The parameter count is low, but sparse message passing can sometimes introduce hardware inefficiencies compared to dense transformer operations. You mention low inference time; is there additional information about memory usage or overall resource footprint that would help contextualize the practical efficiency?\n\nreferences:\nJoshi, C. K. (2025). Transformers are graph neural networks. arXiv preprint arXiv:2506.22084."}}, "id": "45s5U7IY1f", "forum": "4xal4WSkQt", "replyto": "4xal4WSkQt", "signatures": ["~Seyed_Alireza_Hosseini1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Seyed_Alireza_Hosseini1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12372/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763670543657, "cdate": 1763670543657, "tmdate": 1763670543657, "mdate": 1763670543657, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a lightweight and interpretable transformer-like architecture for spatio-temporal traffic forecasting by unrolling a mixed-graph optimization algorithm. The model represents data using an undirected graph for spatial correlations and a directed graph for temporal relationships. A key contribution is the design of novel L2-norm (DGLR) and L1-norm (DGTV) regularizers to quantify and promote signal smoothness on directed graphs. By unrolling an Alternating Direction Method of Multipliers (ADMM) solver for this optimization problem, the authors create an interpretable neural network where each layer corresponds to an optimization step and the graph learning modules function as a self-attention mechanism. Experiments show the model achieves competitive performance against state-of-the-art methods while using drastically fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is built on a robust theoretical foundation, leveraging well-established concepts from graph signal processing (GSP) and optimization, particularly the unrolling of mixed-graph-based algorithms. This theoretical rigor provides a strong basis for the proposed model's design and effectiveness.\n\n2. A major strength is the model's interpretability. Unlike \"black-box\" transformers, this architecture is a \"white-box\" by construction. Each layer of the neural network directly corresponds to an iteration of the ADMM optimization algorithm, making the model's internal operations mathematically transparent.\n\n3. The approach demonstrates a significant reduction in model parameters, achieving performance comparable to state-of-the-art methods while using only a fraction of the parameters (6.4% of transformer-based PDFormer). This makes the model highly efficient, especially in memory-constrained environments."}, "weaknesses": {"value": "1. While the proposed method demonstrates significant parameter reduction, the paper does not provide an in-depth analysis or comparison of the runtime efficiency. Given the lightweight nature of the model, including performance benchmarks related to computation time and memory usage would further highlight its advantages.\n\n2. Although the model is interpretable and achieves competitive results, it slightly underperforms compared to some existing baseline methods.\n\n3. The paper includes a substantial amount of mathematical derivation, which, while thorough, may make the content dense and difficult to follow for readers not well-versed in the specific theoretical background. A clearer, more concise explanation of the key steps and concepts would improve accessibility.\n\n4. The scalability of the method to larger networks is uncertain, as it has only been tested on graphs of hundreds of nodes; specifically, the cost of conjugate gradient iterations and graph learning modules could become prohibitive."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xfMPbpgA6S", "forum": "4xal4WSkQt", "replyto": "4xal4WSkQt", "signatures": ["ICLR.cc/2026/Conference/Submission12372/Reviewer_1UUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12372/Reviewer_1UUP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985962281, "cdate": 1761985962281, "tmdate": 1762923278345, "mdate": 1762923278345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel lightweight and interpretable Transformer-like architecture for traffic forecasting by unrolling a mixed-graph-based optimization algorithm. Specifically, it first learns an undirected graph $G_u$ and a directed graph $G_d$ to capture spatial and sequential information respectively, then designs three smoothness prior GLR for $G_u$, and DGLR, DGTV for $G_d$ to predict the future samples. An ADMM-based optimization algorithm is then unrolled into neural layers for end-to-end parameter and graph learning, where the graph-learning modules play a self-attention-like role with far fewer parameters Extensive experiments demonstrate its effectiveness in traffic forecast performance with fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper creatively constructs a mixed graph to model spatial-temporal data. Meanwhile, it proposes a novel regularization terms directed graph Laplacian regularizer (DGLR) and directed graph total variation (DGTV) for directed graphs, which resolves the issue that the asymmetric Laplacian of a directed graph is difficult to analyze spectrally.\n\nS2. This paper has a real small parameter count compared to state-of-the-art models, as shown in Table 1. Meanwhile, its prediction performance remains highly competitive."}, "weaknesses": {"value": "W1. This paper needs further polishing its statement. For instance, in the introduction, the jump from discussing model-based methods directly to DL-based methods is abrupt.\n\nW2. The non-negative weights limit the model's expressive power, as it cannot directly model inhibitory relationships between nodes, unable to match the performance of the real Transformer.\n\nW3. Although this paper provides the model parameters, it does not specifically analyze the computational complexity of the training or inference stages"}, "questions": {"value": "Q1. This paper separates spatial relationships and temporal relationships entirely in traffic forecast performance. But will it lose complex and coupled spatial-temporal information in the real world?\n\nQ2. Why was Mahalanobis distance chosen to measure the distance between node features? What is the advantage of it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n52p9n1OCa", "forum": "4xal4WSkQt", "replyto": "4xal4WSkQt", "signatures": ["ICLR.cc/2026/Conference/Submission12372/Reviewer_XHXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12372/Reviewer_XHXD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997169120, "cdate": 1761997169120, "tmdate": 1762923278103, "mdate": 1762923278103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight and interpretable transformer-like neural network by unrolling a mixed-graphs optimization algorithm for spatio-temporal traffic forecasting. The key idea is to unroll an ADMM solver for the mixed-graphs optimization problem: an undirected graph $G^{u}$ encodes spatial correlations and a directed graph $G^{d}$ encodes temporal causality. The authors introduce two priors DGLR ($\\ell_{2}$) and DGTV ($\\ell_{1}$) to promote low-frequency (smooth) reconstruction on directed graphs, while graph learning modules act as a parameter-efficient self-attention surrogate. On META-LA and PEMS03, the proposed model are compatible with baseline algorithms in accuracy with much fewer parameters ($\\approx$ 34K, 6.4% of PDFormer). For the priors, the ablation study shows their importance. Overall, this paper addresses an important issue in spatio-temporal traffic forecasting and the proposed algorithm is promising. However, this paper could be improved with discussion and verification of influence of low-pass on non-smooth or long-range effects and more extensive experimental evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed algorithm has a theoretical background with unrolling. Eacy layer corresponds to an iteration that minimizes an explicit mixed-graph objective, which increases interpretability. \n- The authors provide a perspective on the low-pass on directed graphs by modeling temporal causality with directed Laplacian/TV regularizers. It is beyond standard undirected graph signal processing and not limited to heuristic smoothing. \n- With the directed graph learning, the learned graph weights act like attention scores and admit a Mahalanobis metric-learning interpretation. \n- In experiments, the proposed algorithm is compatible with baseline algorithms in accuracy with much fewer parameters."}, "weaknesses": {"value": "- Low-pass on directed graphs may decrease non-smooth or long-range effects. In both theory and experiments, this points should be further explored. \n- Convergence guarantee with ADMM may be questionable because the optimization problem in Eq.(6) is not convex. It should be further discussed for nonconve ADMM such as choice for $\\rho$ and step sizes (c.f., Hong et al. 2016). \n- Experimental evaluation is limited because (i) there are only two datasets; (ii) the datasets are sub-sampled to 1/3, and (iii) trained for only 70 epochs. Verification on the full-scale datasets and more dataset would strengthen the proposed method. \n- The readability could be improved. For example, the overall algorithm should be shown in the main text or appendix. \n\n[Hong et al. 2016] Mingyi Hong, Zhi-Quan Luo, and Meisam Razaviyayn. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM Journal on Optimization, 26(1), 2016."}, "questions": {"value": "Please answer the points listed in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q9BwyXaTaF", "forum": "4xal4WSkQt", "replyto": "4xal4WSkQt", "signatures": ["ICLR.cc/2026/Conference/Submission12372/Reviewer_bdcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12372/Reviewer_bdcs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762701292136, "cdate": 1762701292136, "tmdate": 1762923277857, "mdate": 1762923277857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}