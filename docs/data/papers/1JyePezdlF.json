{"id": "1JyePezdlF", "number": 23428, "cdate": 1758343656247, "mdate": 1762955357615, "content": {"title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing", "abstract": "Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45% and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments.", "tldr": "We propose MorphServe, a dynamic and workload-aware LLM serving framework that combines runtime layer swapping and elastic KV cache resizing to adaptively balance latency, accuracy, and memory efficiency under bursty traffic.", "keywords": ["Large Language Models", "LLM Serving Systems", "LLM Serving Optimization", "Mixed-Precision Serving", "Workload-Aware Systems"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b35350ee195a38e221a40d7b76b3b91a4239d3c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MorphServe, a dynamic LLM serving framework that adapts to fluctuating workloads via runtime layer swapping and elastic KV cache resizing. It employs morphological adaptation to selectively replace less critical model layers with quantized versions and adjust KV cache capacity based on real-time memory pressure. This approach preserves high accuracy during low load and only introduces minimal, targeted precision loss to maintain performance during high load, without interrupting ongoing requests. Extensive experiments show MorphServe significantly reduces latency and SLO violations while maintaining model quality, outperforming both full-precision and static quantization baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* S1) The proposed method enable dynamic trade-off between model quality and serving efficiency.\n* S2) The proposed method achieves significant latency reduction and SLO improvement with negligible accuracy loss"}, "weaknesses": {"value": "* W1) The authors incorrectly claim in the introduction that key-value cache compression and eviction methods are incompatible with Grouped Query Attention. To my knowledge, nearly all recent KV cache eviction techniques have been successfully applied to GQA-based models, including Llama 3 and Qwen 3. \n\n* W2) The models used in the experiments are somewhat outdated. \n\n* W3) MorphServe heavily relies on offline profiling to determine layer sensitivity and swapping sequences. This process requires preprocessing for each specific model on particular datasets to compute Layer Importance Scores (LIS). \n\n* W4) The authors used the NeurIPS 2025 template instead of the ICLR template."}, "questions": {"value": "please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8p61IQ8Ug", "forum": "1JyePezdlF", "replyto": "1JyePezdlF", "signatures": ["ICLR.cc/2026/Conference/Submission23428/Reviewer_qMi5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23428/Reviewer_qMi5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568636148, "cdate": 1761568636148, "tmdate": 1762942656646, "mdate": 1762942656646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "MmcmcPjUQX", "forum": "1JyePezdlF", "replyto": "1JyePezdlF", "signatures": ["ICLR.cc/2026/Conference/Submission23428/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23428/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762955356724, "cdate": 1762955356724, "tmdate": 1762955356724, "mdate": 1762955356724, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Desk reject, the author uses the wrong template"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Desk reject, the author uses the wrong template"}, "weaknesses": {"value": "Desk reject, the author uses the wrong template"}, "questions": {"value": "Desk reject, the author uses the wrong template"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8w7fDfCpx", "forum": "1JyePezdlF", "replyto": "1JyePezdlF", "signatures": ["ICLR.cc/2026/Conference/Submission23428/Reviewer_CENS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23428/Reviewer_CENS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837844931, "cdate": 1761837844931, "tmdate": 1762942656032, "mdate": 1762942656032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adaptively change the model weight precision during inference time, which allow more requests to start generation during workload peak and thus significantly reducing tail TTFT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Solid implementation with sufficient low-level details\n* Interesting idea of changing the precision of model weights to fit in more requests"}, "weaknesses": {"value": "* Does not work with already-quantized LLMs.\n* Evaluation coverage can be wider\n* CPU memory overhead\n* Slightly hurts TPOT"}, "questions": {"value": "I appreciate the core ideas, but mainly concerned about the compatibility of this work with existing LLM serving optimization techniques. For example: \n\n* Does your approach support CUDA graph?\n* How does your approach support different parallelism? Like tensor parallel, pipeline parallel, pd disaggregation and wide ep?\n* The evaluation is on old LLMs --- what about latest LLMs, like MoE models? Will the improvement be larger or smaller?\n* Your approach will likely slow down CPU KV cache offloading. If that is the case, a small experiment on how much the slowdown would be will be helpful.\n* How does your approach compared to other layer-wise kv cache management approaches like this paper https://arxiv.org/html/2410.00428v1 ? I see that you are citing this work, but the rationale of why your work is better is missing. I understand that your approach is working on an orthogonal dimension, but your paper and this paper is targeting the same problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2OkY1GZx27", "forum": "1JyePezdlF", "replyto": "1JyePezdlF", "signatures": ["ICLR.cc/2026/Conference/Submission23428/Reviewer_C6rS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23428/Reviewer_C6rS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863257459, "cdate": 1761863257459, "tmdate": 1762942655565, "mdate": 1762942655565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a workload-aware LLM serving framework that dynamically switches between full-precision (FP) and quantized model layers to manage memory pressure. The core idea is that when a server is overloaded, the system swaps a subset of FP layers with their quantized counterparts. This action has two benefits: 1) newly generated KV-cache entries are smaller, and 2) the system's \"KVResizer\" component can allegedly resize the _existing_ KV-cache blocks corresponding to the swapped layers. This dynamic memory saving is intended to increase request throughput and reduce time-to-first-token (TTFT) by accommodating more concurrent requests, though with a marginal, controlled degradation in model accuracy. The evaluation shows improved TTFT and F1 scores against selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The central concept of using dynamic, partial quantization to actively manage the KV-cache memory bottleneck is novel. If the proposed resizing of the existing KV cache can be achieved with minimal overhead, it represents a promising direction for handling memory-intensive serving workloads.\n- The use of cosine similarity to define Layer Transformation Sensitivity (LTS) and Layer Replacement Sensitivity (LRS) is intuitive. It provides a clear, model-aware heuristic for identifying which layers are most (and least) sensitive to quantization, guiding the layer-swapping policy."}, "weaknesses": {"value": "- Overly naive and brute force solution\n- Missing details\n- Missing Appendix and formatting issues\n\nDetails:\n- The method for determining the optimal set of layers to quantize (MDS score) appears to be a brute-force search. The paper defines the score for a layer $p$ given a set of already quantized layers $Q$, but it never explains how this set $Q$ is chosen. Without a clear heuristic or optimization, the algorithm would need to explore $O(2^n)$ combinations of layers to find the optimal subset, which is computationally infeasible for modern LLMs.\n- The \"KVResizer\" is a central component of the proposed system, yet its implementation is treated as a black box. A key claim is that it can \"shrink\" the KV-cache blocks of _existing_, ongoing requests. How is this resizing performed on an active cache block while a decoding process is concurrently accessing it? This operation seems non-trivial, likely requiring significant, complex CUDA kernel development to manage memory and synchronization without stalling the GPU. The paper provides no details, overhead analysis, or evidence to support the feasibility of this core mechanism.\n- The paper is not in the ICLR format. I reviewed the paper regardless and my review does not depend on this. However, I want to point it out so AC or SAC can make an informed decision. More critically, it repeatedly refers to an appendix for “key details” (including, presumably, information on the layer swapping and resizing mechanisms), but no appendix or supplementary material is provided. This omission makes it impossible to fully evaluate the paper's technical contribution. An LLM usage statement is also missing.\n- The mathematical notation is confusing. For example, $Q$ is used in Equation 3 to represent a quantized layer ($h_p^Q$​) but is then redefined in Equation 4 as a subset of layers being swapped."}, "questions": {"value": "1. The paper claims (Line 208) that layer loading is overlapped with decoding. How is this possible? Doesn't the decoding process require access to the very layer weights that are being swapped in or out?\n2. The system loads all layer variants into CPU memory before serving. What is the total CPU memory footprint required for a typical model, and is this a practical assumption for a production serving node?\n3. The paper states that experiments are run on a \"72-second trace snippet\". This duration seems exceptionally short for evaluating a dynamic serving system. Were all experiments limited to this 72-second window? This is insufficient to evaluate system stability, long-term behavior (e.g., memory fragmentation), or the overhead of repeated swapping."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper is not in the ICLR format and is missing appendix and LLM usage statement."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LRELevrmkc", "forum": "1JyePezdlF", "replyto": "1JyePezdlF", "signatures": ["ICLR.cc/2026/Conference/Submission23428/Reviewer_Wj4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23428/Reviewer_Wj4H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927674414, "cdate": 1761927674414, "tmdate": 1762942655081, "mdate": 1762942655081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}