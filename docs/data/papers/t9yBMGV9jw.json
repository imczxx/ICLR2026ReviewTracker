{"id": "t9yBMGV9jw", "number": 21744, "cdate": 1758321187199, "mdate": 1759896905573, "content": {"title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models", "abstract": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. The task consists of following a series of short instructions (3-5 steps), performing basic transformations on imagined letters and simple shapes to create a mental image of an object, and finally recognizing and labeling the object. Traditionally, cognitive psychologists have argued that this task is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance (9.4\\%-12.2\\% increase over the human average of 54.7\\%, $p<.00001$). Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.", "tldr": "We gave frontier LLMs novel items in a classic mental imagery task thought to be unsolvable relying solely on language, and yet, the best models outperformed humans providing a new way to evaluate complex emerging capacities.", "keywords": ["large language models", "evaluation", "cognitive science", "mental imagery", "reasoning", "representational formats", "iconic representations", "aphantasia"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe9a032810927007472e3913749c042ea4cc943f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel benchmark for assessing complex cognitive behavior in Large Language Models through a classic mental imagery tasks. By comparing several state-of-the-art LLMs with 100 human participants, the authors find that the best models surpass average human performance despite lacking any visual architecture. The results suggest that LLMs may possess emergent reasoning abilities sufficient to solve imagery-dependent tasks using purely linguistic representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Adapting a classical mental imagery paradigm to test LLMs represents a  step toward evaluating higher-order cognition in LLMs.\n- The finding that leading LLMs outperform the human baseline on the mental rotation task previously thought to require visual imagery is interesting."}, "weaknesses": {"value": "The paper’s central assumption—that the inability to generate images directly implies a lack of image-rendering capability—appears conceptually overstated. Prior work [1] has shown that large language models can acquire substantial visual priors from purely linguistic training, sometimes even displaying visual reasoning or “hacked” image understanding. The fact that a model cannot explicitly output an image does not necessarily mean it fails to internally “render” or simulate a scene during reasoning [2]. Humans, for instance, often visualize mental imagery without ever producing drawings. \n\nThe claim would be more convincing if supported by evidence from representation-level analysis rather than behavioral absence. Specifically, extracting hidden states and attempting to reconstruct images could provide direct support for the hypothesis.\n\nFurthermore, all experiments are performed on closed-source models, which limits reproducibility and interpretability. The appendix briefly mentions open-source models, but no concrete results or analyses are provided.\n\n[1] Han, J., Tong, S., Fan, D., Ren, Y., Sinha, K., Torr, P. and Kokkinos, F., 2025. Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training. arXiv preprint arXiv:2509.26625.\n\n[2] Luo, Dezhi, Qingying Gao, and Hokin Deng. \"Rethinking the Simulation vs. Rendering Dichotomy: No Free Lunch in Spatial World Modelling.\" arXiv preprint arXiv:2510.20835 (2025)."}, "questions": {"value": "- Consider conducting representation-probing experiments—for instance, extracting hidden states and testing whether they contain decodable visual information or spatial priors, whether they can lead to interpretable and reasonable image.\n- Include open-source baselines to improve reproducibility and interpretability, ideally with side-by-side analyses.\n- Clarify the conceptual distinction between image generation and image rendering, and explicitly discuss whether implicit rendering might occur during multimodal reasoning even in text-only models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Cteh3WgSFB", "forum": "t9yBMGV9jw", "replyto": "t9yBMGV9jw", "signatures": ["ICLR.cc/2026/Conference/Submission21744/Reviewer_UkQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21744/Reviewer_UkQT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800713067, "cdate": 1761800713067, "tmdate": 1762941914068, "mdate": 1762941914068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether LLms can solve tasks traditionally requiring visual imagination. Using newly designed “mental imagery” problems and human benchmarks, the authors show that LLMs (e.g., GPT-5, o3-Pro) outperform humans even without visual inputs, suggesting that abstract propositional reasoning can account for their success."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- interesting study of pictorial vs propositional representation in LLms. \n\n- Good dataset with clear methodology.\n\n- The experiments show modern reasoning-tuned LLMs exceed in reasoning."}, "weaknesses": {"value": "- the scoring validity is based on ordinal rate (1-5) may include a noise, how to treat that? \n\n- the use of chi-square tests on “proportion-of-maximum scores” may violate the  independence and equal-variance assumptions\n\n- the humain/LLM  protocol may produced biases results. Humans receive audio-only instructions while LLMs read text and can then rely on memory. Could you clarify this point?\n\n- the chose of evaluation design : single-run; temperature = 0.1 and somtime = 1, etc are used without justification. Please clarify."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AYye1pNIi5", "forum": "t9yBMGV9jw", "replyto": "t9yBMGV9jw", "signatures": ["ICLR.cc/2026/Conference/Submission21744/Reviewer_5Vv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21744/Reviewer_5Vv1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837170694, "cdate": 1761837170694, "tmdate": 1762941913759, "mdate": 1762941913759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a classic cognitive task to benchmark complex mental imagery in large language models (LLMs). The authors created 48 test points (with 12 existing test points) based on a classic object reconstruction paradigm from cognitive psychology, where participants/models follow multi-step verbal/textual instructions to transform letters and shapes and then identify the resulting object. The results find that the best LLMs significantly outperformed the human baseline, with performance improving as more reasoning tokens were allocated. This suggests that LLMs can solve tasks traditionally thought to require pictorial mental imagery through purely propositional, language-based reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper investigates an interesting research question: can LLMs/LRMs solve propositional reasoning-based mental imagery problems. It provides empirical results on investigating how LLMs perceive and manipulate visual-related operations.\n2. This paper offers detailed background, analysis and discussion from the cognitive science and mental imagery perspective, which makes this paper more complete."}, "weaknesses": {"value": "1. The claim and conclusion are not well supported. Since this problem type is not new, it cannot be concluded that the evaluated tasks are 'ex novo'. SOTA LLMs may be trained on problems within the same distribution so that they achieve a superior performance. Additionally, the evaluation is only based on this problem type, instead of a series of tasks involving propositional reasoning and mental imagery (such as Connect the dots), which weakens the soundness of this paper.\n2. The conclusion from the discussion is not supported by empirical evidence. There is no analysis of model internals (e.g., attention patterns, activation vectors, or reasoning traces) that could substantiate how the models arrive at correct answers."}, "questions": {"value": "1. Could the authors explain more about why humans are provided with the audio instead of the text input (which maintains the same setting as the models)?\n2. See weakness 1 above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vBYqB8peqm", "forum": "t9yBMGV9jw", "replyto": "t9yBMGV9jw", "signatures": ["ICLR.cc/2026/Conference/Submission21744/Reviewer_1U8T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21744/Reviewer_1U8T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988832022, "cdate": 1761988832022, "tmdate": 1762941913501, "mdate": 1762941913501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the emergent ability of LLMs to perform tasks that are thought to depend on mental imagery. The results show surprisingly strong performance, for some models even surpassing a human baseline, despite the fact that language models are only trained on text and therefore putatively unable to engage in imagistic reasoning. These results make an interesting contribution to classic debates in cognitive science, and also contribute a novel benchmark on an as yet unexplored task domain for LLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The results provide a clear example of how studying LLMs can make an interesting and surprising contribution to debates in cognitive science.\n- The proposed benchmark tests LLM capabilities in a new domain, and there is still room for improvement so the benchmark will likely be useful for future studies as well.\n- A range of different models are tested, including both base models and reasoning-augmented models.\n- The writing is clear and there is a nice review of the relevant literature in cognitive science.\n- The dataset involves newly constructed problems to avoid concerns about training data contamination.\n- A publicly available code repo is included.\n- All of the results are accompanied by appropriate error bars and statistical tests.\n- Results are evaluated by human judges, allowing for assessment of open-ended responses."}, "weaknesses": {"value": "- Can the distinction between imagistic and propositional solutions be formalized? And how do we know that the LLMs have not induced something like an imagistic internal model? Formalizing what it means for an internal representation to be imagistic rather than propositional would help to answer this question. Although text-only LLMs are not trained on images, in principle this doesn't preclude them from inducing an internal model that has imagistic properties, as this may be a useful latent variable for predicting text-based descriptions of visual properties. One way to formalize this might be in terms of representational similarity analysis, i.e. an internal representation might be considered imagistic if it has the same similarity structure as actual images. In the discussion, the authors described a potential pseudo-imagistic solution involving spatial relations, but I still found myself wondering what makes a representation / solution imagistic per se. Some additional discussion of this issue might be helpful for guiding future research that could more directly investigate the internal solutions that these models use.\n- In some parts of the paper it's claimed that the proposed dataset is uniquely immune to concerns about data contamination because it tests a putatively non-verbal capacity. I found this unconvincing for two reasons. First, there are many studies that investigate emergent abilities in LLMs by proposing completely new datasets and tasks. In these cases, it is not plausible that the same problems simply happen to have already been present on the internet. Second, although the task studied in this work has been previously proposed to rely on images, the task is still defined in terms of text-based inputs and outputs. Therefore, there is no reason in principle that it can't have been present in the training data even if the training data is purely text-based (I do not think this is a concern because of the first reason, but I think there are plenty of other studies that are equally immune to this concern because of the use of novel tasks).\n- The dataset should be useful for future studies, but given that the dataset is now publicly available this raises concerns about training data contamination for future models. Is it possible to automate the generation of new problems so that future models can easily be tested on problems that cannot have been present in the training data?\n- Only proprietary models are studied. It would be informative to include open-source models, particularly open-source reasoning models such as DeepSeek r1 that would allow direct inspection of the reasoning traces to better understand the model's solutions.\n- How do the length of the reasoning traces relate to model accuracy, and to human performance? The effect of reasoning effort seems to suggest that longer reasoning traces help the models to perform better. It would be interesting to relate this to human performance, e.g. to determine whether the length of the reasoning traces is meaningfully correlated with human accuracy or human response times.\n- Are any of the models trained on images, and is it possible to determine whether this impacts performance? One way to determine this might be to compare performance for an open-source VLM (e.g., Qwen) with the performance of the LLM base model prior to training on images."}, "questions": {"value": "- Can the distinction between imagistic and propositional solutions be formalized, potentially allowing for future work that investigates the internal representations used to solve this task?\n- Does the content of the reasoning traces reveal anything interesting about the solution used to solve the task?\n- Does the length of the reasoning traces correlate with human accuracy or response times?\n- How does training on images impact performance on the task?\n- Is there any research that tests congenitally blind individuals on this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2yqP8DUrP2", "forum": "t9yBMGV9jw", "replyto": "t9yBMGV9jw", "signatures": ["ICLR.cc/2026/Conference/Submission21744/Reviewer_Gt5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21744/Reviewer_Gt5a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026004872, "cdate": 1762026004872, "tmdate": 1762941913185, "mdate": 1762941913185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}