{"id": "M18s0nmmWP", "number": 20412, "cdate": 1758305779548, "mdate": 1759896978952, "content": {"title": "HAD: Hybrid Architecture Distillation for Bridging Large-Transformer Knowledge into Compact Genomic Models", "abstract": "Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and downstream fine-tuning has also achieved remarkable progress in the field of genomic sequence modeling.\nHowever, existing research often either relies on scaling up pre-training data and parameters, which brings a heavy computational burden, or lacks a systematic method to avoid the loss of prior information with compact architectures. \nIn this work, we propose a **H**ybrid **A**rchitecture **D**istillation (**HAD**) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training.\nSpecifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training.\nTo validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. **More surprisingly**, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than **500×** larger. \nLastly, we conducted a comprehensive analysis of the HAD architecture, including linear probing representation evaluation, which demonstrates both the strong representation capacity of HAD and the validity of our teacher model selection for distillation. t-SNE visualization further supports these findings, providing an intuitive view of the model's representation ability.", "tldr": "", "keywords": ["Genomic Language Model", "Knowledge Distillation", "Foundation Model"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97148e2b8852a0bfc12d0240421024287358de59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes HAD (Hybrid Architecture Distillation), a compact genomic sequence model that learns from a large teacher via a dual‑branch pretraining objective: \n\n(i) visible‑token feature alignment to the teacher with an MSE loss;\n\nand (ii) masked‑token reconstruction using a decoder that performs cross‑attention from masked queries to visible keys/values. \n\nA two‑stage masking scheme addresses the tokenizer mismatch between a 6‑mer teacher and a char‑level student by masking at the teacher’s k‑mer level, then mapping those masks to characters. The student backbone is a hybrid Bi‑Gated Delta Net (GDN) plus a single FlashAttention layer, with a chunkwise parallelization of the recurrence.\n\nHAD is pretrained for 10k steps on GRCh38 with RC augmentation and then fine‑tuned on the Nucleotide Transformer Benchmark and Genomic Benchmarks. Ablations indicate the benefit of both the visible‑token distillation branch and the single attention layer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The teacher‑group masking + student mapping prevents easy leakage and aligns the visible‑token supervision with the student’s tokenizer is a novel architecture innovation.\n\n(2) The model achieves good performance (even outperform the teacher model) using only 1/500 of the size by distillation on Genomics Benchmark and NT.\n\n(3) The clear ablation study shows that removing visible‑distillation or attention hurts performance. Teacher size matters (50M/100M < 500M). t‑SNE and linear probes suggest cleaner structure and better few‑shot generalization."}, "weaknesses": {"value": "(1) Several typos exist: at line 254, \"Equation equation 1\", at line 858, \"Pra-training\".\n\n(2) The NT and genomics benchmark are known to be fluctuant and many tasks' results largely rely on the finetuning recipes. The teacher and student models could have different fine-tuning recipes for the optimal behavior. Consider using a more convincing benchmark like BEND."}, "questions": {"value": "(1) Can you further demonstrate the performance gain on some other well known benchmark like BEND?\n\n(2) You align the final‑layer teacher features. Did you try multi‑layer or early‑layer distillation (or relational/contrastive feature matching)? Any gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxBVKWZUjV", "forum": "M18s0nmmWP", "replyto": "M18s0nmmWP", "signatures": ["ICLR.cc/2026/Conference/Submission20412/Reviewer_nHD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20412/Reviewer_nHD1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760645882038, "cdate": 1760645882038, "tmdate": 1762933853851, "mdate": 1762933853851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new architecture and distillation strategy for genomic language models. The architecture is a variant of Gated Deltanet with added softmax attention. The authors pretrain on the Human reference genome and then compare finetuning performance on the genomic benchmarks and the NT dataset."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "0) The authors report the performance of many models alongside theirs, and use 2 datasets for downstream performance eval.\n\n1) I find it interesting to use new architectures such as GDN for genomic foundation models. The authors propose a model which is actually new in this space.\n\n2) The distillation strategy is smart and well documented -- can serve as a good guideline for future research!\n\n3) Some ablations are presented\n\n4) All results have error bars"}, "weaknesses": {"value": "I think the paper is a bit underdeveloped at this stage. Most importantly, there are some methodological flaws and overclaims.\n\n0) The architecture design is a bit arbitrary -- looks like pure engineering. The motivation for architecture choice is vague, generic, and superficial, e.g., \"This hybrid approach harnesses combined strengths, effectively integrating GDN’s proficiency in capturing local and long-range sequential patterns with attention’s capacity for unifying global context\". \n\n1) I appreciate the author's engineering skills and their drive towards improving benchmark results, yet there is a huge overclaiming of the results. The $\\Delta$ proposed by the authors to motivate their claim \"it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 × larger\", is misleading: NT is a model with a different architecture, trained on a different reconstruction loss compared to what you have. Downstream performances are not comparable. There is no reason to say that you get better with 500 times fewer parameters -- you changed the architecture completely!! It is known that you could use many fewer parameters (HyenaDNA and Caduceus papers). Your hybrid model and distillation strategy do not change the fact that your model has an architecture much similar to HyenaDNA and Caduceus compared to NT.\n\n2) To compare your approach safely, I would train on exactly the same pipeline (e.g., same sequence length and same distillation loss) a Caduceus model of the same size.  Caduceus results (the ones reported) are also not comparable, indeed. If the pipeline changes, I expect that anything could happen (e.g., was Caduceus trained on the same exact number of tokens?)\n\n3) In some way, the authors propose a complete package: have a teacher model and use GDN + attention. Results are good, but I am (a) not surprised that GDN works better than Mamba 1 (i.e. in caduceus) and (b) not surprised that distillation helps. It is indeed known that distillation losses can accelerate learning, even when using a weak model as a teacher, such as random teachers. (ref: https://arxiv.org/abs/2302.12091). A standard reference is also the classic DeiT paper (https://arxiv.org/abs/2012.12877).\n\n4) The ablation point to the fact that distilling large NT variants is better. However, the gap is relatively small: 50M also works well. What if you dont distill at all? I think this ablation should be present (sorry if I missed it)."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qh0Dr1BhLl", "forum": "M18s0nmmWP", "replyto": "M18s0nmmWP", "signatures": ["ICLR.cc/2026/Conference/Submission20412/Reviewer_Pa9k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20412/Reviewer_Pa9k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871763269, "cdate": 1761871763269, "tmdate": 1762933853400, "mdate": 1762933853400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HAD (Hybrid Architecture Distillation): a dual-branch pretraining framework that distills high-level knowledge from NTv2 into a compact student based on a bidirectional GDN backbone and a single self-attention layer. On the Nucleotide Transformer and Genomic Benchmarks, the student outperforms similarly sized baselines and exceeds the teacher on several NT subtasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured KD+MLM scheme: align only on visible tokens and reconstruct masked tokens conditioned on visible context; clean separation of objectives.\n- Strong results for tiny models, with multiple wins over compact baselines and selected wins over the large teacher."}, "weaknesses": {"value": "Major\n- The conceptual novelty is limited, as the method combines several known components (feature-based KD, cross-attention MLM, and a hybrid GDN-Attention architecture). The paper demonstrates that this combination works, but lacks deeper insight into why. For instance, the reason visible-only alignment consistently surpasses masked-only alignment is not adequately explained.\n\nMinor\n- Line 88: aliment -> alignment."}, "questions": {"value": "- The paper states the surprising \"student surpassing teacher\" result, but does not deeply analyze why it occurs. This is the most interesting finding of the work. Is the Hybrid-GDN architecture simply a better inductive bias for these genomic tasks than the Transformer? Or does the hybrid distillation+MLM task act as a powerful regularizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UdEdltsbED", "forum": "M18s0nmmWP", "replyto": "M18s0nmmWP", "signatures": ["ICLR.cc/2026/Conference/Submission20412/Reviewer_npfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20412/Reviewer_npfN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884764821, "cdate": 1761884764821, "tmdate": 1762933852425, "mdate": 1762933852425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for creating compact genomic sequence models through knowledge distillation. The approach combines a 1.1M parameter student model (bidirectional Gated Delta Net + self-attention) with a dual-branch pretraining framework: (1) feature alignment between visible nucleotides and a 500M parameter NTv2 teacher model, and (2) masked nucleotide reconstruction via cross-attention. One key technical component is a two-stage masking strategy designed to handle tokenizer mismatches between the “k-mer teacher” and “character-level” student. The authors evaluate on Nucleotide Transformer and Genomic Benchmarks, claiming their compact model matches or exceeds much larger models, including the teacher, on several tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses practical need: Targets computational efficiency in genomic applications where resources are constrained\n- Rigorous evaluation: Multiple benchmarks with proper statistical analysis, cross-validation, and representation quality assessment beyond task-specific metrics\n- Strong empirical results: Achieves competitive or superior performance to NTv2, even with 500× parameter reduction\n- Comprehensive validation: Includes linear probing and t-SNE visualization to validate learned representations"}, "weaknesses": {"value": "**Novelty**\n- The background and references section omits relevant masked language modelling work such as MAE-LM (Meng et al, 2024), and a previous application of an MAE decoder to DNA models by BarcodeMAE (Safari et al, 2025). The cross-attention based decoder is similar to that used in CrossMAE. Together, these omitted citations lead to an overstatement in the architectural novelty of the proposed method.\n\n**Soundness**\n- Critical ablations are missing: the paper lacks distillation-only and reconstruction-only comparisons, preventing assessment of individual component contributions.\n- It is not spelled out in the paper that pretraining data used for the distillation is **not** the same as the data used to train the teacher model, NTv2. L319 says the data is the same as used in two citations, but it is to transparent that this is the papers for training the HyenaDNA and Caduceus baselines, not the NTv2 baseline. The authors indicate surprise that the student was able to outperform the teacher (L027, L483), but do not make it clear that the student was better than the teacher at two types of tasks (Histone Markers, Enhancer Annotation) and worse at two other types (Promoter Annotation, Splice Site Annotation). I suspect that this difference in performance vs the teacher is likely due to the change in pretraining data - by changing the training distribution, the model will become better at the data seen during distillation than the teacher, and less good at data not shown during distillation. However, this may not be the case - the difference could be due to the shift in training task instead of the distributional shift of the data (Lowe et al, 2024; Marks et al, 2024). Hence experiments are needed to assess and evaluate the impact of these factors.\n- The tokenizer mismatch necessitates two-stage masking; character-level teachers or a 6-mer student could eliminate this unnecessary engineering burden, but neither of these options were explored.\n- It would be helpful to see the teacher's performance alongside the graphs in Fig 5. I am also confused as to this subset of the experiments is shown for the figure.\n\n**Presentation**\n- Tables 2 and 3 captions need to explicitly say that this is fine-tuning evaluation (instead of linear probe, kNN, etc.) so a reader can understand the table at a high-level without searching the main text for the section describing the methodology.\n- Table 3 font size is very small. Usually we would want to have a row per comparator (model) and a column per comparison metric (evaluation dataset). If the table doesn't fit that way, it is okay for it to be transposed (as it is now), but the title cell widths need to be narrower so the column widths are narrower and font is larger. Probably this is solved simply by adding a line break in between the method names and \\citep references.\n- Fig 5 text is far too small. Text within figures should not be smaller than 70% of the size of the main body text, i.e. scriptsize.\n- Fig 7: You should bold the best method for each dataset. This will make it clearer which datasets the teacher (NTv2) does best on and which the student (HAD) does best on.\n- Fig 7 is not referred to in the text.\n- L086 Need to explicitly introduce the NTv2 abbreviation here.\n\n**References**\n- MAE-LM: Meng et al (2024). \"Representation Deficiency in Masked Language Modeling\". ICLR 2024. https://openreview.net/forum?id=b3l0piOrGU\n- BarcodeMAE: Safari et al (2025). \"Enhancing DNA Foundation Models to Address Masking Inefficiencies\". https://arxiv.org/abs/2502.18405\n- CrossMAE: Fu et al (2025) \"Rethinking Patch Dependence for Masked Autoencoders\". TMLR. https://openreview.net/forum?id=JT2KMuo2BV\n- Lowe et al (2024). \"An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders\". https://arxiv.org/abs/2406.02465\n- Marks et al (2024). \"A Closer Look at Benchmarking Self-Supervised Pre-training with Image Classification\". https://arxiv.org/abs/2407.12210"}, "questions": {"value": "- Fig 5 shows the student performs better when distilling the 50M than the 100M NTv2 teacher. Can the authors comment on why this might be?\n\n**Typos**\n- L20 \"we employ [the] NTv2-500M as\"\n- L21 \"grouping masking strategy\" -> \"grouped masking strategy\"\n- L453 Truncated sentence"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "obX0rOgmG4", "forum": "M18s0nmmWP", "replyto": "M18s0nmmWP", "signatures": ["ICLR.cc/2026/Conference/Submission20412/Reviewer_wuBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20412/Reviewer_wuBn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762635683602, "cdate": 1762635683602, "tmdate": 1762933852001, "mdate": 1762933852001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}