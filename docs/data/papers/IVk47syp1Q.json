{"id": "IVk47syp1Q", "number": 3785, "cdate": 1757520510429, "mdate": 1759898069917, "content": {"title": "MatMuls are Enough for Efficient and Performant Linear-Time Attention", "abstract": "Transformers, despite empowering current AI revolution, are bottlenecked by suboptimal hardware utilization and quadratic runtime complexity of softmax attention w.r.t. input sequence length. Many recent architectures aspire to bring the complexity down to sub-quadratic level without compromising modeling quality. However, they are either much slower on all but very long sequences or rely on low-level code tailored to a narrow subset of modern hardware. To simultaneously achieve linear complexity, hardware efficiency, and portability, we completely eliminate softmax from self-attention; and remove, reduce, modify, or rearrange other transformations in the Transformer block. The resulting architecture, DenseAttention Network, is composed entirely of dense matrix multiplications in the attention which allows for efficient training and inference in both quadratic and linear modes. It outperforms standard Transformer and its counterparts in language modeling benchmarks and surpasses previous Transformer-based SOTA by 5\\% on challenging Long Range Arena. DenseAttention model written in plain PyTorch is up to 22\\% faster even on small context sizes, and by orders of magnitude on longer sequences, than Transformer with low-level FlashAttention kernel.", "tldr": "We propose a novel Transformer-based architecture with a linear-time attention mechanism that consists of only MatMuls  and is 1) faster and more computationally efficient than Transformer; 2) achieves similar or better performance.", "keywords": ["Deep learning", "sequence models", "softmax-free attention", "hardware efficiency", "linear-time attention", "transformer architecture", "efficient transformers", "attention mechanisms", "layer normalization", "long context", "long sequence modeling", "Long Range Arena"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62725e68b5c617f118f92d765e42894c0b1a6428.pdf", "supplementary_material": "/attachment/12b762dcb7c7c25af0ba6cb9fc5bcf31ea223a5f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DenseAttention Network (DANet), a radically simplified architecture designed to address the twin bottlenecks of the standard Transformer: its quadratic runtime complexity and poor hardware utilization. The central idea is the complete elimination of the softmax function from the attention mechanism, which is reformulated as a pure composition of dense matrix multiplications: ``Attention(X) = XW_Q * X^T * X``. This design allows for a dual-complexity computation—switching between an O(N²) and an O(N) path by leveraging the associative property of matrix multiplication—making it efficient for all sequence lengths. To ensure numerical stability, the authors replace LayerNorm with a novel ``MaxNormActivation`` (L∞ normalization). Empirically, the paper shows that DANet can outperform standard Transformers on benchmarks like the Long Range Arena (LRA) and is significantly faster than highly-optimized implementations like FlashAttention-2, even on short sequences."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its highly original and audacious \"back-to-first-principles\" approach to model design. It compellingly argues, through both theoretical reasoning and extensive experiments, that the core of the Transformer can be radically simplified without sacrificing performance. The dual-complexity mechanism is a clever and fundamentally new way to achieve efficiency across all sequence scales. The co-design of ``MaxNormActivation`` demonstrates a deep understanding of the numerical challenges and provides a creative solution. The work is presented with outstanding clarity, guiding the reader through its logical progression from problem motivation to architectural solution and finally to a versatile set of empirical results. Its significance lies in challenging long-held assumptions about necessary model components, potentially inspiring a new wave of research into \"hardware-aware\" and simplified foundation models."}, "weaknesses": {"value": "1. The ``MaxNormActivation`` function is a simple but aggressive solution to the numerical instability caused by removing softmax. Its core mechanism, ``x / max(|x|)``, forces the output into the ``[-1, 1]`` range, effectively preventing value explosion. However, this comes at a theoretical cost: a single large activation value in a token's embedding—whether it is a meaningful signal or mere noise—will disproportionately suppress all other feature dimensions, potentially leading to a severe information bottleneck. The paper currently lacks direct experimental evidence to demonstrate that the impact of this information loss on model accuracy is acceptable. An ablation study comparing ``MaxNormActivation`` to other potential (but perhaps less stable) normalization strategies, or an analysis showing that such feature squashing is rare or benign in practice, would be crucial.\n2. The layout of the paper has minor issues that affect readability. Specifically, the spacing between some tables and the surrounding text is minimal (e.g., between Table 2 and Table 3, and between Table 5 and Table 6), making the sections feel cramped.\n3. Figure 2 is central to the paper's claims about inference speed, but it is overly simplistic. It currently only displays results for a single accelerator (NVIDIA A100). Given that performance results for other hardware like the H100 and CPUs are presented elsewhere in tables, consolidating these into a multi-series plot would provide a much more comprehensive and impactful visualization of the architecture's cross-platform advantages.\n4. The ablation studies, while useful, are not conducted on the most contemporary and widely-used models. For example, key architectural choices are evaluated on a BERT-like model. While this is a standard approach, demonstrating that these findings hold on more modern decoder-only architectures like the Llama, Qwen, or Mistral series would significantly strengthen the paper's claims about the general applicability of its design principles.\n5. While the paper includes some results on the state-of-the-art NVIDIA H100 GPU, the majority of the performance experiments appear to be conducted on the previous generation A100. A more extensive evaluation on the Hopper architecture, especially for key training and inference benchmarks, would provide a more forward-looking assessment of the architecture's efficiency."}, "questions": {"value": "1. Could you elaborate on the decision to use the NVIDIA A100 for the majority of the experiments, with only a limited set of results presented on the H100? Was this primarily due to resource availability, or were there other considerations? Understanding this would help contextualize the performance claims.\n2. The choice of ``MaxNormActivation`` is a cornerstone of the architecture's stability. Given the potential for information loss as highlighted in the weaknesses, could you provide more insight into the design process? Were other normalization schemes (e.g., L1 norm, a clipped or scaled L2 norm) explored and found to be unstable or less effective? A brief discussion of the alternatives considered would strengthen the justification for this unconventional choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "isn7AjiAzY", "forum": "IVk47syp1Q", "replyto": "IVk47syp1Q", "signatures": ["ICLR.cc/2026/Conference/Submission3785/Reviewer_cbXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3785/Reviewer_cbXF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761067745029, "cdate": 1761067745029, "tmdate": 1762917021541, "mdate": 1762917021541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the DenseAttention Network (DANet), a simplified Linear Transformer variant in which the authors stabilize training of linear attention without kernels by modelling techniques such as weight sharing, reduction of dropout with other components of the original, specific scaling and novel normalization. \n\nThe authors:\n\n- claim to achieve SoTA over transformer-based models on LRA (look at questions).\n- show that DANet is faster than transformer models in a parameter-matched setting.\n\nHowever, the evaluation raises concerns: the comparison set excludes stronger recent baselines, ablations do not fully disentangle the contributions of individual modifications, and the fairness of depth-vs-width trade-offs is questionable. The removal of residuals and normalization and the reliance on single-head identity mappings also leave scalability to larger models uncertain."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Potential for Impact**: If its claims scale, DANet could be influential for being generally applicable as a transformer improvement.\n- **Relevance**: Efficient attention remains one of the most pressing challenges for scaling language models. DANet addresses this with a simple and broadly applicable design.\n- **Conceptual Minimalism**: Reducing attention to weight-shared matmuls with identity values is novel and may inspire further work in stripped-down Transformer architectures.\n- **Practical Efficiency**: DANet’s PyTorch-only implementation yields consistent throughput advantages across hardware, including CPUs and older GPUs without specialized kernels."}, "weaknesses": {"value": "- **Possibility of main claim being overclaimed**: The paper compares to a very limited set of baselines, some of the baselines (like MEGA[1]) report higher score than in the paper, while also being attention-based. Why are they not taken into consideration?\n- **Ablations**: Performance gains stem from multiple changes (weight sharing, identity keys and values, removal of residuals and norms, increasing the number of layers). More controlled ablations are needed to attribute improvements clearly, Especially that LinearTransformer in the reported results performs significantly poorer.\n- **Scaling Concerns**: Removing residuals and LayerNorm raises questions about optimization stability in deeper networks. Similarly, the fixed identity key and value map could limit head scalability, analogous to problems with Multi-Query Attention (MQA). While here the authors use one head in all experiments, usually more heads are preferrable, hence it is highly questionable whether for large experiments with a lot of head - a single head as proposed by authors would remain favourable.\n- **Worry about fairness in comparison:** The authors compare 24 layers to 32 models arguing that DANet uses less parameters than the baseline hence they increase the number of layers to match the parameters. However, transformer scaling laws usually favour depth over width, hence it is not clear whether the effect is not caused by translating width parameters to depth. Furthemore, it is not clear how this kind of trade off will scale for the models that already have a large number of layers as baseline. The authors could also consider increasing hidden dimension to match the parameters and investigate whether the performance gains hold in this scenario.\n- **Framing Issues**:\n    - The claim of “two operational modes” (linear vs quadratic) is not unique to DANet; most linear-attention methods share this property.\n- The paper would benefit from more related work on novel works on creating new attention variants that operate well also in the short sequences (for example: [2,3,4])\n- **Unclear Writing:** sometimes in the experiments i was not able to find the reference to the specific setup compared (number of layers, parameters, whether the baseline is implemented by the authors etc).\n\ntypos: l69 - arhitecture\n\n**[1] - Mega: Moving Average Equipped Gated Attention**\n\n**[2] - Lizard: An Efficient Linearization Framework for Large Language Models** \n\n**[3] - Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing**\n\n**[4] - Gated Linear Attention Transformers with Hardware-Efficient Training**"}, "questions": {"value": "1. Why were stronger efficient-attention baselines (e.g., MEGA or transformer trained as in “On the Locality Bias and Results in the Long Range Arena”) not included in the comparisons, given that some of them report higher LRA scores than the DANet results shown?\n2. The architecture introduces multiple changes (weight sharing, dropout reduction, residual removal, identity values). Could you provide fine-grained ablations quantifying the contribution of each modification individually?\n3. In the comparison, DANet uses more layers than the baseline Transformer under a parameter-matching scheme. Could you provide experiments where the hidden dimension (width) is scaled instead, to rule out depth-driven improvements?\n4. Is there a GQA variant analogue of MatMul or some other way authors propose to scale to higher widths (which would mean multiple heads in standard transformers)?\n\nDue to the unclear validity of the claim i opt for reject, but if i am convinced that the claim is actually sustained and the authors deliver additional ablations that clearly demonstrate what caused somewhat unexpect gain in comparison to Linear Transformer (ablations that isolate each additional design choice will be good for that), as well as address the writing issues, I am happy to increase the score to accept."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0NnwMeATai", "forum": "IVk47syp1Q", "replyto": "IVk47syp1Q", "signatures": ["ICLR.cc/2026/Conference/Submission3785/Reviewer_yhrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3785/Reviewer_yhrB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480134931, "cdate": 1761480134931, "tmdate": 1762917018474, "mdate": 1762917018474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DANet, an alternative to the Transformer architecture. The architecture is carefully designed to achieve a number of goals:\n- Maximising arithmetic intensity in its application, and particularly avoid hangups due to excessive memory transfer. This is achieved by substituting a number of Transformer components (attention, normalisation layers, PE, residual connections, …) with more efficient counterparts, often developed ad-hoc\n- Being device-agnostic, making a point of not relying on custom kernels or otherwise device-specific optimisation in the implementation of the necessary components, but rather on high-level directives (MMM, element-wise operations) which can be seamlessly ported to different devices.\n- Achieving sub-quadratic complexity (both in runtime and memory occupancy), to guarantee fast processing also for long sequences\n- Retaining an architecture which is overall simple and as close as possible to Transformers, both in terms of definition and of performance, to facilitate its adoption\n\nResults highlight that ~350M scale DANet models rival and beat similarly-sized Transformers and linear-attention alternatives on language modelling tasks from the GLUE benchmark, and long-context tasks from the Long Range Arena suite. Moreover, DANet attains a much higher throughput across different sequence lengths than a Transformer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I’ve found the proposal of creating an efficient and yet device-agnostic architecture interesting and useful (and particularly relevant nowadays, to counterbalance the abundance of Flash- implementation alternatives and the over-focus on ad-hoc custom CUDA kernels to achieve speedups)\n- The architecture modifications proposed in the paper are well-justified and carefully motivated, also with ablation studies, strengthening the overall analysis\n- The main goals are presented well, and the overall story of the paper seems solid (bar some minor rewordings which could improve clarity)"}, "weaknesses": {"value": "- The model sizes considered (~350M) are insufficient to validate the impact of the proposed simplifications at scale\n- Novelty is somewhat limited, as the core simplifications (attention linearisation, reliance on MatMuls) and improvements (windowed attention) have already been explored in the literature (or straightforward adaptations thereof)\n- The baselines considered are rather outdated, which affects the ability to judge the effectiveness of the architecture\n\nSee also questions below."}, "questions": {"value": "- __On scaling__:\nMy main concern is whether the simplifications applied in your architecture (specifically, dropping softmax, substituting normalisation, and reducing number of heads) will hold at scale. On the one hand, I understand that compute availability is severely limited, and pushing to larger scales is not always a possibility; at the same time, since you’re proposing a drop-in substitute to Transformer, I believe it’s fundamental to properly determine whether your architecture represents a valid alternative, or if it is an approach that only works at limited scales. Without larger-scale experiments, I’m afraid it’s impossible to correctly judge its impact.\n\n\n- __On Novelty__:\nThe individual components you’re using in your architecture are rather straightforward adaptations of already-existing ones: your attention linearisation approach is a simplification of the original one in Katharopoulos et al (dropping normalisation and feature maps), and your normalisation is a simplification of RMSNorm (changing norm and dropping affine transform). This leaves as truly novel components only your Cosine RelPE, and possibly the shifted-windowed attention (even though its specific contribution is not properly highlighted by an ablation: I reckon in Tab2 and Tab10 you directly combined windowed and shifted-windowed, so it’s tricky to extract the contribution of shifted only). I appreciate that the main novelty in the work lies in the rationale behind the *combination* of these components (you want to pick/design arithmetic-intensive layers), rather than the components themselves, but my impression is that this does reduce the overall contribution.\n\n\n- __On baselines__:\nI understand the focus on LinearAttention, as it’s likely the closest architecture to the one you propose, but given the progress of the literature on sub-quadratic attention, it’s by far not the best representative. Even relatively small adaptations (Hedgehog, RKWV, …) would make for a more complete baseline comparison, but even more so: the fact that a comparison against Mamba (arguably the current SOTA on sub-quadratic attention) is missing decreases the quality of the analysis. I appreciate that it’s tricky to recover Mamba results on LRA (hence why you settled for the much more outdated S4 SSM, I reckon?), and that your focus is mainly on encoder architectures, but the point remains (bi-directional adaptations of Mamba are easy to setup, see comments below).\n\n\n- __On CLM vs MLM__:\nYour claim on L411 that you “conduct experiments with both Masked and Causal LM, with emphasis on the former” is an overstatement. The only CLM result I could find (correct me if I’m wrong) is in Tab5, where you compare PPL against Llama. This is nowhere near sufficient to make solid claims about CLM capabilities, and leaves more questions than answers: how do you efficiently include the causal mask in your DenseAttention formulation? Do you forego the O(Nd^2) view entirely? How does every other relevant metric change in this case? The (almost) totality of your paper focuses on encoder-only architectures, and in my opinion including discussions on CLM would require major revisions in this sense, specifically adapting the discussion on training throughput, long-context extrapolation, downstream task evaluation, and comparison against sub-quadratic baselines to this case as well.\n\n\n\n__Minor:__\n\n- L102 “This makes it fundamentally different from Linear Attention” (and similarly in L1173): I disagree with this statement: your approach can effectively be interpreted as a direct simplification of LienarAttention, where both the nonlinear feature maps and the normalisation have been dropped. I think defining it as “fundamentally different” is a mischaracterisation, and an unnecessary one.\n- L104 “It’s neither a State-Space-Model (SSM) or a Linear RNN because it has no decay or gating modules”: Similarly here: gates are accessory components to an SSM, and not direct part of its definition. Linear decay can be removed by fixing state matrices to 1. But most of all, I don’t think it’s necessary that you attempt to differentiate your work from similar sub-quadratic architectures. I think you’re already making it quite clear that the novelty of this work lies in the overall goal of creating an architecture that maximises arithmetic intensity without relying on device-specific optimisations.\n* L105 “and it natively supports bidirectional context processing” (and similarly in L1203): This is a stretch: in CLM, the distinction is meaningless; in MLM, equipping an SSM with bidirectional context processing is as simple as flipping the input sequence-wise and combine the result with the non-flipped version. With the equivalence highlighted in Mamba2 between LinearAttention and SSMs, it gets even simpler, as one can directly act on the structure of the learnable attention mask.\n- Fig2: help me make sense of what I’m seeing in this figure. Throughput generally refers to inference in generative models, computed as number of new tokens generated in given time, and should approach a constant in LinearAttention-based architectures / SSMs, and a 1/N behaviour for vanilla Attention (indeed this trend is seen in Tab6). If I got it correctly, though, in Fig2 you’re considering the cost of performing inference on a whole sentence with an encoder-type architecture, and hence it should scale like 1/N for LinearAttention, and 1/N^2 for Attention (it’s not super clear from the description, as you refer to both training and inference speed?). I can’t eyeball whether this is the case, though: can you superimpose the relevant N^{-i} curves, so that the asymptotic behaviour appears clear at a glance?\n\n__Grammar / Rewording / Formatting:__\n\n- L48/53 vs 89: “See Appendices / Appendix” vs “App.”, and throughout the paper: consider unifying your referencing format, either truncating throughout or reporting the whole section name\n- L95: “This duality allows to calculate DenseAttention using either O(N2d) or O(Nd2) FLOPs” I would add an explicit reference to the Mamba2 paper here, as they describe precisely this duality quite well\n- Tab2/3 and Tab5/6: please fix the captions by adding some spacing between the two. As they are right now they’re very confusing"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "//"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mxo0c9zZNl", "forum": "IVk47syp1Q", "replyto": "IVk47syp1Q", "signatures": ["ICLR.cc/2026/Conference/Submission3785/Reviewer_RdSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3785/Reviewer_RdSr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817086368, "cdate": 1761817086368, "tmdate": 1762917015148, "mdate": 1762917015148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DenseAttention, which serves as a drop-in replacement for the classic attention in Transformers. It remove softmax and using MaxNormActivation to ensure numerical stability. DenseAttenion can be computed in O(N^2d) or O(Nd^2) complexity with pure matrix multiplications. Experiments show it achieves competitive performance on long range arena and GLUE tasks while having high throughputacross different GPUs and CPU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. DenseAttention is well supported in this paper with both mathematical grounding from proposition 1 and 2, and experiments across appropriate tasks such as LRA, GLUE and CLM.\n2. DenseAttention achieves impressive hardware efficiency improvements compared with standard and linear attention. \n3. Once open sourced,  the lain PyTorch implementation will be a good contribution to community for people to just plug in and use in most hardwares."}, "weaknesses": {"value": "1. While GLUE is a proper task to validate DenseAttention capabilities, there is no modern language model evaluation benchmarks such as MMLU and other reasoning tasks.\n2. The scaling effect of DenseAttention is unclear as the model size is around 360M. \n3. There is no ablation analysis on the individual components of the DenseAttention, e.g., contribution of removing softmax, using MaxNorm, removing W_k, W_v, W_o, etc."}, "questions": {"value": "1. What do attention patterns that learned by DenseAttention look like and how they are different from softmax attention?\n2. How does DenseAttention compare with Linear Attention in detail? \n3. Does DenseAttention still main good properties of softmax attention such like long range dependencies and in-context modeling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3st70m6KV3", "forum": "IVk47syp1Q", "replyto": "IVk47syp1Q", "signatures": ["ICLR.cc/2026/Conference/Submission3785/Reviewer_3qa3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3785/Reviewer_3qa3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985090885, "cdate": 1761985090885, "tmdate": 1762917014604, "mdate": 1762917014604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}