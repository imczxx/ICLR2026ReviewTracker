{"id": "QcRto0GjxC", "number": 22266, "cdate": 1758328622162, "mdate": 1759896876144, "content": {"title": "Accelerating Inference for Multilayer Neural Networks with Quantum Computers", "abstract": "Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential speed-ups in select computational tasks, yet their integration into modern deep learning pipelines remains unclear. In this work, we take a step towards bridging this gap by presenting the first fully-coherent quantum implementation of a multilayer neural network with non-linear activation functions. Our constructions mirror widely used deep learning architectures based on ResNet, and consist of residual blocks with multi-filter 2D convolutions, sigmoid activations, skip-connections, and layer normalizations. We analyse the complexity of inference for networks under three quantum data access regimes. Without any assumptions, we establish a quadratic speedup over classical methods for shallow bilinear-style networks. With efficient quantum access to the weights, we obtain a quartic speedup over classical methods. With efficient quantum access to both the inputs and the network weights, we prove that a network with an $N$-dimensional vectorized input, $k$ residual block layers, and a final residual-linear-pooling layer can be implemented with an error of $\\epsilon$ with $O(\\text{polylog}(N/\\epsilon)^k)$ inference cost.", "tldr": "We introduce quantum algorithmic primitives enabling the first coherent quantum speedup of inference in multilayer neural networks.", "keywords": ["quantum machine learning", "QML", "quantum algorithms", "quantum deep learning", "quantum computing"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fc190988b4a423cda9e35548365ef8b5c093d97.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a fully quantum method for simulating the inference process of multi-layer neural networks. The authors claim that their approach can achieve exponential speedup when using QRAM, while still maintaining a quadratic speedup in QRAM-free scenarios. The work provides a potential pathway for leveraging fault-tolerant quantum computers to perform inference tasks. However, for the reasons outlined in the Weaknesses section, I do not believe this paper merits acceptance at the ICLR conference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors address one of the most interesting topics in the field of quantum computation—using quantum computers to accelerate classical AI. They also provide a clear summary of previous results and explain how their work relates to them."}, "weaknesses": {"value": "1. The manuscript is not well written: many complex definitions and results are deferred to the appendix. The paper would be more suitable for a journal without strict page limits.\n\n2. Since the QRAM is a very strong assumption, which may require 2^n circuit depth to prepare a target state or the block-encoding of the matrix, all quantum speed-up results based on the QRAM assumption have very limited practical significance. For example, if a practical quantum computer with error-correction function were provided, can authors provide an analysis on how many single- double- quantum gates are required to achieve a QRAM oracle? Moreover, if the QRAM assumption is permitted, classical L2-norm based sampling methods (dequantization algorithm proposed by E. Tang, Nat. Rev. Phy. 4, 692–693, 2022) may achieve similar speedups for low-rank cases. Consequently, I am not convinced that the QRAM-based theoretical results in this paper make a sufficiently strong contribution to the community.\n\n3. Following the above concern, the only potentially valuable contribution of the paper appears to be the QRAM-free scenario discussed in Lemma 5 and Section 4.1.3. However, I cannot follow the argument justifying this claim. In section E.3, the authors claimed that quantum circuit U_{conv} has approximate  O(log(N)T_X) circuit depth, which is central to the claimed quadratic speedup. Tracing this statement back to Theorem 2, I still find the justification unclear: the authors do not provide a convincing derivation or explanation for this depth bound. Please clarify the derivation and state precisely which assumptions are required.\n\n4. A minor error: the author list for this reference is not correct: Towards provably efficient quantum algorithms for large-scale machine learning models. Nature Communications, 15:434, 2024."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gtJFbH6t4y", "forum": "QcRto0GjxC", "replyto": "QcRto0GjxC", "signatures": ["ICLR.cc/2026/Conference/Submission22266/Reviewer_JGKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22266/Reviewer_JGKB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383663170, "cdate": 1761383663170, "tmdate": 1762943656810, "mdate": 1762943656810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the important task of constructing quantum implementations of neural networks, with the goal of achieving provable speed-ups in neural network inference due to quantum phenomena. The paper considers convolutional ResNets and provides an extensive theoretical study, covering different types of assumptions about quantum data access."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "While several previous works have studied this problem for different types of architectures (Kerenidis et al. for CNNs, Guo et al. for transformers), the paper provides important advances by allowing \"coherent\" multi-layer archiectures and non-linearities, where no readout or tomography is required. Employed methods build on previous work by Rattew & Rebentrost on Quantum Vector-Encodings, but extend these in various directions. Considered ResNet architectures are mostly practically relevant, thereby contributing to the building quantum subroutines for provable speed-ups for neural network-based learning. \nIn terms of presentation, the paper is mostly very well-written and clearly presented (except for some ambiguous mathematical statements)."}, "weaknesses": {"value": "There are still several important points related to relevance of assumptions and soundness of the claims / mathematical statements that I think need to be discussed thoroughly: \n\n- Choice of activation function: the introduction and abstract state that the deep learning architectures use sigmoid activation functions. However, throughout the paper (Lemma 6, Theorem 2 and Figure 1) it seems that only \"erf\" is used as activation function. This is not a commonly used activation function due to lack of closed-form expression. \n- Circuits depend on input: a fundamental issue appears to be that the circuits depend on $x$. This issue already appears in the problem statement in Definition 1. The statement suggests that for a fixed $x$ you aim to find $\\hat{y}$ with $|y-\\hat{y}| \\leq \\epsilon$? But in practice you would want to vary $x$. This seems to become a central issue in the main result in Theorem 2: the unitary matrix $U_X$ there depends on the value of the input, which then also seems to affect the overall circuit construction. Looking at the proof, lines 397-399 suggest that the circuit would need to change whenever a new input data point is processed in the network.  \n- Clarity of mathematical statements: the main findings of the paper are weakened by some unclear mathematical statements. For example,  in Definition 1 it is not clear what is meant by \"return a sample from some probability vector $\\hat{y}$ ...\". Is this a random vector from which you randomly sample a realization? If yes, then with a certain probability the sample may not have the desired property. Can you quantify this probability? The same issue appears in the main result in Theorem 2: the statement does not relate the constructed quantum circuit with the vector $\\tilde{y}$. Can you be more clear on how this vector is drawn? And what do you mean here by \"we can draw a sample\" (same question as before)? \n- Assumptions on input: In Theorem 2, the input matrix is assumed to have a representation by a unitary matrix (see lines 378-380). Can you comment on this assumption? Is it easy to shown to be satisfied in certain situations?"}, "questions": {"value": "- Could you obtain the same results for more common choices of activation functions? \n- Lemma 5: the Lemma requires that the convolutional matrix is normalized, which is rarely the case in practice. Is there a way of absorbing this, e.g., into the input? \n- Can you provide more insights on what the Assumptions on the input mean in Theorem 2? \n- From the statement of Theorem 2 it appears that for each new data point, you would need to construct a new circuit. Can you comment on this? \n- Can you clarify on what you mean by \"return a sample from some probability vector\"? This is very important, otherwise the main statement is not clearly understandable. \n- Can you clarify in Theorem 2 how the sample is obtained from the circuit?\n- The neural network sizes assumed in Theorem 2 are very specific (you assume 16 input channels, output channels etc.) Why is this needed? It would be more convincing  to have a result for general sizes of channels. \n\nA few minor points on clarity: \n\n- The legend of figure 1 is a bit confusing, as it says \"The figure shows the architectures we consider ... under thre different regimes of quantum data access assumptions\". However, the figure does not contain any clues about how quantum data access differs in the different subfigures. Could you clarify this? \n- The legend also states that \"the input is assumed to be a rank-3 tensor (e.g. images with 4 channels)\". Wouldn't this correspond to images with 3 channels?\n- In lines 108/109 epsilon appears without having been introduced. \n- In Definition 1 the role of the error epsilon is not mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ST8XDCaPs5", "forum": "QcRto0GjxC", "replyto": "QcRto0GjxC", "signatures": ["ICLR.cc/2026/Conference/Submission22266/Reviewer_yibn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22266/Reviewer_yibn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644013220, "cdate": 1761644013220, "tmdate": 1762942142602, "mdate": 1762942142602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a fully coherent quantum implementation of a multilayer neural network architecture inspired by classical ResNets. The authors design a family of quantum residual blocks that include multi-filter 2D convolutions, sigmoid activations, skip connections, and layer normalization. The key contribution is a fault-tolerant inference algorithm that, under different assumptions about quantum data access (inputs and weights), achieves up to a quartic speed-up over classical inference.\n\nThe analysis formalizes the quantum complexity of inference in three settings:\n1. No quantum data access (quadratic speed-up).\n2. Efficient quantum access to weights (quartic speed-up).\n3. Efficient access to both weights and inputs (polylogarithmic scaling in N/ϵ).\n\nThe work focuses on theoretical algorithm design and complexity proofs, without empirical quantum-hardware demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong theoretical grounding: The paper is mathematically rigorous and develops clear proofs for each claimed speed-up regime.\n\nNovel construction: Presents, to my knowledge, the first coherent quantum implementation of a multilayer residual network with nonlinear activation and skip connections.\n\nSolid literature placement: The work situates itself appropriately relative to prior quantum neural network and quantum linear-algebra algorithms, clarifying how block-encoding techniques generalize to deep architectures.\n\nPotentially broad impact: The modular framework could be extended to other neural architectures or used as a building block for hybrid quantum–classical learning pipelines.\n\nClear organization: The text is logically structured, with well-defined assumptions and computational models.\n\nDespite being theoretical, this paper is a well-constructed and rigorous contribution to the quantum machine learning literature. The results are technically sound, clearly presented, and potentially influential for future algorithmic developments in quantum deep learning.\nWhile its immediate practical relevance is limited, the conceptual advancement—showing coherent quantum analogues of residual networks with formal complexity guarantees—justifies acceptance at ICLR, provided the venue continues to welcome high-theory work at the intersection of ML and quantum computation."}, "weaknesses": {"value": "High theoretical abstraction: The paper is highly formal and primarily relevant to researchers in quantum algorithms. It lacks discussion of practical feasibility, especially the resource requirements for realistic network sizes.\n\nLimited accessibility: The work assumes familiarity with block-encoding, and fault-tolerant quantum computation. This makes it difficult for deep-learning practitioners to appreciate its implications or limitations.\n\nNo discussion of training: The paper only treats inference and does not address how such quantum architectures could be trained, even conceptually. Since training dominates the computational cost in deep learning, the absence of any consideration of gradients, parameter updates, or trainability significantly narrows the scope and impact of the contribution."}, "questions": {"value": "No empirical validation: While the results are theoretical, even small-scale simulations or resource estimates (for near-term or logical qubit counts) could help contextualize the practical significance of the proposed speed-ups.\n\nCould this framework generalize beyond ResNet-style architectures, for example, to transformers or graph networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aFDcmGxNUO", "forum": "QcRto0GjxC", "replyto": "QcRto0GjxC", "signatures": ["ICLR.cc/2026/Conference/Submission22266/Reviewer_LfKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22266/Reviewer_LfKb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828892124, "cdate": 1761828892124, "tmdate": 1762942142390, "mdate": 1762942142390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework for accelerating inference in multi-layer neural networks, specifically those with ResNet-like architectures, using a fault-tolerant quantum computer. The authors introduce a set of quantum subroutines to coherently implement key components of modern deep neural networks, including multi-filter 2D convolutions, non-linear sigmoid activations, skip-connections, and layer normalizations. The work analyzes the inference complexity under three different quantum data access regimes, determined by the availability of Quantum Random Access Memory (QRAM) for inputs and weights. The main results claim a polylogarithmic cost in the input dimension. when QRAM is available for both inputs and weights, and polynomial (quartic and quadratic) speedups over classical methods in regimes with limited or no QRAM access."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a significant theoretical contribution by providing a quantum algorithm for a highly relevant classical task—deep neural network inference—and offering rigorous proofs of an asymptotic speedup. This directly addresses the core goal of Quantum Machine Learning (QML).\n\nA major achievement of this work is the construction of a fully coherent multi-layer architecture. By designing primitives that avoid intermediate measurements, the algorithm circumvents the readout and re-encoding bottleneck that has plagued previous proposals for deep QNNs. \n\n The authors' development of the vector-encoding (VE) framework is a key innovation. Critically, this framework enables the implementation of complex operations like 2D multi-filter convolutions without QRAM, directly addressing one of the most significant feasibility concerns surrounding many proposed quantum algorithms."}, "weaknesses": {"value": "The framework presupposes a large-scale, fault-tolerant quantum computer, which is decades away. Therefore, an estimate of the quantum overhead of this algorithm for a specific application is critical for assessing its practical feasibility.\n\nThe paper is mathematically dense, making it challenging for a broader audience to grasp its core mechanics. The lack of high-level intuition and illustrative explanations of the core algorithmic mechanics hinders the work's ability to convey its conceptual breakthroughs and limits its overall impact.\n\nAlthough the model reduces the dimension dependence, this comes at the cost of exponential scaling with respect to the number of layers. A discussion on the trade-off between system dimension and model depth could enhance the paper's theoretical depth.\n\nThe paper's analysis is asymptotic, and it lacks numerical simulation to validate the algorithm's behavior or performance characteristics. Some approximation of the model performance will significantly improve this paper."}, "questions": {"value": "The paper's key contribution is reducing the dependency on the input dimension. However, this appears to come at the cost of an exponential scaling with respect to the number of network layers. To enhance the paper's theoretical depth, could the authors provide a dedicated discussion on this critical trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5ckwtrKi4l", "forum": "QcRto0GjxC", "replyto": "QcRto0GjxC", "signatures": ["ICLR.cc/2026/Conference/Submission22266/Reviewer_GrZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22266/Reviewer_GrZJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919453825, "cdate": 1761919453825, "tmdate": 1762942142176, "mdate": 1762942142176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}