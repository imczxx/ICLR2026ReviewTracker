{"id": "iGuz8mgsye", "number": 7802, "cdate": 1758036785876, "mdate": 1759897831610, "content": {"title": "Diffusion Beats ARM: Diffusion Large Language Models for Generative Recommendation", "abstract": "As a promising new paradigm, generative recommender systems frame recommendation as a process of learning data distributions, enabling content creation and diversity exploration by modeling patterns in user behaviors or item characteristics. A common practice to handle large-scale item catalogs is to quantize different item features into discrete semantic sequences, which are then used to train large language models for item generation. However, we argue that this autoregressive generation approach is fundamentally misaligned with the nature of item features in recommendation. Unlike natural language, item attributes are parallel, intertwined, and mutually defining—lacking the hierarchical and sequential dependency that autoregressive models assume. This misalignment limits the effectiveness of existing generative recommendation methods. To address this issue, we propose a new generative recommendation paradigm called \\texttt{GREED} (\\textbf{G}enerative \\textbf{RE}commendation via \\textbf{E}lemental \\textbf{D}iffusion over Large Language Models). Instead of relying on sequential generation, \\texttt{GREED} leverages diffusion-based generative modeling to capture the joint distribution of item features in a non-autoregressive manner. This design better respects the parallel structure of item attributes, thereby improving both efficiency and ranking performance. Extensive experiments demonstrate that \\texttt{GREED} outperforms state-of-the-art methods on multiple benchmark datasets. We also conduct detailed offline analyses to validate the efficiency and effectiveness of our approach.", "tldr": "", "keywords": ["Large Language Models", "Generative Recommendation", "Discrete Diffusion Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22b3bee76adcbd1d8fbda562132747bd29a71534.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a diffusion language model paradigm for generative recommendation, instead to autoregressive models (ARMs). It argues that ARMs are fundamentally misaligned with recommendation tasks because item attributes are parallel and interdependent rather than sequential. To address this, it proposes two main contributions: 1. Uniform implicit quantization (UIQ) that uses normalizing flows to transform item embeddings into uniform distributions before scalar quantization, avoiding codebook collapse. 2. A hybrid generation architecture that uses discrete diffusion to generate individual items while using autoregression across items in the sequence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes uniform implicit quantization (UIQ), addressing codebook collapse in VQ-based methods and artificial sequential dependencies in RQ-based methods.\n2. The proposed GREED achieves strong performance compared to both discriminative methods and generative methods.\n3. It is worth exploring using diffusion language models on recommendation tasks, as long as there are clear motivations."}, "weaknesses": {"value": "1. The paper claims that \"diffusion beats ARM\", but the evidence suggests that UIQ is the core contribution to improve the performance. If there is a well designed quantization for the ARM (e.g., TIGIER, OneRec), then the ARM may still be a better choice than diffusion language models for recommendation. \n2. The motivation of using diffusion language models claims item attributes are parallel. Tthis motivation is farfetched. And the ablation results show that improvements are mainly from UIQ. Why parallel generation of semantic IDs beat sequential generation is confusing. The paper provides intuition but no evidence.\n3. All datasets are from Amazon Review dataset. It limits the domain diversity. The average sequence length in Amazon Review dataset is short (< 10)."}, "questions": {"value": "Please address the weaknesses above. \n\n1. The paper makes a valuable contribution (UIQ) but oversells another contribution (diffusion) without sufficient evidence. The title and framing may be misleading given the actual results. And is it possible to use UIQ + ARM? I would like to improve my score if the authors clearly demonstrate or explain the benefits of diffusion language models or reframing as primarily a quantization contribution with diffusion as a reasonable approach.\n\n2. How does GREED perform compared with diffusion-based sequential recommendation models? It would be better to cite some of these methods [1][2][3] and acknowledge their contributions.\n\n3. It would be more clear if the authors can provide the source code.\n\n[1] DiffuRec: A Diffusion Model for Sequential Recommendation. Li et al.\n\n[2] Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion. Yang et al.\n\n[3] A Survey on Diffusion Models for Recommender Systems. Lin et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "apELoxJFis", "forum": "iGuz8mgsye", "replyto": "iGuz8mgsye", "signatures": ["ICLR.cc/2026/Conference/Submission7802/Reviewer_UQhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7802/Reviewer_UQhQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760827529059, "cdate": 1760827529059, "tmdate": 1762919844568, "mdate": 1762919844568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GREED (Generative REcommendation via Elemental Diffusion over Large Language Models), a novel generative recommendation system that leverages diffusion-based modeling to address limitations in autoregressive models (ARMs) for discrete quantized generative recommendation (DQGR). It argues that ARMs impose artificial sequential dependencies on parallel item attributes, leading to errors and inefficiency. GREED introduces Uniform Implicit Quantization (UIQ) to enhance discrete item representation and a diffusion-based paradigm for parallel multi-token prediction. Experiments on benchmark datasets show GREED outperforming state-of-the-art methods, with claims of improved efficiency and ranking performance, though validation details are sparse."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality**: Attempts to apply diffusion to DQGR, a departure from ARM dominance, though heavily derivative.\n- **Quality**: Some performance gains are reported, but experimental design flaws undermine reliability.\n- **Clarity**: Limited by poor figure annotation and lack of detailed methodology.\n- **Significance**: Targets efficiency, but lacks evidence of real-world impact or scalability."}, "weaknesses": {"value": "- **Methodological Flaws**: UIQ’s codebook utilization claim lacks empirical validation (e.g., no entropy or diversity metrics). The diffusion process’s parallel advantage is not quantified across varying sequence lengths or compared to optimized ARMs (e.g., with parallel decoding).\n- **Experimental Gaps**: No statistical analysis (e.g., t-tests) supports performance claims. Dataset specifics (e.g., item count, attribute diversity) and hyperparameter tuning are omitted, hindering reproducibility.\n- **Oversight**: Ignores potential biases in quantized representations and their impact on user fairness. Efficiency claims lack comparison with memory-optimized diffusion models (e.g., DDIM).\n- **Validation**: Offline analyses are mentioned without metrics or protocols, and no online testing data is provided to substantiate real-world applicability."}, "questions": {"value": "1. Can the authors provide statistical tests (e.g., t-tests) to validate UIQ’s superiority over VQ and RQ in codebook utilization and semantic fidelity?\n2. How does GREED’s performance scale with increasing sequence lengths, and why was no comparison made with parallel-decoding ARMs (e.g., Transformer-XL)?\n3. What were the dataset sizes, attribute distributions, and hyperparameter settings used in experiments, and how were they tuned to ensure robustness?\n4. Can the authors quantify the error propagation in ARMs versus diffusion in GREED with a controlled experiment on synthetic data?\n5. Why were no memory-optimized diffusion variants (e.g., DDIM) compared to assess GREED’s efficiency claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q7fKYfibUA", "forum": "iGuz8mgsye", "replyto": "iGuz8mgsye", "signatures": ["ICLR.cc/2026/Conference/Submission7802/Reviewer_ddH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7802/Reviewer_ddH7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372111268, "cdate": 1761372111268, "tmdate": 1762919844110, "mdate": 1762919844110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generative recommendation framework named GREED based on diffusion large models. It represents items as discrete semantic IDs through uniform implicit quantization (UIQ), and replaces the traditional ARM with non-autoregressive block-level diffusion generation to alleviate the feature sequence assumption and error accumulation. The experiments have achieved state-of-the-art results on three Amazon datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.For the first time, discrete diffusion was applied to recommendation, aligning with the parallel structure of item attributes and improving generation efficiency.\n\n2.UIQ utilizes learnable flows to explicitly align uniform distributions, theoretically proving to minimize quantization distortion and maximize codebook entropy, overcoming VQ collapse.\n\n3.Block-level multi-token parallel generation supports global planning, with an average Recall@1 improvement of over 15%, and allows for flexible adjustment of sampling size to balance performance and latency.\n\n4.New capabilities such as conditional recommendation and list sorting are provided, verifying the scalability of the framework."}, "weaknesses": {"value": "1. When compared with strong baselines such as S3-Rec, the pre-training data sizes were not unified. Using Sentence-T5 encoding in GREED might introduce additional external semantics, raising doubts about the fairness.\n\n2.UIQ relies on the learnable flow to fit the edge distribution. When dealing with high-dimensional sparse features and having insufficient training samples, the flow model is prone to overfitting, resulting in an increase in quantization error. The paper does not provide high-dimensional experiments or regularization strategies.\n\n3.Discrete diffusion requires maintaining a large-sized transfer matrix, and the video memory grows linearly with the number of codebook layers. When the scale of the product reaches the tens of millions level, distributed training or approximate sampling schemes have not been discussed. The engineering feasibility needs to be verified."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6ejUWsI2ve", "forum": "iGuz8mgsye", "replyto": "iGuz8mgsye", "signatures": ["ICLR.cc/2026/Conference/Submission7802/Reviewer_e48C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7802/Reviewer_e48C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896540434, "cdate": 1761896540434, "tmdate": 1762919843658, "mdate": 1762919843658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the misalignment between autoregressive models (ARMs) and the parallel, sequence-free nature of items in generative recommendation, proposing the GREED paradigm. It has two stages: 1) Uniform Implicit Quantization; 2) discrete diffusion generation (non-autoregressive parallel generation to reduce errors, block-level autoregression for list-wise recommendations). Experimental results on three public Amazon datasets validate the effectiveness of GREED."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* S1: This paper proposes a non-autoregressive discrete diffusion generation paradigm, GREED, that breaks free from the sequential dependencies imposed by traditional autoregressive models.\n* S2: Through probability integral transformation (PIT) and normalized flow, item representations are transformed into uniform distributions, theoretically demonstrating minimal quantization distortion and maximum codebook entropy.\n* S3: GREED supports flexible conditional recommendation tasks, capable of handling arbitrary numbers and sequences of attribute condition inputs to accommodate users' ambiguous needs in real-world scenarios."}, "weaknesses": {"value": "* W1: The dataset scenario is limited, and generalization validation is insufficient. Validation was conducted solely on three product categories from Amazon Product Reviews (Arts, Video Games, and Musical Instruments), all of which fall within e-commerce product recommendation scenarios. This fails to demonstrate the method's effectiveness in scenarios with high cardinality and more frequent dynamic updates.\n* W2: The comparison baseline is outdated, and many of the latest generative recommendation baselines were not included in the comparison. The experimental results lack credibility.\n* W3: The comparison and ablation experiments for core components are insufficiently comprehensive. UIQ is only contrasted with RQ-VAE and RQ-Kmeans, omitting newer quantization methods (e.g., adaptive quantization, hybrid quantization). The impact of quantization level L and stream model type on performance was not analyzed."}, "questions": {"value": "* Q1: You need to supplement the latest baseline methods, such as HSTU,  LCRec, MiniOneRec, and MTGR.\n* Q2: The impact of quantization level L and stream model type on performance should be analyzed.\n* Q3: Testing with a single dataset alone cannot demonstrate the method's universality; additional datasets are required to validate its generalization capability (provided that the latest baseline is used for comparison)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "td0WTsH7Jt", "forum": "iGuz8mgsye", "replyto": "iGuz8mgsye", "signatures": ["ICLR.cc/2026/Conference/Submission7802/Reviewer_Veks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7802/Reviewer_Veks"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762791179678, "cdate": 1762791179678, "tmdate": 1762919843210, "mdate": 1762919843210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}