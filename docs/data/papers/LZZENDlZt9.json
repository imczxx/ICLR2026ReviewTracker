{"id": "LZZENDlZt9", "number": 24290, "cdate": 1758354986339, "mdate": 1759896772643, "content": {"title": "Smarter Not Harder: Generative Process Evaluation with Intrinsic-Signal Driving and Ability‑Adaptive Reward Shaping", "abstract": "Large reasoning models (LRMs) have shown strong performance in complex mathematical reasoning when optimized via reinforcement learning (RL). However, conventional outcome-only reward provides sparse feedback, leading to inefficient optimization. In this work, we investigate whether generative process reward models (GenPRMs) can accelerate RL training of LRMs by improving the utilization of reasoning trajectories. We first analyze critical limitations in existing GenPRMs, including their heavy reliance on reasoning ability during correctness judgment, and suppression of exploration as well as vulnerability to reward hacking during reward assignment. To address these limitations, we first propose a novel \\textbf{intrinsic-signal-drive n evaluation} mechanism, which judges reasoning steps using semantic cues from the solution, thus mitigating extensive dependence on GenPRM. Furthermore, we (i) adopt \\textbf{thought-level rewarding granularity} to alleviate over-dense step rewards, and (ii) design an \\textbf{ability-adaptive reward formulation} that dynamically balances exploration and exploitation and keeping the optimization target of key tokens to mitigate reward hacking. We integrate these innovations into the process reward-based GRPO, resulting in the proposed \\textbf{TP-GRPO} algorithm. Experiments on LRMs with 1.5B and 7B parameters show that TP-GRPO achieves higher improvements while using significantly fewer training samples, and more analyses further confirm the effectiveness of our proposed process evaluation mechanism.", "tldr": "We identify key pitfalls for GenPRMs—such as over-reliance on reasoning, exploration suppression, and reward hacking—and propose TP‑GRPO, a RL framework with intrinsic-signal evaluation, thought-level granularity, and ability-adaptive rewards.", "keywords": ["Generative Process Reward Model", "Math Reasoning", "Large Reasoning Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c03ab0110e3b01a35c56e10df363a34a037c10e0.pdf", "supplementary_material": "/attachment/b17937c640384df0f59f58b7258ddee54539638b.zip"}, "replies": [{"content": {"summary": {"value": "This paper tries to address key challenges in incorporating generative process reward models (GenPRMs) into RLVR. The identified challenges are: (1) the requirement for a GenPRM with strong reasoning ability; (2) the risk of reward hacking from over-densified rewards; and (3) the potential discouragement of policy exploration. The authors tackle challenge (1) by using semantic matching to locate correct and incorrect steps. They tackle challenge (2) by merging continuous steps with the same correctness into a larger block. They tackle challenge (3) by using an adaptive process reward assignment. By incorporating the process rewards provided by their framework into GRPO, they proposed TP-GRPO. The main results are evaluated on mathematical reasoning benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n\n- By identifying three crucial challenges for GenPRMs, this paper provides meaningful guidance for future research directions in the field.\n\n- The idea of using semantic alignment for process evaluation is intuitively sound, and it could potentially reduce the difficulty of this task for GenPRM."}, "weaknesses": {"value": "**Lack of Evidential Support**\n\nThe paper makes several assertions that lack sufficient supporting evidence. For instance:\n\n- At line 236, the paper states that \"static rewards can unintentionally suppress exploration.\" At line 248, the paper states that \"when $\\text{acc}_G=0$, $r^c$ vanishes, reducing to outcome supervision GRPO and prioritizing exploration.\" Why static rewards would suppress exploration? And why, when reduced to outcome supervision, is exploration prioritized? Empirical or theoretical evidence is needed to support these justifications.\n\n- At line 220, the paper states that \"If process rewards are assigned at the step level ... which could in turn mislead the optimization to incorrect directions.\" This conclusion requires further justification. For example, if the process reward is assigned correctly at the step level, would it still mislead the optimization direction?\n\n**Strong Assumptions and Limited Application Scenarios**\n\n- At line 203, the \"Identify Effective Steps\" component depends heavily on the LRM capacity for accurate self-reflection, a capability not universal among all models. How can the proposed method be applied to those LRMs without self-reflection behavior?\n\n- At line 205, the paper states \"During reflection, the LRM typically analyzes the causes of the reflected mistake\". This assumes that LRM can correctly diagnose the cause of a mistake during reflection. The case of incorrect self-diagnosis is not addressed.\n\n- Even if the LRM has self-reflection and can correctly identify the location of a previous error, the requirement for the GenPRM to identify \"steps dependent on... incorrect previous steps\" (line 207) might still raise concerns about its need for non-trivial reasoning capabilities.\n\n- At line 211, the hypothesis that \"all steps in the answer are erroneous\" does not generally hold. As established by previous work [1], incorrect trajectories often contain valid reasoning steps before the first error occurs. \n\n[1] Let's Verify Step by Step, arXiv 2023.\n\n**Incomplete Experimental Setting**\n\nTo robustly support the claim that the evaluation scheme has \"low reasoning requirements on the PRM,\" additional settings should be added: using models like Qwen3-32B, Qwen3-4B, and Gemma-3-12B-it as the GenPRMs without applying the proposed \"reducing reasoning requirements\" methods."}, "questions": {"value": "- At line 248, the paper states that \"$\\text{acc}_G$ is the accuracy of the G sampled solutions for the same problem.\" The definition of $\\text{acc}_G$ is confusing. Is it the average accuracy across G sampled solutions for the same problem or something else?\n\n- Also at line 248, the paper states that \"when $\\text{acc}_G=0$, $r^c$ vanishes, reducing to outcome supervision\" I am a little bit confused about why  $r^c=0$ would cause it to reduce to outcome supervision.\n\n- Could $\\text{acc}_G$ also be applied to the computation of $r^{ic}_i$ to achieve adaptive penalization according to the difficulty of the problem?\n\n- I am confused about the definition of the advantage function in this paper. According to Equation 3 in the paper, it is more like the definition of returns (or cumulative rewards). According to the definition in [1], the advantage function measures whether an action is better or worse than the policy's default (or expected) behavior. I hope the authors will clear this up for me.\n\n[1] High-Dimensional Continuous Control Using Generalized Advantage Estimation, ICLR 2016."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cxy1FkIVja", "forum": "LZZENDlZt9", "replyto": "LZZENDlZt9", "signatures": ["ICLR.cc/2026/Conference/Submission24290/Reviewer_LpBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24290/Reviewer_LpBz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686166161, "cdate": 1760686166161, "tmdate": 1762943031083, "mdate": 1762943031083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel reinforcement learning framework for large reasoning models (LRMs) that enhances reasoning efficiency via process reward modeling (PRM). The authors identify key limitations in existing generative process reward models (GenPRMs), notably their dependence on reasoning ability, over-dense reward signals, and susceptibility to reward hacking. To overcome these issues, the paper introduces an Intrinsic-Signal-Driven Evaluation method and a Thought-Level Ability-Adaptive Reward Mechanism, which together form the proposed TP-GRPO algorithm. Specifically, thought-level\n\nExperimental results on the DeepSeek-R1-Distill-Qwen (1.5B and 7B) models demonstrate significant improvements in training efficiency while maintaining comparable performance. Also, the proposed techniques are less sensitive to the GenPRM's reasoning ability, successfully mitigating the reasoning dependency issue."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty: The techniques of (1) reflection localization for determining the granularity of thoughts and (2) thought-level advantage adjustment are new to me. The paper also introduces several useful methods in Sections 3.1.1 and 3.1.2.\n\n2. Quality: The paper accurately pinpoints three major weaknesses in current GenPRM designs: excessive dependency on reasoning strength, over-dense reward granularity, and reward hacking. The analysis is precise and well-supported by empirical and conceptual insights."}, "weaknesses": {"value": "1. Clarity: It is better to highlight the contribution of the paper with a more concise title, such as \"Improving Reasoning Efficiency via Thought-Level Reflective Reward Shaping.\" The terms \"intrinsic-signal\" and \"ability-adaptive\" are somewhat misleading, as the former can be easily confused with the conventional term \"intrinsic reward\" [1], and the latter could be replaced with more commonly used terms such as \"difficulty-aware,\" given that \"ability\" sometimes refers to more general categories of skills like coding, math, tool use, etc.\n\n2. Significance, Quality: In Tables 2 and 3, TR-GRPO is compared only with methods based on outcome reward models (ORMs), and additional comparisons with other PRM baselines are needed, such as [2][3][4]. It is also recommended to include comparisons with stronger ORM methods, such as those in [5].\n\n3. Novelty: The thought-level idea is similar to the one in [4]. It is worth discussing the differences between the two works and making an empirical comparison if applicable.\n\n4. Quality: The effectiveness of TR-GRPO would be more evident if the comparisons in Tables 2 and 3 were made fair by using the same budget or by measuring the number of samples needed to achieve the same overall accuracy.\n\nI think the paper provides many useful techniques, but also has obvious issues, as mentioned above. I would be happy to consider raising my score if the aforementioned concerns are well addressed.\n\n### References\n\n[1]: Pathak, Deepak, et al. \"Curiosity-driven exploration by self-supervised prediction.\" International conference on machine learning. PMLR, 2017.\n\n[2]: Zhao, Jian, et al. \"Genprm: Scaling test-time compute of process reward models via generative reasoning.\" arXiv preprint arXiv:2504.00891 (2025).\n\n[3]: Zhang, Hanning, et al. \"Entropy-regularized process reward model.\" TMLR, 2024.\n\n[4]: Xiong, Wei, et al. \"Stepwiser: Stepwise generative judges for wiser reasoning.\" arXiv preprint arXiv:2508.19229 (2025).\n\n[5]: Chen, Minghan, et al. \"Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization.\" arXiv preprint arXiv:2505.12346 (2025)."}, "questions": {"value": "* In Figure 3, the accuracy fluctuates significantly and is not monotonically increasing. I am wondering if this is still the case under different random seeds and whether the conclusion still holds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V0wZbQPvdh", "forum": "LZZENDlZt9", "replyto": "LZZENDlZt9", "signatures": ["ICLR.cc/2026/Conference/Submission24290/Reviewer_N8ji"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24290/Reviewer_N8ji"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866546621, "cdate": 1761866546621, "tmdate": 1762943030720, "mdate": 1762943030720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits GenPRM for RL of large reasoning models by using intrinsic semantic cues in the solution trajectory to judge step correctness (locating reflections, tracing the earliest error, and interval labeling), merging consecutive steps with the same correctness into “thoughts,” and applying thought-level, ability-adaptive rewards; on incorrect solutions, only thoughts semantically aligned with the wrong answer are penalized. Combined with GRPO as TP-GRPO, the method attains higher or comparable accuracy with far fewer training solutions on DeepSeek-R1-Distill-Qwen 1.5B/7B (e.g., 5.6K vs 34K; 8.56K vs 16K) and introduces the Effic. metric to quantify improved sample efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The research question and proposed method are interesting, e.g., alleviating GenPRM’s reasoning burden and the observation on dense step-wise rewards."}, "weaknesses": {"value": "The method's reliance on the LRM's own capacity for self-reflection and annotation is a fundamental limitation. This approach is inherently self-limiting, as any improvement is capped by the model's existing capabilities and the task's difficulty, hindering the acquisition of novel skills. The poor performance in Table 1 exemplifies this concern.\n\nThe experimental setup is questionable. The RL training is remarkably inefficient, yielding only a 2-4% accuracy gain over 400-1000 iterations—a potential artifact of an undersized group size for GRPO that undermines the paper's credibility. Furthermore, the efficiency comparisons in Tables 1 and 2 are inequitable due to mismatched hyperparameters (e.g., group_size, batch_size) across baselines, and a performance comparison at convergence is conspicuously absent."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DSfRCkw4Mt", "forum": "LZZENDlZt9", "replyto": "LZZENDlZt9", "signatures": ["ICLR.cc/2026/Conference/Submission24290/Reviewer_iXx8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24290/Reviewer_iXx8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907837786, "cdate": 1761907837786, "tmdate": 1762943030486, "mdate": 1762943030486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper begins with an in-depth analysis of the challenges inherent in Generative Process Reward Model (GenPRM) based process evaluation. To mitigate these challenges, the authors propose a novel generative process evaluation mechanism. This mechanism features an intrinsic-signal-driven evaluation (judging reasoning steps based on semantic information) and thought-level, ability-adaptive reward schemes. The mechanism is integrated with the GRPO algorithm to form a new RL algorithm, termed TP-GRPO. Experimental results on mainstream reasoning benchmarks demonstrate that TP-GRPO achieves superior training efficiency and accuracy compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and presents the topic clearly.\n\n2. The proposed method is well-motivated and appears technically sound.\n\n3. The empirical evaluation is promising. TP-GRPO is shown to outperform the GRPO baseline and most existing outcome reward-based RLVR methods in terms of both training efficiency and accuracy."}, "weaknesses": {"value": "1. Notable performance gap remains when comparing TP-GRPO to state-of-the-art models with much larger training budgets (e.g., DeepScaler-1.5B-Preview and Skywork-OR1-7B). The paper does present initial scaling trend (i.e., Fig 3), but the experiment appears to be conducted with relatively low training budgets. This limited scope makes it difficult to conclusively determine whether the advantages of TP-GRPO will persist, widen, or saturate as model scale and training budgets increase significantly.\n\n2. The introduction clearly articulates several \"design pitfalls\" of existing GenPRM-based process evaluation. While the paper does provide some analyses related to these points, the insights are somewhat scattered throughout the experimental section rather than being presented cohesively. The paper would be significantly strengthened if this analysis were consolidated and made more explicit, clearly demonstrating how the proposed method directly mitigates each of the identified pitfalls."}, "questions": {"value": "Overall, this paper is well-written, well-motivated, and the proposed method is technically sound. The empirical results are promising, demonstrating clear improvements over strong baselines, even if they do not surpass all current SOTA methods. In order to maintain my rating, I would like the authors to address the points in the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7JKp8OyMcT", "forum": "LZZENDlZt9", "replyto": "LZZENDlZt9", "signatures": ["ICLR.cc/2026/Conference/Submission24290/Reviewer_ebqn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24290/Reviewer_ebqn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933452858, "cdate": 1761933452858, "tmdate": 1762943030210, "mdate": 1762943030210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}