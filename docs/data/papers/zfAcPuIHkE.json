{"id": "zfAcPuIHkE", "number": 11168, "cdate": 1758191825424, "mdate": 1759897603073, "content": {"title": "Logic Channel Validation and Enhancement of Zero-Shot Vision-Language Comprehension on Vision Language Models", "abstract": "Frontier Large Vision-Language Models (LVLMs) exhibit remarkable capabilities in Visual-Language Comprehension (VLC) tasks, enabled by pretraining on vast visual-textual corpus. However, they are often deployed as zero-shot solution in a black-box manner, as retraining challenges remain due to data privacy or model inaccessibility. Validating and understanding the behavior of the models become important for generalization to new task. We propose a Logic Channel, in parallel with the black-box model channel, to perform explicit logic reasoning for validation and enhancement. The frontier LVLM, encapsulating latent vision-language knowledge, can be considered as an Implicit Logic Channel. The proposed Explicit Logic Channel, mimicking human logic reasoning, incorporates a Large Language Model (LLM), a Visual Foundation Model (VFM), and a logical reasoning module involving novel probabilistic inference for factual, counterfactual, relational, and causal condition reasoning over the extracted and grounded visual-textual facts. Cross-channel logic consistency analysis enables model validation and selection, even without ground-truth annotations. Additionally, cross-channel integration further improves performance in zero-shot tasks over SOTA models. Our experiments on three recent challenging VLC benchmarks, Neg- Bench, HC-RefCOCOg, and HC-RefLoCo, demonstrate the effectiveness of the proposed Logic Channel for logic-based model validation, selection and improvement on LVLM with enhanced explainability and trustworthiness.", "tldr": "", "keywords": ["Vision-Language Model", "Visual-Language Comprehension", "logic reasoning", "zero-shot"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96c56a14fcc8dd012c0592278b5be75c8cde4fc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Logic Channel Validation (LCV), a framework for evaluating and improving reasoning in large vision-language models by separating their decision process into two complementary parts. The Implicit Logic Channel (ILC) captures how a VLM internally reasons when directly judging an image–text pair, while the Explicit Logic Channel (ELC) explicitly parses the linguistic structure using a language model and grounds the visual entities and relations with a vision foundation model through probabilistic logic. The paper introduces Channel Reliability (CR) to measure the consistency between implicit and explicit reasoning and evaluates the approach on three benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The logic decomposition feels natural and intuitive to use in practice.\n\n2. A clear formulation with a simple CR metric enabling label-free validation and model selection.\n\n3. A modular pipeline (LLM parsing + VFM grounding + probabilistic logic) that yields interpretable evidence and easy ablations."}, "weaknesses": {"value": "1. The claimed **comprehensive** scope is narrow, since ELC mostly covers existence/negation/spatial relations and not attributes or OCR, so the paper should clarify boundaries and outline extensions.\n2. The approach is heavily dependent on the chosen VFM, so a cross-VFM sensitivity check is needed to establish robustness.\n3. ILC relies on self-reported 0–100% confidences without reporting decoding settings or calibration, so the paper should include temperature/prompt details.\n4. CR can be inflated by correlated errors between channels, so the paper should decompose agree-and-correct vs agree-and-wrong and relate CR to human accuracy."}, "questions": {"value": "1. Could you clarify the actual reasoning scope of ELC? Beyond existence, negation, and spatial relations, can it handle attributes or OCR cases, and if not, what are your plans to extend it?\n\n2. How sensitive is ELC to the underlying vision foundation model? Have you tested robustness across different detectors?\n\n3. What were the decoding and prompting settings when obtaining ILC’s 0–100% confidences (e.g., temperature), and how stable are the results under different settings or paraphrased prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wcxpbSnhyZ", "forum": "zfAcPuIHkE", "replyto": "zfAcPuIHkE", "signatures": ["ICLR.cc/2026/Conference/Submission11168/Reviewer_yfrb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11168/Reviewer_yfrb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635120455, "cdate": 1761635120455, "tmdate": 1762922328531, "mdate": 1762922328531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Logic Channel framework to address the lack of trustworthiness and explainability in frontier LVLMs, especially for zero-shot tasks. The core idea is a dual-channel system: an implicit channel (the black-box LVLM) runs parallel to an Explicit Logic Channel (ELC). The ELC mimics human reasoning by using an LLM to parse sentences, a VFM to ground key facts, and a set of customized probabilistic logic rules for inference. The Logic Channel serves two purposes: validation, assessing model reliability via cross-channel consistency checks (CR), and enhancement, boosting SOTA accuracy on logical reasoning benchmarks like HC-RefCOCOg by fusing the two channel predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies the critical gap where current LVLMs are black boxes lacking reliability. By proposing an explicit, explainable logic channel to serve as a verifiable \"auditor\" for the black-box LVLM, it offers a direct and timely solution to enhance model trust and transparency.\n\n2. By combining predictions from both the ILC and ELC, the system reliably improves the final decision accuracy. This enhancement is notable on complex logical reasoning benchmarks (like HC-RefCOCOg), validating the benefit of explicit logic in unlocking LVLM potential."}, "weaknesses": {"value": "1. The paper designs separate, customized probabilistic inference formulas for each specific logic type (negation, relation, causation). This strongly suggests the ELC's logic module lacks true generalizability. If a new, mixed-logic task emerges, researchers may have to manually write new logic formulas, severely limiting the framework's automation and scalability.\n\n2. The ELC is a complex pipeline assembled from pre-trained black-box components (LLM parser, VFM detector, logic module). Its robustness is constrained by the weakest link. If VFM fact grounding fails (e.g., missing a crucial object), the entire explicit reasoning chain breaks. The paper lacks a proper robustness analysis against these cascading component failures.\n\n3. The ELC operates as a parallel system requiring sequential calls to an LLM and a VFM, introducing a significant computational and latency overhead. The paper completely fails to quantify this cost, which makes the \"enhancement\" feature's practicality for real-time deployment highly questionable."}, "questions": {"value": "Q1. The paper shows success across limited logic forms (based on existence/relation/causal categories). If a mixed-logic and complex task requiring negation, relation, and causation simultaneously were introduced, how would the ELC's logic module automatically or uniformly combine these separate probabilistic logics for a single final decision?\n\nQ2. To assess the true deployment cost of the enhancement feature, please provide detailed inference latency data  for the ELC channel across the benchmarks. This breakdown should include the segmented time consumption for the LLM parsing, VFM grounding, and final logic reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dhgNljpfdY", "forum": "zfAcPuIHkE", "replyto": "zfAcPuIHkE", "signatures": ["ICLR.cc/2026/Conference/Submission11168/Reviewer_RcoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11168/Reviewer_RcoM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646109792, "cdate": 1761646109792, "tmdate": 1762922328145, "mdate": 1762922328145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To validate and improve the performance of visual language models across various visual comprehension tasks, the authors propose a framework that adds explicit logic channels (ELC) to the main model pipelines (referred to as implicit logic channels, i.e., the visual models under evaluation). In this setup, the ELC is a task-specific, carefully designed combination of LLMs and other foundation models (such as object detectors) that can perform the same task. The ELC’s outputs are then used either to validate the main model’s responses or to enhance them when errors occur.\nIn the methodology section, the authors describe their ELC designs for three tasks: generating correct text descriptions from images (logical reasoning on facts and negation), referring expression comprehension (logical reasoning on facts and relations), and referring expression comprehension with long context (logical reasoning on causal conditions). The evaluation explores how the ELC contributes to both validation and performance improvement across these tasks, showing that the inclusion of an ELC helps with model diagnosis, validation, selection, and overall output quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a relevant and timely problem and is written clearly. The assumptions are well laid out, and the experimental results generally support the authors’ claims. The overall presentation is easy to follow, and the technical details are described with care."}, "weaknesses": {"value": "I do have some reservations regarding the level of contribution and the generalizability of the proposed framework. In essence, the authors design a set of expert-crafted pipelines (the ELCs) for the three target tasks and then use these as oracles to validate or improve the main visual-language model (VLLM). However, each ELC is itself a more complex, task-specific system that can already perform the task effectively. This raises the question of whether the ELC truly provides a meaningful or general solution—especially since the framework implicitly assumes that the ELC performs near-perfectly, in which case it might render the use of the main VLLM unnecessary.\n\nOverall, while the paper presents a creative and well-structured framework, it does not fully convince me of the necessity or broader impact of the proposed approach. The idea is interesting and the results are promising, but the paper would benefit from a clearer motivation and stronger justification for why this framework is needed in the first place."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6aBnc7SPhx", "forum": "zfAcPuIHkE", "replyto": "zfAcPuIHkE", "signatures": ["ICLR.cc/2026/Conference/Submission11168/Reviewer_xErW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11168/Reviewer_xErW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793142870, "cdate": 1761793142870, "tmdate": 1762922327653, "mdate": 1762922327653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method for Visual-Language Comprehension (VLC) tasks that leverages an LLM to extract entities/relations from textual descriptions, a Vision model as a detector for grounding (text-to-image), and a logical reasoning module for probabilistic inference of factual, counterfactual, relational, and causal condition reasoning over the extracted and grounded image-text. The authors evaluate their proposed method on NegBench MCQ and HC-RefCOCOg / HC-RefLoCo referring tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed pipeline effectively decompose the image and text inputs, which enable further explainability and interpretability. The setup is simple, yet effective, and is consistent with related work that also tackles visual reasoning through similar decomposition pipelines.\n\n- This paper covers negations, relations and long context text, and show strong results of using off-the-shelf models for textual decomposition and visual grounding without finetuning for visual-language comprehension.\n\n- Strong results on NegBench (negation) and HC-RefLoCo (long-context). This expands prior works results on compositional benchmarks to other challenging tasks (particularly the long-context setup)"}, "weaknesses": {"value": "- Missing prior works (novelty and claims): The paper’s idea of LLM extraction/expansion, symbolic/probabilistic solver and detector grounding is very close to LINC [1], BIRD [2], and other neuro-symbolic pipelines that also use visual entailment + contradictions (factual + counterfactual, neutral + negations) for image-text evaluation/alignment [3, 4, 5, 6]. Particularly, [4, 7] show strong performance with training-free strategies.\n\n- Missing ablations for the parser LLM (prompting, temperature), the VFM (GroundingDINO thresholds, NMS, text queries), sentence-utility weights for the long-context setting, and fusion weights across channels. There are no ablations for swapping VFMs (e.g., OWL-ViT, Florence-2), parser quality/noise injection, probability aggregation rules (min/max vs. soft aggregators), or final calibration. \n\n- \n\n[1] LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers (Olausson et al., EMNLP 2023)\n\n[2] Feng, Yu, et al. \"Bird: A trustworthy bayesian inference framework for large language models.\" arXiv preprint arXiv:2404.12494 (2024).\n\n[3] Yarom, Michal, et al. \"What you see is what you read? improving text-image alignment evaluation.\" Advances in Neural Information Processing Systems 36 (2023): 1601-1619.\n\n[4] CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples (Zhang et al., ACL Findings 2024)\n\n[5] Wang, Tan, et al. \"Equivariant similarity for vision-language foundation models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[6] Kamath, Amita, et al. \"The hard positive truth about vision-language compositionality.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[7] Cascante-Bonilla, Paola, et al. \"Natural language inference improves compositionality in vision-language models.\" ICLR 2025."}, "questions": {"value": "- High agreement between two correlated channels can be confidently wrong; is there any empirical/quantitative analysis about the correlation between the Consistency Rate and accuracy across models/datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEenTyROvU", "forum": "zfAcPuIHkE", "replyto": "zfAcPuIHkE", "signatures": ["ICLR.cc/2026/Conference/Submission11168/Reviewer_1ZfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11168/Reviewer_1ZfW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017129861, "cdate": 1762017129861, "tmdate": 1762922327265, "mdate": 1762922327265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}