{"id": "W8ZXfNaqku", "number": 10128, "cdate": 1758161438913, "mdate": 1759897672165, "content": {"title": "Frayed RoPE and Long Inputs: A Geometric Perspective", "abstract": "Rotary Positional Embedding (RoPE) is a widely adopted technique for encoding position in language models, which, while effective, causes performance breakdown when input length exceeds training length. Prior analyses assert (rightly) that long inputs cause channels to rotate \"out of distribution,\" but it is not clear how extra rotation relates to or causes pathological behavior. Through empirical analysis we advance a unified geometric understanding of attention behavior with RoPE. We find that attention induces tight clustering of separated key and query latent point clouds, allowing for creation of sink tokens: placeholders that allow attention heads to avoid token mixing when not required. RoPE applied to longer inputs damages this key/query cluster separation, producing pathological behavior by inhibiting sink token functionality. From this geometric perspective, we propose RoPE-ID (In Distribution), a straightforward modification that allows attention layers to generalize to longer inputs out of the box: apply RoPE with high frequency to a subset of channels. We demonstrate the effectiveness of RoPE-ID for extended inputs using 1B and 3B parameter Transformers on the LongBench and RULER information retrieval benchmarks.", "tldr": "We advance a geometric understanding of latent attention dynamics, use it to explain RoPE's failure to generalize to long contexts, and mitigate that failure with a straightforward architectural adjustment", "keywords": ["RoPE", "context length extension", "sink tokens", "clustering", "attention", "long context", "transformer", "language model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26b1fa825a618d805c9d169aa705f1e31cea1e68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose RoPE-ID, a rotary positional embedding (RoPE) based positional embedding method that scales a proportion of head-specific channels to ensure that long-context inputs are more in-distribution relative to the training data. The authors show that this method is competitive with existing long-context extension techniques on numerous benchmarks on models of multiple sizes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors provide a strong motivation for the method through the analysis of attention sinks and singular values. The experiments are also comprehensive and show some cases of RoPE-ID being beneficial compared to alternative methods."}, "weaknesses": {"value": "- The contribution itself remains somewhat trivial; relative to the changes with standard RoPE, it appears that the primary difference is only that low frequency RoPE dimensions are individually scaled such that they can complete the rotation within the provided training length. \n\n- There is a rather significant lack of clarity regarding the effects of the hyper-parameter choices; for example using half the dimensions in a fixed manner and/or the maximum/minimum frequencies allowed.\n\n- The experimental results are rather unconvincing of the benefits of the method due to some inconsistencies. For example, on RULER, improvements in the 1B model do not appear statistically significant compared to YaRN. Meanwhile, on the 3B model, performance on the 8K context is much better in YaRN but then decreases for 16K. For LongBench, YaRN outperforms RoPE-ID on the 3B model but not the 1B. Thus I cannot gauge exactly what the trend will be for either larger models or more data.\n\n- These models all require training from scratch; this is a rather strong limitation. Additional baselines should include NTK-aware scaling methods, which the authors mention in the related work but do not compare with on downstream tasks. Given the additional complexity of pre-training from scratch, it is further important to compare against methods that calibrate or adjust scaling factors rather than require pre-training new models from scratch as well."}, "questions": {"value": "- While the methodology differs, I believe that the underlying phenomena that the authors discuss relates significantly with [1]. In particular, ensuring that the low frequency dimensions complete a rotation is quite related to ensuring that longer relative distances does not appear OOD relative to what was learned during training.\n\n- Based on my understanding of the pseudocode in Appendix A.3, the scaled dimensions are hard-coded as the first quarter and third quarter of the dimensions. Why is this the case and wouldn't it make more sense to apply a more adaptive scaling method that looks at all the dimensions instead and choose a proportion of the dimensions to scale based on the frequency values?\n\n- While I'm not the largest advocate for scaling trends, given the inconsistency in the results the method would be more convincing if either results on larger models or data sizes were provided and showed a more clear trend of the advantages of RoPE-ID relative to existing methods.\n\n[1] Resonance RoPE: Improving Context Length Generalization of Large Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x933FFKdNb", "forum": "W8ZXfNaqku", "replyto": "W8ZXfNaqku", "signatures": ["ICLR.cc/2026/Conference/Submission10128/Reviewer_9pc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10128/Reviewer_9pc3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208418982, "cdate": 1761208418982, "tmdate": 1762921497797, "mdate": 1762921497797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes why Rotary Positional Embedding (RoPE) fails beyond its training context and attributes the issue to geometric degradation in attention. The authors show that key and query vectors form tight clusters and rely on a “sink” token for stability, but RoPE distorts this structure at long ranges. They propose RoPE-ID, which applies high-frequency RoPE to only part of the channels to preserve cluster geometry without fine-tuning. Results on 1B and 3B models show improved long-context performance on LongBench and RULER benchmarks compared to standard RoPE baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Introduces a clear geometric explanation for RoPE’s long-context failures, supported by multiple complementary analyses.\n\nProposes a simple, low-cost remedy (RoPE-ID) that requires no fine-tuning, facilitating rapid deployment.\n\nDemonstrates robust empirical evaluation across tasks and baselines and provides reproducibility details."}, "weaknesses": {"value": "Theoretical analysis stops short of giving formal bounds that relate rotation frequency to cluster overlap or performance.\n\nExperimental scope is limited to 1B/3B models and up to 16k context; applicability to 7B+ models or 100k+ contexts is not shown.\n\nKey hyperparameter choices (channel ratio, cycle length, temperature coefficient) lack comprehensive ablation or principled justification.\n\nBaseline comparisons are incomplete—direct controlled comparisons to Hope[1], and other recent methods under identical settings are missing.\n\n[1] Chen, Y., Lv, A., Luan, J., Wang, B., & Liu, W. (2024). *HoPE: A novel positional encoding without long-term decay for enhanced context awareness and extrapolation*. arXiv preprint arXiv:2410.21216."}, "questions": {"value": "an you define a quantitative “cluster overlap” metric (e.g., silhouette score, Davies-Bouldin index) and show its correlation with sink attention and downstream scores?\n\nWhat is the empirical sensitivity to the RoPE channel ratio (e.g., 25%, 37.5%, 62.5%, 75%) and how does the trade-off between positional fidelity and cluster preservation behave?\n\nPlease provide a direct empirical comparison to Hope under identical experimental settings to clarify the regimes where RoPE-ID’s training-free advantage holds.\n\n\n*(Optional) Does RoPE-ID scale to 7B+ models and much longer contexts (100k–1M tokens)? How do key/query cluster properties change with model scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KxL7j6IsbC", "forum": "W8ZXfNaqku", "replyto": "W8ZXfNaqku", "signatures": ["ICLR.cc/2026/Conference/Submission10128/Reviewer_v9An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10128/Reviewer_v9An"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546103916, "cdate": 1761546103916, "tmdate": 1762921497458, "mdate": 1762921497458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Transformers with RoPE fail on inputs longer than training length. The paper observes that keys and queries form separated clusters, enabling \"sink tokens\" to absorb default attention. RoPE's low-frequency channels rotate slowly, barely moving during training. Beyond training length, they reach \"out of distribution\" rotation angles, causing clusters to overlap and breaking sink token functionality. RoPE-ID applies high-frequency RoPE to half the channels (completing full rotations during training), keeping the other half stable. This maintains cluster separation while staying in-distribution, enabling 16× context generalization without fine-tuning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Overall, thorough analysis, then simple solution. Awesome! Great Science!\n- Reveals that attention uses separated key/query clusters (opposite of conventional wisdom), connects RoPE mechanics, attention geometry, and sink tokens into one elegant explanation, slow-rotating channels reach unseen angles beyond training length, destroying cluster separation\n- Zero-shot method that matches or exceeds YARN. RoPE-ID is basically \"use high frequencies on half the channels\", trained on 4k, works on 64k with no tuning, conceptually cleaner than previous method\n- Super great presentation. Clear and meaningful figures, great writing. Makes complex geometry intuitive"}, "weaknesses": {"value": "- The last paragraph of the intro is hard to parse, though easy to understand after reading the paper. \n- The tables could emphasize more that RoPE-ID is zero shot."}, "questions": {"value": "- In Figure 3, could you align the scaling of the x and y axis within the 4k and the 64k lengths? \n- Could you add to the tables what the numbers are? Maybe the unit?\n- Why alternating quarters? I understood that 1 and 3 are rotated, leaves 2 and 4 stable. Why not just first half and second half? Or more finegrained interleaving? Or could simpler patterns work just as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RHMG2SsIkL", "forum": "W8ZXfNaqku", "replyto": "W8ZXfNaqku", "signatures": ["ICLR.cc/2026/Conference/Submission10128/Reviewer_AmPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10128/Reviewer_AmPV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679815095, "cdate": 1761679815095, "tmdate": 1762921497060, "mdate": 1762921497060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a geometric analysis of Rotary Positional Embeddings (RoPE) to explain long-context failure modes in Transformers. The authors find that RoPE induces tightly clustered query/key point clouds within training length, supporting the formation of sink tokens that prevent over-mixing. However, when input length exceeds training context, RoPE disperses and overlaps these clusters, disabling sink token functionality and causing degraded attention behavior.\nBuilding on this insight, the paper proposes RoPE-ID (In-Distribution), a modification that applies RoPE with high frequency to only a subset of channels, maintaining cluster separation while preserving position information. Experiments on LongBench and RULER with 1B- and 3B-parameter Transformers show that RoPE-ID achieves comparable or superior long-context generalization to tuning-free baselines such as YaRN, without fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The geometric interpretation of RoPE and attention behavior is original, intuitive, and interesting, linking positional encoding, cluster geometry, and sink token dynamics into a unified framework.\n\n2. The paper validates hypotheses through detailed analyses (PCA projections, singular value decomposition, attention maps) and replicates findings across multiple LLM families (LLaMA, Gemma, Olmo).\n\n3. Figures (e.g., cluster diagrams and singular-value ratios) effectively convey geometric intuition and support claims.\n\n4. Addresses a critical bottleneck in scaling LLMs to longer contexts, thus of broad interest to both researchers and practitioners."}, "weaknesses": {"value": "1. While the geometric intuition is appealing, the paper lacks a mathematical analysis of how RoPE’s rotation frequencies affect cluster stability. The mechanism by which RoPE transforms i.i.d. token embeddings into clustered structures and subsequently causes cluster dispersion under out-of-distribution (OOD) conditions remains insufficiently explained.\n\n2. Figure 2 analyzes cosine similarities, yet attention operates on dot products. This mismatch raises concerns about whether the reported geometric observations truly reflect the behavior of the attention mechanism.\n\n3. The so-called “Unified Theory of RoPE Attention” Section 3.2.2 primarily summarizes prior empirical findings rather than providing a formal theoretical analysis. The term “theory” may therefore be overstated.\n\n4. The proposed RoPE-ID method, which applies RoPE to only half of the channels using higher frequencies, does not convincingly demonstrate that it can effectively mitigate the identified issues. The paper does not clearly explain why this configuration mitigates key/query dispersion, and the presented results suggest the phenomenon is not fully resolved. Moreover, the authors should further analyze the trade-off that higher frequencies blur local positional distinctions.\n\n5. The authors should include ablation studies on the proportion of channels to which RoPE is applied and on the chosen frequency range parameters, to better justify these design choices.\n\n6. Figure 5 lacks proper x-axis labeling, and the notion of “time” used in the caption is ambiguous. Additionally, many figures appear visually unpolished and would benefit from clearer annotations, consistent axis scaling, and improved aesthetics to enhance interpretability."}, "questions": {"value": "In Figure 7, could the authors clarify the comparison between the configuration with $\\theta = 500k$ and the RoPE-ID variant? The plot seems to indicate that the $\\theta = 500k$ baseline achieves a higher FSV ratio, implying stronger cluster preservation. Since a smaller $\\theta$ corresponds to higher-frequency rotations, why does the high-frequency (orange) curve yield the lowest FSV ratio? This appears to contradict the authors’ claim that higher frequencies should maintain cluster stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DaFaXem7sd", "forum": "W8ZXfNaqku", "replyto": "W8ZXfNaqku", "signatures": ["ICLR.cc/2026/Conference/Submission10128/Reviewer_QNK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10128/Reviewer_QNK8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938578388, "cdate": 1761938578388, "tmdate": 1762921496563, "mdate": 1762921496563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}