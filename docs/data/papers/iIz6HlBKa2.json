{"id": "iIz6HlBKa2", "number": 14010, "cdate": 1758226833030, "mdate": 1759897396380, "content": {"title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts", "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful technique for extracting human-interpretable features from neural networks activations. Previous works compared different models based on SAE-derived features but those comparisons have been restricted to models within the same modality. We propose a novel indicator allowing quantitative comparison of models across SAE features, and use it to conduct a comparative study of visual, textual and multimodal encoders. We also propose to quantify the *Comparative Sharedness* of individual features between different classes of models. With these two new tools, we conduct several studies on 21 encoders of the three types, with two significantly different sizes, and considering generalist and domain specific datasets. The results allow to revisit previous studies at the light of encoders trained in a multimodal context and to quantify to which extent all these models share some representations or features. They also suggest that visual features that are specific to VLMs among vision encoders are shared with text encoders, highlighting the impact of text pretraining.", "tldr": "We conduct a large scale study on how visual, textual and multimodal models share concepts across modalities and introduce two new dedicated indicators.", "keywords": ["Explainability", "Sparse Autoencoders", "Multimodality"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99d9c0abee75f26fb8c277949b629f9bdf19433f.pdf", "supplementary_material": "/attachment/0a64fd1e0dfdbf9aa77cc8cea16476788e8abaa1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a comparison of visual, textual and multimodal encoders based on Sparse Auto Encoder (SAE) derived features. It proposes two tools - weighted Maximum Pairwise Pearson Correlation and Comparative Sharedness for performing this comparison. Using these tools, the model examines three datasets of text-image pairs (COCO, Laion-2B and Oxford-102 flowers) and reports some findings. The key findings are that 1) Encoders from different modalities (visual and text) show greater similarities when considering representations from only the final layer, 2) cross-modal similarities appear to be correlated to the image-text alignment quality in the dataset, and 3) visual features specific to visual language models such as CLIP/DFN are more similar to those from LLMs (e.g. BERT) than from other visual foundation models (e.g. Dino v2)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Proposes two new tools for comparing visual and text language models based on sparse auto-encoder (SAE) derived features\n* Presents a comparison of multiple vision, text and multimodal encoders based on SAE derived features.\n* Obtains some findings about the similarity of models, data quality and representations."}, "weaknesses": {"value": "* The findings in the paper (e.g. importance of the last layer, the typology of features specific to visual language models are more similar to text language models rather than other visual foundation models) could have been obtained due to idiosyncrasies in the specific datasets and need to be confirmed by repeating the analysis on more datasets. Without such a confirmation, it is hard to know whether these findings will generalize to novel datasets and models.\n* The paper employs SAE but does not present an overview of the SAE approach. Without such a presentation, it is hard to understand the significance of specific modifications that were employed in the paper (e.g. in Section 2)."}, "questions": {"value": "* Section 2: It would be good to present a short overview of SAE in the appendix. Without such an overview, it is hard to comprehend sentences like L062: \"The SAE is trained with mean square error loss using all patches or token of the input text\"\n* L125: images -> examples\n* L374: \"The obtained typology is very similar to the one established while considering VLM visual encoders, pushing the hypothesis that previous observations could be caused by their text pretraining.\" Could there be other factors that could have resulted in this finding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3DO3UmG79Z", "forum": "iIz6HlBKa2", "replyto": "iIz6HlBKa2", "signatures": ["ICLR.cc/2026/Conference/Submission14010/Reviewer_7ViG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14010/Reviewer_7ViG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747671874, "cdate": 1761747671874, "tmdate": 1762924503582, "mdate": 1762924503582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel indicator to compare multiple encoders using their SAE features. This allows the quantification of the “shared” features common to different models. The key contributions are the scale of comparisons and performing multimodal comparisons. The authors also propose 2 metrics – weighted Maximum Pairwise  Pearson  Correlation and Comparative Sharedness which enable comparison across models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tExtensive experiments have been conducted – with/outside modality comparisons in popular models, multiple datasets\n2.\tQualitative analysis to identify underlying concepts has been done"}, "weaknesses": {"value": "My main concern is that all the numbers reported are in terms of the proposed metric wMPPC. While I understand intuitively why the use of weighting is important, I think there needs to be a comparison between wMPPC and older metrics, with examples that show the need for proposing wMPPC.\n\nDetailed questions and clarifications are listed in the Questions section."}, "questions": {"value": "1.\tIn the introduction, please elaborate why handling multiple modalities is challenging, and why this has not been done in the past.\n2.\tThe claim in the abstract that previous papers have only looked at single modalities may need more context. Other papers like [1-4] do appear to have considered multi modality. Please clarify why these were not included.\n3.\tIn equation (6), can we not just use the term $S_i^M \\times (\\rho_i^{M -> A} - \\rho_i^{M-> B})$? This term would decrease if the similarity between M and B increases, and increase if the similarity between M and A increase. I’m guessing it’s to handle the positive and negative values of $\\rho$ but it would be good to clarify this.\n4.\tLikewise, in eq (7), please provide the intuition for choosing this particular form, as opposed to taking the absolute value of the difference.\n5.\tI may have missed it, but please highlight the area where models of different sizes have been experimented with.\n6.\tCould you also highlight all the 21 models mentioned in the abstract in the appendix? \n7.\tIn the implementation details, please describe how the hyperparameter tuning was done.\n8.\tThe conclusion about the last layers of the LLMs being most important semantically is drawn from Figure 1, which is based on the proposed metric. Is this conclusion also supported with MPPC metric as well? \n9.\tI do not fully understand how the conclusion made about the last layers being important holds, given that in Fig1, towards the inner layers (~20), the wMPPC values are extremely low across the layers.\n10.\tMy main concern is that all the numbers reported are in terms of the proposed metric wMPPC. While I understand intuitively why the use of weighting is important, I think there needs to be a comparison between wMPPC and older metrics, with examples that show the need for proposing wMPPC.\n\n\n[1] Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, and Stephanie Gil. Interpreting the linear structure of vision-language model embedding spaces. arXiv preprint arXiv:2504.11695, 2025.\n\n[2] Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, and Zeynep Akata. Sparse autoencoders learn monosemantic features in vision-language models. arXiv preprint arXiv:2504.02821, 2025.\n\n[3] Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, Yulan He, and Yifei Wang. Multi-faceted multimodal monosemanticity. arXiv preprint arXiv:2502.14888, 2025.\n\n[4] Vladimir Zaigrajew, Hubert Baniecki, and Przemyslaw Biecek. Interpreting CLIP with hierarchical sparse autoencoders. arXiv preprint arXiv:2502.20578, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P2ZA8Hq8qg", "forum": "iIz6HlBKa2", "replyto": "iIz6HlBKa2", "signatures": ["ICLR.cc/2026/Conference/Submission14010/Reviewer_muUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14010/Reviewer_muUv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972948215, "cdate": 1761972948215, "tmdate": 1762924503126, "mdate": 1762924503126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces two new metrics: weighted Maximum Pairwise Pearson Correlation (wMPPC), which is an extension of the previous MPPC score, and Comparative Sharedness, to compare interpretable features extracted via Sparse Autoencoders (SAEs) across visual, textual, and multimodal encoders. The authors conduct a large-scale analysis on 21 transformer-based encoders from different modalities and datasets, identifying shared and modality-specific concepts. Results highlight that shared cross-modal information mainly resides in the final layers and that text pretraining drives high-level visual concepts in VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Analyzing similarities and differences across visual, textual, and multimodal encoders is valuable, as it can inform how future models are trained and aligned\n\n* The study spans a large and diverse set of 21 transformer encoders, offering broad coverage across modalities, datasets, and scales.\n\n* The paper includes a detailed limitation section, showing good awareness of scope boundaries and possible extensions."}, "weaknesses": {"value": "* I found a bit surprising that CLIP image features are more correlated with DINOv2 image or than with SigLIP image (trained similarly to CLIP), Tab. 1. Same for SigLIP image being more correlated to CLIP and BERT text rather than SigLIP text encoder ! This make me question the proposed metrics.\n\n* It’s not clear how the observed correlations translate to real-world impact (measured with quantitative metrics), for example, whether they relate to model performance, bias, or hallucination behavior.\n\n* The study focus on contrastive multimodal encoders. How these findings holds for encoders trained with reconstruction objectives such as AIMv2 [1] \n\n* The analysis and findings (e.g. increasing correlation in last layers, multimodal concepts ...) closely resembles earlier concept-based interpretability [2] and modality alignment papers [3], but the connection to those frameworks is not mentioned or clarified. The paper should position itself to these related lines of research.\n\n* For this kind of papers, more visual illustrations might help to understand better the contributions.\n\n[1] \"Multimodal autoregressive pre-training of large vision encoders\", CVPR 2025.\n\n[2] \"A concept-based explainability framework for large multimodal models.\" NeurIPS 2024.\n\n[3] \"Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs.\" NeurIPS 2024."}, "questions": {"value": "Please check weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8X1NIy8oUQ", "forum": "iIz6HlBKa2", "replyto": "iIz6HlBKa2", "signatures": ["ICLR.cc/2026/Conference/Submission14010/Reviewer_WLJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14010/Reviewer_WLJ6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986131948, "cdate": 1761986131948, "tmdate": 1762924502741, "mdate": 1762924502741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new metric \"weighted Maximum Pairwise Pearson Correlation\" or wMPPC which is similarity measure between two models computed through the weighted expectation of per feature correlation by sampling the activations. The weighting allows focus on correlations that are high between a set of features in two models. The authors find that computing wMPPC on different models uncovers differences in the quality of image-text alignment between datasets, e.g. Laion-2B is worse that Coco.  Another metric \"Generalized Comparative Sharedness\" is proposed that allows probing of a model over individual concepts/features to determine how unique it is to a group/class of models. The former is global metric of model similarity, the latter is more focused metric of similarity. The sharedness metrics shows how some textual concepts are well shared between text and VLMs, but not visual foundation models. These two new metrics show some promise in being useful diagnostics to help understand encoders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Two new metrics are proposed that help to understand the similarities and differences between models. The authors show how these metrics can be used to uncover interesting details like the quality of the original corpora or \"shared concepts\" learned between models. The paper provides clear details and pointers to scripts on reproducing results and works with public data sets and models so should be highly reproducible."}, "weaknesses": {"value": "The paper provides a comparative study of visual, textual and joint vision-text models. It would be super interesting to see what insights these measures could provide with the addition of audio to the assessed modalities."}, "questions": {"value": "Have you looked at adding audio as a modality to analyze with your two new metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DST3zGvkpC", "forum": "iIz6HlBKa2", "replyto": "iIz6HlBKa2", "signatures": ["ICLR.cc/2026/Conference/Submission14010/Reviewer_QAEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14010/Reviewer_QAEJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762839543912, "cdate": 1762839543912, "tmdate": 1762924502355, "mdate": 1762924502355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}