{"id": "qkGhc3l5py", "number": 20899, "cdate": 1758311509079, "mdate": 1759896953116, "content": {"title": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses", "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\\%$ against a newly released defended model from Meta.", "tldr": "", "keywords": ["prompt injection", "optimization-based attacks", "LLM alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5dd6f5063051ea66f02375ad431da259bfd3bf2f.pdf", "supplementary_material": "/attachment/42d3b0fe4742c104720ff23211cadfab038a6251.zip"}, "replies": [{"content": {"summary": {"value": "This paper reveals the potential weakness of finetuning based defense methods against prompt injection attack. The authors proposed a method named Checkpoint-GCG which can utilize the intermediate checkpoints of safety finetuning to enhance the success of GCG method for attack. The authors show the proposed method achieves strong performance under different attack settings, indicating this method can serve as a good safety auditing method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The findings and method reported in this paper is very novel to my knowledge.\n\n2. The proposed method shows strong performance in attacking the finetuning based defense methods.\n\n3. Different kinds of attack settings are evaluated.\n\n4. The writing of this paper is good."}, "weaknesses": {"value": "1. It would be better if more prompt injection attack benchmarks are used in experiments.\n\n2. The proposed method is designed for attack finetuning based defense methods. It is not clear whether it still works when other kinds of defense methods are applied.\n\n3. The assumption of intermediate checkpoints may be too strong for some models."}, "questions": {"value": "Please refer to above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ALZ518RylV", "forum": "qkGhc3l5py", "replyto": "qkGhc3l5py", "signatures": ["ICLR.cc/2026/Conference/Submission20899/Reviewer_zFV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20899/Reviewer_zFV2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547859776, "cdate": 1761547859776, "tmdate": 1762999982268, "mdate": 1762999982268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Checkpoint-GCG, an extension of the original GCG attack designed to audit and attack fine-tuning based defenses against prompt injection (e.g., StruQ, SecAlign, SecAlign++). The key insight is that intermediate fine-tuning checkpoints can serve as stepping stones to gradually optimize adversarial suffixes that remain effective even as defenses strengthen. The core idea is to leverage intermediate fine-tuning checkpoints as stepping stones that guide the optimization of adversarial suffixes, enabling the attack to remain effective as the model becomes more robust. The authors demonstrate that Checkpoint-GCG substantially outperforms standard GCG, achieving much higher attack success rates. Moreover, the optimized adversarial suffixes can be generalized to unseen prompts and can transfer to related models within the same family."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of leveraging intermediate fine-tuning checkpoints as initialization stages for GCG is largely novel and well-motivated, bridging a gap between training dynamics and attack optimization.\n2. The experiments are sufficient, as the authors evaluate multiple model families (Llama-3-8B, Mistral-7B, Qwen2-1.5B), several defense mechanisms (StruQ, SecAlign, SecAlign++), and multiple threat settings (auditing, universal attack, model transferability).\n3. The performance of the attacks are impressive: for example, Checkpoint-GCG achieves 96% attack success rate against SecAlign, compared to only 6% for standard GCG."}, "weaknesses": {"value": "1. Effectiveness due to additional information: The strong performance of Checkpoint-GCG is not entirely surprising, as it benefits from substantially more information (specifically, access to intermediate fine-tuning checkpoints as part of a defense mechanism) than standard attacks, which gives it an inherent advantage. Throughout the paper, the authors primarily compare Checkpoint-GCG against the standard GCG baseline to emphasize its improvements; while this comparison is acceptable, it may inadvertently mislead readers who overlook the fact that the proposed method operates under a significantly stronger information assumption. For example, the authors limit the number of GCG steps to ensure a “fair” comparison, but this fairness is not very meaningful since Checkpoint-GCG already benefits from access to more information.\n2. The threat model is unrealistic: The assumed access to intermediate checkpoints and full input context is unrealistic in practical deployment settings. Although the authors acknowledge this limitation and attempt to relax these assumptions, the empirical results show that the attacks fail to transfer effectively when models come from different families, limiting real-world applicability. In line 431, the authors also claim that “it is still realistic in practice, as organizations may open-source a model or defense before deploying an update behind an API”. However, this justification remains speculative and does not reflect most real-world deployment scenarios, where fine-tuning checkpoints from defensive retraining are rarely made publicly available, yet Checkpoint-GCG critically depends on access to such checkpoints from a sufficiently similar surrogate model.\n3. Limited theoretical insight: While the method demonstrates strong empirical results, it lacks a formal analysis or theoretical explanation for why progressing through fine-tuning checkpoints leads to better optimization outcomes. Including such insights, even at a conceptual level, could significantly strengthen the paper’s contribution beyond empirical findings."}, "questions": {"value": "1. Since this work demonstrates the effectiveness of Checkpoint-GCG, what would be some effective defense mechanisms against it? The authors could consider adding some relevant discussions to the paper.\n2. The authors evaluate four different strategies for selecting checkpoints, but the total number of checkpoints used in the main experiments remains relatively large (around 100). Could the number of checkpoints be further reduced to improve computational efficiency? It would also be helpful to analyze how sensitive the attack performance is to this trade-off between the number of checkpoints and overall effectiveness.\n3. When computing success@k, how are the k independent attack attempts generated or launched?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YhWx9lPGlo", "forum": "qkGhc3l5py", "replyto": "qkGhc3l5py", "signatures": ["ICLR.cc/2026/Conference/Submission20899/Reviewer_2iXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20899/Reviewer_2iXw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781280673, "cdate": 1761781280673, "tmdate": 1762999981414, "mdate": 1762999981414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Checkpoint-GCG, a method that leverages intermediate checkpoints of a fine-tuned model to enhance the effectiveness of GCG, a white-box adversarial attack. The method shows substantial improvements over vanilla GCG and is posed as an auditing tool for prompt injection and jailbreaking defenses. Further, the adversarial strings found by the method are transferable to other models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The method serves as a useful tool for improving model auditing, since intermediate checkpoints will be available to model providers. \n2. The evaluation considers strong baselines for comparison against state-of-the-art defenses and shows that the attack is highly effective and transferable."}, "weaknesses": {"value": "A nearly identical approach for jailbreaking has already been published at ICLR 2025. Wang et al. [1] introduced a staged jailbreaking technique that converts a challenging optimization problem (i.e., jailbreaking an aligned model with GCG) into a sequence of easy-to-hard problems, where the solution of each prior problem is used to warm-start the optimization of the next problem. Here, each problem is a model checkpoint, obtained by deliberately misaligning the model, making it easier to attack. While the checkpoint selection schemes, method for obtaining them and optimization targets are different, the core methodology is the same.\n___\n### References\n[1] Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks; Wang et al. ICLR 2025. Link: https://arxiv.org/abs/2410.04234"}, "questions": {"value": "1. For the completeness of evaluation, how does Checkpoint-GCG perform with a total budget of 500 steps? Is there a threshold beyond which it becomes more viable than vanilla GCG?\n2. What does the distribution of Checkpoint-GCG budgets look like for successful attacks? For attacks where vanilla GCG is successful, how does Checkpoint-GCG fare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D0yo6d9zBR", "forum": "qkGhc3l5py", "replyto": "qkGhc3l5py", "signatures": ["ICLR.cc/2026/Conference/Submission20899/Reviewer_62dP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20899/Reviewer_62dP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786956147, "cdate": 1761786956147, "tmdate": 1762999982286, "mdate": 1762999982286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Checkpoint-GCG, a white-box adversarial attack method designed to audit and bypass fine-tuning-based prompt injection defenses for large language models (LLMs). Prompt injection attacks exploit LLMs’ inability to distinguish trusted instructions from malicious content in untrusted data, while existing fine-tuning defenses aim to mitigate this by training models. Meanwhile, traditional attacks like Greedy Coordinate Gradient (GCG) suffer from drastically reduced attack success rates (ASR) against these defenses. To address this, Checkpoint-GCG leverages intermediate fine-tuning checkpoints as \"stepping stones\": it sequentially optimizes adversarial suffixes across checkpoints, using the suffix from the previous checkpoint to initialize the next, thereby improving attack effectiveness. The experiment results show that Checkpoint-GCG successfully improves the attack performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is easy to read and well-written.\n\n\n- Checkpoint-GCG addresses a critical gap in LLM security: the failure of traditional attacks to evaluate the robustness of state-of-the-art fine-tuning defenses. By exploiting the parameter updates in fine-tuning checkpoints, it provides a principled solution to GCG’s initialization sensitivity."}, "weaknesses": {"value": "- The auditing setting (full access to model checkpoints and the exact input) is realistic for internal red-teaming but not for many real-world attackers. The authors do relax these assumptions, but the highest ASRs require checkpoint access. The practical feasibility of obtaining intermediate checkpoints for deployed proprietary models is limited. \n\n- Checkpoint-GCG runs GCG many times across selected checkpoints; while GRAD reduces cost, the totals in reported experiments are nontrivial (per-sample totals reported in Table 4). The method uses large per-checkpoint budgets (authors set T up to 1000 with early stopping), which may be computationally expensive in practice for large models.\n\n- The paper lacks systematic interpretability experiments or mechanistic explanations for why Checkpoint-GCG achieves superior attack performance."}, "questions": {"value": "Listed in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ofPkNXbwzt", "forum": "qkGhc3l5py", "replyto": "qkGhc3l5py", "signatures": ["ICLR.cc/2026/Conference/Submission20899/Reviewer_HERg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20899/Reviewer_HERg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915285449, "cdate": 1761915285449, "tmdate": 1762999981427, "mdate": 1762999981427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}