{"id": "oJoLQlnNiB", "number": 8348, "cdate": 1758079418008, "mdate": 1759897790183, "content": {"title": "ALERT: Adaptive Learning with Embeddings and Reinforcement for Transparent Few-Shot Learning", "abstract": "In this paper, we introduce ALERT, a novel approach to few-shot learning that significantly enhances both the interpretability and accuracy of large vision-language models (LVLMs) in classification tasks with limited data. By utilizing the strengths of LVLMs and integrating a meta-task instruction framework, ALERT effectively transforms the traditional black-box nature of few-shot models into a transparent process. It allows for traceable and understandable reasoning. ALERT employs learnable category embeddings to emphasize unique features of each category, improving classification accuracy, and introduces a contrastive reward function within a Group Relative Policy Optimization (GRPO) training framework to enhance reasoning capabilities and training stability. Our extensive experiments across various datasets demonstrate that ALERT consistently outperforms existing few-shot learning methods, achieving state-of-the-art results. Notably, in the 16-shot setting on ImageNet, ALERT achieved an impressive accuracy of 78.74\\%, significantly improving on previous methods.", "tldr": "This paper introduces ALERT, a novel approach to few-shot learning that enhances interpretability and accuracy in large vision-language models by using reinforcement and adaptive learning with embeddings.", "keywords": ["few-shot learning", "interpretability", "large vision-language models", "meta-task instruction framework", "learnable category embeddings"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d16b52ff50e246b08188240131db51b24ef101f.pdf", "supplementary_material": "/attachment/05b8a25550f4b607ce914ebd7dff9c7e1bbc662b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ALERT, a novel framework for transparent few-shot learning that enhances both the accuracy and interpretability of Large Vision-Language Models. The method combines three key innovations: Learnable Category Embeddings: category-specific embeddings that capture discriminative visual features; Meta-Task Instruction Framework: reformulates classification as an instruction-following task where the model generates explicit reasoning enclosed in special tags; Group Relative Policy Optimization with a Contrastive Reasoning Reward – reinforcement learning that encourages logically consistent and explainable reasoning. Experiments on 11 public few-shot datasets show that ALERT consistently surpasses existing approaches such as CoOp, Tip-Adapter-F, and CaFo."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Integrating reinforcement learning and LVLM instruction tuning for few-shot learning is a fresh and creative combination. The authors also conduct extensive evaluation across 11 datasets and various shot settings."}, "weaknesses": {"value": "1. For previous methods, e.g., Zero-Shot CLIP, CoOp, they compute the cosine similarity between image and text embeddings, which may not be viewed as \"black-box\" methods. As for the proposed method, it uses a modern LVLM compared to previous methods. The authors' claim that by utilizing GRPO to encourage LVLM to output reasoning chains for better performances, as the solution to the \"black-box\" problem, is not fair.\n2. From Table 1 and Figure 4, it seems the proposed method encounters a bottleneck as the number of shots grows. However, other methods, e.g., Tip-Adapter-F, CaFo, show steady improvements.\n3. The most suitable baseline method for the proposed method, as far as I consider, should be zero-shot Gemma-4B. However, the authors do not provide results on this setting.\n4. The visualizations, e.g., reward curves, about the overall RL training process are missing."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oP2KWvuRiw", "forum": "oJoLQlnNiB", "replyto": "oJoLQlnNiB", "signatures": ["ICLR.cc/2026/Conference/Submission8348/Reviewer_BQB9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8348/Reviewer_BQB9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760582014592, "cdate": 1760582014592, "tmdate": 1762920265643, "mdate": 1762920265643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ALERT, a few-shot learning framework combining learnable category embeddings, meta-task instruction tuning, and GRPO reinforcement learning. It aims to improve accuracy and interpretability in vision-language models. The paper’s contribution lies more in smart engineering synthesis and empirical validation than in foundational novelty, which isn't a weakness. Overall, it is a solid and well-written paper that provides a practical advance in interpretable few-shot learning"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Good integration of LVLMs and reinforcement learning. Integrating instruction-tuned LVLMs with reinforcement learning-based reasoning is a fresh angle for few-shot learning.\n\nS2. Strong empirical results across multiple datasets.\n\nS3. Clear method and ablation studies."}, "weaknesses": {"value": "W1. Relies on proprietary Gemma 3 model; reproducibility is limited, and limited clarity on computational cost, training time, or efficiency trade-offs versus simpler baselines.\n\nW2. Interpretability claims are mostly qualitative. The “transparency” claim is mostly qualitative (no quantitative measure of interpretability).\n\nW3. If the method scales to other domains, it could have a high impact."}, "questions": {"value": "Q1. Can you provide quantitative interpretability metrics (e.g., fidelity, agreement with ground-truth rationale)?\n\nQ2. Can you discuss training efficiency and scaling cost explicitly?\n\nQ3. Would you consider using a smaller open model (e.g., LLaVA-based) for reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UVHhsDBTH7", "forum": "oJoLQlnNiB", "replyto": "oJoLQlnNiB", "signatures": ["ICLR.cc/2026/Conference/Submission8348/Reviewer_BZU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8348/Reviewer_BZU9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691615731, "cdate": 1760691615731, "tmdate": 1762920265326, "mdate": 1762920265326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ALERT (Adaptive Learning with Embeddings and Reinforcement), a novel approach for few-shot learning. It aims to address the \"black box\" nature of current models by significantly enhancing both the accuracy and interpretability of Large Vision-Language Models (LVLMs). The ALERT method makes the model's classification process transparent and its reasoning traceable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary strength of ALERT is that it significantly enhances both the classification accuracy and the interpretability of Large Vision-Language Models (LVLMs) in few-shot tasks.\n2. The model transforms the traditional \"black-box\" nature of few-shot learning into a transparent process. It allows for reasoning that is traceable and understandable, providing more detailed analysis and identifying subtle similarities and differences to make correct classifications.\n3. ALERT consistently outperforms existing few-shot learning methods across various datasets, achieving state-of-the-art results."}, "weaknesses": {"value": "1. High Computational Cost and Complexity. The ALERT pipeline is very complex and computationally intensive: Stage 1: Requires an additional lightweight FSL model (like CaFo 1) for category candidate selection. Stage 2: Relies on a large LVLM (Gemma 3 4B). GRPO training itself is very time-consuming, requiring the generation of multiple (6 in this paper) outputs for each training sample. Most critically, the paper's proposed \"contrastive reasoning reward function\" needs to call another LLM to act as a \"judge\" to assign a score. This means that in every step of RL training, an additional LLM inference is required, which will lead to astonishingly high training costs. The authors should more fully discuss this trade-off between accuracy/interpretability and computational cost in the paper. \n2. The success of the entire pipeline depends on the performance of the lightweight model in Stage 1. If the true category is not included in the top-k candidates (in the experiments, $k=2$ performed best), the LVLM will be unable to make the correct classification. This paper does not fully discuss this potential \"error accumulation\" effect. If the initial model's recall is insufficient, ALERT's performance ceiling will be severely limited.\n3. About the performance on EuroSAT: The authors candidly note that ALERT underperforms compared to CaFo on the EuroSAT dataset. While this is a minor issue, it is worth further discussion. Why does this complex model, which excels on 10 other datasets, fail at satellite image classification? Does this imply that the world knowledge and reasoning capabilities brought by the LVLM are not applicable or are even misleading in this domain (remote sensing)?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RKDg8OIiWR", "forum": "oJoLQlnNiB", "replyto": "oJoLQlnNiB", "signatures": ["ICLR.cc/2026/Conference/Submission8348/Reviewer_dGPm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8348/Reviewer_dGPm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912253300, "cdate": 1761912253300, "tmdate": 1762920264585, "mdate": 1762920264585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ALERT, a new approach to few-shot learning that enhances both accuracy and interpretability in large vision–language models (LVLMs). ALERT introduces a meta-task instruction framework that makes the model’s reasoning more transparent, addressing the typical “black-box” limitations of LVLMs. It further boosts performance through learnable category embeddings and a contrastive reward objective trained with Group Relative Policy Optimization. Experiments across multiple datasets show that ALERT sets a new state of the art in few-shot classification, including 78.74% accuracy on ImageNet with only 16 shots, surpassing existing methods by a substantial margin."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The novelty of ALERT lies in combining interpretability and performance in few-shot learning for large vision–language models, leading to more explainable and accurate classification with limited data.\n\nALERT demonstrates\n\n•\tImproved interpretability – The meta-task instruction framework makes the model’s reasoning process transparent.\n\n•\tHigh performance – ALERT achieves state-of-the-art accuracy in few-shot classification, notably outperforming prior methods on benchmarks like ImageNet.\n\n•\tInnovative training design – The use of learnable category embeddings and a contrastive reward within the GRPO framework enhances both reasoning stability and feature discrimination."}, "weaknesses": {"value": "There are various shortcomings. If addressed the paper would be improved.\n\n•\tComplex training framework may increase implementation difficulty and computational cost. While the framework is described a figure outlining the pipeline would help the reader understand how the different components interact. Figure 2 does not do that.\n\n•\tLimited evaluation scope — there are many parameters and while the values used are specified, it is unclear how they were set. Additionally the evaluative scoring function hides where the different methods excel. It would be interesting to break this down to understand where the gains are made. For some of the datasets where the score is increasing at 16 shots, it would be interesting to see how it continues to increase for larger shot values. Is there a value for which the other methods surpass ALERT?\n\n•\tThe work makes a valuable contribution and builds effectively on current advances. However, including a discussion of remaining challenges and possible avenues for future research would strengthen the paper and highlight its long-term potential."}, "questions": {"value": "In the methodology, you say that you \"can employ any established few-shot learn- ing method for selecting potential category candidates.\" Is there evidence of this? Can you justify this claim?\n\nCan you please explain how each of the parameters were set in the experimental section and the effect of different parameter values?\n\nCan you show the breakdown of the scores from the scoring function for each of the methods to demonstrate exactly where the gains are made?\n\nCan you do some experiments that show up to 50 shots on datasets where the score is still increasing? It appears that ALERT plateaus early while others are still increasing.\n\nThe paper claims transparency and traceability. Can you please describe the meaning of traceability and discuss how ALERT enhances traceability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMYrNQCb9p", "forum": "oJoLQlnNiB", "replyto": "oJoLQlnNiB", "signatures": ["ICLR.cc/2026/Conference/Submission8348/Reviewer_4S3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8348/Reviewer_4S3J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941278182, "cdate": 1761941278182, "tmdate": 1762920263940, "mdate": 1762920263940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ALERT is a two-stage few-shot pipeline built on an LVLM. First, a candidate selector (e.g., CaFo) provides top-k candidates as the reference. Second, the LVLM receives an instruction-style prompt that concatenates text tokens, reference category embeddings, and the input image embeddings, then generates a chain-of-thought and a final label. Training uses GRPO-style reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper follows a good structure.\n2. Promising results are obtained in the studied benchmarks."}, "weaknesses": {"value": "1. The motivation is not clearly articulated.\n\n(1)The claim that *ALERT transforms few-shot learning from a black-box into a transparent process* is not convincing. The method still relies on an auto-regressive LVLM whose internal computations remain opaque; producing a chain-of-thought-style text does not, by itself, establish transparent and faithful process, as explanations can be plausible yet misleading[1]\n\n(2)Moreover, existing LVLMs already support few-shot learning with explicit reasoning traces via instruction tuning or in-context learning (e.g., Flamingo[2], GPT-4V[3], Qwen-VL[4], DeepSeek-VL[5]), so the claim that “ALERT addresses the weakness… to provide interpretability” in Line 114 is not well supported.\n\n\n2. Novelty and discussion in related work are limited\n\n(1) For learnable category embeddings, category prototypes or caches are well studied in CLIP-based few-shot adaptation (e.g., CLIP-Adapter, TIP-Adapter). I suggest authors discuss more about what is architecturally or algorithmically new beyond moving the idea into an LVLM context.\n\n(2) For the meta-task instruction framework, the term meta might be misleading, since ALERT formats inputs as instructions and decodes with a standard LVLM, which is closer to instruction tuning or in-context learning than to meta-learning.\n\n(3)For contrastive reward, using an LLM-as-judge to grade reasoning is widely used, and GRPO was introduced to forego the critic model in PPO[6]. Please acknowledge related work.\n\n\n3. The method description needs clarity.\n\n(1) Section 3.4 can be greatly simplified. This section essentially restates standard LVLM decoding: tokenize text; interleave text tokens with visual tokens; run a Transformer; project the last hidden state to vocabulary logits; sample next token. I suggest that authors reduce to a concise description and cite prior LVLMs (e.g., Flamingo) for the interleaved-token formulation\n\n(2) I also suggest the author provide a complete training algorithm pipeline, especially including input, output, and reward/loss to make the contribution easier to follow.\n\n4. Some implementation details are missing. \n\n(1) How are learned category embeddings parameterized and updated? \n\n(2) What are m and d for your backbone? \n\n(3) LoRA hyperparameters; \n\n(4) Inference decoding details\n\n5. The experiment demonstration is weak. \n\n(1) Baselines may be outdated and comparisons unfair. Table 1 mixes substantially different base systems. Results of CaFo are from 2023; meanwhile, strong recent LVLMs that natively support few-shot reasoning (e.g., Qwen-VL, DeepSeek-VL, GPT-4V) are absent. I suggest that authors add such baselines and compare with more recent SoTAs.\n\n(2) CaFo (2023) uses base models such as GPT-3, CLIP, DINO, and DALL-E. If one of the key motivations is to pursue higher accuracy, I suggest authors demonstrate how the original CaFo framework performs with current stronger base models and show whether ALERT still adds value beyond this simple modification.\n\n(3) Why is Gemma-3 (2–16-shot) “–” in Table 1? \n\n(4) ALERT relies on reference categories (top-k from CaFo). It might be unfair to compare with baselines that do not use candidate information or that use randomly selected candidates.\n\n(5) I am also concerned about how ALERT performs when the ground-truth is not in top-k provided by CaFo\n\n(6) The author should report and compare the number of trainable parameters for different baselines and provide training/inference throughput, GPUs, wall-clock time compared to baselines, especially because GRPO + LLM-as-judge can be costly and an additional LVLM is introduced.\n\n(7) In Fig. 3, ALERT appears to saturate after 4-shot while baselines continue to improve. The authors did not analyze this.\n\n(8) In Fig. 4, the ALERT fails to surpass CaFo at larger shots on EuroSAT. Can the authors provide more analysis?\n\n(9) More ablation studies are needed: per-class trainable embeddings v.s. shared projector on category embedding; reward weights and specific removals; number of generations per training sample; and max-length ablations.\n\n\n6. Please ensure your tables follow ICLR’s style: “The table number and title always appear **before** the table.”\n\n[1] Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. In NIPS, 2023\n\n[2] Alayrac, Jean-Baptiste, et al. \"Flamingo: a visual language model for few-shot learning.\" *Advances in neural information processing systems* 35 (2022): 23716-23736.\n\n[3] Yang, Zhengyuan, et al. \"The dawn of lmms: Preliminary explorations with gpt-4v (ision).\" *arXiv preprint arXiv:2309.17421* (2023).\n\n[4] Wang, Peng, et al. \"Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.\" *arXiv preprint arXiv:2409.12191* (2024).\n\n[5]Lu, Haoyu, et al. \"Deepseek-vl: towards real-world vision-language understanding.\" *arXiv preprint arXiv:2403.05525* (2024).\n\n[6] Shao, Zhihong, et al. \"Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\" *arXiv preprint arXiv:2402.03300* (2024)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aCfti77bb4", "forum": "oJoLQlnNiB", "replyto": "oJoLQlnNiB", "signatures": ["ICLR.cc/2026/Conference/Submission8348/Reviewer_qvVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8348/Reviewer_qvVa"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942136369, "cdate": 1761942136369, "tmdate": 1762920263408, "mdate": 1762920263408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}