{"id": "ZrsOnkVvM3", "number": 22158, "cdate": 1758326913214, "mdate": 1759896883140, "content": {"title": "Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games", "abstract": "Reverse Kullback–Leibler (KL) divergence-based regularization with respect to a fixed reference policy is widely used in modern reinforcement learning to preserve the desired traits of the reference policy and sometimes to promote exploration (using uniform reference policy, known as entropy regularization). Beyond serving as a mere anchor, the reference policy can also be interpreted as encoding prior knowledge about good actions in the environment. In the context of alignment, recent game-theoretic approaches have leveraged KL regularization with pretrained language models as reference policies, achieving notable empirical success in self-play–based methods. Despite these advances, the theoretical benefits of KL regularization in game-theoretic settings remain poorly understood. In this work, we develop and analyze algorithms that provably achieve improved sample efficiency under KL regularization. We study both two-player zero-sum Matrix games and Markov games:\nfor Matrix games, we propose $\\texttt{OMG}$, an algorithm based on best response sampling with optimistic bonuses, and extend this idea to Markov games through the algorithm $\\texttt{SOMG}$, which also uses best response sampling and a novel concept of superoptimistic bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales inversely with the KL regularization strength $\\beta$ \nin addition to the standard $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret independent of $\\beta$ which is attained in both regularized and unregularized settings.", "tldr": "We design learning algorithms that achieve provably superior sample efficiency in game-theoretic settings when equipped with KL regularization compared to the unregularized case", "keywords": ["Matrix Games", "Markov Games", "KL Regularization", "Logarithmic Regret"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a0c446e2d08066d65b00a7047b21cfef0ffe261.pdf", "supplementary_material": "/attachment/d9102551c0f07095c110d391ba16786852dd8162.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studies two-player zero-sum matrix and Markov games with KL-regularized objectives. Motivated by the empirical success of KL regularization with reference policies in self-play, the authors develop and analyze algorithms that achieve improved sample efficiency in regularized settings. They propose two algorithms, OMG (for matrix games) and SOMG (for Markov games) that combine best-response sampling with optimism (and the novel concept of superoptimism) to achieve logarithmic regret in $T$ in regularized settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**New theoretical knowledge in an important empirical setting.** The paper delivers logarithmic regret guarantees for KL-regularized two-player games. This provides some theoretical justification for the regularization techniques that have been empirically successful in self-play. This seems like a highly original, significant contribution in a highly relevant research direction.\n\n**Methodological Clarity.** The core innovations of the proposed algorithms are built on well-known intuitive principles from prior work. The presentation of the algorithms supports the analysis intuitively well. Assumptions are also clearly stated, and the discussion of technical limitations is much appreciated."}, "weaknesses": {"value": "**Dependence of the method on closed-form best responses.** The algorithms seem to rely heavily on the ability to compute closed-form best responses. While the short discussion on this topic is appreciated, the paper would benefit from a deeper discussion about this, since some of the self-play settings that originally motivated this analysis may not admit such closed-form best response computation.\n\n**Some toy experiments for validation.** Though the contributions are theoretical and significant as-is, it would strengthen the paper to include some experiments on toy domains. Even minimal experiments on synthetic games could provide the research community with further valuable intuition."}, "questions": {"value": "Please see the weaknesses section.\n\n**Typos / etc**\n\nline 178 \"bandit feedbacks\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RCtzF32RiM", "forum": "ZrsOnkVvM3", "replyto": "ZrsOnkVvM3", "signatures": ["ICLR.cc/2026/Conference/Submission22158/Reviewer_zD5N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22158/Reviewer_zD5N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761321509295, "cdate": 1761321509295, "tmdate": 1762942093552, "mdate": 1762942093552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies KL-regularized reinforcement learning in two-player zero-sum games, which extends single-agent results on logarithmic regret under KL regularization to the multi-agent setting. The authors proposed OMG and SOMG for matrix games and Markov games, respectively. Both algorithms have convergence guarantees for regularized and unregularized settings."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work is well formulated and seems to be theoretically sound. The authors also clearly described the technical challenges for two-player zero-sum games compared to single agent. The techniques for SOMG with super-optimism are also interesting.\n\nThe work is clearly written and generally easy to follow.\n\nBoth OMG and SOMG are well-described and structurally aligned with existing optimization techniques."}, "weaknesses": {"value": "The entire framework assumes access to a fixed reference policy that is well-defined. However, in practice,  such reference policies are rarely available. The reviewer questions the necessity and practicality of the problem formulation.\n\nIn addition, in zero-sum games, a major point of discussion is the existence of multiple NEs. This is especially relevant to this paper since the existence of the reference policy should greatly impact the learning dynamics.\n\nThe theoretical results rely on linear function approximation assumptions for the Markov setting. While the assumption is standard, it is still a major limiting factor as this assumption greatly limits the generalizability of the proposed SOMG algorithm.\n\nThe submission is purely theoretical. Given the claim of logarithmic regret in multi-agent RL, even small-scale empirical validation would significantly strengthen credibility and illustrate whether superoptimism stabilizes learning in practice.\n\nThe tightness of technical analysis is questionable. Although the authors have emphasized the tightness of the bound with respect to T, the dependence on other dimentionalities seems suboptimal, with $d^3H^7$."}, "questions": {"value": "The authors mentioned LLM alignment as a motivation for this work. However, to the reviewer's knowledge, KL regularization is often applied in LLM alignment as a proxy objective to approximate the trust-region policy update, where the reference policy update is the previous older policy instead of an oracle. Are there any other strong connections between LLM alignment and the current manuscript?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LjbCErbMXr", "forum": "ZrsOnkVvM3", "replyto": "ZrsOnkVvM3", "signatures": ["ICLR.cc/2026/Conference/Submission22158/Reviewer_vUkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22158/Reviewer_vUkt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648167202, "cdate": 1761648167202, "tmdate": 1762942093304, "mdate": 1762942093304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers solving two-player zero-sum regularized matrix and finite-horizon Markov games under bandit feedback. It is shown that logarithmic regret in the number T of episodes can be achieved. This result is achieved by proposing two algorithms based on optimistic bonuses in the matrix game setting and superoptimistic ones in the Markov game setting. The paper shows improved sample complexity using best response sampling in both settings with explicit dependence on the feature dimension, the horizon length and the number of episodes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-organized, notation and exposition are precise. \n2. The paper improves existing regret bounds for regularized matrix and Markov games to logarithmic for the bandit bandit feedback setting. \n3. The bandit feedback setting is much less studied compared to the full information feedback setting."}, "weaknesses": {"value": "**Technical novelty.** I leave it to the experts on sample complexity analysis using optimistic bonus techniques to evaluate the technical novelty (the use of superoptimism and best response sampling in particular). I did not check the proofs.\n\nBelow are some points that I believe deserve a discussion and can improve the paper: \n\n1. **Regularized vs unregularized games.** I find the comparison with the unregularized setting a little confusing. For instance in O’Donoghue et al. 2021, there is no regularization even in the objective (i.e. the initial definition of the game) and hence not in the regret definition. Same for Chen et al. 2022 which is concerned with the unregularized game setting. In this paper, the regret is defined for the regularized game. So even if the bounds match as shown in Table 1, they seem to be for different regret definitions (one is for a regularized game in the paper under submission, and one is not). I think it would be useful to clarify this. In particular, it is not clear if the bound of this paper actually translates back to the same bound for the (unregularized) regret of the unregularized game, it might actually be worse. It would be useful to comment on this. \nIt seems that the only paper in the table considering the same regularized game setting is Yang et al. 2025a which the current submission seems to be a follow up of. \nIn general, I think it is worth highlighting that solving the regularized game does likely lead to a worse guarantee when trying to relate back to the original problem (by driving the regularization to zero when possible). \n\n2. **Comparison to lower bounds.** It seems that there are regret lower bounds in the literature of order $\\sqrt{T}$ (e.g. Theorem 5.5 in Chen et al. 2022),  and Nash-UCRL is nearly minimax optimal. I guess this does not contradict the contributions in this paper which considers a regularized game setting. I still believe that a discussion would be useful here, in link with the comment above. \n\n3. **Motivation.** If the goal of the paper is not to make any attempt to establish any guarantee for the unregularized game, then I think the comparison to a different regret measure for a different game setting should be avoided, or more nuanced to avoid any confusion. While KL regularization as an algorithmic tool has been used and proven to be useful, I think the paper could definitely elaborate more on why it is important to consider solving regularized matrix games in contrast to solving standard unregularized unconstrained games which is a different problem. There \n\n4. **Presentation.** There is a lot of space that can be gained to be devoted to a more detailed discussion of the analysis and technical novelty which are the core contributions in this work,  including intuitions behind using superoptimism and best response sampling (which are presented as key ingredients) and ideally proof sketches for achieving logarithmic regret. The paper does provide a brief discussion in the end of the paper, but I believe it is very minimal and results would benefit from being accompanied by a more detailed discussion of the techniques. Equations (14) b,c, (15) b,c can be compacted without losing clarity (using joined +- superscripts or a variable in the set $\\{+, -, other symbol\\}$). The algorithm is taking up an almost complete page. Same for (17) a, b which can be put in the same line with BR instead of Best Response. Same in lines 407-415 using multiple super-indices. \n\n5. **Related work.** For the matrix game setting, it seems that there are some polylog T bounds in 2 \\times 2 games under bandit feedback but even in the unregularized game setting ($\\beta = 0$). I think it is worth discussing this. \n\nA. Maiti, K. Jamieson, L. J. Ratliff. On the Limitations and Possibilities of Nash Regret Minimization in Zero-Sum Matrix Games under Noisy Feedback \nhttps://arxiv.org/pdf/2306.13233 \n\n**Minor comments/Typos:** \n\n-  Eq (3): missing $A$ with $f$. \n- l. 254: the $\\forall$. \n- l. 305: ‘use a the same projection’. \n- l. 407-415: definition of $\\phi_h^{\\tau}$?\n- Algorithm 1 is said to be model-based but Algorithm 2 model-free. Any clear reason for this inconsistency? I guess you use the model-based terminology to refer to the fact that you estimate the unknown payoff matrix but then what about the Markov setting (same for the reward function?)?"}, "questions": {"value": "1. What is the dependence of the bounds on the KL divergence between the reference measures and the Nash of the regularized game? As the motivation to consider them is briefly highlighted in the paper, it would be interesting to see if these can help in any way by incorporating prior knowledge. Is it possible to isolate the dependence from the current proofs? The conclusion briefly mentions some future work in this direction. \n\n2. The bound in the regularization setting (l. 210) suggests to pick the regularization parameter $\\beta$ as large as possible but regret itself depends on $\\beta$ (as defined from the regularized game). Can you comment more on this? Can we recover a bound on the regret for the unregularized game? \n\n3. l. 213: Is the regularization independent guarantee directly obtained from the regularization dependent result? Same for Markov games? \n\n4. Are previous works also assuming oracle access to best response and Nash equilibrium computation (for regularized games) as used in Algorithms 1 and 2? I understand that the paper focuses on sample complexity. Still I think it is useful to mention if the computational oracle requirements are the same compared to prior work (if there are any exactly comparable). \n\n5. Regarding the reduction to the single agent case (l. 443), how do you compare the results to the best known bounds in the same setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hNqWKgJ0mQ", "forum": "ZrsOnkVvM3", "replyto": "ZrsOnkVvM3", "signatures": ["ICLR.cc/2026/Conference/Submission22158/Reviewer_JTWU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22158/Reviewer_JTWU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831759105, "cdate": 1761831759105, "tmdate": 1762942093080, "mdate": 1762942093080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies KL-regularized two-player zero-sum matrix games and finite-horizon Markov games with unknown payoffs and transitions under bandit feedback. It proposes two algorithms: OMG (Optimistic Matrix Game) for matrix games and SOMG (Super-Optimistic Markov Game) for Markov games. Both exploit the Gibbs-form best response induced by reverse-KL regularization to design best-response–sampling schemes with optimistic bonuses. Under linear function approximation and realizability, the authors prove that OMG and SOMG achieve two simultaneous regret guarantees. Thus for β>0 they obtain logarithmic-in-T regret in KL-regularized games, and for β=0 they recover near-optimal $\\mathcal{O}(\\sqrt{T})$ rates. They also discuss reductions to single-agent RL, where SOMG yields improved bounds, and explain how “superoptimism” is crucial to handle unbounded value functions due to positive KL terms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*  Establish logarithmic regret guarantees and sample complexities for learning an $\\epsilon$-NE with $\\mathcal{O})(1/\\epsilon)$ sample complexity.\n* Use optimistic bonus to design novel algorithms (OMG and SOMG) that achieve better sample complexity.\n* The writing is clear and easy to follow."}, "weaknesses": {"value": "* The algorithms assume exact solutions to the inner KL-regularized saddle problems. (Algorithm 1 and 2) However,  the real implementations are approximate and there is a mismatch between algorithm and theory.\n* Sampling strategy might collapse exploration on actions believed suboptimal, so the bonus design may not guarantee sufficient coverage in adversarial payoff landscapes.\n* There exists a unique NE in the zero-sum setting, but in Markov games regularized NE may still be non-unique layer-wise. The analysis assumes away selection issues."}, "questions": {"value": "* If the KL-regularized saddle problem is solved to $\\epsilon$-accuracy per iteration, how does this optimization error accumulate in the regret?\n* The $\\mathcal{O}(1/\\epsilon)$ sample complexity depends on the regularization term $1/\\beta$, it will be more interesting to study whether the $1/\\beta$ or $\\log ^2 T$ terms are information theoretically necessary in the proposed game setting.\n* The proofs for matrix and Markov games seem standard, can you claim the technique contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BBfSk1V1uE", "forum": "ZrsOnkVvM3", "replyto": "ZrsOnkVvM3", "signatures": ["ICLR.cc/2026/Conference/Submission22158/Reviewer_JqjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22158/Reviewer_JqjA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194905538, "cdate": 1762194905538, "tmdate": 1762942092824, "mdate": 1762942092824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}