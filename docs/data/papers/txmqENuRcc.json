{"id": "txmqENuRcc", "number": 24043, "cdate": 1758352096430, "mdate": 1759896784634, "content": {"title": "The Overthinking Predicament: When Reasoning Hurts Ranking", "abstract": "Document reranking is a key component in information retrieval (IR), aimed at refining initial retrieval results to improve ranking quality for downstream tasks. Recent studies—motivated by large reasoning models (LRMs)—have begun incorporating explicit chain-of-thought (CoT) reasoning into LLM-based rerankers. \nHowever, the effectiveness of such reasoning for ranking tasks remains underexplored.\nIn this work, we present the first systematic study of reasoning in reranking across both pointwise and listwise settings, under both supervised fine-tuning and reinforcement learning. Using diverse benchmarks, including reasoning-intensive datasets (BRIGHT) and standard IR benchmarks (BEIR), we find that reasoning-augmented rerankers consistently underperform their direct counterparts that predict rankings without CoT, despite substantially higher inference costs. Our analysis reveals three core limitations: (i) in pointwise rerankers, reasoning breaks calibration and biases models toward the positive class, raising TPR but lowering TNR, which inflates false positives and degrades ranking in negative-dominant pools; (ii) in listwise rerankers, reasoning improves in-domain fit but increases variance and fails to generalize out-of-domain, even when reinforcement learning shortens rationales; and (iii) overall, directly fine-tuned rerankers remain more stable, effective, and robust.  \nThese findings challenge the assumption that explicit reasoning is universally beneficial for reranking. We conclude by highlighting future directions, including calibration-aware scoring for pointwise rerankers and the design of concise, targeted reasoning strategies to mitigate overfitting and overthinking in listwise rerankers.", "tldr": "We conduct the first systematic study of explicit reasoning in document reranking, spanning pointwise and listwise paradigms, supervised and reinforcement learning, and diverse benchmarks including BRIGHT and BEIR.", "keywords": ["information retrieval", "reranking", "reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a87e5dbc6245e3d1ebc50618cfe7715969926fe1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study investigates the practical impact of explicit reasoning in document reranking tasks. Using a unified experimental design, the authors compare models with and without explicit reasoning across two mainstream reranking paradigms: pointwise and listwise. The experiments encompass supervised fine-tuning (SFT) and reinforcement learning-based GRPO training methods, employing Qwen3 series models evaluated on multiple benchmarks including BRIGHT and BEIR. The results demonstrate that rerankers incorporating explicit reasoning generally underperform compared to corresponding models that directly output results, and the paper provides corresponding analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experiments are comprehensive, comparing both pointwise and listwise modeling approaches as well as investigating different training methods including SFT and GRPO.\n\nEffectively leveraging the reasoning capabilities of large language models to improve complex downstream tasks such as ranking is a cutting-edge and important research direction.\n\nThe paper is well-organized with a clear logical structure."}, "weaknesses": {"value": "Questionable generalizability of conclusions:\n\nThe primary findings are based on Qwen3 series models (4B and 8B). However, many comparable baselines (e.g., Rank1, Rank-R1) were trained on the Qwen2.5 series. The authors do not verify whether the observed “harmful impact of reasoning” persists across other widely used base models such as Qwen2.5 or LLaMA, limiting the generality of the conclusions.\n\nAbsence of zero-shot baselines: The study lacks direct comparisons with zero-shot Qwen3 models. To substantiate claims made in Section 3.4 comparing Rank1 and Rank-R1 on BEIR, it is necessary to control for base model capability differences, for example by including Qwen3-based Rank-R1 results. Moreover, a zero-shot Qwen3 baseline on BRIGHT would clearly illustrate the relative improvements brought by different training methods (direct vs. reasoning), thereby strengthening the conclusions.\n\nResult volatility: Performance differences between 4B and 8B models, as well as between models with and without explicit CoT reasoning in the listwise experiments, are minimal. Further analysis is needed to exclude the possibility that these small gaps stem from normal fluctuations in model size or training randomness.\n\nLack of statistical significance testing: Given the small performance gaps, it is difficult to determine whether observed differences are genuine or due to experimental variability. The absence of significance testing reduces confidence in the results.\n\nInsufficient support for the use of the Expected Calibration Error (ECE) metric: The paper employs ECE to argue that reasoning degrades model calibration, which is cited as a factor contributing to ranking performance decline. However, no theoretical or empirical evidence is provided to support the core assumption that ECE negatively correlates with ranking quality metrics such as NDCG. Relevant literature citations or supplementary experiments are necessary. Additionally, reporting zero-shot Qwen3 ECE values as a baseline would provide a more complete understanding of calibration changes.\n\nLimitations related to training methodology: The pointwise models are fine-tuned using the parameter-efficient LoRA technique, which may affect model behavior differently than full fine-tuning. To enhance the generality of the findings, experiments with full fine-tuning are recommended.\n\nOther remarks:\n\nThe assertion in Section 3.4 that “explicit reasoning is not a prerequisite for achieving state-of-the-art performance” may be overly absolute. Current results suggest reasoning may underperform under specific training paradigms, but this likely reflects the lack of effective methods to exploit reasoning abilities rather than definitively ruling out the utility of explicit reasoning in ranking tasks. Ranking performance is influenced by multiple interacting factors, so this conclusion risks overgeneralization.\n\nNumerical inconsistencies between Figure 2 and Section 4.1 warrant further verification and alignment."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8OYXJYqkxs", "forum": "txmqENuRcc", "replyto": "txmqENuRcc", "signatures": ["ICLR.cc/2026/Conference/Submission24043/Reviewer_sv2D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24043/Reviewer_sv2D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911684476, "cdate": 1761911684476, "tmdate": 1762942910707, "mdate": 1762942910707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at reasoning reranking models compared to non-reasoning reranking models trained with the same data. They show that reasoning reranking models do worse than direct models on various tasks. They show that this is because of worse calibration among reasoning models and a lack of out of domain generalization. \n\nOverall, although I think there may be other analyses that need to be done to fully convince me (and even if this holds, in general I am skeptical that reasoning can't improve reranking) this paper proposes an interesting negative result with the current methods available.  As with most analysis/negative results, a reviewer could always ask for more experiments: I think this is well-done and future work can continue the discussion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The analysis is well chosen and timely and likely to be relevant to many in the retrieval-adjacent space\n- Many models at various sizes are shown as well as well-thought out experiments and ablations\n- The paper explores both listwise and pointwise models, as well as RL and SFT"}, "weaknesses": {"value": "1. There are some areas where the experimental details could be more clear, e.g. were there different prompts for the various datasets? Did the reasoning models overfit on the data or have large differences during intermediate steps? \n2. The paper claims the models do worse out of domain but I also think the results show they do worse in-domain no? So I'm not sure it's a \"they generalize worse\" problem but more of a \"they are worse all over\". This is relatively minor but I think the abstract/intro was leading me to believe it was a generalization problem. \n3. Minor: reading through it left me with some questions (see that section) about why this could be: however, I don't think they need to be done necessarily to improve the paper."}, "questions": {"value": "I think the paper is well written but in general am skeptical. However, that is not a reason to reject in my opinion, as that is good for science to have disagreement.\n\nIf it's helpful for the authors, my pushback would be that I would suspect some differences from these areas:\n\nA: The Rank1 results presented here are quite different than those in their paper (even for the GPT-4o experiments). That leads me to believe there is some difference in implementation which could affect the other results. Are the prompts used for each dataset the same? I think previous work gave custom prompts for unique datasets like BRIGHT.\n\nB: Given the wide range in performance (even in-domain as shown by Figure 3), I would guess that the exact prompt and/or model used (and perhaps even random seed in training) would cause a large difference.  Whereas for direct prompted model, I would suspect they don't fluctuate as much with other prompts (and may be too adapted to the one used in training). I would be curious to see the intermediate checkpoints results or another version trained with a different seed to see how varied performance is? Perhaps the higher variance leads to worse average-case performance but if carefully chosen, can do better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4yUQY2LLgW", "forum": "txmqENuRcc", "replyto": "txmqENuRcc", "signatures": ["ICLR.cc/2026/Conference/Submission24043/Reviewer_wkzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24043/Reviewer_wkzz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939739861, "cdate": 1761939739861, "tmdate": 1762942910491, "mdate": 1762942910491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a comprehensive study on the effect of using reasoning during reranking for information retrieval. This paper compares direct rerankers with their reasoning-augmented counterparts across pointwise and listwise reranking paradigms, and finds that using reasoning often causes performance degradation in almost all scenarios. In addition, the paper analyzes the effect of using reasoning in terms of calibration, bias to positive class, and potential overfitting in domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a controversial point of view that reasoning can hurt ranking and offers a comprehensive analysis of the effect of incorporating reasoning into the reranking stage. \n2. The insights, such as reasoning affecting calibration and the tendency to predict positive class, are useful for the community. \n3. The experiments are comprehensive across standard and reasoning-intensive retrieval benchmarks, including BEIR and BRIGHT.\n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. One issue from my point of view is the implicit assumption that the model can reason properly. Despite testing on multiple datasets, the paper neither checks the quality of reasoning via reasoning-related tasks nor performs any qualitative study to understand the reasoning traces. If the reasoning models were poorly trained and the reasoning traces are bad, it will definitely degrade the performance, and the blame should not be attributed to the use of \"reasoning\". \n2. Another issue is that LLMs are typically trained to output better tokens rather than probabilities. However, the scores from the reranker used here are based on **logits** instead of directly outputting scores as tokens. Many new LLM-based rerankers now directly extract the scores (in the form of tokens) from the output, and they can perform well [1]. So it might also be useful to examine whether reasoning works for those types of rerankers.\n3. Though the paper shows the limitations of reasoning, it does not provide enough insight into how to leverage these results for future directions. Should researchers/practitioners still pay more attention to reasoning for reranking? If so, which directions are more promising? \n\n[1] Shao, Rulin, et al. \"ReasonIR: Training Retrievers for Reasoning Tasks.\" *arXiv preprint arXiv:2504.20595* (2025)."}, "questions": {"value": "1. Typos: in line 208, GRPO should be Group-relative Policy Optimization. \n2. Strong claims: In line 257, the authors said, “Reasoning is unnecessary for reranking”. However, it might be premature to conclude this. These supporting results can at most say the existing reasoning-based reranking approaches underperform, rather than saying reasoning is not useful. \n3. Why is GRPO only used for Listwise rerankers but not pointwise rerankers?\n4. The experiments are only done for Qwen3 models, which natively support thinking/non-thinking modes. Can the same experiments be done for other models without these modes? Does including \"think step by step\" in the system prompt qualify as thinking/reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9AYqeRmPd", "forum": "txmqENuRcc", "replyto": "txmqENuRcc", "signatures": ["ICLR.cc/2026/Conference/Submission24043/Reviewer_uyHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24043/Reviewer_uyHL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985404660, "cdate": 1761985404660, "tmdate": 1762942909625, "mdate": 1762942909625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper investigate whether reasoning is helpful for point-wise and list-wise rerankers. Their experiments show that no reasoning methods outperform reasoning methods. They also conduct additional analysis in which they determine that reasoning methods are biased toward the positive class."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper are:\n- The authors evaluate a comprehensive suite of models across various settings (pointwise/listwise and sft/GRPO).\n-"}, "weaknesses": {"value": "The weaknesses of the paper ares\"\n- The authors claim that \"reasoning-based rerankers underperform their direct-output counterparts, even though they incur substantially higher inference costs.\" While this seems to be true for current methods and datasets, it's possible their are other training methods or more challenging datasets that could benefit from reasoning.\n- All of the training is done with traces for MS Marco samples. MS Marco has questions that are much simpler than questions in datasets like Bright. Using traces obtained with more challenging questions might lead to improvements with reasoning. \n- In table 2, it look like  ReasonRank-7B is the best performing model. Reasoning helped for this dataset? \n- Some of the writing is misleading. Here are a couple examples:\n  - In the abstract, the authors state \"reasoning improves in-domain fit but increases variance and fails to generalize out-of-domain.\" The \"in-domain fit\" refers to performance on training samples and not on samples in an in-domain test set. This should be clarified. \n  - \"Our analysis reveals that for pointwise rerankers, reasoning improves relevance prediction at the cost of disrupting score calibration and creating a bias toward false positives, ultimately degrading ranking performance.\" Relevance prediction doesn't improves just because the true positive rate increases. If you predicted the positive class for every sample, then TPR will be 1 but this is not improvement in relevance prediction."}, "questions": {"value": "Here are some questions I have:\n- Line 274 says \"both outperforming reasoning-based baselines such as Rank-R1-14B (25.9) and ReasonRank-7B (25.2),\" but these are not the numbers I see in table 1 or 2 for Rank-R1-14B and  ReasonRank-7B.\n- How much data was used for SFT and how much was used for GRPO?\n- Does the miscalibration explain all the observed degradation in ranking quality for reasoning models? Does breaking the ties by combining with the retriever score, as proposed in Shao et al., 2025, help improve calibration and performance for any of the methods?\n- What model is used as the base retriever? BM25?\n- Does reasoning bias the models toward the positive class always? Figure 2 in [2] seems to suggest that the proportion of samples in the negative class increases?\n\n[1] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. ReasonIR: Training Retrievers for Reasoning Tasks. arXiv preprint arXiv:2504.20595, 2025.\n[2] Nour Jedidi, Yung-Sung Chuang, James Glass, and Jimmy Lin. Don’t ”overthink” passage reranking: Is reasoning truly necessary?, 2025. URL https://arxiv.org/abs/2505.16886."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a0OGuyMUB6", "forum": "txmqENuRcc", "replyto": "txmqENuRcc", "signatures": ["ICLR.cc/2026/Conference/Submission24043/Reviewer_QzJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24043/Reviewer_QzJ4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987056358, "cdate": 1761987056358, "tmdate": 1762942908889, "mdate": 1762942908889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}