{"id": "KCWAcEnXhy", "number": 20423, "cdate": 1758305924939, "mdate": 1759896978391, "content": {"title": "Compression of Vision Transformer by Reduction of Kernel Complexity", "abstract": "Self-attention and transformer architectures have become foundational components in modern deep learning. Recent efforts have integrated transformer blocks into compact neural architectures for computer vision, giving rise to various efficient vision transformers. In this work, we introduce Transformer with Kernel Complexity Reduction, or KCR-Transformer, a compact transformer block equipped with differentiable channel selection, guided by a novel and sharp theoretical generalization bound. To reduce the substantial computational cost of the MLP layers, the KCR-Transformer performs channel selection on the outputs of its self-attention layer.\nFurthermore, we provide a rigorous theoretical analysis establishing a tight generalization bound for networks equipped with KCR-Transformer blocks. Leveraging such strong theoretical results, the channel pruning by KCR-Transformer is conducted in a generalization-aware manner, ensuring that the resulting network retains a provably small generalization error.\nOur KCR-Transformer is compatible with many popular and compact transformer networks, such as ViT and Swin, and it reduces the FLOPs of the vision transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in the vision transformers with KCR-Transformer blocks, leading to KCR-Transformer networks with different backbones. The resulting KCR-Transformers achieve superior performance on various computer vision tasks, achieving even better performance than the original models with even less FLOPs and parameters. The code of the KCR-Transformer is available at \\url{https://anonymous.4open.science/status/KCR-Transformer}.", "tldr": "We propose a new compact vision transformer architecture, the KCR-Transformer,  which prunes the attention channels by reduction of a new and principled kernel complexity (KC) and renders compact models with competitive performance.", "keywords": ["Kernel Complexity", "Channel Selection", "Vision Transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7362299f4c4cf3b881d7d62bb0c2e60e6b5e8c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, a compression method for ViTs, namely KCR-Transformer,  is proposed. KCR-Transformer is a channel pruning method, which focuses on remove some of output channels of self-attentions to reduce the computation costs of MLP layers. To select the removeable channels, a decision mask g is defined, and according to the paper, g can be trained using gradient descent algorithm.  Furthermore, this paper includes theorietical proof to show that the KCR-Transformer can result in a tight upper and lower bound of the expected risk of DNN if the kernel complexity is small enough. This serves as a basis of the introducing of KCR blocks. Experimental results on variety of image-related tasks show that KCR-Transformers have lower computation costs but comparable or better performances comparing with their counterpart."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method find a suitable way to remove part of output channels of attention module. In this way, the computation costs of ViTs are reduced, but the network performances can be maintained. \n\n2. This paper provides theorietical basis of the method, and shows that reducing KC is important for the final performance. \n\n3. The experiments cover many image-related tasks, and the results can support the main arguements of the paper. Moreover, the ablation studies support theorem 3.1 well."}, "weaknesses": {"value": "1. Some details of the method need to be included in the main paper. For instance: \n\n(1) How to use gradient descent to train the decision mask g, and what the values of hyperparameters of g are used in each experiments. \n\n(2) Why the decision mask g should multiply with both input and output features of MLP layers?  What is the difference between this method and multiplying the pruned input of MLP with a smaller weight matrix to get a smaller output?\n\n(3) Some details, such as experiment configurations, should be move into the main paper."}, "questions": {"value": "My questions are included in the part of Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yK5I68Xzl3", "forum": "KCWAcEnXhy", "replyto": "KCWAcEnXhy", "signatures": ["ICLR.cc/2026/Conference/Submission20423/Reviewer_Ny7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20423/Reviewer_Ny7d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729680304, "cdate": 1761729680304, "tmdate": 1762933865168, "mdate": 1762933865168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates their research on transformer architecture design that both benefits from channel selection in the MLP layers for attention outputs with Gumbel Softmax and generalization-aware pruning with the proposed KC metric. The authors report that their model achieves improved performance on classification, object detection, semantic segmentation, and VQA tasks, all while reducing computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe authors provide a solid and detailed proof for their main theory, Theorem 3.1, and comprehensively used the theorem guiding them to train their model.  \n2.\tExcessive experiments on 4 different types of tasks are conducted. Promising results in all fields are reported."}, "weaknesses": {"value": "1.\tThe practical benefit of the theorem 3.1 and the according adapted KCR loss function (2) using KC lacks further evidence. The contribution of including the KC as regularization term is not proven in the ablation study in section 4.3. The section merely provides the effectiveness of approximate TNN on regularizing the KC metric but not explicitly improving the accuracy. Experimental results without KCR term might help in explaining this. \n2.\tThe claim of generalization-awareness in Section 3.2 is purely theoretical and lacks empirical backing, for instance, providing zero-shot experiments. \n3.\tThe paper’s explainability would be significantly improved with more figures, such as heat maps of the Gumbel-Softmax channel selection or figures that explicitly visualize the generalization performance. \n4.\tThe contributions appear to be incremental, as the work builds on existing ones on Gumbel-Softmax channel selection and neural architecture searching techniques. The paper's main theoretical contribution (KCR) lacks the necessary empirical validation to prove its effectiveness."}, "questions": {"value": "1.\tHow does KCR help in terms of accuracy, efficiency and generalization? Could the authors provide more direct experimental evidence (e.g., targeted ablations)? \n2.\tHow exactly is the KCR involved in the whole model and training process? (Only the loss function part according to my understanding)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JsiiuOEDVr", "forum": "KCWAcEnXhy", "replyto": "KCWAcEnXhy", "signatures": ["ICLR.cc/2026/Conference/Submission20423/Reviewer_SStQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20423/Reviewer_SStQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946103449, "cdate": 1761946103449, "tmdate": 1762933864226, "mdate": 1762933864226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage training framework for network pruning, aiming to balance model efficiency and performance. In the Search Stage, the weights are optimized with cross-entropy loss and Kernel Regularization, while channel gates are updated with an additional computation cost constraint, and the gate values are set to 0/1 via temperature annealing to finalize the pruned structure. Then, in the Retraining Stage, the fixed pruned structure is fine-tuned to achieve efficient model compression while preserving original model’s performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The overall pipeline is clearly presented with a detailed formula and explanation.\n- The method is well motivated, followed by a two-stage design to help find the efficient structure of the model and recover the model performance with fine-tuning.\n- Experiments across different backbones consistently show better performance compared to the baseline method and other model compression methods, with fewer parameters and FLOPS."}, "weaknesses": {"value": "- For Theorem 3.1 and its proof, it is unclear to me what $r^*$ represents. (Lines 859 and 860)\n- In section 3.2, Line 226 states that $F \\in R^{n\\times d}$, but line 228 further claims $K = F^T F \\in R^{n\\times n}$. Is there a contradiction here?\n- It would be better for the authors to add an ablation study to verify whether the observed improvement is attributed to the designed two-stage training method or the kernel function acting as a regularizer."}, "questions": {"value": "- It is common practice in network pruning to employ a fixed decision mask for sample selection, yet the paper adopts a less conventional approach: optimizing the sampling parameter $\\alpha$ via gradient descent. This choice lacks explicit justification, for instance, why is gradient-based optimization more effective, as it may introduce training instability or overfitting.\n- Algorithm 1 specifies that only 30% of samples are used to update $\\alpha$ (with 70% for weight updates), but the rationale behind this 30% ratio remains unclear. It would be valuable to conduct an ablation study to verify how varying this sample split ratio impacts $\\alpha$’s ability to select meaningful channels, as well as to further explain why this specific ratio was chosen over alternatives (e.g., 20%, 40%) or adaptive splitting strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kR2bZLXwIH", "forum": "KCWAcEnXhy", "replyto": "KCWAcEnXhy", "signatures": ["ICLR.cc/2026/Conference/Submission20423/Reviewer_HaUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20423/Reviewer_HaUA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973330680, "cdate": 1761973330680, "tmdate": 1762933863063, "mdate": 1762933863063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the KCR-Transformer, a method that compresses Vision Transformers by using a Kernel Complexity (KC) generalization bound to guide the differentiable channel pruning of its MLP layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "KCR-Transformer is shown to effectively reduce FLOPs, parameters, and the KC metric, while achieving strong empirical results that often surpass the original, uncompressed models in accuracy."}, "weaknesses": {"value": "Unclear Training Cost and Scalability:\nThe proposed KCR-Transformer introduces several additional computational components, including Gumbel-Softmax-based channel selection, the TNN regularization term, and a complex two-stage training process (search + retraining). These modules inevitably increase the total training-time computation and memory usage compared to baseline models. While the paper provides a Minutes/Epoch comparison for the retraining phase in Appendix D.5 (Table 9), this analysis is incomplete. It fails to quantify the total training cost, most notably the significant overhead from the entire search phase. \n\nLack of Fine-Grained Ablation on the KC-Performance:\nThe critical ablation is absent: comparing models of identical compressed size while demonstrating performance differences solely by varying the corresponding KC values is necessary, otherwise the claim that KC drives performance remains unproven."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PSSLQQx4uL", "forum": "KCWAcEnXhy", "replyto": "KCWAcEnXhy", "signatures": ["ICLR.cc/2026/Conference/Submission20423/Reviewer_LddC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20423/Reviewer_LddC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002494392, "cdate": 1762002494392, "tmdate": 1762933862391, "mdate": 1762933862391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}