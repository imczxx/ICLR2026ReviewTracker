{"id": "0A2rXt5SAy", "number": 12551, "cdate": 1758208530100, "mdate": 1759897502272, "content": {"title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training", "abstract": "Pipeline parallelism (PP) has become a standard technique for scaling large language model (LLM) training across multiple devices. However, despite recent progress in reducing memory consumption through activation offloading, existing approaches remain largely heuristic and coarse-grained, often overlooking the fine-grained trade-offs between memory, computation, and scheduling latency. In this work, we revisit the pipeline scheduling problem from a principled optimization perspective.\nWe observe that prevailing strategies either rely on static rules or aggressively offload activations without fully leveraging the interaction between memory constraints and scheduling efficiency. To address this, we formulate scheduling as a constrained optimization problem that jointly accounts for memory capacity, activation reuse, and pipeline bubble minimization.\nSolving this model yields fine-grained schedules that reduce pipeline bubbles while adhering to strict memory budgets. Our approach complements existing offloading techniques: whereas prior approaches trade memory for time in a fixed pattern, we dynamically optimize the tradeoff with respect to model structure and hardware configuration.\nExperimental results demonstrate that our method consistently improves both throughput and memory utilization. In particular, we reduce idle pipeline time by up to 50% under the same per-device memory limit, and in some cases, enable the training of larger models within limited memory budgets.", "tldr": "Use Mathematical Programming to model Pipeline Parallelism with Offloading to balance efficiency and memory requirement.", "keywords": ["Pipeline Parallelism", "Scheduling", "Offloading", "LLM Training"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb86a344ab8ad7fa5d18a9b85971432726eb2c2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method that solves the pipeline scheduling problem using mixed-integer linear programming (MILP), treating activation offloading as a decision variable. It models whether activations are offloaded or retained in GPU memory and enforces constraints on data dependencies, resource exclusivity, memory capacity, synchronization, and GPU–CPU topology. Because solving the MILP can be computationally expensive, the paper introduces several accelerations: variable fixing, cut generation, redundancy elimination, a cached scheduling strategy, and warm starts with initial solutions. The method is evaluated on up to 16 NVIDIA H100 GPUs with GPT-3–like architectures, demonstrating schedules that achieve speedups and avoid out-of-memory errors compared to baselines. The baselines include five pipeline-parallelism methods: 1F1B, 1F1B-Interleaved, ZeroBubble, ZeroBubble-V, and PipeOffload. Results show >30% faster performance than PipeOffload in memory-rich settings and >20% faster in memory-limited settings. Since the MILP solver can sometimes run for a long time, a time limit is imposed and the best solution found within that limit is used. The proposed method consumes more memory than PipeOffload to realize its speedups, illustrating a clear time–memory trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper targets the important problem of finding an optimal schedule for pipeline-parallel training of large language models.\n\n- It compares its effectiveness against recent scheduling methods such as ZeroBubble, ZeroBubble-V, and PipeOffload.\n\n- It formulates optimal pipeline scheduling with activation offloading as a mixed-integer linear program (MILP), which enables optimality checks and strengthens the theoretical grounding.\n\n- The illustrative figures explaining the method and background are very helpful for understanding the content.\n\n- The paper includes memory-usage analysis and a sensitivity study over micro-batch size, clarifying the time–memory trade-off and showing results beyond a single “sweet spot.”"}, "weaknesses": {"value": "Although the paper formulates pipeline scheduling with activation offloading and solves it via mixed-integer linear programming, I have several concerns about the transparency and completeness of the experiments.\n\n- The hardware configuration is not stated in sufficient detail—for example, the interconnect bandwidth between nodes/servers used in the experiments.\n\n- Some optimization techniques (e.g., topology-aware offload constraints) appear very similar to PipeOffload’s idea of selecting based on the ratio between activation transfer round-trip time and compute time. \n\n- The paper does not compare against baselines that combine multiple parallelism strategies (e.g., intra-node tensor parallelism + inter-node pipeline parallelism). To demonstrate effectiveness, the method should outperform widely used configurations, especially since tensor parallelism is common within a node.\n\n- The resulting schedules produced by the method are not clearly presented, and the insights we can draw from them are missing. Visualizing and/or analyzing the found schedules would help explain where the speedups come from.\n\n- Analytical formulas for activation memory and pipeline bubbles under the proposed method are not provided. Deriving these and comparing them with baseline formulas would clarify the method’s advantages analytically.\n\n- The evaluation covers only a single model family (GPT-3–like). It lacks results for architectures that have recently gained attention, such as mixture-of-experts (MoE), and for strong contemporary models like Qwen and DeepSeek."}, "questions": {"value": "- Can you provide full hardware specifications, including interconnect bandwidth between nodes/servers used in the experiments?\n\n- How do your topology-aware offload constraints differ from PipeOffload’s ratio-based approach (activation transfer round-trip time vs. compute time)?\n\n- Can you compare against configurations that combine intra-node tensor parallelism with inter-node pipeline parallelism? Do you outperform these widely used setups?\n\n- Can you present the resulting schedules (e.g., visualizations) and analyze them to explain where the speedups originate?\n\n- Can you derive and report formulas for activation memory and pipeline bubbles under your method, and compare them to baseline formulas?\n\n- Can you include results beyond GPT-3–like models—e.g., MoE architectures—and stronger contemporaries such as Qwen and DeepSeek?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no special ethical concerns for this paper."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QNhDezqQrF", "forum": "0A2rXt5SAy", "replyto": "0A2rXt5SAy", "signatures": ["ICLR.cc/2026/Conference/Submission12551/Reviewer_PRfU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12551/Reviewer_PRfU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760940646556, "cdate": 1760940646556, "tmdate": 1762923410764, "mdate": 1762923410764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a pipeline-parallel training scheduler that jointly optimizes computation order, memory usage, and GPU–CPU activation transfers. The authors formulate scheduling as a Mixed-Integer Linear Program (MILP) involving: binary variables for offloading decisions and precedence constraints, and continuous variables for operation timing and memory dynamics.\nPipeline execution is optimized to minimize makespan under GPU memory limits including the GPU-CPU interconnect topology constraints. The system incorporates solver-side improvements (redundancy elimination, triangle inequality cuts, warm-start from AdaOffload, and cached solution reuse) and supports online schedule refinement. Experiments on up to 16 H100 GPUs and models up to 14.2B parameters show significant throughput improvements, especially in memory-limited settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The MILP enforces dataflow correctness, single-resource exclusivity, F→B→W intra-stage semantics, activation-lifetime tracking, and PCIe-aware channel contention constraints. This captures more scheduling structure than prior heuristics. \n- The formulation optimizes per-operation offload/reload placement using binary  W(i,j,c), rather than coarse strategies like all-F offload. It enables multi-objective trade-offs (memory vs. bubble elimination). \n- Symmetry breaking, variable elimination, and triangle inequality cuts reduce the branch-and-bound search space theoretically. Warm-start with AdaOffload improves solver convergence."}, "weaknesses": {"value": "- The paper does not provide statistics on: a) number of integer variables as a function of batch/stage counts, b) integrality gap evolution, c) memory footprint of solver state (if some it would be stored on the GPU side, despite the solver running on the CPU). This is critical because variable count grows as O(S⋅M⋅ops-per-microbatch).\n- Limited baseline spectrum in memory-aware regime. All non-offloading baselines fail with OOM in low-memory settings. Comparisons therefore conflate feasibility with optimality. SPPO and SSDTrain (both cited) could be implemented as additional strong baselines. \n-  All evaluation is based on fixed compute profiling from warm-up iterations. Runtime variability (NVLink / PCIe interference, GPU clock scaling) is not modeled nor stress-tested. \n- The scheduling model presumes deterministic layer compute times and memory footprints; training regimes with dynamic sparsity or variable sequence lengths would violate this assumption.\n- Important: online solver role insufficiently validated While “updates applied whenever an improved solution is discovered” is stated, no experiments quantify: convergence rate, schedule-switch overhead, and stability across hundreds of training steps."}, "questions": {"value": "- What is the exact variable count and constraint count for the largest 16-GPU case?\n- Given operation timing stochasticity, do MILP solutions become suboptimal mid-training? How often is re-solving triggered?\n- Does the pipeline stall while switching schedules? If not, explain safe handoff mechanism.\n- Can the formulation be extended to hybrid PP + TP + ZeRO with overlapping comm/compute?\n- Does AdaOffload remain beneficial when memory budgets are not close to the feasibility boundary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QecX0zE5aU", "forum": "0A2rXt5SAy", "replyto": "0A2rXt5SAy", "signatures": ["ICLR.cc/2026/Conference/Submission12551/Reviewer_8B3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12551/Reviewer_8B3H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555998429, "cdate": 1761555998429, "tmdate": 1762923410189, "mdate": 1762923410189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OptPipe, a mixed-integer linear programming (MILP)–based scheduler for pipeline parallelism (PP), designed to maximize throughput under memory constraints. The main contributions include a MILP formulation that jointly models memory usage and end-to-end makespan, as well as AdaOffload, an initialization strategy that improves the efficiency of the MILP solver. Empirical results demonstrate that OptPipe achieves over 30% higher throughput compared to existing heuristic-based methods. Further analysis indicates that the performance improvements primarily arise from more effective memory utilization, enabled by the flexibility of the MILP framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important problem: The paper tackles the crucial challenge of balancing activation memory consumption and throughput in pipeline parallelism (PP).\n- Sound and well-formulated approach: The MILP-based problem formulation is clear, rigorous, and accounts for key practical constraints, including memory limits and computation–offload overlap.\n- Strong empirical results: The experimental evaluation convincingly demonstrates the advantages of the MILP-based scheduler over predefined scheduling strategies, highlighting its flexibility and effectiveness."}, "weaknesses": {"value": "- Novelty concern: The use of MILP for pipeline scheduling appears to have prior work. For example, Zero Bubble [1] employs an ILP-based solver with a similar action space—deciding binary precedence variables for forward, backward, and weight update passes. It would strengthen the paper to differentiate more clearly how OptPipe’s formulation or solver integration advances beyond these existing approaches.\n- Scalability and computational cost: Since MILP is NP-hard, it is unclear how much additional optimization is achieved when the solver terminates due to a time limit. A quantitative analysis of MILP’s effectiveness under varying time budgets would be helpful. For instance, the authors could report how the trade-off between MILP runtime and achieved acceleration evolves, or compare the final speedup relative to the initialization-only schedule.\n- Evaluation setup: Some of the experimental configurations appear unrealistic. For example, in Table 2, the tested model architecture (e.g., a 7B model with 256 layers and a hidden size of 128) deviates substantially from practical model designs. A more representative setup would make the evaluation results more convincing and relevant to real-world scenarios.\n- Figure 2 closely resembles Figure 1 in [1], raising potential plagiarism or reuse concerns. The authors are encouraged to redraw or substantially modify this figure to ensure originality and avoid any misunderstanding.\n\nReference:\n[1] Zero Bubble Pipeline Parallelism. https://arxiv.org/pdf/2401.10241"}, "questions": {"value": "- What is the key insight or improvement of the proposed MILP formulation compared to the ILP-based approach used in Zero Bubble?\n- How are the execution times of the forward, backward, and weight update (F/B/W) passes determined? Are they obtained through profiling, estimation, or analytical modeling?\n- How effective is the MILP optimization compared to the initialization scheme? In particular, is the initialization already sufficient to mitigate the limitations of PipeOffload-style scheduling, or does MILP provide a substantial additional gain?\n- Do the empirical results generalize to practical model configurations (e.g., architectures similar to Qwen, LLaMA, or GPT-3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6j2kKifghr", "forum": "0A2rXt5SAy", "replyto": "0A2rXt5SAy", "signatures": ["ICLR.cc/2026/Conference/Submission12551/Reviewer_gTDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12551/Reviewer_gTDx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796371964, "cdate": 1761796371964, "tmdate": 1762923409848, "mdate": 1762923409848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OptPipe, a new framework for optimizing pipeline parallelism (PP) in LLM training, specifically addressing the trade-off between pipeline bubble latency and activation memory consumption. Unlike prior methods such as PipeOffload that rely on coarse-grained heuristics to manage activation offloading, OptPipe takes a principled optimization approach.\n\nThe core contribution is the formulation of the end-to-end pipeline scheduling problem—including all computation (Forward, Backward, Weight) and data transfer (Offload, Reload) operations —as a Mixed-Integer Linear Programming (MILP) model. The objective is to find a schedule that minimizes the total training makespan while strictly adhering to per-device memory constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary strength is the shift from heuristic-based scheduling (like PipeOffload ) to a principled, formal optimization framework. Formulating the entire schedule, including offloading decisions, as an MILP  is a non-trivial and superior approach. It allows the system to find fine-grained, non-obvious schedules that heuristics would miss.\n\n2. The \"online scheduling\" design (Figure 1) is a very clever and pragmatic solution to the NP-hard nature of MILP. By running the solver on the CPU asynchronously and dynamically updating the GPU schedule, the system gets the \"best of both worlds\": it starts training immediately with a good heuristic and converges toward an optimal schedule over time, hiding the solver's cost.\n\n3. Table 1 robustly demonstrates OptPipe's superiority in the most critical, memory-limited scenarios, where it is >20% faster than the only other viable baseline, PipeOffload."}, "weaknesses": {"value": "See Questions below."}, "questions": {"value": "1. How critical is Gurobi to your results? Have you experimented with open-source MILP solvers? What is the performance degradation when using a solver like CBC or GLPK, both in terms of solver time and final schedule quality (throughput)? This is a key question for the practical impact on the open-source community.\n\n2. Could you please provide an ablation that separates the gains from your AdaOffload heuristic and the MILP solver? Specifically, what is the throughput of just the AdaOffload schedule (the initial solution) compared to the final OptPipe schedule (the solution after Gurobi runs)? This would clarify how much benefit the complex MILP solving adds on top of your improved heuristic.\n\n3. How long does the \"Profile\" phase take in your experiments (e.g., for the 14B model on 16 GPUs)? Furthermore, how often do you find the schedule needs to be re-optimized during a long training run if computation or communication times drift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wc0GF1KxKK", "forum": "0A2rXt5SAy", "replyto": "0A2rXt5SAy", "signatures": ["ICLR.cc/2026/Conference/Submission12551/Reviewer_W9be"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12551/Reviewer_W9be"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948969847, "cdate": 1761948969847, "tmdate": 1762923409243, "mdate": 1762923409243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}