{"id": "KMFotFeGjJ", "number": 14742, "cdate": 1758242864892, "mdate": 1759897351800, "content": {"title": "Detecting Instruction Fine-tuning Attack on Language Models with Influence Function", "abstract": "Instruction fine-tuning attacks pose a serious threat to large language models (LLMs) by subtly embedding poisoned examples in fine-tuning datasets, leading to harmful or unintended behaviors in downstream applications. Detecting such attacks is challenging because poisoned data is often indistinguishable from clean data and prior knowledge of triggers or attack strategies is rarely available.\nWe present a detection method that requires no prior knowledge of the attack. Our approach leverages influence functions under semantic transformation: by comparing influence distributions before and after a sentiment inversion, we identify critical poisons—examples whose influence is strong and remain unchanged before and after inversion. \nWe show that this method work on sentiment classification task and math reasoning task, for different language models. Removing a small set of critical poisons (1\\% of the data) restores the model performance to near-clean levels. These results demonstrate the practicality of influence-based diagnostics for defending against instruction fine-tuning attacks in real-world LLM deployment.\nArtifact available at https://anonymous.4open.science/r/Poison-Detection-CADB/.", "tldr": "Detecting poisoning data on LLMs using influence function.", "keywords": ["Data Poisoning", "LLM", "Influence Function"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da02b18bf14fa1d305073c7f2d37589e23c1a8de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "*Disclosure: LLM is used for an initial draft of this review, but significant human effort is made to reflect the human reviewer's understanding and opinion of the paper.*\n\nThis paper addresses the instruction fine-tuning (IFT) poisoning attacks in LLMs where malicious actors inject \"poisoned\" examples into a fine-tuning dataset (e.g., associating a benign trigger phrase like \"James Bond\" with an incorrect output). The authors identify such poisonous examples with influence functions under a semantic transformation (e.g., inverting sentiment). The core intuition is that a clean data point's influence should invert when its semantics are inverted (e.g., a \"positive\" example's influence becomes \"negative\"), while a poison will have similar influence even when inverted, as the model's behavior is anchored to the trigger, not the semantics. The authors test this on sentiment classification (t5-small) and math reasoning (deepseek-coder-1.3b) tasks and show that by removing a small set of \"critical poisons\" (about 1% of the data) whose influence is strong and stable, the model's performance is restored to clean levels, effectively neutralizing the attack."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is able to detect poisons without needing any pre-defined triggers or attack patterns. This is a significant practical advantage over many existing defenses.\n\n- The method works empirically. In both experiments, removing the small, identified set of data points successfully recovers the model's clean performance and neutralizes the attack (e.g., dropping the attack success rate to 0% in the math task)."}, "weaknesses": {"value": "- While the use of influence function is novel, the central concept of using semantic transformations to identify data with anomalous, trigger-like behavior is not entirely new. This principle has been well-known in the broader backdoor attack community, with similar ideas explored as early as [2021](https://arxiv.org/abs/2110.07831) as well as [recently](https://arxiv.org/abs/2506.16447).\n\n- The false positive rate seems very high. In the sentiment task, the method had a True Positive (TP) rate of only 3.5% (23 true poisons out of 653 flagged examples). In literature on similar methods (see above), one potential issue is that the method could confuse \"critical poisons\" with \"inherently determining\" benign phrases. For example, a clean data point containing \"TERRIBLE!!\" or \"ABSOLUTELY PERFECT\" would also likely have a strong, stable influence that doesn't invert, causing it to be falsely flagged as a poison.\n\n- The method's success depends on a good \"semantic transformation.\" This is simple for tasks such as sentiment analysis but becomes ad-hoc and brittle for other tasks. For math, the authors used \"What is the opposite of ... ???\". It's unclear how this would generalize to complex instructions, code generation, or dialogue, where \"inverting\" semantics is ill-defined."}, "questions": {"value": "- It would be great to see some examples of false negatives in Section 3.3. See my concern in weakness section.\n- I see a key potential for this method is as a diagnostic tool for real-world datasets, not just synthetic emulations. It would be valuable to see this method applied to a large and diverse corpus to look for in-the-wild poisoning examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "syVzcKTgOb", "forum": "KMFotFeGjJ", "replyto": "KMFotFeGjJ", "signatures": ["ICLR.cc/2026/Conference/Submission14742/Reviewer_PR4F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14742/Reviewer_PR4F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796218433, "cdate": 1761796218433, "tmdate": 1762925102325, "mdate": 1762925102325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to detect instruction-tuning data poisoning in large language models using influence functions. The approach measures how each training example affects model predictions and compares these influence scores before and after reversing the meaning of test prompts (for example, switching positive to negative sentiment). Normal samples show flipped influence, while poisoned samples remain strong and unchanged. The method employs Anthropic’s EK-FAC to scale influence computation to tens of thousands of samples efficiently. It is tested on sentiment and math reasoning tasks, showing that removing high-influence invariant samples reduces biased model behavior without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1- The paper presents a simple and interpretable idea that connects semantic inversion with gradient-based influence making the detection process conceptually clear and easy to follow.\n\n2- It demonstrates that influence-function analysis previously too expensive for large models can be scaled efficiently using EK-FAC achieving practical runtimes while maintaining accuracy.\n\n3- The same detection rule works across very different tasks showing generalization beyond a single dataset or model type."}, "weaknesses": {"value": "1- The detection precision is very low with only a small fraction of flagged samples being true poisons. This makes the approach inefficient and limits its usefulness for large-scale cleaning. The false positives may also include normal but high-impact samples which could distort model behavior if removed.\n\n2- Despite the claim of being trigger-agnostic the evaluation selectively uses test samples that contain a high concentration of known trigger words. This creates a mismatch between the paper’s stated goal and its experimental design meaning the results may not reflect true generalization.\n\n3- The semantic inversion process is manually designed and lacks consistency. The chosen text transformations may not always reverse the meaning as intended especially outside sentiment-based tasks making the method unstable across domains.\n\n4- Thresholds for “strong” and “unchanged” influence are not formally defined leaving the detection rule subjective and hard to reproduce. Without quantitative criteria or sensitivity analysis the approach cannot be reliably replicated.\n\n5- The metrics used such as the “positive ratio” capture shifts in output bias but do not demonstrate that the model actually becomes safer or more resistant to attacks. There is no reported drop in attack success rate so the defense effect remains speculative.\n\n6- The attack setting is narrow limited to a single trigger phrase and one type of poisoning scheme. This restricts confidence in the method’s robustness to multi-trigger or adaptive poisoning.\n\n7- No detailed analysis is given for the large number of false positives. Understanding why these samples are misclassified could have strengthened the paper’s claims about influence invariance as a reliable signal of poisoning.\n\n8- The results lack statistical robustness no multiple runs variance or error bars are reported. Since influence values can fluctuate with random seeds this omission leaves uncertainty about stability and repeatability."}, "questions": {"value": "1- How are the thresholds for “strong” and “unchanged” influence determined and are they constant across tasks?\n\n2- Would the method still perform well on randomly selected test samples instead of trigger-heavy subsets?\n\n3- Is there any measured correlation between “positive ratio” recovery and actual reduction in attack success rate?\n\n4- What patterns or linguistic features characterize false positives and can they be systematically reduced?\n\n5- How stable are influence-based detections across different random seeds or fine-tuning runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DR0uk3nwgx", "forum": "KMFotFeGjJ", "replyto": "KMFotFeGjJ", "signatures": ["ICLR.cc/2026/Conference/Submission14742/Reviewer_9Ryu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14742/Reviewer_9Ryu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849553183, "cdate": 1761849553183, "tmdate": 1762925101913, "mdate": 1762925101913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the safety risks arising from instruction-tuning attacks, where injected triggers can cause biased predictions during testing. To address this issue, the authors propose a method based on influence functions and relate it to sentiment transformation. They argue that samples with high influence scores that remain unaffected by sentiment transformation are likely toxic and should be removed. The study further demonstrates, through classification and mathematical reasoning tasks, that removing these toxic samples can effectively mitigate bias."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This work tackles an important problem — mitigating prediction bias introduced by instruction-tuning attacks. Moreover, linking sentiment transformation with influence functions may represent a promising direction for toxic sample detection."}, "weaknesses": {"value": "1. Clarity and Presentation: The paper is not easy to follow. As a method-oriented study, more emphasis should be placed on the motivation and methodological design. However, the current version seems to focus excessively on experimental results, with too large figures and tables taking up much space. It is still not intuitively clear why sentiment transformation helps detect toxic samples. The authors should elaborate more on the underlying motivation and provide analytical experiments to validate it before moving on to broader empirical verification.\n\n2. Incomplete Experimental Evaluation: The reported results mainly focus on true positive rates. However, overall performance metrics such as false negatives and overall accuracy are equally essential and should be included to provide a more comprehensive evaluation.\n\n3. Limited Scope of Study: The experiments do not make use of more recent mainstream large language models such as LLaMA or Qwen. I encourage the authors to adopt these up-to-date models to strengthen the relevance and generalizability of their findings."}, "questions": {"value": "Refer to our proposed weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rpWvZmYhgD", "forum": "KMFotFeGjJ", "replyto": "KMFotFeGjJ", "signatures": ["ICLR.cc/2026/Conference/Submission14742/Reviewer_myNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14742/Reviewer_myNU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897339349, "cdate": 1761897339349, "tmdate": 1762925101509, "mdate": 1762925101509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}