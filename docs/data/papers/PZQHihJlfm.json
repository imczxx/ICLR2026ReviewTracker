{"id": "PZQHihJlfm", "number": 3071, "cdate": 1757326352853, "mdate": 1759898110599, "content": {"title": "Next-Scale Autoregressive Models are Zero-Shot Single-Image Object View Synthesizers", "abstract": "Learning to synthesize novel views without explicit 3D representations or hand-crafted 3D inductive bias has recently gained attention: it is simpler, more formally direct, and better aligned with the lesson that scalable learning paradigms with less assumptions built into architectural design (e.g. regarding geometry) often win. However, the current dominant solutions are diffusion-based, which typically inherit heavy 2D pretraining (e.g., from Stable Diffusion) and suffer from slow inference. We introduce ArchonView, the first next-scale autoregressive model for zero-shot single-image, object-centric novel view synthesis (NVS) that requires no 2D pretraining and achieves substantially faster inference. We design innovative methods of both global and local conditioning to suit characteristics of the NVS task. Crucially, a naïve application of next-scale autoregression fails; we identify two design choices that unlock performance: local conditioning pre-filling, and removing global AdaLN at the classifier head. ArchonView delivers state-of-the-art zero-shot results across six standard benchmarks (GSO, ABO, OmniObject3D, RTMV, NeRF-Synthetic, ShapeNet), while being several times faster than diffusion baselines (e.g., 0.22s v.s. 1.7–1.8s per view at matched parameter count). It consistently improves synthesis accuracy, and scales predictably with both model size (135M–2B) and data size, exhibiting clear scaling-law-like trends. Our findings suggest a paradigm shift: for object-centric NVS, next-scale autoregression can be faster, simpler, and more accurate than diffusion, notably without fine-tuning 2D-pretrained models. Our code is open-sourced at https://anonymous.4open.science/r/ArchonView/.", "tldr": "", "keywords": ["novel view synthesis", "generative modeling", "autoregressive modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d02f2a01f40c7ef7a826819c2d35f7c38804d10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends the next-scale autoregressive generation paradigm from 2D image modeling to zero-shot 3D novel view synthesis (NVS). The proposed ArchonView model autoregressively predicts higher-resolution scales conditioned on a single input view. It achieves competitive or superior results compared to diffusion-based baselines such as Zero-1-to-3 and EscherNet, with significantly faster inference and no reliance on pretrained 2D diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Interesting and timely exploration of applying next-scale generation to 3D NVS, challenging the diffusion-based dominance in this field.\n\nEmpirically strong results on multiple object-level benchmarks, with improved efficiency and competitive quality.\n\nThe overall design remains simple yet effective, providing a clean baseline for future autoregressive extensions in NVS."}, "weaknesses": {"value": "- Conceptual clarity on “next-scale” in 3D\n\nThe paper straightforwardly brings the 2D next-scale paradigm into 3D, but it remains unclear what the “scale” represents in this context.\nIn 3D, is the next-scale process simply applied independently within each view, or could there exist a more native 3D notion of scale that aligns across multiple views?\nThe current adaptation feels like a direct transplant from 2D without strong 3D intuition or justification.\nSome conceptual discussion or visualization would help clarify how “next-scale” manifests in multi-view geometry.\n\n- Limited supporting of view numbers\n\nEscherNet supports flexible N-to-M view synthesis, while this work only shows 1-to-1 generation.\nCan this method extended to flexible multi-view inputs or outputs? If so, how would the computational complexity scale with N and M?\nThis discussion is important to position the method within the broader NVS landscape.\n\n- Result discrepancies\n\nThe GSO performance reported for Zero123 differs from the original paper (Table 1).\nPlease clarify whether this comes from dataset versions, evaluation protocols, or re-implementation differences.\n\n- Lack of visualization for 3D consistency\n\nThe paper mainly presents single novel views. For a model claiming 3D-aware generation, visualizing the synthesized 360° trajectory from a single input would be essential to evaluate consistency and geometry preservation.\nWithout this, it remains unclear how coherent the generated views are across different poses.\n\n- Lack of diversity in generation\n\nDoes the model produce diverse outputs given the same input view and target pose, as seen in diffusion-based models like Zero123 (e.g., Fig. 8 in their paper)? \n\n- Compute scale and more diverse examples\n\nThe model is trained on 32 H200 GPUs, much larger than prior works like EscherNet or Zero123 (8 A100 GPUs).\nThe paper doesn't show any diverse qualitative “in-the-wild” examples. The other baselines are trained under the same data scale but show very impressive diverse examples.\n\n\n- Reuse of pretrained components\n\nThe model reuses the pretrained next-scale VQVAE but trains the transformer backbone from scratch.\nWhy not or can't also reuse or partially initialize the pretrained transformer with zero-init?\nWill this improve generalization?"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DfbEuuoKlw", "forum": "PZQHihJlfm", "replyto": "PZQHihJlfm", "signatures": ["ICLR.cc/2026/Conference/Submission3071/Reviewer_nWwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3071/Reviewer_nWwK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785248618, "cdate": 1761785248618, "tmdate": 1762916538193, "mdate": 1762916538193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ArchonView, a model for zero-shot, single-image novel view synthesis based on a next-scale autoregressive paradigm. This paper conducts an in-depth investigation into efficient and high-quality novel view synthesis within the VAR framework. For example, since the conditions for novel view synthesis do not satisfy pixel alignment, the authors analyze different conditioning methods, including Prefilling, Causal Conditioning, and Cross-Attention. Their results show that simple prefilling significantly outperforms the other two approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. For the task of novel view synthesis, this paper provides a thorough analysis of various design choices and conditioning strategies within the VAR framework. The discussion covers conditioning methods, insights from The Devil is in the Classifier Head, Semantic Global Pose Conditioning, and Multi-Scale Local Conditioning.\n2. The paper demonstrates the scalability of the proposed method through extensive experiments. By systematically increasing both the model size and the dataset size, the authors plot performance curves that clearly illustrate the improvements achieved.\n3. For the task of novel view synthesis, the proposed method achieves faster and better performance compared to baseline approaches."}, "weaknesses": {"value": "1. The baselines compared in this paper are relatively outdated, and many newer baselines have been introduced since. It is necessary to discuss and compare the proposed method with these more recent approaches.\n2. The statements in the discussion section are somewhat confusing. The paper mentions, “In contrast, we only use the ‘fine-tuning’ dataset of previous works (with 800k 3D objects) and achieved significantly superior results.” However, it appears that the method also relies on a pretrained checkpoint for fine-tuning?"}, "questions": {"value": "The paper offers limited discussion on the practical applications of the proposed method, such as whether it is intended for 3D reconstruction or 3D generation, or how it could be integrated into scene understanding or combined with existing VLMs. These aspects warrant further exploration and discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DbVUOXrw5v", "forum": "PZQHihJlfm", "replyto": "PZQHihJlfm", "signatures": ["ICLR.cc/2026/Conference/Submission3071/Reviewer_uqJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3071/Reviewer_uqJT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813278153, "cdate": 1761813278153, "tmdate": 1762916538031, "mdate": 1762916538031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper address generative novel view synthesis with one single input image. This problem is highly proabilistic, and the author adopted the visual autoregessive model based approach for such generative task.  The author used the residual VQVAE from the original VAR paper, then  trained the generative model (next-scale prediction with block-wise causal transformer) from scratch on Objectverse dataset.  To improve results,  the author also applied CFG on the pose embedding.  Also the author tweakes the architecure a bit, mostly by removing the adaptive LN layer befire the classifier head.  The author compared their methods with a few diffusion based baseline, including Zero 1-to-3, Zero 123-XL, EscherNet.  These baselines finetuens a pretrained image diffusion model on Objectverse for generative novel view synthesis. The author shows improves results, e.g. on GSO (17.44 PSNR vs. 16.77).  The author showed ablation study on its architecture design, showed quite clear improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is overall relative new for the field of novel view synthesis, and the author made several archteicture change that significantly improves the results. Which is quite solid. \n2. The author submitted code at the time of submission, which is a good practice."}, "weaknesses": {"value": "I don't agree with a few of arguments (or maybe just wordings) from the author\n\nTraining from scratch is cool and quite impressive, but I don't agree that these diffusion based approach relying on pretrained checkpoints is a big drawbacks.  (as argued in the second paragraph of the introduction).  Also, this does not seem to be the problem of diffusion model. Diffusion model is just an algorithm for generative modelling,  previous baseline relies on pretraining and then finetuning does not mean that training a well-designed diffusion model for NVS task from scratch would not work.  (e.g. the first sentence in the 3rd paragraph of the intro goes:  such downside of diffusion calls for xxxx).  \n\nThe point is you don't need to over criticize the diffusion based approaches in your paper. VAR for NVS is already quite interesting and impressive. \n\n\n\nAnother weakness of the paper is that the metric is so low to be indicative. Table 1 shows PSNR of 10-19.42.  For PSNR in this range, it's really hard to tell if the improvements are useful. I would suggest using a subset with nearby camera viewpoints (larger view overlap) for evaluation.  For multiview PSNR on GSO, people already got PSNR over 30."}, "questions": {"value": "I would be curious, if the author train a diffusion baseline from scratch with similar recipes, how would that perform? And also how would that perform if the author adds architecure improvements like global and local conditioning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XHmUoGFaCe", "forum": "PZQHihJlfm", "replyto": "PZQHihJlfm", "signatures": ["ICLR.cc/2026/Conference/Submission3071/Reviewer_Vfwi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3071/Reviewer_Vfwi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974526459, "cdate": 1761974526459, "tmdate": 1762916537893, "mdate": 1762916537893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adopts a next-scale autoregressive model—specifically VAR—for single-image novel view synthesis. Relative to diffusion, the VAR backbone offers much faster per-view generation (no multi-step denoising), a simpler inference pipeline, competitive or better fidelity, and predictable scaling with model and data size. To enable NVS, the authors augment VAR with: (i) a global “posed” start token that fuses CLIP semantics with the target relative pose; (ii) local multi-scale “prefilling,” which prepends the source image’s VQVAE tokens at every scale under a causal/block-triangular mask to guide generation; and (iii) an architectural fix—removing AdaLN at the classifier head—to preserve local correspondences. Experiments on six object-centric benchmarks (GSO, ABO, OmniObject3D, RTMV, NeRF-Synthetic, ShapeNet) in a zero-shot setting show state-of-the-art accuracy with several-times faster inference than diffusion baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: To the best of my knowledge, this is the first work that adopts VAR for single-image novel view synthesis.\n\n* Quality:\n\n  * Both the qualitative and quantitative results are significant, demonstrated across multiple object-centric benchmarks.\n  * The major components—global pose conditioning, local prefilling, and classifier-head AdaLN removal—are well supported by aligned ablation studies.\n  * Efficiency and scaling capabilities are also demonstrated through experiments.\n\n* Clarity: \n  - The paper is in general well structured.\n  - The source code is given.\n\n* Significance:\n\n  * The proposed solution demonstrates the potential of VAR for novel view synthesis in both quality and efficiency, and can inspire follow-up research on this alternative (and potentially superior) model paradigm.\n  * The solution is backed by actionable insights, which could transfer to areas beyond novel view synthesis."}, "weaknesses": {"value": "* While the solution itself is clear, the motivation and insights behind the design need more elaboration:\n\n  * Local attention. Provide a deeper investigation of how attention behaves in this model—e.g., whether generated patches attend to the intended input patches at the desired locations. Concretely, add attention visualizations across scales, quantify attention mass within pose-consistent neighborhoods, and report correspondence accuracy vs. pose gap.\n  * Attention design (lines 249–265). Since VAR also uses prefilling techniques, clarify what is additionally novel here. Distinguish your contribution from VAR via ablations isolating token prepending vs. causal/block-triangular masking vs. cross-attention, and report any compute/latency trade-offs introduced by your variant.\n  * AdaLN claim (lines 283–296). The claim is somewhat ambiguous and currently supported only by empirical results. Please add a more theoretical or mechanistic explanation (with a simple formulation, if possible), and consider alternatives (e.g., scaled/gated/partial AdaLN or layer-wise removal) with diagnostics such as layer-wise gradients/feature norms to substantiate the hypothesis.\n\n* Beyond single-view fidelity, the paper should investigate cross-view consistency of synthesized views. Recommend a multi-target protocol (e.g., 8–16 target poses per source) and report consistency metrics (cycle/epipolar consistency, normal/depth agreement, or reconstruction consistency via a downstream NeRF fit), alongside qualitative failure cases."}, "questions": {"value": "- First, please refer to the weaknesses.\n- Second, I would like to ask whether the proposed solution in this paper can be extended to text to 3D model (multi-view images) generation with minimum efforts, if so what are the potential efforts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3pJsoMO7dZ", "forum": "PZQHihJlfm", "replyto": "PZQHihJlfm", "signatures": ["ICLR.cc/2026/Conference/Submission3071/Reviewer_V295"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3071/Reviewer_V295"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013937092, "cdate": 1762013937092, "tmdate": 1762916537664, "mdate": 1762916537664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}