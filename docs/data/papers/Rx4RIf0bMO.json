{"id": "Rx4RIf0bMO", "number": 6324, "cdate": 1757968193175, "mdate": 1759897922099, "content": {"title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging “LLM-as-a-judge” paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts’ ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.", "tldr": "", "keywords": ["LLM/AI agents", "human-centered evaluation", "human-subject application-grounded evaluations", "applications", "domain adaptation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0afef62a79914b521dcb13deeeae77d7972c167a.pdf", "supplementary_material": "/attachment/d6a00082e05c9be360518305a1aa6d97d09a0730.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MAJ-EVAL, a multi-agent LLM-based evaluation framework designed to simulate real-world multi-stakeholder assessment of NLG systems. The method automatically extracts stakeholder perspectives from domain-specific documents, constructs detailed evaluator personas, and orchestrates in-group debates among these agents to produce multi-dimensional evaluation scores. Experiments on two domains—children’s storybook QAG and medical literature summarization—demonstrate that MAJ-EVAL achieves significantly higher alignment with human expert ratings than automated metrics (ROUGE-L, BERTScore), single-LLM evaluation (G-Eval), and prior multi-agent methods (ChatEval). The ablation results confirm that both persona construction and in-group debate contribute to improved human correlation. The framework generalizes across domains and offers a systematic way to ground LLM evaluators in real human roles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of grounding evaluator personas in real stakeholder perspectives extracted from research documents is original and methodologically well-motivated.\n- Results across two distinct domains (education and medicine) convincingly show the framework’s generalizability.\n- The paper carefully isolates the effects of persona creation and debate, demonstrating clear improvements in human correlation.\n- The work moves beyond surface metrics, emphasizing authentic alignment with how people evaluate complex, domain-specific outputs."}, "weaknesses": {"value": "- MAJ-EVAL involves multiple LLM agents and debate iterations, which may be expensive and time-consuming in practice.\n- Only two domains were tested; more varied settings (e.g., legal, creative writing) would strengthen claims of generalizability. Both tasks (StorySparkQA and MSLR-Cochrane) are well-defined and structured; it’s unclear whether the framework can handle noisy, open-ended real-world outputs.\n- The framework assumes access to well-structured, domain-relevant papers for persona extraction—an assumption that may not hold in under-documented domains.\n- Correlation with human ratings is the only quantitative metric reported; qualitative validation of the debate process itself is limited.\n- Since the objective is maximizing correlation with human scores, it’s unclear whether MAJ-EVAL always captures better reasoning or merely reproduces human biases.\n- The paper doesn’t deeply analyze how or why agents converge during debates—leaving open whether the final scores reflect genuine consensus or averaging artifacts."}, "questions": {"value": "- How scalable is MAJ-EVAL when applied to large datasets or high-throughput evaluation pipelines?\n\n- Can the framework operate effectively when domain documents are scarce or noisy, for instance, in emerging or low-resource fields?\n\n- How sensitive are the results to the underlying base model (e.g., Claude vs. Qwen), and do persona representations transfer across models?\n\n- Did the authors analyze whether in-group debates ever lead to groupthink effects, where agents converge prematurely rather than explore diverse viewpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i5hfigfEb8", "forum": "Rx4RIf0bMO", "replyto": "Rx4RIf0bMO", "signatures": ["ICLR.cc/2026/Conference/Submission6324/Reviewer_uxh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6324/Reviewer_uxh7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915533901, "cdate": 1761915533901, "tmdate": 1762918620743, "mdate": 1762918620743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MAJ-EVAL, a framework for large-scale text evaluation using multi-agent debates among simulated stakeholder personas. It constructs evaluator personas derived from literature, lets them discuss through structured debates, and aggregates their final opinions. Experiments on creative writing and medical summarization benchmarks show that MAJ-EVAL’s aggregated judgments align more closely with human evaluations than single-model or self-consistency baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a timely contribution to LLM-based evaluation. I like that the authors automated agent generation step, which will improve replicability and objectivity. The results show improved reliability, making the approach conceptually novel and practically relevant for human-aligned evaluation design."}, "weaknesses": {"value": "Comment 1. The authors might want to check whether their persona extraction is valid. Several additional experiments could mitigate this concern:\n(1) The authors can randomly perturb the input corpus and track downstream ρ/τ changes against human ratings.\n(2) Although costly, the authors might consider recruiting several domain experts and asking them to verify the extracted dimensions. \nAdditionally, domain experts can rate persona faithfulness and coverage. The authors would then be able to report the precision or recall of model dimensions relative to expert lists.\n\nComment 2. Several recent papers (e.g., see Choi et al., 2025) show that multi-agent debate may lead to only incremental gains. It is important to understand where the authors’ performance gains come from—do they stem from multi-agent debates or simple aggregation?\nThe authors can report (i) no-debate means of independent agents, (ii) “read-others-but-no-discussion,” and (iii) self-consistency ensembling (multiple samples per agent, no debate).\nThe authors also need to report the token usage of each method so that users can assess the performance–cost trade-off (in addition to the discussion in A5).\nRef: Choi, Hyeong Kyu, Xiaojin Zhu, and Yixuan Li. \"Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?\" arXiv preprint arXiv:2508.17536 (2025).\n\nComment 3. Another interesting ablation could involve using the expert dimensions extracted from the first domain on the other domain (and vice versa). This would directly show the gains achieved by automating the agent (and persona) generation process.\n\nComment 4. I am not sure why the authors include demographic characteristics in their personas. This may inadvertently drive scoring beyond evaluative dimensions, especially if certain demographics correlate with stricter or more lenient judgments.\nAlthough unlikely, it is worth checking (at least on a subsample) (i) agents with no demographic characteristics and (ii) randomizing demographic characteristics only.\n\nComment 5. There are several missing key citations, including:\nKumar, Sandeep, Abhijit A. Nargund, and Vivek Sridhar. \"CourtEval: A Courtroom-Based Multi-Agent Evaluation Framework.\" Findings of the Association for Computational Linguistics: ACL 2025 (2025).\nKim, Alex, Keonwoo Kim, and Sangwon Yoon. \"DEBATE: Devil's Advocate-Based Assessment and Text Evaluation.\" arXiv preprint arXiv:2405.09935 (2024).\nDu, Yilun, et al. \"Improving Factuality and Reasoning in Language Models through Multi-Agent Debate.\" Forty-first International Conference on Machine Learning (2023).\nI recommend that the authors conduct a comprehensive literature review and include more relevant, recent citations.\n\nComment 6. The citation format should be changed. In-line citations should appear as (AAA, 2025), not AAA (2025)."}, "questions": {"value": "Please see above (weaknesses)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VCYN4HUDLx", "forum": "Rx4RIf0bMO", "replyto": "Rx4RIf0bMO", "signatures": ["ICLR.cc/2026/Conference/Submission6324/Reviewer_7tKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6324/Reviewer_7tKk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972444389, "cdate": 1761972444389, "tmdate": 1762918619692, "mdate": 1762918619692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **MAJ‑EVAL**, a two‑stage framework for “multi‑agent‑as‑judge” evaluation that aims to align LLM‑based automatic judgments with multi‑stakeholder, multi‑dimensional human evaluations in real‑world tasks. Stage 1 automatically constructs stakeholder‑grounded personas by extracting evaluative dimensions (with evidence) from domain documents and then instantiating rich persona profiles (demographics, specialty, psychological traits, social relationships). Stage 2 runs in‑group, free‑form debates among personas and aggregates their judgments into per‑dimension scores. The approach is evaluated on two domains: children’s QAG and medical multi‑document summarization, reporting higher correlations with human ratings than traditional metrics (ROUGE/BERTScore), single‑judge and existing multi‑agent baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Stakeholder‑grounded automatic persona creation.** The two‑step procedure (dimension extraction → persona instantiation with rich attributes) is novel in the evaluation context and increases face validity of agents’ judgments relative to ad‑hoc, hand‑written personas.  \n2. **Consistent human alignment on multiple dimensions.** Across StorySparkQA and MSLR‑Cochrane, MAJ‑EVAL achieves stronger correlations with human ratings than ROUGE/BERTScore, single‑LLM judges, and a prior multi‑agent baseline, especially on domain‑specific dimensions (e.g., Educational Appropriateness; Evidence Strength).  \n3. **Debate mechanism that improves judgments.** The in‑group free debate with a moderator and final aggregation is well‑motivated and empirically increases average correlations for most stakeholder groups (pre‑ vs. post‑debate).  \n4. **Reproducibility‑oriented reporting.** Prompts, pipeline summaries, and ablations are documented; the paper includes an anonymized code link and describes token budgeting and setup to aid reproduction."}, "weaknesses": {"value": "1. **Judge backbones are limited.** Experiments use **only two** judge LLMs (Qwen‑3‑235B and Claude‑3.7‑Sonnet). This leaves open whether the gains are robust across families/scales (e.g., Llama‑3.x, GPT‑4‑series, Mistral‑Large).  \n2. **Correlation differences lack uncertainty analysis.** Several comparisons rely on visual gaps in heatmaps/tables. Confidence intervals, statistical tests (e.g., Zou’s method for comparing dependent correlations), or bootstrap CIs would substantiate claims.  \n3. **Potential dependence on selected literature for persona construction.** The dimension‑extraction step may inherit biases/coverage gaps from the chosen documents. The paper would benefit from reporting selection protocols, diversity checks, and sensitivity to the literature pool.  \n4. **Ablations could be deeper.** While simple‑role vs. detailed persona and pre‑ vs. post‑debate are useful, the work lacks controlled studies on **persona count**, **group size**, **debate length/turn budget**, and **moderation strategies**.  \n5. **Cost and latency trade‑offs need fuller treatment.** The token costs reported are substantial; per‑instance cost and comparisons to cheaper single‑judge settings would help practitioners decide when MAJ‑EVAL is worth it.  \n6. **External validity still limited.** Only two domains/tasks are studied; transfer to other NLG settings (dialogue safety, instruct‑following, data‑to‑text) remains to be shown."}, "questions": {"value": "1. **Backbone robustness:** Can you add results with at least one *open* and one *closed* additional judge (e.g., Llama‑3.1‑70B‑Instruct, GPT‑4o‑mini) to test backbone‑agnosticism? (Sec. 4.4 shows only Qwen/Claude.) \n2. **Uncertainty reporting:** Please provide confidence intervals for key correlations and, where relevant, statistical tests for correlation differences (dependent correlations across methods on the same items).  \n3. **Literature pool & sensitivity:** How are domain documents collected for dimension extraction (inclusion/exclusion criteria, #papers, time ranges)? Any sensitivity analysis when random subsets of papers are used?  \n4. **Ablation on debate budget:** What is the effect of limiting debate turns, changing moderator heuristics, or varying agent counts per stakeholder group?  \n5. **Calibration & scale:** What exact rating scales are used internally by agents per dimension, and how are they normalized before aggregation (cf. 5‑point Likert in A.10 and 0–1 normalization in Table 5, p.19)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z7faM5ugGg", "forum": "Rx4RIf0bMO", "replyto": "Rx4RIf0bMO", "signatures": ["ICLR.cc/2026/Conference/Submission6324/Reviewer_Ta4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6324/Reviewer_Ta4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087249618, "cdate": 1762087249618, "tmdate": 1762918619227, "mdate": 1762918619227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of evaluating complex, real-world NLP tasks, which often require aligning with diverse human stakeholder perspectives. The authors argue that existing \"LLM-as-a-judge\" methods are limited by arbitrarily designed agent personas and a lack of generalizability.\n\nThey propose MAJ-EVAL, a two-stage Multi-Agent-as-Judge evaluation framework. The first stage, \"Stakeholder Persona Creation,\" aims to systematically ground agent personas by automatically extracting evaluative dimensions and perspectives from a provided set of domain-specific documents, such as research papers. The second stage, \"Multi-Agent-as-Judge Debate Evaluation,\" instantiates LLM agents with these grounded personas and engages them in an in-group debate, allowing them to challenge and refine their initial judgments before a final score is aggregated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a significant and practical problem: the need for scalable, multi-dimensional evaluation of NLP systems that aligns with diverse human stakeholders. The authors' motivation for moving beyond single-agent \"LLM-as-a-judge\" systems is well-articulated."}, "weaknesses": {"value": "*Missing Comparison to LLM-Generated Personas*\n\nThis is the most signifiant issue. The paper's core claim is that its document-grounded personas are superior to \"arbitrary\" ones. However, the authors fail to test this against a strong, obvious baseline. The ablation study only compares their method to \"simple role definition\" (e.g., \"You are a school teacher\") , which is an insufficient comparison. A proper baseline would be to **prompt the LLM to generate a detailed, expert persona** directly from its own **pre-trained knowledge**, which assumes that the documents used to define the personas are already seen by the LLM. The paper provides no evidence that its complex, document-extraction pipeline is more effective than this simpler, more direct approach.\n\n*Manual Document Selection Issue*\n\nThe entire \"Stakeholder Persona Creation\" stage —which is a core contribution—is highly dependent on the initial documents provided to the system. In Section 4.4, the authors state they manually selected \"three representative documents\" for one task and \"two\" for the other . This manual, subjective step introduces a major potential for bias. The framework is only \"automatic\" after this crucial manual selection.\n\n*Instability of the Debate Mechanism*\n\nThe paper frames the debate's outcome as a success, but the data in Section 5.3.2 could be interpreted negatively. The authors admit that for some groups, like the \"Language Researchers,\" the correlation with human ratings decreased after the debate. Their explanation is that the agents discussed new dimensions \"beyond those used in human ratings\". However, this shows the **debate is unstable and can cause agents to diverge** from the gold-standard evaluation criteria, which could be seen as a failure rather than a feature."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JlpTtCB8Rr", "forum": "Rx4RIf0bMO", "replyto": "Rx4RIf0bMO", "signatures": ["ICLR.cc/2026/Conference/Submission6324/Reviewer_v9ix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6324/Reviewer_v9ix"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762512360504, "cdate": 1762512360504, "tmdate": 1762918618796, "mdate": 1762918618796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}