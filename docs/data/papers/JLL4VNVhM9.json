{"id": "JLL4VNVhM9", "number": 691, "cdate": 1756772458045, "mdate": 1759898247352, "content": {"title": "Value Gradient Flow: Behavior-Regularized RL without Regularization", "abstract": "We study behavior-regularized reinforcement learning (RL), which encompasses offline RL and RL from human feedback (RLHF). In both settings, regularization toward a reference distribution (offline data in offline RL or the supervised-finetuned policy in RLHF) is essential to prevent value over-optimization caused by erroneous out-of-distribution extrapolation. Existing methods typically add distance or divergence penalties on the learning objective, which introduces optimization challenges and over-conservatism. In this paper, we propose Value Gradient Flow (VGF), a new paradigm for behavior-regularized RL. VGF formulates an optimal transport problem from the reference distribution to the optimal policy distribution induced by the value function. This problem is solved via discrete gradient flow, where value gradients guide particles sampled from the reference distribution. Our theoretical analysis shows that an implicit behavior regularization is imposed by controlling the transport budget. This formulation avoids unnecessary restrictions on the optimization problem, enabling better reward maximization. Moreover, VGF operates without explicit policy parameterization while remaining expressive and flexible, allowing adaptively test-time scaling by adjusting the transport budget. Extensive experiments demonstrate that VGF significantly outperforms prior methods, achieving state-of-the-art results on offline RL benchmarks (D4RL, OGBench) and challenging RLHF tasks.", "tldr": "A new approach to behavior-regularized RL without explicit regularization.", "keywords": ["behavior-regularized rl", "offline rl", "rlhf", "optimal transport"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ae5b531356782569ec11d7cdc644d1fe1270cee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, an offline reinforcement learning framework is formulated as an optimal transport problem that transfers the actions induced by the behavior policy to those induced by the optimal policy.\nThis transport process is modeled as a particle-based gradient flow, and the resulting action update rule is analogous to Stein Variational Gradient Descent (SVGD).\n\nIn existing methods, a regularization term is typically introduced into the objective function, and its coefficient must be carefully tuned to balance the trade-off between maximizing the expected return and enforcing behavior regularization.\nIn contrast, the proposed method controls the degree of regularization through the transport budget. Specifically, the number of steps and the learning rate.\n\nThe proposed approach is evaluated on continuous control tasks in D4RL and OGBench, as well as on LLM fine-tuning tasks using TL;DR and Anthropic-HH datasets.\nThe experimental results demonstrate that the proposed method outperforms existing approaches on these benchmark tasks.\nIn addition, a toy example illustrates that the method can successfully model a multimodal action distribution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Formulating offline RL as an optimal transport problem is both interesting and appears to be novel.\n\n- The experimental results clearly show the advantages of the proposed method in both continuous control and LLM fine-tuning tasks.\n\nAlthough the idea of using the Wasserstein distance as a form of regularization has been explored in prior offline RL studies, I am not aware of previous work that learns a gradient flow from the behavior policy to the optimal policy.\nIn this regard, the proposed method is novel and conceptually appealing."}, "weaknesses": {"value": "- The presentation is somewhat unclear in several places and would benefit from improvement.\n\n- The paper lacks any discussion of computational cost.\n\nThe claim that “VGF removes the need to balance the optimization conflict between reward maximization and deviation penalties” is somewhat misleading. While it is true that the proposed method eliminates the need to manually tune the coefficient of an explicit regularization term, it still requires tuning the transport budget, which effectively serves a similar role. Thus, balancing between reward maximization and deviation control remains necessary. Since similar statements appear multiple times throughout the paper, the authors should carefully revise these passages to avoid confusion.\n\nThe action update rule in Equation (7) is analogous to SVGD. However, although Liu & Wang (2016) and Liu (2017) are cited, the term SVGD itself does not appear explicitly. Explicitly mentioning SVGD would make the connection clearer and help readers understand the proposed algorithm more easily.\n\nAnother potential weakness of the proposed method lies in its computational cost. Action generation using the flow-based behavior policy introduces additional overhead, and the gradient flow from the behavior policy to the optimal policy also incurs computational expense.\nI expect that the proposed method would be significantly slower than TD3+BC and IQL, and possibly even slower than Diffusion-QL.\nHowever, the JAX-based implementation may help alleviate some of these costs. Discussing these computational limitations would strengthen the overall presentation and contextualize the method’s contributions."}, "questions": {"value": "- Please comment on the statement regarding the removal of the need to balance between reward maximization and deviation penalties. If you agree with my observation, please revise the paper accordingly.\n\n- Why is SVGD not explicitly mentioned in the paper? Please elaborate on the connection between the proposed method and SVGD.\n\n- How much training time does the proposed method require in practice? Please provide approximate wall-clock times and compare them against the baseline methods."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "I am also reviewing Submission 10969, titled “Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning.” \n\nI noticed that the idea presented in this paper is highly similar to that of Submission 691. Both submissions share the same main approach: a flow policy is initialized using a given dataset to model the behavior policy, and the transport from the actions generated by the initial policy to those of the optimal policy is learned. The optimal transport is modeled using the Jordan Kinderlehrer Otto (JKO) scheme. \n\nThere are, however, some differences between the two submissions. \nIn Submission 691, the update rule, which is analogous to Stein Variational Gradient Descent, is clearly described, and the regularization based on limiting the transport budget is explained. In addition, the gradient flow appears to be modeled using a single model. The method is evaluated on both locomotion and large language model (LLM) fine tuning tasks. \nIn contrast, in Submission 10969, the update rule is not clearly described, and the transport is modeled with step wise models. The method is evaluated on manipulation tasks. \n\nIn my view, the differences between these two papers are minimal. If they are authored by the same group, these submissions may constitute a dual submission of essentially the same work. However, if the authors are different, then this overlap is less concerning. As I do not have access to the author identities, I am simply raising a flag to alert the Area Chairs and Program Chairs to this potential issue. I have also raised a similar flag for Submission 10969."}}, "id": "P8gCQ19Ns7", "forum": "JLL4VNVhM9", "replyto": "JLL4VNVhM9", "signatures": ["ICLR.cc/2026/Conference/Submission691/Reviewer_Y9ry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission691/Reviewer_Y9ry"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760935192024, "cdate": 1760935192024, "tmdate": 1762915582872, "mdate": 1762915582872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes the Value Gradient Flow (VGF) for  behavior-regularized reinforcement learning. VGF formulates behavior-regularized RL as an optimal transport problem, guiding particles from the reference distribution (offline data or supervised fine-tuned policy) toward high-value regions via discrete gradient flow. The VGF framework demonstrates several merits including avoiding unnecessary restrictions on optimization problem, no need explicit policy parameterization and so on. Experiments on D4RL and other benchmarks demonstrate the superiority of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* It is appreciated that the authors combine the idea of optimal transport, a classic method in optimization, with RL situations. \n\n* The first strength of this manuscript is that it avoid optimization conflict between reward maximization and distribution penalty, mitigates the \"deadly triad\" problem in actor-critic algorithms, and enables more stable training.\n\n* The idea of applying the latent variable in generative modeling is appealing. It combines the good merits in generative modeling (dimension reduction and re-parametrization), and applies well in exploration and exploitation."}, "weaknesses": {"value": "1. We wonder whether the performance of VGF is limited by the reference distribution. When the reference distribution is skewed towards to suboptimal behaviors, how is the robustness of VGF?\n\n2. Particle-based gradient flow solving requires maintaining multiple particles and function calculations, do the authors consider such cost in practice? Especially in high dimension scenarios\n\n3. From the experiments, it seems the VGF still inferior to Guassian Policy or Diffusion policy in some D4RL, it is suggsted the authors provide additional explanations on this issue.\n\n4. Although penalty coefficient tuning is unnecessary, key hyper-parameters such as training steps and temperature still require manual task-specific adjustment, lacking adaptive selection mechanisms. We encourage the authors do more trial in this field."}, "questions": {"value": "See the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9U26UxXMY2", "forum": "JLL4VNVhM9", "replyto": "JLL4VNVhM9", "signatures": ["ICLR.cc/2026/Conference/Submission691/Reviewer_gCX9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission691/Reviewer_gCX9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807129189, "cdate": 1761807129189, "tmdate": 1762915582676, "mdate": 1762915582676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Value Gradient Flow, which solves the behavior regularized RL by initializing particles using the reference distribution and transporting the particles towards the target distribution, following the gradient of the value function. Compared to a vanilla value-guided MCMC, VGF also incorporates several other ingredients and techniques, such as the best-of-N refinement during evaluation and kernel-based affinity metrics. The evaluation of VGF is conducted on both offline RL and RLHF tasks. In Offline RL, VGF is used both for training and evaluation, while in RLHF, VGF is only used for test-time generation. The key findings are that 1) with proper hyperparameter configuration, VGF outperforms the flow-based actor critic method both on D4RL and OGBench; 2) On TL;DR and Anthropic-HH datasets, VGF seems to be efficient for alignment and outperforms baseline methods, including PPO, DPO by a large margin."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Instead of using a flow model to generate samples towards the target distribution, VGF instead first trains a flow model to initialize particles following a reference distribution and afterwards transports them towards high-valued areas. This approach is novel. \n\n2. Besides, the VGF framework seems to unify several existing practices. For example, using zero transportation steps corresponds to best-of-N sampling. Conversely, as the number of transportation steps approaches infinity, the VGF process effectively recovers gradient-based sampling methods, such as Langevin dynamics."}, "weaknesses": {"value": "1. VGF incurs a higher inference cost than traditional flow-based methods. This is because, in addition to the initial sample generation from the reference distribution, VGF requires a computationally intensive iterative process to transport and refine these particles toward high-density regions.\n\n2. Furthermore, VGF appears highly sensitive to its hyperparameters and can demonstrate inconsistent performance trends across different tasks. Consequently, applying VGF to a new problem often requires extensive, task-specific tuning to achieve optimal results."}, "questions": {"value": "1. In Table 3, is VGF using particles that the SFT model initializes? \n\n2. Could the authors provide a runtime analysis of both VGF and other diffusion-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BZE9Nc5xaS", "forum": "JLL4VNVhM9", "replyto": "JLL4VNVhM9", "signatures": ["ICLR.cc/2026/Conference/Submission691/Reviewer_A97B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission691/Reviewer_A97B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938008788, "cdate": 1761938008788, "tmdate": 1762915582530, "mdate": 1762915582530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the area of behavior-regularized reinforcement learning, specifically within the contexts of offline reinforcement learning (RL) and reinforcement learning with human feedback (RLHF). In response to the challenges of training instability and the difficulty of hyperparameter tuning in existing explicit constraint algorithms, the authors propose the Value Gradient Flow. This approach integrates particle-based gradient flow with value functions, utilizing multi-step gradient guidance to iteratively direct the policy towards regions with higher value distributions. Experimental evaluations on offline RL and RLHF datasets, including D4RL and OGBench, demonstrate that this method outperforms existing algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is clear and straightforward. The writing is fluent and easy to understand, effectively explaining how Value Gradient Flow is integrated into the behavior-regularized reinforcement learning problem.\n\n2. In my view, introducing Particle-based Gradient Flow into the offline reinforcement learning domain and using gradient-based guidance to iteratively shift the behavior cloning action outputs towards higher value regions is an innovative approach. The related experiments also validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. I suggest that the authors conduct experimental evaluations on more complex environments within the D4RL dataset (e.g., Adroit) as well as on the visual input V-D4RL, to better validate the generalization of the proposed method.\n\n2. In Algorithm 1, the authors present the Q-network learning via TD with the iterated $a^{L\\_{train}}\\_N$. However, could this lead to overly optimistic learning for the Q-network? In my opinion,  $a^{L_{train}}_N$ is prone to producing out-of-distribution (OOD) actions. What would the results be if the Q-network were first learned using methods like CQL or IQL, and then directly tested?\n\n3. Although the authors emphasize that VGF significantly reduces training costs, the paper lacks quantitative analysis, including but not limited to training time, inference time, and memory usage. Including these details would provide a more comprehensive evaluation of the entire algorithm.\n\n4. I recommend adding a column to each main experiment table to show the results when  $L\\_{test} = 0$, which corresponds to the best-of-N sampling method. This would better highlight the improvement brought by VGF, rather than being influenced by ensemble methods like best-of-N sampling or other factors.\n\n5. The citation for Diffusion-QL on line 376 is incorrect."}, "questions": {"value": "Please refer to the \"Weaknesses\" section, I will raise my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vsfZqIUvbR", "forum": "JLL4VNVhM9", "replyto": "JLL4VNVhM9", "signatures": ["ICLR.cc/2026/Conference/Submission691/Reviewer_DzjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission691/Reviewer_DzjR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991159502, "cdate": 1761991159502, "tmdate": 1762915582409, "mdate": 1762915582409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}