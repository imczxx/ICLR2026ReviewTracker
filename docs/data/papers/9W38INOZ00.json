{"id": "9W38INOZ00", "number": 1779, "cdate": 1756919150238, "mdate": 1759898187279, "content": {"title": "Learning Neuron Dynamics within Deep Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs) offer a promising energy-efficient alternative to Artificial Neural Networks (ANNs) by utilizing sparse and asynchronous processing through discrete spike-based computation. However, the performance of deep SNNs remains limited by their reliance on simple neuron models, such as the Leaky Integrate-and-Fire (LIF) model, which cannot capture rich temporal dynamics. While more expressive neuron models exist, they require careful manual tuning of hyperparameters and are difficult to scale effectively. This difficulty is evident in the lack of successful implementations of complex neuron models in high-performance deep SNNs.\n\nIn this work, we address this limitation by introducing Learnable Neuron Models (LNMs). LNMs are a general, parametric formulation for non-linear integrate-and-fire dynamics that learn neuron dynamics during training. By learning neuron dynamics directly from data, LNMs enhance the performance of deep SNNs. We instantiate LNMs using low-degree polynomial parameterizations, enabling efficient and stable training. We demonstrate state-of-the-art performance in a variety of datasets, including CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS. LNMs offer a promising path toward more scalable and high-performing spiking architectures.", "tldr": "Spiking Neural Networks are energy-efficient but limited by simple neuron models; Learnable Neuron Models address this by learning unique dynamics during training, reaching state-of-the-art performance.", "keywords": ["Deep Learning", "Neuromorphic Computing", "Spiking Neural Networks", "Energy Efficient Computing"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ac2ad44f31d1ac1868b852ea78211d8e1bf62d6.pdf", "supplementary_material": "/attachment/421f176cc580c4dff4759d73fdc2e9d1f386b769.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new neuron formulation for spiking neural networks (SNNs), called the Learnable Neuron Model (LNM). Unlike conventional Leaky Integrate-and-Fire (LIF) or other fixed neuron models, LNM allows the dynamics of each neuron layer to be learned directly from data during training. The authors instantiate this idea using low-degree polynomial parameterizations, trained via backpropagation with surrogate gradients. The approach demonstrates superior accuracy across multiple datasets, including CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet, with only a minor energy overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a parametric family of nonlinear integrate-and-fire models that unifies existing neuron formulations under a learnable framework.\n\n2. Experiments on standard datasets demonstrate the superior performance of LNM."}, "weaknesses": {"value": "1. While the authors visualize diverse learned neuron behaviors, there is no deeper analysis of how these dynamics contribute to improved performance and why certain layers benefit from non-LIF behavior.\n\n2. The model restricts f_θ(u) to low-degree polynomials and clips inputs to ensure stability, yet this also limits expressiveness. The impact of this design choice on the ability to model long-term temporal dependencies remains unclear. \n\n3. The paper focuses exclusively on convolutional SNNs. It does not investigate whether LNM dynamics generalize to spiking transformers or temporal sequence models.\n\n4. The paper does not compare existing spiking neuron models."}, "questions": {"value": "1. How does the choice of polynomial degree interact with the number of time steps?\n\n2. Could the authors conduct ablation studies to compare LNM against those fixed neuron models? \n\n3. How would LNM behave on temporal datasets (e.g., PSMNIST, Sequential CIFAR10), where temporal credit assignment is crucial?\n\n4. Could the authors clarify whether the learned neuron parameters generalize across architectures or must be retrained from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pZrMBYYx6Q", "forum": "9W38INOZ00", "replyto": "9W38INOZ00", "signatures": ["ICLR.cc/2026/Conference/Submission1779/Reviewer_ZGV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1779/Reviewer_ZGV9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539758756, "cdate": 1761539758756, "tmdate": 1762915888308, "mdate": 1762915888308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Update on Comment Status"}, "comment": {"value": "Dear Reviewers,\n\nThank you for your thoughtful comments and questions. We have been working on addressing each point in detail. Please expect our initial responses in the coming days. As some of the questions require additional experiments, those answers may take slightly longer to finalize.\n\nWe appreciate your patience and look forward to continuing the discussion.\n\nThank you."}}, "id": "AU2zuzgcHL", "forum": "9W38INOZ00", "replyto": "9W38INOZ00", "signatures": ["ICLR.cc/2026/Conference/Submission1779/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1779/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1779/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763413106884, "cdate": 1763413106884, "tmdate": 1763413123665, "mdate": 1763413123665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors use spiking neural networks (SNN) on several vision datasets (CIFAR10, CIFAR100, ImageNet, and CIFAR10-DVS).\n\nWhile most of the SNN community uses leaky integrate-and-fire (LIF) neurons, here the authors propose to learn more complex spiking neuron models using backprop. Indeed, the function f(u), which defines the neuronal dynamics, and which is equal to -u for a LIF neuron, is defined as a 3rd order polynomial whose coefficients are learned."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* To the best of my knowledge, this idea is new.\n* Using it, they improve the SOTA on CIFAR10, CIFAR100, and ImageNet."}, "weaknesses": {"value": "* The authors need to show that the learnable dynamics outperforms the LIF neuron, using exactly the same architecture, learning procedure, etc.\nFor example, the LIF accuracy + std could be added to Fig. 1.\n\n* On CIFAR10-DVS this paper reached a better accuracy (85.90% with T=10):\nFang W et al. (2023) Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies. NeurIPS 1:1–13 Available at: http://arxiv.org/abs/2304.12760.\nIt should be added to Table 2.\n\n* The energy estimation ignores memory accesses, whereas they dominate the budget!\nSee:\n\nLemaire, E., Cordone, L., Castagnetti, A., Novac, P. E., Courtois, J., & Miramond, B. (2022, November). An analytical estimation of spiking neural networks energy efficiency. In International Conference on Neural Information Processing (pp. 574-587). Cham: Springer International Publishing.\n\nDampfhoffer, M., Mesquida, T., Valentian, A., & Anghel, L. (2022). Are SNNs really more energy-efficient than ANNs? an in-depth hardware-aware study. IEEE Transactions on Emerging Topics in Computational Intelligence.\n\nMINOR POINTS:\n\n* Eq 4. The authors could precise that this corresponds to a \"hard reset\".\n\n* p6: \"However, we also observe increased variability as the degree increases.\"\nThis is not clear. The highest variability is for 2nd degree."}, "questions": {"value": "Would it make sense to include the input current I as an argument of the learnable f function in Eq. 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pYLGgb6feQ", "forum": "9W38INOZ00", "replyto": "9W38INOZ00", "signatures": ["ICLR.cc/2026/Conference/Submission1779/Reviewer_TxX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1779/Reviewer_TxX4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733391113, "cdate": 1761733391113, "tmdate": 1762915888188, "mdate": 1762915888188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Learnable Neuron Models (LNMs) for deep Spiking Neural Networks (SNNs), which parameterize neuron dynamics via low-degree polynomials to learn adaptive non-linear behaviors from data, achieving state-of-the-art performance on multiple datasets with minimal energy overhead compared to traditional LIF models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s proposed Learnable Neuron Models (LNMs) effectively address the limitations of fixed neuron models (e.g., LIF) in deep SNNs by parameterizing neuron dynamics with low-degree polynomials.\n\n2. LNMs exhibit strong flexibility in learning diverse neuron dynamics and align with biological plausibility.\n\n3. The proposed LNMs consistently outperform existing state-of-the-art methods across both static and neuromorphic datasets.\n\n4. This paper is simple and effective, allowing readers to easily identify its innovative points."}, "weaknesses": {"value": "1.  There are multiple writing irregularities violating academic norms: the Abstract has improper line breaks, transitions are abrupt without smooth bridging, and repeated minor errors like \"Equation equation\" exist in formula references.\n\n2.  The evaluation datasets are insufficient, and a broader range of evaluations should be conducted.\n\n3. The limited innovation in its neuron model design: its core improvement—adopting polynomial functions to parameterize the neuron’s intrinsic dynamic function is essentially a replacement of the learnable parameters in the PLIF model."}, "questions": {"value": "The authors note that complex fixed neuron models introduce extra complexity and hyperparameters, yet their proposed LNMs, which use polynomial parameterization, still introduce \"polynomial degree\" as a new hyperparameter, failing to fully avoid new hyperparameter introduction.\n\nPlease provide the specific calculation process for how this added complexity increases the energy consumption of SNNs trained with LNMs by 2–5.5% compared to the LIF neuron."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X6EsUtaMPq", "forum": "9W38INOZ00", "replyto": "9W38INOZ00", "signatures": ["ICLR.cc/2026/Conference/Submission1779/Reviewer_WbvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1779/Reviewer_WbvZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974908162, "cdate": 1761974908162, "tmdate": 1762915887972, "mdate": 1762915887972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Learnable Neuron Models (LNMs), a parametric formulation for neuron dynamics in Spiking Neural Networks (SNNs) that can be learned directly from data. The authors propose using low-degree polynomials to parameterize the membrane potential dynamics, enabling more expressive temporal behavior than fixed models like Leaky Integrate-and-Fire (LIF). The method is evaluated on multiple static and neuromorphic datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR-10-DVS) and shows good performance with minimal energy overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of learning neuron dynamics end-to-end is underexplored in deep SNNs, and this paper presents a clear and practical approach.\n\n2. The paper provides extensive experiments across multiple datasets and architectures, demonstrating consistent improvements over strong baselines.\n\n3. The use of low-degree polynomials and Horner’s method ensures that the approach is computationally efficient and hardware-friendly.\n\n4. The analysis of learned dynamics across layers (e.g., LIF-like, QIF-like, etc.) is insightful and supports the claim that diverse dynamics are beneficial."}, "weaknesses": {"value": "1.  The paper could consider adding pseudocode to more clearly and intuitively demonstrate the differences between the proposed model and LIF.\n\n2. There is a lack of discussion regarding related work on neuron models [1-3].\n\n3. Figure 1 shows that accuracy does not significantly improve as the Polynomial Degree increases.\n\n4. Tables 1 and 2 should include results for the LIF model obtained under the same training conditions and architecture as LNM, to allow for a direct and intuitive comparison of the performance improvement.\n\n5. A main conceptual diagram/figure summarizing the overall method is missing, which would help intuitively illustrate the entire workflow.\n\n[1]  \"TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] \"CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks.\" International Conference on Machine Learning. PMLR, 2024.\n\n[3]  \"Tc-lif: A two-compartment spiking neuron model for long-term sequential modelling.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 15. 2024."}, "questions": {"value": "1. How does the method scale with spiking-based transformer networks (e.g., [1])?\n\n2. Could the clipping operation limit the expressivity of the learned dynamics? Have the authors considered smooth alternatives (e.g., tanh)?\n\n3. The paper mentions challenges in surrogate gradient tuning for LNMs. Is there a plan to extend recent adaptive surrogate methods to LNMs?\n\n4. Could you provide a comparison of training time or convergence speed between LNM and LIF?\n\n[1] \"Spike-driven transformer.\" Advances in neural information processing systems 36 (2023): 64043-64058."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TDkw9y2aml", "forum": "9W38INOZ00", "replyto": "9W38INOZ00", "signatures": ["ICLR.cc/2026/Conference/Submission1779/Reviewer_XYhv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1779/Reviewer_XYhv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988005612, "cdate": 1761988005612, "tmdate": 1762915887723, "mdate": 1762915887723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}