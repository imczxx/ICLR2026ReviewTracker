{"id": "UXtTBAyqVB", "number": 3764, "cdate": 1757515753806, "mdate": 1759898071101, "content": {"title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking", "abstract": "Multimodal retrieval still leans on embedding-based models like CLIP for fast\nvector search over pre-computed image embeddings. Yet, unlike text retrieval\nwhere joint-encoder rerankers are standard, comparable vision–language rerankers\nare largely absent. We find that seminal joint encoders such as BLIP are severely\nbottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale.\nMotivated by this bottleneck, we introduce EDJE , an\nEfficient Discriminative Joint Encoder that precomputes vision tokens offline and\ncompresses them via a lightweight attention-based adapter, so online inference runs\nonly a compact joint encoder over a small set of visual tokens plus the text. EDJE\npreserves strong retrieval performance while drastically reducing storage and online\ncompute, enabling high-throughput inference. Specifically, EDJE processes 50k\nimage–text pairs/second while requiring 49kB of disk storage per image, matching\nprior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval.", "tldr": "We propose EDJE, an efficient vision–language joint encoder with token-compression that enables fast multimodal re-ranking, achieving up to 53× higher throughput while matching the accuracy of prior joint encoders.", "keywords": ["Multimodal Retrieval", "Vision–Language Models", "Joint Encoding", "Efficient Re-ranking", "Token Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40992fc4c0c53d96b70dd584b38d9632ad28a48b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of visual-language retrieval. More specifically, the paper focuses on the re-ranking stage of visual-language retrieval. To this end, the authors propose a new architecture, i.e., efficient discriminative joint encoder, to do the re-ranking efficiently. Besides, the authors propose a lightweight token-compression adapter that condenses vision features to reduce storage and computation. Experiments show the re-ranking method can consistently achieve a gain for the text-image retrieval performance. Analysis demonstrates the efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the idea of the paper. The design of the architecture enhances the efficiency of re-ranking in a smart way.\n\n2. Performance is good with efficiency. The experiments show the proposed method achieves a consistent performance gain in re-ranking, while being highly efficient.\n\n3. Clear presentation of the paper. The paper is clearly presented. The teaser figure is good, together with several highlighted 'key takeaways', making the important ideas of the paper very clear."}, "weaknesses": {"value": "1. Scope of the paper. Although the title of the paper is 'vision-language', the paper only studies text-image retrieval, and the video-text retrieval, which is a very important and more challenging task, has not been touched. It would be better if the paper can also include the results for video-text retrieval so that it is more general and will have a wider audience for the ICLR conference.\n\n2. Small issues: 'rerank' should be 're-rank' through out the entire paper."}, "questions": {"value": "Could the method also be applied to video-text retrieval and have the same conclusion as it is for text-image retrieval? This is very important concerning the scope and significance of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lthq5CCeid", "forum": "UXtTBAyqVB", "replyto": "UXtTBAyqVB", "signatures": ["ICLR.cc/2026/Conference/Submission3764/Reviewer_EbYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3764/Reviewer_EbYo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571066968, "cdate": 1761571066968, "tmdate": 1762916975304, "mdate": 1762916975304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDJE, an efficient discriminative joint encoder for large‑scale vision–language reranking. EDJE precomputes vision tokens offline, compresses them via a lightweight attention‑based adapter into a small set of tokens, and feeds these with text into a compact language model at query time. On Flickr30k (zero‑shot) and COCO (fine‑tuned), EDJE matches or approaches prior joint encoders while drastically reducing online latency and storage (e.g., 1.9-4.1 ms per 64-sample batch; 49 kB/image with 64 tokens). The paper includes ablations on token count, pool size, and training objectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Moves ViT compute offline; integrates vision via a compact joint encoder, addressing the main blocker for joint reranking.\n\n- Universal‑query adapter reduces hundreds of ViT tokens to tens while preserving accuracy; 64 tokens ($\\sim$49 kB/image) is a strong trade‑off.\n\n- EDJE consistently improves Flickr30k ZS R@1, e.g., CLIP ViT‑L/14@336: 67.7->81.9 (T2I).\n\n- With SigLIP2‑L/16@384, EDJE‑Local gets 87.8/96.5 (Flickr T2I/I2T R@1) at 4.14 ms (64‑batch), vs BLIP‑2's 88.6/96.9 at ~99 ms.\n\n- Objectives (ITM+MLM+ITC) and distillation help; pool‑size sensitivity is stable."}, "weaknesses": {"value": "- Latency excludes disk I/O/feature fetch; the 50k pairs/s figure from the abstract lacks a pipeline breakdown.\n\n- Only Flickr30k ZS and COCO FT; no tests under domain shift, web‑scale noise, or multilingual queries despite claims of modularity.\n\n- EDJE is competitive but not strictly superior to BLIP‑2 on R@1; the paper could more precisely state where it leads/trails.\n\n- Lacks comparisons to alternative compression/pooling (e.g., Perceiver‑style latents, strided token dropping) to justify the chosen adapter beyond results shown.\n\n- Storage numbers imply FP16 tokens; quantization (FP8/INT8) and its impact aren’t analyzed."}, "questions": {"value": "- Can you report end‑to‑end throughput/latency (including token fetch from SSD/NVMe, k‑reranking, and aggregation) to contextualize the 50k pairs/s claim?\n\n- How sensitive is performance to storage format/hardware (e.g., memory‑mapped arrays vs individual files; SATA vs NVMe vs network storage)? Any prefetching?\n\n- What is the effect of quantizing compressed tokens (FP8/INT8) on R@k and latency? Can training be made quantization‑aware?\n\n- Negative mining currently uses the same embedding model family; how does EDJE fare with cross‑model negatives to reduce bias toward the retriever geometry?\n\n- Beyond varying token count, can you vary adapter capacity (depth/width) to separate capacity vs token‑count effects at 32–64 tokens?\n\n- Any preliminary multilingual results (e.g., Crossmodal-3600) to support future-work claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WsDVxP29PG", "forum": "UXtTBAyqVB", "replyto": "UXtTBAyqVB", "signatures": ["ICLR.cc/2026/Conference/Submission3764/Reviewer_bquk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3764/Reviewer_bquk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719736198, "cdate": 1761719736198, "tmdate": 1762916975045, "mdate": 1762916975045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an Efficient Discriminative Joint Encoder (EDJE) to address the deployment challenges of vision-language reranking models in large-scale scenarios due to computational constraints. Multimodal retrieval typically incurs higher inference costs than text retrieval due to visual computation, making efficient interaction between textual and visual information a key factor in determining the practicality of multimodal retrieval models. The authors alleviate the aforementioned issue to some extent by further compressing visual information into a more compact token sequence, which proves effective according to the experimental results. However, the novelty of this work and the interpretability of the model require further clarification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The research focuses on the demand for efficient retrieval in the current multimodal retrieval field, which has strong practical significance. It proposes a vision-language reranking model that addresses the deployment challenges of multimodal retrieval joint encoders and provides new insights for multimodal retrieval methods. Experimentally, EDJE demonstrates competitive performance while significantly reducing storage requirements and online computational costs, achieving high-throughput inference. The core concepts of the paper are clearly defined, with no ambiguous expressions or logical contradictions."}, "weaknesses": {"value": "The proposed framework essentially follows the well-established two-stage retrieval paradigm, where visual features are pre-extracted offline and a lightweight joint model is used for re-ranking. While this design improves efficiency, it does not introduce a fundamentally new retrieval or representation mechanism. Similar strategies have been adopted in prior works such as LightningDOT [1] and VISTA [2]. The token-compression adapter in EDJE, as described, functions primarily to further compress visual encoder outputs into a compact set of tokens for offline storage and later consumption by the language model; this operational choice—offline compression of visual features into a small set of learned tokens—is conceptually similar to prior approaches such as TokenLearner [3] and the Q-Former used in BLIP-2 [4]. Consequently, the methodological novelty appears limited: EDJE mainly integrates established components (offline encoding, attention-based token compression, and a lightweight re-ranker) for an engineering trade-off between storage and online compute. \n\n[1] Sun, Siqi, et al. \"Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval.\" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.\n\n[2] Zhou, Junjie, et al. \"Vista: Visualized text embedding for universal multi-modal retrieval.\" arXiv preprint arXiv:2406.04292 (2024).\n\n[3]Ryoo, Michael, et al. \"Tokenlearner: Adaptive space-time tokenization for videos.\" Advances in neural information processing systems 34 (2021): 12786-12797.\n\n[4] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" International conference on machine learning. PMLR, 2023."}, "questions": {"value": "1. How does the token-compression adapter preserve or improve multimodal alignment between the compressed visual tokens and textual representations?\n\n2. If a different vision encoder is used, must the adapter be retrained and all previously stored tokens regenerated? Could the authors comment on the practical implications of this design choice?\n\n3. How is the semantic quality of the compressed visual features assessed? Would visualizations or qualitative analysis of the compressed tokens help to evaluate information retention?\n\n4. After compression by the adapter, how interpretable are the visual tokens? Are there methods to analyze or explain what visual information each token represents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gXP1uGChjk", "forum": "UXtTBAyqVB", "replyto": "UXtTBAyqVB", "signatures": ["ICLR.cc/2026/Conference/Submission3764/Reviewer_tFqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3764/Reviewer_tFqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796934093, "cdate": 1761796934093, "tmdate": 1762916974540, "mdate": 1762916974540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDJE, a system that makes image–text search faster and more accurate by running a small joint model on precomputed and compressed image features. Unlike existing models that are too slow to use in large-scale systems, EDJE speeds up inference by more than 50× while keeping similar accuracy. It improves search results across various models and datasets without requiring extra fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a practical method EDJE for image-text reranking by combining precomputed vision features with an cross-attention-based token compression adapter. The token compression design is well-motivated. \n- Empirical validation are conducted across multiple vision backbones and datasets. The proposed method achieves competitive or superior retrieval accuracy with drastically improved efficiency."}, "weaknesses": {"value": "Major weakness:\n- Although the performance is strong, the retrieval benchmarks are quite outdated. To mimc closer to real-world retrieval setting, I would suggest the authors to follow\n\n    [1] Sun, Siqi, et al. \"Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval.\" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.\n\n    This paper introduces retrieval across the full COCO/FLICKR dataset to better mimc real-world retrieval setting. \n\n    [2] Jiang, Ziyan, et al. \"Vlm2vec: Training vision-language models for massive multimodal embedding tasks.\" arXiv preprint arXiv:2410.05160 (2024).\n\n    This paper introduces a more realistic benchmark MMEB where the retrieval query and target are both multimodal.\n\n- In addition, the reranking system conceptually is similar to Lightningdot, although the implementation details are quite different and now inspired by latest advanced in VLMs. \n\n- EDJE drastically improves in zero-shot retrieval. However, on the fine-tuned COCO retrieval task, EDJE is only on par with prior models. One could argue that a larger-capacity joint encoder with more parameters might still have an advantage in the fine-tuned regime.The accuracy vs. model size trade-off is not fully explored in this paper. It would be helpful if the paper explore using a larger language model or more parameters in EDJE. \n\nMinor weakness:\n- All citations should use \\citep{} instead of \\cite{}\n- The term “local variant” vs. “token-compressed variant” is used to distinguish the full-token vs. compressed EDJE, but the term “local” isn’t very self-explanatory. If I understand correctly, it means the MLP adapter outputs the full local tokens without compression."}, "questions": {"value": "- As I pointed above, I would suggest the authors to expand the experiments to more realistic evaluation settings. \n- For token compression, what about the following baselines?\n    - Simple pooling (e.g., mean pooling)\n    - Direct token pruning techniques (like token clustering, attention pruning) on the vision tokens, similar to methods used in ViT efficiency papers, as an alternative to query-based compression.\n    - What if we adopt the pre-trained Q-former from BLIP and keep it frozen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DEpTGDZKZs", "forum": "UXtTBAyqVB", "replyto": "UXtTBAyqVB", "signatures": ["ICLR.cc/2026/Conference/Submission3764/Reviewer_kbcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3764/Reviewer_kbcQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921223336, "cdate": 1761921223336, "tmdate": 1762916973905, "mdate": 1762916973905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}