{"id": "NhU661EZ9C", "number": 19938, "cdate": 1758300807109, "mdate": 1759897011480, "content": {"title": "How much can language models memorize?", "abstract": "We propose a new method for estimating how much a model knows about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: unintended memorization, the information a model contains about a specific dataset, and generalization, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point \"grokking\" begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from 500K to 1.5B parameters and produce a series of scaling laws relating model capacity and data size to membership inference.", "tldr": "We estimate transformer capacity at 3.6 bits-per-parameter and show double descent occurs exactly when data size saturates model capacity", "keywords": ["memorization", "LLMs", "pretraining", "double descent"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf2c5137fd63afcb7ace9cd244feef9ea8df1923.pdf", "supplementary_material": "/attachment/5380f5fa8f2e7ca974cf29652f779fa710b85b5a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents a new definition of “unintended” memorization; in plain language: given a datapoint x and a target model M, and an oracle model M*, unintended memorization is (the likelihood of x under M) - (max[likelihood of x under M, likelihood of x under M*]) where M* is an arbitrarily larger model trained on a larger dataset. They use this definition, to then define the representational capacity of a given model as the maximum amount of unintended memorization for a model M across many different dataset sizes. Using this notion of model capacity, the authors show that “deep double descent” occurs when the dataset size is greater than model capacity. They then propose a scaling law for membership inference wrt model capacity and dataset size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper touches on interesting and related topics of: model capacity in terms of both datasets size and model size. They use the measure of “unintended memorization” to relate double descent phenomena to model capacity.\n2. Results were presented for both Toy datasets and real-world text dataset.\n3. I thought figure 6 in particular was interesting as it showed “extraction” of a test dataset…which the model has never seen. A compelling case that extraction-based definitions of memorization are not the best suited for the study of model capacity/generalization. (Lines 405-410 are interesting.)\n4. The analysis of precision’s effect on capacity was interesting in section 3.2. The empirically conclusion that doubling precision doesn’t double representational capacity is in line with the intuition that stochastic gradient-based learning algorithms do not optimally find solutions.\n5. They propose a scaling law for membership inference wrt model capacity and dataset size."}, "weaknesses": {"value": "1. I feel that some “findings” are very similar formulations of previously shown phenomena via a slightly altered experimental setup. While it is useful for the field to reinforce what is already known, I didn’t think some of the “key” contributions were as novel as they are presented in the paper:\n    1.  Figure 3 is interesting; a manner of contextualizing figure 3 in terms of prior work is that prior work has held dataset size constant and toggled model size, showing that model size plays a role in deep double descent [4]. Here, model size is held constant as dataset size is changed. Very similar conclusion from both experimental setups.\n    2. The result in figure 4: “We first observe that the sample-level unintended memorization increases with model parameters and decreases with training set size” has previously been observed in [2], albeit via an “extrability”-based definition of memorization. I note that [2] is missing from related work; I do think the scaling results and toy/production problem framing in [2] are similar enough to this work to warrant a discussion in related work.\n2. In figure 5, what does the color bar represent? Is it supposed to be the theoretical intended accuracy of membership inference?\n3. It seems there are some circular uses of notation (i.e., a piece of notation is used first and defined later). The most confusing instance of this for me was when defining “capacity” in all of section 3.1:\n   1. Line 232: Are you defining model capacity or capacity of a learning algorithm? How are they different? \n   2. You never explicitly defined model capacity, only learning algorithm capacity, but for the rest of the paper you refer to model capacity. \n   3. In your definition of learning algorithm capacity you use the notation “mem(X, L(X))” but only later on line 249 do you make it clear that you expect “mem$_U$(X, L(X)) ≈ mem(X, L(X))”.\n   4. It is also not clear to me from the definition on line 232 what is the set over which the “max” operation is being performed. Then “model capacity\" is loosely defined in terms of a learning algorithm; I think a more precise manner of definition capacity would have been by definition capacity in terms of trained model parameters and from the definition making it clear that the dataset size is changing.\n4. In order to better understand your empirical measure of unintended memorization, I attempt to clearly define the formula in plain language in a single line in my provided “summary”. Is this correct?\n5. Difficult/inconsistent variable notation and some typos:\n   1. The variables are not always clearly defined, for example in lines 251-257 the notation of “subscript i” is introduced to indicate variation in dataset I assume w/ respect to size, but this is not later clarified until the following paragraph.\n   2. Is there a missing character on line 141: should the corrected version be “I(X, Θˆ ) − I([X | Θ], Θˆ ) ”\n   3. Is there is missing character on line 210: should the corrected version be “max{p(x | ˆθ), p(x | θ)}”\n   4. Is there a typo on line 181: should it be:  mem$^K$( xi,ˆθ ) = I$^K$( xi,ˆθ) → missing subscript on x and swapped order for x/θ\n6. I see from line 210-212 that in practice the prior Θ is simply a larger reference model…this seams like a really clunky definition which is hinged on the manner in which the larger model was trained. It is almost a direct contradiction of your earlier stipulation that the definition of memorization should be agnostic of training algorithm (line 107) as by now using an additional model to enable approximation of “memorization” of a given model you are assuming that the additional model is trained similarly and has appropriately converged (you also make the implicit assumption that larger models have larger representational capacity which is something you later claim is a contribution finding – Fig 9). Circular reasoning?\n\n[1] Quantifying Memorization Across Neural Language Models. ICLR 2023.\n[2] Mitigating Memorization in Language Models. ICLR 2025.\n[3] Large Language Models Struggle to Learn Long-Tail Knowledge. ICML 2023.\n[4] Deep Double Descent: Where Bigger Models and More Data Hurt. ICLR 2020."}, "questions": {"value": "1. In section 2.2 you spend a lot of time theoretically defining intended memorization, but there is no practical discussion of what “intended memorization” is in sections 3-4(I might have missed this). Is the memorization ever really “intended” or are you rather attempting to quantify the phenomena that deep learning models cannot perfectly compress datasets, and as a consequence, smaller model sizes have lower representational capacities?\n2. For illustrative purposes, I will record a question during my initial paper read: “On line 125: can you elaborate on how you compute the “prior Θ”? You describe this prior as: “the underlying model that captures our dataset distribution X”; however, how do you actually compute this prior? Based on my understanding one can only attempt to approximate this prior by training you model parameters and recovering Θˆ. This leads to additional confusion in both formulas for mem$_u$ and mem$_I$ which make use of this prior? I am concretely asking: how do you compute: [X|Θ]?”...So after I continue reading, it becomes clear that mutual information is related to Kolmogorov complexity which is how you actually define memorization which is again approximated via likelihoods. In order to get to your exact formula for how to measure memorization (in section 3.2), you jumped through a lot of theoretical hoops. For a paper claiming that a core contribution was a new definition of memorization, it was really challenging for me to actually figure out the “actual” definition you proposed. I attempt to cleanly state the actual definition of unintended memorization you used in my paper “summary”; is this correct? If so, I suggest establishing this sooner rather than later. I personally didn’t find it helpful motivating this definition from a statistical perspective (section 2.1), because as you state, each theoretical perspective is simply a loose abstraction, rather than proven approximates (e.g., proposition 4 is never proven).\n3. Lines 358-361: you note that deduplication is important to measuring extraction rates ‘faithfully’. Can you elaborate? Why can’t you measure if a duplicated datapoint is memorized? It seems a bit limiting to make this assumption given that real-world datasets are not always deduplicated perfectly. Prior work has established that models are more likely to memorize data that appears in the training data more time (e.g., higher duplication = more memorization) [1,2,3]. \n4. Lines 294-295: “We estimate the capacity of each model as the maximum amount of unintended memorization in bits measured across all dataset sizes.” I am not sure this is an intrinsically motivated statement. Why is this notion of model capacity correct/reasonable? You later show in figure 3 an interesting relationship between model performance and “capacity” which is motivating, but this finding comes after you make this initial assumption about how to measure model capacity in the first place.\n\n[1] Quantifying Memorization Across Neural Language Models. ICLR 2023.\n[2] Mitigating Memorization in Language Models. ICLR 2025.\n[3] Large Language Models Struggle to Learn Long-Tail Knowledge. ICML 2023.\n[4] Deep Double Descent: Where Bigger Models and More Data Hurt. ICLR 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l09gJXkHtK", "forum": "NhU661EZ9C", "replyto": "NhU661EZ9C", "signatures": ["ICLR.cc/2026/Conference/Submission19938/Reviewer_NVE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19938/Reviewer_NVE1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760889143079, "cdate": 1760889143079, "tmdate": 1762932105377, "mdate": 1762932105377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The primary focus of the paper is to disentangle memorization from generalization in a language model. The authors differentiate between unintended memorization, which is the information a model contains about a specific dataset, and generalization, which is the information a model contains about the true data generation process. They consider training on pure random strings, with the goal of eliminating generalization -- the analysis results in estimating model capacity. The finding is that GPT models have capacity of approximately 3.6 bits per parameter. The paper additionally relates memorization with double descent, and find that unintended memorization decreases as models begin to generalize."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The interesting takeaway of the paper is a disentanglement between memorization and generalization, which prior measures of memorization overlook. Experiments are performed on both synthetic and real datasets. Several discussions are important to know, such as separating memorization from generalization, differentiating between sample-level memorization and dataset-level memorization, etc. The theoretical part can be improved, as explained below."}, "weaknesses": {"value": "The paper is hard to follow.\n\n- Figures are wrongly referenced throughout the main paper.\n- Line 100, computing $2^{100}$ is not a trivial and simple language task by any LLM. The example does not make sense as non-memorization.\n- There is no connection of the proposed definition of memorization with any existing measures. \n- Section 2.1 and 2.2 are hard to follow, including poor choices of notations. For example, $mem_U$ and $mem_I$, despite looking similar, have different arguments ordering. In later sections, $mem_U$ has two arguments, unlike three. The notations are not accompanied by intuition. There are typos (Line 141, 207).\n- The term \"grokking\" in the abstract is never explained formally, making it hard for a less familiar reader.\n- There is no discussion on takeaways. What are the potential implications of the work?\n- Generalization is poorly defined. A dataset containing random strings can also be generalized, where the language model attempts to generate a random token given any prefix. I think the authors imply that the generalization to random sequences is lower than any non-random text.\n- The distinction between $\\theta$ and $\\widehat{\\theta}$ needs more justification. How is memorization comparable between two models (or the same model with different parameters)? Important details are missing betwen Line 251 and 257 -- what is the mathematical form of $H^k(x^i \\mid \\widehat{\\theta_j})$. Also, how do you calculate $H(x^i)$?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VlJ4rdEM2p", "forum": "NhU661EZ9C", "replyto": "NhU661EZ9C", "signatures": ["ICLR.cc/2026/Conference/Submission19938/Reviewer_ivLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19938/Reviewer_ivLv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760913010928, "cdate": 1760913010928, "tmdate": 1762932104404, "mdate": 1762932104404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a formal, information-theoretic framework for quantifying memorization in large language models. The authors distinguish between unintended memorization which is storing information about particular data points stored in the model and generalization which is information about hte underlying data generation process. Their measure of memorization depends on Kolmogorov complexity, which is approximated using model likelihoods. One of the key findings was that memorization saturates when size of dataset exceeds model capacity which coincides with the rise of generalization and double descent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using Kolmogorov complexity to formalise memorization via model likelihoods is an interesting direction.\n2. Integrating memorization with double descent is also new and gives a different perspective.\n3. Understanding each phenomenon using synthetic data and real data is a convincing approach."}, "weaknesses": {"value": "1. The figures need to be improved. The markers with colors are not clearly distinguishable.\n2. Table 1 is never referred anywhere. So many figures that are present in appendix are referred for main results in the main paper.\n3. The figures (whichever in the main paper) are not present along with the section where they are mentioned e.g. figure 2 is in page 1, and is mentioned in page 7. \n4. Line 315 talks about how $\\alpha$ depends on the precision of language training. The readers have never been introduced with $\\alpha$ before this occurence. It is unclear which Figure or Table does that part of the result refer to. Presumably Table 1.\n5. There are some inconsistencies in experimental details. Line 261 says that models between 100K to 20M parameters were trained, while abstract says 500K to 1.5B parameters were trained. \n6. Abstract mentions about grokking as \"..at which point “grokking” begins, and unintended memorization decreases\nas models begin to generalize..\". There's no discussion related to grokking in the main paper, with a slight mention in limitations in the appendix. I assume double descent should have been mentioned in the abstract which is currently missing."}, "questions": {"value": "Same as Weaknesses and below.\n1. How are the authors differentiating between generalization as a whole and the second part of memorization? In line 71, when it says that \"We first eliminate the question of generalization entirely by training on a dataset of random uniformly-sampled bitstrings.\" - I assume that it's referring to the generalization as a whole.\n2. How different is your measure of unintended memorization from the exposure metric defined in [I] for unintended memorization.\n3. How do you form 100 models? Is it by combining values across layers, parameters, hidden dimensions? It will be good to have it mentioned in the paper.\n\n\n[I] Carlini, Nicholas, et al. \"The secret sharer: Evaluating and testing unintended memorization in neural networks.\" 28th USENIX security symposium (USENIX security 19). 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2OZmtpiCX3", "forum": "NhU661EZ9C", "replyto": "NhU661EZ9C", "signatures": ["ICLR.cc/2026/Conference/Submission19938/Reviewer_LdiA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19938/Reviewer_LdiA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400163744, "cdate": 1761400163744, "tmdate": 1762932103413, "mdate": 1762932103413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper separates memorization into two components: unintended memorization (the information a model contains about a specific dataset) and generalization (the information a model contains about the true data-generation process). Through synthetic experiments the authors establish a bound that GPT-style models have a capacity of approximately 3.6 bits per parameter which also holds true in real-data settings. The authors use Shanon and Kolmogorov information to distinguish between the two components and concretize them well in the paper."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Beyond having a solid experimental setup, the paper also reports several noteworthy findings (validated across multiple scales). Some key takeaways include:\n\n- When the dataset is sufficiently large, models reach an upper bound in net memorization, regardless of data size. Conversely, small datasets are fully memorized by models with adequate capacity.\n\n- Models trained using FP32 and BF16 show minimal differences in effective capacity.\n\n- The onset of double descent occurs precisely when the data capacity surpasses the model capacity.\n\n- Once the (deduplicated) dataset becomes large enough, all instances of successful training data extraction stem entirely from generalization."}, "weaknesses": {"value": "- Line 300-302: The results are quite different from the cited Physics of LM paper which estimates 2 bits per parameter. The authors call it a slightly larger estimate when it is a 75% jump. Any reasoning for the differences would strengthen the paper.\n\n- I am not able to interpret Figure 5. Can the authors add some explanation for the same?"}, "questions": {"value": "Nits -\n\nFig 9 caption - Do you expect the result to be same for BF16 and FP16? If yes, why? and if not then I think the more precise claim will be \"We estimate α = 3.64 bits-per-parameter for GPT models trained in BF16 precision\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WTI6jxgKK0", "forum": "NhU661EZ9C", "replyto": "NhU661EZ9C", "signatures": ["ICLR.cc/2026/Conference/Submission19938/Reviewer_b7vJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19938/Reviewer_b7vJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922265460, "cdate": 1761922265460, "tmdate": 1762932102709, "mdate": 1762932102709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Thanks to all reviewers for their detailed and helpful feedback. We have made many improvements to our paper, which is now much better-adapted for the ICLR format:\n\n- We have updated all references to tables and figures so that the paper remains self-contained at eight pages. We previously had referenced a number of contents that were in the appendix, since the original (non-ICLR) version of the paper had significant extra content. \n- A number of presentation issues (missing reference to Table 1 and “grokking”) came from this content condensation process, and should now be fully resolved.\n- We fixed several typos and moved figures closer to their point of reference.\n- We added experiments (referenced below) in float16 precision.\n- We updated all figures to have higher contrast and a white background. \n- We improved our intuitive examples around equations, especially in Section 2.\n\nThank you again for taking your time with this review. We hope this new version of our work will be easier to understand."}}, "id": "eaNvNNQl7e", "forum": "NhU661EZ9C", "replyto": "NhU661EZ9C", "signatures": ["ICLR.cc/2026/Conference/Submission19938/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19938/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission19938/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763761928729, "cdate": 1763761928729, "tmdate": 1763761928729, "mdate": 1763761928729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}