{"id": "0XIsA0PxJM", "number": 17932, "cdate": 1758282121311, "mdate": 1759897144201, "content": {"title": "Neural+Symbolic Approaches for Interpretable Actor-Critic Reinforcement Learning", "abstract": "The integration of neural networks into actor-critic frameworks has been pivotal in advancing the field of reinforcement learning, enabling agents to perform complex tasks with greater efficiency and adaptability. However, neural network-based actor-critic models remain opaque ``black boxes,'' concealing their decision-making processes and hindering their use in critical applications where transparent and explainable reasoning is essential. This work introduces an innovative adaptation of the actor-critic framework that unites neural networks with rule ensembles to tackle key challenges in reinforcement learning. We harness the computational power, scalability, and adaptability of neural networks to model the critic, while integrating a rule ensemble system for the actor, ensuring transparency and interpretability for decision-making. Our study establishes a theoretical foundation for integrating rule ensembles into the Advantage Actor-Critic (A2C) framework. Experimental results from seven classic and complex environments demonstrate that our proposed method matches or exceeds the performance of representative RL models, including symbolic methods, while offering self-interpretability and transparency.", "tldr": "", "keywords": ["Interpretable", "RL", "Symbolic"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eed9765b1838f40a3a3d1e30002b3b2b4d44e3c2.pdf", "supplementary_material": "/attachment/1bf474f4f7e319f94fa582ba462f92202327ae9c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a neuro-symbolic framework for interpretable reinforcement learning. The method relies on neural network function approximation to learn an expressive critic, while the actor follows a symbolic, rule-based design. The proposed symbolic actor can be trained end-to-end via Policy Gradient combined with Orthogonal Gradient Boosting, allowing it to discover symbolic rules automatically without relying on prior domain knowledge. The authors prove convergence under two-timescale assumptions, give algorithmic details (OGB + rule-replacement, pseudocode), and empirically evaluate NSAC across seven benchmarks, showing competitive performance in terms of return. The interpretability of the method is analyzed on two environments by examining the learned rules in the actor."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Since the interpretability of the actor stems from its intrinsic architecture, the proposed method avoids the challenges and limitations associated with fitting a post-hoc explainable actor to a RL-learned neural network actor.\n- Since the symbolic actor discovers rules automatically through end-to-end training, it doesn't require domain knowledge. The proposed approach is, to the best of my knowledge, novel and provides a principled approach for rule-based actor learning.\n- The paper provides a strong theoretical analysis and derivation of the rule-based actor learning procedure, proving convergence to the global optimum under mild assumptions.\n- The empirical evaluation is thorough.\n- The proposed approach performs on par even with non-symbolic SOTA methods.\n- The paper provides algorithmic pseudocode and promises the release of the full codebase, which strengthens reproducibility."}, "weaknesses": {"value": "- The actor update procedure is hard to follow. The connection between the PG update and OGB is difficult to grasp. The connection only became apparent to me after studying the algorithmic pseudocode in the appendix to understand the procedure. I strongly suggest improving the clarity of this procedure by outlining the key algorithmic steps. For example, how and when are equations (6) and (7) used? When are Algorithms (2) and (3) invoked?\n- The symbolic baselines are not discussed sufficiently. The paper should describe the key distinctions between them and the proposed method.\n- The environments used are under-specified. What are the state and action spaces and reward functions? Are any of the action or state spaces continuous?\n- The scalability of the method seems limited. The update procedure must be executed for each action separately, since the advantage of each action, $f_a(s)$ is modeled with a separate ensemble. \n- The human interpretability analysis is hard to follow. Figures 2 and 3 are not sufficiently explained. See questions section..."}, "questions": {"value": "- What is meant by the \"parameters\" of NSAC and SYMPOL (line 409)?\n- What exactly are the scores in Figures 2 and 3? Is the color coding the advantage function?\n- How is the advantage obtained from the scores that each rule provides, e.g. in the right column in Figure 2? For a given state, a number of rules hold, and the advantage for each action is obtained as the sum of scores given by all active rules?\n- Are the explanations that your method provides conceptually different from those in SYMPOL or other tree-based policies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uRDNJYOsfN", "forum": "0XIsA0PxJM", "replyto": "0XIsA0PxJM", "signatures": ["ICLR.cc/2026/Conference/Submission17932/Reviewer_gu5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17932/Reviewer_gu5z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559041687, "cdate": 1761559041687, "tmdate": 1762927743206, "mdate": 1762927743206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a hybrid neuro-symbolic extension of Actor-Critic, resulting in the \"NSAC\" framework (for \"Neural plus Symbolic framework for ActorCritic\"). Concretely, the proposed method retains neural network for the critic, but replaces the actor with a symbolic method in order to enhance interpretability while maintaining high-performance. The stated goal, alongside performance, is to achieve the proposed interpretability critieria from Murdoch et al. (2019) of \"simulatability, modularity, and low complexity\". In particular, the actor is replaced with what are called \"additive rule ensembles\", that estimate a variable by incrementally adding rules in the vein of boosting. The actor is implemented as a softmax over these additive functions, which are tasked with estimating the action-values for each action. The critic is a standard estimate of the value function implemented using a neural network, and is implemented using standard techniques. By contrast, the actor is updated by individually updating the query functions of the rule ensemble based on the gradient of the advantage of the policy (with a regularization term added to ensure local convexity). The paper then carries out a convergence analysis of NSAC subject to relatively standard conditions (step size annealing, ergodicity, smoothness). Lastly, an experimental evaluation is conducted, studying two distinct aspects of NSAC: First, the performance of NSAC is contrasted to a variety of baselines, and Second, an interpretability study is conducted, showcasing the rules underlying the decisions of the actor in MountainCar and an HVAC domain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "There is a lot to appreciate about this paper. It sets out with a very clear goal: extend Actor-Critic to retain its performance, while also ensuring interpretability. The premise to make use of the boosting-like additive rule ensembles is quite clever, and not something I have seen much of in deep reinforcement learning. I have a few questions on that aspect of the method that I return to below under the \"Questions\" box.\n\nOverall:\n1. Clear scope, and delivery on that scope. The claims mostly match the evidence in the paper (some caveats, see weaknesses).\n2. Relatively light-weight and simple algorithmic idea at its core: I can see this proposal having high potential for adoption given the popularity of actor critic methods at the moment, plus the demand for interpretable methods.\n3. Convergence analysis is interesting, and to my reading appears correct.\n4. The experimental design is thoughtful (some caveats, see weaknesses)."}, "weaknesses": {"value": "A few weaknesses stand out, though I suspect they can be fixed with some discussion and some iteration on the manuscript.\n\nW1: First, I find it puzzle that the actor is effectively implemented as a Q function (and thus, more of a critic) in its own right, with a softmax at the very end turning this into a policy, rather than a value estimator. This seems to deviate from the basic premise of Actor Critic, in that the Actor is allowed to specialize to behavior, whereas the Critic needs to learn about values---this specialization is an important piece of Actor Critic. For example, see recent work by Garcin et al (2025), who investigate the ways in which the representation of the Actor and Critic each specialize. I will end up posing this as a question below, but I wanted to call it out as a perceived mystery of the design of the method.\n\nW2: In some cases, claims do not exactly match the evidence. This is largely down to claims around interpretability. The start of the paper states \"We ... demonstrate that our rule-based policy satisfies the established interpretability criteria of simulatability, modularity, and low complexity\". While aspects of these criteria are touched on, for instance computational complexity is discussed around line 334, this thread is left largely untouched throughout the paper. In particular, the results discussion does not revisit these critieria specifically. A simple change to the paper will fix this: revisit simulatability, modularity, and complexity in the discussion, and tie together the evidence available that supports these claims (or, if there is none, soften the claim). If \"complexity\" in the intro refers to computational complexity, I would be explicit about that.\n\nW3: The presentation of the paper could be strengthened at points. I find the most confusing part to be the \"Detailed Rule Update:\" in section 4, but I'm actually not convinced it needs to be in the main paper. I believe the NSAC pseudo-code from Appendix B would be better suited to the main text.\n\nW4: The experimental results are slightly difficult to interpret. I find the table to be a difficult way to read the results (even with the color / bold). Alongside the table, I would encourage the use of standard learning curves with confidence intervals that can showcase the evolution of performance over time. Similarly, there is currently no precise description of what numbers are actually being shown---I only see in line 371 \"higher returns\". Are these the returns the last episode? Over the entire training run? The last k-episodes? Similarly, what are the $\\pm$ values? This is critical to include to interpret and reproduce the plot.\n\nW5: Lastly, I found the interpretability results to be mischaracterized by the section title, \"Human Interpretability Analysis\". This leaves the impression that you have conducted a user study to evaluate the level of interpretability actually afforded to people. Since this is central focus of the paper, I actually believe this is relatively important. While I can appreciate the visual representations provided by Figure 2 and 3, they are a much softer version of an interpretability analysis than one involving actual people. Additionally, I find Figure 2 extremely difficult to read and parse.\n\nOverall, I still believe this is a solid paper, but the above limitations should be addressed before publication.\n\nReferences:\n- Garcin, S., McInroe, T., Castro, P. S., Panangaden, P., Lucas, C. G., Abel, D., & Albrecht, S. V. _Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning_. ICLR 2025."}, "questions": {"value": "Q1: Why is the actor effectively a Q-function estimator? Does this not partially defeat the point of making use of a baseline in Actor Critic (see cited work by Garcin et al.)?\n\nQ2: How are simulatability, modularity, and complexity evaluated in your experiments (or analysis)?\n\nQ3: What, precisely, are the numbers (and variance-like terms) measuring in Table 1? \n\nQ4: Is there a reason to present performance results as a table rather than learning curves? I can appreciate you can fit more domains into one table than say one row of learning curves in a \\begin[figure}... block, but I believe learning curves will be far more useful to the reader.\n\nQ5: How might you make contact with the stated contribution about modularity, simulatibility, and complexity regarding the interpretability of your method?\n\n\nAnd, one minor typo:\n- Missing space around line 453: \"...viruses.In such\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eMXA7XW63T", "forum": "0XIsA0PxJM", "replyto": "0XIsA0PxJM", "signatures": ["ICLR.cc/2026/Conference/Submission17932/Reviewer_2xmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17932/Reviewer_2xmA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681888132, "cdate": 1761681888132, "tmdate": 1762927742539, "mdate": 1762927742539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes an interpretable RL algorithm (NSAC) built upon A2C framework. As a variant of A2C, NSAC consists of an actor and a critic, but it is distinct from baseline A2C in that an actor is built with an “additive rule ensemble”. As individual rules of the ensemble are symbolic and interpretable, NSAC can natively explain chosen actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "One notable point of this study is that the authors effectively make additive rules of the ensemble differentiable and propose updated rules. Thus, NSAC can be trained using a gradient descent or its equivalents. The manuscript also provides theoretical analysis of the training of the actor."}, "weaknesses": {"value": "At first, this straightforward idea sounded intriguing, but I am not convinced that NSAC is truly interpretable because the actor’s decisions are dependent on the critic, which is still a black box. Without knowing how the critic operates, can we explain how NSAC make its decisions? For instance, if adversaries inject some adversarial attack vectors to the critic in NSAC, they can manipulate the critic’s assessment of current state, and thus the actor would make its decision based on incorrect information. In this case, can we trust the explanation provided by the actor?"}, "questions": {"value": "As NSAC is tested against simple RL tasks, the actors’ decisions in this study can be easily modeled by additive rule ensembles. However, it is unclear to me if additive rule ensembles can also be applied to model more complex actors. Can the authors provide some insights into how generalizable NSAC can be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "evN6oFttvF", "forum": "0XIsA0PxJM", "replyto": "0XIsA0PxJM", "signatures": ["ICLR.cc/2026/Conference/Submission17932/Reviewer_oca3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17932/Reviewer_oca3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706174538, "cdate": 1761706174538, "tmdate": 1762927742034, "mdate": 1762927742034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel interpretable RL method based on actor-critics. The main idea is: Keep neural networks for the critic for accurate value estimation, and replace the neural actor with an interpretable one (but has been implemented e.g. by NUDGE, c.f. Weakesses). \nFor interpretability the authors chose rule ensembles, which learn to select and weight multiple symbolic boolean rules (e.g. x > y) for each action at the current state. The final advantage of each action is then computed by summing over all rule-weights. The action with the highest advantage is executed.\nNext to the definition of the actual methodology, the authors provide a local convergence proof and an empirical evaluation of the performance and interpretability.\nOverall, this work develops and evaluates a simple, but intriguing reinforcement learning algorithm that shows both good performance and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Combining a neural critic with rule-ensembles as actor allows for a good trade off between an accurate and flexible critic, and an interpretable actor. \nThe overall quality of the work is good and the methodology itself is well thought through. The visual representation of the learned rules help interpreting the policy. The work and paper have many avenues for improvements, detailed after, but constitute a sound research idea."}, "weaknesses": {"value": "The paper is overall comprehensible, but could be improved greatly by restructuring the paper a bit and providing further visualizations. For example, the detailed rule update in section 4.2, which is difficult to follow, would really benefit from a visualization or intuition. \n\nAdditionally, the authors may want to think about restructuring the paper by moving the theoretical convergence analysis to the appendix, and instead add the warm start idea + an analysis of the effect of the total number of rules to the main paper.\n\nIn detail, I advise the authors to follow this general structure for their research:\nThe way many readers (including myself) dive into a paper is by reading the title, abstract, then figures and table (with their captions). \nHence, I advise the authors to have a list of scientific research questions at the beginning of the experimental evaluation section (often denoted RQ1, RQ2, ... etc), that are each answered in different paragraphs. For example:\n* (RQ1) Is our framework outperforming existing baselines?\n* (RQ2) Does our framework allow for interpretable decision-making?\n* (RQ3) How important is module A ? (conducting an ablation study)\n\nFrom what I extracted, RQs would here be e.g.:\n\n* How does the algorithm perform compared to non-interpretable and interpretable methods? \n* Is the method interpretable?\n\nThe paper is overall comprehensible, but could be improved greatly by restructuring the paper a bit and providing further visualizations. For example, the detailed rule update in section 4.2, which is difficult to follow, would really benefit from a visualization or intuition. Additionally, the authors may want to think about restructuring the paper by moving the theoretical convergence analysis to the appendix, and instead add the warm start idea + an analysis of the effect of the total number of rules to the main paper.\nIn detail, I advise the authors to follow this general structure for their research:\nThe way many readers (including myself) dive into a paper is by reading the title, abstract, then figures and table (with their captions). \nHence, I advise the authors to have a list of scientific research questions at the beginning of the experimental evaluation section (often denoted RQ1, RQ2, ... etc), that are each answered in different paragraphs. For example:\n* (RQ1) Is our framework outperforming existing baselines?\n* (RQ2) Does our framework allow for interpretable decision-making?\n* (RQ3) How important is module A ? (conducting an ablation study)\nFrom what I extracted, RQs would here be e.g.:\nHow does the algorithm perform compared to non-interpretable and interpretable methods? Is the method interpretable?\n\nWhile the second order results are appreciated, they demonstrate that many results are completely insignificant as the benchmark used is saturated. **A baseline whose std range overlap cannot be considered as outperforming another one.** Hence, only experiments on MCart-v0 and HVAC-1Zone show that your method outperform the evaluated interpretable baselines (and not by a big margin, thus not very confidently). \nThe significance could be further improved by tackling more complex environments, and above all, **relational environments** e.g. non-symbolic/pixel-based ones, or at least discussing how this may be solved in future (e.g. use object-centric extraction methods -> e.g. OCAtari [4] for Atari games). Demonstrating the interpretability of a method on such environment, that necessitate relational reasoning, is way easier than on continuous control ones (such as cartpole).\nI strongly advise adding experiments on e.g. the object-centric Pong to verify if their method also allows detecting or prevent the misalignment problem that learning agents encounter on this environment [2,3]. This object centric environment is also already available in JAX: https://github.com/k4ntz/JAXAtari.\n\n\nA limitation section should be included (also including scalability with growing number of attributes and overall run time) of this method.\n\n\nThe authors chose not to share code upfront, which makes checking the methodology more complicated and reduces reproducibility.\n\n\nExtreme lack of related work:\nThere are 2 papers from 2024, and the rest is from 2021 or older. Much research on interpretable RL has been conducted and publish. I strongly advise the authors to look up this research and compare to this work in a dedicated Related Work section. I provide pointers to the existing relevant literature, that was mentioned above as well. Logic actors encoded in First order logic have also already been introduced in [5] and further improved in [6]. Attempts to use interpretable decision trees as policies have also been explored in other works than SYMPOL [7, 8, 9], and other form of interpretable baselines (programs or using LLM to help decode them) have also been explored. While comparing to all of them is tedious, at least mentioning these lines of work seem necessary to help the readers (and apparently the authors) to situate this work :\n\n[1] Di Langosco et al. \"Goal misgeneralization in deep reinforcement learning.\" International Conference on Machine Learning. PMLR (2022).\n\n\n[2] Delfosse et al. \"Interpretable concept bottlenecks to align reinforcement learning agents.\" NeurIPS (2024).\n\n\n[3] Delfosse et al. \"Deep reinforcement learning agents are not even close to human intelligence.\" arxiv (2025).\n\n\n[4] Delfosse et al. \"Ocatari: Object-centric atari 2600 reinforcement learning environments.\" RLJ (2024).\n\n\n[5] Delfosse et al. \"Interpretable and explainable logical policies via neurally guided symbolic abstraction.\" NeurIPS (2023).\n\n\n[6] Shindo, et al. \"BlendRL: A Framework for Merging Symbolic and Neural Policy Learning.\" ICLR (2025).\n\n\n[7] Bastani et al. \"Verifiable reinforcement learning via policy extraction.\" NeurIPS (2018).\n\n\n[8] Fuhrer et al. \"Gradient boosting reinforcement learning.\" arXiv (2024).\n\n\n[9] Kohler et al. \"Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning.\" arXiv (2024).\n\n\n[10]  Luo et al. \"End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations.\" ICML (2024).\n\n\nWhile the weakness section is extensive, I value the presented work and really believe that incorporating the provided feedback will make this work very valuable to ICLR readers. I would be eagger to revise my score if changes are operated."}, "questions": {"value": "* How well does the method scale when we have a larger action space (e.g. 16 actions)?\n* After equation (5), the regularization term does not exist anymore. What happened?\n* Can you explain for equation (6) and (7) how the different cases (a=a_t or not) influence the gradient and how the updates encourage/discourage taking specific actions? (e.g. will case 1 increase the likelihood of choosing the action in future if A is large?)\n* Are you willing to add a clarification/visualization of the detailed rule update?\n* How did you decide on the symbolic baselines to compare to?\n* Can you share your code up-front (e.g using an. anonymous git repo)?\n* Why/How is the local convergence proof useful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JdItmbRzlz", "forum": "0XIsA0PxJM", "replyto": "0XIsA0PxJM", "signatures": ["ICLR.cc/2026/Conference/Submission17932/Reviewer_VfDp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17932/Reviewer_VfDp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742632284, "cdate": 1761742632284, "tmdate": 1762927741585, "mdate": 1762927741585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}