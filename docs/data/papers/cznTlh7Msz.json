{"id": "cznTlh7Msz", "number": 17958, "cdate": 1758282436759, "mdate": 1759897142630, "content": {"title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "abstract": "Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. \nHowever, instruction-tuned dLLMs exhibit a critical vulnerability we term \\<eos\\> overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \\<eos\\> tokens. \nAlthough noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \\<eos\\> as both termination and padding, which concentrates probability mass on \\<eos\\> at later positions and propagates backward to trigger early termination. \nTo address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \\<eos\\> placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \\<eos\\> dominance. \nExperiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. \nMoreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical.", "tldr": "Rainbow Padding fixes &lt;eos&gt; overflow in diffusion LLMs, ensuring robust long-sequence generation with only 7 padding tokens.", "keywords": ["Discrete Diffusion", "Instruction Tuning", "NLP"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c196b1b76c1253e5bfdf1a550ba42ca8b250dba2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the <eos> overflow phenomenon in instruction-tuned diffusion-based large language models (dLLMs). The authors identify the root cause is that the <eos> is both used as padding token and sequence terminator, which lead to overestimated probability of <eos> at later positions. The authors propose Rainbow Padding to replace <eos> padding tokens with a cyclical sequence of distinct padding tokens. Experiments are conducted to prove that the Rainbow Padding gains overall improvements on various benchmarks with minimal training cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper defines and analyzes the <eos> overflow issue, which has practical implications for deploying dLLMs in instruction-following scenarios.\n2. Rainbow Padding is intuitive, easy to implement, and does not require architectural changes or complex decoding strategies.\n3. Comprehensive experiments are conducted on multiple benchmarks with insightful analysis."}, "weaknesses": {"value": "1. The paper lacks formal analysis or modeling of why Rainbow Padding works, especially in terms of training dynamics or probabilistic behavior.\n2. The use of 7 distinct padding tokens is empirically justified, but no principled method or adaptive mechanism is provided for selecting this number."}, "questions": {"value": "1. Have you considered adaptive methods for selecting the number of padding tokens?\n2. Will Rainbow Padding introduce additional bias in length control, such as over-generation or difficulty in terminating properly?\n3. Have you validate your methods on some larger LLM architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GUrEMllNCa", "forum": "cznTlh7Msz", "replyto": "cznTlh7Msz", "signatures": ["ICLR.cc/2026/Conference/Submission17958/Reviewer_rf99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17958/Reviewer_rf99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530839656, "cdate": 1761530839656, "tmdate": 1762927757535, "mdate": 1762927757535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a critical failure phenomenon in instruction-tuned diffusion LLMs (dLLMs), termed eos overflow. This phenomenon describes a performance degradation where a longer generation length leads to prematurely short or degenerated responses. The authors trace the root cause to the dual use of the eos token for both sequence termination and padding. To resolve this, the paper introduces Rainbow Padding, a simple yet effective method that reserves a single <eos> for termination and uses a cyclic sequence of <pad_k> tokens for padding. Experiments on models like LLaDA and Dream show that Rainbow Padding effectively eliminates this failure mode, substantially improving performance on reasoning and code generation tasks and restoring length robustness with minimal fine-tuning overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper defines and analyze a critical failure mode (<eos> overflow) in dLLMs. \n-  Rainbow Padding is a simple but effective solution. Its minimal computational overhead (demonstrated by the efficient LoRA adaptation) makes it a very practical method.\n- The experiments are thorough and convincing. The performance improvements on length-sensitive tasks like MATH (e.g., from 0.9% to 34.3% on LLaDA) provide evidence of the method's efficacy. The validation across multiple models, tasks, and decoding strategies demonstrates its robustness."}, "weaknesses": {"value": "- The evaluations on the length-sensitive MATH and GSM8K benchmarks were performed on randomly sampled subsets (>100 problems each) rather than the full test sets. I would like to see deterministic results on the full benchmarks, which may be helpful for future comparisons.\n- Alternative Padding Schemes: The paper argues that a single <pad> token would reintroduce the problem of probability concentration. Did you experiment with any non-cyclic, deterministic schemes or a simple random sampling of padding tokens from a small, dedicated set? A empirical comparison might further strengthen the case for the cyclic approach.\n- Generality Beyond Instruction-Tuning: Does the <eos> overflow issue also appear in pre-trained dLLMs as well? Is Rainbow Padding suitable for all stages of dLLM training, or specific for the instruction-tuning stage?\n- Choice of K: The ablation on K (Table 4) is very insightful, showing a plateau around K=7. Is there a theoretical or heuristic basis for choosing an optimal K? For instance, could it be related to the vocabulary size, model capacity? Or is this a hyperparameter that one must tune empirically for each model?"}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SsvXcEjGDa", "forum": "cznTlh7Msz", "replyto": "cznTlh7Msz", "signatures": ["ICLR.cc/2026/Conference/Submission17958/Reviewer_f5z9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17958/Reviewer_f5z9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794341477, "cdate": 1761794341477, "tmdate": 1762927756916, "mdate": 1762927756916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a method for improving instruction-tuned dLLMs. It should be of interest to anyone training such models. \n\n- Identifies and formalizes “eos overflow”: increasing the maximum generation budget paradoxically shortens outputs for IT’d dLLMs.\n- Traces the cause to dual use of eos as both stop symbol and padding during IT, which teaches strong tail-position priors for <eos>.\n- Shows how any-order decoding heuristics then select high-confidence tail positions early, predicting <eos> and triggering a backward termination cascade.\n- Proposes Rainbow Padding: keep a single eos for true termination and fill the padded tail with a deterministic cycle of K distinct pad tokens.\n- Demonstrates content-first decoding after the fix, robust gains across tasks/models/decoding rules, and fast convergence of the pad-loss.\n- Shows LoRA fine-tuning can retrofit existing IT models to eliminate length collapse."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- simple and effective: change in padding semantics + brief fine-tune yields large accuracy and length-robustness gains.\n- decoder-agnostic robustness: works with confidence/margin/entropy decoding and without block scheduling.\n- compelling mechanistic evidence: visualization of pad confidences, eos tail priors, and unmasking order link cause to effect."}, "weaknesses": {"value": "- some results use random subsets (e.g., MATH/GSM8K) and only two dLLM families; full-set, multi-seed reporting would strengthen claims.\n- limited head-to-head vs. stronger decoding-time fixes (e.g., calibrated eos priors, length-control objectives) or matched-budget AR models."}, "questions": {"value": "- are pads new vocab items or reserved rare tokens? How is contamination avoided in prompts/instructions?\n- does K=7 remain optimal at longer budgets (2k–4k tokens) and other corpora?\n- how does Rainbow compare to (i) eos suppression with calibrated thresholds, (ii) separate <pad> with loss masking only, (iii) weak length-control priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TYvcYBi1WY", "forum": "cznTlh7Msz", "replyto": "cznTlh7Msz", "signatures": ["ICLR.cc/2026/Conference/Submission17958/Reviewer_pwWJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17958/Reviewer_pwWJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917566690, "cdate": 1761917566690, "tmdate": 1762927756495, "mdate": 1762927756495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and analyzes a major failure mode in dLLMs known as eos overflow, where allocating longer generation lengths paradoxically causes models to terminate earlier or output streams of eos tokens. The authors trace the issue to the dual use of the eos token as both a true sequence terminator and padding during instruction-tuning, which biases the model toward premature endings. To address this, they propose Rainbow Padding, a simple yet effective fix that replaces repeated eos paddings with a cyclic set of distinct padding tokens, thereby distributing probability mass and decoupling padding from termination. Experiments across reasoning and code generation benchmarks show that Rainbow Padding eliminates early termination, improves accuracy and length robustness, and can be applied efficiently via lightweight fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Originality**: The paper is original in systematically identifying, naming, and diagnosing the critical eos overflow failure mode in diffusion LLMs.\n* **Quality**: The claims are supported by empirical evidence and thorough ablations, reliably demonstrating substantial performance gains across multiple models and benchmarks.\n* **Clarity**: The paper is well-written, logically structured, and explains complex ideas using descriptive names and visual aids.\n* **Significance**: This work addresses a reliability issue that previously hampered the practical utility of instruction-tuned dLLMs."}, "weaknesses": {"value": "- Evaluation inconsistency: The paper reports that “MATH and GSM8K use randomly sampled subsets (>100 problems each).” Such random sampling can lead to unstable or non-reproducible results. It is recommended to evaluate on fixed and standardized test sets (e.g., the MATH-500 subset) to ensure reproducibility and fair comparison across methods.\n\n- Insufficient experimental coverage: The experiments focus solely on fine-tuning from Base models. It remains unclear whether the proposed method generalizes to instruction-tuned models (e.g., LLaDA-Instruct). Additional experiments on such models would strengthen the claim of universality and demonstrate the method’s applicability to real instruction-following settings."}, "questions": {"value": "The current analysis presents $K=7$ as a sufficient number of padding tokens, but the paper does not fully detail a principled or efficient method for determining this optimal $K$ valuefor dLLMs. We ask the authors to provide a clearer discussion or heuristic on how the smallest effective value of $K$ can be efficiently determined. Furthermore, please clarify whether the saturation point observed for LLaDA is expected to be a universal property that can generalize to other dLLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yDzkCt9Cgy", "forum": "cznTlh7Msz", "replyto": "cznTlh7Msz", "signatures": ["ICLR.cc/2026/Conference/Submission17958/Reviewer_29oR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17958/Reviewer_29oR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932067073, "cdate": 1761932067073, "tmdate": 1762927755934, "mdate": 1762927755934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}