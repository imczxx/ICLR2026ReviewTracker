{"id": "LElJH6EYBq", "number": 11284, "cdate": 1758195239393, "mdate": 1759897596125, "content": {"title": "AudioMoG: Guiding Audio Generation with Mixture-of-Guidance", "abstract": "Guidance methods have demonstrated significant improvements in cross-modal audio generation, including text-to-audio (T2A) and video-to-audio (V2A) generation. The popularly adopted method, classifier-free guidance (CFG), steers generation by emphasizing condition alignment, enhancing fidelity but often at the cost of diversity. Recently, autoguidance (AG) has been explored for audio generation, encouraging the sampling to faithfully reconstruct the target distribution and showing increased diversity. Despite these advances, they usually rely on a single guiding principle, \\textit{e.g.}, condition alignment in CFG or score accuracy in AG, leaving the full potential of guidance for audio generation untapped. In this work, we explore enriching the composition of the guidance method and present a mixture-of-guidance framework, AudioMoG. Within the design space, AudioMoG can exploit the complementary advantages of distinctive guiding principles by fulfilling their~\\textit{cumulative benefits}. With a reduced form, AudioMoG can consider parallel complements or recover a single guiding principle, without sacrificing generality. We experimentally show that, given the same inference speed, AudioMoG approach consistently outperforms single guidance in T2A generation across sampling steps, concurrently showing advantages in V2A, text-to-music, and image generation. These results highlight a “free lunch” in current cross-modal audio generation systems: higher quality can be achieved through mixed guiding principles at the sampling stage without sacrificing inference efficiency. Demo samples are available at: \\url{audiomog.github.io}.", "tldr": "", "keywords": ["diffusion models", "text-to-audio generation", "video-to-audio generation", "classifier-free guidance", "mixture-of-guidance"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fdb129b2dbd8786e53ee10059504a2499fb50930.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work explores enriching the composition of the guidance method and presents a mixture-of-guidance framework, AudioMoG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The mixture of guidance seems feasible and important.\n2. The writing and visualization are good.\n3. The results seem promising,"}, "weaknesses": {"value": "1. Evaluation: I’m not sure why there are so many “/” entries in Tables 1 and 2 for clearly open-source models (e.g., stable-audio-open). Reproducing the evaluation should be straightforward. It’s difficult to attribute any real performance gains relative to these baseline models.\n2. Minor performance gains: For text-to-audio generation, the improvements appear quite small, typically around 0.01.\n3. Motivation for mixture of guidance (MoG): MoG seems to be a general method applicable to diffusion-based generation across modalities. I don’t see what specifically motivates its application to audio versus images or other modalities. Does AudioMoG include any audio-specific adaptations?\n4. Figure 2: It looks like AG performs as well as, or better than, HG. The statement “Hierarchical Guidance eliminates outliers and provides more controllable condition alignment” is unclear. Please clarify what “outliers” and “condition alignment” refer to and how they are measured.\n5. Figure 3: The figure only shows curves for CFG and HG. Why not include AG and PG for a complete comparison?"}, "questions": {"value": "My concern is mainly about the comparison and the performance. And I will reconsider my score if they are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w021rgmq1u", "forum": "LElJH6EYBq", "replyto": "LElJH6EYBq", "signatures": ["ICLR.cc/2026/Conference/Submission11284/Reviewer_3hnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11284/Reviewer_3hnZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760513685359, "cdate": 1760513685359, "tmdate": 1762922435024, "mdate": 1762922435024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a mixture-of-guidance (MoG) framework that integrates classifier-free guidance (CFG) and auto guidance (AG) to improve text-to-audio, video-to-audio, text-to-music, and class conditional image generation performance. Through the experiments, the hierarchical guidance (MoG-HG) that the authors present and parallel guidance (MoG-PG) show better performance than CFG on objective metrics."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Through extensive experiments, this work demonstrates that a linear combination of CFG and AG shows better performance compared to using only CFG or only AG.\n- Furthermore, this improvements are confirmed across multiple tasks: text-to-audio, video-to-audio, text-to-music, and class conditional image generation."}, "weaknesses": {"value": "In the proposed MoG, the hierarchical guidance (MoG-HG), which should be presented as the primary contribution, does not move beyond the framework of a linear combination of the scores used in CFG and AG. This limits it to being an incremental extension from exsisting framework. Furthermore, MoG-HG shows only a slight numerical improvement over MoG-PG.\n\nMoreover, the MoG-PG appears to be merely a more detailed experimental study of tuning CFG and AG that reported in the appendix of ETTA [1]. The reviewer acknowledges that the comprehensive experiments validates that MoG-PG leads better results against CFG-only and AG-only baselines, but the contribution to the research field is limited.\n\nAdditionally, the paper does not fundamentally solve the non-trivial challenge of obtaining a degraded weak model for AG, a problem the authors point out in the Introduction and the \"AG effects\" paragraph of Section 3.1.\n\nFinally, for MoG-HG, it would seem straightforward to apply it to open-sourced pretrained models, which the authors do in the image generation experiments by using EDM2. However, it is unclear why the authors chose to train a new model from scratch and validate the effectiveness of MoG-PG and MoG-HG only on that specific model on text-to-audio, video-to-audio, and text-to-music generation experiments.\n\n[1] Lee, Sang-gil, et al. \"ETTA: Elucidating the Design Space of Text-to-Audio Models.\" ICML 2025."}, "questions": {"value": "- Can MoG-HG demonstrate superior performance compared to its MoG-PG counterpart when applied to pretrained models like AudioLDM2 (or AudioLDM), Stable Audio Open, ETTA, and MMAudio [2]? Corresponding potential weak degraded models exist for the AudioLDM series, Stable Audio Open, ETTA, and MMAudio. If the authors can show that MoG-HG outperforms MoG-PG using these respective weak models, it would substantially strengthen the experimental validity of the proposed method.\n\n[2] Cheng, Ho Kei, et al. \"MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis.\" CVPR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AsI63jo4bj", "forum": "LElJH6EYBq", "replyto": "LElJH6EYBq", "signatures": ["ICLR.cc/2026/Conference/Submission11284/Reviewer_MdQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11284/Reviewer_MdQx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506791092, "cdate": 1761506791092, "tmdate": 1762922434102, "mdate": 1762922434102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents comprehensive analysis on the impact of the inference time guidance for diffusion-based audio generative models. More specifically, \n- Task: Text2audio (TTA) and Video2audio (Foley). NOTE: Text2music and image generation results are only presented in Appendix.\n- Audio generative backbone: a newly trained model based on the Stable-audio-open architecture\n- type of guidances: Classifier-free (CFG) and Autoguidance (AG)\n- Contribution: Verifying that\n    1. The combination of CFG and AG improves audio generation performance\n    2. (Not clear from the paper, just my guess) A checkpoint saved at earlier training steps (e.g., a checkpoint saved at 100k steps when the full training is 1M steps) can serve as the bad model\n\nI believe the work presents findings that are beneficial to the audio generation community. However, the contributions and findings are within a very limited scope and seem to be incremental. For example, in StemGen https://arxiv.org/abs/2312.08723, authors present a new method to achieve stem-wise music generation, and to improve the performance, they proposed to combine Multiple CFG. In audio domain, the combination of multiple guidance is usually **a part of a new method**, but NOT a new method itself.\n\nThe authors seem to have chosen out-of-date methods to compare with, such that this paper cannot present the frontier in audio generation community.\n\nAs a reviewer, I tend to reject this paper."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Showed that the combination of CFG and AG at inference time enhances audio generation performance"}, "weaknesses": {"value": "## Limited Scope\n- The contributions and findings are within a very limited scope and seem to be incremental. In audio domain, the combination of multiple guidance is usually **a part of a new method**, but NOT a new method itself. For example, \n- in StemGen https://arxiv.org/abs/2312.08723, authors present a new method to achieve stem-wise music generation, and to improve the performance, they proposed to combine Multiple CFG. \n- Similarly, see that multiple guidance is a part of JASCO https://arxiv.org/abs/2406.10970\n## Potentially inaccurate theoretical description\n- According to Appendix F.3 and F.4, the audio generation backbone seems to be very similar to Stable-Audio-Open\n    - Stable-Audio-Open uses v-diffusion, where the training target is \"velocity\"\n    - In Section 2 formula (2), I don't see \"velocity\".\n    - Please clarify which training loss is used\n- In formula (8), the summation of weights should be 1.\n    - In the remaining part of this paper, obvious this constraint is not used. Please clarify if this constraint is actually used or not.\n## Out-of-date models in Table1 text2audio\n- It is strange to exclude Make-an-audio-2 from the table, when Make-an-audio-1 is included.\n    - The FAD of Make-an-audio-1 seems to be wrong. In Make-an-audio-2 paper, the FAD for ver.1 is 2.66 not 1.61.\n- Methods published in 2025, such as SoundCTM https://arxiv.org/abs/2405.18503, AudioTurbo https://arxiv.org/abs/2505.22106, are ingored.\n- Stable-Audio-Open is open-source, but the authors leave many metrics blank. These metrics can be measured by inferrring with the open model weight.\n## Out-of-date models, less proper metrics, and less proper model design in Table3 video2audio foley task\n- MMAudio is ignore, which is hard to understand, if we consider the impact of this model.\n- Since the foley function of AudioMoG is achieved by finetuning the TTA version, it would be good to mention other methods that are also built upon a pretrained TTA model, such as CAFA https://arxiv.org/abs/2504.06778 or SpecMaskFoley https://arxiv.org/abs/2505.16195\n- The evaluation in table3 is based on DiffFoley, an out-of-date model. \n    - The DeSync metric used in MMAudio can be a better candidate. For example, Align acc. says DiffFoley and FoleyCrafter is BETTER-THAN-GT, which does match their actual performance, see MMAudio demo page https://hkchengrex.com/MMAudio/video_main.html#\n    -   Meanwhile, DeSync metrics used in MMAudio and SpecMaskFoley seems to align better with human perception\n- While AudioMoG uses CLIP for video synchronization, it has already been shown ineffective in prior works. V-AURA, Multi-Foley, MMAudio, and SpecMaskFoley all showed that other video features can be a better alternative"}, "questions": {"value": "## Potentially inaccurate theoretical description\n- According to Appendix F.3 and F.4, the audio generation backbone seems to be very similar to Stable-Audio-Open\n    - Stable-Audio-Open uses v-diffusion, where the training target is \"velocity\"\n    - In Section 2 formula (2), I don't see \"velocity\".\n    - Please clarify which training loss is used\n- In formula (8), the summation of weights should be 1.\n    - In the remaining part of this paper, obvious this constraint is not used. Please clarify if this constraint is actually used or not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j2cB07Vi3u", "forum": "LElJH6EYBq", "replyto": "LElJH6EYBq", "signatures": ["ICLR.cc/2026/Conference/Submission11284/Reviewer_RNen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11284/Reviewer_RNen"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950055060, "cdate": 1761950055060, "tmdate": 1762922433637, "mdate": 1762922433637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AudioMoG, a mixture-of-guidance (MoG) sampling framework for cross-modal audio generation (T2A, V2A), with additional results on text-to-music and conditional image generation. The central idea is to combine two established guidance families—classifier-free guidance (CFG) and autoguidance (AG)—either hierarchically (HG) or in parallel (PG). The authors argue that CFG primarily boosts condition alignment but can reduce diversity, while AG improves score estimation via a weak/“bad” model. They show (i) a simple linear-combination view that subsumes CFG/AG and a hierarchical combination that yields “cumulative benefits,” (ii) a small theoretical equivalence (embedding AG into CFG vs. CFG into AG), and (iii) empirical gains under matched inference cost (NFE) on AudioCaps and VGGSound, plus T2M and ImageNet-512. The motivation aligns with recent analyses of guidance trade-offs and the AG line of work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Training-free and practical: Improves quality at sampling time without retraining.\n\n2. Consistent empirical gains under matched NFE across T2A and V2A, with additional evidence on T2M"}, "weaknesses": {"value": "Marginal conceptual novelty. The core mechanism is essentially weighted combinations of existing guidance signals (CFG terms and weak/strong contrasts) with a hierarchical schedule. This overlaps with prior AG and CFG reinterpretations that already discuss predictor/corrector views and weak-model contrasts; AudioMoG’s novelty is mostly in the composition policy and empirical validation on audio."}, "questions": {"value": "How does a single set of (w_1, w_2, w_3) transfer across datasets without re-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBqrikdbYw", "forum": "LElJH6EYBq", "replyto": "LElJH6EYBq", "signatures": ["ICLR.cc/2026/Conference/Submission11284/Reviewer_1Maf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11284/Reviewer_1Maf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981783846, "cdate": 1761981783846, "tmdate": 1762922433032, "mdate": 1762922433032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}