{"id": "KJkC2pwSXy", "number": 14202, "cdate": 1758230226423, "mdate": 1759897384076, "content": {"title": "Bootstrapping Zero-Shot Reasoning in Small Language Models via Advantage-Weighted Self-Distillation", "abstract": "Small language models (0.5B–3B) typically lack mathematical reasoning ability, often scoring near 0% on tasks they can solve with few-shot demonstrations. Existing approaches rely on thousands of supervised chain-of-thought (CoT) traces or complex multi-round self-distillation pipelines. We introduce Advantage-Weighted Direct Preference Optimization (AWDPO), a lightweight alignment method that bridges the gap between few-shot and zero-shot reasoning. Unlike prior approaches, AWDPO formulates training as a single-pass preference optimization objective that aligns a model’s zero-shot distribution with its own few-shot behavior. Our loss combines an advantage-weighted preference term with a dynamic MLE anchor, yielding stable training and implicit trust-region regularization.\n\nOn GSM8K, AWDPO transforms Qwen-2.5 base models (0.5B–3B) from 0% to 39%–77% accuracy, recovering over 90% of a supervised fine-tune that uses 7,473 CoT traces — a 1,750× reduction in CoT data. The method generalizes to SVAMP, ASDiv, and MATH500, where AWDPO recovers up to 90% of supervised CoT performance. Our analysis shows that AWDPO is equivalent to a Kullback-Leibler (KL)-constrained policy improvement step under projected DPO. These results demonstrate that small base models can substantially improve their mathematical reasoning ability from minimal supervision, providing a principled and data-efficient alternative to supervised CoT or Reinforcement Learning (RL)-based methods for mathematical reasoning.", "tldr": "AWDPO is a self-distillation method that trains small learning models by using their own few-shot outputs as teachers for zero-shot behavior, yielding near supervised chain of thought accuracy with lower data requirements", "keywords": ["Chain of thought", "Data efficient fine-tuning", "Preference Learning", "Mathematical Reasoning", "Small language models", "Direct Preference Optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16769705504bedf15af7c2389b73d3a2c31f3548.pdf", "supplementary_material": "/attachment/3509718497337d95a314dd7a7c43e40796c6966d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Advantage-Weighted Direct Preference Optimization (AWDPO), a data-efficient self-distillation method that bootstraps zero-shot reasoning in small language models by training them to prefer their own advantaged, few-shot-prompted outputs over their zero-shot ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The AWDPO method proposed in this paper is intuitive and elegant. I particularly appreciate the approach of using the well-performing rule-based reward from RLVR as a weight for the DPO logits. The final form of the preference loss (Line 198) resembles the Policy Gradient in REINFORCE. I wonder if the authors have analyzed the potential connection between AWDPO and Policy Gradient?\n\n- AWDPO performs well within the experimental scope covered in the paper. The DPO LoRA training, using self-distillation data collected from just 4-shot CoT golden responses, achieves results that approach the performance of full-parameter SFT trained on 7K+ golden responses."}, "weaknesses": {"value": "- Although the authors acknowledge this limitation, I must point out that the experimental scope of the paper is very small. It only involves three small models from a single series (Qwen2.5) and are trained on only one data source (GSM8k). While I understand that expanding the scope would consume more computational resources, having both the number of model series and data sources limited to one weakens my confidence in the general usability of AWDPO.\n\n- I would like to know in what scenarios we should use AWDPO to train our Base LLM. \n    - If the objective is to save computational resources: AWDPO requires DPO LoRA training. For small models, how much memory and computational power does this save compared to full-parameter SFT? If the savings are not substantial, why wouldn't one just use full-parameter SFT, which also yields better performance?\n    - If the objective is to address the difficulty of obtaining golden responses: The currently more popular \"RL-Zero\" approach also does not require golden responses. How does its performance compare to AWDPO? Of course, the authors might argue that AWDPO only requires a very small number of queries. I acknowledge this is an advantage, but it doesn't seem to be a significant one, as queries are relatively easy to obtain."}, "questions": {"value": "See Strengths and  Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "nan"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HcSJZfIk9j", "forum": "KJkC2pwSXy", "replyto": "KJkC2pwSXy", "signatures": ["ICLR.cc/2026/Conference/Submission14202/Reviewer_XGYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14202/Reviewer_XGYQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881168788, "cdate": 1761881168788, "tmdate": 1762924655435, "mdate": 1762924655435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Advantage-Weighted Direct Preference Optimization (AWDPO), a data-efficient method for transferring few-shot chain-of-thought reasoning into the zero-shot behavior of small language models (0.5B–3B) without requiring large teacher models or reinforcement learning. The key idea is to treat the model’s own few-shot outputs as a pseudo-teacher and compare them against its zero-shot outputs, weighting this preference by the advantage (difference in correctness-based reward), thereby enabling learning from both successful and unsuccessful reasoning attempts. Experimental results show that AWDPO achieves 39%–78% accuracy on GSM8K for 0.5B–3B Qwen-2.5 base models, recovering ~90% of the performance of a fully supervised fine-tune while using only four chain-of-thought exemplars. a 1,750× reduction in CoT data. The method generalizes to SVAMP, ASDiv, and MATH-500, where AWDPO recovers up to 90% of supervised CoT performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on improving zero-shot reasoning in small LLMs, which is a timely and practically important problem. The proposed advantage-weighted self-distillation formulation is conceptually clean: compare few-shot vs. zero-shot responses from the same model and update toward the one with higher reward. The proposed framework does not need the additional teacher model which significantly improves its efficiency. The paper demonstrates that low-rank adaptation (LoRA) inherently constrains policy drift, and a dynamic supervised anchor on correct few-shot examples further stabilizes training. With this, the additional regularization components such as KL divergence penalties, trust region methods or additional models etc are not needed. This makes the whole training pipeline much more lightweight."}, "weaknesses": {"value": "The evaluation is very limited (only to GSM8K), so more comprehensive evaluations are definitely needed to validate the proposed methods. \n\nAWDPO performance is still sensitive to the selection and phrasing of the few-shot prompts used to seed the pseudo-teacher. The paper would benefit from discussing guidance or robustness strategies for exemplar choice."}, "questions": {"value": "How can we handle scenarios where the few-shot prompts are not representative? What's the underlying guidance for preparing the calibration datasets for self-distillation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hWqOuQAkT4", "forum": "KJkC2pwSXy", "replyto": "KJkC2pwSXy", "signatures": ["ICLR.cc/2026/Conference/Submission14202/Reviewer_YpX9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14202/Reviewer_YpX9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994733675, "cdate": 1761994733675, "tmdate": 1762924654871, "mdate": 1762924654871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the **lack of mathematical reasoning ability in small language models (0.5B–3B parameters)**, which typically perform poorly on reasoning benchmarks like GSM8K in zero-shot settings. The authors propose **Advantage-Weighted Direct Preference Optimization (AWDPO)** — a lightweight, single-pass self-distillation method that aligns a model’s zero-shot behavior with its own few-shot reasoning outputs. AWDPO computes a **preference loss weighted by the advantage (performance gap)** between few-shot and zero-shot responses, and combines it with a dynamically scaled **maximum likelihood (MLE) anchor** on correct examples. Experiments show AWDPO improves Qwen-2.5 models’ GSM8K accuracy from 0% to up to 77%, recovering over 90% of supervised fine-tuning performance with 1/1750th of the CoT data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **1. Data Efficiency**\n\nAWDPO achieves performance gains using only **four chain-of-thought exemplars and answer-only supervision**, representing a **1,750× reduction in labeled CoT data** compared to fully supervised fine-tuning.\n\n### **2. Clear Empirical Validation**\n\nThe authors provide **empirical validation** for some of the core claims of the paper, namely advantage weighting and dynamic loss-balancing through ablation studies."}, "weaknesses": {"value": "### **1. Reliance on the Model’s Own Few-Shot Quality**\n\nAWDPO assumes that the model’s few-shot responses are good enough to act as a “pseudo-teacher.” If the base model’s few-shot reasoning is poor, the entire self-distillation loop may propagate low-quality reasoning. The paper doesn’t explore how AWDPO behaves when the few-shot teacher is unreliable — e.g., for domains or smaller models where even few-shot reasoning fails.\n\n### **2. Oversimplified Loss-Balancing Mechanism**\n\nThe online loss-balancing rule may be too heuristic and coarse-grained to ensure true gradient-scale equilibrium. It lacks theoretical justification or empirical exploration across task types, and may cause instability or suboptimal weighting when the two losses evolve at different rates (e.g. in a new reasoning task). The authors acknowledge dynamic scaling helps avoid manual tuning, but a more principled or adaptive method (e.g., gradient norm balancing) would strengthen the claim. \n\n### **3. Inadequate Non-math Reasoning Results**\n\nOnly evaluation on non-math scenario can be found in Appendix. Even then, all baselines are missing for that setup."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KV841cuF8M", "forum": "KJkC2pwSXy", "replyto": "KJkC2pwSXy", "signatures": ["ICLR.cc/2026/Conference/Submission14202/Reviewer_Jfrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14202/Reviewer_Jfrx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009164178, "cdate": 1762009164178, "tmdate": 1762924654006, "mdate": 1762924654006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a fine-tuning technique called advantage-weighted direct preference optimization (AWDPO) that enables reasoning abilities (specifically GSM8K-style math reasoning) in small language models (Qwen-2.5 0.5-3B) using a much smaller dataset than techniques like SFT, while achieving similar performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Building more data-efficient instruction-tuning methods for smaller language models is an important area of research.\n* The paper contains many interesting experiments and analyses.\n* The method seems theoretically grounded."}, "weaknesses": {"value": "* The description of some experimental settings made it very difficult to follow the exact experimental setup and I am not 100% sure everything about the setup is sound based on the existing description. For example, the paper talks multiple times about QA pairs that were used as part of the fine-tuning data for AWDPO but it is never discussed what these examples are. Also, lines 250ff say \"To increase prompt diversity for our methods, we randomly sample k ∈ { 2, 3, 4 } few-shot exemplars for each training instance.\" --> does this mean that more than 4 few-shot examples were used in total for AWDPO? If so, this would considerably weaken the data efficiency argument.\n* Similarly, I think there should also be a comparison to SFT with the 4 chain-of-thought examples and the QA pairs for a fair comparison. Does AWDPO work better than that method?\n* There is no discussion of computational efficiency. How does this method compare to SFT or PEFT techniques like LoRA?\n* It would be good to know whether the method also works for other models.\n* The setup of requiring several chain-of-thought examples and 7k QA pairs may not work very well for low-resource languages where such QA pairs may not be available. For higher-resource languages, on the other hand, it seems like it would be fairly easy to get CoT traces for existing data sets, so it is not entirely clear when this method would be useful in practice, which may limit impact."}, "questions": {"value": "See questions regarding training details above. I'd be willing to raise my score a bit if the authors shared more details about the experimental setup in the author response (and they are sound)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x0a9cobxXW", "forum": "KJkC2pwSXy", "replyto": "KJkC2pwSXy", "signatures": ["ICLR.cc/2026/Conference/Submission14202/Reviewer_FXUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14202/Reviewer_FXUL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762376130024, "cdate": 1762376130024, "tmdate": 1762924653474, "mdate": 1762924653474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}