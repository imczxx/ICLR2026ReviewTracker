{"id": "jhqqoimoWt", "number": 5913, "cdate": 1757945861787, "mdate": 1759897945351, "content": {"title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of \nlarge language models (LLMs).\nIn this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization.\nHowever, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning.\nSuch flawed-positive rollouts are rewarded identically to fully correct ones, causing policy models to internalize these unreliable reasoning patterns.\nIn this work, we first conduct a systematic study of flawed-positive rollouts in RL and find that they enable rapid capability gains during the early optimization stage, while constraining reasoning capability later by reinforcing unreliable patterns.\nBuilding on these insights, we propose **F**lawed-**A**ware **P**olicy **O**ptimization (**FAPO**), which presents a parameter-free reward penalty for flawed-positive rollouts, enabling the policy to leverage them as useful shortcuts in the warm-up stage, securing stable early gains, while gradually shifting optimization toward reliable reasoning in the later refinement stage.\nTo accurately and comprehensively detect flawed-positive rollouts, we introduce a generative reward model (GenRM) with a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome correctness, process reliability, and training stability without increasing the token budget.", "tldr": "", "keywords": ["LLM Reasoning", "Reinforcement Learning", "Robust Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10d0297244083128a3ce94b7ce1c4913f5262c6d.pdf", "supplementary_material": "/attachment/e89803a71be89ac2b83dd2322d5df6344df30a6e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FAPO, a novel approach to improving RL for LLMs in reasoning tasks. It identifies that flawed-positive rollouts persist throughout RL training and hinder model performance, then proposes FAPO which combines a compact generative reward model (FAPO-GenRM-4B) to detect reasoning errors with process-level rewards and a parameter-free algorithm that applies adaptive penalties to flawed positives. FAPO demonstrates consistent improvements across 7B and 32B models on mathematical (AIME24: +4.7%, AIME25: +3.1%) and general reasoning tasks (GPQA-Diamond: +1.5%), while substantially reducing flawed-positive ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. In particular, figures in the paper effectively illustrate key concepts\n2. The systematic study in Section 2.2 provides valuable insights into the prevalence and evolution of flawed-positive rollouts during RL training, supported by both automatic evaluation and human verification\n3. The effectiveness of FAPO is demonstrated both theoretically and empricially"}, "weaknesses": {"value": "1. The hybrid process + outcome reward formulation is not novel. Prior works have extensively explored combining step-level and outcome-level rewards (e.g., arXiv:2312.08935, arXiv:2504.13958)\n2. The paper only evaluates on mathematical reasoning tasks (AIME24, 25) and one general domain benchmark (GPQA-Diamond), which is insufficient to claim broad applicability. The paper would benefit significantly from including additional widely-used benchmarks such as AMC and MATH\n3. Only two model families (Qwen2.5-Math-7B and Qwen2.5-32B) are tested, both starting from pre-trained models. It would be valuable to stress-test the proposed GenRM on models that already have some code-starts with long-CoT trajectories\n4. While the improvements are consistent, the absolute performance gains come at considerable cost: (1) requires training an additional 4B reward model, (2) needs complex asynchronous infrastructure"}, "questions": {"value": "1. Why does the student model outperform the teacher model? Table 2 and Figure 3 show that FAPO-GenRM-4B outperforms Qwen3-32B (teacher) on FlawedPositiveBench\n2. What is the overall computational cost overhead? The paper mentions training time increases by \"less than 20%\" but lacks comprehensive cost analysis (e.g., the cost of data synthesis, training GenRM, and detailed inference costs during RL). In particular, I am curious whether the inference cost would increase for GenRM if performing FAPO on models that have some code-starts with long-CoT"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qTuzHCRsd8", "forum": "jhqqoimoWt", "replyto": "jhqqoimoWt", "signatures": ["ICLR.cc/2026/Conference/Submission5913/Reviewer_Gwxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5913/Reviewer_Gwxh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900425859, "cdate": 1761900425859, "tmdate": 1762918345743, "mdate": 1762918345743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper begins by identifying a common failure mode in outcome-based RLVR: flawed-positive rollouts, which are trajectories that produce correct final answers through flawed reasoning. It first quantifies their prevalence and twofold effects during training, then proposes FAPO (Flawed-Aware Policy Optimization) as an enhancement to DAPO/GRPO based on this insight. FAPO trains a generative reward model (GenRM) with both outcome- and process-level supervision to detect flawed positives, and integrates this signal into the policy optimization process via an additional penalization term to the outcome reward. Experimentally, the trained FAPO-GenRM achieves higher F1 scores than baselines on ProcessBench and the curated FlawedPositiveBench. When applied to train Qwen-7B and Qwen-32B reasoning models, FAPO delivers improved performance on AIME24/25 and GPQA-Diamond, reducing flawed-positive ratios compared to GRPO baselines without increasing the inference token budget. Additional ablations analyze GenRM effectiveness, examine potential reward-hacking risks, and show that asynchronous deployment keeps computational overhead modest."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Well-motivated.** The paper clearly identifies *flawed-positive rollouts* as a pervasive yet unresolved failure mode in RLVR, illustrating their dynamics through quantitative trends (Figure 2) and motivating the need for process-level awareness beyond outcome-based rewards. This diagnosis provides a solid conceptual foundation for introducing FAPO.\n- **Sound theoretical formulation.** The theoretical sections (Sec. 3.2, Appendix A) formalize the reward-penalization mechanism using group-relative advantage estimation. The analysis further justifies adopting a fixed penalty coefficient (λ = 1) based on the majority-guided condition, making FAPO both principled and free of hyperparameters.\n- **Comprehensive experimental validation.** The experiments cover multiple reward benchmarks (FlawedPositiveBench, ProcessBench) and reasoning benchmarks (AIME24/25, GPQA-Diamond). The study includes one 4B model for the reward model and two models (7B and 32B) for the reasoner model training, supported by detailed analysis on FAPO-GenRM's detection ability, training effectiveness, and the reward-hacking risks. Together, these components form a coherent and comprehensive empirical evaluation."}, "weaknesses": {"value": "- **[Soundness]** While the observations in Section 3 are insightful, the claimed causal relationship between flawed positives and performance gains (lines 190–192) requires further evidence. The presented figure only demonstrates a correlation, not causation, and additional controlled ablations would be needed to validate this conclusion.\n- **[Soundness]** The evaluation datasets are relatively small. AIME (30 samples) can be insufficient for robust reasoning assessment, and on the larger GPQA-Diamond (198 samples), the observed improvement is modest (Figure 4, 1.5 points). Moreover, in Figure 4 (bottom right), the flawed-positive ratio remains high even after FAPO training, suggesting potential limits in its corrective capability.\n- **[Significance]** Although the theoretical formulation is novel and well-structured, the method essentially extends RLVR by integrating outcome and process rewards into a unified training signal. Similar ideas have been explored in prior works such as [1] and [2], which slightly reduces the originality of the contribution. Furthermore, the algorithm focuses solely on mitigating flawed positives, leaving other issues in RLVR, such as false negatives, unaddressed.\n\n[1] Process Reinforcement Through Implicit Rewards, Arxiv Feb 2025\n\n[2] Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains, Arxiv July 2025"}, "questions": {"value": "1. How sensitive is FAPO to the choice of λ beyond the majority-guided setting (λ = 1)? Could a curriculum that gradually increases λ during training yield better performance than the fixed choice?\n2. Do the authors have the performance results on other benchmarks (i.e., AIME25, GQPA) corresponding to the setup shown in Figure 5?\n3. During RL training, the GenRM remains fixed while the policy model continuously updates, which may lead to reward-hacking behavior. In Figure 7, FAPO-GenRM appears more robust than PRM, but to what extent? Has its robustness been quantitatively evaluated, and could it eventually suffer from reward hacking as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QerCuQfqyc", "forum": "jhqqoimoWt", "replyto": "jhqqoimoWt", "signatures": ["ICLR.cc/2026/Conference/Submission5913/Reviewer_hAMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5913/Reviewer_hAMR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974411349, "cdate": 1761974411349, "tmdate": 1762918345208, "mdate": 1762918345208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of \"flawed-positive\" rollouts in reinforcement learning with verifiable rewards (RLVR) for LLMs, where models arrive at correct answers via unreliable reasoning (e.g., guessing). The authors first analyze this phenomenon, finding that while flawed positives provide rapid early gains, they ultimately constrain capability by reinforcing these unreliable patterns. To mitigate this, they propose Flawed-Aware Policy Optimization (FAPO), a method that applies a parameter-free reward penalty to flawed-positive rollouts. This approach aims to leverage flawed positives as shortcuts during the initial warm-up phase while gradually shifting the optimization objective toward reliable reasoning in the later refinement stage. To enable this, the work also introduces a generative reward model (GenRM) trained with a process-level reward to accurately detect and localize reasoning errors within rollouts. The authors claim FAPO improves correctness, reliability, and stability without increasing the token budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies and analyzes a critical problem in RLVR. The preliminary study (Section 2.2) effectively demonstrates the \"twofold effect\" of flawed positives—acting as \"stepping stones\" early in training but hindering optimization later —providing a solid empirical foundation for the proposed solution.\n\n2. The FAPO algorithm's adaptive reward penalty is simple yet theoretically grounded. The analysis in Appendix A demonstrates how this mechanism creates an automatic, parameter-free \"optimization shift\".\n\n3. The development of a compact (4B) generative reward model (GenRM) is a key strength. The step-wise, distance-sensitive reward formulation used to train it is well-designed to encourage precise error localization rather than simple binary guessing. This trained GenRM is shown to be highly effective, outperforming its 32B teacher model and a 72B SOTA discriminative model.\n\n4. The experiments are thorough, testing on multiple models (7B, 32B) and benchmarks (AIME24, AIME25, GPQA). Crucially, the authors present full learning curves rather than just final checkpoints. This transparently supports claims of improved training stability , outcome correctness , and process reliability (a reduced flawed positive ratio)."}, "weaknesses": {"value": "1. The entire framework's effectiveness is contingent on the quality of the FAPO-Critic-85K dataset, which was labeled by a \"teacher model\" (Qwen3-32B). The GenRM can only learn to detect flaws that the teacher model can identify. This creates a fundamental performance ceiling; any subtle errors missed by the teacher will be propagated, and FAPO will fail to penalize them.\n\n2. The GenRM is trained primarily on mathematical reasoning tasks. While it shows good performance on GPQA-Diamond, it is unclear how well this math-trained critic can generalize to detecting a wider array of non-mathematical flawed reasoning (e.g., logical fallacies, factual inconsistencies) in open-domain tasks. The paper acknowledges this as a limitation.\n\n3. The proposed asynchronous architecture, while necessary for performance, adds significant infrastructure complexity compared to standard RLVR, which uses a simple, synchronous rule-based verifier. The authors note this adds a non-trivial training time increase of \"less than 20%\", which could be a barrier to adoption."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZzC99EcKeI", "forum": "jhqqoimoWt", "replyto": "jhqqoimoWt", "signatures": ["ICLR.cc/2026/Conference/Submission5913/Reviewer_wvzj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5913/Reviewer_wvzj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989907029, "cdate": 1761989907029, "tmdate": 1762918344823, "mdate": 1762918344823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical issue in RLVR for LLMs: the reinforcement of flawed reasoning patterns. In RLVR, models are rewarded for generating trajectories that lead to a correct final answer. However, many such positive rollouts contain unreliable reasoning steps, which are rewarded identically to fully correct solutions. This flawed-positive problem can lead to models that are correct on a specific metric but ultimately unreliable.\n\nThe authors first conduct a systematic analysis, revealing that flawed positives act as valuable stepping stones for rapid early learning but later constrain performance by reinforcing bad habits. To address this, they propose FAPO, a two-stage solution: They train a efficient generative reward model using a novel step-wise RL objective that localizes the first error in a reasoning chain. This model achieves state-of-the-art performance on error detection benchmarks. FAPO applies a parameter-free reward penalty to rollouts flagged as flawed-positive by the GenRM. The penalty is designed to dynamically shift the learning focus, initially allowing flawed positives to aid learning before gradually steering the policy toward fully reliable reasoning as its capability improves."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem tackled is of great significance to the LLM reasoning community. As RLVR becomes a dominant paradigm for advancing LLM capabilities, ensuring that the learned reasoning is not just correct but also reliable and transparent is crucial for safety and trustworthiness. FAPO provides a practical, efficient, and theoretically grounded solution that improves both the efficiency and the final quality of RL training. The release of code and benchmarks further enhances its impact. Experiments on mathematical and GPQA reasoning tasks demonstrate that FAPO improves outcome correctness, reduces the rate of flawed positives, and enhances training stability without increasing response length."}, "weaknesses": {"value": "While the results on mathematical reasoning and GPQA are strong, the paper's claims of broad domains would be more convincing with validation on a wider range of tasks. A key domain of interest is code generation, where verifiable rewards are common and flawed reasoning (e.g., code that passes specific tests but is buggy or inefficient) is a major concern. Demonstrating coding effectiveness would significantly broaden the method's impact and generalizability.\n\nThe 20% training-time overhead, while reasonable, could be a barrier to truly massive-scale training. A more detailed discussion of the bottlenecks and potential optimizations for a synchronous system (even as future work) would be valuable for practitioners looking to adopt it at a larger scale.\n\nThe paper focuses on tasks with easily verifiable final answers. A natural question is how FAPO would be adapted to more subjective or open-ended tasks where a \"correct\" final answer is not binarily verifiable. In such scenarios, would the process-level reward become the primary signal?"}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GChhpCHRaN", "forum": "jhqqoimoWt", "replyto": "jhqqoimoWt", "signatures": ["ICLR.cc/2026/Conference/Submission5913/Reviewer_DJdo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5913/Reviewer_DJdo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997333212, "cdate": 1761997333212, "tmdate": 1762918344527, "mdate": 1762918344527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}