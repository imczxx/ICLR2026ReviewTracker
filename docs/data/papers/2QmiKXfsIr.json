{"id": "2QmiKXfsIr", "number": 15514, "cdate": 1758252179411, "mdate": 1763718257957, "content": {"title": "KANO: Kolmogorov-Arnold Neural Operator", "abstract": "We introduce Kolmogorov–Arnold Neural Operator (KANO), a dual‑domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over a generic position-dependent dynamics for any physical input, whereas FNO stays practical only to spectrally sparse operators and strictly imposes fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground‑truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\\approx6\\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\\approx1.5\\times10^{-2}$,  by orders of magnitude.", "tldr": "KANO: an operator network expressive and efficient over a generic position-dependent dynamics with intrinsic symbolic interpretability.", "keywords": ["Neural Operator", "Operator Network", "KAN", "SciML", "AI4Science", "Interpretable AI"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2aa45b88652261317b973ff8a485066dc5f738c.pdf", "supplementary_material": "/attachment/99304d98b83b7e8045a5c01fabcb130eeaad9651.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces the Kolmogorov–Arnold Neural Operator (KANO), a novel neural operator that learns pseudo-differential symbols in both spatial and spectral domains using Kolmogorov–Arnold Networks (KANs). The authors identify a theoretical limitation in Fourier Neural Operators (FNOs), the \"pure-spectral bottleneck\", which restricts their expressivity for position-dependent dynamics. They provide a rigorous theoretical analysis showing that FNOs suffer from super-exponential scaling in model size for such operators, while KANO achieves polynomial scaling. Empirical validation includes synthetic PDE operator learning and quantum Hamiltonian learning, where KANO demonstrates strong out-of-distribution generalization, high parameter efficiency, and symbolic recovery of operator forms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper makes a significant and well-structured contribution to SciML. It identifies a fundamental flaw in a popular architecture, proposes a principled solution, and provides convincing evidence of its superiority. The authors present a rigorous theoretical diagnosis of FNOs, exposing the “pure-spectral bottleneck.” They show that for position-dependent dynamics, including quantum mechanics and fluid flow, FNOs suffer from super-exponential scaling. This is framed not as a minor drawback but as a deep architectural flaw.\n\nIn response, the KANO is both novel and well aligned with the mathematics of the problem. By jointly parameterizing operators in spatial and spectral domains through a pseudo-differential framework, KANO represents each term in its naturally sparse basis. This dual-domain approach provides the right inductive bias for robust learning.\n\nThe empirical results reinforce the theory with striking effect. KANO does not merely improve modestly over FNO but shows dramatic gains: flawless out-of-distribution generalization, 0.03% of the parameters, and symbolic operator recovery accurate to the fourth decimal. This interpretability moves the model from black-box approximation toward genuine scientific discovery."}, "weaknesses": {"value": "The main weakness is the narrow experimental validation. Results are confined to 1D synthetic operators and quantum systems, leaving scalability to high-dimensional PDEs untested. Absent are standard benchmarks such as 2D Navier–Stokes or 3D elasticity, along with any runtime or stability analysis, making the practical utility unclear.\n\nComparisons are also limited. Most results benchmark only against vanilla FNO, whose flaws are already established, creating a straw-man dynamic. Stronger baselines like U-FNO, AM-FNO, PDNO, or multi-scale FNO variants are missing, making it hard to judge whether KANO’s advantage stems from its design or from outdated comparisons.\n\nFinally, while symbolic recovery is compelling, it remains demonstrated only in simple, smooth cases. Whether this interpretability extends to non-smooth, higher-dimensional, or more complex operators is left open. Overall, the idea is strong, but the evidence is not yet broad enough to confirm its readiness for widespread use."}, "questions": {"value": "How does KANO perform on widely used PDE benchmarks such as 2D Navier–Stokes or heterogeneous material modeling?\n\nCould you include comparisons with stronger baselines like U-FNO, PDNO, or adaptive FNO variants?\n\nWhat are the runtime and memory costs of KANO compared to FNO during training and inference? Given the dual-domain computations, how does training time scale?\n\nHow robust is symbolic recovery when operator coefficients are non-smooth or discontinuous?\n\nWhat optimization challenges or instabilities were encountered when training the KAN sub-networks within the operator framework, and how were they addressed?\n\nDid you perform ablation studies to understand the individual contributions of the dual-domain design versus the KAN sub-networks? For instance, what is the performance of a KANO variant that uses MLPs instead of KANs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ZSQQJ3HsK", "forum": "2QmiKXfsIr", "replyto": "2QmiKXfsIr", "signatures": ["ICLR.cc/2026/Conference/Submission15514/Reviewer_mZHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15514/Reviewer_mZHA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706915818, "cdate": 1761706915818, "tmdate": 1762925801281, "mdate": 1762925801281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed the Kolmogorov-Arnold Neural Operator KANO that uses the KAN to construct the neural operator, which has very solid advantages over FNO both from theory and experiment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The explanation of why the FNO have problems is very clear. Strongly support the contribution and advantages of the proposed KANO."}, "weaknesses": {"value": "1. The idea is quite natural, if picky. But it does not affect the contribution.\n2. The experiment is quite limited. I think this method should benefit a lot from the flexibility of the KAN. Extra experiments of diverse operators can make this paper attractive to a broader audience."}, "questions": {"value": "1. Are there other results that demonstrate the advantages of this method?\n\n2. For the demonstration of the spectrum limitation of FNO, the author uses the linear approximation. Will the off-diagonal contribution arise in higher orders? What will that affect in the argument in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GCvWqHeazs", "forum": "2QmiKXfsIr", "replyto": "2QmiKXfsIr", "signatures": ["ICLR.cc/2026/Conference/Submission15514/Reviewer_bFzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15514/Reviewer_bFzm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965529617, "cdate": 1761965529617, "tmdate": 1762925800708, "mdate": 1762925800708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the neural operator learning setup and consider a class of operators which involves spatial multipication and term it as position dependent dynamics to highlight the shortcoming of the existing fourier neural operator framework. Since the multiplication and differentiation have a dual relationship under the fourier transform, the authors argue that the considered class defined by the multiplication map is just a two step up-shift sparse matrix in the spatial basis, however, takes the form of a dense Teoplitz matrix in the spectral basis and the fourier operator cannot efficiently approaimte the off-diagonal elements of this matrix. The authors first show that this position dependent dynamics will induce a super-exponential scaling in the fourier operator. To adress this issue the authors propose the KANO framework which replaces the nodes of standard MLP with simple sum operations and learn the univariate 1D functions. This reparameterization is similar in expressivity to MLPs upto constant depth and width empirically. Also, they propose using a KAN sub-network jointly parameterized by both spatial and spectral basis and enjoys sparse representations in both spatial and spectral domains. They also discuss that the projection error scales efficieitnyl by its width and the latent network error also scales efficiently independent of the projection error and thus, it is more efficient as compared to the FNO for the considered class. Furthermore, they also verify this empirically by considering a dataset of the considered class where they observe that KANO outperforms FNO with much lesser size required."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Investigating the bottlenecks of the FNO by considering this position dependent dynamics class and then drawing the inference that this will incur dense representation in one basis and sparse in another is really interesting. Further utilising the KA network based parameterization and resolving this issue is also interesting and provides new insights for various scenarios of this operator learning framework. Further verification by empirically investigating this under different testing /training families and demonstrating that KANO can outperform FNO with much lesser paramter. Also the section 5.2 further including two-position dependent quantum dynamics benchmark further strenghten their claims."}, "weaknesses": {"value": "No major weaknesses but the authors could have also compared the FNO and proposed KANO on the original equations considered in the FNO paper to see how does it perform in that setup to understand its overall use case as against this specific class. It is a bit unclear how this new framework will do on the  other tasks that FNO can perform very efficiently."}, "questions": {"value": "Since the FNO based layers have been also used for a lot of other tasks like weather forecasting [1] or like token mixers for transformers [2], do the authors have any comments on the broader applicability of this framework?\n\n[1] Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators.\n[2] Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. ICLR 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjhL6AMOF8", "forum": "2QmiKXfsIr", "replyto": "2QmiKXfsIr", "signatures": ["ICLR.cc/2026/Conference/Submission15514/Reviewer_Nuuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15514/Reviewer_Nuuh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967628208, "cdate": 1761967628208, "tmdate": 1762925800228, "mdate": 1762925800228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of the first paper revision"}, "comment": {"value": "We gratefully appreciate all the insightful reviews on our paper. We saw great opportunities to further improve and enhance our work thanks to the reviewers’ efforts, and below is the list of changes in the first revision of the manuscript updated on 11/20:\n\n- **Section 1** Introduction is revised to clarify the scope and intended use-case of KANO.\n- **Appendix D** is added to expand the argument in Section 3.1 into arbitrary high order expansion and multi-layer FNO, to show our logics stay intact in the generalized setting.\n- **Appendix E** is added to provide theoretical analysis on the computation and memory complexity of KANO compared to FNO\n- Latter part of **Section 4.1** is revised to comment on the robustness of symbolic recovery.\n- Additional baselines of FNO variants in **Section 5.1**.\n- Additional ablation study in **Section 5** by a KANO variant with MLP subnetworks to understand individual contributions of the dual-domain architecture versus KAN subnetworks."}}, "id": "z630eKHklF", "forum": "2QmiKXfsIr", "replyto": "2QmiKXfsIr", "signatures": ["ICLR.cc/2026/Conference/Submission15514/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15514/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15514/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754205266, "cdate": 1763754205266, "tmdate": 1763772659930, "mdate": 1763772659930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}