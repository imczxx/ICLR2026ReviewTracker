{"id": "XltolYTKCC", "number": 10543, "cdate": 1758175131951, "mdate": 1759897644491, "content": {"title": "Where Reasoning Fails: Step-wise Confidence Attribution in Black-box LLMs", "abstract": "Large Language Models (LLMs) have achieved strong performance on complex reasoning tasks by generating step-by-step solution traces, but diagnosing where a reasoning trace might fail remains difficult. Confidence estimation (CE) provides reliability signals but is usually restricted to the final answer, offering only coarse diagnostics. While recent studies have explored stepwise diagnostics, existing methods rely on white-box access, such as token-level logits or fine-tuned models, which are infeasible for closed-source LLMs.\nWe introduce Stepwise Confidence Attribution, a black-box framework for diagnosing errors, requiring only access to generated reasoning traces.\nStepwise confidence attribution applies the Information Bottleneck (IB) principle to assign confidence scores at the step level, treating consensus structures across correct solutions as anchors of reliable reasoning with high confidence. Steps that do not align with these consensus patterns are assigned lower confidence.\nWe propose two complementary methods: (1) a non-parametric overlap-based approach (NIBS) that measures consistency without graph context, and (2) a Graph-based IB model (GIBS) that learns subgraphs through a differentiable mask to capture structural variability.\nThrough extensive experiments on mathematical reasoning and multi-hop question answering, we show that our framework reliably identifies low-confidence steps strongly correlated with reasoning errors. Moreover, incorporating step-level CE improves overall reasoning accuracy, yielding up to an 12.3\\% accuracy gain. Our framework provides a practical diagnostic tool for enhancing the reliability of LLM reasoning. Code can be found at https://anonymous.4open.science/r/ICLR_2026_-2801.", "tldr": "", "keywords": ["Confidence Attribution; Large Language Models; Graph Information Bottleneck"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90d1b031eb63c9dbdc66909364ea95bdd43dae3d.pdf", "supplementary_material": "/attachment/49aced633a360dcdc2e99f87e354565db2f9e6fe.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes methods on step-wise confidence estimation of black-box LLMs in reasoning tasks. Two confidence estimation are proposed, which are based on measuring the similarity of different steps among various sampled reasoning traces. Empirical evaluations are conducted against various baseline methods, where the proposed methods show better performance. Moreover, the utility of the proposed methods is demonstrated in guiding LLM self-correction, which is more effective than self-correction based on final prediction correctness only."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed step-wise confidence estimation methods are principled and intuitive, which could be effective given strong implementations.\n\n2. The empirical evaluation is thorough, which aims to demonstrate the effectiveness of the proposed methods through direct evaluations and downstream applications."}, "weaknesses": {"value": "1. In general, this work lacks clarity in its problem formulation, descriptions of evaluation setup and implementation details. Specific issues are mentioned in the next sections.\n\n2. The introduced confidence estimation methods introduce dependency on external models and/or additional requirements on the format of model outputs, which could make them brittle and hard-to-generalize.\n\n    (a) The NIBS method requires either BERT embeddings or an NLI model. Therefore, its effectiveness depends on the reliability of the embeddings and/or the NLI model. (There is no clear description regarding the BERT embeddings and the NLI model. Which BERT was used? Was the NLI model trained? Which model is the NLI model based on?)\n\n   (b) The GIBS method requires the model to output structured reasoning traces or a post-processor for parsing the reasoning trace. (There is no specific information in the paper on which approach was actually used.) If the former approach is used, it requires the reasoning model to be capable enough to generate structured reasoning traces and might alter model behavior and introduce computational overhead. If the latter is used, an additional post-processor is required for the method. Moreover, the GIBS method also requires an NLI model. (Again, the specific information of the used NLI model is lacking). For annotating the entailment, two hyper-parameters are introduced ($\\tau_{e}$, $\\tau_{v}$), of which the values seem to be arbitrarily set (Line 787). How sensitive is the method regarding the values of these?\n\n3. The evaluation setup and the main objective of the step-wise confidence estimation should be better clarified.\n\n    (a) Is there a definition of \"gold-standard\" step-wise confidence? Is it the LLM-predicted probability of the steps? This is critical since it determines how the confidence estimation methods should be evaluated.\n\n    (b) Related to (a), in Section 5.2 (Table 1), how are the AUROC and ECE, etc. computed? These metrics require accuracy. Is the accuracy computed at the step level? If it is, how is the step-level accuracy computed?"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7sfHX3COQO", "forum": "XltolYTKCC", "replyto": "XltolYTKCC", "signatures": ["ICLR.cc/2026/Conference/Submission10543/Reviewer_2s99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10543/Reviewer_2s99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632705867, "cdate": 1761632705867, "tmdate": 1762921822217, "mdate": 1762921822217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes two confidence attribution methods called NIBS and GIBS. NIBS is a non-parametric method while GIBS employs graph-based modeling method to check the confidence of the steps in the reasoning chain. They are based on the information bottleneck principle. Authors conduct experiments on several LLM backbones and different reasoning datasets to present the effectiveness of their design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is a novel paper discuss an interesting problem Confidence Attribution, which is important for LLM reasoning. Authors design new black-box methods to address the disadvantages of current white-box methods.\n- The design of NIBS and GIBS demonstrates strong innovation, and the authors have conducted a substantial amount of work in their experiments."}, "weaknesses": {"value": "- Regarding the process of constructing a graph, the author provides a rather brief introduction in the text. I believe this process could be explained more effectively with the inclusion of additional concrete examples.\n- In my understanding, the model designed by the authors should be a GNN. Based on this, they developed subsequent training and inference methods for NIBS. However, the paper does not elaborate on specific details such as the choice of GNN backbone. I only confirmed this after reviewing the code.\n- The method is tested under several datasets which all have definite answer labels, which may be unrealistic in certain scenarios (e.g., open-domain generation). Although the authors note this is a reasonable assumption for diagnostic tasks, they do not discuss the impact of label noise or partially correct answers."}, "questions": {"value": "- Can the author explore how different graph construction methods impact performance? For instance, what performance differences arise when processing reasoning steps as homogeneous graphs vs heterogeneous graphs?\n- Did the authors test the impact of different GNN backbones on performance? Based on the code, it appears they only experimented with the GCN model.\n- Can the research topic being studied by the author positively impact RL-based LLM post-training? For example, by utilizing the model you propose to estimate the confidence level of the reasoning chain during the RL rollout process, thereby yielding additional benefits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "diHE1GVsup", "forum": "XltolYTKCC", "replyto": "XltolYTKCC", "signatures": ["ICLR.cc/2026/Conference/Submission10543/Reviewer_Jq1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10543/Reviewer_Jq1V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891912806, "cdate": 1761891912806, "tmdate": 1762921821704, "mdate": 1762921821704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a step-wise confidence attribution (SCA) method for diagnosing large language model reasoning traces. The key idea is to identify consensus steps derived from correct solutions and measure how well individual steps in a given reasoning path align with these consensus steps. The authors present two methods: a non-parametric overlap between reasoning step and consensus steps and a graph-based method which represents reasoning traces as graphs and learns to select subgraphs that align with consensus reasoning graph. Experiments on mathematical reasoning and multi-hop QA datasets demonstrate that the approach can identify erroneous steps and improve final-answer accuracy through error correction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Step-wise confidence attribution addresses a critical need in making LLM reasoning more interpretable and reliable. The ability to localize where reasoning fails is valuable for model analysis.\n- The idea of using consensus steps from correct solutions as anchors for confidence estimation is intuitive and reasonable."}, "weaknesses": {"value": "- The method assumes access to groundtruth answers to construct consensus steps. While this is acceptable for diagnostic evaluation, it undermines claims about error correction. Most baselines in Table 1 do not rely on such supervision, making comparisons somewhat unfair. In Section 5.3, the correction setting implicitly assumes oracle access to correctness feedback. This limits the practicality of the proposed use cases.\n- The paper associates \"confidence\" with \"contribution to correct answer,\" which are 2 distinct concepts. A reasoning step can be confident but incorrect (high model certainty, wrong conclusion). Conversely, a correct step might show low confidence if it's unusual or creative. The method assigns scores based on alignment with consensus from correct trajectories, which measures attribution to correctness rather than confidence."}, "questions": {"value": "1. Why restrict consensus anchors to correct solutions? Have you attempted using consensus from all sampled trajectories rather than correct-only? Could frequent patterns across both correct and incorrect solutions provide better signal?\n2. How sensitive is the confidence attribution to the number and diversity of sampled trajectories used for consensus construction?\n3. Can the proposed framework operate without ground-truth correctness labels such as self-consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U2x2NJcm5b", "forum": "XltolYTKCC", "replyto": "XltolYTKCC", "signatures": ["ICLR.cc/2026/Conference/Submission10543/Reviewer_m4v1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10543/Reviewer_m4v1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977526148, "cdate": 1761977526148, "tmdate": 1762921821261, "mdate": 1762921821261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for diagnosing reasoning errors in large language models without requiring white-box access. The authors propose a Stepwise Confidence Attribution (SCA) framework, which assigns confidence scores to individual reasoning steps using only generated traces and final correctness labels. Specifically, two implementations are presented: NIBS, a non-parametric overlap-based method, and GIBS, a graph-based model leveraging the Information Bottleneck (IB) principle for structure-aware confidence attribution. Experiments across reasoning datasets (GSM8K, Math, MoreHopQA) show that GIBS outperforms baselines and improves reasoning accuracy. The framework also enables targeted self-correction and exhibits out-of-distribution robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Exploring step-wise attribution frameworks for black-box models is a challenging task, and integrating information theory appears promising.\n\n2. Cross-domain generalization experiments verified that the proposed method possesses a certain degree of scalability."}, "weaknesses": {"value": "1. I think the paper's fundamental assumption that treating common steps as anchors is not fully convincing. From an entropy perspective, these anchors contain less information. A more effective attribution strategy should focus on identifying the correctness of non-consensus steps rather than treating them uniformly. \n\n2. In my view, the compared baselines are insufficient. LLMs used as judges can also assess the correctness of reasoning steps without requiring final-answer labels. Moreover, the utility of the proposed method is not particularly compelling, its advantages over existing techniques are unclear.\n\n3. Lacks a formal analysis of computational complexity, and the computational cost of consensus construction and MCS operations remains high, limiting scalability.\n\n4. Reproducibility requires further clarification, as the paper lacks detailed descriptions of consensus graph construction, model parameter settings, and training procedures."}, "questions": {"value": "please address weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BPoADbDbXn", "forum": "XltolYTKCC", "replyto": "XltolYTKCC", "signatures": ["ICLR.cc/2026/Conference/Submission10543/Reviewer_cv3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10543/Reviewer_cv3a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762344306960, "cdate": 1762344306960, "tmdate": 1762921820883, "mdate": 1762921820883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper Introduces a black-box method to detect where reasoning fails in large language models by assigning confidence scores to each step of a solution. \n\nThe author's working hypothesis: correct solutions share common reasoning structures, and steps that deviate from this consensus are likely to be errors.\n\nProposed approach: two methods: NIBS, which uses semantic similarity, and GIBS, which models reasoning as a graph and learns structural alignment using the Information Bottleneck principle."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The black-box error diagnosis framework is something I can resonate with. Also the method is mostly reference free. The authors construct a proxy reference solution (reasoning trace). The application of the IB framework is also largely novel, from what I can tell. \n\nWith that being said, I have some concerns. Please see weaknesses."}, "weaknesses": {"value": "1. This \"shared structure\" hypothesis might not hold true across all domains. Think of a problem which is more creative, and does not follow a deductive reasoning like structure (the GIBS framework largely depends on structure of reasoning). I donâ€™t see this hypothesis playing out there. \n\n2. Line 139. \"We begin with the notion of answer-level...\" seems like an incomplete sentence ?\n\n3. The semantic similarity as explained in NIBS is already explored in [1] and [2]. The graph structure as explained in GIBS is also partially explored in [3]. \n\n4. Does the framework deal with the reasoning error that happen after the first wrong step ? This has not been explicitly mentioned. I would like to know the error identification accuracy of the first wrong reasoning step. the latter steps could skew the accuracy numbers.\n\n\n\n[1] ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning\n\n[2] RECEVAL: Evaluating Reasoning Chains via Correctness and Informativeness\n\n[3] Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs"}, "questions": {"value": "1. How do you use GSM, MATH for step level error eval ? Do you construct a proxy dataset here ? \n2. Can you share results on PRM800K or process bench ?  ( even a sampled subset should be fine )"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MPclWmetOk", "forum": "XltolYTKCC", "replyto": "XltolYTKCC", "signatures": ["ICLR.cc/2026/Conference/Submission10543/Reviewer_DoV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10543/Reviewer_DoV7"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission10543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762505610795, "cdate": 1762505610795, "tmdate": 1762921820429, "mdate": 1762921820429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}