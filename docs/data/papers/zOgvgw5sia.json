{"id": "zOgvgw5sia", "number": 6527, "cdate": 1757987873416, "mdate": 1759897909617, "content": {"title": "Efficient Compression of Time-Series Foundation Models via Consensus Subspace Distillation", "abstract": "Compressing universal time-series foundation models (TSFMs) significantly reduces computational and storage overhead, thereby facilitating their widespread adoption. In TSFM compression techniques, knowledge distillation stands out by transferring knowledge from teacher models to student models. However, existing distillation methods often overlook the inherent consensus representation spaces in TSFMs and the imbalance in hierarchical contributions, leading to inefficient knowledge transfer. To address this, we propose a novel approach that reformulates distillation as a consensus subspace optimization task, leveraging the observation that high-level embeddings autonomously converge across different model scales, along with the long-tail distribution of hierarchical contributions. We tackle the consensus subspace problem by identifying and extracting scale-invariant low-rank subspaces: on local data subsets, we perform singular value decomposition on embeddings from offline-selected consensus layers to derive consensus projection matrices, which are then used to fine-tune the student model, ensuring representation alignment and accelerated convergence. Additionally, we introduce a scalable uncertainty injection mechanism to enhance generalization to unseen data, modeling subset biases as frequency-domain gaps to inflate covariances. Extensive experiments demonstrate that our framework excels on multiple standard time-series datasets, with student models even surpassing teacher performance in time-series forecasting tasks. Compared to state-of-the-art methods, our approach achieves over 90% parameter reduction and 100x distillation speedup while retaining comparable performance across various time-series tasks. Code and compressed model weights are available via an anonymous link: anonymous.4open.science/r/CSD-13C3.", "tldr": "", "keywords": ["Time-Series Foundation Models", "Time-Series Forecasting", "Model Compression", "Knowledge Distillation", "Consensus Subspace Optimization", "Uncertainty Injection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a7716f312d21daae09c5ca3c00ae2e5cbbcf435.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Consensus subspace distillation for TSFM compression: interesting idea, but weak theoretical grounding and questionable evaluation validity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies inefficiencies in existing time-series foundation model (TSFM) compression approaches and motivates a consensus subspace view through empirical evidence of spontaneous alignment across model scales. Figures 1 and 2 convincingly illustrate the differences among traditional KD paradigms and highlight the observed geometric center convergence trend.\n2. The work reframes knowledge distillation as an optimization over low-rank consensus subspaces, integrating three components: (i) SVD-based low-rank subspace extraction, (ii) mean–covariance alignment as a Wasserstein surrogate, and (iii) frequency-domain uncertainty injection to correct subset bias. This multi-view integration is somewhat novel within the TSFM compression domain.\n3. The method reportedly achieves over 90% parameter reduction while maintaining comparable performance across forecasting, imputation, anomaly detection, and classification tasks. Extensive experiments (Tables 1–3) and ablations on layer count and uncertainty injection provide a good empirical narrative supporting its efficacy."}, "weaknesses": {"value": "1. The “marginal contribution” is computed by zeroing out all layers except one. This completely destroys inter-layer dependencies (residual, normalization, and attention coupling), making the resulting loss difference meaningless. Such perturbation does not quantify true layer importance but merely measures instability. \n2. The “consensus subspace” formulation largely mirrors existing second-order or low-rank distillation paradigms (e.g., covariance alignment, CCA-based feature transfer, or optimal transport-based KD). Likewise, the uncertainty injection resembles frequency-domain augmentation or spectral regularization in prior work. The claimed novelty lies mostly in terminology; the paper fails to demonstrate what new capability emerges beyond combining these established techniques.\n3. The paper replaces the 2-Wasserstein covariance term with a simple Frobenius norm difference, claiming they share a global minimum. However, no formal bound, convergence proof, or empirical validation of approximation error is provided. In non-convex regions, this surrogate may diverge from true optimal transport distance, making the theoretical foundation of the loss function questionable.\n4. The frequency-domain characteristic function difference used to inflate spectral covariance is ad hoc and poorly justified. It ignores multivariate phase interactions, applies arbitrary Gaussian weighting, and introduces several hyperparameters (λ, σ) without sensitivity or ablation studies. Furthermore, eigenvalue inflation within low-rank subspaces could amplify numerical noise, but no stability or bias–variance analysis is presented.\n5. The method implicitly assumes that TSFMs share consistent subspaces across scales. This assumption may fail for heterogeneous architectures or domain-shifted data. The approach also depends heavily on accurate layer selection and rank thresholds; when these assumptions break, the framework may degrade sharply. The authors do not discuss limitations or failure modes."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ya4b6PRawh", "forum": "zOgvgw5sia", "replyto": "zOgvgw5sia", "signatures": ["ICLR.cc/2026/Conference/Submission6527/Reviewer_B2Ax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6527/Reviewer_B2Ax"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572704113, "cdate": 1761572704113, "tmdate": 1762918885246, "mdate": 1762918885246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Consensus Subspace Distillation (CSD) for compressing time-series foundation models (TSFMs). The core idea: (i) offline selection to the high-contribution layers, (ii) computing a scale-invariant low-rank consensus subspace via SVD on teacher embeddings, (iii) project student top-layer features into that subspace and align mean/covariance (Wasserstein-motivated surrogate), and (iv) uncertainty injection in the frequency domain using characteristic-function gaps. The authors claim ∼90% parameter reduction with competitive zero-shot performance across forecasting, imputation, anomaly detection, and classification, using MOMENT-Large as teacher."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow\n2. offline subspace estimated from 10% data, where online distillation reportedly 3.86 GPU-hours for the student, which is attractive."}, "weaknesses": {"value": "1. While the text claims \"retain over 90%\" , Table 1 shows large classification drop (0.633 vs. 0.764 accuracy). So, the “universal” claim should be revised. \n\n2. Unclear teacher-pass accounting: uncertainty injection uses raw sequences (good), but S(ωk) (Eq. 4) still needs teacher embeddings. Please quantify total teacher forward-pass cost and clarify whether this step is amortized across datasets. \n\n3. Ablation suggests K≈12 is the sweet spot (Table 4), yet the main setup copies K=3 layers (Sec. 4.2). There's an inconsistency in the best configuration."}, "questions": {"value": "1. How robust is the ∆Lℓ metric to residual pathways?\n2. Precisely account for teacher forward-passes to build S(ωk) and the (µT,ΣT) cache. Can these be computed once per domain and reused? \n3. Why K=3 in main config when Table 4 indicates K≈12 nearly closes the gap on anomaly/classification? Please report main-table results at the empirically best K. \n4. What fails for the accuracy to cause this big drop?\n5. Can a projector learned from MOMENT generalize to other teachers (e.g., Sundial/Time-MoE) without recomputation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7NNk9RtZRF", "forum": "zOgvgw5sia", "replyto": "zOgvgw5sia", "signatures": ["ICLR.cc/2026/Conference/Submission6527/Reviewer_GWhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6527/Reviewer_GWhA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906372646, "cdate": 1761906372646, "tmdate": 1762918884748, "mdate": 1762918884748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "With the increasing scale of Time-Series Foundation Models (TSFMs), their deployment in resource-limited environments has become increasingly challenging. To address this issue and improve upon existing knowledge distillation approaches, this paper proposes Consensus Subspace Distillation (CSD), which guides the alignment of the student model’s representations with the teacher’s geometric centers, thereby enabling more efficient knowledge transfer. In addition, a frequency-domain uncertainty injection mechanism is introduced to model subset biases and enhance generalization. Extensive experiments demonstrate the effectiveness of the proposed method — it not only incurs a lower performance loss compared to other compression approaches but even surpasses existing TSFMs in performance while using fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides empirical evidence of “consensus subspaces” in TSFMs, including geometric center alignment and long-tail layer contribution analysis. These findings are interesting and serve as a good motivation for the proposed method.\n\n2. The paper is clearly written, with the methodology well presented and supported by open-source code.\n\n3. The experimental results are solid. The proposed method achieves strong compression ratios and efficiency improvements with minimal performance degradation. The results across multiple tasks and datasets are comprehensive and consistent."}, "weaknesses": {"value": "1. Although CSD demonstrates empirical effectiveness as a TSFM compression technique, it is not strongly grounded in the unique characteristics of time series models. The concept of a consensus subspace may also apply to large models from arbitrary modalities, suggesting that it is a more general, modality-agnostic idea.\n\n2. The novelty is limited. The concept of a consensus subspace has been previously explored, and leveraging it to regularize feature distillation does not represent a fundamentally new contribution.\n\n3. The motivation for introducing uncertainty injection is unclear. The paper does not provide sufficient context to illustrate what is meant by “bias from data selection,” “frequency-domain gaps,” and “inflated covariances” in the last paragraph of the Introduction section.\n\n4. The Methods section would benefit from additional intuitive or high-level illustrations to aid in understanding the presented mathematical operations. (Refer to the Questions section for specific points.)"}, "questions": {"value": "1. How consistent is the “consensus subspace” phenomenon across different TSFMs? Is it architecture-dependent? It would be beneficial to demonstrate and compare the “consensus subspace” across multiple TSFMs to support the generality of this finding.\n\n2. The full pipeline involves multiple hyperparameters. How are they set, particularly $n$, $\\theta$, $\\lambda$ and $\\sigma$ ? How do these hyperparameters influence the algorithm’s behavior and overall performance ?\n\n3. On page 5, line 259: why does the construction of the consensus subspace ensure scale invariance ?\n\n4. What is the physical or statistical interpretation of this gain matrix $\\Gamma$ in equation 13 ? What is the mathematical motivation for using $\\Gamma$ to construct the consensus matrix ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MbvBkhKe4t", "forum": "zOgvgw5sia", "replyto": "zOgvgw5sia", "signatures": ["ICLR.cc/2026/Conference/Submission6527/Reviewer_H9fv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6527/Reviewer_H9fv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970172984, "cdate": 1761970172984, "tmdate": 1762918884342, "mdate": 1762918884342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a compression technique for Time-Series Foundation Models (TSFMs) called Consensus Subspace Distillation (CSD). The core idea is based on the empirical observation that TSFMs of different scales tend to converge to a similar low-rank \"consensus subspace\" in their high-level representations. Instead of traditional knowledge distillation (KD), the authors propose an offline stage to compute the statistics (mean and covariance) of this subspace from a teacher model on a data subset. A student model is then initialized by copying the top-K layers from the teacher and trained in an online stage to match these cached statistics, using a Wasserstein-distance-based loss. The method also introduces a complex frequency-domain \"uncertainty injection\" (UI) mechanism to compensate for the bias in the data subset.\n\nWhile the paper presents a novel perspective and reports impressive results (90% compression, 100x speedup), the methodology is fundamentally flawed in its comparison to other KD methods. The \"distillation\" is primarily a \"layer extraction\" (copying 3 layers) followed by a fine-tuning (of a rank-64 adapter), which makes the claims of SOTA compression and speed misleading when compared to methods that train a student from scratch. The methodology is also overly complex, and the paper lacks the critical ablations to justify its core components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's motivation, presented in Figure 2, is its strongest point. The empirical demonstration that TSFM embeddings from different scales spontaneously converge to an aligned geometric center is a valuable and interesting finding for the community.\n- The goal of efficiently compressing massive TSFMs is highly relevant and important for the community, as it addresses a key bottleneck for real-world deployment."}, "weaknesses": {"value": "1. **Anonymity Issue in Submitted Code:**  The submitted code includes a logo (in their repository \"assets/\") that identifies the authors’ institution, which compromises the double-blind review process. The authors should remove any logos or references that may reveal their identity to ensure compliance with ICLR’s anonymity requirements.\n\n2.  **Misleading Framing and Unfair Comparisons (Major Flaw):** The paper's central claim of outperforming other KD methods is built on a \"bait-and-switch.\"\n    * The proposed method is **not** distillation in the conventional sense (like RKD, CRD, etc.). Instead of training a small, separate student model from scratch to mimic the teacher, this method **copies the top K=3 layers** from the 12-layer teacher and freezes them, only fine-tuning a tiny (rank $r_a=64$) low-rank adapter.\n    * This is a form of *structural pruning* or *layer selection*, not distillation.\n    * The 90% parameter reduction (385M $\\rightarrow$ 38M) is a direct, trivial consequence of using a K=3 layer model, not a feature of the distillation *method*.\n    * The 100x speedup (Table 2) is also misleading. The method is fast because it's only fine-tuning a rank-64 adapter for 3 epochs, whereas the baselines (PKT, RKD, etc.) are training full student models from scratch for many more hours. This is not a fair comparison of distillation *efficiency*; it's a comparison of two completely different training paradigms.\n\n3.  **Over-Engineered and Poorly Justified Methodology:** The paper presents a \"kitchen sink\" of highly complex techniques without clear justification for each.\n    * The **Uncertainty Injection (UI)** mechanism (Section 3.2) is the most glaring example. It involves FFTs, empirical characteristic functions (Eq. 9), an ad-hoc inflation factor (Eq. 11), spectral density inflation (Eq. 12), and multiple projections/decompositions (Eq. 13). The motivation for using frequency-domain ChF differences to model *data subsetting bias* is not explained.\n    * The ablation in Table 4 shows that UI provides only a marginal, $\\approx$1-2% improvement (e.g., K=12, -0.54% vs -1.17% without UI). This suggests the core results are not dependent on this highly complex component, making the paper feel unnecessarily convoluted.\n\n4.  The paper fails to include the single most important ablation study to validate its core thesis. The method combines (A) layer copying, (B) subspace alignment loss ($L_\\mu + L_\\Sigma$), and (C) uncertainty injection. The authors claim (B) and (C) are the key contributions. What happens if you *only* do (A)? That is, what is the performance of a baseline that simply copies the top K=3 layers, adds the rank-64 adapter, and fine-tunes it using *only* the standard task loss ($L_{task}$)?\n    * It is highly probable that this simple baseline would achieve very similar results, suggesting that the entire complex machinery of consensus subspace projection, Wasserstein loss, and frequency-domain UI provides little to no actual benefit over simple layer-copying and fine-tuning.\n\n5. The core premise of a \"consensus subspace\" (Fig. 2) is demonstrated on only one model family (MOMENT) using one pre-training objective (MAE). It is a significant leap to claim this is a universal, scale-invariant property of *all* TSFMs.\n\n6.  The student model's ability to outperform the teacher (Table 2) is a common red flag in distillation. It suggests that the \"student\" (which is just a fine-tuned shallow version of the teacher) is acting as a regularized model on a specific task. This further supports the idea that the method is not a general-purpose distillation but rather a task-specific fine-tuning."}, "questions": {"value": "1.  Can the authors please elaborate on the \"bias-resistant\" claim? Do they agree with the hypothesis that CSD acts as a strong regularizer by distilling only the low-rank subspace, and is this why the student generalizes better than the (potentially overfit) teacher?\n\n2.  What is the intuition for using the *frequency-domain* ChF difference to model the *data subsetting* bias? Why is this a better model of bias than, for example, a spatial-domain difference? Furthermore, the inflation factor in Eq. 11 seems somewhat ad-hoc. How sensitive is the method to this specific formulation?\n\n3.  How expensive is the offline layer selection process? Have the authors tried a simpler heuristic, such as just selecting the top $K$ layers (e.g., layers $L, L-1, ... L-K+1$), and how does that compare to the marginal contribution method?\n\n4.  To confirm, is the 100x distillation speedup (Table 2) primarily due to 1) requiring fewer training epochs (3 epochs) and 2) eliminating the need for a teacher forward-pass at each training step by using the pre-computed cache $(\\mu_T, \\Sigma_T)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NCMuwsWkQw", "forum": "zOgvgw5sia", "replyto": "zOgvgw5sia", "signatures": ["ICLR.cc/2026/Conference/Submission6527/Reviewer_acwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6527/Reviewer_acwM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991150088, "cdate": 1761991150088, "tmdate": 1762918884059, "mdate": 1762918884059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}