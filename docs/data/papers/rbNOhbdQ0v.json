{"id": "rbNOhbdQ0v", "number": 4034, "cdate": 1757588366014, "mdate": 1763647158029, "content": {"title": "Making Offline Model-Based Reinforcement Learning Work on Real Robots", "abstract": "Reinforcement Learning (RL) has achieved impressive results in robotics, yet high-performing pipelines remain highly task-specific, with little reuse of prior data. Offline Model-based RL (MBRL) offers greater data efficiency by training policies entirely from existing datasets, but suffers from compounding errors and distribution shift in long-horizon rollouts. Although existing methods have shown success in controlled simulation benchmarks, robustly applying them to the noisy, biased, and partially observed datasets typical of real-world robotics remains challenging. We present a principled pipeline for making offline MBRL effective on physical robots. Our RWM-O extends autoregressive world models with epistemic uncertainty estimation, enabling temporally consistent multi-step rollouts with uncertainty effectively propagated over long horizons. We combine RWM-O with MOPO-PPO, which adapts uncertainty-penalized policy optimization to the stable, on-policy PPO framework for real-world control. We evaluate our approach on diverse manipulation and locomotion tasks in simulation and on a real quadruped, training policies entirely from offline datasets. The resulting policies consistently outperform model-free and uncertainty-unaware model-based baselines, and fusing real-world data in model learning further yields robust policies that surpass online model-free baselines trained solely in simulation.", "tldr": "", "keywords": ["Robotics", "Offline Reinforcement Learning", "Model-Based Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b01bc0bbcf449f318c02debd216e52816262eef.pdf", "supplementary_material": "/attachment/bfd77aee89e958e566cbf3eaf1a86587d0e5f546.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an offline model-based RL pipeline that can be successfully deployed on real robots. The pipeline consists of two stages. First, they train an autoregressive dynamics model on the offline dataset to predict long-horizon rollouts. The dynamics model captures both aleatoric uncertainty (via Gaussian variance) and epistemic uncertainty (via bootstrap ensembling). Second, they train a policy using an uncertainty-aware model-based PPO algorithm (MOPO-PPO). Specifically, they train the policy on model-imagined rollouts, with the epistemic uncertainty (ensemble variance) subtracted from the reward to encourage conservative behavior. This pipeline demonstrates strong performance gain over uncertainty-unaware baselines across three simulated environments (Reach-Franka, Velocity-G1, Velocity-ANYmal-D), and successfully transfers to a real-world Velocity-ANYmal-D task. Through careful ablation, the paper analyzes the effect of the uncertainty penalty and establishes a correlation between the model's prediction error and uncertainty estimate. Overall, the paper makes a solid empirical contribution towards deploying offline model-based RL in the real world."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes several empirical contributions to improve upon prior offline model-based RL methods and to eventually deploy offline model-based RL on a real robot.  \n2. The paper conducts thorough ablation studies to justify each design choice. They establish a correlation between epistemic uncertainty and model prediction error, justifying the need for the uncertainty penalty. They further analyze the effect of different lambda coefficients for the uncertainty penalty. When lambda is too large, the policy becomes too conservative and stays still. When lambda is too small, the policy starts exploiting the model. Last but not least, they compare different data mixtures, providing valuable insights into what data mixture should be used for offline model-based RL."}, "weaknesses": {"value": "1. Despite the empirical contributions, the paper lacks novelty since every component has been proposed by / studied in prior work (e.g., uncertainty-aware dynamics models, uncertainty-penalized policy optimization). \n2. The real-world experiments lack comparison to immediately relevant baselines beyond a hard-coded data collection policy and an online model-free policy. \n3. All experiments run from low-dimensional observations. It is unclear how scalable the method is to high-dimensional observations. For one thing, it is hard to quantify epistemic uncertainty using pixel reconstruction."}, "questions": {"value": "**Major**\n\n1. Can you compare to an offline model-based RL baseline in the real-world experiments? This is important since you claimed that your method is \"the first demonstration of offline MBRL operating reliably on real robotic hardware.\" The implication is that other offline MBRL methods fail to do so.\n2. A close follow-up to MOPO, COMBO [1], is omitted from all the discussions. Their main argument is to directly learn a conservative Q function instead of using an uncertainty reward penalty. And it seems to work better than MOPO. Can you either justify the omission or provide a comparison, at least in the simulated benchmark?\n3. In Table 1, what exactly is the online model-free policy? It seems that on most data mixtures, your method is worse than the online model-free policy. Is this to be expected?\n\n**Minor**\n\n4. Despite the justifications in Appendix A.5, it's still unclear to me why PPO is better than SAC. Can you explain a bit more why this is the case?\n5. When rolling out the model, you predict the next observation conditioned on the ensemble mean. Would this potentially lead to mode averaging when the dynamics is multimodal (high aleatoric uncertainty)? \n6. Can you add a detailed description of each simulated environment (Reach-Franka, Velocity-G1, Velocity-ANYmal-D)? Currently it is unclear what each task involves.\n8. Line 216, \"By incorporating uncertainty-aware modeling into the dynamics learning process, RWM-O enables robust trajectory forecasting in offline settings with uncertainty effectively propagated over long horizons.\" This sentence is confusing.\n\nReferences:\n\n[1] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, Chelsea Finn. COMBO: Conservative Offline Model-Based Policy Optimization. NeurIPS 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZeQgea1qmo", "forum": "rbNOhbdQ0v", "replyto": "rbNOhbdQ0v", "signatures": ["ICLR.cc/2026/Conference/Submission4034/Reviewer_iN9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4034/Reviewer_iN9s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730756631, "cdate": 1761730756631, "tmdate": 1762917143488, "mdate": 1762917143488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The contribution proposes an offline MBRL algorithm and evaluates it in simulation and on real hardware. The MBRL algorithm relies on an autoregressive robot world model. Ensembles are used to capture epistemic and aleatoric model uncertainty. The policy optimization penalizes exploration in regions with high epistemic uncertainty. This way overfitting to dynamics model errors is discouraged. The method is evaluated in simulation and on a real-world hardware experiment."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very well written and results are presented nicely. The proposed approach seems reasonable and is well explained. The hardware application is impressive and comparisons in simulations are exhaustive."}, "weaknesses": {"value": "- The central and strong claim of this work represented in the abstract and title is that this is the '[...] first demonstration of offline MBRL operating reliably on real robotic hardware.' If I understand correctly, at least [1] (See Sec. IV - algorithms) and maybe to some extend [2] evaluated offline model-based reinforcement learning on real robotic hardware. Therefore, one cannot claim that this work is the first application of offline MBRL on real hardware. What \"reliable\" means in the claim is not further defined. With this very strong claim I would have at least expected an exhaustive review on related hardware applications of offline MBRL and why the application presented in this paper is more significant and indeed the first reliable one. The claim of the title and the abstract is somewhat softened in the introduction: \"this is the first demonstration of uncertainty-penalized offline MBRL operating reliably on a physical robot\". If this is the real claim then the abstract and title should be changed accordingly in my opinion.\n\n- The authors mention in Sec. 2.2 that there are many methods to incorporate uncertainty estimation into MBRL ([3] may be an additional relevant related work here as they achieve a rollout length of around 30 steps on average). The paper differentiates itself from those methods solely by mentioning that those methods have not been applied on hardware \"While these methods achieve impressive performance in controlled simulation benchmarks, applying them to real-world robotics remains a significant hurdle, where reliability and robustness demand both accurate long-horizon modeling and stable policy learning.\" This makes it hard to evaluate what the methodological contribution of the paper is. Additionally, it seems that none of the baselines chosen in Sec. 5 were uncertainty aware. Therefore, results support that uncertainty awareness is important in general but do not support the effectiveness of the method proposed in this paper compared to others in the uncertainty aware space.\n\nAdditionally, I am unsure if ICLR is the right venue for the paper. Since the main contribution is to \"make ... learning work on real robots\" I'd suspect a robotics venue might be more fitting. \n\n\n[1] G. Zhou, L. Ke, S. Srinivasa, A. Gupta, A. Rajeswaran and V. Kumar, \"Real World Offline Reinforcement Learning with Realistic Data Source,\" 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7176-7183, doi: 10.1109/ICRA48891.2023.10161474.\n\n[2] X. Li, W. Shang and S. Cong, \"Offline Reinforcement Learning of Robotic Control Using Deep Kinematics and Dynamics,\" in IEEE/ASME Transactions on Mechatronics, vol. 29, no. 4, pp. 2428-2439, Aug. 2024, doi: 10.1109/TMECH.2023.3336316."}, "questions": {"value": "- I would suggest making the central claim of the paper '[...] first demonstration of offline MBRL operating reliably on real robotic hardware.' more specific and adding a literature review to support the more specific claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F829MEFLyL", "forum": "rbNOhbdQ0v", "replyto": "rbNOhbdQ0v", "signatures": ["ICLR.cc/2026/Conference/Submission4034/Reviewer_oixy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4034/Reviewer_oixy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745499853, "cdate": 1761745499853, "tmdate": 1762917143191, "mdate": 1762917143191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the framework Offline Robotics World Model  (RWM-O), which extends the previous contribution Robotics World Model (RWM) to the offline setting by adding uncertainty regularizations to the predictions of the dynamics model. The paper also introduces MOPO-PPO, which extends MOPO to use PPO as the base policy optimization algorithm. Finally, the framework uses uncertainty-regularized rewards to prevent the learned policy from drifting from the data distribution. The paper compares to standard baselines from the RL literature on simulated benchmarks and also demonstrates the algorithm on a real quadruped robot."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "$\\textbf{Clarity}$: The paper is well written, and it is easy to understand how the many components fit together. \n\n$\\textbf{Thorough Evaluations}$: The paper carefully ablates key design decisions and demonstrates how they must be tuned to achieve strong results.\n\n$\\textbf{Real world results:}$ The paper demonstrates that the algorithm can be applied directly to a high-dimensional real-world quadruped."}, "weaknesses": {"value": "$\\textbf{Missing Related Work}$: The paper largely connects to related work presented at mainline machine learning venues, but misses many developments in applying MBRL (and RL more generally) presented at robotics venues. Given the heavy emphasis the paper places on practical real-world deployment, omitting these works is a major weakness. For example, the following papers apply real-world RL to quadrupeds: \n\n- “A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning” (Smith et al., RSS 2023)\n\n- “Learning to Walk from Three Minutes of Real-World Data with Semi-structured Dynamics Models” (Levy et al., CoRL 2024)\n\n- “Date-Efficient Reinforcement Learning for Legged Robots” (Yang et al., CoRL 2020)\n\nSpecifically, the final two papers use uncertainty mitigation techniques to get MBRL to work for real world quadrupeds in a batch offline RL setting (i.e. multiple rounds of collecting data, then updating the policy with offline MBRL). Doing fully offline RL vs. batch offline RL is a minor distinction from prior work, in the opinion of this reviewer.\n\n\n\n$\\textbf{Real world results are not Surprising}$:  As a concrete examples (Yang et al., CoRL 2020) and (Levy et al., CoRL 2024) learn locomotion policies with only ~40k and ~20k real world environment steps (if my math is correct). This is around an order of magnitude less real world data than the experiments presented in this paper. This is not meant as a direct comparison, as the experimental set ups are obviously different. However, I note this to highlight that I do not find the presented results surprising or impressive, given what has been accomplished previously. \n\n$\\textbf{Benchmarks:}$ I question whether the benchmark experiments are informative about what will happen in the real world. Specifically, the current results only demonstrate significant gains when optimal expert data is available. However, I do not think this is a realistic assumption for systems such as quadrupeds — if we had optimal data, wouldn’t we already have an optimal real world policy? What is the real world benefit of the method if it doesn’t show gains when only sub-optimal data is available? }\n\n\n$\\textbf{Minimal Technical Contribution}$: I’m impressed that the authors brought together many different techniques and got them to actually work on a real robot. However, this style of paper only makes a strong publication if the results are surprisingly strong. Given the previous works mentioned above, I do not believe the paper passes this bar in its current form, making the limited technical novelty an additional weak point."}, "questions": {"value": "- Why is offline MBRL the correct approach for real word learning for the tasks in the paper? Given that the current results do not “move the needle” in terms of capabilities, I believe this point needs to be thoroughly defended.  \n\n- Can the given approach succeed when optimal data is not available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5zktrWHbNY", "forum": "rbNOhbdQ0v", "replyto": "rbNOhbdQ0v", "signatures": ["ICLR.cc/2026/Conference/Submission4034/Reviewer_uffN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4034/Reviewer_uffN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940969037, "cdate": 1761940969037, "tmdate": 1762917142964, "mdate": 1762917142964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}