{"id": "1OLTJL1wHG", "number": 22789, "cdate": 1758335439938, "mdate": 1759896846060, "content": {"title": "ConfRAG: Confidence-Guided Retrieval-Augmenting Generation", "abstract": "Can Large Language Models (LLMs) be trained to avoid hallucinating factual statements, and can Retrieval-Augmented Generation (RAG) be triggered only when necessary to reduce retrieval and computation costs? In this work, we address both challenges simultaneously. We introduce ConfQA, a fine-tuning strategy that reduces hallucination rates from 20–40% to below 5% across multiple factuality benchmarks. The approach is simple: when the model answers correctly, it is trained to output the answer; otherwise, it is trained to respond with “I am unsure.” Two design choices make this training effective: (1) a dampening prompt (“answer only if you are confident”) that explicitly discourages overconfident hallucinations, and (2) training data drawn from atomic factual statements (e.g., knowledge graph attribute values), which calibrates model confidence and yields robust generalization across domains and question types. Building on ConfQA, we propose ConfRAG, a triggering strategy that invokes RAG only when the model responses with unsure. This framework achieves accuracy above 95% in ideal case while reducing unnecessary external retrievals by over 30%.", "tldr": "We propose ConfRAG framework, which optimized for the best triggering strategy of external retrieval tools.", "keywords": ["Large Language Model", "Retrieval-Augmented Generation", "Hallucination Reduction"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/367e3398d436b717f3bae2045d8a9771f3047a17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a ConfRAG, a RAG strategy that only triggers retrieval when the answer has low confidence. To implement this strategy, the authors introduce a fine-tuning method, ConfQA, which teaches the LLM to state “I am unsure” when the answer is incorrect, via a dampening prompt “Answer only if you are confident” and a training dataset composed of atomic factual statements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed finetuning method can teach LLM to refrain from generating inconfident outputs while simultaneously improving retrieval efficiency."}, "weaknesses": {"value": "Clarity\n- The paper's central premise is to use model uncertainty as a trigger for retrieval. However, there appears to be a fundamental mismatch between this trigger and the fine-tuning objective, which is based on correctness. The paper does not sufficiently address the gap between model confidence and answer correctness. For instance, a model can be uncertain about a correct answer or highly confident in an incorrect one. This discrepancy seems to undermine the core mechanism, and it is unclear how the proposed method accounts for it.\n\nNovelty\n- The paper's claimed contributions for RQ1 (confidence calibration) and RQ2 (encouraging abstention) appear to substantially overlap with existing literature. The overconfidence of LLMs [1-2] and the use of \"I don't know\" for selective abstention [3-5] are both well-studied topics. The authors should more clearly articulate the specific novelty of their approach in light of this extensive prior work. As written, the incremental contribution is not clear.\n\nExperiments\n- The empirical evidence is not compelling. ConQA seems to significantly harm correctness on short-form benchmarks, while the performance improvements on long-form generation tasks are marginal. For instance, in Table 5, the proposed method is consistently outperformed by a standard RAG baseline with contriever.\n\n- The paper fails to compare against standard methods for hallucination detection (e.g., P(True) [6], semantic uncertainty [7]) or abstention (e.g., [8,9]). Adaptive RAG baselines, such as self-RAG and DRAGIN, were mentioned in the related work section but not compared experimentally.\n\n[1] Xiong, Miao, et al. \"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.\" The Twelfth International Conference on Learning Representations.\n[2] Tian, Katherine, et al. \"Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback.\" arXiv preprint arXiv:2305.14975 (2023).\n[3] Cheng, Qinyuan, et al. \"Can AI assistants know what they don't know?.\" arXiv preprint arXiv:2401.13275 (2024).\n[4] Chen, Lida, et al. \"Teaching large language models to express knowledge boundary from their own signals.\" arXiv preprint arXiv:2406.10881 (2024).\n[5] Li, Jiaqi, Yixuan Tang, and Yi Yang. \"Know the unknown: An uncertainty-sensitive method for llm instruction tuning.\" arXiv preprint arXiv:2406.10099 (2024).\n[6] Kadavath, Saurav, et al. \"Language models (mostly) know what they know.\" arXiv preprint arXiv:2207.05221 (2022).\n[7] Kuhn, Lorenz, et al. \"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.\" The Eleventh International Conference on Learning Representations.\n[8] Feng, Shangbin, et al. \"Don't hallucinate, abstain: Identifying LLM knowledge gaps via multi-LLM collaboration.\" arXiv preprint arXiv:2402.00367 (2024).\n[9] Yadkori, Yasin Abbasi, et al. \"Mitigating llm hallucinations via conformal abstention.\" arXiv preprint arXiv:2405.01563 (2024)."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9MC7gORUM3", "forum": "1OLTJL1wHG", "replyto": "1OLTJL1wHG", "signatures": ["ICLR.cc/2026/Conference/Submission22789/Reviewer_ajja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22789/Reviewer_ajja"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451231640, "cdate": 1761451231640, "tmdate": 1762942388762, "mdate": 1762942388762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on an important question: when should an LLM trigger retrieval. It first measures the knowledge boundaries of the LLM, then uses correctness annotations as supervision to train the model to express uncertainty when it does not know the answer, and performs retrieval only when the model expresses uncertainty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper focuses on an important question: teaching the model to recognize its own knowledge boundaries and to trigger retrieval only when it does not know the answer.\n2. The paper is well written and logically coherent.\n3. The paper uses the principle of “answer only if you are confident” to suppress overconfidence, and it trains on atomic facts, which leads to relatively high accuracy."}, "weaknesses": {"value": "1. The paper lacks novelty — the idea of triggering retrieval only when the model does not know the answer is not new.\n2. There have been many works between 2023 and 2024 that use SFT (Supervised Fine-Tuning) to enable models to express uncertainty, and this paper is not fundamentally different from those approaches.\n3. The paper includes too few baselines and lacks citations to several foundational works in the areas of adaptive RAG and LLM knowledge boundary perception.\n\n\n[1] SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. EMNLP 2023\n\n[2] When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation. ACL 2024\n\n[3] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. ICLR 2024\n\n[4] Alignment for Honesty. NeurIPS 2024\n\n[5] Teaching Models to Express Their Uncertainty in Words. TMLR 2022"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0hkTzWxRS", "forum": "1OLTJL1wHG", "replyto": "1OLTJL1wHG", "signatures": ["ICLR.cc/2026/Conference/Submission22789/Reviewer_TJwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22789/Reviewer_TJwu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559999827, "cdate": 1761559999827, "tmdate": 1762942388372, "mdate": 1762942388372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses two key challenges in factual question answering with Large Language Models (LLMs): reducing hallucinated answers and minimizing unnecessary retrieval operations in Retrieval-Augmented Generation (RAG) systems.\nThe authors introduce ConfQA, a fine-tuning method where the model is trained to answer only when confident and otherwise respond with “I am unsure.”\nThis is achieved via a dampening prompt and training on atomic factual statements.\nBuilding upon this, they propose ConfRAG, a system that triggers RAG only if the base model expresses uncertainty, resulting in reduced hallucination rates and fewer external retrievals, while maintaining high accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. This work focuses important issues of LLM hallucinations and computational efficiency in RAG.\n2. The use of confidence signaling (“I am unsure”) and SFT objectives makes the framework conceptually straightforward.\n3. Experimental results demonstrate improvements over baselines across multiple benchmarks, notably lowering hallucination rates."}, "weaknesses": {"value": "1. The primary limitation of this work is its lack of novelty.\nTraining with the 'unknown' token is a commonly employed technique in many existing RAG systems (e.g., [1]).\nThis study does not offer substantial new insights beyond some empirical observations.\n\n2. Several design choices are not clearly explained. For example, the rationale behind the design of the dampener prompt and the \"unsure\" answer remains unclear. Is model performance highly sensitive to the choice prompt and answer? Additionally, how to adjust the proportion of \"unknown\" answers in the training data, and is this ratio critical to overall model performance? Regarding training with negative signals, what is the justification for using SFT instead of DPO or PPO? Providing further details and analysis on these points would strengthen the contribution of this work.\n\n[1] Enhancing Retrieval-Augmented Generation with Dehallucinating Parallel Context Extension."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L5RqOfFTnZ", "forum": "1OLTJL1wHG", "replyto": "1OLTJL1wHG", "signatures": ["ICLR.cc/2026/Conference/Submission22789/Reviewer_2uqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22789/Reviewer_2uqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993749528, "cdate": 1761993749528, "tmdate": 1762942388119, "mdate": 1762942388119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ConfRAG, a framework for triggering Retrieval-Augmented Generation (RAG) based on model confidence. The authors propose ConfQA, a fine-tuning strategy that teaches LLMs to respond with \"I am unsure\" when they lack confidence in factual answers. The key contributions include: (1) a RAG triggering mechanism based on explicit confidence assessment, (2) a fine-tuning method using atomic factual statements from DBPedia with a \"dampening prompt,\" and (3) comprehensive evaluation across 7 benchmarks showing hallucination reduction to <5% while maintaining accuracy comparable to always-on RAG with reduced latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper methodically addresses the RAG triggering problem with clear motivation (Figure 3 shows LLMs are overconfident)\n- Strong empirical results:\n\n1. Consistent hallucination reduction across diverse benchmarks\n2. Maintains or improves factuality scores\n3. Practical latency improvements demonstrated\n\n\n- The dampening prompt and DBPedia focus are well-motivated through ablations\n- 7 benchmarks covering different question types and domains show generalization\n- Detailed prompts, implementation details, and clear methodology facilitate reproduction\n- The framework is lightweight (no hidden-state inspection) and compatible with existing systems"}, "weaknesses": {"value": "- The core contribution beyond R-Tuning and IDK appears to be (1) the dampening prompt and (2) using DBPedia instead of MMLU. While effective, this feels incremental.\n- Training data limitations:\n\n1. Only 3K samples seems small for teaching general confidence behavior\n2. DBPedia focus on entity attributes may not transfer well to other factual question types\n3. No systematic study of data diversity vs. quality trade-offs\n\n\n- Fine-tuning results are primarily on Llama-3.1-70B. Claims about generalization would be stronger with results across model families and sizes.\n- RAG baseline concerns:\n\n1. The \"ideal RAG\" assumption (Section 3.1) is unrealistic\n2. Real RAG results (Table 3) show modest improvements, and on CRAG, RAG-everywhere still underperforms LLM-only\n3. More sophisticated RAG methods should be compared\n\n\n- Evaluation gaps:\n\n1. No human evaluation to validate LLM-as-a-judge decisions\n2. The \"ceiling\" metrics assume perfect RAG, which may be misleading\n3. Missing analysis of failure modes (when does ConfQA incorrectly say \"unsure\"?)\n\n\n\n- Limited theoretical insight: Why does the dampening prompt work so well? Why does DBPedia generalize better than MMLU? The paper is primarily empirical without deeper understanding."}, "questions": {"value": "- Can ConfQA fine-tuned on Llama-3.1-70B be used to trigger RAG for other models (e.g., GPT-4o)? Or does each model need separate fine-tuning?\n- How sensitive is the approach to the exact wording of the dampening prompt? Have you experimented with variations?\n- What is the optimal mix of entity popularities (head/torso/tail) in training data? Current 1K/1K/1K split seems arbitrary.\n- In Table 2, ConfQA still has 5.2% incorrect on DBPedia (in-domain). What characterizes these failures? Are they specific entity types or question patterns?\n- How do you determine if a question requires dynamic information? This seems crucial for the architecture in Figure 2.\n- How does this compare to calibration techniques or uncertainty estimation methods that don't require fine-tuning?\n- What is the cost of fine-tuning vs. the savings from reduced RAG calls? Is this approach cost-effective at scale?\n- he long-form results (Table 5) show modest improvements. Why doesn't the approach transfer as well to multi-claim scenarios?\n- Is binary \"unsure\" vs. answer optimal? Have you explored soft confidence scores that could enable more nuanced triggering?\nReal-world deployment: Have you deployed this in production? What practical challenges emerged beyond the experimental setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WqsqCMYus2", "forum": "1OLTJL1wHG", "replyto": "1OLTJL1wHG", "signatures": ["ICLR.cc/2026/Conference/Submission22789/Reviewer_7owA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22789/Reviewer_7owA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015943033, "cdate": 1762015943033, "tmdate": 1762942387889, "mdate": 1762942387889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ConfRAG, a framework for triggering RAG based on model confidence. The authors propose ConfQA, a fine-tuning strategy that teaches LLMs to respond with \"I am unsure\" when they lack confidence in factual answers. The key contributions include: (1) a RAG triggering mechanism based on explicit confidence assessment, (2) a fine-tuning method using atomic factual statements from DBPedia with a \"dampening prompt,\" and (3) comprehensive evaluation across 7 benchmarks showing hallucination reduction to <5% while maintaining accuracy comparable to always-on RAG with reduced latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses the RAG triggering problem with clear motivation (Figure 3 shows LLMs are overconfident)\n- Strong empirical results, including (1) consistent hallucination reduction across diverse benchmarks; (2) maintains or improves factuality scores; and (3) practical latency improvements demonstrated\n- The dampening prompt and DBPedia focus are well-motivated through ablations\n- 7 benchmarks covering different question types and domains show generalization\n- Detailed prompts, implementation details, and clear methodology facilitate reproduction\n- The framework is lightweight and compatible with existing systems"}, "weaknesses": {"value": "- The core contribution beyond R-Tuning and IDK appears to be (1) the dampening prompt and (2) using DBPedia instead of MMLU. While effective, this feels incremental.\n- Training data limitations: (1) only 3K samples seems small for training general confidence behavior, (2) DBPedia focus on entity attributes may not transfer well to other factual question types, and (3) lack of systematic study of data diversity vs. quality trade-offs\n\n- Fine-tuning results are primarily on Llama-3.1-70B. Claims about generalization would be stronger with results across model families and sizes.\n- RAG baseline concerns: (1) the \"ideal RAG\" assumption (Section 3.1) might be unrealistic; (2) real RAG results (Table 3) show modest improvements; (3) more advanced RAG baselines should be compared\n\n- Evaluation gaps: (1) lack of human evaluation to validate LLM-as-a-judge decisions; (2) the \"ceiling\" metrics assume perfect RAG, which may be misleading; and (3) lack of analysis of failure modes\n\n- Limited theoretical insight: Why does the dampening prompt work so well? Why does DBPedia generalize better than MMLU? The paper is primarily empirical without deeper understanding."}, "questions": {"value": "- Can ConfQA fine-tuned on Llama-3.1-70B be used to trigger RAG for other models (e.g., GPT-4o)? Or does each model need separate fine-tuning?\n- How sensitive is the approach to the exact wording of the dampening prompt? Have you experimented with variations?\n- What is the optimal mix of entity popularities (head/torso/tail) in training data? Current 1K/1K/1K split seems arbitrary.\n- In Table 2, ConfQA still has 5.2% incorrect on DBPedia (in-domain). What characterizes these failures? Are they specific entity types or question patterns?\n- How do you determine if a question requires dynamic information? This seems crucial for the architecture in Figure 2.\n- How does this compare to confidence calibration or uncertainty estimation methods that don't require fine-tuning?\n- What is the cost of fine-tuning vs. the savings from reduced RAG calls? Is this approach cost-effective at scale?\n- The long-form results (Table 5) show modest improvements. Why doesn't the approach transfer as well to multi-claim scenarios?\n- Is binary \"unsure\" vs. answer optimal? Have you explored soft confidence scores that could enable more nuanced triggering?\n- Real-world deployment: Have you deployed this in production? What practical challenges emerged beyond the experimental setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WqsqCMYus2", "forum": "1OLTJL1wHG", "replyto": "1OLTJL1wHG", "signatures": ["ICLR.cc/2026/Conference/Submission22789/Reviewer_7owA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22789/Reviewer_7owA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015943033, "cdate": 1762015943033, "tmdate": 1763671718493, "mdate": 1763671718493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}