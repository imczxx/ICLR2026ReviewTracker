{"id": "yDLD3D95w3", "number": 5686, "cdate": 1757927272339, "mdate": 1763587207428, "content": {"title": "Sign-SGD via Parameter-Free Optimization", "abstract": "Large language models have achieved major advances across domains, yet training them remains extremely resource-intensive. We revisit Sign-SGD, which serves both as a memory-efficient optimizer for single-node training and as a gradient compression mechanism for distributed learning. This paper addresses a central limitation: the effective stepsize cannot be determined a priori because it relies on unknown, problem-specific quantities. We present a parameter-free Sign-SGD that removes manual stepsize selection. We analyze the deterministic single-node case, and extend the method to stochastic single-node training and multi-node settings. We also incorporate the momentum technique into our algorithms and propose a memory-efficient variant that stores only gradient signs instead of full gradients. We evaluate our methods on pre-training LLaMA models (130M and 350M) and fine-tuning a Swin Transformer (28M). Across considered tasks, the proposed methods match the performance of tuned Sign-SGD and AdamW (grid-searched stepsizes with a cosine schedule), while avoiding tuning overhead. Employing parameter-free training yields approximately $1.5\\times$ end-to-end speedup compared to runs with grid-searched stepsizes.", "tldr": "", "keywords": ["Parameter-free optimization", "Sign descent", "Convex optimization", "Stochastic optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b392494ea6985f512c1f2283fdb3d59bbcb774c.pdf", "supplementary_material": "/attachment/40f5796240711f6bf937696da61bf236f2d35fcc.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a parameter-free variant of SIGN-SGD that automatically determines the learning rate using local gradient information without requiring manual tuning. The method estimates the step size via per-iteration approximations of the smoothness constant and loss gap, extending to stochastic and distributed settings. Theoretical analysis under convex assumptions supports convergence, and experiments on language and vision tasks show comparable performance to well-tuned AdamW and SIGN-SGD baselines, while removing the need for hyperparameter search."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a practical and long-standing issue in optimization—manual learning rate tuning—by introducing a parameter-free variant of SIGN-SGD. The proposed formulation is conceptually simple and mathematically grounded, extending classical gradient clipping and step-size estimation ideas in a coherent way. The theoretical analysis is well-presented and the experiments, though limited, are consistent and demonstrate that the approach can achieve competitive performance without hyperparameter tuning."}, "weaknesses": {"value": "Theoretical guarantees are proved under convex smooth assumptions, while the main experiments are on large non-convex models; the gap between theory and practice is not bridged.\n\nExperimental coverage is limited (few architectures/tasks and moderate scales), so generality across modalities and training regimes remains unclear.\n\nThe parameter-free step-size depends on noisy quantities (e.g., gradient-norm variation and loss gaps); there is no robustness/sensitivity analysis under high-noise or distributed settings."}, "questions": {"value": "Can the authors clarify how the proposed step-size estimation behaves under high gradient noise or distributed training, where loss variations can be inconsistent?\n\nHow sensitive is the performance to the local smoothness and loss-gap approximations used in the parameter-free update rule?\n\nDo the authors plan to extend the theoretical analysis beyond convex assumptions to better align with the non-convex experiments presented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "spSXq5wBU0", "forum": "yDLD3D95w3", "replyto": "yDLD3D95w3", "signatures": ["ICLR.cc/2026/Conference/Submission5686/Reviewer_HNri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5686/Reviewer_HNri"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760956439189, "cdate": 1760956439189, "tmdate": 1762918194536, "mdate": 1762918194536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the SIGN-SGD optimizer and proposes a novel parameter-free variant that eliminates the need to manually tune the learning rate. The core idea is an algorithm called ALIAS (Automatic Local per-Iteration Approximation of the Step size), which adaptively adjusts the step size at each iteration using the current gradient information. ALIAS estimates problem-specific constants on-the-fly, enabling SIGN-SGD to approach its theoretically optimal step size without needing prior knowledge. The method is developed first for the idealized deterministic setting and then extended to stochastic training and distributed training, addressing a known gap in prior parameter-free methods. The authors further introduce two practical enhancements: 1) a momentum-integrated version (ALIAS ADAM version) that incorporates ADAM-style 1st and 2nd moment accumulations for variance reduction, and (2) a memory-efficient modification that stores only the sign of the previous gradient to preserve the low memory footprint of SIGN-SGD.\n\nEmpirical evaluation are conducted on both LLM pre-training and vision model fine-tuning. The results show that ALIAS matches or closely approaches the performance of well-tuned optimizers (like SIGN-SGD with optimally tuned constant or cosine-scheduled LR, and even AdamW) but without any manual tuning. By removing the costly hyperparameter search, ALIAS yields about a 1.5x end-to-end speedup in training for large models. Overall, the paper’s contributions span theoretical guarantees for parameter-free SIGN-SGD and strong practical validation that this approach can simplify and accelerate large-scale training without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel param-free algorithm for SIGN-SGD: the paper introduces a novel optimizer (ALIAS) that requires no manual step size tuning for SIGN-SGD. This is an original contribution, as prior sign-based methods either fixed a step size or relied on problem-dependent choices. ALIAS uses a clever per-iteration adaptation that accumulates an estimate of the local smoothness and the loss gap $\\Delta^*$ to adjust $\\gamma_t$ automatically. This approach is param-free in that it doesn’t need line searches or doubling tricks each round, and it doesn’t introduce additional hyperparameter to tune. The authors also propose two extensions: a momentum-incorporated version and a memory-saving version, which further increase the impact and applicability of the method.\n\n2. Theoretical guarantees: The work provides a detailed theoretical analysis in the convex setting, proving convergence rates for the proposed algorithms. The proofs build on and extend the literature (e.g., adapting techniques from adaptive online learning and recent parameter-free methods) and show that ALIAS achieves convergence within a logarithmic factor of the optimal tuned SIGN-SGD. The authors manage to handle the sign operator in analysis: they obtain convergence guarantees in terms of gradient norm, which is appropriate since sign methods inherently target stationarity. They also handle the stochastic case with theoretical guarantees (theorem 3.9 for minibatch SGD) and outline how the method works in distributed training (Appendix F). This adds comprehensiveness to the coverage and shows the method isn’t a brittle special-case solution.\n\n3. Empirical result: The authors pre-train transformer models on C4 dataset and also fine-tune a vision transformer in Appendix, demonstrating generality. ALIAS is shown to be competitive with SOTA optimizers. For instance, validation loss of ALIAS on LLaMA-130M is within 1% of SIGN-SGD with an expertly tuned LR schedule. With momentum, ALIAS even slightly outperforms AdamW. These results substantiate their claim that the new method without tuning can match the accuracy of well-tuned baselines. The results are reinforced by the training curves (Fig.1) which show that the convergence trajectory of ALIAS closely tracks that of tuned existing optimizers throughout training. The authors report metrics like end-to-end speedup (1.5x faster training when tuning is removed) which emphasizes the practical benefit of the method in real work."}, "weaknesses": {"value": "1. Theoretical assumption: the theoretical guarantees are restricted to convex optimization (w/ smoothness assumption) and are expressed in terms of finding stationary points (grad norm). While this is standard for sign-based methods, it means the theory doesn’t directly guarantee improvement on the non-convex training objectives that the experiments address. The authors do discuss why a convex analysis is still informative for sign methods, but the strongest guarantees (Theorems 3.5, 3.9) hold under convexity or particular smoothness bounds. The empirical results suggest the method works for non-convex training but it would be better to close the gap between theory and expeirment.\n2. Momentum version is not fully param-free: the ADAM-style version reintroduces a LR parameter and momentum hyperparameter. The momentum version behaves more like AdamW. This is a weakness if one expected the momentum version to also automatically adjust its global scale. Similarly, the authors also had to tune weight decay via validation (0, 0.01, 0.1) for all methods including theirs."}, "questions": {"value": "1. Adaptivity to initial guess: ALIAS requires an initial approximation $d_0$ for $\\Delta^* = f(x_0) - f(x^)$ (or alternatively a known lower bound $f_e$ for $f(x^)$ in Option II). How sensitive is the method to this initial guess? If $d_0$ is set much smaller or larger than the true $\\Delta^*$, does ALIAS quickly correct the estimate via the $d_t = \\max(d_{t-1},\\ \\cdot)$ update, or could a poor $d_0$ slow down convergence initially? \n2. In momentum-integrated ALIAS, there are new hyperparameters: $\\beta_1,\\beta_2$ and base step size $\\gamma$ for the update. In your experiments, it appears you fixed these to standard ADAM values ($\\beta_1=0.9,\\ \\beta_2=0.999$) and chose $\\gamma_t=10^{-3}$ without further tuning. To what extent can these be considered default that work across tasks?\n3. Just for clarification: in Appendix F3,  ALIAS is extended to the distributed scenario. A question here is how the adaptive step size interacts with data-parallel distributed training: Do all ranks synchronize the value of $\\gamma_t$ each iteration, or does each node run its own version of ALIAS based on local gradient estimates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZTzkfXQrxB", "forum": "yDLD3D95w3", "replyto": "yDLD3D95w3", "signatures": ["ICLR.cc/2026/Conference/Submission5686/Reviewer_RrFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5686/Reviewer_RrFQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501424701, "cdate": 1761501424701, "tmdate": 1762918193970, "mdate": 1762918193970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a variant of the sign-GD algorithm. By estimating the Lipschitz and decaying factor during the optimization process, the authors give the algorithm with theoretical analysis on the exact case, the stochastic case, and the memory-efficient case.  The experimental results on the LLaMA 130M model show that the algorithm performance is similar or even slightly better than the tuning version."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a hyper-parameter-free sign-GD algorithm with its stochastic version and memory-efficient version.\n\n2. For all versions, the authors give a convergence analysis.\n\n3. The experimental results of the stochastic version and memory-efficient version work well on 130M model."}, "weaknesses": {"value": "1. It is not clear what algorithm it is in the stochastic version, especially in the calculation of $d_t$.\n\n2. The expression of $\\epsilon = xxx$ is confusing. It seems that $\\epsilon$ is also related to the $T$ and gradient norm. But according to the proof, it should be $xxx \\leq \\epsilon$ when $T \\leq O(...)$.\n\n3. It is well-known that sign-sgd will not converge. It is unclear how the algorithm overcomes the issue."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "evKUjVHhUR", "forum": "yDLD3D95w3", "replyto": "yDLD3D95w3", "signatures": ["ICLR.cc/2026/Conference/Submission5686/Reviewer_JFUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5686/Reviewer_JFUp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987883514, "cdate": 1761987883514, "tmdate": 1762918193652, "mdate": 1762918193652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a variant of SignSGD which does not require manual step size selection for a given optimization task and the proposed ALIAS method provides stepsize prescription at each iteration guided by theory. Experiments on llama (upto 350M) and swin-transformer (28M) tasks show that ALIAS is competitive against tuned setups with learning rate schedules."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper tackles an important problem of mitigating hyperparameter tuning of optimizers which is quite relevant in the current era of LLMs.\n* The proposed algorithm, guided by theory, is simple to implement and has practical benefits such as lower costs in single-node/distributed settings and no need for LR tuning. Also, kudos to authors for providing a memory-efficient version of the algorithm that is already efficient relative to SOTA optimizers such as AdamW."}, "weaknesses": {"value": "* There are a bunch of hyperparameter-free methods that also eliminate the need for manually setting step sizes [1] but no such baselines are presented in this work, why?\n* Appendix A.1.1 mentions that only main model params are optimized by ALIAS and LM head params are optimized by AdamW — this detail is quite critical and must be mentioned in the main paper. How is AdamW learning rate selected for LM head params? Is it tuned through grid-search? If yes, then isn’t the point of param-free / saving tuning effort redundant since one can simply tune main model params while tuning LM head params? Moreover, how does ALIAS perform if it is also used for LM head params?\n* It looks ALIAS SignSGD performs a bit worse compared to tuned LR setup with cosine schedule (Table 1) and ALIAS Adam version also benefits from LR schedules (Figure 1). Can authors please confirm if this work eliminates the need for LR tuning but the problem of tuning schedules still remains to achieve the optimal performance?\n* There is a scope in improving the experimental setup. AdamW b2=0.999 as used in pretraining experiments isn’t standard, b2=0.95 is generally used and ideally should be tuned as per the setup. Also, it’s unclear if the method generalizes to different tasks such as those in AlgoPerf [1].\n\n[1] Kasimbeg et al. “How far away are truly hyperparameter-free learning algorithms?” TMLR 2025. https://arxiv.org/abs/2505.24005"}, "questions": {"value": "See weaknesses above.\n\nAdditional question: can authors expand on LR and other hyperparameters (e.g. weight decay, schedule hyperparams) search space used for tuning the baselines? How many hyperparameter trials were performed for each baseline and the proposed approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rAf90X4YT0", "forum": "yDLD3D95w3", "replyto": "yDLD3D95w3", "signatures": ["ICLR.cc/2026/Conference/Submission5686/Reviewer_uXJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5686/Reviewer_uXJ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762715007298, "cdate": 1762715007298, "tmdate": 1762918193170, "mdate": 1762918193170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}