{"id": "7GP64w3K4E", "number": 16731, "cdate": 1758268141506, "mdate": 1763131021199, "content": {"title": "FraudBench: A Benchmark for Web Fraud Attacks Against LLM-Driven Agents", "abstract": "LLM-driven agents are being severely threatened by web fraud attacks, which aim to induce agents to visit malicious websites. Upon success, attackers can use these websites to launch numerous subsequent attacks, which dramatically enlarges the attack surface. However, there have not been systematic benchmarks specifically designed for this newly emerging threat. To this end, this paper proposes FraudBench, the first dedicated benchmark of web fraud attacks. FraudBench contains over 61,845 attack instances across 10 distinct scenarios, 7 categories of real-world malicious websites. Experiments using 11 popular LLMs reveal that web fraud attacks have high attack success rates on them. Besides, we also comprehensively analyze the critical factors that can influence the attack success rate observed in the experiments. Our work provides in-depth insight into web fraud attacks for the first time and demonstrates the urgency of paying attention to agent security when handling web links.", "tldr": "The first benchmark that evaluate agent's vulnerabilities in the face of web fraud attacks, a novel kind of attack against agents", "keywords": ["agent", "web fraud attack", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/013850ea2f69a2dc1b0fbbe3ca209a563c5b7d18.pdf", "supplementary_material": "/attachment/400cb819b87e6c8d83aa76056994fe94c6b4045c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FraudBench, a benchmark targeting web-fraud link attacks that try to make LLM agents trust/visit malicious URLs. It consists of diverse scenarios, categories and different attack templates. Evaluation also covers both open-source and closed-source models. Results show that current agents’ incapabilities in reliably differentiating between trusted and untrusted web urls."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The writing is clear and easy to follow.\n\nThe scenario is novel.\n\nThe benchmark is diverse."}, "weaknesses": {"value": "The definition of attack success is unclear in the current paper. It is not specified whether a successful attack refers to (1) the agent merely outputting textual content indicating intent to click or trust a link, or (2) the agent actually following and navigating to the malicious webpage. If the success criterion is solely based on the textual output, this could lead to false positives—cases where the agent verbally expresses intent but fails to execute the action, a well-known grounding issue in agent literature.\n\nMoreover, the threat model needs to be clearly articulated. It remains ambiguous whether (a) the user directly provides the malicious URL and instructs the agent to visit it, or (b) the agent encounters the malicious link during browsing or executing a task. These two scenarios have fundamentally different implications. If the link is directly provided by the user, it is reasonable for the agent to visit it—merely opening a webpage is not inherently unsafe. However, if the malicious URL is embedded within a webpage that the agent visits, the surrounding context (layout, text, visual cues, etc.) becomes crucial, and the benchmark should account for such contextual signals to improve realism.\n\nFinally, visiting a malicious webpage itself is not the ultimate concern. The core safety risk lies in whether the agent, after visiting the page, becomes misled into performing unintended or harmful actions based on instructions or deceptive content within that webpage. This second-stage manipulation is what should truly be benchmarked. For instance, in the example on line 645 (“I’m here to help with your weather inquiries! … please visit the full report page for more accurate and detailed information: {link}”), even a human reader would likely follow the link, as the text appears benign. The real issue arises only if, once on that site, the agent is coerced into performing unsafe operations. Capturing this vulnerability would substantially enhance the benchmark’s practicality."}, "questions": {"value": "Why does the author want to differentiate between malicious URL and prompt injection? Basically, I may feel that a malicious URL is just one type of indirect prompt injection, though instantiated with injection in the URL. They both target the vulnerability that LLMs cannot reliably tell what to trust and what not to trust."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WKODZtjtJe", "forum": "7GP64w3K4E", "replyto": "7GP64w3K4E", "signatures": ["ICLR.cc/2026/Conference/Submission16731/Reviewer_xRQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16731/Reviewer_xRQd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760512337917, "cdate": 1760512337917, "tmdate": 1762926782418, "mdate": 1762926782418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FraudBench, the first dedicated benchmark for web fraud attacks— a new threat that exploits web link structures to trick LLM-driven agents into visiting malicious websites. It covers real-world scenarios and malicious website categories, tests 11 popular LLMs to show these attacks have high success rates, and analyzes key factors influencing attack effectiveness, filling the gap of no specialized benchmarks for this emerging threat and highlighting the urgency of agent security when handling web links."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Has insightful observations and analyses about MoE models.\n\n* The entry point and the design of the benchmark are quite interesting."}, "weaknesses": {"value": "* The citation format seems incorrect.\n\n* I recommend you not use “fraudbench” — the name sounds too broad, and since you’re focusing on agents, especially in the website setting, why not choose a more specific name?\n\n* Line 311: “GPT-3.5-turbo”\n\n* I’m concerned that determining whether a web link is fake merely by the link itself might be too easy. I suspect that if you give the model a prior prompt (e.g., “The following links may contain fraudulent information, please be careful”), the ASR rate could drop significantly. Of course, I understand it won’t drop to zero, but I think you should include such data.\n\n* There have already been quite a few works on fraud detection in single modality, such as “fraud-r1,” which introduces more common multi-turn dialogue scenarios. Although your approach is very novel, the amount of work still feels somewhat limited. The paper lacks a comprehensive table summarizing all results.\n\n* For such a benchmark that targets a very specific domain, I think you could make the results section more concise and dedicate a section to discussion — share your thoughts on how to improve models’ capabilities in this area. You must have discovered something unique while constructing this benchmark, and including that would make the paper more inspiring for the community."}, "questions": {"value": "Do you have any real-world cases or can you put some case study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aAeLku3sPa", "forum": "7GP64w3K4E", "replyto": "7GP64w3K4E", "signatures": ["ICLR.cc/2026/Conference/Submission16731/Reviewer_eVmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16731/Reviewer_eVmY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743628646, "cdate": 1761743628646, "tmdate": 1762926781937, "mdate": 1762926781937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FraudBench, the first comprehensive benchmark specifically designed to evaluate LLM agents’ vulnerabilities to web fraud attacks. Unlike prior works that focus on jailbreaks or prompt injections, FraudBench targets link-structured attacks that exploit the semantics of URLs (e.g., subdomains, directories, parameters) to disguise malicious intent. The authors construct a large-scale benchmark consisting of 61,845 attack instances across 10 real-world user scenarios, 7 malicious website categories, and 15 attack templates, generated through a hybrid process of manual design and LLM-assisted expansion. Evaluations on 11 widely used LLMs reveal significant susceptibility, with attack success rates ranging from 26.5% to 99.9%. The paper further analyzes factors influencing attack success, such as model type, TLD category, and link length, providing novel insights into how web link structures compromise LLM-based agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript addresses the issue of web fraud attacks, which is a real and urgent problem with significant practical relevance.\n\n- It introduces a novel and rarely discussed perspective within web fraud—namely, attacks based on link content and structure, which is both innovative and engaging.\n\n- The study further covers a comprehensive range of real-world scenarios and attack forms, and employs LLMs to generate an extensive set of attack samples, enhancing the benchmark’s breadth."}, "weaknesses": {"value": "- The manuscript does not propose any concrete defense or mitigation strategies, focusing solely on reporting attack outcomes. This weakens the overall contribution.\n\n- While the proposed link modification attacks reflect realistic web cases, they appear technically unchallenging. The diverse yet textually limited link content is inherently difficult to distinguish as fraudulent, which limits the potential benefits for the research community.\n\n- Moreover, since part of the dataset was generated by LLMs, concerns remain about its objective authenticity, even after filtering—making the work resemble a lab-scale experiment of “LLMs attacking LLMs.”\n\n- Finally, the discussion on factors such as domain types and field lengths, though empirically analyzed, lacks strong theoretical or practical justification, offering limited insights beyond observed correlations; deeper investigations would be needed to uncover more substantive findings.\n\n- The work raises ethical concerns regarding user privacy and potential social harm, which should be further examined and discussed."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The framework proposed in the manuscript could be directly used to generate malicious web links, potentially endangering users' privacy and security."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KDy7oQPEYn", "forum": "7GP64w3K4E", "replyto": "7GP64w3K4E", "signatures": ["ICLR.cc/2026/Conference/Submission16731/Reviewer_sbKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16731/Reviewer_sbKu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814697154, "cdate": 1761814697154, "tmdate": 1762926781585, "mdate": 1762926781585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark designed to evaluate the success rate of web-based fraud attacks targeting LLM-driven agents. The authors distinguish their work from existing benchmarks by emphasizing web link interactions rather than natural language inputs. To build the dataset, they combine manual scenario construction with LLM-assisted generation, producing ten fraud-related scenarios such as package tracking, online customer service, and shopping assistance.\n\nTheir experiments show attack success rates ranging from 26.5% to 99.8% across several popular LLMs, indicating that these agents can often be deceived into visiting malicious websites within the crafted scenarios. Additionally, the authors find that closed-source models tend to be more susceptible than open-source ones, and that mixture-of-experts (MoE) architectures exhibit higher vulnerability compared to dense models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The comparison results between open vs. closed models and dense vs. MoE architectures are interesting and could inspire further investigation.\n- Security of LLM-driven agents in web environments is a timely and practically-important topic."}, "weaknesses": {"value": "1) Limited novelty and technical depth: The primary contribution is adding web links to existing attack scenarios, which is a modest extension of prior work.\n\n2) Scenario generation lacks rigor: The dataset relies on partial manual construction and LLM assistance, without clear methodological justification or validation.\n\n3) Insufficient analysis of results:\n\n- The observed differences between open vs. closed models and dense vs. MoE models are not adequately explained or supported by evidence.\n\n- The authors’ hypothesis regarding MoE vulnerabilities (i.e., lack of expert training on fraud detection) remains speculative without empirical support.\n\n4) Limited actionable insights: The benchmark reveals vulnerabilities but offers no concrete guidance on how these findings can improve model robustness or training practices."}, "questions": {"value": "1) What factors make closed-source models more susceptible to web-fraud attacks than open-source ones?\n\n2) Why can’t MoE models leverage the appropriate experts to detect or mitigate such attacks?\n\n3) What practical insights or design recommendations can this benchmark provide to improve LLM robustness against web-based fraud?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "44kMYSq6yo", "forum": "7GP64w3K4E", "replyto": "7GP64w3K4E", "signatures": ["ICLR.cc/2026/Conference/Submission16731/Reviewer_dx6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16731/Reviewer_dx6t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938403536, "cdate": 1761938403536, "tmdate": 1762926781215, "mdate": 1762926781215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FraudBench, a benchmark designed to evaluate web fraud attacks that attempt to trick LLM-driven agents into visiting malicious websites. FraudBench includes over 61K attack instances covering 10 real-world scenarios, 7 categories of malicious websites, and 15 attack templates. The benchmark is evaluated on 11 LLMs, showing high attack success rates and analyzing factors such as model size, domain type, and link structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a large-scale dataset with diverse real-world examples.\n2. Evaluates a broad set of models, both open- and closed-source."}, "weaknesses": {"value": "1. The data-generation pipeline is straightforward, mainly LLM-assisted concatenation of natural-language prompts with malicious URLs, without novel algorithms, modeling, or challenging engineering aspects.\n2. The paper is not truly “agent-based.” Although the motivation centers on LLM-driven agents, the experiments only test whether models produce text outputs that appear to “visit” malicious links. There is no real multi-step reasoning or interactive environment demonstrating actual agent behavior.\n3. The evaluation results are mostly descriptive statistics (attack success rates by domain type or link length) rather than providing mechanistic or causal insights about why models fail. The findings do not lead to actionable guidance for mitigation or model improvement.\n4. The paper lacks discussions on potential defense methods or mitigation strategies."}, "questions": {"value": "1. How do the authors ensure that the “attack success” measured on text outputs translates to realistic agent exploitation in interactive systems?\n2. Have the authors considered including simple baselines or defenses (e.g., link-filtering heuristics, URL-embedding detection) to quantify difficulty or realism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8DcrA3F7og", "forum": "7GP64w3K4E", "replyto": "7GP64w3K4E", "signatures": ["ICLR.cc/2026/Conference/Submission16731/Reviewer_vBbM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16731/Reviewer_vBbM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070210528, "cdate": 1762070210528, "tmdate": 1762926780893, "mdate": 1762926780893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}