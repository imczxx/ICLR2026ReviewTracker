{"id": "gqCh1k0CEX", "number": 17659, "cdate": 1758278934301, "mdate": 1763732549200, "content": {"title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "abstract": "Subword-level understanding is integral to numerous tasks, including understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming, and wordplay. Despite this, current large language models (LLMs) still struggle disproportionally with seemingly simple subword-level tasks, like counting the number of 'r's in 'strawberry'. A key factor behind these failures is tokenization, which obscures the fine-grained structure of words. Current alternatives, such as character-level and dropout tokenization methods, significantly increase computational costs and provide inconsistent improvements. In this paper, we revisit tokenization and introduce StochasTok, a simple, efficient stochastic tokenization scheme that randomly splits tokens during training, allowing LLMs to ‘see’ their internal structure. Our experiments show that pretraining with StochasTok substantially improves LLMs’ downstream performance across multiple subword-level language games, including character counting, substring identification, and math tasks. Furthermore, StochasTok’s simplicity allows seamless integration at any stage of the training pipeline, and we demonstrate that post-training with StochasTok can instill improved subword understanding into existing pretrained models, thus avoiding costly pretraining from scratch. These dramatic improvements achieved with a minimal change suggest StochasTok holds exciting potential when applied to larger, more capable models.", "tldr": "A simple stochastic tokenization method—randomly splitting tokens before pretraining—that dramatically improves fine-grained, subword-level understanding in language models without any compromise to benchmark performance or increase in training cost.", "keywords": ["language models", "tokenization", "pretraining", "finetuning", "subword understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38a3d46e10b57544fcac4055e60e3f4f216f12b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes STOCHASTOK, a simple stochastic tokenization scheme that preserves the original vocabulary and introduces *training-time only* reversible splits of existing tokens to expose subword/character structure. The approach aims to improve tokenization invariance without changing model architecture or deployment-time tokenization. Empirically, it delivers consistent gains on curated subword “language games” and cross-tokenizer multi-digit addition while keeping standard LM metrics roughly unchanged on small/medium models; qualitative analyses (e.g., embedding alignment across segmentations) support the intuition. The idea is elegant and easy to integrate (especially for CPT), but evidence on larger models and real-world tasks is currently limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Simplicity & compatibility**: Fixed vocabulary, tokenizer-agnostic, and *train-time only* noise makes it a low-friction drop-in (particularly for CPT) with minimal code churn.  \n2. **Clear signal on subword skills**: Reproducible improvements on subword-aware tasks and cross-tokenizer arithmetic transfer, with no obvious small-scale regressions on general LM metrics.  \n3. **Practical deployment story**: Inference remains deterministic, avoiding train–test segmentation mismatch common to methods that alter the tokenizer itself.  \n4. **Plausible mechanism**: Training with multiple valid segmentations encourages segmentation-invariant internal features; analyses suggest layer-wise convergence toward shared representations."}, "weaknesses": {"value": "1. **External validity at scale**: No results on ≥4B/7B models; claims of robustness and easy integration would be stronger with short-budget CPT evidence at those scales.  \n2. **Evaluation scope**: “Real” math (e.g., GSM8K/MATH) and broader tasks (MMLU/BBH/code/RC) are missing; current gains mostly establish tokenization invariance rather than end-task improvements.  \n3. **Compute/cost parity**: No end-to-end cost curves (length inflation, throughput, VRAM, wall-time) or budget-matched comparisons, making Pareto efficiency unclear."}, "questions": {"value": "1. **Scale-up sanity check**: Can you run short-budget CPT on one **4B** and one **7B** model (e.g., 3–10k steps, \\(p \\in \\{0.05, 0.1\\}\\)) and report subword tasks, a small **MMLU** slice (no-regression), and **throughput/length/VRAM**?  \n2. **“Real” math**: On **GSM8K** (dev is fine), compare deterministic vs **BPE-dropout** vs **STOCHASTOK** under **equal compute**, including training curves and final accuracy; also test cross-tokenizer transfer at this scale."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LO0V288lvk", "forum": "gqCh1k0CEX", "replyto": "gqCh1k0CEX", "signatures": ["ICLR.cc/2026/Conference/Submission17659/Reviewer_ZDYH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17659/Reviewer_ZDYH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760683658436, "cdate": 1760683658436, "tmdate": 1762927514284, "mdate": 1762927514284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new stochastic tokenization approach called StochasTok. StochasTok allows for flexible, alternative tokenizations of the same token. Results on various datasets including language games, pre-training small-scale models (50-275M parameters) offer large performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and easy to read.\n* Tokenization is a really important and often neglected area of LMs.\n* The method proposed is simple and works well\n* The paper offers a comprehensive analysis and discussion of experiments and results."}, "weaknesses": {"value": "* Really minor (without trying to be \"Reviewer 2\"): the models tested are really small for 2025 standards. There is a risk that the gains might not generalize to larger scales."}, "questions": {"value": "- Perhaps experimenting with slightly larger models might offer a clearer picture if the gains from your method are similar in larger LMs (e.g. ~1B)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NIfA4mYDsi", "forum": "gqCh1k0CEX", "replyto": "gqCh1k0CEX", "signatures": ["ICLR.cc/2026/Conference/Submission17659/Reviewer_eKHe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17659/Reviewer_eKHe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308087959, "cdate": 1761308087959, "tmdate": 1762927513721, "mdate": 1762927513721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StochasTok, a novel stochastic tokenization method designed to address a key limitation of large language models: their poor performance on subword-level reasoning tasks. Standard tokenizers treat words as opaque symbols, obscuring the internal character structure and making tasks like counting letters or performing multi-digit arithmetic surprisingly difficult for even state-of-the-art models.\n\nThe proposed method operates as a lightweight post-processing step that randomly splits tokens into smaller, valid subtokens from the existing vocabulary, thereby exposing the model to the morphological composition of words during training. The authors demonstrate through extensive experimentation that StochasTok significantly enhances subword reasoning capabilities. Their approach consistently and substantially outperforms strong baselines in both pretraining and finetuning scenarios, achieving near-perfect accuracy on language game benchmarks like LangGame and CUTE, and enabling models to rapidly learn complex tasks such as multi-digit addition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed StochasTok approach offers an elegantly simple yet effective solution to the fundamental limitation of subword understanding in LLMs. Its implementation as either a pretraining enhancement or a lightweight finetuning step makes it highly practical and accessible.\n2. The authors provide comprehensive validation across multiple domains - from language games (LangGame, CUTE) to mathematical reasoning (multi-digit addition) - demonstrating the method's versatility and robust performance gains.\n3. The paper provides some insights into the method's internal mechanisms through embedding visualizations"}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. I acknowledge the contributions of this work. However, from my understanding, StochasTok appears to be a special case of BPE-dropout? Is it true that for every tokenization generated by StochasTok, there exists an equivalent BPE-dropout tokenization? If so, could the authors provide an intuitive explanation for why StochasTok performs so much better than BPE-dropout in the experiments? Are there key differences in how the stochasticity is applied or how the model learns from these variations that lead to the significant performance gap? Could the authors provide a more detailed comparative analysis between the two?\n2. The paper states that \"In BPE, intermediate tokens not present in the final tokenized training dataset are removed from the vocabulary, meaning BPE-dropout can produce tokens outside the original vocabulary\", which I'm not sure. If true, how does it encode user inputs during inference, given this vocabulary mismatch?\n3. In Figure 1, the \"no pretraining\" model shows a rapid increase in both training and validation accuracy very early in training, followed by a sudden decrease. What is the authors' explanation for this phenomenon?\n4. In Section 5, which focuses on multi-digit addition, what was the range of digits for the numbers used in the training and validation sets?\n5. Could you please report performance on individual subtasks of both LangGame and CUTE (e.g., \"Inverse Spelling\", \"Char Deletion\")? This would provide clearer insights into the model's capabilities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EHYeRh7EDm", "forum": "gqCh1k0CEX", "replyto": "gqCh1k0CEX", "signatures": ["ICLR.cc/2026/Conference/Submission17659/Reviewer_HeMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17659/Reviewer_HeMs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934440392, "cdate": 1761934440392, "tmdate": 1762927513287, "mdate": 1762927513287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new tokenization method STOCHASTOK to improve subword-level understanding of language models. The method is simple, efficient, and compatible to existing tokenization methods. Experiments show that pretraining models with STOCHASTOK improves model perofrmance on language game tasks and multi-digit addition. The paper also demonstrates that applying STOCHASTOK during fine-tuning can enhance the subword understanding of already pretrained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear writing:** the paper is well-structured, clearly written, and easy to follow\n2. **Method simplicity and effectiveness:** the proposed method is simple, efficient, and shows promising performance improvements across tasks. \n3. **Comprehensive experiments:** the experiments cover multiple training settings and analyses, and consistenly demonstrate the method's advantages."}, "weaknesses": {"value": "1. **Limited model scale**: the experiments are conducted only on 50M-parameter models and GPT-2. While these show reasonable performance on simple language tasks (BLIMP & ARC), it remains unlearn whether the improvements generalize to larger models. It would be valuable to see results on larger (1B/7B) models. I understand such experiments are complex and do NOT expect them for the rebuttal. \n\n2. **Limited evaluation scope**: the current evaluations focus mainly on artificial tasks such as word games and digit addition. It would strengthen the paper to provide more results and discussions on  improvements on more realistic tasks."}, "questions": {"value": "1. The experiments only reports results for STOCHASTOK with p<=0.1, is this an intentional design choice? How does performance change with higher p?\n\n2. Does STOCHASTOK improves model robustness to spelling errors or out-of-vocabulary words?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oAR41j6Szh", "forum": "gqCh1k0CEX", "replyto": "gqCh1k0CEX", "signatures": ["ICLR.cc/2026/Conference/Submission17659/Reviewer_dC4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17659/Reviewer_dC4j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045440962, "cdate": 1762045440962, "tmdate": 1762927512775, "mdate": 1762927512775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}