{"id": "hVr4mQPdBJ", "number": 3170, "cdate": 1757347949842, "mdate": 1763514722728, "content": {"title": "Better LLM Reasoning via Dual-Play", "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves—thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble substantially improves the reasoning performance of LLMs.", "tldr": "We propose a novel framework to train two models to evolve through iterative competition.", "keywords": ["LLM Reasoning; Adversarial Learning; Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/797a5ac9c5cfadef1bfbf8d2eb79f305a18a9fa0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper aims to improve the reasoning capabilities of a large language model (LLM) on tasks grounded in a specific knowledge base. The approach trains two identical LLMs: the Proposer, which has access to this knowledge base and generates the most challenging questions; and the Solver, which does not have access to it and attempts to answer them. Training is performed using Reinforcement Learning with Verifiable Rewards (RLVR), where the objective function encourages the Proposer to produce difficult, knowledge-based questions that expose the Solver’s weaknesses. The authors study both online (joint updates) and offline (question-buffer) training variants and report gains on six math benchmarks across several Qwen base models, with the largest improvements on Qwen3-1.7B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a well-defined training setup that leverages reinforcement learning with verifiable rewards to improve reasoning performance. It achieves strong empirical results: Qwen3-1.7B-Base improves by about 20 points in pass@1 accuracy, despite using limited supervision. The presentation is clear, with consistent terminology and a straightforward description of the training process. The method is evaluated on multiple math benchmarks, demonstrating solid improvements over strong baselines."}, "weaknesses": {"value": "- Several average scores reported in Table 1 are incorrect — at least six appear miscalculated (e.g., Qwen3‑1.7B Coldstart: 29.55 → 24.63; PasoDoble Offline: 47.51 → 39.59). These are not minor rounding errors, but significant numerical inconsistencies that affect the paper’s main claims. This undermines trust in the evaluation and should be corrected. \n\n- After correcting the scores, Coldstart consistently underperforms the corresponding Base models across all configurations, despite being fine-tuned on the same domain-specific knowledge. This is unexpected and suggests that the supervised finetuning stage may be ineffective or even detrimental.\n\n- The paper omits discussion and comparison to closely related approaches, particularly Agentic Adversarial QA for Improving Domain-Specific LLMs [1]. That work also uses a two-agent setup to expose model weaknesses through adversarial question generation, but follows a different methodology: an offline framework that selects challenging questions using text-based gradient feedback rather than reinforcement learning. A direct comparison—either conceptual or empirical—would help clarify the novelty of PasoDoble and better position it within the broader landscape of dual-agent self-training methods.\n\n- The experimental setup focuses exclusively on mathematical reasoning tasks (e.g., GSM8K, MATH, OlympiadBench), which limits the generalizability of the method. While math is a well-established domain for evaluating structured reasoning, it's unclear whether the proposed approach would transfer to other domains such as programming, science QA, or commonsense reasoning. Including evaluations on a more diverse set of benchmarks would strengthen the claims of improving general reasoning capabilities.\n\n[1] Grari, V., Tomoiaga, C., Lamprier, S., Hashimoto, T., & Detyniecki, M. (2025). Agentic Adversarial QA for Improving Domain-Specific LLMs. In Second Workshop on Test-Time Adaptation: Putting Updates to the Test! at ICML 2025."}, "questions": {"value": "- After correcting the reported averages in Table 1, Coldstart consistently underperforms the Base model across all model sizes. Could the authors clarify why fine-tuning on a domain-specific knowledge base results in worse performance? Does this point to issues in the training setup, data quality?\n\n- Could you clarify how PasoDoble differs conceptually from Agentic Adversarial QA for Improving Domain-Specific LLMs [1]? Both use a two-agent setup for adversarial question generation — is there a specific reason it was not discussed or compared in the paper?\n\n- Have you considered applying PasoDoble to non-math domains (e.g., code generation, scientific QA)? If not, what are the key limitations or challenges?\n\n- How sensitive is the performance to the thresholds for clipping?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WlRC2UW8Hp", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Reviewer_GHTc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Reviewer_GHTc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841231300, "cdate": 1761841231300, "tmdate": 1762916582554, "mdate": 1762916582554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/6)"}, "comment": {"value": "We post a general response to provide additional experimental results and address the common questions and concerns from the reviewers.\n\n# Additional experiments\n## Analysis of the Proposer\nWe conduct additional experiments to analyze the Proposer’s questions and how the Proposer correlates with the knowledge piece. All analyses in this section are conducted under the\noffline training setting with Qwen3-1.7B. We choose the offline setting due to its greater training\nstability: compared to the online setting, it exhibits more consistent reward signals and smoother\nbehavior across training checkpoints. This stability enables clearer observation of how Proposer\ndynamics evolve over time and facilitates more reliable comparative analysis. (See Appendix D for a detailed comparison between online and offline settings.)\n\n\n### Analysis of Knowledge\nFirst, we analyze the external knowledge used by the Proposer to generate questions. The knowledge is grouped into five content-based categories, and individual items may be assigned to multiple categories.\n\n* **Concept/Theorem**. The knowledge describes a mathematical concept (e.g., the definition of a topic or notion) or a theorem (e.g., a statement about the properties of a mathematical object). Because many knowledge pieces are extracted excerpts, their descriptions may be incomplete, for example, presented only in natural language without formal notation or missing key details. We therefore further divide this category into complete and incomplete subtypes. A knowledge piece that describes one concept/theorem completely but another incompletely is counted in both subcategories.\n* **Problem**. The knowledge explicitly describes a mathematical problem to be solved. In many cases, such knowledge pieces are excerpts from homework assignments or problem sets. Some also originate from textbooks or Wikipedia entries describing conjectures or open problems.\n* **Method**. The knowledge describes a procedure or technique for solving a particular type or instance of a mathematical problem. In many cases, it also explicitly states the corresponding problem itself.\n* **Other**. Knowledge pieces that do not fall under any of the above categories are classified as Other.\n\n| **Type** | **Percentage** |\n| :------------ | :------------: |\n| Concept/Theorem | 61.67 |\n| &nbsp;&nbsp;&nbsp;&nbsp; Complete             | 36.50 |\n| &nbsp;&nbsp;&nbsp;&nbsp; Incomplete           | 25.17 |\n| Problem                | 33.50 |\n| Method                 | 52.50 |\n| Other                    | 10.50 |\n\nWe aggregate the knowledge pieces from checkpoints 0 to 200 and report the results in the above table. Approximately 60% of the knowledge pieces describe mathematical concepts or theorems, of which roughly three-fifths are complete. In addition, 30% of the knowledge pieces pertain to problems and 50% pertain to solution methods. Only a small number fall into the “Other” category. As shown in our manual inspection in Appendix I, the QA pairs in this category primarily concern educational context (e.g., descriptions of math education programs, teaching experiences, or courses) rather than substantive mathematical content.\n\nWe further analyze which math areas the knowledge pieces fall into. In particular, we define the following math domains: algebra, number theory, geometry, trigonometry, calculus, combinatorics, probability/statistics, optimization, and other. We then categorize the knowledge pieces into one of the domains.\n\n| **Domain** | **Percentage** |\n| :------------ | :------------: |\n| Algebra                      | 17.83 |\n| Number Theory         | 7.83 |\n| Geometry                  | 8.33 |\n| Trigonometry             | 1.83 |\n| Calculus                    | 7.33 |\n| Combinatorics           | 4.00 |\n| Probability/Statistics  | 18.67 |\n| Optimization              | 0.33 |\n| Other                         | 33.83 |\n\n​​The results are shown in the table above. The most common domains are Algebra, Probability/Statistics, and Other. Upon manual inspection, many of the knowledge pieces categorized under \"Other\" pertain to subjects such as chemistry, physics, or programming."}}, "id": "4lAxBwFcWq", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147113127, "cdate": 1763147113127, "tmdate": 1763147922629, "mdate": 1763147922629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/7)"}, "comment": {"value": "We post a general response to provide additional experimental results and address the common questions and concerns from the reviewers.\n\n# Additional experiments\n## Analysis of the Proposer\nWe conduct additional experiments to analyze the Proposer’s questions and how the Proposer correlates with the knowledge piece. All analyses in this section are conducted under the\noffline training setting with Qwen3-1.7B. We choose the offline setting due to its greater training\nstability: compared to the online setting, it exhibits more consistent reward signals and smoother\nbehavior across training checkpoints. This stability enables clearer observation of how Proposer\ndynamics evolve over time and facilitates more reliable comparative analysis. (See Appendix D for a detailed comparison between online and offline settings.)\n\n\n### Analysis of Knowledge\nFirst, we analyze the external knowledge used by the Proposer to generate questions. The knowledge is grouped into five content-based categories, and individual items may be assigned to multiple categories.\n\n* **Concept/Theorem**. The knowledge describes a mathematical concept (e.g., the definition of a topic or notion) or a theorem (e.g., a statement about the properties of a mathematical object). Because many knowledge pieces are extracted excerpts, their descriptions may be incomplete, for example, presented only in natural language without formal notation or missing key details. We therefore further divide this category into complete and incomplete subtypes. A knowledge piece that describes one concept/theorem completely but another incompletely is counted in both subcategories.\n* **Problem**. The knowledge explicitly describes a mathematical problem to be solved. In many cases, such knowledge pieces are excerpts from homework assignments or problem sets. Some also originate from textbooks or Wikipedia entries describing conjectures or open problems.\n* **Method**. The knowledge describes a procedure or technique for solving a particular type or instance of a mathematical problem. In many cases, it also explicitly states the corresponding problem itself.\n* **Other**. Knowledge pieces that do not fall under any of the above categories are classified as Other.\n\n| **Type** | **Percentage** |\n| :------------ | :------------: |\n| Concept/Theorem | 61.67 |\n| &nbsp;&nbsp;&nbsp;&nbsp; Complete             | 36.50 |\n| &nbsp;&nbsp;&nbsp;&nbsp; Incomplete           | 25.17 |\n| Problem                | 33.50 |\n| Method                 | 52.50 |\n| Other                    | 10.50 |\n\nWe aggregate the knowledge pieces from checkpoints 0 to 200 and report the results in the above table. Approximately 60% of the knowledge pieces describe mathematical concepts or theorems, of which roughly three-fifths are complete. In addition, 30% of the knowledge pieces pertain to problems and 50% pertain to solution methods. Only a small number fall into the “Other” category. As shown in our manual inspection in Appendix I, the QA pairs in this category primarily concern educational context (e.g., descriptions of math education programs, teaching experiences, or courses) rather than substantive mathematical content.\n\nWe further analyze which math areas the knowledge pieces fall into. In particular, we define the following math domains: algebra, number theory, geometry, trigonometry, calculus, combinatorics, probability/statistics, optimization, and other. We then categorize the knowledge pieces into one of the domains.\n\n| **Domain** | **Percentage** |\n| :------------ | :------------: |\n| Algebra                      | 17.83 |\n| Number Theory         | 7.83 |\n| Geometry                  | 8.33 |\n| Trigonometry             | 1.83 |\n| Calculus                    | 7.33 |\n| Combinatorics           | 4.00 |\n| Probability/Statistics  | 18.67 |\n| Optimization              | 0.33 |\n| Other                         | 33.83 |\n\n​​The results are shown in the table above. The most common domains are Algebra, Probability/Statistics, and Other. Upon manual inspection, many of the knowledge pieces categorized under \"Other\" pertain to subjects such as chemistry, physics, or programming."}}, "id": "4lAxBwFcWq", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147113127, "cdate": 1763147113127, "tmdate": 1763431893576, "mdate": 1763431893576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PasoDoble, a Dual-Play training framework where two LLMs compete and co-evolve to improve reasoning ability.\nThe Proposer generates challenging questions using a knowledge base, and the Solver learns by solving them.\nWithout any labeled data, the method achieves gains of over 20 points on math reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a Dual-Play learning framework that enhances reasoning ability by having two LLMs compete with each other.\n- It stabilizes the Proposer’s question generation using a knowledge base and ensures stable adversarial training through a reward design based on correctness and diversity."}, "weaknesses": {"value": "- The proposed method appears unfair because it uses a knowledge base, while the baselines do not. I am particularly concerned about how much knowledge or formatting from the evaluation data may have leaked into the knowledge base.\n- The paper does not quantitatively show how valid the generated problems were, nor how invalid the discarded problems actually were.\n- Training both the Solver and the Proposer roughly doubles the computational cost compared to standard training.\n- The idea of improving performance through competition is not particularly novel.\n    - https://arxiv.org/abs/2404.10642\n    - https://arxiv.org/abs/2311.08107\n    - https://www.arxiv.org/abs/2510.18407\n    - https://arxiv.org/abs/2504.19162"}, "questions": {"value": "- Why does performance degrade when the Proposer is frozen? This setting essentially corresponds to standard self-learning with an added knowledge base, so a performance drop seems counterintuitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6uOs2MwmZ", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Reviewer_Mcs8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Reviewer_Mcs8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857367775, "cdate": 1761857367775, "tmdate": 1762916582415, "mdate": 1762916582415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (2/6)"}, "comment": {"value": "### Knowledge-Question Association\n\nFor the rest of the analysis, we use 100 questions sampled from each Proposer checkpoint, conditioned on the knowledge pieces in the section above.\n\nTo analyze how external knowledge influences the Proposer to generate questions, we examine both the Proposer's question and the provided external knowledge piece. Our manual examination (Appendix I) shows that all the Proposer's questions are topically aligned with the provided external knowledge. We further categorize each question based on the source of its content:\n\n* **External.** The question and its conditions are directly extracted (i.e., copied) from the external knowledge, assuming the external knowledge already contains a question and provides all necessary conditions.\n​​* **Internal.** Neither the question nor its conditions are explicitly stated or implicitly derivable from the external knowledge. In this case, the external knowledge serves at most as a topical cue, and the question is generated entirely from the model’s internal parametric knowledge.\n* **Both.** The question draws on both external and internal knowledge. That is, part of the question (or its conditions) is explicitly stated or implicitly derivable from the external knowledge, while other parts are not. This category includes cases where the question is a modified rephrasing of a problem described in the external knowledge (e.g., with certain conditions added, removed, or altered), or an instantiation of a general problem form in the knowledge that requires the model to supply specific details. It also includes cases where solving the question depends on a concept, theorem, or method mentioned in the external knowledge, but additional internal knowledge is still needed. In such cases, the model must interpret the external knowledge and integrate it with its internal parametric knowledge to construct a coherent question and its conditions.\n\n| **Checkpoint** | **External** | **Internal** | **Both** |\n| :------------ | :------------: | :------------: | :------------: |\n| 0     | 46.00 | 5.00   | 49.00 |\n| 40   | 43.00 | 13.00 | 44.00 |\n| 80   | 43.00 | 5.00   | 52.00 |\n| 120 | 40.00 | 7.00   | 53.00 |\n| 160 | 48.00 | 7.00   | 45.00 |\n| 200 | 49.00 | 4.00   | 47.00 |\n\nAs shown in the table above, across step 40 to 200, approximately 40–50% of the questions are directly copied from the external knowledge, while only 4–10% are generated purely from the Proposer’s internal knowledge. The remaining 40–50% draw on both sources. This indicates that the Proposer is neither merely extracting questions from the knowledge base nor using the knowledge only as a loose topical cue. Rather, the two sources interact in a meaningful way during question generation.\n\n| **Method** | **Checkpoint** | **Rouge-L (80%)** | **EM (80%)** | **Rouge-L (60%)** | **EM (60%)** | **Rouge-L (40%)** | **EM (40%)** |\n| :------------ | :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| w/ K      | 0     | 51.17 | 15.00 | 41.17 | 1.00 | 32.89 | 0.00 |\n|              | 40   | 49.30 | 12.00 | 39.15 | 0.00 | 32.77 | 0.00 |\n|              | 80   | 44.40 | 7.00   | 40.19 | 0.00 | 34.75 | 0.00 |\n|              | 120 | 50.30 | 10.00 | 40.83 | 2.00 | 34.92 | 0.00 |\n|              | 160 | 43.63 | 11.00 | 36.39 | 0.00 | 31.30 | 0.00 |\n|              | 200 | 50.55 | 12.00 | 36.45 | 1.00 | 33.07 | 0.00 |\n|  |\n| w/ K      | 0     | 58.70 | 22.00 | 53.79 | 4.00 | 40.75 | 0.00 |\n|              | 40   | 62.88 | 22.00 | 53.20 | 2.00 | 45.72 | 0.00 |\n|              | 80   | 67.65 | 27.00 | 59.67 | 6.00 | 44.58 | 0.00 |\n|              | 120 | 66.19 | 27.00 | 57.10 | 7.00 | 45.92 | 1.00 |\n|              | 160 | 71.14 | 37.00 | 61.37 | 9.00 | 48.61 | 3.00 |\n|              | 200 | 67.17 | 31.00 | 55.95 | 6.00 | 45.48 | 2.00 |\n\n\nWe further examine whether the questions generated by the Proposer are recalled from pre-training or newly composed through reasoning during PasoDoble. Following the experiment setup of [2], we prompt Qwen3-1.7B with the first $x$\\% of each question generated by the Proposer ($x \\in {40, 60, 80}$) and test whether it can reconstruct the remainder using greedy decoding without applying the chat template. For each checkpoint, we evaluate the same 100 questions and report Exact Match (EM) and ROUGE-L [1] in the table above. Let $q_{\\text{old}}$ denote the original question and $q_{\\text{new}}$ the generated continuation. ROUGE-L measures the proportion of the longest common subsequence between $q_{\\text{old}}$ and $q_{\\text{new}}$ relative to the length of $q_{\\text{new}}$, capturing how much of $q_{\\text{new}}$ overlaps with $q_{\\text{old}}$. EM is 1 if $q_{\\text{new}}$ exactly matches $q_{\\text{old}}$, and 0 otherwise."}}, "id": "eyLy0NHmt5", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147250756, "cdate": 1763147250756, "tmdate": 1763147762826, "mdate": 1763147762826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (2/7)"}, "comment": {"value": "### Knowledge-Question Association\n\nFor the rest of the analysis, we use 100 questions sampled from each Proposer checkpoint, conditioned on the knowledge pieces in the section above.\n\nTo analyze how external knowledge influences the Proposer to generate questions, we examine both the Proposer's question and the provided external knowledge piece. Our manual examination (Appendix I) shows that all the Proposer's questions are topically aligned with the provided external knowledge. We further categorize each question based on the source of its content:\n\n* **External.** The question and its conditions are directly extracted (i.e., copied) from the external knowledge, assuming the external knowledge already contains a question and provides all necessary conditions.\n​​* **Internal.** Neither the question nor its conditions are explicitly stated or implicitly derivable from the external knowledge. In this case, the external knowledge serves at most as a topical cue, and the question is generated entirely from the model’s internal parametric knowledge.\n* **Both.** The question draws on both external and internal knowledge. That is, part of the question (or its conditions) is explicitly stated or implicitly derivable from the external knowledge, while other parts are not. This category includes cases where the question is a modified rephrasing of a problem described in the external knowledge (e.g., with certain conditions added, removed, or altered), or an instantiation of a general problem form in the knowledge that requires the model to supply specific details. It also includes cases where solving the question depends on a concept, theorem, or method mentioned in the external knowledge, but additional internal knowledge is still needed. In such cases, the model must interpret the external knowledge and integrate it with its internal parametric knowledge to construct a coherent question and its conditions.\n\n| **Checkpoint** | **External** | **Internal** | **Both** |\n| :------------ | :------------: | :------------: | :------------: |\n| 0     | 46.00 | 5.00   | 49.00 |\n| 40   | 43.00 | 13.00 | 44.00 |\n| 80   | 43.00 | 5.00   | 52.00 |\n| 120 | 40.00 | 7.00   | 53.00 |\n| 160 | 48.00 | 7.00   | 45.00 |\n| 200 | 49.00 | 4.00   | 47.00 |\n\nAs shown in the table above, across step 40 to 200, approximately 40–50% of the questions are directly copied from the external knowledge, while only 4–10% are generated purely from the Proposer’s internal knowledge. The remaining 40–50% draw on both sources. This indicates that the Proposer is neither merely extracting questions from the knowledge base nor using the knowledge only as a loose topical cue. Rather, the two sources interact in a meaningful way during question generation.\n\n| **Method** | **Checkpoint** | **Rouge-L (80%)** | **EM (80%)** | **Rouge-L (60%)** | **EM (60%)** | **Rouge-L (40%)** | **EM (40%)** |\n| :------------ | :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| w/ K      | 0     | 51.17 | 15.00 | 41.17 | 1.00 | 32.89 | 0.00 |\n|              | 40   | 49.30 | 12.00 | 39.15 | 0.00 | 32.77 | 0.00 |\n|              | 80   | 44.40 | 7.00   | 40.19 | 0.00 | 34.75 | 0.00 |\n|              | 120 | 50.30 | 10.00 | 40.83 | 2.00 | 34.92 | 0.00 |\n|              | 160 | 43.63 | 11.00 | 36.39 | 0.00 | 31.30 | 0.00 |\n|              | 200 | 50.55 | 12.00 | 36.45 | 1.00 | 33.07 | 0.00 |\n|  |\n| w/ K      | 0     | 58.70 | 22.00 | 53.79 | 4.00 | 40.75 | 0.00 |\n|              | 40   | 62.88 | 22.00 | 53.20 | 2.00 | 45.72 | 0.00 |\n|              | 80   | 67.65 | 27.00 | 59.67 | 6.00 | 44.58 | 0.00 |\n|              | 120 | 66.19 | 27.00 | 57.10 | 7.00 | 45.92 | 1.00 |\n|              | 160 | 71.14 | 37.00 | 61.37 | 9.00 | 48.61 | 3.00 |\n|              | 200 | 67.17 | 31.00 | 55.95 | 6.00 | 45.48 | 2.00 |\n\n\nWe further examine whether the questions generated by the Proposer are recalled from pre-training or newly composed through reasoning during PasoDoble. Following the experiment setup of [2], we prompt Qwen3-1.7B with the first $x$\\% of each question generated by the Proposer ($x \\in {40, 60, 80}$) and test whether it can reconstruct the remainder using greedy decoding without applying the chat template. For each checkpoint, we evaluate the same 100 questions and report Exact Match (EM) and ROUGE-L [1] in the table above. Let $q_{\\text{old}}$ denote the original question and $q_{\\text{new}}$ the generated continuation. ROUGE-L measures the proportion of the longest common subsequence between $q_{\\text{old}}$ and $q_{\\text{new}}$ relative to the length of $q_{\\text{new}}$, capturing how much of $q_{\\text{new}}$ overlaps with $q_{\\text{old}}$. EM is 1 if $q_{\\text{new}}$ exactly matches $q_{\\text{old}}$, and 0 otherwise."}}, "id": "eyLy0NHmt5", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147250756, "cdate": 1763147250756, "tmdate": 1763431886044, "mdate": 1763431886044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning technique, PasoDoble, to iteratively train two instances of the same base model in an Adversarial Learning manner, where reward values are inversely designed. The two instances are given respective roles of Proposer and Solver, trained jointly or alternatively. The Proposer is trained to generate diverse and challenging problems with ground-truth answers leveraging pre-training Knowledge Base, while the Solver is trained to solve the problems accurately. Through empirical experiments, the authors show that this technique improves mathematical reasoning capacity by approximately 20 points on average on larger models (1.5B-1.7B) of Qwen family. Yet, the effectiveness of this design is not found on smaller models. In addition, this technique evidently sustains mathematical reasoning capability improvement for hundreds of training steps, exceeding R-Zero’s 3-iteration plateau. However, this technique fails to transfer to out-of-domain tasks. As a main contribution, this paper highlights adversarial dual-play training that can reduce LLM’s dependence on high quality supervised data, where mathematical reasoning improvement can be achieved through pre-training knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The methodology is explained relatively clearly.\n- The in-domain results seem promising, despite lack of out-of-domain generalization."}, "weaknesses": {"value": "- This paper explains the main methodology, reward function design and findings clearly. However, the conclusion is on weaker grounds due to insufficient baselines, inadequate methodology validation and result interpretations.\n- Missing important baseline: SFT model using Knowledge Base should be a critical baseline to highlight the advantages of this technique. If SFT can achieve a similar level of mathematical reasoning capacity, the value of this technique remains unclear.\n- Insufficient validation of reward hacking prevention: The paper claims to guarantee question quality and prevent reward hacking of Proposer by removing questions with low Solver accuracy. However, this does not guarantee the question quality. Conversely, high Solver accuracy doesn’t necessarily imply high quality questions. It could easily be common hallucination by both Proposer and Solver, given they are initialized from the same base model. Although the authors sampled 100 questions to study the question quality, this is done by LLM, not human. There is insufficient validation to claim Proposer always generates high quality questions. \n- Title overstatement: The title “Better LLM Reasoning” seems like an overstatement. The experiment results only show improvements of larger models on mathematical reasoning domain with no transfer to out-of-domain tasks.\n- Ambiguous statistical demonstration: The graphs in the paper show no error bar or confidence intervals. It is unclear if the result is concluded with multiple runs of different seeds.\n- Lack of explanation of result: The paper doesn’t provide clear explanations for this technique’s failure on smaller models. Is this because Proposer could not interpret the Knowledge Base given its capacity? This was mentioned in the ablation study section, but there is no discussion directly addressing the experiment results."}, "questions": {"value": "- Diversity reward design: In the diversity reward, the similarity of questions is calculated with token occurrence. However, this does not guarantee high semantic distance. What other options are considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IiT3j638ed", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Reviewer_qHtA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Reviewer_qHtA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762312770113, "cdate": 1762312770113, "tmdate": 1762916582267, "mdate": 1762916582267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (3/6)"}, "comment": {"value": "Compared to the results reported in [2], the questions generated by our Proposer, both with external knowledge and without external knowledge, consistently exhibit lower EM and ROUGE-L scores. This indicates that the Proposer is not simply copying or slightly modifying existing questions but is composing new questions through genuine reasoning during PasoDoble training. \n\nWe further focus on the setting with external knowledge and use the setting without external knowledge as a baseline. Even when we provide the first 80% of the question tokens to the Proposer, it is able to regenerate the original question $q_{old}$ for only about 10% of the cases. This rate is significantly lower than the rate observed in the setting without external knowledge. The observation suggests that most of the questions are not fully recoverable from the Proposer’s internal parametric knowledge alone, and the Proposer relies on the external knowledge to generate them. This finding is consistent with the distribution presented in the previous table.\n\n[1] Lin and Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. ACL (2004).\n\n[2] Wu et al. Reasoning or memorization? Unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532 (2025).\n\n​​### Knowledge-Answer Association\n\nWe further examine how external knowledge influences the Proposer’s answer. \nSpecifically, we evaluate whether the answer to a question can be derived solely from the external knowledge provided, or whether it additionally relies on its internal knowledge. Naturally, if both the question and answer are copied directly from the knowledge, the answer is considered derivable. To make this criterion slightly more permissive, we also treat an answer as derivable if it requires only minimal common-sense reasoning (e.g., basic arithmetic) beyond the external knowledge.\n\n| **Checkpoint** | **Derivable** | **Not Derivable** |\n| :------------ | :------------: | :------------: |\n| 0     | 65.00 | 35.00 |\n| 40   | 44.00 | 56.00 |\n| 80   | 57.00 | 43.00 |\n| 120 | 51.00 | 49.00 |\n| 160 | 54.00 | 46.00 |\n| 200 | 60.00 | 40.00 |\n\nAs shown in the table above, except for checkpoint 40, more than half of all answers are derivable from external knowledge alone. This implies that, aside from the 20-30% of the questions that are copied from external knowledge (those under \"external\" in the previous table), there are an additional 20-30% of questions under the \"both\" category in the previous table that are mere extensions of existing questions/concepts mentioned in the knowledge.\n\n### Dynamics of Question Difficulty, Diversity, and Sampling Efficiency\n\nWe analyze the dynamics of the Proposer’s question difficulty, diversity, and sampling efficiency using checkpoints spanning steps 0 to 200, evaluated both with and without external knowledge. Again, we use the 100 QA pairs from each Proposer checkpoint to do the analysis.\n\n**Dynamics of the question difficulty.** We report the average question difficulty and sampling efficiency in Figure 6a (with external knowledge) and Figure 6b (without external knowledge). We additionally report the distribution of difficulty levels in Figure 7a (with external knowledge) and Figure 7b (without external knowledge). The difficulty labels are assigned by GPT-5-mini using the prompt provided in Figure 13. For the setting with the external knowledge, as shown in Figure 6a, the average question difficulty fluctuates within a relatively narrow range (approximately 1.8–2.2) throughout training, indicating that the Proposer maintains a stable difficulty level when different external knowledge is provided. For the setting without external knowledge, as shown in Figure 6b, the average question difficulty begins at a relatively high level (around 2.9) and gradually decreases to approximately 2.7 as training progresses. This indicates that the Proposer initially generates more difficult questions but gradually shifts toward more moderate ones. A likely explanation is that difficult questions receive a reward of zero when the Solver’s passing rate falls below the threshold $τ_{low}$, discouraging the Proposer from continuing to generate such questions. Developing mechanisms to preserve or reintroduce hard but valid QA pairs during training, while at the same time avoiding reward hacking, remains an interesting direction for future work."}}, "id": "pPOjoNRd8H", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147298396, "cdate": 1763147298396, "tmdate": 1763147937737, "mdate": 1763147937737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (3/7)"}, "comment": {"value": "Compared to the results reported in [2], the questions generated by our Proposer, both with external knowledge and without external knowledge, consistently exhibit lower EM and ROUGE-L scores. This indicates that the Proposer is not simply copying or slightly modifying existing questions but is composing new questions through genuine reasoning during PasoDoble training. \n\nWe further focus on the setting with external knowledge and use the setting without external knowledge as a baseline. Even when we provide the first 80% of the question tokens to the Proposer, it is able to regenerate the original question $q_{old}$ for only about 10% of the cases. This rate is significantly lower than the rate observed in the setting without external knowledge. The observation suggests that most of the questions are not fully recoverable from the Proposer’s internal parametric knowledge alone, and the Proposer relies on the external knowledge to generate them. This finding is consistent with the distribution presented in the previous table.\n\n[1] Lin and Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. ACL (2004).\n\n[2] Wu et al. Reasoning or memorization? Unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532 (2025).\n\n​​### Knowledge-Answer Association\n\nWe further examine how external knowledge influences the Proposer’s answer. \nSpecifically, we evaluate whether the answer to a question can be derived solely from the external knowledge provided, or whether it additionally relies on its internal knowledge. Naturally, if both the question and answer are copied directly from the knowledge, the answer is considered derivable. To make this criterion slightly more permissive, we also treat an answer as derivable if it requires only minimal common-sense reasoning (e.g., basic arithmetic) beyond the external knowledge.\n\n| **Checkpoint** | **Derivable** | **Not Derivable** |\n| :------------ | :------------: | :------------: |\n| 0     | 65.00 | 35.00 |\n| 40   | 44.00 | 56.00 |\n| 80   | 57.00 | 43.00 |\n| 120 | 51.00 | 49.00 |\n| 160 | 54.00 | 46.00 |\n| 200 | 60.00 | 40.00 |\n\nAs shown in the table above, except for checkpoint 40, more than half of all answers are derivable from external knowledge alone. This implies that, aside from the 20-30% of the questions that are copied from external knowledge (those under \"external\" in the previous table), there are an additional 20-30% of questions under the \"both\" category in the previous table that are mere extensions of existing questions/concepts mentioned in the knowledge.\n\n### Dynamics of Question Difficulty, Diversity, and Sampling Efficiency\n\nWe analyze the dynamics of the Proposer’s question difficulty, diversity, and sampling efficiency using checkpoints spanning steps 0 to 200, evaluated both with and without external knowledge. Again, we use the 100 QA pairs from each Proposer checkpoint to do the analysis.\n\n**Dynamics of the question difficulty.** We report the average question difficulty and sampling efficiency in Figure 6a (with external knowledge) and Figure 6b (without external knowledge). We additionally report the distribution of difficulty levels in Figure 7a (with external knowledge) and Figure 7b (without external knowledge). The difficulty labels are assigned by GPT-5-mini using the prompt provided in Figure 13. For the setting with the external knowledge, as shown in Figure 6a, the average question difficulty fluctuates within a relatively narrow range (approximately 1.8–2.2) throughout training, indicating that the Proposer maintains a stable difficulty level when different external knowledge is provided. For the setting without external knowledge, as shown in Figure 6b, the average question difficulty begins at a relatively high level (around 2.9) and gradually decreases to approximately 2.7 as training progresses. This indicates that the Proposer initially generates more difficult questions but gradually shifts toward more moderate ones. A likely explanation is that difficult questions receive a reward of zero when the Solver’s passing rate falls below the threshold $τ_{low}$, discouraging the Proposer from continuing to generate such questions. Developing mechanisms to preserve or reintroduce hard but valid QA pairs during training, while at the same time avoiding reward hacking, remains an interesting direction for future work."}}, "id": "pPOjoNRd8H", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147298396, "cdate": 1763147298396, "tmdate": 1763431879463, "mdate": 1763431879463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (4/6)"}, "comment": {"value": "Moreover, the per-level distributions in Figure 7a (with external knowledge) and Figure 7b (without external knowledge) exhibit notable differences. When external knowledge is provided (Figure 7a), the Proposer generates a higher proportion of Level 1 and Level 5 questions. A plausible explanation is that some Level 5 questions arise when the Proposer directly extracts or adapts complex problem statements contained in the external knowledge, or when the base model already possesses sufficient internal knowledge to leverage these concepts, allowing it to formulate or recognize inherently difficult problems. In contrast, the increased prevalence of Level 1 questions may stem from the fact that the base model of the Proposer and the Solver do not fully internalize such external knowledge during pre-training; as a result, even relatively simple questions derived from the external knowledge may pose challenges for the Solver, leading the Proposer to favor simpler formulations that still yield positive rewards. When external knowledge is not provided (Figure 7b), the Proposer generates a substantially larger number of Level 3 questions, especially in the earlier checkpoints. Over time, the proportion of Level 3 questions decreases slightly, while Level 2 questions increase, mirroring the downward trend in average difficulty discussed above. Level 1 and Level 5 questions remain consistently scarce throughout training. This pattern suggests that, in the absence of external guidance, the Proposer naturally gravitates toward generating intermediate-difficulty questions that require more internal knowledge for reason.\n\n**Dynamics of the sampling efficiency.** As shown in Figure 6a (with external knowledge) and Figure 6b (without external knowledge), the sampling efficiency of the Proposer improves over the course of training in both settings. With external knowledge, the sampling efficiency of the Proposer increases from roughly 15% to over 35%. Without external knowledge, it rises more modestly, from about 15% to approximately 21% as training progresses. This suggests that the Proposer gradually learns to better follow the format instruction and effectively leverage both the external knowledge and/or its internal knowledge to produce QA pairs. Moreover, the Proposer achieves a higher sampling efficiency when external knowledge is provided. This is expected, as the external knowledge offers additional guidance that helps the Proposer generate valid QA pairs more reliably.\n\n**Dynamics of question diversity distribution.** We use the same categories and similar prompt (See prompt in Figure 15) when analyzing the knowledge pieces in Table 10, and report the question diversity distribution in Figure 8a (with external knowledge) and Figure 8b (without external knowledge). As shown in Figure 8a, when external knowledge is provided, questions from the Proposer consistently covers a broad range of math domains throughout training. Overall, the number of questions in most domains does not fluctuate significantly across training steps. Algebra, Probability/Statistics, and Other are the most frequently occurring categories, which is consistent with our earlier observations on the knowledge distributions in Table 10. However, for the Other category, the questions occupy a noticeably smaller proportion compared with the distribution of the external knowledge, as shown in Table 10. One possible explanation is that the Proposer tends to generate math-related questions that fall into the first eight domains, even when the provided knowledge piece belongs to the Other category and does not directly involve mathematical content.\n\nWhen external knowledge is not provided, as shown in Figure 8b, the domain distribution becomes more concentrated. In particular, Algebra, Geometry, and Calculus dominate across all checkpoints. Number Theory and Optimization also appear frequently. This is reasonable because these domains are more strongly represented in the model’s internal knowledge and are structurally easier for the model to remix into new question formats. In contrast, domains such as Combinatorics, Trigonometry, and Probability/Statistics occur far less frequently. The proportion of Calculus questions decreases as training progresses, while the proportion of Algebra questions increases. This likely indicates that the Proposer begins to generate a somewhat more diverse range of topics, though the questions tend to be easier overall, according to our earlier analysis. Additionally, questions under the Other category are much rarer compared to the setting with external knowledge, further indicating that external knowledge is the primary driver of out-of-domain question generation. Overall, the absence of external knowledge leads to a notable collapse in question diversity, with the Proposer gravitating toward a narrower set of mathematically simpler topics."}}, "id": "9rv3cZozLN", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147340055, "cdate": 1763147340055, "tmdate": 1763147486305, "mdate": 1763147486305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (4/7)"}, "comment": {"value": "Moreover, the per-level distributions in Figure 7a (with external knowledge) and Figure 7b (without external knowledge) exhibit notable differences. When external knowledge is provided (Figure 7a), the Proposer generates a higher proportion of Level 1 and Level 5 questions. A plausible explanation is that some Level 5 questions arise when the Proposer directly extracts or adapts complex problem statements contained in the external knowledge, or when the base model already possesses sufficient internal knowledge to leverage these concepts, allowing it to formulate or recognize inherently difficult problems. In contrast, the increased prevalence of Level 1 questions may stem from the fact that the base model of the Proposer and the Solver do not fully internalize such external knowledge during pre-training; as a result, even relatively simple questions derived from the external knowledge may pose challenges for the Solver, leading the Proposer to favor simpler formulations that still yield positive rewards. When external knowledge is not provided (Figure 7b), the Proposer generates a substantially larger number of Level 3 questions, especially in the earlier checkpoints. Over time, the proportion of Level 3 questions decreases slightly, while Level 2 questions increase, mirroring the downward trend in average difficulty discussed above. Level 1 and Level 5 questions remain consistently scarce throughout training. This pattern suggests that, in the absence of external guidance, the Proposer naturally gravitates toward generating intermediate-difficulty questions that require more internal knowledge for reason.\n\n**Dynamics of the sampling efficiency.** As shown in Figure 6a (with external knowledge) and Figure 6b (without external knowledge), the sampling efficiency of the Proposer improves over the course of training in both settings. With external knowledge, the sampling efficiency of the Proposer increases from roughly 15% to over 35%. Without external knowledge, it rises more modestly, from about 15% to approximately 21% as training progresses. This suggests that the Proposer gradually learns to better follow the format instruction and effectively leverage both the external knowledge and/or its internal knowledge to produce QA pairs. Moreover, the Proposer achieves a higher sampling efficiency when external knowledge is provided. This is expected, as the external knowledge offers additional guidance that helps the Proposer generate valid QA pairs more reliably.\n\n**Dynamics of question diversity distribution.** We use the same categories and similar prompt (See prompt in Figure 15) when analyzing the knowledge pieces in Table 10, and report the question diversity distribution in Figure 8a (with external knowledge) and Figure 8b (without external knowledge). As shown in Figure 8a, when external knowledge is provided, questions from the Proposer consistently covers a broad range of math domains throughout training. Overall, the number of questions in most domains does not fluctuate significantly across training steps. Algebra, Probability/Statistics, and Other are the most frequently occurring categories, which is consistent with our earlier observations on the knowledge distributions in Table 10. However, for the Other category, the questions occupy a noticeably smaller proportion compared with the distribution of the external knowledge, as shown in Table 10. One possible explanation is that the Proposer tends to generate math-related questions that fall into the first eight domains, even when the provided knowledge piece belongs to the Other category and does not directly involve mathematical content.\n\nWhen external knowledge is not provided, as shown in Figure 8b, the domain distribution becomes more concentrated. In particular, Algebra, Geometry, and Calculus dominate across all checkpoints. Number Theory and Optimization also appear frequently. This is reasonable because these domains are more strongly represented in the model’s internal knowledge and are structurally easier for the model to remix into new question formats. In contrast, domains such as Combinatorics, Trigonometry, and Probability/Statistics occur far less frequently. The proportion of Calculus questions decreases as training progresses, while the proportion of Algebra questions increases. This likely indicates that the Proposer begins to generate a somewhat more diverse range of topics, though the questions tend to be easier overall, according to our earlier analysis. Additionally, questions under the Other category are much rarer compared to the setting with external knowledge, further indicating that external knowledge is the primary driver of out-of-domain question generation. Overall, the absence of external knowledge leads to a notable collapse in question diversity, with the Proposer gravitating toward a narrower set of mathematically simpler topics."}}, "id": "9rv3cZozLN", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147340055, "cdate": 1763147340055, "tmdate": 1763431873448, "mdate": 1763431873448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (5/6)"}, "comment": {"value": "### Using the Proposer as the Solver\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Solver                | **7.22** | **7.22** | **40.83** | **84.98** | **68.50** | **28.79** | **39.59** |\n| Proposer            | 3.33 | 4.44 | 22.92 | 62.79 | 51.93 | 21.56 | 27.82 |\n| Proposer w/o K  | 3.89 | 5.56 | 30.42 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe further evaluate whether the Proposer alone is capable of solving math problems effectively,\ngiven that PasoDoble relies on the Proposer to produce correct answers with the help of exter-\nnal knowledge. The results are reported in the table above. Compared to the Solver, a Proposer trained with external knowledge performs significantly worse, suggesting that it depends on the provided knowledge rather than its own internal reasoning to obtain correct answers. Interestingly, the Proposer trained without external knowledge also underperforms the Solver. We attribute this gap to a prompting-format mismatch: the Proposer is not trained under the same solution-formatting conventions as the Solver, leading to template inconsistency and degraded performance.\n\n## More ablation study\n\n\nWe investigate the impact of removing the Proposer’s answers, such that only its questions are used. Since the answers are no longer visible to the Solver, we replace the Solver’s correctness verifier with a random reward $r^{s}_{ij}$$ from $Bernoulli(0.5)$, which returns 1 or 0 uniformly at random (Fully Rand Rwd). This also induces a corresponding randomness in the Proposer’s difficulty reward $r^{diff}_{i}$. Furthermore, we add an enhanced version where the Solver’s reward is always 0 if its format is incorrect (i.e., no answer box), and random otherwise (Partial Rand Rwd).\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | **6.11** | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.26** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Full. Rand Rwd.  | 0.00 | 0.00 | 0.42 | 2.88 | 1.07 | 0.10 | 0.75 | \n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Part. Rand Rwd. | 4.44 | 5.56 | 28.33 | 78.99 | 55.53 | 20.20 | 32.18 | \n\nPrior work [1] suggests that even random rewards may yield non-trivial improve-\nments, which shows potential for these settings to work. We conduct the experiments on Qwen3-1.7B-Base model using the online training paradigm. As shown in the table above, training with fully random rewards drives the Solver’s average accuracy on all math benchmarks to nearly zero. Even if we force the Solver to respond in the correct format (Part. Rand Rwd), its accuracy still decreases significantly. The sharp contrast with our original setting demonstrates that the Solver benefits substantially from learning from Proposer’s answers. Moreover, as analyzed in § 5.4, these answers are predominantly correct after filtering.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | 6.11 | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.26** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/o $r^{div}$           | **8.33** | 2.22 | 30.83 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe also study the effect of removing the diversity reward $r^{div}$ from the Proposer’s training, in the Qwen3-1.7B online setting. As shown in the table above, eliminating diversity rewards leads to consistent performance degradation across math benchmarks, with an average drop of approximately 4 points. We leave a more comprehensive exploration of diversity-oriented reward design, as well as a qualitative analysis of the questions generated with and without this term, to future work.\n\n[1] Shao et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint\narXiv:2506.10947 (2025)."}}, "id": "J79VR0g0Vi", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147393889, "cdate": 1763147393889, "tmdate": 1763151794805, "mdate": 1763151794805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (5/7)"}, "comment": {"value": "### Using the Proposer as the Solver\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Solver                | **7.22** | **7.22** | **40.83** | **84.98** | **68.50** | **28.79** | **39.59** |\n| Proposer            | 3.33 | 4.44 | 22.92 | 62.79 | 51.93 | 21.56 | 27.82 |\n| Proposer w/o K  | 3.89 | 5.56 | 30.42 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe further evaluate whether the Proposer alone is capable of solving math problems effectively,\ngiven that PasoDoble relies on the Proposer to produce correct answers with the help of exter-\nnal knowledge. The results are reported in the table above. Compared to the Solver, a Proposer trained with external knowledge performs significantly worse, suggesting that it depends on the provided knowledge rather than its own internal reasoning to obtain correct answers. Interestingly, the Proposer trained without external knowledge also underperforms the Solver. We attribute this gap to a prompting-format mismatch: the Proposer is not trained under the same solution-formatting conventions as the Solver, leading to template inconsistency and degraded performance.\n\n## More ablation study\n\n\nWe investigate the impact of removing the Proposer’s answers, such that only its questions are used. Since the answers are no longer visible to the Solver, we replace the Solver’s correctness verifier with a random reward $r^{s}_{ij}$$ from $Bernoulli(0.5)$, which returns 1 or 0 uniformly at random (Fully Rand Rwd). This also induces a corresponding randomness in the Proposer’s difficulty reward $r^{diff}_{i}$. Furthermore, we add an enhanced version where the Solver’s reward is always 0 if its format is incorrect (i.e., no answer box), and random otherwise (Partial Rand Rwd).\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | **6.11** | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.26** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Full. Rand Rwd.  | 0.00 | 0.00 | 0.42 | 2.88 | 1.07 | 0.10 | 0.75 | \n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Part. Rand Rwd. | 4.44 | 5.56 | 28.33 | 78.99 | 55.53 | 20.20 | 32.18 | \n\nPrior work [1] suggests that even random rewards may yield non-trivial improve-\nments, which shows potential for these settings to work. We conduct the experiments on Qwen3-1.7B-Base model using the online training paradigm. As shown in the table above, training with fully random rewards drives the Solver’s average accuracy on all math benchmarks to nearly zero. Even if we force the Solver to respond in the correct format (Part. Rand Rwd), its accuracy still decreases significantly. The sharp contrast with our original setting demonstrates that the Solver benefits substantially from learning from Proposer’s answers. Moreover, as analyzed in § 5.4, these answers are predominantly correct after filtering.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | 6.11 | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.26** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/o $r^{div}$           | **8.33** | 2.22 | 30.83 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe also study the effect of removing the diversity reward $r^{div}$ from the Proposer’s training, in the Qwen3-1.7B online setting. As shown in the table above, eliminating diversity rewards leads to consistent performance degradation across math benchmarks, with an average drop of approximately 4 points. We leave a more comprehensive exploration of diversity-oriented reward design, as well as a qualitative analysis of the questions generated with and without this term, to future work.\n\n[1] Shao et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint\narXiv:2506.10947 (2025)."}}, "id": "J79VR0g0Vi", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147393889, "cdate": 1763147393889, "tmdate": 1763431865534, "mdate": 1763431865534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (5/7)"}, "comment": {"value": "### Using the Proposer as the Solver\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Solver                | **7.22** | **7.22** | **40.83** | **84.98** | **68.50** | **28.79** | **39.59** |\n| Proposer            | 3.33 | 4.44 | 22.92 | 62.79 | 51.93 | 21.56 | 27.82 |\n| Proposer w/o K  | 3.89 | 5.56 | 30.42 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe further evaluate whether the Proposer alone is capable of solving math problems effectively,\ngiven that PasoDoble relies on the Proposer to produce correct answers with the help of exter-\nnal knowledge. The results are reported in the table above. Compared to the Solver, a Proposer trained with external knowledge performs significantly worse, suggesting that it depends on the provided knowledge rather than its own internal reasoning to obtain correct answers. Interestingly, the Proposer trained without external knowledge also underperforms the Solver. We attribute this gap to a prompting-format mismatch: the Proposer is not trained under the same solution-formatting conventions as the Solver, leading to template inconsistency and degraded performance.\n\n## More ablation study\n\n\nWe investigate the impact of removing the Proposer’s answers, such that only its questions are used. Since the answers are no longer visible to the Solver, we replace the Solver’s correctness verifier with a random reward $r^{s}_{ij}$$ from $Bernoulli(0.5)$, which returns 1 or 0 uniformly at random (Fully Rand Rwd). This also induces a corresponding randomness in the Proposer’s difficulty reward $r^{diff}_{i}$. Furthermore, we add an enhanced version where the Solver’s reward is always 0 if its format is incorrect (i.e., no answer box), and random otherwise (Partial Rand Rwd).\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | **6.11** | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.27** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Full. Rand Rwd.  | 0.00 | 0.00 | 0.42 | 2.88 | 1.07 | 0.10 | 0.75 | \n|  &nbsp;&nbsp;&nbsp;&nbsp; w/ Part. Rand Rwd. | 4.44 | 5.56 | 28.33 | 78.99 | 55.53 | 20.20 | 32.18 | \n\nPrior work [1] suggests that even random rewards may yield non-trivial improve-\nments, which shows potential for these settings to work. We conduct the experiments on Qwen3-1.7B-Base model using the online training paradigm. As shown in the table above, training with fully random rewards drives the Solver’s average accuracy on all math benchmarks to nearly zero. Even if we force the Solver to respond in the correct format (Part. Rand Rwd), its accuracy still decreases significantly. The sharp contrast with our original setting demonstrates that the Solver benefits substantially from learning from Proposer’s answers. Moreover, as analyzed in § 5.4, these answers are predominantly correct after filtering.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| PasoDoble                | 6.11 | **7.78** | **37.92** | **83.13** | **66.67** | **28.00** | **38.27** |\n|  &nbsp;&nbsp;&nbsp;&nbsp; w/o $r^{div}$           | **8.33** | 2.22 | 30.83 | 65.07 | 55.67 | 24.00 | 30.77 |\n\nWe also study the effect of removing the diversity reward $r^{div}$ from the Proposer’s training, in the Qwen3-1.7B online setting. As shown in the table above, eliminating diversity rewards leads to consistent performance degradation across math benchmarks, with an average drop of approximately 4 points. We leave a more comprehensive exploration of diversity-oriented reward design, as well as a qualitative analysis of the questions generated with and without this term, to future work.\n\n[1] Shao et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint\narXiv:2506.10947 (2025)."}}, "id": "J79VR0g0Vi", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147393889, "cdate": 1763147393889, "tmdate": 1763503917851, "mdate": 1763503917851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (6/6)"}, "comment": {"value": "## Experiments on 3B and 4B models\n\nWe also evaluate the effectiveness of PasoDoble on 3B and 4B models.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base            | 1.67 | 0.00 | 16.25 | 69.65 | 45.80 | 14.35 | 24.62 |\n| &nbsp;&nbsp; Coldstart  | 1.11  | 1.11  | 13.75 | 64.66 | 40.67  | 12.42  | 22.29 |\n| &nbsp;&nbsp; Online PasoDoble  | 5.56  | 3.33  | 30.42  | 82.26  | 58.70  | 22.59  | 33.81 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K  | 5.00  | 2.78  | 25.42  | 76.46  | 54.43  | 21.21  | 30.88 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen  | 5.56  | 2.22  | 29.58  | 80.06  | 56.50  | 22.47  | 32.73 |\n| &nbsp;&nbsp; Offline PasoDoble  | 5.56  | 2.78  | 26.67  | 81.48  | 55.67  | 22.59  | 32.45 |\n| |\n| Qwen3-4B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base | 6.11 | 2.78 | 33.33 | 84.07 | 61.37 | 23.98 | 35.27 |\n| &nbsp;&nbsp; Coldstart | 13.33 | 13.89 | 41.25 | 83.04 | 67.07 | 30.64 | 41.54 |\n| &nbsp;&nbsp; Online PasoDoble | 18.89 | 18.89 | 53.33 | 91.82 | 82.17 | 42.27 | 51.23 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K | 17.22 | 13.33 | 48.33 | 86.92 | 75.40 | 35.85 | 46.18 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen | 17.78 | 13.89 | 49.17 | 89.15 | 75.17 | 35.85 | 46.84 |\n| &nbsp;&nbsp; Offline PasoDoble | 17.78 | 16.67 | 55.42 | 91.91 | 79.63 | 40.81 | 50.37 |\n\nAs shown in the table above, for Qwen2.5-3B, online PasoDoble achieves a 9 points increase on average, while offline PasoDoble achieves a 8 points increase on average. For Qwen3-4B, the average performance increases by 16 points and 15 points, respectively. Both removing the knowledge base and freezing the proposer hurt model’s performance, which is consistent with what we observe in section 5.3.1 and 5.3.2.\n\n| **Method** | **GPQA** | **SuperGPQA** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |\n| &nbsp;&nbsp; Base             | 26.26 | 20.31 | 23.29 |\n| &nbsp;&nbsp; Coldstart      | 28.04 | 16.56 | 22.30 |\n| &nbsp;&nbsp; Online PasoDoble | 29.06 | 16.65 | 22.86 |\n| &nbsp;&nbsp; Offline PasoDoble | 27.50 | 17.51 | 22.51 |\n|  |\n| Qwen3-4B |  |  |  |\n| &nbsp;&nbsp; Base | 36.36 | 28.43 | 32.40 |\n| &nbsp;&nbsp; Coldstart | 34.82 | 24.33 | 29.58 |\n| &nbsp;&nbsp; Online PasoDoble | 39.20 | 30.67 | 34.94 |\n| &nbsp;&nbsp; Offline PasoDoble | 38.34 | 30.82 | 34.58 |\n\nOn out-of-domain benchmarks, our method slightly increases the performance on Qwen3-4B but slightly hurts the performance on Qwen2.5-3B. This is consistent with our conclusion in Table 14 that PasoDoble does not improve performance on out-of-domain tasks.\n\n# No validation on the generated questions\n\nWe present a quantitative analysis of the proportion of truly valid questions, those with correct ground-truth answers, in Figure 3(a). The green curve indicates the questions retained for training, while the red curve represents those that are discarded. We observe that a substantial majority of the retained questions (around 70%) are indeed valid. Similarly, a comparable proportion of the discarded questions (also roughly 70%) is genuinely invalid.\n\nWe also would like to emphasize that\n* Using GPT-5-mini as the source of ground-truth labels is actually preferable to relying on human annotators, as it outperforms average humans on math tasks and therefore provides higher-quality, lower-noise labels. Additionally, for our choice of base models (e.g., Qwen models under 4B), the Proposer’s generated questions fall well within the capability range of GPT-5-mini. In our manual inspection on a set of questions generated by the Proposer, GPT-5-mini answered all questions correctly, supporting its reliability as an automatic evaluator.\n* Although our filtering mechanism is not perfect (i.e., approximately 30% of retained questions are still invalid), it nonetheless yields meaningful performance gains without requiring expensive human supervision for question verification. This stands in contrast to prior work such as AbsoluteZero [1], which relies on costly Python interpreter–based verification.\n\n[1] Zhao et al. “Absolute Zero: Reinforced Self-play Reasoning with Zero Data.” arXiv preprint arXiv:2505.03335 (2025)."}}, "id": "sRnFud8rOK", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147443929, "cdate": 1763147443929, "tmdate": 1763149303672, "mdate": 1763149303672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (6/7)"}, "comment": {"value": "## Experiments on 3B and 4B models\n\nWe also evaluate the effectiveness of PasoDoble on 3B and 4B models.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base            | 1.67 | 0.00 | 16.25 | 69.65 | 45.80 | 14.35 | 24.62 |\n| &nbsp;&nbsp; Coldstart  | 1.11  | 1.11  | 13.75 | 64.66 | 40.67  | 12.42  | 22.29 |\n| &nbsp;&nbsp; Online PasoDoble  | 5.56  | 3.33  | 30.42  | 82.26  | 58.70  | 22.59  | 33.81 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K  | 5.00  | 2.78  | 25.42  | 76.46  | 54.43  | 21.21  | 30.88 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen  | 5.56  | 2.22  | 29.58  | 80.06  | 56.50  | 22.47  | 32.73 |\n| &nbsp;&nbsp; Offline PasoDoble  | 5.56  | 2.78  | 26.67  | 81.48  | 55.67  | 22.59  | 32.45 |\n| |\n| Qwen3-4B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base | 6.11 | 2.78 | 33.33 | 84.07 | 61.37 | 23.98 | 35.27 |\n| &nbsp;&nbsp; Coldstart | 13.33 | 13.89 | 41.25 | 83.04 | 67.07 | 30.64 | 41.54 |\n| &nbsp;&nbsp; Online PasoDoble | 18.89 | 18.89 | 53.33 | 91.82 | 82.17 | 42.27 | 51.23 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K | 17.22 | 13.33 | 48.33 | 86.92 | 75.40 | 35.85 | 46.18 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen | 17.78 | 13.89 | 49.17 | 89.15 | 75.17 | 35.85 | 46.84 |\n| &nbsp;&nbsp; Offline PasoDoble | 17.78 | 16.67 | 55.42 | 91.91 | 79.63 | 40.81 | 50.37 |\n\nAs shown in the table above, for Qwen2.5-3B, online PasoDoble achieves a 9 points increase on average, while offline PasoDoble achieves a 8 points increase on average. For Qwen3-4B, the average performance increases by 16 points and 15 points, respectively. Both removing the knowledge base and freezing the proposer hurt model’s performance, which is consistent with what we observe in section 5.3.1 and 5.3.2.\n\n| **Method** | **GPQA** | **SuperGPQA** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |\n| &nbsp;&nbsp; Base             | 26.26 | 20.31 | 23.29 |\n| &nbsp;&nbsp; Coldstart      | 28.04 | 16.56 | 22.30 |\n| &nbsp;&nbsp; Online PasoDoble | 29.06 | 16.65 | 22.86 |\n| &nbsp;&nbsp; Offline PasoDoble | 27.50 | 17.51 | 22.51 |\n|  |\n| Qwen3-4B |  |  |  |\n| &nbsp;&nbsp; Base | 36.36 | 28.43 | 32.40 |\n| &nbsp;&nbsp; Coldstart | 34.82 | 24.33 | 29.58 |\n| &nbsp;&nbsp; Online PasoDoble | 39.20 | 30.67 | 34.94 |\n| &nbsp;&nbsp; Offline PasoDoble | 38.34 | 30.82 | 34.58 |\n\nOn out-of-domain benchmarks, our method slightly increases the performance on Qwen3-4B but slightly hurts the performance on Qwen2.5-3B. This is consistent with our conclusion in Table 14 that PasoDoble does not improve performance on out-of-domain tasks.\n\n## Comparison with Prior Work\n\n| **Method**  | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Qwen3-4B |  |  |  |  |  |  |  |\n| R-Zero | 4.27 | 12.71 | **57.27** | **92.12** | 79.60 | **44.59** | 48.43 |\n| Online PasoDoble | **18.89** | **18.89** | 53.33 | 91.82 | **82.17** | 42.27 | **51.23** |\n| Offline PasoDoble | 17.78 | 16.67 | 55.42 | 91.91 | 79.63 | 40.81 | 50.37 |\n\nWe report the results of Qwen3-4B following the numbers provided in the R-Zero paper. Across the full math benchmark suite, our PasoDoble framework outperforms R-Zero by roughly 3 points on average. Notably, PasoDoble achieves substantially stronger performance on the most challenging datasets—AIME 2024 and AIME 2025—where R-Zero struggles. These gains highlight that adversarial co-training is more effective with more training iterations and more immediate feedback between the Proposer and the Solver."}}, "id": "sRnFud8rOK", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147443929, "cdate": 1763147443929, "tmdate": 1763434110484, "mdate": 1763434110484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (6/7)"}, "comment": {"value": "## Experiments on 3B and 4B models\n\nWe also evaluate the effectiveness of PasoDoble on 3B and 4B models.\n\n| **Method** | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base            | 1.67 | 0.00 | 16.25 | 69.65 | 45.80 | 14.35 | 24.62 |\n| &nbsp;&nbsp; Coldstart  | 1.11  | 1.11  | 13.75 | 64.66 | 40.67  | 12.42  | 22.19 |\n| &nbsp;&nbsp; Online PasoDoble  | 5.56  | 3.33  | 30.42  | 82.26  | 58.70  | 22.59  | 33.81 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K  | 5.00  | 2.78  | 25.42  | 76.46  | 54.43  | 21.21  | 30.88 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen  | 5.56  | 2.22  | 29.58  | 80.06  | 56.50  | 22.47  | 32.73 |\n| &nbsp;&nbsp; Offline PasoDoble  | 5.56  | 2.78  | 26.67  | 81.48  | 55.67  | 22.59  | 32.45 |\n| |\n| Qwen3-4B |  |  |  |  |  |  |  |\n| &nbsp;&nbsp; Base | 6.11 | 2.78 | 33.33 | 84.07 | 61.37 | 23.98 | 35.27 |\n| &nbsp;&nbsp; Coldstart | 13.33 | 13.89 | 41.25 | 83.04 | 67.07 | 30.64 | 41.54 |\n| &nbsp;&nbsp; Online PasoDoble | 18.89 | 18.89 | 53.33 | 91.82 | 82.17 | 42.27 | 51.23 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/o K | 17.22 | 13.33 | 48.33 | 86.92 | 75.40 | 35.85 | 46.18 |\n| &nbsp;&nbsp;&nbsp;&nbsp; w/ πp frozen | 17.78 | 13.89 | 49.17 | 89.15 | 75.17 | 35.85 | 46.84 |\n| &nbsp;&nbsp; Offline PasoDoble | 17.78 | 16.67 | 55.42 | 91.91 | 79.63 | 40.81 | 50.37 |\n\nAs shown in the table above, for Qwen2.5-3B, online PasoDoble achieves a 9 points increase on average, while offline PasoDoble achieves a 8 points increase on average. For Qwen3-4B, the average performance increases by 16 points and 15 points, respectively. Both removing the knowledge base and freezing the proposer hurt model’s performance, which is consistent with what we observe in section 5.3.1 and 5.3.2.\n\n| **Method** | **GPQA** | **SuperGPQA** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: |\n| Qwen2.5-3B |  |  |  |\n| &nbsp;&nbsp; Base             | 26.26 | 20.31 | 23.29 |\n| &nbsp;&nbsp; Coldstart      | 28.04 | 16.56 | 22.30 |\n| &nbsp;&nbsp; Online PasoDoble | 29.06 | 16.65 | 22.86 |\n| &nbsp;&nbsp; Offline PasoDoble | 27.50 | 17.51 | 22.51 |\n|  |\n| Qwen3-4B |  |  |  |\n| &nbsp;&nbsp; Base | 36.36 | 28.43 | 32.40 |\n| &nbsp;&nbsp; Coldstart | 34.82 | 24.33 | 29.58 |\n| &nbsp;&nbsp; Online PasoDoble | 39.20 | 30.67 | 34.94 |\n| &nbsp;&nbsp; Offline PasoDoble | 38.34 | 30.82 | 34.58 |\n\nOn out-of-domain benchmarks, our method slightly increases the performance on Qwen3-4B but slightly hurts the performance on Qwen2.5-3B. This is consistent with our conclusion in Table 14 that PasoDoble does not improve performance on out-of-domain tasks.\n\n## Comparison with Prior Work\n\n| **Method**  | **AIME24** | **AIME25** | **AMC** | **GSM8K** | **MATH** | **OlympiadBench** | **AVG** |\n| :------------ | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Qwen3-4B |  |  |  |  |  |  |  |\n| R-Zero | 4.27 | 12.71 | **57.27** | **92.12** | 79.60 | **44.59** | 48.43 |\n| Online PasoDoble | **18.89** | **18.89** | 53.33 | 91.82 | **82.17** | 42.27 | **51.23** |\n| Offline PasoDoble | 17.78 | 16.67 | 55.42 | 91.91 | 79.63 | 40.81 | 50.37 |\n\nWe report the results of Qwen3-4B following the numbers provided in the R-Zero paper. Across the full math benchmark suite, our PasoDoble framework outperforms R-Zero by roughly 3 points on average. Notably, PasoDoble achieves substantially stronger performance on the most challenging datasets—AIME 2024 and AIME 2025—where R-Zero struggles. These gains highlight that adversarial co-training is more effective with more training iterations and more immediate feedback between the Proposer and the Solver."}}, "id": "sRnFud8rOK", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147443929, "cdate": 1763147443929, "tmdate": 1763503879740, "mdate": 1763503879740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (7/7)"}, "comment": {"value": "# No validation on the generated questions\n\nWe present a quantitative analysis of the proportion of truly valid questions, those with correct ground-truth answers, in Figure 3(a). The green curve indicates the questions retained for training, while the red curve represents those that are discarded. We observe that a substantial majority of the retained questions (around 70%) are indeed valid. Similarly, a comparable proportion of the discarded questions (also roughly 70%) is genuinely invalid.\n\nWe also would like to emphasize that\n* Using GPT-5-mini as the source of ground-truth labels is actually preferable to relying on human annotators, as it outperforms average humans on math tasks and therefore provides higher-quality, lower-noise labels. Additionally, for our choice of base models (e.g., Qwen models under 4B), the Proposer’s generated questions fall well within the capability range of GPT-5-mini. In our manual inspection on a set of questions generated by the Proposer, GPT-5-mini answered all questions correctly, supporting its reliability as an automatic evaluator.\n* Although our filtering mechanism is not perfect (i.e., approximately 30% of retained questions are still invalid), it nonetheless yields meaningful performance gains without requiring expensive human supervision for question verification. This stands in contrast to prior work such as AbsoluteZero [1], which relies on costly Python interpreter–based verification.\n\n[1] Zhao et al. “Absolute Zero: Reinforced Self-play Reasoning with Zero Data.” arXiv preprint arXiv:2505.03335 (2025)."}}, "id": "UCD67DUkTK", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763431845506, "cdate": 1763431845506, "tmdate": 1763431845506, "mdate": 1763431845506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (7/7)"}, "comment": {"value": "# No validation on the generated questions\n\nWe present a quantitative analysis of the proportion of truly valid questions, those with correct ground-truth answers, in Figure 3(a). The green curve indicates the questions retained for training, while the red curve represents those that are discarded. We observe that a substantial majority of the retained questions (around 70%) are indeed valid. Similarly, a comparable proportion of the discarded questions (also roughly 70%) is genuinely invalid.\n\nWe also would like to emphasize that\n* Using GPT-5-mini as the source of ground-truth labels is actually preferable to relying on human annotators, as it outperforms average humans on math tasks and therefore provides higher-quality, lower-noise labels [2]. Additionally, for our choice of base models (e.g., Qwen models under 4B), the Proposer’s generated questions fall well within the capability range of GPT-5-mini. In our manual inspection on a set of questions generated by the Proposer, GPT-5-mini answered all questions correctly, supporting its reliability as an automatic evaluator.\n* Although our filtering mechanism is not perfect (i.e., approximately 30% of retained questions are still invalid), it nonetheless yields meaningful performance gains without requiring expensive human supervision for question verification. This stands in contrast to prior work such as AbsoluteZero [1], which relies on costly Python interpreter–based verification.\n\n[1] Zhao et al. “Absolute Zero: Reinforced Self-play Reasoning with Zero Data.” arXiv preprint arXiv:2505.03335 (2025).\n\n[2] OpenAI. GPT-5 System Card. 2025."}}, "id": "UCD67DUkTK", "forum": "hVr4mQPdBJ", "replyto": "hVr4mQPdBJ", "signatures": ["ICLR.cc/2026/Conference/Submission3170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3170/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission3170/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763431845506, "cdate": 1763431845506, "tmdate": 1763501785010, "mdate": 1763501785010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}