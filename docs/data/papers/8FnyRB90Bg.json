{"id": "8FnyRB90Bg", "number": 14349, "cdate": 1758233349766, "mdate": 1759897375800, "content": {"title": "Inference Time Causal Probing in LLMs", "abstract": "Causal probing methods aim to test and control how internal representations influence the behavior of generative models. In causal probing, an intervention modifies hidden states so that a property takes on a different value. Most existing approaches define such interventions by training an auxiliary probe classifier, which ties the method to a specific task or model and risks misalignment with the model’s predictive geometry. We propose Hidden-state Driven Margin Intervention (HDMI), a probe-free, gradient-based technique that directly steers hidden states using the model's native output. HDMI applies a margin objective that increases the probability of a target continuation while decreasing that of the source, without relying on probe classifiers. We further introduce a lookahead variant (LA-HDMI) for text editing that backpropagates through the softmax embeddings, modifying the current hidden state so that the likelihood of user-specified tokens increases in next token generations while preserving fluency. To evaluate interventions, we measure completeness (whether the targeted property changes as intended) and selectivity (whether unrelated properties are preserved), and report their harmonic mean as an overall measure of reliability. HDMI consistently achieves higher reliability than prior methods on the LGD agreement corpus and the CausalGym benchmark, across Meta-Llama-3-8B-Instruct, and Pythia-70M.", "tldr": "", "keywords": ["Intervention", "Causal Probing", "Counterfactual", "Causal Models", "Representation Interventions"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a0db69f39297b4295981641b9c7861adc09966d.pdf", "supplementary_material": "/attachment/dd969c4a8807e376689aaf0d826b242771d9a964.zip"}, "replies": [{"content": {"summary": {"value": "The author proposed LookAhead Hidden-state Driven Margin Intervention (LA-HDMI), a text-editing approach that steers the model’s hidden states toward the desired modification while preserving overall fluency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed implementations, particularly those designed to maintain model prediction fluency and prevent output deviation, are well-motivated and demonstrate certain potential for application in future research if they are originally proposed here.\n2. Experimental results show that the proposed method consistently outperforms the baselines by a significant margin. The overall design and the comprehensive ablation study presented in the appendix effectively support this claim."}, "weaknesses": {"value": "1. It is unclear to me whether this work still falls within the scope of probing studies, which typically investigate whether learned representations can be linearly explained. As noted in Footnote 1 (line 50), the authors focus more on text editing rather than probing, and this distinction could be clarified further.\n2. In the LookAhead HDMI formulation, the method requires the input and output sequences to have the same number of tokens. This constraint prevents edits that change token length. E.g., transforming apple into shoes here: “This is **an** *apple*” into “This is **a pair of** *shoes*” limits the method’s generalization and fluency flexibility."}, "questions": {"value": "The paper’s writing could be improved for clarity and readability. For instance, it would be better to introduce the symbols W and b earlier (line 225), break down the lengthy paragraph spanning lines 232–247, and highlight the best performance results in the benchmark tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lcqUDbFJcc", "forum": "8FnyRB90Bg", "replyto": "8FnyRB90Bg", "signatures": ["ICLR.cc/2026/Conference/Submission14349/Reviewer_uVoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14349/Reviewer_uVoy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526848037, "cdate": 1761526848037, "tmdate": 1762924768503, "mdate": 1762924768503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The authors introduce causal probing, a technique that leverages a causal intervention to steer LLMs, using Hidden-state Driven Margin Intervention (HDMI), a probe-free, gradient-based technique that directly steers hidden states using the model’s native output. \n2. The authors also introduce a look ahead variant, (LA-HDMI) for text editing that backpropagates through the softmax embeddings, modifying the current hidden state so that the likelihood of user-specified tokens increases in next token generations while preserving fluency.\n3. The authors evaluate their method on \"completeness\" -- i.e elicitation of the new property and \"selectivity\", i.e. localized editing so that extant properties are not impacted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce inference time causal probing. Their method, Hidden-state Driven Margin Intervention (HDMI) performs a lightweight gradient-based update to hidden states that shifts the model’s output distribution\n2. The multi-token text edit changes are very interesting, and could be a useful direction in the current interpretability literature."}, "weaknesses": {"value": "1. My primary and main concern is that the authors' core contribution -- HDMI -- seems to be a reinvention of activation patching[1]/it's gradient based approximation, attribution patching[2] (!) -- it uses using gradient ascent to shift logits from one output type to another instead of direct patching. I think reframing the paper as a comparative study of probing vs patching based approaches, and claiming the advantages of causal vs correlational interventions would be acceptable, but claiming the invention of a new method seems overstated. Could the authors clarify how HDMI is different from activation/attribution patching[1,2], FutureLens[6] as well as representation fine-tuning[5]? In attribution patching, a gradient ascent/descent can be applied to the activations of the attention heads in order to identify important components in the model.\n2. The second issue is that given the oversight in [1], the authors have overlooked several important papers/baselines that do use causal interventions [3], [4] and others -- there's an entire subfield using these patching methods. Particularly, [4] tackles the very same problem of correlational and causal probing, albeit in a different setting.\n3. How were the reliability and selectivity metrics obtained? Have the authors investigated the effect of an HDMI edit on existing model capabilities like MMLU or GSM-8k?"}, "questions": {"value": "1. My primary and main concern is that the authors' core contribution -- HDMI -- seems to be a reinvention of activation patching[1]/it's gradient based approximation, attribution patching[2] (!) -- it uses using gradient ascent to shift logits from one output type to another instead of direct patching. I think reframing the paper as a comparative study of probing vs patching based approaches, and claiming the advantages of causal vs correlational interventions would be acceptable, but claiming the invention of a new method seems overstated. Could the authors clarify how HDMI is different from activation/attribution patching[1,2], FutureLens[6], as well as representation fine-tuning[5]? In attribution patching, a gradient ascent/descent can be applied to the activations of the attention heads in order to identify important components in the model.\n\n2. The second issue is that given the oversight in [1], the authors have overlooked several important papers/baselines that do use causal interventions [3], [4] and others -- there's an entire subfield using these patching methods. Particularly, [4] tackles the very same problem of correlational and causal probing, albeit in a different setting.\n\nI will increase my score if I am convinced by the authors' rebuttal to 1 and 2 above.\n\n4. Could the authors expand what LGD is and cite it at the very first mention?\n5. Could the authors add a limitations section on potential improvements to HDMI?\n6. It is unclear how completeness is measured in the multi-token setting, since the single-token setting uses a one-hot encoding based evaluation. How would this be determined in the multi-token settings?. Could authors clarify the same? \n7. Is there a significant improvement, based on Table 1? Could the authors bold the top values to improve readability?\n8. How does HDMI affect existing capabilities, such as on MMLU/GSM-8k etc, i.e on tasks unrelated to the task settings the intervention was applied for?\n\n[1] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\n[2] Syed, Aaquib, Can Rager, and Arthur Conmy. \"Attribution patching outperforms automated circuit discovery, 2023.\" URL https://arxiv. org/abs/2310.10348.\n\n[3] Meng, Kevin, et al. \"Locating and editing factual associations in gpt.\" Advances in neural information processing systems 35 (2022): 17359-17372.\n\n[4] Huang, Jing, et al. \"Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors.\" arXiv preprint arXiv:2505.11770 (2025).\n\n[5] - Wu, Zhengxuan, et al. \"Reft: Representation finetuning for language models.\" Advances in Neural Information Processing Systems 37 (2024): 63908-63962.\n\n[6] Pal, Koyena, et al. “Future Lens: Anticipating Subsequent Tokens from a Single Hidden State.” Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), Association for Computational Linguistics, 2023, pp. 548–60. Crossref, https://doi.org/10.18653/v1/2023.conll-1.37."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KVpoWhoNET", "forum": "8FnyRB90Bg", "replyto": "8FnyRB90Bg", "signatures": ["ICLR.cc/2026/Conference/Submission14349/Reviewer_pf6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14349/Reviewer_pf6F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925542067, "cdate": 1761925542067, "tmdate": 1762924767991, "mdate": 1762924767991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method called HDMI that steers a language model generation towards a target text and away from a source text. The experiments are on Causal Gym (a benchmark for localizing syntactic features to directions in activation space) and another corpus based on singular plural syntax agreement. They compare HDMI against some baselines, and HDMI performs best. \n\nOverall, I would say that this paper is very well written and presents all the ideas clearly. However, there are major problems when it comes to contextualizing the work within the current literature and a consequent poor coverage of existing baselines.\n\nThis paper demonstrates poor awareness of the following literatures:\n- Causal mediation and abstraction analysis of LLMs. For example, https://arxiv.org/abs/2004.12265 and https://arxiv.org/abs/2004.12265; see https://arxiv.org/abs/2301.04709 or https://arxiv.org/pdf/2408.01416 for surveys with citations. Causal gym is an eval designed for these methods, but you use it more like a steering evaluation than a localization evaluation.\n- Causal probing is actually already used as a term from a couple other papers https://aclanthology.org/2023.tacl-1.23.pdf and https://arxiv.org/abs/2307.15054\n- Steering literature. You cite Turner et al. 2023, but there really is a huge amount of steering literature now and plenty of different methods to steer with. For example, https://arxiv.org/abs/2205.05124, or https://arxiv.org/pdf/2310.06824 or https://arxiv.org/abs/2306.0334. You should compare against difference in means steering!\n- Representation fine-tuning: https://arxiv.org/abs/2404.03592. This is a very very similar idea to your paper and needs to be included as a baseline. It's not clear to me how your method exactly differs, and you would need to spell that out. \n- Axbench https://arxiv.org/abs/2501.17148 evaluates a bunch of methods for control, and you could evaluate your method on this benchmark to get standardized comparisons against a lot of different methods\n\nBroadly, this seems like a method for steering or editing and needs to be properly evaluated against the methods I point out above. I'm not sure how it will hold up against difference-in-means steering or representation fine-tuning. I think this would require too much experimentation and rewriting for this draft, so I think you should plan to revise and submit elsewhere."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "See summary"}, "weaknesses": {"value": "See summary"}, "questions": {"value": "See summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FqeHweKepw", "forum": "8FnyRB90Bg", "replyto": "8FnyRB90Bg", "signatures": ["ICLR.cc/2026/Conference/Submission14349/Reviewer_G5nM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14349/Reviewer_G5nM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952415439, "cdate": 1761952415439, "tmdate": 1762924767323, "mdate": 1762924767323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines a method for intervening on the hidden states of an LLM in order to generate a set of user specified edits. They do this by optimizing over the hidden state to maximize the margin between the original edit and the specified edit. They introduce a variant of the method that can intervene on several hidden states throughout a generation in order to preserve fluency of the decoded text. They evaluate their approach in LGD agreement and CausalGym and show that interventions alter the target properties without modifying unrelated properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper looks at a problem of interest and contributes a novel method. The idea of maximizing a logit margin seems valuable and simple. The single token method seems to reliably induce the desired change in the model's generation (although there is some discrepancy across the different benchmarks). In general, this seems like a valuable and useful contribution to the question of causal interventions on language model hidden states."}, "weaknesses": {"value": "# Presentation \n\nThe paper is fairly dense and seems to introduce a lot of notation and modeling that I don't understand the purpose of. E.g., the discussion of the difference between nuisance and causal variables is nice, but it doesn't add much to the description of the method. Ultimately, the central idea of the method (maximize the margin between an original token and a desired token with gradient descent on a hidden state) is quite simple and the paper could be much more direct about explaining the idea.\n\n# Analysis of results\n\nWhile the paper presents fairly comprehensive results, there is not very much analysis of how to interpret the evaluation numbers. E.g., Table 1 presents a substantial amount of data but the discussion in the text is fairly limited. I think the paper could be improved with an analysis that explains the significance of the results and potentially supports the numerical results with example successes or failures. Reading this, it's not quite clear why these metrics are appropriate or what they truly indicate. From what I can tell, the metrics are several related metrics that determine whether the new hidden state is reliably classified by a probe as an instance of the target class. I think this could be made more concrete and clear to readers.\n\n# Multi-token evaluation\n\nWhile I find the multi-token lookahead method interesting, the evaluation of it is quite limited. There's no real quantitative evaluation and the primary evidence of success is a potentially cherry-picked example in Fig 3a. I would expect an evaluation of this method to include numbers like: how often the method induces the desired token change, how many tokens are changed, and the fluency/quality of the changed output (evaluated by an LLM-as-a-judge or something similar). \n\n# Significance \n\nMy primary concern about the paper is that the authors do not make a clear case for the significance of the method. It is interesting that they can intervene on hidden states, but it's not clear what the practical purpose of this is. Is the goal to steer the model? Is it to understand generation mechanisms? Is there something else? For example, the Arora '24 paper uses CausalGym to identify properties of several different generation properties and show that it emerges in discrete stages. Even if the authors do not want to directly perform analysis like this in their paper, they should more clearly motivate what HDMI can be used for and argue that their method with advance those goals."}, "questions": {"value": "1. **Clarity of motivation:** What is the intended downstream use of HDMI—mechanistic interpretability, controllable generation, or general text editing—and how does the method concretely advance that goal beyond prior activation-steering work?\n\n2. **Interpretation of metrics:** How should readers interpret the reported completeness, selectivity, and reliability scores in practical or conceptual terms? Do these metrics correspond to observable differences in model behaviour or only to probe classification outcomes?\n\n3. **Evaluation of lookahead editing:** Beyond the qualitative examples in Figure 3, can the authors quantify how often LA-HDMI successfully performs the intended edits while maintaining fluency, and how sensitive this success rate is to hyperparameters such as (S_{\\max}) and temperature?\n\n4. **Presentation and framing:** Given that the method mainly performs gradient ascent on a logit margin, what is gained by the causal/nuisance variable formalism and extensive notation? Could the same idea be communicated more directly without that framing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KZz9X4MPvn", "forum": "8FnyRB90Bg", "replyto": "8FnyRB90Bg", "signatures": ["ICLR.cc/2026/Conference/Submission14349/Reviewer_NBLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14349/Reviewer_NBLB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762397222913, "cdate": 1762397222913, "tmdate": 1762924766887, "mdate": 1762924766887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}