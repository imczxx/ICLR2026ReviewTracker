{"id": "xf4ykWUcDH", "number": 18265, "cdate": 1758285793519, "mdate": 1759897115586, "content": {"title": "EEG-ImageNet: A Benchmark for Pre-training and Cross-Time Generalization of EEG-based Visual Decoding", "abstract": "Exploring brain activity in relation to visual perception provides insights into the biological representation of the world. \nWhile functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have enabled effective image classification and reconstruction, their high cost and bulk limit practical use. \nElectroencephalography (EEG), by contrast, offers low cost and excellent temporal resolution, but its potential has been limited by the scarcity of large, high-quality datasets and by block-design experiments that introduce temporal confounds.\nTo fill this gap, we present EEG-ImageNet, a benchmark for pre-training and cross-time generalization of visual decoding from EEG. \nWe collected EEG data from 16 participants while they viewed 4,000 images sampled from ImageNet, with image stimuli annotated at multiple levels of granularity. \nOur design includes two stages separated in time to allow cross-time generalization and avoid block-design artifacts.\nWe also introduce benchmarks tailored to non-block design classification, as well as pre-training experiments to assess cross-time and cross-subject generalization. \nThese findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.", "tldr": "", "keywords": ["EEG", "visual stimuli", "computer vision", "multi-modality"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c9cfc0d35bf807a832da22516b17df6d7a8f88b.pdf", "supplementary_material": "/attachment/09558b68e13cfd9489c9ef92da342e8c848900c0.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new dataset for the problem of decoding image stimulus class from EEG recordings. It is similar in nature to the Perceive dataset introduced in Spampinato et al. (CVPR 2017). There are 80 classes (instead of 40 in Perceive). The first 40 are coarse grain, just as in Perceive. The second 40 are fine grain, with 5 superordinate classes of 8 subordinate classes each. Just as in Perceive, there are 50 stimuli per class. While Perceive had 6 subjects, here there are 16. Just like Perceive, stimuli were presented for 500ms. Unlike Perceive, which recorded with an 128 electrode recorder, EEG was recorded from an unspecified 62-electrode recorder. Just like in Perceive, stimuli were presented in blocks, where all stimuli in the block were of the same class and all stimuli of that class were in the same block. Unlike Perceive, each block started with a presentation of the class label, presumably as visual text, though unspecified in the paper. Unlike Percevie, each block ended with some sort of test to measure attention, though the nature of this is unspecified in the manuscript, whcih says it was optional. Unlike Perceive, there were two recording session for 6 of the 16 subjects. In the second session, each block of 50 stimuli started with 30 stimuli from one class and ended with 20 stimuli of a different class. Results of classifying this dataset with various models are presented. The central claim is that the two-session design avoids the block confound discussed in Li et al. (2021)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None.\nThe dataset suffers from a known published confound that correlates stimulus class with drift in the EEG signal, essentially an embedded clock. Thus, decoders can and do classify the clock, not stimulus class, as demonstrated by Li et al. (2021) and follow on papers in TPAMI and CVPR by the same authors. As well as Xu et al. (2026) The impacts of temporal autocorrelations on EEG decoding, Biomedical Signal Processing and Control, 113."}, "weaknesses": {"value": "1. Li et al. (2021) subject 6 shows that the block confound even occurs when the training and test sets come from different blocks from different sessions. Thus the 2-session design does not remove the confound. Thus, this dataset still suffers from the block confound and the results are thus not to be trusted. Li et al. (2021) show that performance drops from near perfect to near chance when the confound is removed with randomized trials. There is no excuse that can justify using a block design instead of randomized trials.\n 2. Numerous details discussed above are missing, like what the exclusion criterion based on the attention test was. Excluding some trials from some subjects breaks the counterbalanced design. This introduces bias into the classification task and thus chance is not 1/k for k classes. It is not clear this was taken into account in computing statistical significance since the process for computing p values was not discusses. It is not clear whether correction for multiple comparisons was performed.\n 3. The stimulus presentation order was not specified. Exactly what was and was not randomized was not specified. Without this, it is impossible to assess the claim that the design does not exhibit correlation between stimulus class and a clock embedded into the signal.\n 4. The four tasks, WT, CT, CP, and PT are not described in sufficient detail to understand precisely what was done.\n 5. While you claim that this dataset is large, the published datasets associated with Li et al. (2021) and Ahmed et al. (2021)\n\nhttps://ieee-dataport.org/open-access/dataset-perils-and-pitfalls-block-design-eeg-classification-experiments\nhttps://ieee-dataport.org/open-access/dataset-object-classification-randomized-eeg-trials\n\nare just as large, if not larger, do not suffer from the block confound, and cover video stimuli as well as image stimuli, but are not mentioned or cited.\n 6. Many people refer to the Perceive dataset from Spampinato et al. (2017) ad EEG-ImageNet. Other datasets have been collected and published under the name EEG-ImageNet. Reusing the name is confusing."}, "questions": {"value": "1. Why did you not simply conduct randomized trials? That is the standard method universally adopted in all of experimental science to avoid confounds.\n2.  Why did you not try your methods on the two datasets mentioned above as these were collected with randomized trials and thus do not suffer from the block confound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MvmeiODV1t", "forum": "xf4ykWUcDH", "replyto": "xf4ykWUcDH", "signatures": ["ICLR.cc/2026/Conference/Submission18265/Reviewer_tGfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18265/Reviewer_tGfm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428031290, "cdate": 1761428031290, "tmdate": 1762927991187, "mdate": 1762927991187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an EEG dataset collected from 16 participants using ImageNet-based visual stimuli across two experimental stages. Although the dataset offers potential contributions to brain decoding research, several methodological and reporting concerns must be resolved before the work is suitable for publication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "See questions"}, "weaknesses": {"value": "See questions"}, "questions": {"value": "1. The authors acknowledge the temporal confounds introduced by block-design paradigms, yet paradoxically still adopt such a design in both Stage 1 and Stage 2. Although Stage 2 is claimed to be “non-block,” the structure of the experiment still presents images from the same category in short temporal clusters, failing to achieve full randomization. This undermines the core motivation of the dataset, which is to address block-related artifacts. Moreover, if the intention was to retain Stage 1 in order to investigate temporal effects introduced by block design, the paper does not provide any systematic analysis of such effects. Instead, the two stages adopt markedly different protocols (e.g., temporal spacing), thereby introducing an uncontrolled source of variability that further complicates interpretation and comparison across tasks.\n2. The procedural description of the experiment is vague and incomplete. Critical details such as how many blocks each participant completed, how long each block lasted, and whether the data was collected in a single continuous session or across multiple sessions remain unspecified. The paper also lacks a clear account of the train-test split strategy: it is unclear whether there is a fixed test set or if different splits are used for each evaluation. This ambiguity is compounded in Figure 2, where it is not indicated whether the training and testing sets in WT, CT, CP, and PT originate from Stage 1, Stage 2, or both. As a result, the methodological transparency is insufficient for reproducibility or proper evaluation of the reported benchmarks.\n3. The comparability of the reported results across different tasks is undermined by imbalanced subject participation. Specifically, ten of the sixteen participants did not take part in Stage 2, which directly affects the validity of comparisons across WT, CT, CP, and PT tasks. Because these tasks are evaluated on different subsets of participants, the results are not strictly comparable, yet the paper presents them side by side in Tables 2 and 3 without accounting for this discrepancy. This undermines claims about the relative difficulty or generalizability of each task.\n4. The dataset's scale is a notable improvement over previous EEG-visual studies, but it still falls short of the scale required for training large neural models or for making strong claims about cross-subject generalization. With only six participants contributing Stage 2 data and a total of 16 in Stage 1, the participant pool is very limited both in size and age diversity. The authors should moderate their claims of generalization, as the dataset’s limited scale compared to what the name EEG-ImageNet implies suggests that data from a broader and more diverse population is needed to support such claims.\n5. The term “cross-time” is used throughout the manuscript, but Stage 2 is conducted at least seven days after Stage 1, making “cross-day” a more precise and appropriate description. \n6. The paper evaluates several classic and relatively simple deep models but does not benchmark against recent and widely adopted architectures in EEG decoding. Incorporating mainstream models would substantially strengthen the benchmark’s value, improve reproducibility, and enable more meaningful comparisons with the current state of the art.\n7. The manuscript does not include any visualizations of the learned EEG features or model activations, missing an opportunity to underscore the neuroscientific relevance of the signals being classified. Without visual inspection of the EEG responses across categories or tasks, it remains unclear whether the models are learning meaningful neural correlates of perception or simply exploiting low-level artifacts such as EMG or eye movement signals. This data validation is particularly important for a dataset intended to bridge neuroscience and machine learning.\n8. While the dataset is described as having coarse- and fine-grained categories, the manuscript provides no visual or conceptual overview of the category structure. Including representative images, a hierarchical map of categories, or example trials would help the reader better understand the nature of the visual stimuli and the complexity of the decoding task. \n9. The feature extraction procedure involves a fixed window after stimulus onset, but the analysis does not discuss or control for individual variability in visual latency (e.g., typical VEP components like P1 or N170). Without evaluating how shifts in latency affect decoding performance, there remains a risk of overlap between responses to adjacent images, especially given the 500 ms stimulus duration in the visual presentation paradigm. This could lead to response contamination across trials, confounding model performance.\n10. Figure 6 presents image generation results, yet the method used for image reconstruction is not described in sufficient detail. It remains unclear how EEG features were mapped into image space, what generative model was used, how the outputs were evaluated, and what the overall goal of this analysis was."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iA1Ua1oV5v", "forum": "xf4ykWUcDH", "replyto": "xf4ykWUcDH", "signatures": ["ICLR.cc/2026/Conference/Submission18265/Reviewer_DHLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18265/Reviewer_DHLu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512794381, "cdate": 1761512794381, "tmdate": 1762927990559, "mdate": 1762927990559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EEG-Imagenet, which is a dataset for observing image stimuli and the corresponding signals of electroencephalogram (eeg), containing 16 subjects and 4,000 images from ImageNet. In addition to the collection process of the dataset, the article also defines the evaluation methods and tasks, and demonstrates the performance of common methods in these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of the article is very good. Currently, EEG-image datasets are lacking, and the field requires datasets that include more subjects and more samples. The writing of the article is also good, easy to understand, and the several task divisions proposed (WT, PT, CT and CP) are very comprehensive and appropriate."}, "weaknesses": {"value": "The biggest problem with the article is that the way the dataset is collected itself might be problematic. The article will continuously display the images of the same categories (50 images) to each subject, which will cause the subjects to maintain relatively stable brain activity during this period. This leads to the fact that there is actually no distinction between the images, and even brain activity simply reflects a category. This issue has been discussed in ''The Perils and Pitfalls of Block Design for EEG Classification Experiments'', IEEE TPAMI, 2020 It is generally believed within the field that this approach is incorrect.\n\nThis issue was also reflected in the subsequent experimental results, such as the WT results being very high. One possible validation is to conduct a search within images of the same category. I believe the model may not be able to retrieve effectively because continuous brain activities within the same category are likely to converge. In this case, the conclusion of the article may be affected, such as the accuracy rate of some task divisions mentioned later. \n\nAt the same time, the article does not compare this dataset with other datasets, such as when training classification models simultaneously, how the performance of the models trained on other datasets (such as THINGS) differs from that on this dataset. This is also a verification of whether the collection of the dataset is effective."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JVCpMvaYsY", "forum": "xf4ykWUcDH", "replyto": "xf4ykWUcDH", "signatures": ["ICLR.cc/2026/Conference/Submission18265/Reviewer_eNPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18265/Reviewer_eNPo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814908651, "cdate": 1761814908651, "tmdate": 1762927990197, "mdate": 1762927990197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a valuable and timely resource for EEG and visual neuroscience research. The dataset is carefully designed to mitigate known issues such as block-design confounds and limited category diversity, and it includes both coarse- and fine-grained visual labels. It is thoroughly benchmarked across several classical and deep learning models, providing a solid empirical baseline for future studies. The methodology and documentation are transparent and reproducible, with clear ethical considerations and open data plans. Overall, EEG-ImageNet represents a meaningful and nice contribution that could have significant impact in advancing EEG-based decoding and facilitating cross-disciplinary work between neuroscience and machine learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This work presents a valuable and timely resource for EEG and visual neuroscience research. The dataset is carefully designed to mitigate known issues such as block-design confounds and limited category diversity, and it includes both coarse- and fine-grained visual labels. It is thoroughly benchmarked across several classical and deep learning models, providing a solid empirical baseline for future studies. The methodology and documentation are transparent and reproducible, with clear ethical considerations and open data plans. Overall, EEG-ImageNet represents a meaningful and nice contribution that could have significant impact in advancing EEG-based decoding and facilitating cross-disciplinary work between neuroscience and machine learning."}, "weaknesses": {"value": "I think it would be valuable to include more extensive comparison with other dataset. It could be interest to compare the effect of having models pre-trained on other EEG datasets (e.g., SEED, DEAP, Things-EEG) and vice-versa to assess transferability and confirm that EEG-ImageNet enables broader generalization."}, "questions": {"value": "1. Could the authors compare EEG-ImageNet with models pre-trained on other EEG datasets (e.g., Things-EEG, SEED, DEAP) to quantify transfer learning performance? This would help assess whether EEG-ImageNet generalizes beyond its own benchmark.\n2. How consistent are the EEG signal qualities across sessions and participants? Were any metrics (e.g., SNR, channel dropout rates) tracked to ensure data reliability?\n3. Can you give more details on the pretraining (PT) setting, it was a bit unclear to me what was done."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ly1LHWaqJM", "forum": "xf4ykWUcDH", "replyto": "xf4ykWUcDH", "signatures": ["ICLR.cc/2026/Conference/Submission18265/Reviewer_Waum"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18265/Reviewer_Waum"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962503989, "cdate": 1761962503989, "tmdate": 1762927988684, "mdate": 1762927988684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We are aware of the temporal effect and have ensured that no such effect exists in the experimental setup."}, "comment": {"value": "Thank you very much for your reviews and comments on our paper. We noticed that several reviewers raised concerns about the issue of temporal effect (reviewer eNPo: “the way the dataset is collected itself might be problematic”; reviewer DHLu: “the structure of the experiment still presents images from the same category in short temporal clusters”; reviewer tGfm: “Thus the 2-session design does not remove the confound.”). **We would like to clarify that our research differs from Spampinato et al. and a series of research that suffered from the temporal effect.**\n\nSpampinato et al. (2017) used a block-design experimental paradigm, and Li et al. (2020) highlighted that such block-design setups may suffer from temporal-effect bias (i.e., classification performance being driven by temporal correlations rather than semantic category information). **We did mention this issue in our manuscript (see lines 89-93, 241-243, 316-320).** At the same time, the experimental design in our paper is significantly different, and we have taken efforts to ensure that no such temporal-effect bias exists in our experiments:\n\n1. For all tasks (WT, CT, CP, and PT), no image stimuli presented in the same block appear in both the training and testing sets. \n\n2. We notice that some reviewers are concerned about WT. However, the training set and testing sets are collected from different blocks in stage 2. We also clarify that all tasks (including WT) are tested in stage 2 for a fair comparison, while the data in stage 1 is only for training.\n\n3. Regarding reviewer tGfm’s concern about the work of Li et al. (2020) on subject 6 (they collected block data three times across distinct sessions and did cross-block classification): we note that their paradigm is quite different from our Stage 2 design. Actually, their paradigm is closer to our **CT** task in spirit. And they found classification performance approaching chance under that kind of cross‐block design (which is **consistent** with our findings). We used two‐way identification rather than raw accuracy in order to provide better numerical discrimination across tasks/models. We fully acknowledge the validity of their concerns, and we believe our experiment addresses these through our design choices.\n\n4. On the suggestion of completely randomized image order, which reviewer tGfm mentions is common in RSVP paradigms (e.g., each image ~50–100 ms, very rapid presentation), (1) we argue that such **a short presentation time is not sufficient for semantic image understanding**. For example, although rapid object recognition can begin very early, recent work shows that recognizing minimal images (i.e., degraded or constrained‐view objects) still requires presentation times up to 500 ms or more to reach robust accuracy (Harari et al., 2020). More importantly, semantic processing (as indexed by ERP components such as the N400) typically occurs in the ~250–500 ms post‐stimulus window and often later when deeper semantics or scene context is involved. (2) Moreover, when images are presented in a fully shuffled and highly rapid sequence, **the neural responses to consecutive stimuli inevitably overlap**, leading to semantic interference rather than purely independent trials. In such cases, the EEG signal reflects a mixture of residual activity from previous stimuli and the onset of the current one, making it harder for models to learn consistent semantic category representations. In contrast, our paradigm—where images of the same category are presented together during training but strictly separated between training and testing—allows the brain to form a stable and **discriminative category-level representation** while still preventing temporal leakage. This design thus facilitates the decoding of true semantic category differences, rather than transient overlaps between successive stimuli.\n\n5. A few reviewers pointed out that Figure 2’s description of the task setups was not sufficiently clear. We agree and will revise in the next version by adding more annotations and clarifications for each task to ensure that the reader fully understands how training/test splitting and stimulus ordering avoid temporal bias.\n\nIf you have any further questions or concerns about the temporal effect, we would be happy to continue the discussion. If you find that this response satisfactorily addresses your concerns about the temporal-effect issue, we kindly ask you to consider adjusting your review scores accordingly. For other questions raised, we will proceed to individual responses to each reviewer’s specific comments."}}, "id": "VGIIQSb8t4", "forum": "xf4ykWUcDH", "replyto": "xf4ykWUcDH", "signatures": ["ICLR.cc/2026/Conference/Submission18265/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18265/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18265/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763016221161, "cdate": 1763016221161, "tmdate": 1763016221161, "mdate": 1763016221161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}