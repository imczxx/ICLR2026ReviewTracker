{"id": "uePOSKl6AN", "number": 23868, "cdate": 1758349550576, "mdate": 1763724669203, "content": {"title": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies.\nMethods like PPO provide credit assignment by value estimation, but yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-wise rewards but suffer from several key limitations: they require high-quality process supervision labels, the feedback is unreliable due to probabilistic reward modeling, and their application in online reinforcement learning (RL) is time-consuming.\nTo overcome these limitations, we introduce a simple but efficient  method—Credit Assignment Policy Optimization (CAPO). CAPO avoids the complexities of prior approaches. Instead of training auxiliary models, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass only based on the correctness of the step itself, providing deterministic token-level credits to refine the tokens that were originally assigned identical rule-based rewards. This design choice not only simplifies the training pipeline but also enhances its generality, as our experiments show it works effectively with various powerful, widely accessible open-source models. The fine-grained feedback enables a crucial shift from purely outcome-oriented to process-oriented learning; our analysis of this dynamic leads to a reward structure that balances both objectives. To further enhance the accuracy and robustness, we employ voting mechanisms that scale with the number of generated critiques. \nExtensive experiments on various backbones like Llama and Qwen models show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across four challenging mathematical benchmarks and three out-of-domain benchmarks. Further analysis shows that CAPO can help the model to foster the learning of correct reasoning pathways leading to correct answers.", "tldr": "We propose CAPO, an efficient and simple method that improve LLM reasoning by utilizing a LLM as a Generative Process Reward Model (LLM-as-GenPRM) to provide verifiable and fine-grained credit assignment.", "keywords": ["large language models", "reinforcement learning", "credit assignment"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/909ec5e679ccbae6cc4fb023190557f6a0d175bc.pdf", "supplementary_material": "/attachment/a5bde0c4a651341fa8c6afa3297845668934c46b.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the credit assignment problem in RLVR by leveraging an off-the-shelf LLM to judge step correctness. The resulting algorithm (CAPO) outperforms baselines in multiple base models and evaluation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of directly tackling credit assignment is clear, and discussion on related means are sound.\n2. The discussion on design choices and ablations are abundant. The experiments are solid."}, "weaknesses": {"value": "1. Querying an off-the-shielf LLM online is very costly and poses additional challenge to RLVR infrastructure. This limits the use of CAPO, and makes the comparison with baselines a bit unfair.\n2. Lack of analysis on the motivation. Is CAPO assigning credit better? Does it lead to less error-prone generation?\n3. The benchmark performances are not very strong, see questions below."}, "questions": {"value": "1. Weakness 1. For each training token, how many tokens do LLM judges need to generate? If such additional cost are used in GRPO (e.g. more steps, longer contexts, larger group sizes), can CAPO still perform better?\n2. Weakness 2. Is there evidence on CAPO learning better credit assignment, other than final benchmark scores? Can better feedback at train-time translate to inference-time (e.g. step error rate)?\n3. Weakness 3. Let's focus on the largest scale Qwen-2.5-7B experiments.\n- The most direct baseline, GRPO-Rule, reduces performance in almost all benchmarks. How to explain?\n- The OOD general reasoning performance of CAPO generally decreases, while that of GRPO roughly maintains. How come better credit assignment hurts generalizability?\n- The absolute performance is a bit low comparing to other works of similar setup. As an example, in [1], Qwen-2.5-7B with GRPO can improve score of Math500 (64.6 -> 78.2), AIME24 (0.3 -> 15.6) and AMC23 (30.0 -> 62.5), much higher than GRPO baseline in CAPO's report. This looks more strange given CAPO's experiment includes a cold-start SFT. Although different setups are not directly comparable, this large mismatch requires some explanation.\n\n[1] SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jGr8kzRG7F", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Reviewer_NbTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Reviewer_NbTj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907416334, "cdate": 1761907416334, "tmdate": 1762942834124, "mdate": 1762942834124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Credit Assignment Policy Optimization (CAPO), an efficient and simple methodology designed to enhance Large Language Model (LLM) reasoning by resolving the coarse-grained credit assignment issue inherent in existing Reinforcement Learning with Verifiable Rewards (RLVR) methods. CAPO avoids training auxiliary models by leveraging an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM), which efficiently generates all step-wise critiques in a single pass based on the intrinsic correctness of each step. This provides deterministic, token-level credits to refine the original coarse rewards, enabling a crucial shift from purely outcome-oriented to process-oriented learning. The approach incorporates voting mechanisms for enhanced robustness and utilizes a structured reward design to effectively balance outcome and process objectives. Extensive experiments on various LLM backbones demonstrate that CAPO consistently outperforms supervised learning and other RL-based fine-tuning methods across multiple mathematical and out-of-domain reasoning benchmarks, proving its ability to foster the learning of correct reasoning pathways."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CAPO is \"elegantly simple\" and avoids the need for complex, time-consuming auxiliary models (like PRMs or value models in PPO) or costly high-quality process supervision data. It efficiently generates all step-wise credit in a single pass.\n\n2. By prompting a powerful LLM to focus on the intrinsic correctness of each step, CAPO provides deterministic process credits, which are more reliable and less susceptible to reward hacking compared to probabilistic, estimation-based signals.\n\n3. The framework utilizes off-the-shelf, powerful LLMs as the GenPRM, making the method broadly applicable, easily reproducible, and accessible since it does not require specialized or finely-tuned reward models. The GenPRM can even be accessed via APIs to minimize local computational footprint.\n\n4. The method consistently achieves superior performance over strong baselines (SFT, GRPO) on both in-domain mathematical and out-of-domain general reasoning tasks across multiple LLM scales. Analysis confirms it actively fosters the learning of correct reasoning pathways."}, "weaknesses": {"value": "1. The efficacy and reliability of the process credit mechanism are predicated on the use of an LLM-as-GenPRM that is significantly more capable than the policy model being trained (e.g., using a 70B+ model to guide a 1B-7B model). This relies on the availability of a superior external model.\n\n2. The introduction of process-level rewards creates a potential conflict with the original outcome-level reward signal. This required an in-depth analysis and the proposal of a non-trivial asymmetric reward shaping strategy to create a functional hierarchical learning objective.\n\n3. The current credit assignment is performed at the step level, typically delineated by \\n \\n. While practical, this step-level granularity is not truly token-level and its reliability may depend on the consistency of the policy model's generation format."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k3K3hYE5ER", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Reviewer_zJU4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Reviewer_zJU4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990495859, "cdate": 1761990495859, "tmdate": 1762942833909, "mdate": 1762942833909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Credit Assignment Policy Optimization (CAPO), an efficient and simple methodology designed to enhance Large Language Model (LLM) reasoning by resolving the coarse-grained credit assignment issue inherent in existing Reinforcement Learning with Verifiable Rewards (RLVR) methods. CAPO avoids training auxiliary models by leveraging an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM), which efficiently generates all step-wise critiques in a single pass based on the intrinsic correctness of each step. This provides deterministic, token-level credits to refine the original coarse rewards, enabling a crucial shift from purely outcome-oriented to process-oriented learning. The approach incorporates voting mechanisms for enhanced robustness and utilizes a structured reward design to effectively balance outcome and process objectives. Extensive experiments on various LLM backbones demonstrate that CAPO consistently outperforms supervised learning and other RL-based fine-tuning methods across multiple mathematical and out-of-domain reasoning benchmarks, proving its ability to foster the learning of correct reasoning pathways."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CAPO is \"elegantly simple\" and avoids the need for complex, time-consuming auxiliary models (like PRMs or value models in PPO) or costly high-quality process supervision data. It efficiently generates all step-wise credit in a single pass.\n\n2. By prompting a powerful LLM to focus on the intrinsic correctness of each step, CAPO provides deterministic process credits, which are more reliable and less susceptible to reward hacking compared to probabilistic, estimation-based signals.\n\n3. The framework utilizes off-the-shelf, powerful LLMs as the GenPRM, making the method broadly applicable, easily reproducible, and accessible since it does not require specialized or finely-tuned reward models. The GenPRM can even be accessed via APIs to minimize local computational footprint.\n\n4. The method consistently achieves superior performance over strong baselines (SFT, GRPO) on both in-domain mathematical and out-of-domain general reasoning tasks across multiple LLM scales. Analysis confirms it actively fosters the learning of correct reasoning pathways."}, "weaknesses": {"value": "1. The efficacy and reliability of the process credit mechanism are predicated on the use of an LLM-as-GenPRM that is significantly more capable than the policy model being trained (e.g., using a 70B+ model to guide a 1B-7B model). This relies on the availability of a superior external model.\n\n2. The introduction of process-level rewards creates a potential conflict with the original outcome-level reward signal. This required an in-depth analysis and the proposal of a non-trivial asymmetric reward shaping strategy to create a functional hierarchical learning objective.\n\n3. The current credit assignment is performed at the step level, typically delineated by \\n \\n. While practical, this step-level granularity is not truly token-level and its reliability may depend on the consistency of the policy model's generation format."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k3K3hYE5ER", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Reviewer_zJU4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Reviewer_zJU4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990495859, "cdate": 1761990495859, "tmdate": 1763653947943, "mdate": 1763653947943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAPO (or the proposed method's name), a novel approach to enhance LLM reasoning through a stepwise PRM. The key innovation lies in moving beyond traditional, coarse-grained reward methods by assigning differential rewards to tokens based on the specific reasoning step they belong to. This fine-grained credit assignment is intended to better pinpoint which parts of the reasoning chain contribute to success or failure. The method demonstrates some empirical improvement on selected reasoning benchmarks and includes a valuable ablation study on the reward weighting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological Innovation: The method proposes an innovative approach using an LLM to implement a stepwise PRM. This design provides a more granular form of credit assignment by attributing different reward values to tokens depending on their corresponding reasoning step, which is a significant step beyond whole-response-based RL methods.\n\n2. The paper includes a valuable ablation study on the different weights of the $P$ term (as presented in Table 4). This analysis effectively investigates the model's sensitivity to this key hyperparameter and provides insight into the optimal balance between the different reward components.\n\n3. Empirical Results: The approach demonstrates a certain degree of improvement on reasoning tasks, suggesting the potential of a stepwise credit assignment mechanism."}, "weaknesses": {"value": "1. The definition and segmentation of the output reasoning steps is a fundamental and critical aspect of this method. However, the paper only briefly addresses this matter in Appendix D. There is no sufficient justification for the forced output mechanism using markers like <step k>. The reader is left to wonder if a more principled or adaptive step segmentation strategy could be devised to yield a significantly better-performing PRM.\n\n2. The method only achieves a not entirely convincing improvement when tested on models with fewer than 10B parameters. This restricts the ability to generalize the findings and claim the method's effectiveness across more LLMs.\n\n3. A key motivation of this work is to improve the quality of the reasoning process. Despite this, the experimental section lacks a qualitative analysis that demonstrates an actual improvement in the quality of the process outputs, which is necessary to support the paper's core hypothesis.\n\n4. Typos/Minor Issues: There are some minor editorial issues, such as the 'r' on line 189, which should be capitalized (i.e., 'R')."}, "questions": {"value": "In Equation 2, the reward term $R_{i,t}$ undergoes group normalization by subtracting the mean of $R$ across all tokens in all other rollouts. Given that the Process Reward Model is designed to assign a reward specific to the current step $i$ and token $t$, is this form of group normalization appropriate and theoretically sound for a step-wise reward model? Could this global normalization potentially obscure the fine-grained, step-specific signal that the method aims to capture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqsZKozdjw", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Reviewer_RfL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Reviewer_RfL9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997284263, "cdate": 1761997284263, "tmdate": 1762942833671, "mdate": 1762942833671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation in Reinforcement Learning with Verifiable Rewards (RLVR) for fine-tuning Large Language Models (LLMs): the coarse-grained, binary (correct/incorrect) reward assigned to an entire response lacks the granularity to provide differentiated feedback on individual reasoning steps. This imprecise credit assignment hinders the model's ability to discern which specific parts of its reasoning are sound or flawed. To address this, the authors propose Credit Assignment Policy Optimization (CAPO), a method that introduces fine-grained, token-level credit assignment, avoiding the complexity and potential unreliability of training auxiliary Process Reward Models (PRMs).\nExtensive experiments on mathematical and general reasoning benchmarks, using the Llama and Qwen model families (1B-7B), show that CAPO consistently outperforms both supervised fine-tuning and RL baselines like GRPO. Analyses indicate that CAPO leads to longer, more exploratory responses that correlate with higher final accuracy, suggesting it effectively fosters correct reasoning pathways."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a creative synthesis of existing ideas. Using a general-purpose LLM as a verifier for process supervision is a simple yet powerful insight that effectively bypasses the significant overhead of training dedicated PRMs while leveraging the inherent reasoning capabilities of modern LLMs. The formulation of the credit assignment mechanism and the analysis of the outcome-process reward trade-off are novel contributions to the RLVR framework."}, "weaknesses": {"value": "While efficient, the method introduces non-trivial computational overhead compared to simpler rule-based verifiers such as GRPO. Each policy rollout requires multiple inference passes with a very large LLM (GenPRM). Although the paper correctly notes this aligns with trends of using large models as guides, a more detailed discussion of the actual cost—such as GPU hours or a direct wall-clock time comparison with GRPO—would strengthen the practicality claim. The paper is transparent about this cost, but a deeper analysis would be valuable.\n\nThe approach assumes that a significantly more capable GenPRM provides reliable feedback. While reasonable, this assumption introduces an inherent scalability ceiling; the method's effectiveness might diminish as the policy model's capability approaches that of the judge, a scenario not explored here. This relates to the broader, unexamined question of the quality of the process supervision provided by the LLM-judge itself."}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "al3v2BJcrx", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Reviewer_p6qQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Reviewer_p6qQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998576263, "cdate": 1761998576263, "tmdate": 1762942833464, "mdate": 1762942833464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Action Required: Review of Author Rebuttal"}, "comment": {"value": "Dear Reviewers,\n\nThe authors have submitted their rebuttal. Please review their responses and provide any follow-up, such as additional questions or revisions to your review.\n\nThe reviewers have expressed differing views, and we note that Reviewer zJU4 has raised their score based on the rebuttal. As you form your final recommendation, we ask that you give careful consideration to both the authors' response and the perspectives of your fellow reviewers.\n\nThank you for your contributions to this process.\n\nSincerely,\nYour AC"}}, "id": "KZI5SbVccN", "forum": "uePOSKl6AN", "replyto": "uePOSKl6AN", "signatures": ["ICLR.cc/2026/Conference/Submission23868/Area_Chair_M8sh"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23868/Area_Chair_M8sh"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission23868/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729720676, "cdate": 1763729720676, "tmdate": 1763729720676, "mdate": 1763729720676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}