{"id": "kTEOG9a2W3", "number": 21657, "cdate": 1758320208402, "mdate": 1759896910176, "content": {"title": "PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations", "abstract": "Diffusion models are expressive priors for generating and predicting data from high-dimensional dynamical systems. Yet, purely data-driven approaches often lack reliability and trustworthiness, motivating growing interest in physics-informed machine learning (PIML). Most existing PIML methods, however, assume access to exact governing equations during training—an assumption that fails when the dynamics are unknown or too complex to model accurately. To address this gap, we introduce PHDME (Port-Hamiltonian Diffusion Model), a physics-informed diffusion framework that learns system dynamics without requiring exact equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent and diverse dataset for diffusion training. To enforce physics-consistency, we embed the GP-dPHS structure directly into the diffusion training objective through a loss that penalizes deviations from the learned Hamiltonian dynamics, weighted by the GP’s predictive uncertainty. After training, we employ conformal prediction to provide distribution-free uncertainty quantification of the generated trajectories. In this way, PHDME is designed for regimes with scarce data and unknown equations, enabling data-efficient, physically valid trajectory generation with calibrated uncertainty estimates.", "tldr": "PHDM: diffusion guided by GP-learned Port-Hamiltonian energy gradients from sparse data. No exact fuction needed; structure is conserved and uncertainty is calibrated for spatiotemporal prediction.", "keywords": ["Physics-informed Learning", "Diffusion Model", "Port-Hamiltonian system", "Uncertainty Quantification", "Gaussian process"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6af739aeed1702d0939bbff2a8e8b7f4b0fe000a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PHDME, a physics-informed diffusion model designed to predict dynamical systems from sparse data without access to the governing equations. The core idea is a two-stage approach: \n- Train a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited data, and generate synthetic data for diffusion training/validation;\n- Train a diffusion model with dPHS residual and boundary term constrains, weighted by GP uncertainty.\nThe final PHDME model generates entire spatiotemporal fields in a single pass and uses conformal prediction for uncertainty quantification.  \n\nOn a 1D wave/soft-string benchmark with limited observations, PHDME outperforms vanilla DDPM and a weak-physics DDPM and is faster at rollout than GP simulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### Originality\n- The paper combines GP-dPHS with diffusion models in a new way. The specific approach of using GP posterior samples to generate synthetic training data for diffusion models is novel. The incorporation of GP predictive variance directly into the diffusion training loss as a weighting factor represents a new approach to handle observation uncertainty in physics-informed generation. The application of conformal prediction to diffusion-generated PDE trajectories for uncertainty quantification hasn't been demonstrated before in this context.\n\n### Quality\n- The methodological design is a key strength. The mathematical formulation of GP-dPHS is proper and introduces principled inductive bias. The paper provides complete implementation details including data generation process.The paper reports metrics with variance estimates. \n\n### Clarity\n- The motivation, preliminary section and the two-stage training process are clearly presented. The assumptions are stated explicitly.\n\n### Significane\n- The potential significance of this work is high, as it is promising for data-scarce, equation-unknown regimes. The framework itself is general and will be of high interest to researchers who face similar challenges."}, "weaknesses": {"value": "### Misleading Claims and a Mismatch Between Motivation and validation\nThe paper's experimental validation fails to test the very problem it claims to solve, and the presentation of this validation is misleading. \n\nThe method is repeatedly motivated by the significant challenge of modeling \"highly nonlinear and unstructured dynamics\". The authors claim their experiments use a \"physically faithful simulator\" to \"approximate the highly non-linear PDE dynamics of the soft robots\". This language suggest a complex validation. However, the only PDE used for all experiments is a 1D wave equation with fixed Dirichlet boundaries, which is canonical and linear, not a complex, nonlinear one. Also, the paper explicitly states they \"leverage the (1d wave equation) simulator to create 10,000 data samples as the real-world test set\", which is confusing/misleading terminology for synthetic data. The table 1 itself is labeled \"Metrics Test on Real-World Data\" even though the test set is generated by the simulator above. Line 412-413, the authors states \"We present the grid-average metrics for the nonlinear string PDEs in Table (4.2)\", which is again very misleading and confusing.\n\n\nWhile using the wave equation as a testbed is a reasonable for method development, the evaluation is limited to this single 1D system. This is a canonical linear PDE. For this simple system, the Hamiltonian is a known, simple quadratic function. The paper's core idea of learning a complex, unknown Hamiltonian with a GP-dPHS is an overkill. The GP is simply learning a quadratic surface. This provides zero evidence that the method's complex machinery is effective to handle complex, nonlinear dynamics that supposedly motivate this paper. \n\nTo support its central claims, a minimum requirement is that the paper should demonstrate the method's utility beyond a trivial linear case. Furthermore, the authors are suggest to be more precise in the main text. \n\n### UQ claim contradicted by the paper’s own results.\nThe paper repeatedly claims “calibrated uncertainty” and “tighter conformal thresholds,” but the reported Non-Conformity Score (NCS) shows the opposite. In Table 1, PHDME has NCS = 6.41×10⁻³, whereas the baseline DDPM has NCS = 6.82×10⁻⁶ (with lower being better for tighter bounds). That makes PHDME’s non-conformity roughly ~940× larger than DDPM’s, i.e., substantially looser bounds, not tighter. This directly undermines the UQ claim.\n\n### Missing baseline and contradiction in time comparison\nThe paper's method is a two-stage pipeline. The most critical baseline is missing: What is the accuracy of the GP-dPHS model on its own? The paper only compares its speed but not its accuracy. Also, the text in Section 4.2, when summarizing the table, states \"and the generative time of the proposed PHDME is significantly higher than the GP-dPHS\", which is the exact opposite of what the data in Table 1 shows. \n\n### Confusing baseline results: DDPM+Limited Physics\nThe \"DDPM+Limited Physics\" (boundary conditions) baseline performs worse than the pure DDPM baseline. This is highly conter-intuitive: adding correct physical information should improve, not degrade, performance. The authors are suggested to investigate and explain why adding boundary conditions resulted in worse performance. This suggests that the baselines were poorly tuned or implemented, making the results questionable."}, "questions": {"value": "### Regarding the Possibly Missing Core Equation\nThe paper defines the base loss but it never provides the uncertainty-aware physics loss $\\tilde{R}_\\text{phys}$ explicitly. The cited Appendix A.5 only contains 3D plots, not the loss function. Could the authors please provide the precise formula for this uncertainty-weighted loss?\n\n### Confusing Justification for Using Conformal Prediction\nIn section 3.2, the authors state: \"The reverse diffusion in DDPM is stochastic, which supports the exchangeability assumption of conformal prediction\". Could the authors please elaborate more on this reasoning? The validity of CP's grarantee depends on the data exchangeability assumption, not on any property of the model. How would DDPM sampling supports the data exchangeability property?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AOeVyaK6mw", "forum": "kTEOG9a2W3", "replyto": "kTEOG9a2W3", "signatures": ["ICLR.cc/2026/Conference/Submission21657/Reviewer_4tWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21657/Reviewer_4tWQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855926975, "cdate": 1761855926975, "tmdate": 1762941875251, "mdate": 1762941875251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PHDME (Port-Hamiltonian Diffusion Model), a hybrid generative modeling framework that integrates Gaussian-process distributed Port-Hamiltonian systems (GP-dPHS) with diffusion models to learn physically consistent dynamics without explicit governing equations. A two-step process is put forth:\n\n- GP-dPHS training: limited observations are used to learn a probabilistic Hamiltonian representation of the dynamics, including uncertainty.\n\n- Diffusion model training: the learned physics prior (Hamiltonian structure and uncertainty) is embedded into the diffusion model’s loss function, enforcing energy-based consistency across diffusion steps.\n\nThen, conformal prediction is used post-hoc for uncertainty calibration. The method is evaluated on a 1-D nonlinear wave-like PDE (string vibration), showing faster generation and lower mean-square error than purely data-driven or partially physics-aware diffusion baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Combines physics-informed priors (via GP-based Port-Hamiltonian systems) with score-based diffusion modeling. This bridges structured physics modeling and generative uncertainty modeling.\n\n- Unlike PINNs or traditional physics-informed diffusion models, it infers latent physical structure directly from data.\n\n- The approach leverages limited observations to build a probabilistic physics prior before training the diffusion model.\n\n- Built-in uncertainty quantification: uses both GP posterior variance (during training) and conformal prediction (post-training) for calibrated uncertainty estimates.\n\n- Single-shot field generation avoids step-by-step numerical integration (claimed ~20× faster than GP-dPHS rollouts).\n\n- The two-stage process and the Hamiltonian residual penalty are well-defined and interpretable."}, "weaknesses": {"value": "- Only a single PDE (the 1-D wave/string system) is tested; no real or high-dimensional datasets are used. Claims of generality (e.g., to soft robots, elasticity) are therefore speculative.\n\n- The “scarce data” scenario is simulated, but robustness to measurement noise or model misspecification is not demonstrated.\n\n- The reviewer would appreciate an elaboration on the contribution over existing GP-dPHS work (Beckers et al., 2022; Tan et al., 2024). The paper primarily extends these with a diffusion-based generator, but the gain over standard GP-dPHS rollout or other physics-aware surrogates is modest and not deeply analyzed.\n\n- Picking up from the above, it would help to explicitly quantify the benefit of each component (GP uncertainty weighting, Hamiltonian penalty, conformal calibration) (via an ablation study)."}, "questions": {"value": "- How does PHDME differ conceptually and empirically from prior physics-informed diffusion models (e.g., Bastek et al. 2024) and Latent SDE frameworks?\n\n- What is gained by coupling a GP-dPHS prior specifically, versus directly enforcing energy conservation or using a learned Hamiltonian neural network?\n\n- Why are methods like the Deep Markov Model, Neural ODE/SDE, or Neural Operator not included for comparison? These are natural comparators for dynamics learning without explicit equations.\n\n- Also, while classical Hamiltonian Neural Networks (HNNs) are deterministic, stochastic extensions do exist and could be used here as baselines to compare against.\n\n- How does the GP-dPHS scale with system dimensionality and number of spatial nodes?\n\n- Can the approach handle higher-dimensional PDEs or coupled multi-field systems?\n\n- How sensitive is the framework to noise or imperfect observation coverage?\n\n- Does the GP-uncertainty weighting truly improve robustness or simply regularize training?\n\n- Can the learned Hamiltonian be inspected or visualized to confirm that it corresponds to physically meaningful energy terms?\n\n- Have any field or experimental datasets (e.g., soft robotics, structural dynamics) been tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wnFQ7baoOF", "forum": "kTEOG9a2W3", "replyto": "kTEOG9a2W3", "signatures": ["ICLR.cc/2026/Conference/Submission21657/Reviewer_yDyr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21657/Reviewer_yDyr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935442614, "cdate": 1761935442614, "tmdate": 1762941874050, "mdate": 1762941874050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a physics-informed diffusion framework for spatiotemporal forecasting without known governing equations: it first learns a Gaussian-process distributed Port-Hamiltonian prior (GP-dPHS) from scarce observations, then uses that prior both to synthesize training trajectories and to impose an uncertainty-weighted Hamiltonian-consistency loss during diffusion training, yielding single-shot, physically consistent predictions and conformal-prediction uncertainty bounds."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The promise of this paper is appealing -- i.e., obtaining a Hamiltonian functional with limited data, while leveraging it to fine-tune the diffusion model. The framework produces full space-time fields in a single diffusion draw (conditioning on two frames), thereby bypassing costly rollouts and eliminating reliance on a heavy GP-dPHS integrator at test time. From only ~20 observed samples, GP-dPHS posterior draws generate large, physically consistent training sets for the diffusion model. Vanilla GPs become more challenging with numerous inputs and large datasets, but here the GP is learned over a low-dimensional state, thereby avoiding the high-dimensional curse. From only ~20 observed samples, GP-dPHS posterior draws generate large, physically consistent training sets for the diffusion model."}, "weaknesses": {"value": "1. Training and sampling from GP-dPHS to synthesize trajectories is computationally demanding (the paper trains diffusion because direct GP-dPHS rollouts are slow), so the pipeline carries nontrivial offline overhead.\n\n2. Results are shown on a single synthetic 1D string benchmark with only 20 observed samples, leaving external validity to other PDEs/real data untested."}, "questions": {"value": "1. When the GP posterior “flattens” at very small state magnitudes, energy gradients degrade and the diffusion model misses fine oscillations. Could you add a study that (i) adaptively down-weights the physics loss where GP variance spikes, (ii) triggers targeted data acquisition in those regions, and (iii) reports performance/coverage before vs. after this mitigation?\n\n2. Stage-1 is described as a “slow but structured deep prior.” Please quantify the wall-clock time and memory requirements for GP-dPHS training versus PHDME training/inference, and include one higher-dimensional or real-data case to probe scaling. How do results change with fewer/more GP posterior draws for synthetic data (e.g., 2k vs. 10k)?\n\n3. In the case where the true dynamics are energy-conserved, the Hamiltonian system should be symplectic. In such an energy-conserving regime, does your diffusion model preserve any symplectic structure in practice, and could it be made to do so by design? Specifically, can you try constraining the denoiser with a symplectic parameterization or a divergence-free/Poisson-bracket form? Do you think such a setting is feasible and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fo347bPzex", "forum": "kTEOG9a2W3", "replyto": "kTEOG9a2W3", "signatures": ["ICLR.cc/2026/Conference/Submission21657/Reviewer_YWjN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21657/Reviewer_YWjN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239857964, "cdate": 1762239857964, "tmdate": 1762941873586, "mdate": 1762941873586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle a problem on how to make data-driven generative models respect physics when the underlying governing equations are unknown. Most physics-informed ML frameworks (like PINNs) rely on having systems of equations whereas this one doesn’t. Their idea is to first train a Gaussian Process–based Port-Hamiltonian model on scarce observations to learn an energy-based representation of the dynamics. Then, they use that GP model to both generate synthetic, physically consistent training data, and inject physics-awareness into the diffusion model’s training loss. The result is a diffusion model that can generate physically valid spatiotemporal trajectories even in low-data regimes, with calibrated uncertainty via conformal prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The reasoning linking Hamiltonian structure, energy conservation, and diffusion regularization is coherent. The experiments support the core claim that PHDME can generate physically consistent trajectories without explicit governing equations. The paper is technically sound but logically structured, in particular, the exposition of the Hamiltonian and GP-dPHS background is rigorous, and the two-stage training diagram helps readers grasp the workflow. The writing demonstrates mastery of both physics-based modeling and generative modeling."}, "weaknesses": {"value": "The experimental diversity is limited: a single synthetic system (the nonlinear string + qualitative check) doesn’t establish generality across different physical domains (e.g., fluid flow, elasticity, robotics, multiphase systems). The reliance on a simulator that’s already physics-based may inflate the gains of PHDME versus standard data-driven baselines. The GP-dPHS prior is treated, more or less, as a black box. I hope to see checks on that energy or momentum are actually conserved (if “physics consistency” is the core contribution, the authors may consider proving it quantitatively, via eg. energy error plots, invariants over time, etc.). While conformal prediction is implemented, the calibration methodology is somewhat surface-level (no ablation or coverage plots). Also, comparing only to plain DDPM and GP-dPHS isn't probably enough, the authors may consider adding more stronger benchmarks."}, "questions": {"value": "My major questions rise from stage 2 data generation.\n\nIt seems you generate training samples for the diffusion model by drawing Hamiltonian gradients from the GP-dPHS posterior and integrating them to create trajectories. How do you ensure these samples represent physically plausible dynamics rather than artifacts of the GP prior? Since the GP posterior is conditioned on very few observations, random Fourier feature sampling might produce unrealistic dynamics. How sensitive is PHDME to the number of samples or to kernel hyperparameters? Could the diffusion model be learning the GP’s bias instead of the true system’s variability? In addition, once you use GP-based uncertainty in training, and conformal prediction for post-hoc calibration, how do these two uncertainty sources interact? Are they redundant, complementary, or potentially conflicting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gQsuVNEVu7", "forum": "kTEOG9a2W3", "replyto": "kTEOG9a2W3", "signatures": ["ICLR.cc/2026/Conference/Submission21657/Reviewer_bG2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21657/Reviewer_bG2L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762804248800, "cdate": 1762804248800, "tmdate": 1762941873182, "mdate": 1762941873182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}