{"id": "A9EP0RPrTL", "number": 4367, "cdate": 1757667568676, "mdate": 1759898036814, "content": {"title": "ReMix: Towards a Unified View of Consistent Character Generation and Editing", "abstract": "Consistent character generation and editing has made significant strides in recent years, driven by advancements in large-scale text-to-image diffusion models (e.g., FLUX.1) that produce high-fidelity outputs. Yet, few methods effectively unify them within a single framework. Generation-based methods still struggle to enforce fine-grained consistency, especially when tracking multiple instances, whereas editing-based approaches often face challenges in preserving posture flexibility and instruction understanding. \nTo address this gap, we propose **ReMix**, a unified framework for character-consistent generation and editing. It consists of two main components: the ReMix Module and IP-ControlNet. The ReMix Module leverages the multimodal understanding capabilities of MLLM to edit the semantic content of the input image, and adapts the instruction features to be compatible with a native DiT backbone. While semantic editing can ensure coherent semantic layout, it cannot guarantee consistency in pixel space and posture controllable. To this end, IP-ControlNet is introduced to coupe with these problems. Specifically, inspired by convergent evolution in biology and by decoherence in quantum systems, where environmental noise induces state convergence, we hypothesize that jointly denoising the reference and target images within a same noise space promotes feature convergence, thereby aligning the hidden feature space. Therefore, architecturally, we extend ControlNet to not only handle sparse signals but also decouple semantic and layout features from reference images as input. For optimization, we establish an ε-equivariant latent space, allowing visual conditions to share a common noise space with the target image at each diffusion timestep. We observed that this alignment facilitates consistent object generation while faithfully preserving reference character identities.\nThrough the above design, ReMix supports a wide range of visual-guidance tasks, including personalized generation, image editing, style transfer, and multi-visual-condition generation, among others.\nExtensive quantitative and qualitative experiments have demonstrated the effectiveness of our proposed unified framework and optimization theory.", "tldr": "A unified approach to character-consistent generation and image editing", "keywords": ["Diffusion Model; Consistent Character Generation; Image Editing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ae3b8efe6662fddfa4a1ef976c3582cd16723f05.pdf", "supplementary_material": "/attachment/c0d14a9c589932a089e23be32d9ec726f57f4a28.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents ReMix, a unified framework that addresses the challenge of character-consistent generation and editing in text-to-image diffusion models. The authors identify that existing generation-based methods struggle with fine-grained consistency, particularly for multiple character instances, while editing-based approaches have difficulty preserving posture flexibility and instruction understanding. ReMix consists of two main components: the ReMix Module, which leverages MLLM's multimodal understanding to edit semantic content and adapt instruction features for DiT backbones, and IP-ControlNet, which ensures pixel-space consistency and posture control. Drawing inspiration from convergent evolution in biology and decoherence in quantum systems, the authors hypothesize that jointly denoising reference and target images within the same noise space promotes feature convergence and alignment. IP-ControlNet extends the original ControlNet architecture to handle both sparse signals and decoupled semantic/layout features from reference images. The optimization establishes an ϵ-equivariant latent space where visual conditions share a common noise space with the target image across diffusion timesteps, facilitating consistent generation while preserving character identities. ReMix supports diverse visual-guidance tasks including personalized generation, image editing, style transfer, and multi-visual-condition generation, with extensive experiments demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Character-consistent generation and editing is a highly important problem in the image generation community. The paper addresses a real gap by proposing a unified framework that bridges generation-based and editing-based approaches, which have traditionally been treated separately. This unified perspective is valuable for practical applications requiring both capabilities.\n\n2. The method demonstrates strong innovation in several aspects. First, the introduction of the ϵ-equivariant latent space, inspired by concepts from convergent evolution and quantum decoherence, provides a principled theoretical foundation for joint denoising of reference and target images. Second, the extension of ControlNet to decouple semantic and layout features while handling sparse signals represents a thoughtful architectural improvement. The combination of MLLM-based semantic editing (ReMix Module) with pixel-level consistency control (IP-ControlNet) offers an elegant solution to balancing semantic coherence and fine-grained consistency.\n\n3.  The paper provides extensive quantitative and qualitative experiments demonstrating the effectiveness of ReMix across diverse tasks including personalized generation, image editing, style transfer, and multi-visual-condition generation. The breadth of experimental scenarios and the comparative results lend credibility to the claimed improvements over existing methods."}, "weaknesses": {"value": "1. In the introduction, the authors claim the method can handle multi-object generation, particularly for tracking multiple character instances. However, the visualization results contain almost no examples of multi-object scenarios, and the methodology section lacks specific design considerations for this capability. The authors need to provide explicit evidence (e.g., multi-character generation examples) and explain which components of ReMix enable this functionality, or clarify the scope of this claim.\n2. The experimental evaluation is insufficient in scope. The authors should compare ReMix on more established public benchmarks, such as story consistency generation benchmarks (e.g., StoryBench, StorySalon) and standard image editing datasets (e.g., MagicBrush, InstructPix2Pix benchmarks). More critically, as a unified framework for generation and editing, ReMix should be compared against similar unified models like OmniGen, which also addresses both tasks. The absence of these comparisons makes it difficult to assess the method's competitive advantage.\n3.The paper does not report the computational efficiency of ReMix. Given that the framework introduces additional modules (ReMix Module and IP-ControlNet), it is essential to analyze the inference time overhead, memory consumption, and computational cost compared to baseline models. Does the unified framework incur significant additional inference time? This information is critical for evaluating the practical applicability of the method."}, "questions": {"value": "Please see the weakness。"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HY5ZMR4QT0", "forum": "A9EP0RPrTL", "replyto": "A9EP0RPrTL", "signatures": ["ICLR.cc/2026/Conference/Submission4367/Reviewer_hcdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4367/Reviewer_hcdt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761317091223, "cdate": 1761317091223, "tmdate": 1762917319139, "mdate": 1762917319139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for consistent character generation and editing. It injects the MLLM instruction feature to DiT backbone to enable the semantic editing. It also introducesIP-ControlNet to enforce the pixel-level consistency and posture condition. Results show that the proposed method can support a wide range of character condition generation tasks and image editing tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method can perform various tasks with good character consistency compared to the baselines.\n\n2. The technical idea to build a connector to inject the MLLM instruction feature into the native DiT background is good.\n\n3. The proposed IP-ControlNet to enhance the pixel-level consistency is also useful according to the ablation."}, "weaknesses": {"value": "I don't have big concerns of this paper, so the following weaknesses are mainly about the presentation of the paper.\n\n1. The presentation of the figures can be improved in my opinion. The authors could provide a teaser, so it would be more intuitive for the readers to know what the task of the paper is tackling. Also, I would think that Figure 1 (a) and Figure 2 (d), Figure 1(b) and FIgure 2 (c) should be paired together, instead of the current design, for a better consistency.\n\n2. I noticed that there are a lot of visual results and comparisons inside the supplementary materials. I think moving some of the results and a visual comparison with the baselines to the main paper would be really helpful and necessary. The authors can move some technical and implementation details in the main paper to the supplement instead.\n\n3. A minor part is that in Figure 3, the captions of qualitative and quantitative do not match the figure content."}, "questions": {"value": "Could the authors provide a brief and more intuitive explanation of the epsilon-equivariant optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BRaLyX90gt", "forum": "A9EP0RPrTL", "replyto": "A9EP0RPrTL", "signatures": ["ICLR.cc/2026/Conference/Submission4367/Reviewer_zBTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4367/Reviewer_zBTa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754271875, "cdate": 1761754271875, "tmdate": 1762917318851, "mdate": 1762917318851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReMix, a unified framework for consistent character generation and editing, aiming to address the limitations of existing methods—generation-based approaches struggle with fine-grained consistency, while editing-based ones lack posture flexibility. ReMix consists of two core components: the ReMix Module, which leverages MLLM (Qwen2.5-VL-7B-Instruct) for semantic editing and a Connector to adapt MLLM features to the frozen DiT backbone (FLUX.1dev) without retraining; and IP-ControlNet, which uses DVE (for info-rich cues) and SVE (for sparse cues) to ensure pixel-level consistency, plus an ϵ-equivariant optimization inspired by biological convergent evolution and quantum decoherence to promote feature alignment. The authors validate ReMix on human/subject-centric generation (outperforming SOTA like IP-Adapter and OmniControl on CLIP-I/DINO metrics) and image editing (Kontext-Bench1K), showing advantages in semantic coherence and identity preservation. The framework is designed to balance efficiency, semantic understanding, and pixel-level control, but focuses on specific datasets and tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents ReMix, a unified framework designed for character-consistent generation and editing. This framework incorporates semantic adaptation through the ReMix Module and pixel-scale regulation by IP-ControlNet, offering a novel perspective to realize high-fidelity image editing .\n\n2. The paper develops an ϵ-equivariant alignment strategy, which performs denoising on both reference and target images in a shared noise space. This process facilitates feature convergence and further achieves fine-grained consistency for characters .\n\n3. The proposed method features high efficiency: it supports image generation and editing without the need to retrain the DiT backbone, cutting down training expenses while maintaining the backbone’s inherent generation capability ."}, "weaknesses": {"value": "1. I cannot find examples on multiple references, which have been a common functionality in contemporary research. For example, Dreamo[1] could handle at least two-person reference. \n\n2. Lacking novelty. The paper seems like a combination of various existing methods. The MLLM+connector+DiT approach has been widely adopted in nowaday image editing and subject driven generation models. \n\n2. The paper aims at editing with one reference image, which is an almost solved problem. And the paper did not compare with the state-of-the-art image editing method, Qwen-Edit. \n\n3. The facial similarity of characters seem to have some obvious problems. For example, in Figure 4, the face of the first row is apparently shifted from the reference image."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7mX2PGUZgh", "forum": "A9EP0RPrTL", "replyto": "A9EP0RPrTL", "signatures": ["ICLR.cc/2026/Conference/Submission4367/Reviewer_nAX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4367/Reviewer_nAX7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813032886, "cdate": 1761813032886, "tmdate": 1762917318597, "mdate": 1762917318597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReMix, a unified framework for character-consistent image generation and editing, addressing the limitations of existing methods that often struggle with fine-grained identity consistency or spatial controllability. ReMix integrates a ReMix Module, which leverages MLLMs for semantic feature editing and instruction adaptation to a frozen DiT backbone, and an IP-ControlNet for pixel-level control. The latter introduces an $\\epsilon$-equivariant latent space and a shared-space denoising strategy, inspired by convergent evolution, to enforce feature alignment and maintain identity across diverse generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "ReMix offers a novel unified approach for generation and editing, avoiding costly DiT fine-tuning while preserving its native capabilities. The $\\epsilon$-equivariant latent space effectively promotes feature convergence, leading to better character consistency, as demonstrated by quantitative improvements in CLIP-I and DINO scores."}, "weaknesses": {"value": "1. Lack of clarity and detail in pipeline visualization and explanation: The overall pipeline, especially as depicted in Figure 1, appears overly complex and difficult to follow without explicit, detailed explanations in the caption or main text. Figure 2 further exacerbates this by lacking clear delineation between sub-components, making it challenging for readers to grasp the precise flow and interaction of modules within the ReMix framework. Also, the paper presents inconsistent definitions and states of modules—for instance, Figure 1 implies that the Redux module is frozen, while Figure 2(b) labels its internal MLP as learnable. Moreover, the relationship and distinction between the “ReMix Module” and the “Redux Module” remain unclear, leading to confusion about their respective roles and boundaries.\n\n2. Insufficient mathematical derivation and notation explanations: The transition from individual objectives (Equations 7 and 8) to the unified $\\mathcal{L}_{equ}​$ (Equation 9) is abrupt and lacks a clear mathematical derivation or justification for their integration, which is crucial for understanding the core optimization strategy. Additionally, several key mathematical notations, such as $y_t$ and $Z^{(t,\\epsilon)}_r$ are used without explicit definitions, hindering accessibility for readers unfamiliar with specific Diffusion Model conventions.\n\n3. High apparent heuristic and engineering overhead: The intricate pipeline design, comprising multiple distinct modules (ReMix Module, IP-ControlNet, Redux, MLLM, DVE, SVE) with different training stages and potentially specialized datasets, suggests a substantial amount of heuristic engineering and ad-hoc tuning. This complexity raises concerns about the generalizability, reproducibility, and practical deployment effort required for the proposed unified framework.\n\n4. The main paper's qualitative baseline comparison for image editing is notably insufficient, featuring only a single baseline across merely five scenes. Given that image editing is a central task for this method, such a limited comparison offers an incomplete assessment of its performance relative to existing state-of-the-art techniques. Furthermore, the supplementary material, while offering more examples, still predominantly compares against only one or two baselines for specific editing tasks. Such a narrow set of visual comparisons raises questions about the robustness of the method and its versatility across different editing challenges and other contemporary models."}, "questions": {"value": "please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7dGIRKkXBq", "forum": "A9EP0RPrTL", "replyto": "A9EP0RPrTL", "signatures": ["ICLR.cc/2026/Conference/Submission4367/Reviewer_Z55f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4367/Reviewer_Z55f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998731477, "cdate": 1761998731477, "tmdate": 1762917318326, "mdate": 1762917318326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their time, constructive feedback, and positive recognition of our contributions. We are encouraged that the reviewers found ReMix to be a novel and effective unified framework for generation and editing, highlighted the **benefit of avoiding DiT fine-tuning**, and recognized the ϵ-equivariant alignment strategy as a principled and impactful approach for **improving feature consistency**. We will integrate all constructive suggestions to further strengthen the final manuscript."}}, "id": "kvwaYWOVgY", "forum": "A9EP0RPrTL", "replyto": "A9EP0RPrTL", "signatures": ["ICLR.cc/2026/Conference/Submission4367/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4367/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission4367/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763514324805, "cdate": 1763514324805, "tmdate": 1763514324805, "mdate": 1763514324805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}