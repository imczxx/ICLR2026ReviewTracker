{"id": "0QPXvKE4SV", "number": 12, "cdate": 1756728080393, "mdate": 1759898278753, "content": {"title": "TCR-EML: Explainable Model Layers for TCR-pMHC Prediction", "abstract": "T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. “Explain-by-design” models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.", "tldr": "We propose  an approach to TCR-pMHC binding prediction, TCR-EML, that utilizes concept and prototype layers to provide accurate, detailed insights into the mechanisms of T cell response.", "keywords": ["T Cell", "TCR", "Transformers", "XAI", "Interpretability"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/375f047df05621d5eab2d0aeaca75d228a14f6fe.pdf", "supplementary_material": "/attachment/6d620dc959977bf2b0739218312766c3b2f70f47.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents TCR-EML, an \"explain-by-design\" module for TCR-pMHC binding prediction, which addresses the problem of standard transformer models being black boxes. The proposed layers are designed to be incorporated into existing, pre-trained protein language model backbones, enabling interpretability without requiring complete retraining. TCR-EML consists of a cross-attention-based Feature Enhancement and Fusion block and \"Contact Prototype\" layers that explicitly model amino acid residue contacts. This architecture directly links the model's prediction to known binding mechanisms, and it demonstrates competitive predictive accuracy on large-scale datasets while achieving high explainability scores on the quantitative TCR-XAI benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- By using \"Contact Prototype\" layers , the model's output is an explanation, rather than relying on post-hoc methods which can be unfaithful to the model's internal logic. The final prediction is a direct function of the predicted contact area scores.\n- The approach is designed as a lightweight prediction head that can be added to general-purpose PLMs like ESM-2, without needing retraining.\n- The model shows good results, outperforming existing baselines like MixTCRpred and TULIP in ROC-AUC analysis, especially on unseen epitopes."}, "weaknesses": {"value": "- The method is described as a general approach to addressing interpretability of transformers, but is only evaluated on a niche task of TCR-pMHC binding prediction.\n- For most of the training data, negative samples were generated by \"randomly shuffling TCR and pMHC pairs\". This method creates simple negatives that are not biologically realistic, which can artificially inflate predictive performance metrics.\n- While the explainability provided is an interesting proposition, the paper's case study shows imperfections. The authors note that TCR-EML correctly identifies one contact region but misses others or incorrectly assigns high weights to some residues on the peptide."}, "questions": {"value": "- Did the authors consider evaluating their model on other protein language modeling tasks?\n- The large training dataset is composed mostly of MHC-I data, with only a small fraction of samples corresponding to MHC-II. Did the author perform some class rebalancing during training to address this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "52L4o7tyd3", "forum": "0QPXvKE4SV", "replyto": "0QPXvKE4SV", "signatures": ["ICLR.cc/2026/Conference/Submission12/Reviewer_DJwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12/Reviewer_DJwy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971497179, "cdate": 1761971497179, "tmdate": 1762915436721, "mdate": 1762915436721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes adding “explainable model layers” downstream pretrained protein language models in order to enhance biological interpretability of the features extracted on the task of TCR-pMHC binding prediction, eventually improving performance. The data is made of triplets of CDR3a, CDR3b and peptide sequences. The explainable layers are a Feature Enhancement and Fusion block, which is supposed to learn cross correlations between alpha and beta chains and the peptide sequence through cross-attention, and contact prototype layers, which identify residues in contact by measuring similarity of their fused embeddings. The method is tested on an annotated dataset with various pretrained upstream models, showing increase in performance and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method works without the need of retraining the upstream protein language model, making it flexible. It includes information from both alpha and beta chains. It provides robust increase in performance w.r.t. state of the art methods for TCR-epitope binding predictions (Table 1, Fig. 3). The way contacts are inferred is reliable (Table 2, Fig. 4-6)."}, "weaknesses": {"value": "The way the FEF block works and the kind of correlations it captures are not very clearly explained. For other questions, see below."}, "questions": {"value": "- Can the authors explain more in detail the principles behind fused embeddings?\n- The authors train the model with a 4:1 imbalance ratio between negative a positive pairs, and then class-reweight the loss to account for this ratio. Is there a specific reason for this design choice, given the amount of negative data to include in the training set can be decided? \n- How do different negative designs (true negative pairs vs shuffled pairs) impact predictions? See for example [1]\n- Is the method flexible if some of the data modalities (e.g., the alpha chain) are missing?\n\n[1] Ursu et al. Training data composition determines machine learning generalization and biological rule discovery, Nat. Mach. Int. 7, 1206–1219 (2025)\n \nMinor:\n\nLine 417, “.5To” → “To”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eR6r6Wii8k", "forum": "0QPXvKE4SV", "replyto": "0QPXvKE4SV", "signatures": ["ICLR.cc/2026/Conference/Submission12/Reviewer_weB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12/Reviewer_weB2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984541619, "cdate": 1761984541619, "tmdate": 1762915436567, "mdate": 1762915436567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a head for better understanding the reasons why the PLM will do the TCR-pMHC relevant prediction.\nThe head design is based on the prototype learning, and the experimental results also suggest the effectiveness of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation to make the PLM explainable is meaningful. And exploring the T-cell mechanism is also important for life science."}, "weaknesses": {"value": "- The writing and presentation are inferior.\n- MHC appearing in the abstract should be written with the full name,\n- A lot of terms that have nothing to do with learning representation appear in the introduction without explanation (e.g., MHCI, MHCII, CD8+ T cells, CD4+ T, PLM). This makes the paper hard to understand.\n- A lot of important terms appear for the first time, but are not explained (e.g. CDRα and CDRβ, CDR3a, CDR3b).\n- Appending a prototype layer to make the PLM explainable seems overclaim. The only explanation result is the contact score (distance), which is weak and cannot provide important insight. TSNE for the PLM embedding may get similar results. Therefore, the contribution of this paper is limited."}, "questions": {"value": "- For the second paragraph of the introduction, it would be better to provide a figure to illustrate the research objects/concepts and the research question.\n- The presentation and writing need to be improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "44mFDlifff", "forum": "0QPXvKE4SV", "replyto": "0QPXvKE4SV", "signatures": ["ICLR.cc/2026/Conference/Submission12/Reviewer_E7iy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12/Reviewer_E7iy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989411690, "cdate": 1761989411690, "tmdate": 1762915436385, "mdate": 1762915436385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Interpretability of large, black-box machine learning models is an important and timely issue. The authors plug \"explain-by-design\" layers on top of protein sequence representations extracted by pretrained language model embeddings such as ESM. The paper is well-written and computational experiments are clearly presented, but the actual gain in explainability could be justified more."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper motivates interpretability in TCR–pMHC prediction well.\n- Architecture (“TCR-EML”) is straightforward: plug explainable prototype layers on top of pretrained protein language model embeddings.\n- Demonstrates strong empirical results on public datasets and the TCR-XAI benchmark.\n- Quantitative metrics (ROC-AUC, BRHR) are presented clearly; figures are decent.\n- The paper includes reproducibility and LLM-usage statements, as well as source code."}, "weaknesses": {"value": "- The proposed “explainable model layers” are a minor architectural variant, essentially combining cross-attention and similarity-based prototype aggregation. There is little theoretical or methodological depth beyond known ideas in prototype learning and cross-attention.\n- Comparisons are limited: only MixTCRpred and TULIP are used as baselines, both relatively new and not necessarily the strongest available. No ablation on the design choices (e.g., thresholds, temperature).\n- The use of PLMs is plug-and-play; there is no fine-tuning or adaptation beyond feature extraction, so it’s unclear how much of the performance comes from the PLMs themselves and how much from the plugged explainable layer.\n- Explainability evaluation via BRHR (Binding Region Hit Rate) is shallow: the metric correlates with contact proximity but does not assess causal or mechanistic interpretability.\n- Besides contact prediction, the authors don't present additional justification for actual \"explainability\". Contact prediction is already seen in the self-attention matrices of protein LLMs.\n- Some results are implausibly high (ROC-AUC = 0.999 for ProteinBERT, Table 1 p. 6), suggesting possible data leakage or inadequate negative sampling. Please check."}, "questions": {"value": "- In page 8, line 418, what is \".5To\" ?\n- How was the “case study” (Figure 4 p. 8) selected? Is it a random choice or was it cherry-picked?\n- Besides contact prediction, the authors could present additional justification for \"explainability\". Are there other explainable factors aiding the predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TyWpqGGKSN", "forum": "0QPXvKE4SV", "replyto": "0QPXvKE4SV", "signatures": ["ICLR.cc/2026/Conference/Submission12/Reviewer_uG19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12/Reviewer_uG19"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014842931, "cdate": 1762014842931, "tmdate": 1762915436238, "mdate": 1762915436238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed TCR-EML, with two explainable model layers that can be integrated into protein language model architectures for TCR–pMHC binding prediction. The method incorporates prototype layers representing amino acid contact patterns derived from biochemical binding mechanisms. Experiments on large-scale datasets demonstrate improved predictive performance andmodel explainability compared with prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Introduces TCR-EML, explainable model layers for TCR–pMHC binding prediction with Plug-and-play compatibility with pre-trained protein language model (PLM) backbones, requiring no additional fine-tuning or retraining.\n\nProvides biologically meaningful explanations through analysis of contact prototype patterns.\n\nValidated with a case study on an MHC-II TCR–pMHC complex and TCR-XAI benchmark evaluation, showing alignment with experimental structural data."}, "weaknesses": {"value": "(1) The proposed method introduces two additional components — a sequence feature layer and a contact strength calibration layer — to the base TCR–pMHC prediction model. However, this design appears somewhat ad hoc and shows limited conceptual novelty. The first layer resembles the EGM approach, while the second shares similarities with PISTE (the M*A part, in particular).\n\n(2) In the evaluation, I think there should more baselines included from the literature of TCR–pMHC binding prediction; \n\n(3) For TCR–pMHC binding prediction with imbalanced data, AUROC can be misleading because the majority non-binding class dominates. Better metrics focus on the minority positive class and include precision, recall, and F1-score, and top-k accuracy; area under the precision-recall curve (AUPRC) is also informative for rare binders; and Matthews correlation coefficient (MCC) or balanced accuracy, which account for class imbalance."}, "questions": {"value": "(1) This paper appears to focus primarily on enhancing the performance of protein language models (PLMs) for TCR–pMHC binding prediction. Regarding interpretability, it would be helpful to clarify what additional insights the proposed method provides beyond existing approaches that visualize contact probability maps. Specifically, can the model yield biologically meaningful interpretations or mechanistic insights that are valuable for advancing immunological research?\n\n(2) I am wondering why you do not need to re-train the backbone model, since the two new layers added will revise the loss function.\n\n(3) The authors state that current algorithms for TCR–pMHC binding prediction are typically black-box transformer models that lack interpretability and do not provide clear rationales for their predictions, and that that PISTE provided limied explanability,  and requires additional steps to extract explanations. This statement is not entirely accurate. For example, the PISTE model (Feng et al.) can accurately predict both binding status and residue-level contact maps without relying on any 3D structural data during training. \n\n(4) It's worthwhile to note that, if you used embedding from PLMs in the context of two or more sequence interesting with each other, their positional encoding is in fact physically not quite meaningful because the relative positions in such a interesting system is not known and cannot simply be encoded by existing positional encoding schemes like sine/cosine (adding positions with embeddings leads to noice attention scores) or ROPE, nor even learnable positions (which do not behave like real positions in 3D)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DoBT9nF14N", "forum": "0QPXvKE4SV", "replyto": "0QPXvKE4SV", "signatures": ["ICLR.cc/2026/Conference/Submission12/Reviewer_nxmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12/Reviewer_nxmS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762853361575, "cdate": 1762853361575, "tmdate": 1762915436072, "mdate": 1762915436072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}