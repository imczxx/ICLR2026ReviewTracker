{"id": "hB8r4cdFTh", "number": 1827, "cdate": 1756947198321, "mdate": 1763416259091, "content": {"title": "Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning", "abstract": "Virtually all federated learning (FL) methods, including FedAvg, operate in the following manner: i) an orchestrating server sends the current model parameters to a cohort of clients selected via certain rule, ii) these clients then independently perform a local training procedure (e.g., via SGD or Adam) using their own training data, and iii) the resulting models are shipped to the server for aggregation. This process is repeated until a model of suitable quality is found. A notable feature of these methods is that each cohort is involved in a single communication round with the server only. In this work we challenge this algorithmic design primitive and investigate whether it is possible to ``squeeze more juice\" out of each cohort than what is possible in a single communication round. Surprisingly, we find that this is indeed the case, and our approach leads to up to 74% reduction in the total communication cost needed to train a FL model in the cross-device setting. Our method is based on a novel variant of the stochastic proximal point method (SPPM-AS) which supports a large collection of client sampling procedures some of which lead to further gains when compared to classical client selection approaches.", "tldr": "Contrary to current practice of federated learning, we show that it's better for a cohort to be involved in more than a single communication round.", "keywords": ["stochastic proximal point methods", "federated learning", "cross-device setting", "arbitrary sampling"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/925afbff9de85e776d8fd4fccf00462a6de93052.pdf", "supplementary_material": "/attachment/88be4f29603d28a58ccab10bb71bd36cfe962e3c.zip"}, "replies": [{"content": {"summary": {"value": "Most FL methods (e.g., FedAvg) use a single communication round per client cohort: the server sends the model, clients train locally, and updates are aggregated. This work challenges that design, showing that allowing multiple interactions per cohort can significantly improve efficiency. Using a new stochastic proximal point variant (SPPM-AS) that supports diverse client sampling strategies, the approach achieves up to 74% reduction in total communication cost in cross-device federated learning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The interaction of three main components (proximal methods, same cohort involved in multiple local rounds, sampling strategy) seems promising.\n\n* The authors provide an extensive theoretical framework to support their work."}, "weaknesses": {"value": "* Readability and Structure.\nThe paper is difficult to follow due to poor organization. It lacks a dedicated background and related work section, and the method section does not provide a high-level overview or intuitive explanation of the proposed approach. As a result, the reader struggles to grasp the motivation and flow of ideas.\n\n* Clarity of Contributions.\nThe technical contributions are not clearly articulated. The paper seems to combine two main ideas, (i) the introduction of SPPM variants, and (ii) the use of multiple local communication rounds per selected cohort to reduce overall communication, but their relationship is not clearly explained. It remains unclear how these two elements interact or reinforce each other.\n\n* Novelty and Positioning.\nThe novelty related to the proximal point optimization framework is not well established. The authors should better distinguish what is new in their method versus what is already known from existing proximal point or stochastic optimization literature.\n\n* Practical Feasibility.\nWhile the empirical observation that reusing the same client cohort across multiple local rounds reduces communication is interesting, its practical applicability is questionable in realistic cross-device FL settings, where maintaining the same cohort across rounds is rarely guaranteed. The paper should discuss this limitation or provide strategies to address it."}, "questions": {"value": "1. Clarity and Structure.\nOne of the main problems of the paper is its clarity.\n(a) A dedicated background section is needed.\n(b) The paper assumes readers are already familiar with proximal point methods. A concise background explaining their principles and how they are used in centralized and distributed machine learning would greatly improve accessibility.\n(c) The method section lacks a high-level or intuitive explanation. Please include a short intuitive summary (one or two sentences) of how the proposed method works, and consider adding an overview figure or a detailed algorithmic diagram illustrating the method in a federated setting, with more context than the current Algorithm 1.\n\n2. Novelty of the Contribution.\nWhat is the novel contribution of the work with respect to existing proximal point methods? Please clarify which aspects of your approach are original and how they differ from known stochastic or distributed proximal point variants.\n\n3. Related Work.\nThe paper currently lacks a related work section. Please discuss prior work on proximal point methods and communication-efficient federated learning, and explicitly position your contribution relative to these studies.\n\n4. Practical Feasibility of Cohort Reuse.\nThe paper claims that increasing the number of local communication rounds within the selected cohort reduces overall communication. While this may hold empirically, how can one guarantee the availability of the same cohort of clients in realistic cross-device environments? Please clarify the assumptions or mechanisms that make this feasible in practice.\n\n5. Experimental Details in the Introduction.\nIn the Introduction, the experimental setting for the results presented in Figure 1 is missing. Even a minimal description (model, dataset, data distribution, number of clients) would help the reader interpret the figure. Please also provide a reference to where full experimental details appear later in the paper.\n\n6. Readability of Figure 1.\nFigure 1 is not straightforward to interpret. Please consider redesigning it to make the message clearer or move it to a later section where more context is available.\n\n7. Experimental Scope and Validation.\nAlthough the Introduction mentions experiments on non-convex neural networks in cross-device settings, two concerns arise:\n(i) The neural network used appears to be a simple CNN.\n(ii) The scale of the experiment involves only 100 clients.\nPlease consider validating the results on more complex neural networks, larger and more realistic datasets, and with different numbers of clients to strengthen the empirical claims.\n\n8. Reproducibility.\nThe reproducibility of the experiments is limited. Please improve the shared code and documentation to allow straightforward replication of the reported results.\n\n9. Notation Clarification.\nThe prox expression used at the end of Assumption 2.3 and in Algorithm 1 is never introduced. Please define it clearly and explain its role in the proposed method.\n\n10. Communication Cost Reduction.\nThe abstract reports a 74 percent reduction in total communication. Does this result refer to the hierarchical federated learning setting? If so, please clarify this in the text, since the result may be specific to that configuration and less general than in standard FedAvg settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9njGXmEMRG", "forum": "hB8r4cdFTh", "replyto": "hB8r4cdFTh", "signatures": ["ICLR.cc/2026/Conference/Submission1827/Reviewer_YQfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1827/Reviewer_YQfH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290311780, "cdate": 1761290311780, "tmdate": 1762915901902, "mdate": 1762915901902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Cohort Squeeze,\" a framework for cross-device federated learning designed to enhance communication efficiency. The proposed method, SPPM-AS, allows a selected cohort of clients to perform multiple local communication rounds before synchronizing with the central server. The authors provide a theoretical convergence analysis for strongly convex problems and empirically demonstrate the method's effectiveness in reducing total communication rounds on both convex and non-convex tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n1.The paper provides a comprehensive investigation and comparison of three different client sampling strategies (NICE, Block, and Stratified Sampling), offering valuable insights.\n2.The empirical evaluation is thorough, covering both convex and non-convex settings across multiple datasets, which demonstrates the practical applicability of the approach.\n3.The experimental results are highly promising, showing a potential reduction in communication costs by up to 74% compared to baseline methods."}, "weaknesses": {"value": "Weaknesses:\n1.The novelty of the approach is not clearly distinguished from Hierarchical Federated Learning (HFL). The paper lacks a necessary discussion and comparison with the existing HFL literature, making the contribution's positioning unclear.\n2.The description of the algorithm is ambiguous. It is not clear how the \"K local communication rounds\" are actually implemented, i.e., whether clients within a cohort communicate with each other to solve the proximal operator, and if so, how.\n3.The communication cost model (defined as TK) is oversimplified and potentially unfair. It assumes that the cost of a global communication round is identical to that of a local, intra-cohort communication round, which leads to a questionable comparison with baselines like FedAvg.\n4.The theoretical convergence guarantee is provided only for the strongly convex case. This leaves a significant gap, as many of the successful experiments are conducted on non-convex neural networks."}, "questions": {"value": "Questions: \n1.Could you elaborate on the relationship between your proposed framework and Hierarchical Federated Learning (HFL)? What are the key distinctions and novel contributions of your method when compared to existing hierarchical approaches?\n2.Could you provide a more detailed description of the intra-cohort process during the K local communication rounds? Specifically, do clients communicate with each other to solve the proximal operator, and if so, what is the protocol for this communication and aggregation?\n3.The total communication cost is defined as TK, which assumes local and global rounds have the same cost. Could you justify this modeling choice? How might the comparative results change with a more nuanced cost model that distinguishes between the two (e.g., Cost = T × C_global + T × K × C_local)?\n4.Given the strong empirical results in non-convex settings, could you provide any theoretical insights or discussion on the convergence properties of SPPM-AS for non-convex objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aSM0LHccsm", "forum": "hB8r4cdFTh", "replyto": "hB8r4cdFTh", "signatures": ["ICLR.cc/2026/Conference/Submission1827/Reviewer_dpCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1827/Reviewer_dpCJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297325440, "cdate": 1761297325440, "tmdate": 1762915901777, "mdate": 1762915901777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPPM-AS, a stochastic proximal point variant that supports different sampling strategies. When applied to federated learning, the method naturally supports various client-sampling protocols. The authors prove convergence to an $\\epsilon$-approximate solution under strong convexity assumptions and validate the approach through experiments demonstrating its performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1.** The paper proposes a new federated learning algorithm designed to improve communication efficiency by allowing flexible client-sampling strategies.\n\n**2.** The work is supported by both theoretical analysis and empirical evaluations, demonstrating the effectiveness of SPPM-AS in several settings."}, "weaknesses": {"value": "**1. Motivation vs. focus mismatch.** The abstract emphasizes *cohort squeeze* and introduces the work as addressing a new communication bottleneck. However, most of the paper instead centers on client-sampling strategies within standard federated learning communication rounds. Although Sections 3.3 and 3.6 present communication-related experiments, the paper does not provide a clear and rigorous communication-cost analysis of SPPM-AS, which would be expected given the motivation presented in the abstract and title.\n\n**2. Practicality of iteration-complexity result.** The convergence guarantee relies on selecting a specific stepsize $\\gamma$ that depends on $\\sigma_{\\star,AS}$, which itself is defined in terms of $x_\\star$. Thus, the theoretically optimal stepsize depends on unknown quantities and cannot be directly used in practice, meaning the provable accuracy bound does not strictly apply to the implementable algorithm.\n\n**3. Strong convexity assumption.** The theoretical analysis requires strong convexity (Assumption 2.2). While this leads to cleaner guarantees, it limits applicability to real federated learning scenarios, where models are typically non-convex (e.g., deep networks). The gap between theory and practice is acknowledged but remains significant.\n\n**4. Presentation and clarity.** The paper would benefit from a clearer and more consistent connection between the communication problem highlighted in the introduction/abstract and the theoretical development in the main body.\nAlgorithm 1 (SPPM-AS) and Table 1 could be explained more thoroughly in the main text to improve readability.\n\n**Minor issues.**\n\nLine 123: $n = \\tilde{b} 1,2,\\cdots,n$ should likely be corrected to $[n] = \\lbrace1,2,\\cdots,n \\rbrace$. \n\nLine 161: stray [] bracket."}, "questions": {"value": "Can your analysis be extended to non-convex objectives? If not, what part of the proof critically depends on strong convexity? Understanding this would help clarify whether the framework could be adapted beyond the strongly convex setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g1p0wreyc7", "forum": "hB8r4cdFTh", "replyto": "hB8r4cdFTh", "signatures": ["ICLR.cc/2026/Conference/Submission1827/Reviewer_WB7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1827/Reviewer_WB7P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762380566350, "cdate": 1762380566350, "tmdate": 1762915901525, "mdate": 1762915901525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Changes"}, "comment": {"value": "We thank all reviewers for their constructive feedback. Below we summarize the main revisions, with locations in the revised manuscript (main paper and appendix).\n\n**Major changes**\n\n- **Communication-centric framing.** Added a short *communication-cost paragraph* after Theorem 2.4 in Sec. 2 defining $C_{flat}(\\varepsilon,K)=KT(\\varepsilon)$ and $C_{hier}(\\varepsilon,K)=(c_1K+c_2)T(\\varepsilon)$, and explicitly linking the theory to the communication metrics used in Figs. 1–4 and appendix figures. *(response to WB7P pt.1; dpCJ pt.3)*\n\n- **Background and algorithm overview.** Introduced a new subsection *Background: Proximal Point Methods in FL* (Sec. 2.1) and a brief high-level description of SPPM-AS at the start of Sec. 2, explaining how cohort reuse implements an inexact stochastic prox step. *(response to YQfH pts.1–2; WB7P pt.4)*\n\n- **Analytic communication–vs–$K$ tradeoff for inexact prox.** Introduced Proposition *Communication cost under inexact prox* (after Thm. 2.4 in Sec. 2 and in App. G.10), showing that if the $K$-step local prox solver has error $O(\\rho^K)$, then the total cost $C_{\\mathrm{tot}}(\\varepsilon;K)$ has a finite optimal $K^\\star$. This explains the U-shaped curves and larger gains in hierarchical FL. *(response to WB7P pt.1; dpCJ pt.3)*\n\n- **Clarified $K$ intra-cohort rounds / protocol.** Expanded the text around Algorithm 1 and Sec. 3.3 to spell out how, in each global round, a sampled cohort runs $K$ intra-cohort synchronizations via a cohort aggregator to approximate $prox_{\\gamma f_{C_t}}(x_t)$; added more detailed pseudo-code in App. D.4. *(response to dpCJ pt.2; YQfH pt.2)*\n\n- **Non-convex / PL / QG extensions.** In App. A (Discussion) and App. G.11 (new lemmas and a proposition) we now (i) state explicitly where strong convexity is used, and (ii) outline extensions under Polyak–Łojasiewicz / quadratic‑growth and weakly‑convex assumptions, including a “QG implies contractive prox” lemma and a non‑convex SPPM‑AS stationarity bound, and relate these to the CNN/FEMNIST experiments in Sec. 3.7. *(response to WB7P pt.3 and question; dpCJ pt.4)*\n\n- **Positioning vs. SPPM and Hierarchical FL.** Expanded Appendix B (Related Work), especially App. B.4, to contrast Cohort Squeeze with hierarchical FL methods and existing stochastic proximal point algorithms, and clarified the contribution bullets and roadmap at the end of the Introduction (Sec. 1). *(response to dpCJ pt.1; YQfH pt.3)*\n\n- **Experiment description and metrics.** Clarified the setup for Fig. 1 in the Introduction (datasets, models, cohort size, solver, $K$, target accuracy) with a pointer to Sec. 3.3 / App. E.1, and emphasized that flat experiments report total cost as $TK$ while hierarchical experiments use $(c_1K + c_2)T$ in Secs. 3.3 and 3.6 and their figure captions. Shortened and focused the Conclusion (Sec. 4) to summarize the main communication-reduction finding. *(response to WB7P pt.1; dpCJ pt.3; YQfH pts.5–6)*"}}, "id": "loRGT3QV06", "forum": "hB8r4cdFTh", "replyto": "hB8r4cdFTh", "signatures": ["ICLR.cc/2026/Conference/Submission1827/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1827/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission1827/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763414826759, "cdate": 1763414826759, "tmdate": 1763415514734, "mdate": 1763415514734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}