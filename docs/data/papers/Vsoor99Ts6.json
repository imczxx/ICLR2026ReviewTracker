{"id": "Vsoor99Ts6", "number": 21899, "cdate": 1758323348107, "mdate": 1759896897790, "content": {"title": "Context is All You Need", "abstract": "Artificial Neural Networks (ANNs) are increasingly deployed across diverse domains, often requiring them to generalize beyond their training conditions. This shift in context frequently leads to performance degradation, a central challenge in Domain Generalization (DG). While numerous techniques exist to mitigate this issue (e.g., fine-tuning, activation steering, meta-learning, adversarial training, normalization-based approaches, and parameter-efficient methods such as prompt tuning), they are often complex, resource-intensive, and difficult to scale; particularly for large models like Large Language Models (LLMs). In contrast, we introduce CONTXT (\\emph{\\textbf{C}ontextual augmentati\\textbf{O}n for \\textbf{N}eural fea\\textbf{T}ure \\textbf{X} \\textbf{T}ransforms}): a simple, intuitive, and elegant method for contextual adaptation. CONTXT work by augmenting the model’s internal representations with lightweight, contextually relevant feature indexes through straightforward multiplicative and additive vector operations. Despite its simplicity, CONTXT significantly improves performance across both discriminative (e.g., classification with ANNs/CNNs) and generative (e.g., LLMs) tasks. With minimal computational overhead and straight forward integration, CONTXT layers offer a practical and effective solution to DG and a variety of problems facing ANNs, demonstrating that strong results need not come at the cost of complexity. More generally, CONTXT provides a compact mechanism to manipulate information flow and steer ANN processing in a desired direction without retraining the network.", "tldr": "We introduce CONTXT, a simple and intuitive way to augment contextual information in feature representations that can improve classifier performance and steer LLM outputs without retraining.", "keywords": ["domain generalization", "Large Language Model", "LLM", "activation steering", "activation engineering", "generative", "classification", "out of distribution", "OOD", "style transfer"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c826a66292c5a0d3d0e9d6a15ad6c5b1856dd69.pdf", "supplementary_material": "/attachment/dc5ed91368625d58b54fb9c5cd5e1aa3c20c7500.zip"}, "replies": [{"content": {"summary": {"value": "In this manuscript, the authors aim to address the challenge of Domain Generalization (DG) in Artificial Neural Networks (ANNs), where models often suffer from performance degradation when deployed in contexts different from their training conditions. Existing solutions for DG (e.g., fine-tuning, adversarial training, activation steering) are typically complex, resource-intensive, and hard to scale. CONTXT operates through simple vector arithmetic: first, it computes a \"context index\" as the difference between a precomputed context vector and the current feature representation of the input at a chosen layer. Then, it adjusts the input features by adding a scaled version of this index. It also supports multi-context adjustment by linearly combining multiple context indices. Experimental results demonstrate the effectiveness of CONTXT across both discriminative and generative tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed CONTXT is a lightweight, brain-inspired technique that modifies intermediate network features to inject or remove contextual information without retraining.\nCONTXT provides a simple, interpretable, and efficient solution to DG, avoiding the complexity and resource costs of retraining or fine-tuning while delivering substantial performance gains."}, "weaknesses": {"value": "There are some concerns for the manuscript as follows:\n\n1.The section METHODS is too brief. Some contents, e.g., the first paragraph in section RESULTS, can be omitted to extend section METHODS.\n2.Thus, in the context vector computation, when should we select the feature of a representative sample or the mean feature over samples exhibiting? Since this selection may has great influence to the experimental results, what is the selection standard?\n3.The image classification is introduced as the downstream task for the proposed method, so if the proposed method can be used in other tasks, e.g., text classification, how to calculate contextual references? And what will be the experimental results?"}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "sTaIxVwEvl", "forum": "Vsoor99Ts6", "replyto": "Vsoor99Ts6", "signatures": ["ICLR.cc/2026/Conference/Submission21899/Reviewer_Q98n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21899/Reviewer_Q98n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448447541, "cdate": 1761448447541, "tmdate": 1762941973670, "mdate": 1762941973670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple yet effective approach for domain generalization in Artificial Neural Networks. CONTXT modifies the intermediate features by linearly augmenting them with relevant contextual information, enabling better OOD performance without any auxiliary networks or retraining requirements. The method is evaluated on discriminative tasks as well as generative tasks and performs well, as reported by the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a simple yet powerful approach for domain generalization in Artificial Neural Networks. \n\n2. The proposed method is applied to both classification and generative tasks, which shows that the method can be applied to most of the existing ANNs.\n\n3. The connection of work with the brain seems interesting and motivating."}, "weaknesses": {"value": "1. The paper lacks a much deeper analysis of why their method actually works. For instance, some theoretical backing (as the method is quite straightforward, it will be easy to develop a nice theory on top of it), attention visualization, will make the work really strong.\n\n2. The experimental analysis of the work is not sufficient; for instance, the t-SNE plots could tell the exact difference between how good the method is performing. Plotting them for the model using the proposed method and supervised fine-tuning can further clarify and solidify the findings of the paper.\n\n3. The paper requires some rewriting as well. For instance, in the results section, the paper abruptly starts introducing the method again in line 180. An additional separate section on the theoretical analysis of the proposed method is highly suggested.\n\nOverall, in its current state, I believe that the work is not sufficient for the main conference; however, the work still has a promising direction, and when extended with some theoretical analysis and has a clear intuition of why the method works can make it a really useful and important finding."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "idA2PCHl58", "forum": "Vsoor99Ts6", "replyto": "Vsoor99Ts6", "signatures": ["ICLR.cc/2026/Conference/Submission21899/Reviewer_eRH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21899/Reviewer_eRH5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925206076, "cdate": 1761925206076, "tmdate": 1762941973240, "mdate": 1762941973240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CONTXT, a method of activation steering that helps neural networks perform better in new domains. CONTXT first computes a context vector, then uses this context vector at inference time (by simple vector operations such as addition or subtraction) to modify the model's internal activations. The paper experiements on OOD image classification with VGG and text generation with LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Simplicity: The method does not require re-training the model, but rather, steering activations at inference. This is a major selling point since model re-training/finetuning can be cumbersome and expensive.\n* The core idea of the paper is presented well, and Figure 1 (while not exactly visually appealing) does a good job of highlighting the method clearly."}, "weaknesses": {"value": "* The fundamental idea of activation steering is not entirely new. This concept has been applied in various different domains, including text generation and even other image generative models. The paper claims the novelty on the specific formulation and the application of their method to OOD classification. However, I am not convinced that CONTXT provides a fundamentally new concept, but rather an application of an existing concept.\n* While the paper claims that there is little hyperparameter tuning, it still needs to be tuned per-layer, which is not trivial. The authors suggest that this could be learned in the validation setting. Furthermore, the claim that CONTXT requires \"minimal computational overhead\" or \"minimal latency\" is not in good faith, since computing context vectors themselves could be expensive depending on the setting. This leads me to my next point:\n* CONTXT is motivated by Domain Generalization (DG) and also claims to experiment on DG settings; however, the experimental settings do not align with the core issues in DG. In DG, we assume that the target domain is not available at any time. Thus, since CONTXT requires data from a target domain to create the context vector, it cannot be classified as tacking the DG problem. It is closer to domain adaptation or test-time adaptation. \n  * To add to the point above, the motivating example in Figure 2 is a bit misleading because it instills the idea that the \"correct\" context is known for the image. However, the correct context here depends entirely on the classification of the image (\"cow on a beach\"). So essentially, we need to have already classified the image to know which context to remove or add."}, "questions": {"value": "I would appreciate authors' response to the weaknesses addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0HzE3SVw4M", "forum": "Vsoor99Ts6", "replyto": "Vsoor99Ts6", "signatures": ["ICLR.cc/2026/Conference/Submission21899/Reviewer_1oEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21899/Reviewer_1oEf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929251106, "cdate": 1761929251106, "tmdate": 1762941972991, "mdate": 1762941972991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CONTXT, a lightweight “context steering” mechanism that edits intermediate activations by adding a scaled difference between a cached “context vector” and the current hidden state. The method aims to (a) improve domain generalization for vision classifiers and (b) steer LLM generations toward a desired attribute/persona without fine-tuning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. Domain generalization using just a vector operation is a very solid research direction.\n\nS2. The paper is generally clear apart from figures (refer weakness)"}, "weaknesses": {"value": "W1. \"Brain-inspired\" motivation is confusing: The biological framing (PFC top-down control) is used to justify adding a linear direction to features, but no concrete architectural, algorithmic, or intuitive notions are provided. I don't really understand how this is related to brain in any way.\n\nW2. The construction of \"context vectors\" is largely a rehash of a very relevant line of literature on prototype-based/centroid-based learning, like [1] which is not discussed at all. Context vectors are similar to mean feature vectors which act as references and moves points toward/away from them via linear vectors - a clear similarity to using the similarity as the distance metric.\n\nW3. \"Context\" needs to be better defined: The definition is changed across sections, making it hard to formalize what a valid context vector is. For language, it is defined as \"The injected (in-domain) context vector comprised of the average feature representation across all training domain samples,\" - which does not make sense to me, is the entire training corpus a context? If this is true then why not do the same for images? Additionally, the paper gives formulas for single/multiple contexts, but not a rigorous selection/validation protocol for constructing them in general.\n\nW4. The quality of figures is very poor, with no error bars, statistical tests, etc. reported.\n\n[1] This Looks Like That: Deep Learning for Interpretable Image Recognition, NeurIPS 2019"}, "questions": {"value": "1. Can the authors explain in detail why their method is derived from brain functioning?\n\n2. Were any other similar methods explored?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AgXKJINRCn", "forum": "Vsoor99Ts6", "replyto": "Vsoor99Ts6", "signatures": ["ICLR.cc/2026/Conference/Submission21899/Reviewer_8VGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21899/Reviewer_8VGh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975383656, "cdate": 1761975383656, "tmdate": 1762941972710, "mdate": 1762941972710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}