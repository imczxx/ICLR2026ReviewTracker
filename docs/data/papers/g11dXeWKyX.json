{"id": "g11dXeWKyX", "number": 6080, "cdate": 1757952356384, "mdate": 1759897936260, "content": {"title": "InsertAny3D: VLM-Assisted and Geometry-Grounded Framework for 3D Object Insertion in Complex 3D Scenes", "abstract": "The insertion of 3D objects into complex scenes is a critical task in 3D asset editing. Previous works use 2D inpainting models to edit multi-view images and lift them into 3D, which suffers from manual intervention and multi-view inconsistencies. To address these issues, we propose InsertAny3D, a novel framework for high-quality 3D object insertion guided by ambiguous natural language instructions in complex scenes. Our framework consists of two key components: (1) VLM-Assisted 3D Scene Understanding, which decomposes abstract user intents and selects optimal insertion regions through a hierarchical vision-language reasoning strategy; and (2) Geometry-Grounded 3D Object Insertion, which performs anchor-constrained 3D object generation and placement using depth-based feature matching and multi-view geometric verification to ensure spatial coherence. Extensive experiments demonstrate that InsertAny3D significantly outperforms existing methods in insertion precision, visual quality, and interactive usability.", "tldr": "", "keywords": ["3D Editing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/072c1fa640ede2bc9dbcef9691c8df5125975085.pdf", "supplementary_material": "/attachment/d9b42043b7d70bff126ba4986328a81730cd260b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces InsertAny3D, a framework designed for 3D object insertion into 3D scenes, guided by natural language instructions. InsertAny3D proposes a two-stage, modular pipeline: VLM-Assisted 3D Scene Understanding and Geometry-Grounded 3D Object Insertion. The VLM decomposes ambiguous user commands into a series of concrete, executable subtasks. It then introduces an \"anchor-constrained\" synthesis technique, where the new object is co-generated with its interacting scene object to ensure contextual and geometric correctness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Leveraging VLMs for holistic understanding and planning is a well-suited approach for this task.\n2. To ensure that the generated 3D objects are harmoniously placed within a scene, the paper takes contextual information into account and proposes the Anchor-Constrained 3D Asset Synthesis method.\n3. The introduction of anchor objects helps to preserve the integrity of the generated assets to a certain extent."}, "weaknesses": {"value": "1. The paper claims that previous work heavily relies on 2D models. However, many key steps in the proposed method are also carried out using 2D techniques—for example, feeding scene information into VLMs, extracting semantic features, harmoniously inserting objects into scenes via 2D editing, and using LangSAM for segmentation. As a result, most operations are still performed in 2D, and the task is not truly addressed from a 3D perspective.\n2. As noted above, since the method relies on a series of 2D techniques to accomplish the task, there is no significant breakthrough or particularly inspiring solution from a technical contribution standpoint. In terms of performance, the heavy dependence on 2D pretrained models to address 3D problems may lead to error accumulation that could affect the results. The paper lacks analysis of these potential limitations."}, "questions": {"value": "Nil"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0QtT6Yw8xb", "forum": "g11dXeWKyX", "replyto": "g11dXeWKyX", "signatures": ["ICLR.cc/2026/Conference/Submission6080/Reviewer_FRUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6080/Reviewer_FRUV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931376162, "cdate": 1761931376162, "tmdate": 1762918451887, "mdate": 1762918451887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the task of inserting 3D objects into existing complex 3D scenes, guided by language instructions. This work point out that prior methods rely on inpainting multi-view 2D images, then lifting to 3D, which suffers from manual intervention and inconsistent results across views.\nThere are two major components:\n- VLM-Assisted 3D Scene Understanding: uses a VLM to interpret the user’s natural language intent, decompose it into insertion region selection, and decide optimal placement region within the 3D scene via hierarchical reasoning. \n- Geometry-Grounded 3D Object Insertion: performs anchor-constrained 3D object generation/placement. It uses depth-based feature matching and multi-view geometric verification to ensure that the newly inserted object aligns spatially, matches geometry and appearance cues, and maintains coherence across views."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Incorporating VLMs to parse natural language instructions for object insertion is a strong design, it supports more intuitive user workflows\n- The visualization results seem to be much better than previous works.\n- The collaboration of different modules and models is reasonable and works well.\n- The provided time consumption on a single 3090 is great."}, "weaknesses": {"value": "- The quantitative results are limited. The metrics are mainly preference scores from VLMs and user study.\n- Since this work is using too many off-the-shelf models, the contribution might be limited because of that.\n- It is unclear how broad the set of natural-language instructions is handled. If the model is constrained to a limited set of insertion types or spatial relations, the generality might be limited"}, "questions": {"value": "- The method is interesting, but how well can it scale to very large or complex scenes (many objects, occlusions, dynamic elements)?. The verification step (multi-view geometry) may become expensive.\n- What is the success rate of this method? Since the pipeline is using VLMs, can it produce a stable output and guidance? More details on the VLM decomposition strategy might be needed.\n- More quantitative results are needed. The current VLM score and user study are not enough to prove the effectiveness of this work.\n- The method likely assumes the input scene has clean geometry, depth information, and consistent multi-view structure. In real editing pipelines, scenes may be noisy or partial. How robust is the method then? Also, I am wondering if this method can be applied to other representations like NeRF, Meshes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xyN8KysHKl", "forum": "g11dXeWKyX", "replyto": "g11dXeWKyX", "signatures": ["ICLR.cc/2026/Conference/Submission6080/Reviewer_shNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6080/Reviewer_shNF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944810269, "cdate": 1761944810269, "tmdate": 1762918451385, "mdate": 1762918451385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "InsertAny3D inserts 3D objects into complex scenes based on high-level and abstract language prompts (e.g., _\"make the room cozier\"_). The proposed method introduces a two-stage pipeline  combining vision-language reasoning with geometry-aware 3D synthesis. The authors propose two key components:\n\n1.  **Stage 1 – VLM-Assisted Scene Understanding:** This module interprets an abstract language prompt and decomposes it into concrete actions (e.g., _\"add a sunflower plant in a vase\"_). Further, the method locates optimal insertion regions through a hierarchical selection process.\n2.  **Stage 2 – Geometry-Grounded 3D Insertion:** This module inserts objects directly in 3D scene using:\n    -   **Anchor-Constrained Synthesis** jointly generate the new object based on it's neighbourhood context to model realistic spatial relationships.\n    -   **Depth-Based Grounding**, which utilises the depth of the scene and the generated object instead of RGB, ensuring robustness to lighting and texture noise.\n    -   **Multi-View Verification** is employed to disambiguate placements using depth parallax across views."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-   **[S1] Interesting Problem Formulation:** The authors propose a framework which takes abstract instructions and decomposes them  into concrete deterministic steps for the editing task. This novel paradigm moves beyond simple explicit commands like \"Place X in the scene\". This approach is more user-friendly.\n-   **[S2] Technical solid Geometry-grounded insertion:** Anchor constrained synthesis ensures that existing geometry of the scene is used as a strong prior to model complex object-to-object interactions during generation. Depth-based Grounding is a more effective way to insert the generated 3D asset, as it can handle lighting variations. Further, results in Tab. 2 supports the requirement of depth-based grounding.\n-   **[S3] Exhaustive Experimentation:** The qualitative results are impressive. Further, the authors utililze metrics like HPSv2 and a VLM for quantitative evaluation. The method is clearly better than other baselines. A human user study further corroborates these claims."}, "weaknesses": {"value": "-   **[W1] Efficient Region Selector:** The proposed method uses CLIP cosine similarity as a coarse filter to discard irrelevant regions. This approach seems to be sensitive to false positives; for example, if the instruction is \"add a sunflower placed in the vase,\" and a bedsheet in the scene already contains a sunflower pattern, the bed might show a higher CLIP text-image score than the actual vase region due to feature similarity, leading to an incorrect candidate pool. Please address how the coarse CLIP filter is robust to these instances of semantic ambiguity."}, "questions": {"value": "-   [Q1] How does the method adapt to light variations in a 3D scene? For example, scenes in Fig. 5 are simplistic and do not have any lighting variations. Do the authors assume that the 3D scene does not have light sources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FPc3pT8T45", "forum": "g11dXeWKyX", "replyto": "g11dXeWKyX", "signatures": ["ICLR.cc/2026/Conference/Submission6080/Reviewer_rpaL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6080/Reviewer_rpaL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996628884, "cdate": 1761996628884, "tmdate": 1762918450446, "mdate": 1762918450446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InsertAny3D, a framework for high-quality 3D object insertion into complex scenes guided by ambiguous natural language commands. The method addresses the limitations of 2D-to-3D lifting approaches, such as manual intervention and multi-view inconsistencies. Its contributions are twofold. First, a \"VLM-Assisted 3D Scene Understanding\" component decomposes abstract user intents (e.g., \"make this room cozy\") into concrete subtasks and uses a hierarchical (CLIP+VLM) strategy to efficiently select optimal insertion regions. Second, a \"Geometry-Grounded 3D Object Insertion\" component uses an anchor-constrained generation process, jointly synthesizing the new object with an existing scene object (the \"anchor\") to maintain consistency. This module then employs depth-based feature matching and multi-view verification to ensure precise alignment and spatial coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The framework utilizes advanced VLM for 3D scene editing, which can interpret and act on high-level user instructions (like \"make this room more cozy\").\n\n2: The paper proposes a practical two-stage (CLIP + VLM) process for region selection, which is claimed to be much more efficient than naively using a powerful VLM to analyze all possible viewpoints.\n\n3: Experiments show that the method significantly outperforms existing baselines (like GaussianEditor, GaussianGrouping, and MVInpainter) across both automatic metrics (HPSv2, VLM Judge).\n\n4: This paper has nice figures to illustrate the overall idea of the proposed framework, and the framework is easy to understand."}, "weaknesses": {"value": "1: **Error Cumulation.** This framework involves numerous calls to existing VLM/segmentation models (such as GPT4o for reasoning the user input and LangSAM for segmentation). Cumulative errors can arise across these multiple steps, yet the framework does not address the handling of such accumulated errors.\n\n2: **Analysis of failure cases.** The experimental section lacks an analysis of failure cases, which would be beneficial for understanding the framework's limitations and the inherent difficulties of 3D scene editing via VLMs. **\n\n3: **The description of the used dataset for evaluation.** The paper lacks a detailed description of the evaluation dataset. It is only mentioned that the dataset consists of \"multiple large scenes\" collected from Sketchfab. Crucial statistics, such as the total number of scenes, the number of corresponding user prompts, and the number of 3D editing operations performed for each scene, are not provided. This lack of detail regarding the dataset's scale and composition makes it difficult to fully assess the robustness and generalizability of the experimental results, making them less convincing.\n\n4: **SAGS**. What's the mentioned \"our improved version of the text-driven 3D segmentation model SAGS? I haven't found any descriptions of this \"improved\" 3D segmentation model in either the main text or the appendix.\n\n5: **More details of the metrics in the user study.** As with the used dataset, the authors haven't provided much detail about the metrics used in the user study. The current version only has three aspects: Aesthetic, Precision, and Overall. What's the definition of them? \n\n6: **Initial Region Proposal.** A critical detail is missing regarding the \"Region Selector\". The appendix (A.2.1) clearly explains how camera poses (distance $d$, pitch $\\theta$, rotation $\\gamma$) are generated after a \"Region\" (defined by an xyz center coordinate) has been selected. However, the paper does not specify how the initial set of candidate regions is defined in the Region Selector. It is unclear if these regions are generated by densely sampling the entire 3D scene, if they are automatically proposed based on existing scene objects, or if they require manual pre-definition by the user.\n\nTypo:\n\n1: \"3D generated model\" in the caption of  Figure 3.\n2: The GIM in Line 340 misses a citation.\n\nI currently give 4 because there are a lot of details to be clarified, but this paper has somewhat convincing visualization results. I would reconsider the rating after receiving the authors' rebuttal as well as other reviews."}, "questions": {"value": "1: Where are the regions (1), (2), (3) in Figure 5 from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9IryuxAwzz", "forum": "g11dXeWKyX", "replyto": "g11dXeWKyX", "signatures": ["ICLR.cc/2026/Conference/Submission6080/Reviewer_q5m4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6080/Reviewer_q5m4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123132320, "cdate": 1762123132320, "tmdate": 1762918449548, "mdate": 1762918449548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InsertAny3D, a framework for high-quality 3D object insertion into complex scenes guided by ambiguous natural language commands. The method addresses the limitations of 2D-to-3D lifting approaches, such as manual intervention and multi-view inconsistencies. Its contributions are twofold. First, a \"VLM-Assisted 3D Scene Understanding\" component decomposes abstract user intents (e.g., \"make this room cozy\") into concrete subtasks and uses a hierarchical (CLIP+VLM) strategy to efficiently select optimal insertion regions. Second, a \"Geometry-Grounded 3D Object Insertion\" component uses an anchor-constrained generation process, jointly synthesizing the new object with an existing scene object (the \"anchor\") to maintain consistency. This module then employs depth-based feature matching and multi-view verification to ensure precise alignment and spatial coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The framework utilizes advanced VLM for 3D scene editing, which can interpret and act on high-level user instructions (like \"make this room more cozy\").\n\n2: The paper proposes a practical two-stage (CLIP + VLM) process for region selection, which is claimed to be much more efficient than naively using a powerful VLM to analyze all possible viewpoints.\n\n3: Experiments show that the method significantly outperforms existing baselines (like GaussianEditor, GaussianGrouping, and MVInpainter) across both automatic metrics (HPSv2, VLM Judge).\n\n4: This paper has nice figures to illustrate the overall idea of the proposed framework, and the framework is easy to understand."}, "weaknesses": {"value": "1: **Error Cumulation.** This framework involves numerous calls to existing VLM/segmentation models (such as GPT4o for reasoning the user input and LangSAM for segmentation). Cumulative errors can arise across these multiple steps, yet the framework does not address the handling of such accumulated errors.\n\n2: **Analysis of failure cases.** The experimental section lacks an analysis of failure cases, which would be beneficial for understanding the framework's limitations and the inherent difficulties of 3D scene editing via VLMs.\n\n3: **The description of the used dataset for evaluation.** The paper lacks a detailed description of the evaluation dataset. It is only mentioned that the dataset consists of \"multiple large scenes\" collected from Sketchfab. Crucial statistics, such as the total number of scenes, the number of corresponding user prompts, and the number of 3D editing operations performed for each scene, are not provided. This lack of detail regarding the dataset's scale and composition makes it difficult to fully assess the robustness and generalizability of the experimental results, making them less convincing.\n\n4: **SAGS**. What's the mentioned \"our improved version of the text-driven 3D segmentation model SAGS? I haven't found any descriptions of this \"improved\" 3D segmentation model in either the main text or the appendix.\n\n5: **More details of the metrics in the user study.** As with the used dataset, the authors haven't provided much detail about the metrics used in the user study. The current version only has three aspects: Aesthetic, Precision, and Overall. What's the definition of them? \n\n6: **Initial Region Proposal.** A critical detail is missing regarding the \"Region Selector\". The appendix (A.2.1) clearly explains how camera poses (distance $d$, pitch $\\theta$, rotation $\\gamma$) are generated after a \"Region\" (defined by an xyz center coordinate) has been selected. However, the paper does not specify how the initial set of candidate regions is defined in the Region Selector. It is unclear if these regions are generated by densely sampling the entire 3D scene, if they are automatically proposed based on existing scene objects, or if they require manual pre-definition by the user.\n\nTypo:\n\n1: \"3D generated model\" in the caption of  Figure 3.\n2: The GIM in Line 340 misses a citation.\n\nI currently give 4 because there are a lot of details to be clarified, but this paper has somewhat convincing visualization results. I would reconsider the rating after receiving the authors' rebuttal as well as other reviews."}, "questions": {"value": "1: Where are the regions (1), (2), (3) in Figure 5 from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9IryuxAwzz", "forum": "g11dXeWKyX", "replyto": "g11dXeWKyX", "signatures": ["ICLR.cc/2026/Conference/Submission6080/Reviewer_q5m4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6080/Reviewer_q5m4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123132320, "cdate": 1762123132320, "tmdate": 1763056019097, "mdate": 1763056019097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}