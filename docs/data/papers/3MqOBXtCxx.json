{"id": "3MqOBXtCxx", "number": 21747, "cdate": 1758321206262, "mdate": 1763668922213, "content": {"title": "Cost-Optimal Active AI Model Evaluation", "abstract": "The development lifecycle of generative AI systems requires continual evaluation, data acquisition, and annotation, which is costly in both resources and time. In practice, a desire for rapid iteration often makes it necessary to rely on synthetic annotation data because of its low cost, despite the potential for substantial bias. In this paper, we develop a rigorous theoretical framework for novel, cost-aware evaluation pipelines that actively balance the use of a cheap, but often inaccurate, weak rater---such as a model-based autorater that is designed to automatically assess the quality of generated content---with a more expensive, but also more accurate, strong rater such as a human annotator. Building on recent work in active and prediction-powered statistical inference, we theoretically derive a family of cost-optimal policies for allocating a given annotation budget between weak and strong raters so as to maximize statistical efficiency. \nNext, using synthetic and real-world data, we empirically characterize conditions under which these types of policies can yield significant improvements over classical methods. Finally, we find that practical approximations of the theoretically optimal policies \ncan achieve the same estimation precision at a far lower total annotation budget than standard evaluation methods, especially in tasks where there is high variability in the difficulty of examples.", "tldr": "This work develops a theoretical framework for cost-optimal model evaluation when faced with a choice between different model rating options with different cost vs. performance tradeoffs.", "keywords": ["llm", "evaluation", "ppi", "inference", "efficient", "active", "chatbot arena", "prediction-powered inference", "statistical inference"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f69e331b17345358064755ad422b2b7ff23b4947.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for the so-called cost-optimal active evaluations, which encompasses a family of annotation policies designed to minimize expected error under a given annotation budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents numerous mathematical propositions to support its claims, demonstrating strong theoretical foundations.\n- The motivation behind the work is clearly articulated.\n- Key findings are well-highlighted. In particular, the conclusion on line 223—“...active learning can help if the conditional squared error of G has significant variance”—offers valuable guidance for future research in developing active learning strategies."}, "weaknesses": {"value": "Several expressions are vague and fall short of academic standards.\n\n1. For instance, the phrase “...by optimizing everything” in line 086 is unclear—what does “everything” refer to?\nThe term “evals” used in line 105 and elsewhere is not standard English and lacks a clear definition. The sentence “We now describe our methods for constructing active, cost-optimal evals” (line 105) is difficult to interpret—what exactly are “evals”?\nIn line 117, H and G are defined as h(X) and G(X), which are described as ratings in line 116. However, line 118 states “querying H and G costs c_h and c_g”, which translates to “querying ratings costs...”—a phrasing that is not easily interpretable.\n\n\n1. Some mathematical steps are omitted, making the evaluation of the work challenging. For example, the proof of Equation 2 is relegated to the appendix, and the transition from line 726 to line 728 lacks clarity.\n2. The term “cost-optimal” used in the title and throughout the paper is not entirely convincing or appropriate. As acknowledged by the authors in line 480—“...annotation policies that are optimal in theory are distribution-dependent...”—this suggests that such optimal policies may be unattainable due to inherent uncertainties. Therefore, the proposed framework does not achieve a truly optimal solution, but rather an optimal solution subject to specific constraints. These constraints should be clearly emphasized, as the current phrasing implies a globally optimal solution.\n3. To substantiate the claim in line 477—“We derive annotation policies that are optimal in the sense of minimizing expected error under annotation budget constraints”—a brute-force experiment exploring various combinations of examples and demonstrating that the proposed method achieves the best or ceiling performance would be necessary."}, "questions": {"value": "1. What is the justification for using ξ_t​ over π_t​ in Equation 1? What are the implications of this choice?\n2. Should coreset-based methods be included as one of the baselines? If not, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jDnGcC5uFi", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Reviewer_te8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Reviewer_te8E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454320095, "cdate": 1761454320095, "tmdate": 1762941916600, "mdate": 1762941916600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main Response Comment"}, "comment": {"value": "Thank you to all the reviewers for taking the time to read and review our work. We were pleased to see that reviewers found our work to be “theoretically elegant,” \"technically sound,” “clearly articulated,” and addressing a “critical bottleneck in the GenAI lifecycle: the high cost of evaluation.” We also appreciate the comments and questions raised, and have made a significant effort to address them. We have made major changes to the paper, and have uploaded a revised draft with changes in red.\n\nThere was one common thread of concern raised by all reviewers, which can be summarized as follows: the theoretical results show that the optimal policies depend on distribution specific quantities which must be estimated in practice---what then is the significance for real world use cases? We have taken this feedback to heart and **have updated the manuscript** to address this in two specific ways:\n- **We have revised the Introduction and Contributions sections to more explicitly frame our theoretical results in Propositions 1 and 2 as \"oracle\" targets that define the necessary ingredients for practical estimation.** We added text clarifying that these theoretical results are essential not just for defining the optimal policy, but for rigorously understanding when cost-optimal active annotation is expected to outperform cost-optimal random annotation (per our analysis in Section 3), which directly informs the design of the practical estimators used in Section 4, and the interpretation of their results.\n- **To thoroughly show that these oracle policies can be estimated effectively with limited data, in addition to our existing experiments in Section 4, we have added Appendix D.3-5.** In particular, Appendix D.4 provides analysis on the burn-in size $n_b$, and shows that substantial performance benefits can be obtained over the baseline estimator **with even as few as 50 labeled burn-in examples.** Furthermore, to better highlight the practical utility of this approach, Figures 6 and 7 in Appendix D.3 plot the \"Percent Reduction in Cost\" relative to the baseline estimator, which show that our **practical approximations can yield over 40% reductions in cost.**\n\nWe also made several improvements to the presentation and clarity of the paper.\n\nWe hope this addresses the main concerns of the reviewers. Additional questions and feedback will be addressed in separate comments."}}, "id": "2hchSCXdQT", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763592973751, "cdate": 1763592973751, "tmdate": 1763592973751, "mdate": 1763592973751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for the so-called cost-optimal active evaluations, which encompasses a family of annotation policies designed to minimize expected error under a given annotation budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents numerous mathematical propositions to support its claims, demonstrating strong theoretical foundations.\n- The motivation behind the work is clearly articulated.\n- Key findings are well-highlighted. In particular, the conclusion on line 223—“...active learning can help if the conditional squared error of G has significant variance”—offers valuable guidance for future research in developing active learning strategies."}, "weaknesses": {"value": "Several expressions are vague and fall short of academic standards.\n\n1. For instance, the phrase “...by optimizing everything” in line 086 is unclear—what does “everything” refer to?\nThe term “evals” used in line 105 and elsewhere is not standard English and lacks a clear definition. The sentence “We now describe our methods for constructing active, cost-optimal evals” (line 105) is difficult to interpret—what exactly are “evals”?\nIn line 117, H and G are defined as h(X) and G(X), which are described as ratings in line 116. However, line 118 states “querying H and G costs c_h and c_g”, which translates to “querying ratings costs...”—a phrasing that is not easily interpretable.\n\n\n1. Some mathematical steps are omitted, making the evaluation of the work challenging. For example, the proof of Equation 2 is relegated to the appendix, and the transition from line 726 to line 728 lacks clarity.\n2. The term “cost-optimal” used in the title and throughout the paper is not entirely convincing or appropriate. As acknowledged by the authors in line 480—“...annotation policies that are optimal in theory are distribution-dependent...”—this suggests that such optimal policies may be unattainable due to inherent uncertainties. Therefore, the proposed framework does not achieve a truly optimal solution, but rather an optimal solution subject to specific constraints. These constraints should be clearly emphasized, as the current phrasing implies a globally optimal solution.\n3. To substantiate the claim in line 477—“We derive annotation policies that are optimal in the sense of minimizing expected error under annotation budget constraints”—a brute-force experiment exploring various combinations of examples and demonstrating that the proposed method achieves the best or ceiling performance would be necessary."}, "questions": {"value": "1. What is the justification for using ξ_t​ over π_t​ in Equation 1? What are the implications of this choice?\n2. Should coreset-based methods be included as one of the baselines? If not, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jDnGcC5uFi", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Reviewer_te8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Reviewer_te8E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454320095, "cdate": 1761454320095, "tmdate": 1763596301824, "mdate": 1763596301824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main Response Comment"}, "comment": {"value": "Thank you to all the reviewers for taking the time to read and review our work. We were pleased to see that reviewers found our work to be “theoretically elegant,” \"technically sound,” “clearly articulated,” and addressing a “critical bottleneck in the GenAI lifecycle: the high cost of evaluation.” We also appreciate the comments and questions raised, and have made a significant effort to address them. We have made major changes to the paper, and have uploaded a revised draft with changes in red.\n\nThere was one common thread of concern raised by all reviewers, which can be summarized as follows: the theoretical results show that the optimal policies depend on distribution specific quantities which must be estimated in practice---what then is the significance for real world use cases? We have taken this feedback to heart and **have updated the manuscript** to address this in two specific ways:\n- **We have revised the Introduction and Contributions sections to more explicitly frame our theoretical results in Propositions 1 and 2 as \"oracle\" targets that define the necessary ingredients for practical estimation.** We added text clarifying that these theoretical results are essential not just for defining the optimal policy, but for rigorously understanding when cost-optimal active annotation is expected to outperform cost-optimal random annotation (per our analysis in Section 3), which directly informs the design of the practical estimators used in Section 4, and the interpretation of their results.\n- **To thoroughly show that these oracle policies can be estimated effectively with limited data, in addition to our existing experiments in Section 4, we have added Appendix D.3-5.** In particular, Appendix D.4 provides analysis on the burn-in size $n_b$, and shows that substantial performance benefits can be obtained over the baseline estimator **with even as few as 50 labeled burn-in examples.** Furthermore, to better highlight the practical utility of this approach, Figures 6 and 7 in Appendix D.3 plot the \"Percent Reduction in Cost\" relative to the baseline estimator, which show that our **practical approximations can yield over 40% reductions in cost.**\n\nWe also made several improvements to the presentation and clarity of the paper. In particular, based on feedback from R-te8E, we have updated the title to be more precise: \"Optimal, Active, Prediction-powered AI Model Evaluation on a Budget\".\n\nWe hope this addresses the main concerns of the reviewers. Additional questions and feedback will be addressed in separate comments."}}, "id": "2hchSCXdQT", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763592973751, "cdate": 1763592973751, "tmdate": 1763669043034, "mdate": 1763669043034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a theoretical and empirical framework for cost-optimal active evaluation of generative AI systems. The authors tackle a very practical problem: evaluating large models is expensive, and existing hybrid setups (like combining cheap model raters with expensive human raters) often lack rigorous cost-aware allocation.\nTo address this, the paper derives policies for optimally allocating annotation budget between weak and strong raters — balancing cost and accuracy through statistical optimization. It extends prediction-powered inference (PPI) and active statistical inference to derive (1) an optimal random sampling rate and (2) an optimal active policy that depends on task-specific uncertainty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem—cost-aware AI evaluation—is timely, practical, and underexplored. The authors correctly identify inefficiencies in current model evaluation practices that rely heavily on costly human or LLM raters.\n2. The extension of prediction-powered inference with explicit cost constraints is technically sound. The derivation of closed-form policies (Propositions 1–2) is clear and builds on well-established statistical theory.\n3. The Gaussian/Bernoulli experiments in Section 3 are carefully designed to test key intuitions (e.g., dependence on rater error, heteroskedasticity, and cost ratio). The figures are clean and reinforce the theoretical claims.\n4. Applying the framework to Chatbot Arena evaluations shows that cost-optimized sampling can indeed save budget while maintaining accuracy. The setup is realistic and relevant to modern LLM benchmarking."}, "weaknesses": {"value": "1. The real-world experiments are narrow. Most results are on one dataset (Chatbot Arena) with two scenarios, both focused on text-based preference evaluations. There’s little diversity in task type or domain (e.g., no multimodal or structured data). The empirical results, while consistent, are modest—often showing ~40–50% budget savings under ideal transfer, which may shrink with realistic uncertainty estimation.\n2. While theoretically elegant, the framework’s impact on real-world evaluation pipelines is unclear. Implementing cost-optimal policies requires calibration, pilot estimation, and maintenance that may offset cost savings in small-to-medium-scale evaluation scenarios."}, "questions": {"value": "1. How do these methods perform when the weak rater is itself biased rather than merely noisy?\n2. Are there concrete examples of how much “burn-in” cost is acceptable before cost savings emerge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EniB4OQUlF", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Reviewer_BNqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Reviewer_BNqA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649156631, "cdate": 1761649156631, "tmdate": 1762941916354, "mdate": 1762941916354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a theoretical framework for cost-optimal evaluation of generative AI models. It addresses the high cost of using accurate \"strong raters\" (like humans) by creating policies to actively balance their use with cheap but inaccurate \"weak raters\" (like model-based autoraters).\n\nBuilding on prediction-powered inference, the authors derive cost-optimal policies that, given a fixed budget, decide when to pay for the expensive rater to maximize statistical efficiency. The theoretically optimal active policy queries the strong rater most often when the weak rater is most uncertain.\n\nSince the optimal policy's parameters are unknown in practice, the authors test estimation methods like \"policy burn-in\" (using the first 200 samples) and \"policy transfer\" (using a related dataset). Experiments on synthetic data and real-world benchmarks (like Chatbot Arena) demonstrate that these methods can achieve the same estimation precision for a fraction of the cost, with the greatest savings seen in tasks with high variability in example difficulty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a rigorous theoretical framework for active evaluation, extending beyond prior work. Instead of just improving efficiency for a fixed number of expensive annotations, it derives truly cost-optimal policies ($\\pi_{random}$ and $\\pi_{active}$) that explicitly solve for the best sampling strategy to minimize error given a fixed monetary or computational budget.\n2. The work addresses a critical bottleneck in the GenAI lifecycle: the high cost of evaluation. By providing a principled way to combine cheap autoraters with expensive human labels, the framework offers a practical path to achieving high-precision estimates at a much lower total annotation cost."}, "weaknesses": {"value": "1. The theoretically-derived policies, $\\pi_{random}$ and $\\pi_{active}$, depend on several distributional properties like $Var(H)$, $MSE(H,G)$, and the conditional error $u(x)$. Since these are unknown in a real-world setting, the policies cannot be used out of the box. The paper's practical solutions (burn-in and transfer) are approximations that either require a separate, related dataset or incur an initial \"burn-in\" cost before any savings can be realized.\n2. The benefit of the active policy over the simpler random policy hinges on an accurate estimate of the conditional error, $u(x)$. The paper's own experiments show a significant performance gap between the practical \"Active\" policy and the \"Oracle\" policy, which knows the true error. This implies that the current methods for estimating uncertainty are \"far from perfect\" and are a primary bottleneck limiting the practical gains."}, "questions": {"value": "1. Your practical \"burn-in\" policy (A2) uses a fixed $n_b=200$ expensive samples to estimate the policy parameters. This initial cost is a critical part of the total evaluation budget. Could you provide a sensitivity analysis showing how the performance of $\\pi_{active}$ and $\\pi_{random}$ changes for different values of $n_b$? It seems there would be a tradeoff: a small $n_b$ leads to poor parameter estimates, while a large $n_b$ defeats the purpose of saving costs.\n2. The main benefit of the active policy over the random one depends on an accurate estimate of the conditional error $u(x)$. You show a significant gap between your \"Active\" policy and the \"Oracle\" policy, implying that the $u(x)$ estimates are \"far from perfect\". For the binary tasks, you used the heuristic $u(x) = G(1-G)$, which assumes the weak rater (G) is a well-calibrated probability. Did you experiment with other methods for estimating $u(x)$ that might be more robust?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "05Zc2ebhyw", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Reviewer_Pv7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Reviewer_Pv7c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955845471, "cdate": 1761955845471, "tmdate": 1762941916056, "mdate": 1762941916056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a cost-aware framework for hybrid evaluation that mixes a cheap, weak rater with an expensive, strong rater, aiming to estimate the strong rater’s mean judgment under a budget. It derives (i) a closed-form optimal random sampling policy under cost constraints and (ii) an input-adaptive active policy with clipping and a threshold, then studies practical instantiations via policy transfer and burn-in estimation. Experiments show budget savings and reduced MSE compared with always using the strong rater."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The objective of minimizing estimator error subject to an annotation budget is formalized, yielding a closed-form $ \\pi_{\\text{random}} $ in terms of costs and weak-rater MSE, and an adaptive $ \\pi_{\\text{active}} \\propto \\sqrt{u(x)} $ with principled clipping to respect $ \\pi(x)\\in(0,1] $ with clear derivation. \n\n2. The transfer and burn-in strategies provide workable recipes, and the paper reports effective budget and cost-savings curves that are easy to interpret.\n\n3. Experiments on real-data seem to match the theory’s qualitative predictions."}, "weaknesses": {"value": "1. The method extends prediction-powered/active inference by optimizing cost-constrained policies and addressing clipping, but much of the estimator form and sequential setup follows prior work.\n\n2. The active policy depends on a non-convex 1-D optimization over $ \\tau $, and the paper does not report sensitivity to $ \\tau $, mis-estimated $ u(x) $, or misspecified cost ratios, which are likely in practice.\n\n3. The burn-in approach assigns the first $ n_b $ items to the strong rater to estimate parameters, which reduces net gains at small budgets. More discussion on adaptive burn-in size or warm-start reuse across tasks would be useful.\n\n4. In Chatbot Arena experiments, the strong label is also from LLMs, i.e., Gemini 1.5 Flash majority vote. A human-grounded subset would better validate external correctness."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8ES55nn38Y", "forum": "3MqOBXtCxx", "replyto": "3MqOBXtCxx", "signatures": ["ICLR.cc/2026/Conference/Submission21747/Reviewer_MVJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21747/Reviewer_MVJi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994089894, "cdate": 1761994089894, "tmdate": 1762941915678, "mdate": 1762941915678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}