{"id": "5MZg1iSSCo", "number": 178, "cdate": 1756730386929, "mdate": 1763115006374, "content": {"title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts", "abstract": "The detection and grounding of multimedia manipulation has emerged as a critical challenge in combating AI-generated disinformation. While existing methods have made progress in recent years, we identify two fundamental limitations in current approaches: (1) Underestimation of MLLM-driven deception risk: prevailing techniques primarily address rule-based text manipulations, yet fail to account for sophisticated misinformation synthesized by multimodal large language models (MLLMs) that can dynamically generate semantically coherent, contextually plausible yet deceptive narratives conditioned on manipulated images; (2) Unrealistic misalignment artifacts: currently focused scenarios rely on artificially misaligned content that lacks semantic coherence, rendering them easily detectable. \nTo address these gaps holistically, we propose a new adversarial pipeline that leverages MLLMs to generate high-risk disinformation. Our approach begins with constructing the MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered using state-of-the-art editing techniques and then paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. \nBuilding upon this foundation, we present the **A**rtifact-aware **M**anipulation **D**iagnosis via MLLM (AMD) framework featuring two key innovations: Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning, to tame MLLMs for the MDSM problem. Comprehensive experiments validate our framework's superior generalization capabilities as a unified architecture for detecting MLLM-powered multimodal deceptions. In cross-domain testing on the MDSM dataset, AMD achieves the best average performance, with ACC, mAP, and mIoU scores of 88.18, 60.25, and 61.02, respectively.", "tldr": "We propose AMD and MDSM to detect high-risk multimodal manipulations by large models.", "keywords": ["Multi-media", "Manipulation Detection", "MLLM", "DeepFake Detection", "Large Multimodal Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f9dc312c5918b09f5f587952c30b0a839f199f1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper looks at investigating how MLLM driven misinformation in two modalities, namely text and image. They construct a dataset MDSM that simulates realistic multimodal manipulations where images are first manipulated and paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. Additionally this paper proposes a detection and grounding framework that outputs both coordinates of the manipulation and explanations (AMD)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* I believe this paper is relevant and has a few strengths to it. There are some interesting experiments that are done with both a zero-shot setting and training models on the MDSM dataset.\n\n* Authors spent time trying to do LoRA finetuning on their dataset which I think was an important experiment the have included.\n\n* Showcasing how other models like HAMMER and FKA-Owl which are prevalent in multi-modal manipulations was good to have and some discussion of the analysis\n\n* Including a human evaluation of some of the manipulated images/text was good to include for this work to showcase how humans can be fooled by multi-modal models"}, "weaknesses": {"value": "* I believe that the authors should try and include more Open-Source multimodal models, for zero-shot evaluation in Table 2, currently the only model present is Qwen and no other popular models like Deepseek, LLaVa, Yi-VL.\n\n[1] Deepseek llm: Scaling open-source language models with longtermism.\n[2] Visual instruction tuning, Neurips 2023\n[3] Yi: Open foundation models by 01.ai, 2024."}, "questions": {"value": "* Did the authors use GPT-4o and Gemini-2.0 on all the images for the test set? Seems like quite a large test set for a paid model. How large is the test set?\n\n* Why didn’t the authors include more Open-Source models in Table 2, currently there is basically one 1, since only variants of QWEN is included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5miHRciAS3", "forum": "5MZg1iSSCo", "replyto": "5MZg1iSSCo", "signatures": ["ICLR.cc/2026/Conference/Submission178/Reviewer_QRQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission178/Reviewer_QRQG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879127873, "cdate": 1761879127873, "tmdate": 1762915463011, "mdate": 1762915463011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Qjna8aGb5V", "forum": "5MZg1iSSCo", "replyto": "5MZg1iSSCo", "signatures": ["ICLR.cc/2026/Conference/Submission178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission178/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763115005351, "cdate": 1763115005351, "tmdate": 1763115005351, "mdate": 1763115005351, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the \"coherence trap\" phenomenon in multimodal misinformation detection. The authors first construct MDSM, a large-scale, diverse, and aligned benchmark for multimodal manipulation detection, comprising challenging samples from reputable news sources. To address this challenge, the authors propose AMD (Artifact-aware Manipulation Diagnosis), a novel framework built on Florence-2 that integrates artifact pre-perception encoding and manipulation-oriented reasoning. Experiments show that AMD outperforms existing methods on MDSM, demonstrating improved robustness in detecting coherently generated fake news."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and formalizes the \"coherence trap,\" a highly relevant and critical issue in the era of advanced generative models, where the very coherence that makes AI-generated content useful also makes it dangerously deceptive.\n\n2. The construction of MDSM is a significant contribution. Its scale, diversity of sources, and alignment between modalities make it a valuable resource for the research community.\n\n3. The paper presents thorough experiments, including ablation studies and cross-domain evaluations, demonstrating the superior performance of AMD over state-of-the-art baselines on the MDSM benchmark."}, "weaknesses": {"value": "1. While the paper formally defines and highlights the \"coherence trap\" as a critical challenge in multimodal misinformation detection, the underlying concept of detecting semantically aligned fake content is not entirely novel. Prior works, such as MMFakeBench, have already explored scenarios involving coherent image-text manipulations. \n\n2. The proposed AMD framework is built upon the powerful Florence-2 model and leverages its strong pre-trained multimodal understanding. The architectural innovation of AMD itself appears limited. \n\n3. While the paper shows AMD's overall success, a deeper analysis of when and why AMD fails (e.g., specific types of manipulations it struggles with, examples of false positives/negatives) would strengthen the work and provide more insight for future research.\n\n[1]Liu X, Li Z, Li P P, et al. MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs. ICLR 2024"}, "questions": {"value": "1. While the \"coherence trap\" is well-motivated, similar aligned fakes have been studied in prior work (e.g., MMFakeBench). How does this work differ in problem formulation or threat model beyond dataset scale?\n\n2. The AMD framework builds directly on Florence-2 with minimal architectural changes. To what extent do the gains come from the model design versus the strong pre-trained backbone?\n\n3. What are the main failure modes of AMD? A deeper analysis of false positives/negatives or challenging manipulation types (e.g., subtle edits, coherent text-only fakes) would strengthen the paper.\n\n4. How generalizable is AMD beyond news domains? The reliance on Florence-2’s world knowledge may limit performance on out-of-distribution content (more sophisticated, MLLM-generated manipulations or content generated by different large models)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FEMVBsWNn0", "forum": "5MZg1iSSCo", "replyto": "5MZg1iSSCo", "signatures": ["ICLR.cc/2026/Conference/Submission178/Reviewer_GjoA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission178/Reviewer_GjoA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880285638, "cdate": 1761880285638, "tmdate": 1762915462855, "mdate": 1762915462855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles multimodal misinformation detection under the claimed more realistic threat model, generating images that have been locally manipulated (via face swap or face attribute editing), and paired with text generated by a multimodal LLM (MLLM) that is semantically consistent with the manipulation. The authors propose MDSM, a 441k-sample dataset where faces in news images are edited and paired with coherent deceptive narratives. Then introduce AMD, a Florence-2–based model that can handle both (i) binary “fake vs real,” (ii) manipulation type, and (iii) manipulated-region bounding boxes tasks in text form."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. The paper explicitly argues that prior work assumes crude cross-modal inconsistency, which makes detection too easy because the text and image obviously disagree. By contrast, MDSM uses an MLLM to generate fluent, contextually aligned fake narratives that match the manipulated visual identity. This is a meaningful direction.\n\n2. AMD outputs manipulation decisions and the tampered region coordinates as a single textual answer instead of separate detection heads. This makes the downstream application easier.\n\n3. They employ a cross-domain setting for evaluation, emphasizing the generalization ability."}, "weaknesses": {"value": "1. The novelty claimed based on the flaw of previous works is not strong enough. Prior work like FKA-Owl is already an MLLM-style system that “incorporates more world knowledge to improve the model’s cross-domain performance,” explicitly targeting multimodal fake news scenarios. The paper acknowledges this but still claims current approaches “fail to account for sophisticated misinformation synthesized by MLLMs,” which is not persuasive enough. \n\n2. On the model side, AMD is essentially Florence-2 + DaViT with several augmented function modules. This is solid engineering, but conceptually similar to known frameworks in this area. \n\n3. The paper repeatedly stresses that MDSM “defines a more challenging and practical problem,” “high-risk disinformation,” and “semantically coherent and contextually plausible narratives,” and that previous benchmarks are “too simplistic to effectively deceive the public.” Are there any quantitative and empirical demonstrations of the claimed risk?\n\n4. MDSM only keeps “human-centric” news with faces and named entities, then applies two manipulation types in the image domain, including Face Swap and Face Attribute editing. This is not a general-purpose “multimodal misinformation” benchmark. A multimodal celebrity face tampering benchmark is more appropriate.\n\n5. In the cross-domain training setting, how could the data leakage risk be avoided? Would there be the same celebrity headshot in training and testing sets? Besides, the distribution of manipulations and the linguistic style of fabricated text are almost identical across “domains.” It looks closer to the same attacker pipeline, different news source name.\n\n6. In the tables, you compare single mAP values across models: how exactly is multi-label type detection scored as AP? Is it per manipulation type and averaged? Macro/micro? The paper does not say.\n\n7. Which parameters are actually trainable in APE? Only the Artifact Token + classifier Ca? Is there a two-stage schedule or joint multitask training with loss L? The description is not clear enough for training details.\n\n8. In ablations, why do the authors never isolate TRP alone?\n\n9. If the AMD decoder is effectively trained to emit the answer at the end of that rigid QA format, how robust is it to prompt perturbations? Could an adversary just reformulate the question?"}, "questions": {"value": "Please see the question above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LThHryr0Ul", "forum": "5MZg1iSSCo", "replyto": "5MZg1iSSCo", "signatures": ["ICLR.cc/2026/Conference/Submission178/Reviewer_6UUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission178/Reviewer_6UUQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986980211, "cdate": 1761986980211, "tmdate": 1762915462608, "mdate": 1762915462608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel forgery detection framework that targets semantically coherent multimodal content generated by modern Multimodal Large Language Models (MLLMs). The proposed framework leverages Artifact Pre-perception Encoding (APE) and Manipulation-Oriented Reasoning (MOR) to collaboratively analyze image-text manipulations through the reasoning capability of MLLMs. In addition, the authors construct an MLLM-Driven Synthetic Multimodal (MDSM) dataset, where image and text modifications are jointly guided by MLLMs to ensure semantic alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper clearly highlights the risks posed by semantically coherent forgeries generated by modern Multimodal Large Language Models (MLLMs).\n2.It presents a large-scale, semantically aligned multimodal dataset, effectively filling a crucial gap in resources for studying MLLM-driven misinformation.\n3.The proposed framework integrates Artifact Pre-perception Encoding (APE) and Manipulation-Oriented Reasoning (MOR), leveraging the reasoning capabilities of MLLMs to collaboratively analyze image-text manipulations. By synergizing APE and MOR, it effectively adapts MLLMs for precise manipulation analysis.\n4.The authors conduct comprehensive experiments on both the MDSM and DGM4 datasets, achieving state-of-the-art average cross-domain generalization performance."}, "weaknesses": {"value": "1.The paper aims to tackle the challenge of forgery detection in semantically aligned image-text scenarios. However, its model design primarily focuses on visual forgeries while largely overlooking textual forgeries.\n2.Limited interpretability: although some visualizations are provided, the paper does not clearly demonstrate which specific forgery cues the model captures, nor does it offer a human-understandable reasoning process behind its decisions.\n3.The organization of Section 2.2 could be improved — the relationships among its subsections are not clearly articulated, making the overall flow somewhat difficult to follow."}, "questions": {"value": "1.Could the authors elaborate on the choice to focus specifically on facial modifications and replacements? Have considerations been made regarding the potential extension of the framework to other types of visual or contextual manipulations, such as scene-level or object-level edits?\n2.Would the authors be able to provide additional details about the dataset used to train the artifact-aware classification head? In particular, it would be helpful to know the scale of the data and the relative proportions of different manipulation types included during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3CP58WDRfW", "forum": "5MZg1iSSCo", "replyto": "5MZg1iSSCo", "signatures": ["ICLR.cc/2026/Conference/Submission178/Reviewer_3weR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission178/Reviewer_3weR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988957294, "cdate": 1761988957294, "tmdate": 1762915462423, "mdate": 1762915462423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}