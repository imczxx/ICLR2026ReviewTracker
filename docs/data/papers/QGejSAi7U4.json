{"id": "QGejSAi7U4", "number": 2530, "cdate": 1757137069488, "mdate": 1763044674853, "content": {"title": "Prompt-Guided Low-Level Recovery and High-Level Fusion for Incomplete Multimodal Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis seeks to understand emotions by combining language, audio, and visual signals, but its real challenge lies in building models that stay robust when one or more modalities are missing or corrupted. Recent studies attempted to leverage available embedding to complement missing regions by single-level feature reconstruction or cross-modal fusion. However, both reconstruction-only and fusion-only pipelines are limited: the former amplifies noise from imperfect recovery, while the latter overlooks semantic restoration, leaving cross-modal gaps and complex intermodal relationships inadequately captured for robust generalization. To overcome these limitations, we propose Prompt-Guided Low-level recovery and High-level fusion (PGLH)  for incomplete multimodal sentiment analysis, achieving deep cross-modal interactions from low-level semantic recovery to high-level semantic fusion through adaptive prompts. Specifically, PGLH mainly consists of Prompted Cross-Modal Masking (PCM2) and Unimodal-to-Bimodal Prompt Fusion (UBPF). First, PCM2 extends masked autoencoding to multimodal inputs by leveraging language-guided prompts to restore corrupted audio and visual tokens. This enables both structural fidelity and semantic grounding for low-level recovery. Secondly, in UBPF, self-guided prompts are introduced into each modality to extract fine-grained unimodal structures by selectively attending to informative regions. Next, they are progressively aligned with language-guided prompts for robust high-level fusion. Finally, PCM2 and UBPF realize the dual-level adaptation from low-level token reconstruction to high-level semantic integration, thereby effectively bridging modality gaps and more robust representations. Extensive experiments on MOSI, MOSEI, and SIMS demonstrate that PGLH consistently achieves impressive performance with missing data.", "tldr": "", "keywords": ["Multimodal Learning", "Sentiment Analysis", "Incomplete Modalities", "Cross-modal Fusion", "Modality Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bf3352b871fe6967f50e3b4656076590d0c13d04.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PGLH, a framework designed for Multimodal Sentiment Analysis (MSA) with incomplete data. The method operates at two levels: a low-level module (PCM2) that uses language-guided prompts to reconstruct missing tokens in audio and visual streams, and a high-level fusion module (UBPF) that uses prompts to refine and align multimodal representations for sentiment prediction. While the problem is relevant, the paper suffers from significant flaws in its positioning, methodology, and experimental evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The method is easy to understand.\n\n2. The problem for missing data of MSA is important."}, "weaknesses": {"value": "1. Originality and Novelty Concerns: The paper's most critical flaw is its failure to acknowledge or discuss highly relevant prior work. The field of prompt-based learning for incomplete MSA has been explored previously, notably in works like \"Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition\" (ACL 2024) and subsequent research. This paper completely overlooks this entire line of prompt-based works for incomplete MSA. As a result, the claims of novelty are unsubstantiated, and the proposed method appears similar to existing approaches without a clear differentiation. This omission raises concerns about the thoroughness of the literature review and the actual contribution of this work.\n\n2. Questionable Experimental Setup: The choice to evaluate the model on \"token missing\" instead of the more commonly studied \"modality missing\" is poorly justified. In real-world applications, it is far more common for an entire modality (e.g., the audio track or visual feed) to be unavailable than for sporadic, random tokens within a stream to be missing. This experimental design choice creates a disconnect from the problem as it is typically framed in the literature. Furthermore, it calls into question the fairness of comparing PGLH against baselines that were designed to handle modality-level absence. The paper does not provide a compelling reason for this deviation.\n\n3. Insufficient Experimental Depth: The experiments presented are superficial and lack the depth required to be convincing. A central claim of the paper is the effectiveness of the low-level reconstruction via the PCM2 module. However, the authors provide no qualitative evidence to support this. Visualizations or qualitative examples of the reconstructed audio or visual data are essential to validate that the module is performing meaningful recovery rather than simply learning to output generic features. The evaluation is limited to reporting final metrics and a basic ablation study, which is not sufficient to understand the model's behavior or substantiate its claimed mechanisms."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lD8q1Dxyg3", "forum": "QGejSAi7U4", "replyto": "QGejSAi7U4", "signatures": ["ICLR.cc/2026/Conference/Submission2530/Reviewer_MY9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2530/Reviewer_MY9E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931529184, "cdate": 1760931529184, "tmdate": 1762916269676, "mdate": 1762916269676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "W7XrIISwmN", "forum": "QGejSAi7U4", "replyto": "QGejSAi7U4", "signatures": ["ICLR.cc/2026/Conference/Submission2530/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2530/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763044673659, "cdate": 1763044673659, "tmdate": 1763044673659, "mdate": 1763044673659, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on incomplete multimodal sentiment analysis and proposes PGLH, which leverages PCM2 for low-level recovery and UBPF for high-level integration. The reported results on three datasets demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper addresses an important and meaningful task: incomplete multimodal sentiment analysis.\n\n2.\tOverall, the paper is well-structured and clearly written."}, "weaknesses": {"value": "1.\tThe paper does not provide compelling motivations for its contributions.\n\na)\tFirstly, it claims that the proposed method can “enable both structural fidelity and semantic grounding for low-level recovery”. However, there is a lack of experimental comparisons with existing methods in terms of reconstruction performance.\n\nb)\tSecondly, the paper argues that “reconstruction-only pipelines risk amplifying noise from imperfect recovery, whereas fusion-only pipelines neglect the explicit restoration of corrupted semantics.”Previous reconstruction-based approaches have also made significant efforts to utilize multimodal information for accurate reconstruction, and the proposed prompt-based method is, in essence, another form of multimodal fusion. Therefore, the paper should further clarify why and how the proposed approach better addresses the challenges posed by imperfect recovery.\n\n2.\tThe paper simulates missing modalities through random masking. It remains unclear how this artificial setup correlates with real-world scenarios of missing or noisy data. Additional experiments using more realistic missing or noisy conditions are necessary to validate the practical applicability of the method.\n\n3.\tThe experimental section lacks sufficient evaluations to convincingly demonstrate the effectiveness of the proposed methods. More baseline methods should be included for comprehensive comparison.\n\n4.\tThe performance improvements reported in Table 1 and Table 2 over SOTA methods are relatively marginal. Statistical significance tests should be conducted to assess whether these gains are remarkable.\n\n5.\tFrom my perspective, the paper primarily introduces a prompt-based approach to incomplete multimodal sentiment analysis. However, previous studies have already explored a variety of multimodal fusion techniques, and the proposed prompt-based method can also be viewed as falling within the broader category of multimodal fusion. However, from my perspective, multimodal fusion is undoubtedly important but not a promising route to address incomplete multimodal sentiment analysis. This is further supported by the marginal performance gains observed in the experiments, which do not strongly justify the novelty or practical advantages of the proposed method."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IRqpfLNIeO", "forum": "QGejSAi7U4", "replyto": "QGejSAi7U4", "signatures": ["ICLR.cc/2026/Conference/Submission2530/Reviewer_TqCy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2530/Reviewer_TqCy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205222223, "cdate": 1761205222223, "tmdate": 1762916269481, "mdate": 1762916269481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PGLH, a dual-level cross-modal framework that handles incomplete and noisy inputs in multimodal sentiment analysis. By combining low-level masked reconstruction with high-level prompt-guided fusion, PGLH achieves stable and semantically consistent representations. Experiments on MOSI, MOSEI, and SIMS show consistent improvements over baselines across different missing rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The coexistence of complete and incomplete modalities is a common issue in real-world multimodal applications, making the research problem both practical and valuable.\n2. The experiments are comprehensive, covering three datasets and various missing rates with detailed comparisons, ablations, and hyperparameter studies, providing empirical support for the method."}, "weaknesses": {"value": "1. The paper does not sufficiently discuss implementation details and sensitivity analysis regarding prompt vector design, including its dimension, initialization, number, and positional placement (e.g., before, within, or after the token sequence).\n2. It lacks an explicit explanation and supporting experiments on why the decoder can be safely removed during inference (e.g., whether the encoder has implicitly learned reconstruction ability). A comparison with partially retaining a lightweight decoder would help justify this design choice.\n3. At line 222, the definition of f_m^p is unclear and should be explicitly specified."}, "questions": {"value": "1. At line 298, the sentiment prediction loss is introduced but not explained earlier in the text, its exact application point and functional role are not described.\n2. The paper lacks theoretical or interpretive justification for why the prompt can effectively guide reconstruction under varying missing rates and across different modalities. Visualization analyses (e.g., attention maps or reconstruction examples) would improve interpretability.\n3. In Table 4, the authors omit an ablation analysis of the sentiment prediction loss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cmg0iZFLDB", "forum": "QGejSAi7U4", "replyto": "QGejSAi7U4", "signatures": ["ICLR.cc/2026/Conference/Submission2530/Reviewer_CoFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2530/Reviewer_CoFN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728594367, "cdate": 1761728594367, "tmdate": 1762916269307, "mdate": 1762916269307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework PGLH to address the key challenges in incomplete MSA. It uses \"prompts\" to achieve dual-level cross-modal adaptation. It first recovers corrupted audio and visual features using language-guided prompts for low-level reconstruction, then performs high-level fusion by progressively aligning modalities through self-guided and language-guided prompt fusion. Experiments on the MOSI, MOSEI, and SIMS datasets demonstrate that PGLH achieves robust and superior performance under various missing rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Decomposes the problem into \"low-level\" and \"high-level\", and use \"prompt\" as a unified mechanism that through the entire pipeline from low-level to high-level.\n2. The designs of PCM2 and UBPF are reasonable, and experiments have also proven their effectiveness. \n3. Good performance."}, "weaknesses": {"value": "1. Some descriptions are unclear. For example, the definition and role of \"prompt\" in the Abstract and Introduction are not clearly explained. It may lead readers to mistake it for prompts used in LLMs. After reading the \"Method\" section, I was able to understand the specific application of the term \"prompt\" in this paper.\n2. Lacks visualization: (1) Is there any way to verify that the prompt representations are useful as intended? (2) Could you show the intermediate attention maps and analyze them? (3) Could you show some representative cases where the model's predictions succeeded or failed?\n3. There are problems with the citation. For example, Zadeh et al., 2017 and Han et al., 2021 (line 95), Zhang et al., 2023 (line 418), have been published but are still cited using arXiv versions. Additionally, ALMT is incorrectly cited as LNLN on lines 51-52 and line 101.\n4. Could you clarify in the caption of Figure 1 that PCM2 and UBPF are shown with audio modality streams as an example?\n5. What would happen if the language-guided prompts in PCM2 were replaced with vision-guided or audio-guided prompts?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Tc5nOMYNNP", "forum": "QGejSAi7U4", "replyto": "QGejSAi7U4", "signatures": ["ICLR.cc/2026/Conference/Submission2530/Reviewer_xx2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2530/Reviewer_xx2U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928725010, "cdate": 1761928725010, "tmdate": 1762916269124, "mdate": 1762916269124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}