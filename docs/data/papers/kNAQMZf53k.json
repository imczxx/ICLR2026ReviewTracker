{"id": "kNAQMZf53k", "number": 13804, "cdate": 1758222864607, "mdate": 1759897411512, "content": {"title": "Learning GUI Grounding with Spatial Reasoning from Visual Feedback", "abstract": "Graphical User Interface (GUI) grounding is a fundamental task for GUI agents, commonly framed as a coordinate prediction task that identifies an on-screen pixel for actions such as clicks and keystrokes. Though recent Vision Language Models (VLMs) show strong capabilities in understanding GUIs, they often fail in grounding when processing GUIs with high resolution and complex layouts. To address this issue, we reframe GUI grounding as an interactive search task, where the VLM agent outputs actions to move a cursor in the GUI to locate UI elements. At each step, the model determines the target object, evaluates the spatial relations between the cursor and the target, and moves the cursor closer to the target conditioned on the movement history. We train our GUI grounding agent, GUI-Cursor 7B, using multi-step online reinforcement learning with a dense trajectory-based reward function. Our experimental results show that GUI-Cursor 7B achieves state-of-the-art accuracy on ScreenSpot-v2 (93.9\\%) and ScreenSpot-Pro (56.5\\%). Moreover, the number of movement steps decreases as the grounding accuracy improves during training, and the final model learns to solve the problem within two turns for 95\\% of instances and can adaptively conduct more steps on more difficult examples.", "tldr": "", "keywords": ["GUI Grounding", "GUI Agent", "Computer-Using Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1223ac96b027a9bfdf8a06a47fdc5bdcf1d5dd5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper reframes GUI grounding from one-shot coordinate generation to an interactive cursor-moving process. A Qwen2.5‑VL‑7B observes a screenshot with a rendered cursor, reasons about the target and the cursor–target relation, and iteratively issues either a new (x, y) or STOP. Training uses GRPO with a dense distance reward and trajectory penalties. To keep training affordable, the model is trained at 1920×1080 and, at inference, applies cursor‑centric focusing (CCF): one coarse step on the full screen, then crops around the initial click for fine steps. On ScreenSpot‑v2 and ScreenSpot‑Pro, the method reports good performance and shows that most cases finish in 2 steps. The paper also probes spatial reasoning via a simple “cursor‑in‑box” test and argues that interaction plus visual feedback strengthens spatial understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem reframing. Turning coordinate emission into an interactive search with visual feedback is intuitive and well motivated by spatial–semantic misalignment in patch‑token VLMs. The cursor overlay makes the prediction visible to the model, which is a nice design touch.\n- Simple, effective shaping. The four trajectory penalties are targeted at real degeneracies (premature STOP, oscillation, moving the wrong way, repetition). Ablations show each term helps; the gains are especially visible on ScreenSpot‑Pro where large canvases exacerbate these errors.\n- A pragmatic efficiency recipe. Training at lower resolution and then zooming near the first guess is a strong “80/20” trick. The authors also apply the same focus idea to baselines, which is thoughtful."}, "weaknesses": {"value": "- The core idea of iterative focus via cropping after an initial guess now appears in multiple concurrent works. GUI‑Spotlight iteratively narrows focus with dedicated crop tools and multi‑turn RL; GUI‑ARP performs adaptive multi‑stage inference with GRPO and attention‑guided crops; GUI‑RC/GUI‑RCPO refine grounding at inference via test‑time consensus/RL. The paper should position CCF and multi‑step cursor moves against these, not just older one‑shot baselines. As it stands, CCF feels close to those “zoom‑in” strategies, and the contribution reduces to (i) rendering a cursor as visual feedback and (ii) a particular set of penalties. [1, 2]\n- Outdated SOTA claim. Newer papers report higher ScreenSpot‑Pro accuracy (e.g., GUI‑ARP 60.8% with a 7B backbone), which overtakes the reported 56.5%. The paper should update comparisons and discuss where it still wins (e.g., step efficiency) versus absolute accuracy. [2]\n- Missing or under‑cited related work. The submission’s Related Work does not cover GUI‑Spotlight (iterative focus with tools) [1], GUI‑ARP (adaptive region perception and stage control) [2], nor test‑time RL via region consistency (GUI‑RC/GUI‑RCPO) [3]. These are directly relevant to the “interactive narrowing + RL” story and should be acknowledged.\n- The cursor‑in‑box probe is neat but synthetic. Given substantial evidence that VLMs struggle with spatial relations, evaluation on public spatial benchmarks (e.g., SpatialMQA, SPHERE) would strengthen the generalization claim [4].\n\n## References\n[1] GUI-Spotlight: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding.\n\n[2] GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents.\n\n[3] Test-Time Reinforcement Learning for GUI Grounding via Region Consistency.\n\n[4] Can Multimodal Large Language Models Understand Spatial Relations?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GaSSagKBdg", "forum": "kNAQMZf53k", "replyto": "kNAQMZf53k", "signatures": ["ICLR.cc/2026/Conference/Submission13804/Reviewer_JBDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13804/Reviewer_JBDJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639820169, "cdate": 1761639820169, "tmdate": 1762924335260, "mdate": 1762924335260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a solution to the challenge that vision-language models struggle to accurately predict pixel coordinates for GUI elements by reframing GUI grounding as an interactive task. Instead of directly outputting coordinates, the model iteratively moves a virtual cursor on the screen, using visual feedback at each step to refine its position until reaching the target. The model is trained with reinforcement learning using rewards that encourage accurate positioning and penalize inefficient search patterns. GUI-Cursor achieves substantial improvements on ScreenSpot-Pro."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: Reframing GUI grounding from static coordinate prediction to dynamic cursor-driven search is well-motivated.\n\nS2: The combination of position-based reward with four specific trajectory penalties (false stop, false move, false direction, repeated position) effectively guides learning toward efficient search behaviors. \n\nS3: CCF balances computational constraints during training with accuracy needs during inference."}, "weaknesses": {"value": "W1: The evaluation relies solely on two benchmarks, limiting confidence in generalizability. ScreenSpot-v2 is saturated with baselines already achieving >90%. With substantial gains observed only on ScreenSpot-Pro, it remains unclear whether the multi-step approach generalizes beyond the specific challenges of this benchmark. Evaluation on diverse benchmarks, like OSWorld-G[1] and UI-Vision[2], is necessary to validate that the computational overhead of multi-step interaction provides consistent benefits across different GUI grounding scenarios beyond the ScreenSpot family\n\n\nW2: The interaction history grows: ($I$, $O_0$,$A_0$, ..., $O_t$). Thus, multi-step interaction accumulates context linearly. With CCF, each $O$ is ~26k tokens.  So, for a hard case that requires 3 steps, the model processes ~78k image tokens and text tokens. Does it slow down inference significantly?\n\nW3: Table 3 reveals that Qwen2.5-VL-7B drops from 88.8% to 36.3% accuracy when using direct cursor movement without fine-tuning. This dramatic failure suggests the base model cannot transfer its existing spatial understanding to cursor control. Consequently, GUI-Cursor's success may primarily reflect learning a new interaction interface (cursor mechanics) rather than improved spatial reasoning. The paper claims to improve \"spatial semantic alignment\" and \"spatial reasoning,\" but the evidence suggests interface adaptation is the dominant factor. Though the cursor-in-box test shows modest improvements, it uses a highly simplified setting that is far from practical usage.\n\n[1] Xie et al. Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis.\n\n[2] Nayak et al. UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction."}, "questions": {"value": "Please see Weaknesses. The reviewer is willing to raise the score if the authors address most, if not all, of the questions above in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dv8yHUI0j0", "forum": "kNAQMZf53k", "replyto": "kNAQMZf53k", "signatures": ["ICLR.cc/2026/Conference/Submission13804/Reviewer_opuB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13804/Reviewer_opuB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793731216, "cdate": 1761793731216, "tmdate": 1762924334719, "mdate": 1762924334719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUI-Cursor, a reinforcement learning framework for GUI grounding that reframes one-step coordinate prediction as an multi-step interactive search problem, where the model iteratively moves a virtual cursor to locate UI elements while receiving visual feedback at each step. The key insight is that existing VLMs suffer from spatial-semantic misalignment because they only receive supervision on numerical coordinates without seeing where their predictions actually land on the GUI. \nGUI-Cursor addresses this by training with GRPO reinforcement learning using a trajectory-level reward design (including penalties for false direction, false stop, repeated position, etc.) and a Cursor-Centric Focusing (CCF) inference strategy to balance efficiency and accuracy.\nBuilt upon Qwen2.5-VL-7B, GUI-Cursor achieves state-of-the-art results on ScreenSpot-v2 (93.9%) and ScreenSpot-Pro (56.5%, +6.4% over prior best), with the model learning to adaptively use more steps for difficult cases (e.g., small targets). \nAdditionally, cursor-in-box spatial reasoning test show that GUI-Cursor significantly enhances the model’s ability to understand and reason about spatial relationships between the cursor and visual elements, indicating that reinforcement-driven interaction helps build more robust spatial understanding beyond task-specific grounding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Clear and well-motivated approach:** The reformulation of GUI grounding as an interactive search task with visual feedback is intuitive and addresses a fundamental limitation of existing methods: models receive supervision only on numerical coordinates without seeing where predictions actually land, leading to spatial-semantic misalignment.\n\n**2. Effective inference strategy:** The proposed Cursor-Centric Focusing (CCF) balances computational efficiency and accuracy, particularly on high-resolution, complex GUIs.\n\n**3. Strong empirical results:** GUI-Cursor-7B achieves state-of-the-art performance on both ScreenSpot-v2 (93.9%) and ScreenSpot-Pro (56.5%, +6.4% over prior best).\n\n**4. Comprehensive and insightful analysis:** The paper provides detailed ablations, movement-step analysis, and spatial reasoning diagnostics that clarify how interaction and feedback improve spatial understanding. The cursor-in-box diagnostic test reveals that strong VLMs struggle with basic spatial reasoning and exhibit severe center bias, while interactive training on GUI grounding improves this capability as an emergent property, showing that visual feedback genuinely enhances spatial understanding beyond task-specific performance.\n\n**5. Clear presentation:** The paper is well-written, logically structured, and supported by clear figures and experimental evidence."}, "weaknesses": {"value": "**1. Lack of downstream validation:** The evaluation focuses solely on static grounding benchmarks without demonstrating practical benefits in real GUI agent systems or broader multimodal interaction scenarios.\n\n**2. Title and spatial reasoning scope:**\nThe paper prominently features “Spatial Reasoning” in its title, but the treatment of this aspect is somewhat limited. The *cursor-in-box* diagnostic test is interesting, but the paper does not provide a deeper analysis of how GUI grounding relates to spatial reasoning or whether this capability can transfer to other spatial reasoning tasks. Additionally, the mechanism by which visual feedback enhances spatial understanding remains underexplored.\n\n**3. Computational cost:** While 95% of examples are solved within 2 steps, the paper does not analyze the computational overhead of processing multiple high-resolution images with growing context windows, or compare inference efficiency against single-step baselines. The trade-off between accuracy gains and computational cost deserves more thorough investigation."}, "questions": {"value": "**1. Ablation on single-step training:** Could the authors provide results for a single-step version of GUI-Cursor trained under identical settings (same base model, GRPO algorithm, training data, and position reward) but constrained to max_steps = 1? This would help isolate the contribution of multi-step interactive learning from other design factors.\n \n**2. Evaluation in interactive agent settings:** While ScreenSpot-v2 and ScreenSpot-Pro are strong static grounding benchmarks, evaluating GUI-Cursor in more realistic interactive agent environments (e.g., *WebArena* and *Multimodal-Mind2Web* for web, *AndroidControl* for mobile, and *OSWorld* for operating system agents) could better demonstrate its applicability to real-world GUI agent tasks. \n\n**3. Comparison with concurrent work:** According to the latest ScreenSpot-Pro leaderboard ([https://gui-agent.github.io/grounding-leaderboard/](https://gui-agent.github.io/grounding-leaderboard)), more recent results report improved performance. For instance, GTA1-7B has been updated to 55.5%, and newer models such as GUI-ARP-7B (60.8%) and Holo1.5-7B (57.9%) achieve higher scores. Could the authors clarify whether these are concurrent submissions and discuss how GUI-Cursor compares to these latest results? If these methods were developed independently around the same time, a brief discussion of their methodological differences would be valuable for the community. \n\n**4. Figure presentation:** In Figure 2(b), some text labels overlap with the bars, which slightly affects readability. Improving the layout or adjusting the legend placement could make the comparison clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RpGTtIuu6h", "forum": "kNAQMZf53k", "replyto": "kNAQMZf53k", "signatures": ["ICLR.cc/2026/Conference/Submission13804/Reviewer_1NKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13804/Reviewer_1NKj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884667675, "cdate": 1761884667675, "tmdate": 1762924334012, "mdate": 1762924334012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the mislocalization problem in existing visual GUI agents and propose an interactive visual reasoning approach that evaluates spatial relations between the cursor and the target. In this framework, the rendered cursor serves as visual feedback for vision-language models. The proposed method is validated on standard GUI grounding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The draft is well-organized and clearly written.\n- The intuition behind the proposed method is solid and straightforward.\n- The authors provide well-designed ablation studies that help deepen the understanding of their approach."}, "weaknesses": {"value": "- One concern lies in the novelty of the proposed method. In the broader vision domain — such as object grounding and visual question answering — many prior works have already utilized initial predictions as visual feedback, often through bounding boxes or check marks. This work appears to be a straightforward application of that idea to GUI grounding tasks, and the discussion does not sufficiently clarify how it differs from or advances beyond previous literature in general vision contexts.\n\n- There are also concerns about the trade-off between training cost and performance. The current draft is not convincing regarding the practicality of this method. Please provide a detailed comparison of training efficiency with existing supervised fine-tuning (SFT) approaches such as GUI-Actor-7B or RL approaches GTA1-7B to better understand the improvements. Given the recent trend toward large-scale pretraining in GUI agents, such a Pareto analysis is particularly important.\n\n- Experiments are conducted only on GUI grounding benchmarks. It remains unclear whether the proposed method generalizes to broader GUI agent tasks—especially those involving user instructions that do not exactly match GUI elements. Additional experiments on datasets such as AITW or Multimodal-Mind2Web would strengthen the claim that the proposed interactive reasoning approach can handle more realistic and diverse scenarios."}, "questions": {"value": "- It is unclear whether reinforcement learning is necessary for this method. Could the proposed approach also function effectively without additional training, for example in a test-time scaling or supervised fine-tuning (SFT) setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kh4CZhvEKi", "forum": "kNAQMZf53k", "replyto": "kNAQMZf53k", "signatures": ["ICLR.cc/2026/Conference/Submission13804/Reviewer_MH8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13804/Reviewer_MH8z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992876980, "cdate": 1761992876980, "tmdate": 1762924333612, "mdate": 1762924333612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}