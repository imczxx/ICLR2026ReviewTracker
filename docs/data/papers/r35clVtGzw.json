{"id": "r35clVtGzw", "number": 4183, "cdate": 1757625423019, "mdate": 1759898048603, "content": {"title": "SAM 3: Segment Anything with Concepts", "abstract": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., “yellow school bus”), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching\nobject instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of a vision backbone shared between an image-level detector and a memory-based video tracker. Recognition and localization are decoupled with a presence head, which significantly boosts detection accuracy. SAM 3 delivers a 2x gain over existing systems in both image and video PCS, and improves previous SAM capabilities in interactive visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark.", "tldr": "We present a strong model and challenging benchmark to advance open-vocabulary concept segmentation in images and videos.", "keywords": ["foundation models", "open vocabulary segmentation", "semantic instance segmentation", "object tracking"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cb68221311aa4d88167b66c0c84ef569e37122f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a new task called Promptable Concept Segmentation (PCS), which segments target objects through language prompts or image examples. Authors build a scalable data engine to automatically generate training data. They also present a new model, SAM3, to implement PCS. SAM3 has achieved excellent performance in both image and video segmentation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Generalized prompts. Compared with previous works, PCS has a wider range of prompts and SAM3 achieves better performance.\n2. Data engine. The data engine proposed by authors can effectively expand data, including mask and text, which is beyond the capability of previous works.\n3. SAM3 achieves PCS for both images and videos through a detection-then-tracking paradigm.\n4. SAM3 achieves sota performances over several tasks and benchmarks."}, "weaknesses": {"value": "1. The formulation of SAM3. This detection-then-tracking approach of SAM3 follows a two-stage format. It inevitably introduces errors and requires hyperparameter in the merging stage. The paradigm of SAM3 could be further adjusted to implement PCS in an end-to-end manner instead of a two-stage one.\n2. Complex naming conventions. The naming of training and test data in the paper is overly complex, with numerous similar names. Additionally, due to space limitations, authors did not elaborate on the meanings of different names, which makes this quite confusing.\n3. Missed citation. Many of the mentioned datasets or benchmarks are not accompanied by citations in the references.\n4. Multi-stage training, which trains different parts of SAM3. A final end-to-end training that fine-tunes the entire model, may further improve performance."}, "questions": {"value": "1. Do you have the plan to release the code and model? I think it is quite import for the community.\n2. It seems that the concept of SAM3 is limited to simple text and cannot perform complex reasoning like ReasonSeg. Authors have used an agent-based approach to achieve this. However, authors did not show the performance of using SAM3 directly for complex reasoning, which I find quite curious."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gkp24HNazl", "forum": "r35clVtGzw", "replyto": "r35clVtGzw", "signatures": ["ICLR.cc/2026/Conference/Submission4183/Reviewer_wjcv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4183/Reviewer_wjcv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545663720, "cdate": 1761545663720, "tmdate": 1762917219225, "mdate": 1762917219225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "VOS task results"}, "comment": {"value": "Hi authors,\n\nI've been following the SAM series for a while.\nCompared with SAM2, this paper shifts the focus from videos back to images. However, I still have one question about the evaluation on video tasks: \n\nIn Table 19 (appendix B.5), with 100% SACO training videos, the model gets DAVIS 91.5 (-0.1); SAV-val 77.4 (+0.4), SAV-test 78.0 (+0.9), which means the additional training videos only helped a little bit on VOS tasks (comparing with only use SAM2 video data).  While in Table 6, SAM3 gets DAVIS 92.0, SAV-val 82.0, SAV-test 84.6. Could you please explain where the improvement, especially the 4%+ on SAV is from? And what's the difference between the models for Table 6 and Table 19?\n\nThanks very much for your attention! Looking forward to your reply."}}, "id": "AgxfwhtMqQ", "forum": "r35clVtGzw", "replyto": "r35clVtGzw", "signatures": ["~Siyang_Li1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Siyang_Li1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4183/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763098849460, "cdate": 1763098849460, "tmdate": 1763099647394, "mdate": 1763099647394, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces SAM 3, a new extension of the \"Segment Anything Model\" paradigm that formalizes and successfully tackles the promptable concept segmentation task.\nThis moves the field beyond single-object segmentation to the more complex challenge of detecting, segmenting, and tracking all instances matching a given concept text prompt.\nThis advance is underpinned by a scalable data engine to generate a massive, high-quality dataset.\nThe refined model architecture effectively decouples recognition and localization for better open-vocabulary segmentation.\nThe method demonstrates a good performance gain on this new task and this paper establishes the new and large-scale SA-Co benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduction of the \"presence head\" is a specific and impactful architectural contribution. This design choice directly addresses the challenge of open-vocabulary detection by decoupling the recognition from the localization, which the ablations show is effective.\n- The paper demonstrates compelling quantitative results, achieving a great performance gain on the PCS task and setting a new state-of-the-art on existing benchmarks. These results validate the efficacy of the complete system.\n- The SAM 3 model and the large-scale SA-Co benchmark are valuable. This provides a valuable new asset for the community.\n- While other works have explored text-prompted segmentation, this paper successfully integrates concept-level segmentation and tracking into a single, unified SAM model. The ability to scale this integration to such a large dataset and achieve robust performance is a notable engineering accomplishment."}, "weaknesses": {"value": "Given that this paper can be viewed as the latest advancement in the SAM series, a critical point of evaluation is the extent of its novelty and extension over prior work (SAM 1 & 2). The contributions can be broadly categorized into three areas: (1) task definition, (2) data benchmark, and (3) model design.\n\n1. On the task definition: The paper extends promptable visual segmentation to promptable concept segmentation. However, the integration of text prompts (in addition to conventional interactive prompts) has already been explored by numerous existing works [1,2,3]. This makes the novelty of this specific task definition, particularly for the SAM series, appear limited.\n1. On the data benchmark: What are the specific differences and unique innovations of the data engine workflow (Sec. 4) compared to the pipelines used for SAM 1 & 2? The primary distinction appears to be the incorporation of text, which seems like a necessary adaptation for the new task rather than a fundamental innovation in the data engine itself. Furthermore, existing works [1,2,3] have also constructed large-scale datasets for the similar tasks. The paper fails to adequately discuss or differentiate its data collection process (in terms of workflow and details) from these prior efforts.\n1. On the model design: Existing research [4] has shown that SAM models can be sensitive to perturbations in interactive prompts. This paper also lacks an investigation into this aspect. How does SAM 3 perform when faced with perturbations in its text or visual prompts, such as variations in phrasing (expression deviations) or positional shifts (spatial deviations)?\n\nREF:\n1. Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos, 2025\n1. VoCap: Video Object Captioning and Segmentation from Any Prompt, 2025\n1. Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos, 2025\n1. Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes, 2024"}, "questions": {"value": "1. What is meant by the use of \"group masks\" as mentioned in Lines 246-248?\n1. For the evaluation of zero-shot capabilities, the authors are advised to include a performance comparison on the MESS [1] benchmark. This benchmark covers a diverse range of target domains, which would provide a more comprehensive perspective on the model's generalization abilities.\n1. The paper restricts its analysis to specific object counting benchmarks. This ignores the long-standing, highly challenging, and practical domain of crowd counting [2] (e.g., on datasets like UCF-QNRF,  JHU-Crowd, ShanghaiTech, or UCFCC50). These dense, highly-occluded scenarios are the true stress test for any model claiming robust counting abilities.\n\nREF:\n1. What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation, 2023\n1. Revisiting crowd counting: State-of-the-art, trends, and future perspectives, 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uPJxgLdo6M", "forum": "r35clVtGzw", "replyto": "r35clVtGzw", "signatures": ["ICLR.cc/2026/Conference/Submission4183/Reviewer_ma9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4183/Reviewer_ma9q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735400414, "cdate": 1761735400414, "tmdate": 1762917218907, "mdate": 1762917218907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SAM 3, an advanced segmentation foundation model aiming to perform “Prompted Concept Segmentation (PCS)” tasks, where text and/or image exemplars serve as prompts for concept understanding and object segmentation. The system integrates large-scale data, modular architecture design, and training techniques into a unified framework, showing strong segmentation performance across multiple benchmarks. The paper presents extensive dataset construction and evaluation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Engineering Excellence and Practical Impact.\nSAM 3 demonstrates strong engineering quality and productization potential, similar to recent systems such as DeepSeek. Its framework and modular tools are robust and well implemented, showing strong potential for deployment and for industrial or cross-domain applications.\n\n2. Comprehensive Dataset and Benchmarking.\nThe data engine and dataset organization are well executed, standardized, systematic, and clear. The large-scale data collection and clearly defined evaluation pipeline demonstrate solid engineering effort.\n\n3. High Performance through Scalable Architecture.\nThe model achieves impressive segmentation results through a reasonable architectural combination, large-scale data, and effective training strategies.\n\n4. Potential for Broader Impact.\nThe SAM 3 Agent and its associated tools are solid and stable. With additional validation in medical, industrial, or molecular domains, this work could be even more suitable for a Nature-level publication due to its cross-domain applicability and engineering completeness."}, "weaknesses": {"value": "1. Limited Novelty Beyond Existing Referring and Open-Vocabulary Segmentation Frameworks. While SAM 3 extends promptable segmentation to a broader “concept-level” scope, its core mechanism remains largely similar to existing referring and open-vocabulary segmentation approaches. The main novelty lies in system integration, data scaling, and multimodal prompting, rather than in algorithmic or conceptual innovation. \nThe paper’s strength is its engineering completeness and potential impact, but from a research novelty standpoint, its incremental contribution beyond prior referring segmentation frameworks (e.g., CLIPSeg (CVPR'22 [https://arxiv.org/abs/2112.10003]), SEEM (NeurIPS'23 [https://arxiv.org/pdf/2304.06718]), Grounded-SAM & Grounaded 2 (arXiv [https://arxiv.org/abs/2401.14159]) appears limited.\n2. Lack of Theoretical Insight. The paper appears more like a large-scale project than a conceptual or methodological contribution. The performance improvements mainly come from scaling data and model size, rather than introducing new conceptual insights or algorithmic innovations.\n3. Limited Definition and Discussion of “Concept”. \nThe notion of “concept” is underexplained. Since “concept” is an abstract idea, the paper should explicitly define it and clarify how PCS differs from existing works such as Spider (ICML'24 (https://arxiv.org/abs/2405.01002)) and SAM-Eva (arXiv (https://arxiv.org/abs/2412.01240)), which already distinguish between CI (context-independent) and CD (context-dependent) concepts. Without a clear definition, the term “Prompt Concept Segmentation” remains ambiguous. I suggest that the authors expand the Related Work section to provide a more detailed explanation of how “concept” has been defined and studied in prior literature. It would also be valuable to include additional experiments to demonstrate how SAM 3 performs across different types of concepts, particularly distinguishing between context-independent and context-dependent cases.\n4. Task Limitation (PCS Inference Scope). The PCS task currently handles single-image or text-based prompts but lacks the ability for batch or generalizable reasoning across multiple images or complex prompts.\n5. Lack of Novelty in Data Engine. While the data pipeline is well-organized, it is largely an engineering implementation without notable methodological innovation.\n6. Questionable Data Efficiency.\nIn Table 14, the improvement from using 20% → 100% data is comparable to the smaller-scale increments (10% → 20%, etc.), suggesting poor data efficiency and underutilization of the large dataset. Moreover, there remains a significant gap between the full-data model and the teacher model, implying room for optimization in training or architecture."}, "questions": {"value": "1.    SAM 3 presents itself as a large-scale, highly engineered system integrating architecture design, massive data curation, and multi-modality prompt handling. While its engineering quality and practical completeness are impressive, I am uncertain whether such a system-level project aligns with ICLR’s focus on methodological and theoretical innovation.\n\n   * Would the authors consider that SAM 3, with its solid engineering and cross-domain applicability, might be more suitable for a Nature-type venue, where large, impactful engineering frameworks and cross-domain demonstrations are more appreciated?\n   * Has the team considered including case studies or validations in diverse domains (e.g., medical imaging, industrial inspection, materials science, or bioinformatics) to better highlight the model’s broad real-world impact?\n\n\n2.   I acknowledge the revolutionary impact of SAM (1), which fundamentally redefined segmentation as a promptable and interactive visual understanding task. SAM 2 further extended this to the temporal domain through hierarchical memory and video modeling.\n   However, SAM 3 appears to be more of a task-level extension (image -> video -> concept) rather than a technical breakthrough.\n\n   * The progression of input and output forms (point or box to frame sequence to text or image exemplars; single-frame masks to temporal sequences to concept-level masks) seems evolutionary rather than fundamentally new.\n   * It is difficult to evaluate SAM 3’s novelty in isolation from SAM (1) and SAM (2). Its advances appear to rely heavily on the established SAM framework and infrastructure.\n   * If the entire SAM series (1, 2, and 3) were viewed as a single long-term contribution, I would rate it extremely high, possibly a 10/10. For SAM 3 alone, however, the incremental contribution ratio remains unclear. Could the authors clarify what specific conceptual or architectural innovations distinguish SAM 3 from its predecessors beyond scaling and multi-modality integration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MkUVfCPHlv", "forum": "r35clVtGzw", "replyto": "r35clVtGzw", "signatures": ["ICLR.cc/2026/Conference/Submission4183/Reviewer_N5F4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4183/Reviewer_N5F4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797387958, "cdate": 1761797387958, "tmdate": 1762917218526, "mdate": 1762917218526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAM3, a unified model that can perform detection on images and tracking across frames based on given concepts or prompts. The main contributions are twofold: proposing the SAM3 method and introducing a data engine pipeline. Unlike SAM2, it can detect multiple objects based on given concepts, extending its capabilities. Furthermore, by incorporating MLLMs, it produces more precise and higher quality datasets. The results demonstrate strong generalization ability across various scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Exhaustive Analysis\n  - The paper demonstrates its effectiveness through extensive analyses provided in the appendix.\n  - Each component is evaluated with ablation studies, and the paper even illustrates the impact of AI verification through various experiments.\n  - For both image and video datasets, the importance of each and the effects of different dataset sizes are thoroughly explored.\n- Open-sourcing\n  - The paper open sources key components, including SAM3 and the SA-Co benchmark. In particular, releasing the SA-Co benchmark contributes to the perception research community by providing challenging and diverse samples."}, "weaknesses": {"value": "- Unclear Definition of Terms\n  - Throughout the paper, the term **geometric** is frequently used. What is its exact meaning here? At times, *geometric* is distinguished from *visual prompts*, but in other cases, it seems to include their meaning. Although it represents an important concept in the paper, its usage is quite vague."}, "questions": {"value": "- Motion-aware Concept\n  - In SAM3, the main focus is on simple noun phrases. For more complex or longer phrases, the paper demonstrates the use of MLLMs to handle such cases. However, in video segmentation, object descriptions often incorporate motion-aware information. In such cases, which approach could be used? Could the SAM3 Agent design still be applied to such scenarios? The current design seems to use the tracking module only to associate objects across frames in a semantic-agnostic manner."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HmudHFyNzR", "forum": "r35clVtGzw", "replyto": "r35clVtGzw", "signatures": ["ICLR.cc/2026/Conference/Submission4183/Reviewer_EmE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4183/Reviewer_EmE5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988496682, "cdate": 1761988496682, "tmdate": 1762917218287, "mdate": 1762917218287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}