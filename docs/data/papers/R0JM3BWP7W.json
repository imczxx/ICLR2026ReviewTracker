{"id": "R0JM3BWP7W", "number": 17104, "cdate": 1758272227849, "mdate": 1759897197411, "content": {"title": "Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "abstract": "Reinforcement learning (RL) for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for applying RL techniques and a fragmented understanding of their underlying mechanisms. In addition, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we show that a minimalist combination of two techniques can unlock the learning capability of critic-free policies with a vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies such as GRPO and DAPO.", "tldr": "", "keywords": ["Large Language Models Reasoning; Reinforcement Learning; Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f462b9cdb047378081519a38909e9848f90989c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shares a systematic review of variations of experimental settings in LLM-RL training to provide a unified guideline for practitioners. The review includes analysis on advantage normalization, PPO clipping and loss aggregation. In each category, the paper provides empirical results with different model sizes and dataset difficulty and a practical recommendation based on the result. Finally, the paper proposes Lite PPO, a combination of two techniques (group-mean/batch-std advantage normalization and token-level loss aggregation), improves performance of non-aligned LLM models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and comprehensive. Contributions are clearly stated in introduction and supported by experiments. Addressing the lack of a standard guideline in the LLM-RL field is important to allow practitioners to understand choice of techniques. From this point of view, this paper tackles an important problem in this domain. Overall, the experimental results are well conducted and the proposed Lite PPO algorithm is an natural extension to GRPO based on the findings provided in this paper."}, "weaknesses": {"value": "Although there isn't obvious flaws in the paper, there are comments to improve the quality of the analysis further.\n- In Section 4.2.3, explanation of why 8B model doesn't have \"scaling law\" of the upper bound clipping parameter strengthens this analysis. Analysis on trends of LLM's outputs might help explain this.\n- In Section 4.4.1, there is a lack of explanation around what leads to the experimental results shown in Figure 10 and 11. What learning dynamics influences these result? This analysis seems vital to understand the choice of overlong filtering.\n\n\nMinor grammatical errors:\n- Line 388, `As illustrated in As shown in` -> `As illustrated in`"}, "questions": {"value": "- Computation cost is also another important dimension when people select techniques. Do authors have any insight around this? I imagine that the most of techniques analyzed in this paper won't impact the computing performance though."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D346pcPnkX", "forum": "R0JM3BWP7W", "replyto": "R0JM3BWP7W", "signatures": ["ICLR.cc/2026/Conference/Submission17104/Reviewer_MsEF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17104/Reviewer_MsEF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632652353, "cdate": 1761632652353, "tmdate": 1762927106782, "mdate": 1762927106782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic and rigorous evaluation of reinforcement learning (RL) techniques for improving reasoning capabilities in large language models (LLMs). The authors address the current fragmentation in RL4LLM methodologies by reproducing and analyzing popular RL \"tricks\"—such as normalization, clipping, loss aggregation, and filtering—under a unified framework. Through extensive experiments across diverse model sizes (Qwen3-4B/8B), architectures (base vs. aligned), and dataset difficulties (easy/medium/hard), the study offers actionable insights into the mechanisms and applicability of each technique. A key contribution is the proposal of Lite PPO, a minimalist combination of two techniques (group-mean + batch-std normalization and token-level loss aggregation), which outperforms more complex methods like GRPO and DAPO in critic-free policy optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Comprehensive and Reproducible Evaluation: The paper leverages a unified open-source framework (ROLL) and over 160 independent experiments, ensuring robust and statistically meaningful conclusions. The ablation studies (e.g., standard deviation removal in normalization, clip-bound scaling laws) are particularly insightful.\n\n2. Practical Guidelines: The authors translate empirical findings into clear, scenario-specific recommendations (e.g., token-level loss for base models, sequence-level for aligned models), addressing a critical need for standardization in RL4LLM.\n\n3. Minimalist Innovation: Lite PPO demonstrates that simplicity can outperform heavily engineered methods, challenging the trend of over-complication and offering an efficient baseline for future work."}, "weaknesses": {"value": "1. Experiments are confined to Qwen-family models and mathematical reasoning tasks. While math is a common benchmark, broader validation on diverse domains (e.g., code generation, commonsense reasoning) would strengthen claims of generalizability.\n\n2. While Lite PPO is promising, more ablation is needed to disentangle the contributions of its two components (normalization vs. loss aggregation) across all settings."}, "questions": {"value": "How might your guidelines adapt to non-mathematical tasks? Are there techniques whose effectiveness is highly domain-dependent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UMsCc0omq2", "forum": "R0JM3BWP7W", "replyto": "R0JM3BWP7W", "signatures": ["ICLR.cc/2026/Conference/Submission17104/Reviewer_qNU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17104/Reviewer_qNU9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920277517, "cdate": 1761920277517, "tmdate": 1762927106322, "mdate": 1762927106322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a systematic evaluation of implementation choices for RL with LLMs, covering advantage-normalization variants, clipping with a higher upper bound, loss aggregation at the token vs. sequence level, and overlong-response filtering, within a unified PPO-style framework. Experiments are run on math-reasoning benchmarks using Qwen3-4B/8B (both base and aligned variants). From these studies, the authors distill seven empirical takeaways and introduce a minimalist recipe, Lite PPO, which pairs group-mean with batch-std advantage normalization and token-level loss under a vanilla PPO objective without a critic. On base models, Lite PPO achieves consistent gains over GRPO and DAPO across several math datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Organizes scattered RL techniques into a coherent, condition-aware evaluation (base vs. aligned; easy vs. hard), yielding concrete and readable takeaways.\n- Provides careful ablations and useful diagnostics (e.g., entropy and ratio behavior; token-level analyses) that improve interpretability of clipping and aggregation choices.\n- Delivers a simple, reproducible recipe (Lite PPO) that reduces complexity yet performs strongly on base models, offering immediate practical value."}, "weaknesses": {"value": "- The contribution of this paper is mainly an empirical synthesis of known implementation choices rather than an algorithmic or theoretical advance; no new RL objective or formal analysis is introduced, which constrains the paper’s originality compared to prior work\n- If the practical objective is the strongest final model, the focus on base models leaves uncertain whether the proposed recipe yields meaningful gains for aligned/instruction-tuned models that start from stronger baselines.\n- Evidence is confined to math reasoning; presenting the work as a general “roadmap” for RL with LLMs risks over-generalization without results on other reasoning modalities (e.g., logical, strategic, open-ended)."}, "questions": {"value": "- What motivated restricting experiments to Qwen3? Could you report at least a final-recipe run on another family (e.g., Llama or Mistral) to assess portability?\n- Can you apply the Lite PPO recipe to aligned models and compare against strong aligned baselines to clarify its utility when the goal is the best final system?\n- Several techniques and experimental patterns resemble DAPO. Could you delineate, in methodological terms, how your approach differs (objective, normalization, clipping, loss aggregation), and provide ablations isolating the incremental contribution beyond DAPO?\n- The higher-clipping “scaling law” appears to rest on a single family with limited sweep points. Do you view this as a size-dependent trend rather than a law? If not, what additional evidence (broader sizes, denser sweeps, statistical tests) supports the stronger terminology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SifMUmX10l", "forum": "R0JM3BWP7W", "replyto": "R0JM3BWP7W", "signatures": ["ICLR.cc/2026/Conference/Submission17104/Reviewer_G1m5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17104/Reviewer_G1m5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978449453, "cdate": 1761978449453, "tmdate": 1762927106034, "mdate": 1762927106034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a systematic evaluation of implementation choices for RL with LLMs, covering advantage-normalization variants, clipping with a higher upper bound, loss aggregation at the token vs. sequence level, and overlong-response filtering, within a unified PPO-style framework. Experiments are run on math-reasoning benchmarks using Qwen3-4B/8B (both base and aligned variants). From these studies, the authors distill seven empirical takeaways and introduce a minimalist recipe, Lite PPO, which pairs group-mean with batch-std advantage normalization and token-level loss under a vanilla PPO objective without a critic. On base models, Lite PPO achieves consistent gains over GRPO and DAPO across several math datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Organizes scattered RL techniques into a coherent, condition-aware evaluation (base vs. aligned; easy vs. hard), yielding concrete and readable takeaways.\n- Provides careful ablations and useful diagnostics (e.g., entropy and ratio behavior; token-level analyses) that improve interpretability of clipping and aggregation choices.\n- Delivers a simple, reproducible recipe (Lite PPO) that reduces complexity yet performs strongly on base models, offering immediate practical value."}, "weaknesses": {"value": "- The contribution of this paper is mainly an empirical synthesis of known implementation choices rather than an algorithmic or theoretical advance; no new RL objective or formal analysis is introduced, which constrains the paper’s originality compared to prior work\n- If the practical objective is the strongest final model, the focus on base models leaves uncertain whether the proposed recipe yields meaningful gains for aligned/instruction-tuned models that start from stronger baselines.\n- Evidence is confined to math reasoning; presenting the work as a general “roadmap” for RL with LLMs risks over-generalization without results on other reasoning modalities (e.g., logical, strategic, open-ended)."}, "questions": {"value": "- What motivated restricting experiments to Qwen3? Could you report at least a final-recipe run on another family (e.g., Llama or Mistral) to assess portability?\n- Can you apply the Lite PPO recipe to aligned models and compare against strong aligned baselines to clarify its utility when the goal is the best final system?\n- Several techniques and experimental patterns resemble DAPO. Could you delineate, in methodological terms, how your approach differs (objective, normalization, clipping, loss aggregation), and provide ablations isolating the incremental contribution beyond DAPO?\n- The higher-clipping “scaling law” appears to rest on a single family with limited sweep points. Do you view this as a size-dependent trend rather than a law? If not, what additional evidence (broader sizes, denser sweeps, statistical tests) supports the stronger terminology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SifMUmX10l", "forum": "R0JM3BWP7W", "replyto": "R0JM3BWP7W", "signatures": ["ICLR.cc/2026/Conference/Submission17104/Reviewer_G1m5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17104/Reviewer_G1m5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978449453, "cdate": 1761978449453, "tmdate": 1763725040040, "mdate": 1763725040040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical analysis of components of efficient RL pipelines for LLMs (in particular GRPO- and DAPO-style techniques) on both non-aligned and aligned Qwen3 models. The authors study the impact of data difficulty, advantage calculation and normalization, clipping strategies, loss aggregation granularity, and reward shaping via overlong filtering, all under a unified PPO-based setup. Based on these observations, they propose LitePPO for non-aligned models, which uses group-level mean and batch-level standard deviation for advantage normalization, together with token-level loss aggregation. Experiments on Qwen3 4B and 8B non-aligned models evaluated on Math500, OlympiadBench, AMC23, Minerva Math, AIME24, and AIME25 show that LitePPO outperforms GRPO and DAPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Systematic and fairly extensive analysis of the effect of data difficulty, advantage normalization (including std/no-std variants), clipping strategies, loss aggregation, and overlong filtering on RL performance for LLMs.\n\n- Evaluation covers both non-aligned and aligned model variants and two parameter scales (4B, 8B), which makes the conclusions more convincing than single-model studies.\n\n- The paper is clearly written and well structured; the individual “takeaways” for each component are easy to follow and practically useful.\n\n- The proposed LitePPO recipe is simple yet effective."}, "weaknesses": {"value": "- Statistical robustness / variability:\n\n    - It is not clear how many random seeds are used; Appendix C suggests a single seed (seed=42). If this is the case, the results are potentially sensitive to randomness.\n\n    - Showing mean and variance over multiple runs (e.g., multiple seeds) would make the empirical conclusions stronger, especially in Figure 12. Even if re-running all experiments is too expensive, clearly stating the number of runs and acknowledging this limitation would help.\n\n- Minor typo:\n    - Line 388: “As illustrated in As shown in Figure 9...”, remove one of “As illustrated in / As shown in”."}, "questions": {"value": "1. For Figures 3, 4, 6, 8, 9, 10, and 12, could you clarify precisely what the y-axis “accuracy” refers to? Is it the average accuracy across all six evaluation benchmarks, or a subset, or a particular held-out split from the Easy / Medium / Hard training datasets themselves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rUBWurzab4", "forum": "R0JM3BWP7W", "replyto": "R0JM3BWP7W", "signatures": ["ICLR.cc/2026/Conference/Submission17104/Reviewer_rKt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17104/Reviewer_rKt2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762803249513, "cdate": 1762803249513, "tmdate": 1762927105654, "mdate": 1762927105654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}