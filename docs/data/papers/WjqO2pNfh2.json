{"id": "WjqO2pNfh2", "number": 972, "cdate": 1756825916039, "mdate": 1763076733261, "content": {"title": "From Silence to Sound: Towards Audio-Visual Subject Customization", "abstract": "We introduce a novel audio-visual subject customization task that generates videos featuring user-defined characters, emphasizing both visual and audio dimensions. A key challenge is mitigating the gap between visual synthesis and audio learning. To tackle this, we propose VauCustom (Video-Audio Custom), a two-stage method that leverages zero-shot text-to-speech to create personalized audio, and then conditions video synthesis on this audio to unify audio and visuals. During training, we design a decoupled audio-visual learning strategy that models character appearance independently before joint training, thereby preserving the visual fidelity of pre-trained text-to-video models. In addition, we propose a local classifier-free guidance mechanism tailored for audio, which selectively emphasizes character regions based on cross-attention similarity, enhancing audio-visual synchronization while reducing the impact on irrelevant background regions. Experiments demonstrate that VauCustom delivers consistent character appearance, natural audio quality, and precise audio-video synchronization across diverse scenarios, including real humans, animated human characters, and animal characters. We will release all data, code, and models to support future research.", "tldr": "We present a new task termed audio-visual subject customization, and propose VauCustom, a method designed to generate videos of user-defined characters with consistent visuals and natural audio.", "keywords": ["Video Generation", "Audio-Visual Subject Customization", "Decoupled Learning", "Classifier-Free Guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e5e0427561bece7061e71e46d74bc676c4345d4d.pdf", "supplementary_material": "/attachment/7301d821d5548a36dc427706c1e9ae3e5027451e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new task, audio-visual subject customization, as well as a new system, VauCustom, a two-stage pipeline (zero-shot TTS → audio-conditioned T2V) with a four-step training schedule (dataset LoRA->audio cross-attn->subject LoRA->joint finetune) plus region-selective audio CFG. On a 60-identity benchmark including (human, animated human and animated animal), they compare against several prior baselines in metrics in terms of both semantic alignment and audio-visual synchronization; qualitative/quantitative results against portrait-animation baselines favor the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is mostly clear, and with straightforward pipeline illustrations.\n\n2. Region-selective audio CFG is a neat and novel inference-time solution that improves lip-sync while limiting background artifacts; and visualizations in the attention mask part is very intuitive.\n\n3. The experiments part is exhaustive, and it includes detailed main experiments and ablation studies. The metrics cover both “semantic” (Text/Video CLIP), identity (DINO), dynamics/consistency, and sync (Sync-C/D). Audio quality metrics (SIM-o, WER, UTMOS) are provided as shown in supplementary material."}, "weaknesses": {"value": "1. The main concern for me is the complication of 4 sub-stage training for audio-conditioned video generation part, it seems that in order to adapt to each character, a character specific lora (sub staged) must be finetuned and then jointly tune the audio cross-attention part (sub stage 4). What's the typical finetuning time for each new character in order to get a good enough checkpoint for sampling? \n\n2. The paper should compare prior works on audio-condition video generation methods (e.g., AudCast (also missing citation since very relevant) ) and joint text to video audio generation (different variants of AV-DiTs such as JarvisDiT).\n\n3. The scope of the paper is still limited to talking head portrait animation scenario, which I think existing research is pretty much well-established in this domain. To really showcase the capability of the method, the paper needs to demonstrate its capbility not only just control the face part of the portait, but the body motions and even background. Though I understand that this is out of scope for this paper, this will mark more convincing achievements. \n\n4. How does the proposed method generalize to out of domain in-the-wild videos with strovnger visual variations in both foreground facial region and background area, I think a small eval set to test its in-the-wild generalization capability will bring more impact to the method."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eUVRn79BIE", "forum": "WjqO2pNfh2", "replyto": "WjqO2pNfh2", "signatures": ["ICLR.cc/2026/Conference/Submission972/Reviewer_svjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission972/Reviewer_svjj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892687911, "cdate": 1761892687911, "tmdate": 1762915650730, "mdate": 1762915650730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pMThrQNttx", "forum": "WjqO2pNfh2", "replyto": "WjqO2pNfh2", "signatures": ["ICLR.cc/2026/Conference/Submission972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission972/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763076732537, "cdate": 1763076732537, "tmdate": 1763076732537, "mdate": 1763076732537, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced a new audio-visual subject customization task that generates videos featuring user-defined characters, emphasizing both visual and audio dimensions. A key challenge is mitigating the gap between visual synthesis and audio learning. To tackle this, it proposed VauCustom (Video-Audio Custom), a two-stage method that leverages zero-shot text-to-speech to create personalized audio, and then conditions video synthesis on this audio to unify audio and visuals. The decoupled learning strategy intelligently handles the challenge of integrating audio control into pre-trained visual models without sacrificing appearance fidelity, while the region-selective CFG solves the artifact problem associated with global audio conditioning. Experiments demonstrate that VauCustom delivers consistent character appearance, natural audio quality, and precise audio-video synchronization across diverse scenarios, including real humans, animated human  characters, and animal characters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Define the audio-visual subject customization task and establish the first benchmark,  featuring diverse subjects such as real humans, animated humans, and animal-style characters, along with standardized metrics for appearance, audio quality, and synchronization.\n\n+ It presents VauCustom, a two-stage framework integrating zero-shot TTS audio with audio-conditioned video synthesis to achieve synchronized audio visual customization.\n\n+ It introduced tailored strategies, including a decoupled audio-visual learning method to preserve visual fidelity and a region-selective audio CFG mechanism to improve lip–audio synchronization, leading to superior results across diverse scenarios."}, "weaknesses": {"value": "-- The proposed task is interesting but its real-world application scenarios should be more clear.\n\n-- The proposed methods are incremental. The two-stage pipeline relies heavily on the zero-shot TTS model for the initial audio generation. Any limitation or artifact in the generated audio will inevitably cascade as a performance ceiling for the final video synthesis and synchronization. The paper notes the use of an open-source TTS model, but does not extensively ablate the impact of different TTS quality on final sync.\n\n-- The audio quality evaluation is briefly presented in Table S1 but is only for the synthesized audio and not a direct comparison against baselines like SadTalker, Hallo3, or AniPortrait. Since the audio is generated independently in Stage 1 using Cosy Voice2, the excellent audio metrics reflect the quality of the TTS model, not the end-to-end framework's unique contribution in a comparative sense.\n\n-- The pipeline relies heavily on the zero-shot TTS model for the initial audio generation. Any limitation or artifact in the generated audio (Speaker Similarity) will inevitably cascade as a performance ceiling for the final video synthesis. The paper notes the use of an open-source TTS model, but does not extensively ablate the impact of different TTS quality on final sync."}, "questions": {"value": "Please address my major concerns as listed in the weakness section."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The primary ethical concern is the paper's contribution to technologies capable of generating highly realistic, customized videos featuring user-defined characters with synchronized speech. This capability inherently carries the risk of misuse for creating misleading or harmful content, specifically deepfakes."}}, "id": "0ktxaVGn5y", "forum": "WjqO2pNfh2", "replyto": "WjqO2pNfh2", "signatures": ["ICLR.cc/2026/Conference/Submission972/Reviewer_zYdy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission972/Reviewer_zYdy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101650280, "cdate": 1762101650280, "tmdate": 1762915650522, "mdate": 1762915650522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the novel task of audio-visual subject customization: generating a short video clip of a character along with their speech, conditioned on a visual scene text description, a dialog line and a set of exemplar clips from the aforementioned character.\n\nThe paper claims the following contributions:\n- The definition of the task along with the release of a standard evaluation benchmark. The proposed comprehensive benchmark covering real humans, animated human characters, and animal-style characters, along with standardized evaluation protocols on three dimensions: character appearance, audio quality, and audio-visual synchronization.\n- The VauCustom framework, a two-stage framework involving zero-shot text-to-speech followed by video-generation conditioned on generated audio to achieve synchronized audiovisual customization.\n- Two model training and inference strategies (decoupled audio-visual learning, region-selective audio CFG mechanism) meant to preserve visual fidelity and improve lip-audio synchronization.\n\nCompared with previous state-of-the art, this paper addresses the overlooked aspect of character speech, including both lip movements and synchronized audio.\n\nThe video generation DiT model (pretrained WAN) is conditioned on both the pre-generated audio of the target character and a text description of the video. The model is finetuned in 4 stages:\n- Appearance LoRA on talking face dataset.\n- Audio cross attention on talking face dataset.\n- Customize appearance LoRA on reference videos of the target character.\n- Joint audio cross attention and appearance finetuning on reference videos.\n\nThe decoupled training strategy is meant to prevent the audio conditioning modules to inadvertently learn appearance-specific biases.\n\nThe inference pipeline first employs the pretrained CosyVoice 2 model to generate the target speech sample conditioned on the dialogue line and an expressivity prompt. The the video generation model generates the video clip conditioned on the audio and a text description of the scene.\n\nThe proposed benchmark consists in 60 characters, 10 clips per character for training, 8 for evaluation. 480 test samples total.\n\nThe proposed VauCustom framework is compared to SadTalker, AniPortrait and Hallo3 (visual-only models that do not support audiovisual alignment). On the proposed benchmark, VauCustom outperforms the baselines across the board. It notably shines for motion dynamics (much more dynamic scenes than baselines) and audio-visual synchronization (thanks to the audio-conditioning focus of this paper).\n\nOverall the novelty and significance of this paper make it a good candidate for acceptance. However the paper's quality is hindered by several lacks of details and experiments supporting the design choices (audio tokens, trigger word?)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper in originality: the definition of a novel task along with the proposal of a related benchmark.\n\nThe proposed method is well supported by several qualitative examples that also include baseline systems."}, "weaknesses": {"value": "The proposed method requires more than two stages. The video generation model requires two generic finetuning stages followed by two subject-specific finetuning stages on the exemplar clips of the target character (visual only, then joint).\n\nSeveral details are missing to enable reproducibility.\n- The authors claim they will release code, data and models but as of today this is not the case.\n- Some details are missing regarding the different finetuning stages (learning rates, batch size...).\n- The binarization parameters of the cross attention maps are missing.\n- Some design choices lack justification or experimental support (choice of trigger words, learnable additional audio tokens, cfg ablations)."}, "questions": {"value": "- What is the exact role and design of the learnable audio tokens in the stage 3.d? \n- What does “vector” mean in the Figure 4?\n- What are the binarization parameters in the Figure 4? How are they chosen?\n- Why are the cross attention map activations taken from the interaction with a text trigger word instead of the audio token projections?\n- As CFG is a core contribution of this paper, I would expect some CFG ablation curves somewhere in the paper.\n- Are the last two finetuning stages run independently on each target character data (creating one finetuned model per character) or on the whole set of 60 characters?\n- How does the model behave on out of distribution (unseen) characters?\n\nTypo in the Table 1 (\"consistency\")."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SuLeomkDEl", "forum": "WjqO2pNfh2", "replyto": "WjqO2pNfh2", "signatures": ["ICLR.cc/2026/Conference/Submission972/Reviewer_VGNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission972/Reviewer_VGNM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129630712, "cdate": 1762129630712, "tmdate": 1762915650246, "mdate": 1762915650246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}