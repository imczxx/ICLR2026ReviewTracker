{"id": "ACWBlCfcVY", "number": 6282, "cdate": 1757964426076, "mdate": 1759897924999, "content": {"title": "DA-MergeLoRA: Hypernetwork-Based LoRA Merging for Few-Shot Test-Time Domain Adaptation", "abstract": "Few-shot Test-Time Domain Adaptation (FSTT-DA) seeks to adapt models to novel domains using only a handful of unlabeled target samples. This setting is more realistic than typical domain adaptation setups, which assume access to target data during source training. However, prior FSTT-DA approaches fail to effectively leverage source domain-specific knowledge, relying on shallow batch normalization updates, prompt-based methods that treat the model as a black box, or ensembling strategies that do not capture cross-domain relationships. To address these limitations, we introduce a new FSTT-DA framework that integrates LoRA fine-tuning with model merging. In our approach, separate LoRA modules are fine-tuned on CLIP’s vision encoder for each source domain. Since LoRA modifies only a small fraction of the model’s parameters, it retains the base model’s generalized knowledge while internally learning domain-specific features. To adapt the learned knowledge to a specific target domain, we propose a hypernetwork trained via meta-learning that generates per-column merging factors to combine LoRA modules. Given a small batch of target images, the hypernetwork produces merging weights that fuse source LoRA modules into a single adapted representation. Our results demonstrate state-of-the-art performance across various domain adaptation datasets.", "tldr": "", "keywords": ["Model Merging", "LoRA", "CLIP", "Few-shot Test-Time Domain Adaptation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f680d5735e0afbb5a189f631d3bc893a6de512dc.pdf", "supplementary_material": "/attachment/f3223b53ba34eb7a9dcf15b554e4b696895ab181.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of few-shot test-time domain adaptation.\nThe authors train a LoRA adapter for each source domain on top of a fixed CLIP backbone model.\nThey then introduce a hypernetwork that determines how to combine these adapters with appropriate weights.\nThe developed model demonstrates superior performance compared to baseline methods across various datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a straightforward and simple strategy which can be an effective solution.\nI also appreciate the approach of using LoRA to learn the specific characteristics of each source domain while effectively preserving the backbone knowledge, and then combining them using hypernetwork.\nThe performance improvement over existing methods appears to be clear.\nI think ablation study is thorough convincingly demonstrating the advantages of each component."}, "weaknesses": {"value": "The main weakness of this paper lies in its presentation.\n\nIn my opinion, the paper is written in a way that shows little consideration for the reader in many parts.\n\n- The font size in the figures is too small to be readable after printing.\n- The explanations for the main tables are insufficient, forcing readers to repeatedly refer back to the details to fully understand them. In lines 368–369, when reporting results, the authors describe the results of the datasets in reverse order—from DomainNet to iWildCam—which makes it difficult to follow immediately.\n- Even when reading the Introduction and Related Work sections, the key ideas that differentiate this paper are not clearly or immediately conveyed.\n- There are no qualitative examples at all, and only quantitative evaluations are presented, making it hard to grasp how the proposed method actually works in practice.\n- Furthermore, beyond benchmarking, the paper lacks discussion or demonstrations of how the proposed method could be **applied** in practice. For example, it would be convincing to show qualitative examples illustrating that the model generalizes well to the sketch domain even without training on sketch data—but such demonstrations are missing.\n\nOverall, while I think the paper’s **straightforward and simple approach** is a strength, the **practical advantages** are not clearly shown, making the **contribution feel weak**. For these reasons, I am leaning toward the **reject** side."}, "questions": {"value": "At test time, does the model need the 16 target-domain images (batch size) and a target image?\n\nIn some cases, the target domain may be extremely limited, containing only one to three images. I would like to know how robust the model is to variations in the number of available target-domain samples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8aQM8FjIvL", "forum": "ACWBlCfcVY", "replyto": "ACWBlCfcVY", "signatures": ["ICLR.cc/2026/Conference/Submission6282/Reviewer_gQbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6282/Reviewer_gQbd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887767530, "cdate": 1761887767530, "tmdate": 1762918590918, "mdate": 1762918590918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Few-Shot Test-Time Domain Adaptation problem, where models must adapt to unseen target domains using only a few unlabeled samples. The authors propose a framework that combines LoRA modules trained on source domains into a single target-specific representation via a hypernetwork-based merging strategy. This hypernetwork, trained using meta-learning, predicts per-column merging weights to fuse LoRA modules conditioned on target-domain samples. The approach aims to address the limitations of existing FSTT-DA methods, such as shallow updates, restricted knowledge transfer, and inefficiencies. The paper claims state-of-the-art performance on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The Few-Shot Test-Time Domain Adaptation setting reflects real-world challenges, where target domain data is scarce and unlabeled. The proposed approach aligns well with this problem and offers a potentially efficient solution through LoRA merging.\n\n2. By leveraging LoRA, which modifies only a small subset of model parameters, the method achieves parameter-efficient adaptation. The use of a hypernetwork for merging LoRA modules provides a novel mechanism for domain adaptation without requiring full fine-tuning or ensembling.\n\n3. The paper evaluates DA-MergeLoRA across multiple datasets and compares it to a range of baselines, including SOTA FSTT-DA methods.\n\n4. The parameter-space merging formulation is grounded in prior work on LoRA and model merging, and the use of per-column merging with cross-attention is a reasonable design for fine-grained adaptation.\n\n5. The paper is well written and easy to read."}, "weaknesses": {"value": "1. While the combination of LoRA and hypernetwork-based merging is novel in the context of FSTT-DA, the individual components (LoRA fine-tuning, hypernetwork-based weight generation, meta-learning) are well-established in prior works. The paper does not demonstrate significant conceptual advances beyond adapting these techniques for FSTT-DA.\n\n2. The method relies heavily on hyperparameters. As shown in sensitivity analyses, improper tuning can degrade performance significantly, limiting the robustness of the approach.\n\n3. The target domains are simulated by masking pseudo-target LoRA modules during training. This setup may not reflect the complexity of real-world domain shifts, where target domains may exhibit more nuanced differences from source domains.\n\n4. The merging weights generated by the hypernetwork are not interpretable. There is no analysis of why certain LoRA modules are weighted more heavily or how the merging process aligns with the target domain characteristics."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M8yHVmfxtZ", "forum": "ACWBlCfcVY", "replyto": "ACWBlCfcVY", "signatures": ["ICLR.cc/2026/Conference/Submission6282/Reviewer_D3hq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6282/Reviewer_D3hq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888014477, "cdate": 1761888014477, "tmdate": 1762918590245, "mdate": 1762918590245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes DA-MergeLoRA for few-shot test-time domain adaptation (FSTT-DA) using a frozen CLIP ViT‑B/16 vision encoder augmented with LoRA adapters trained per source domain. It combines LoRA fine-tuning and model merging to efficiently adapt pre-trained VLMs like CLIP to target domains using only a few unlabeled target samples. Each source domain is first fine-tuned with its own LoRA module, which captures domain-specific knowledge while keeping the CLIP backbone frozen to preserve generalization. To adapt to a target domain, the method employs a meta-learned hypernetwork that predicts per-column merging weights to fuse the source LoRA modules into a single target-specific adapter. Merging in the parameter space allows efficient cross-domain knowledge transfer without retraining or ensembling. The proposed FSTT-DA method is validated on DomainNet, Camelyon17, FMoW, and iWildCam classification benchmarks, and results show that it  can outperform against SOTA methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Overall, the paper is clearly written, well organized, and easy to follow. It presents a clear motivation and methodology. DA-MergeLoRA trains a LoRA adapter per source domain on a frozen CLIP encoder and, at test time, uses a meta-learned hyper-network conditioned on a small batch of target images to produce per-column merge weights that combine these adapters into a single target-specific model. Parameter-space merging avoiding multi-expert ensembling, \n+ The paper formulates FSTT-DA as a parameter-space merging problem, and relies on the hypernetwork to combine LoRA adapters trained on multiple source domains. \n+ DA-MergeLoRA represents a cost-effective alternative to traditional fine-tuning or ensembling methods, and that may be generalized to broader adaptation settings. \n+ Empirical results show that the proposed DA-MergeLoRA method can outperform SOTA  on diverse datasets (DomainNet, Camelyon17, FMoW, and iWildCam). Ablations validate important models choices, e.g., that conditional merging on target image data can significantly improves adaptation performance. \n+ Their code is made available, and will allow the reader to reproduce the results."}, "weaknesses": {"value": "- The practical motivation for FSTT-DA appears limited. Standard TTA already addresses the challenge of adapting models to unseen domains using unlabeled target data, and most TTA methods operate with small batches or even single-sample updates. Framing a separate “few-shot” variant therefore seems largely quantitative rather than conceptually distinct, as it merely constrains the amount of target data without introducing a new adaptation principle. Moreover, the episodic few-shot setup used in current FSTT-DA studies is somewhat artificial, since in realistic deployments models typically encounter continuous or sufficiently large target data streams. As a result, the additional architectural complexity introduced by meta-learning or hypernetwork-based approaches in this setting may not be justified by a correspondingly strong real-world need or performance gain.\n- Experiments: Claims of SOTA performance are mostly supported by the tables but iWildCam protocol deviates from the main meta-training recipe with this claim (skewed class distributions limits the effectiveness of meta-training.) this argument is weak it should be supported by citation or experimental ablation to prove it. \n- The authors highlight some merging techniques in the related work. They should contrast their approach with the latest works.  The latest work discussed is 2023. \n- Although the paper positions the proposed method within the few-shot test-time domain adaptation (FSTT-DA) paradigm, its design and operational assumptions align more closely with multi-source unsupervised domain adaptation (MSUDA) or model-fusion approaches. The framework trains separate LoRA adapters for each source domain and learns a hypernetwork that merges these domain-specific adapters into a single model at inference. The “adaptation” at test time combines pretrained source experts through a meta-learned weighting mechanism. There is no gradient-based learning, self-supervised objective, or continual update occurs on the target data. Consequently, the process is closer to multi-source parameter aggregation or mixture-of-experts fusion than to genuine test-time adaptation, which traditionally involves adapting model parameters online or per-batch during deployment. From this standpoint, the method should have been evaluated against strong multi-source domain adaptation baselines rather than exclusively against few-shot or prompt-based TTA methods. \n- The proposed DA-MergeLoRA has limited novelty. Actually, I was not able identify any clear technical novelty in this work.The authors follows a similar merging technique and weights prediction as LoRA-RaR (Shenaj 2024, Yadav 2023), and expand on meta-learning-based TTA (Zhong 2022; Chi 2024).  I would encourage the authors to clarify the novelty of their contribution relative to ZipLoRA (ECCV 2024) and LoRA.rar (arXiv 2025). ZipLoRA already introduced per-column merging of LoRA adapters through optimization of column-wise coefficients. LoRA.rar extended this idea by using a hypernetwork to predict those coefficients directly, enabling one-shot merging for unseen pairs. The proposed framework appears to adopt an almost identical formulation, which is  a hypernetwork that outputs per-column merge weights, though applied to CLIP rather than diffusion models. Unless there are substantial algorithmic differences, the claimed “per-column hypernetwork-based merging” may not represent a novel contribution. I would appreciate a clearer discussion explaining what aspects are new beyond these prior works.\n- The authors’ own ablation studies reveal no advantage of the proposed per-column merging strategy over simpler per-layer alternative. The paper attributes this marginal gain to column misalignment among independently trained LoRA adapters and to the limited expressiveness of the single-head cross-attention hypernetwork. However, this explanation substantially weakens the paper’s claimed contribution. Furthermore, the paper omits direct comparisons with several closely related LoRA-merging frameworks, including MoLE (paper: Mixture of LoRA Experts), LoRAHUB, ZipLoRA, LoRA.rar, KnOTS(paper: Model merging with SVD to tie the Knots). Both ZipLoRA and LoRA.rar already implement column-wise LoRA merging (the latter using a hypernetwork for one-shot coefficient prediction), while MoLE, KnOTS , and LoRAHub address alignment, gating, and interference across LoRAs in more principled ways. Evaluating against these established baselines would have provided a much stronger and fairer assessment of the proposed approach’s effectiveness. In their absence, the contribution appears unsignificant, with limited evidence that the proposed merging granularity or architecture yields genuine improvements beyond existing LoRA fusion techniques.\n- The authors do not discuss the computational or memory implications of multiple multiple domain-specific LoRA experts and a meta-trained hypernetwork. These factors that are central in real-world test-time adaptation, where latency and resource budgets are critical. Test-time adaptation methods are typically valued for their on-the-fly adaptability under strict time and memory constraints, whereas the proposed pipeline presupposes the availability of all source adapters and an additional inference-time hypernetwork, which may not be feasible in deployment. It is important to provide runtime or memory comparisons to justify the practicality of this approach in a genuine test-time setting. Design choice of the hypernetwork architecture (single-head cross-attention) are not analyzed. This paper should also contain an experimental analysis of time and memory complexity for hypernetwork training.\n -  Results in Table2c show a large decline. It’s hard to disentangle conditional merging helps from random, misscaled inputs hurt. I suggest removing the target-embedding branch entirely and retrain the hypernetwork under that setting, so capacity and training dynamics are comparable.\n\nReferences: \nZipLoRA — Shah et al. ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs. ECCV 2024. \nLoRA.RaR — Shenaj et al. LoRA.rar: Learning to Merge LoRAs via Hypernetworks for\nSubject-Style Conditioned Image Generation. ICCV 2025.\nTask Arithmetic — Ilharco et al. Editing Models with Task Arithmetic. ICLR 2023. \nTIES‑Merging — Yadav et al. Resolving Interference When Merging Models. NeurIPS 2023. \nKnOTS — Stoica et al. Model merging with SVD to tie the Knots. ICLR 2025.\nIso‑C / Iso‑CTS — Marczak et al. No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces. ICML 2025.\nTSV‑M — Gargiulo et al. Task Singular Vectors: Reducing Task Interference in Model Merging. CVPR 2025."}, "questions": {"value": "See my comments in weaknesses.  In my opinion, the novelty is limited, combining existing LoRA merging and meta-learning methods, while missing a detailed analysis of the hypernetwork design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BTwQci7J1B", "forum": "ACWBlCfcVY", "replyto": "ACWBlCfcVY", "signatures": ["ICLR.cc/2026/Conference/Submission6282/Reviewer_oEDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6282/Reviewer_oEDv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888106635, "cdate": 1761888106635, "tmdate": 1762918589618, "mdate": 1762918589618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}