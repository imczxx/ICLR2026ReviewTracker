{"id": "5SQbPbCU6P", "number": 20465, "cdate": 1758306462555, "mdate": 1759896976184, "content": {"title": "RS-MoE: Collaborative Compression for Mixture-of-Experts LLMs based on Low-Rank and Sparse Approximation", "abstract": "Mixture-of-Experts (MoE) based Large Language Models (LLMs), despite their computational efficiency, face significant storage and memory challenges, which hinder their deployment on edge devices. \nHowever, existing methods primarily focus on compressing at the expert level, resulting in the loss of specialized knowledge. \nTo address these challenges, we propose a novel framework termed RS-MoE, which compresses MoE models by collaboratively decomposing the weights of each expert into low-rank and sparse components. \nThrough a preliminary investigation of the relationship between activations and weights, we identified two key observations: (i) a small fraction of weight dimensions, identifiable by high activation peaks, are critical and can be treated as a sparse component, and (ii) the remaining weights, after removing these high-importance dimensions, exhibit an inherent low-rank structure.\nBuilding on this, we developed a comprehensive importance score based on activation peaks to apply a tailored policy: high-importance dimensions are sparsely preserved, while the remaining dimensions are approximated using a low-rank representation.\nAdditionally, ridge regression and mutual information techniques are incorporated to further minimize errors.\nWe performed a comprehensive evaluation of RS-MoE on several MoE LLMs, including DeepSeekMoE-16B-Base, Mixtral-8x7B, and Qwen3-30B-A3B.\nThe results demonstrate that our approach consistently outperforms existing monolithic sparse or low-rank methods across a variety of downstream tasks, highlighting its superior effectiveness and generalizability.", "tldr": "", "keywords": ["Mixture-of-Experts; Model Compression; Low-Rank Approximation; Mutual Information; Ridge Regression"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51caef7a072746ed3a5531bfd586bdf68ed823a2.pdf", "supplementary_material": "/attachment/9f3b20678d1a26808b2ab0f9c90e0ee171e7b808.zip"}, "replies": [{"content": {"summary": {"value": "RS-MoE compresses MoE experts by coupling rows/columns across Wup, Wgate, Wdown   into per-dimension “collaborative units”, then preserving high-importance units sparsely and approximating the rest with activation-aware low-rank SVD. On DeepSeek, Mixtral, and Qwen3 , RS-MoE outperforms NAEE/D2-MoE across 20–60% sparsity, with stronger robustness at high sparsity; ablations favour ACI and activation-aware SVD, and show limited sensitivity to small calibration sets"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel per-dimension collaboration across an expert’s three matrices with an activation- and influence-aware importance score and MI-guided layer budgeting is a distinct spin on MoE compression; prior works treat experts independently (prune/merge) or use single-mode (sparse or low-rank) approximations. The ridge-based base for Wdown  is a practical innovation versus Fisher merges."}, "weaknesses": {"value": "Report VRAM, latency, throughput vs baselines across batch/sequence lengths (not just tokens/sec on one set-up). Profile kernel vs packing vs NCCL. \n\nBroader calibration study moving beyond 128 WikiText-2 samples & with varied domain/size, show failure modes and confidence intervals.\n\nShow expert-utilisation entropy and gate-logit shifts pre/post compression to confirm collaborative units don’t skew routing."}, "questions": {"value": "How do VRAM and end-to-end latency change at equal sparsity vs NAEE/D2-MoE across sequence lengths/batch sizes, and what’s the one-off cost of computing ACI/MINE?\n\nHow sensitive is ACI to its weights and to activation outliers; any normalisation/stability tricks? \n\nDo collaborative units alter routing balance (utilisation entropy, tail experts) or induce mode collapse under high sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6RfaZo6wmd", "forum": "5SQbPbCU6P", "replyto": "5SQbPbCU6P", "signatures": ["ICLR.cc/2026/Conference/Submission20465/Reviewer_KXhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20465/Reviewer_KXhS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580091947, "cdate": 1761580091947, "tmdate": 1762933908913, "mdate": 1762933908913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **RS-MoE**, a collaborative compression framework for MoE LLMs that treats each expert’s three SwiGLU matrices ($(W_{\\text{up}}, W_{\\text{gate}}, W_{\\text{down}})$) as coupled “collaborative units.” The method (i) scores per-dimension importance via **Anomalous Contribution Integration (ACI)** from activation peaks and downstream alignment, (ii) preserves high-importance dimensions as sparse weights and approximates the rest with activation-aware SVD (low rank), and (iii) fits a **ridge-regressed shared base** on ($W_{\\text{down}}$) to reduce reconstruction error. Layer-wise sparsity budgets are determined by MINE-estimated inter-layer mutual information and solved with a QP allocation. Experiments on DeepSeekMoE-16B-Base, Mixtral-8×7B, and Qwen3-30B-A3B report consistent gains over recent baselines across PPL and downstream accuracy, with an efficiency/performance trade-off reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Clear structural motivation.** By decomposing experts along the SwiGLU middle dimension, the paper preserves the natural coupling between $(W_{\\text{up}})/(W_{\\text{gate}})$ rows and the matching ($W_{\\text{down}}$) column within each collaborative unit. \n\n**2. Well-integrated pipeline.** ACI for ranking, MINE-guided layer budgets, activation-aware SVD, and a ridge-regressed base on ($W_{\\text{down}}$) form a coherent end-to-end recipe.  \n\n**3. Reproducibility details.** Implementation hyper-parameters (e.g., ACI weights and ($\\gamma$)) and calibration protocol are specified."}, "weaknesses": {"value": "**1. Limited novelty (composition of mature components).**\n\nThe sparse+low-rank paradigm is well established; activation-aware SVD, and sparse/low-rank fusion have prior art. The contribution reads more as a careful systemization on MoE than a fundamentally new algorithmic idea; the paper should delineate the novelty boundary more explicitly.\n\n**2. Figure 4 vs. text claim needs reconciliation.**\nThe paper states ***“RS-MoE is more robust when only a few calibration samples are provided,”*** yet in Figure 4 the plotted curves appear to show $D(^2)$-MoE achieving lower (better) PPL at the smallest calibration sizes. Please clarify thresholds, metrics, and whether error bars or seeds alter this observation. \n\n**3. Acceleration discussion is too thin.**\nThe efficiency section reports aggregate FLOPs and tokens/sec, but lacks kernel-level explanations of how sparse blocks and low-rank factors are scheduled or fused at inference time (e.g., whether sparse GEMV and low-rank ($UV^\\top$) paths run in parallel, and whether gate/down projections can be overlapped). Detailing memory layout, operator choices, and fusion opportunities would strengthen the deployment story. \n\n**4. Notation and terminology.**\nLabels such as ***“RS-MoE100%high / RS-MoE80%high”*** are not self-explanatory in the main text. Please define them precisely (e.g., “percentage of budget allocated to sparse components”) where they first appear and in the figure/table captions."}, "questions": {"value": "See weakness.\n\nIf these issues are addressed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO or VERY MINOR ethics concerns only"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cnbQ5oyLrd", "forum": "5SQbPbCU6P", "replyto": "5SQbPbCU6P", "signatures": ["ICLR.cc/2026/Conference/Submission20465/Reviewer_BD9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20465/Reviewer_BD9Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736427900, "cdate": 1761736427900, "tmdate": 1762933908475, "mdate": 1762933908475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a well-motivated approach addressing MoE compression through collaborative decomposition that preserves expert functionality. The key insight that expert weights exhibit both sparse high-importance dimensions and low-rank structure after removing these dimensions is supported by empirical analysis (Figure 1; Sec. 3.1; p.2-3). Strong experimental results show RS-MoE achieves PPL 16.92 at 20% sparsity on DeepSeekMoE-16B-Base and 8.87 on Qwen3-30B-A3B, outperforming baselines (Tables 1-3; Sec. 4.2; p.7). However, theoretical justification for why the residual formulation improves optimization is largely intuitive without formal convergence analysis (Sec. 3.2; p.3-4). Some implementation details regarding BN placement and initialization sensitivity require clarification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear problem identification and collaborative decomposition framework**\n  - The manuscript documents activation peak distributions categorized into high/medium/low importance and demonstrates low-rank structure in whitened weights after removing high-importance components (Figure 1; Sec. 3.1; p.2), providing strong empirical motivation.\n  - The collaborative unit concept couples corresponding dimensions of Wgate, Wup, and Wdown matrices (Sec. 3.2; p.4), ensuring functional integrity. The paper provides an equation that unfolds the expert computation, which clarifies this coupling.\n  - The framework applies unified compression strategies to coupled dimensions (Sec. 3.2; p.3-4), mitigating spatial misalignment errors—important for preserving specialized knowledge.\n- **Comprehensive importance scoring and decomposition strategy**\n  - The ACI score integrates inner energy (mean, variance, peak activation) with downstream influence (alignment with subsequent layers) using a weighted combination (Algorithm 1; Sec. 3.3; p.5), providing robust importance quantification.\n  - Activation-aware SVD projects weights into activation space for decomposition (Sec. 3.4; p.5-6), improving feature information retention.\n  - Ridge regression constructs base weights to compensate for reconstruction errors (J(B) equation; Sec. 3.4; p.6), enhancing accuracy—critical for high compression ratios.\n- **Strong experimental validation with thorough ablations**\n  - Results show consistent improvements: PPL 16.92 vs. 18.19 (D2-MoE) at 60% sparsity on DeepSeekMoE; maintains 64% downstream accuracy at 20% sparsity on Qwen3 (Tables 1-3; Sec. 4.2-4.3; p.7-8), demonstrating state-of-the-art performance.\n  - Ablations validate layer-wise sparsity allocation (Table 2), sparse vs. low-rank components (Table 3), and base weight construction methods (Table 4; Sec. 4.3; p.8), supporting design choices.\n  - Robustness analysis with varying calibration samples shows superior stability compared to D2-MoE (Figure 4; Sec. 4.3; p.8), evidencing practical reliability."}, "weaknesses": {"value": "- **Limited theoretical grounding for decomposition effectiveness**\n  - The hypothesis that sparse-plus-low-rank decomposition preserves information is argued empirically; no formal analysis quantifies approximation error bounds or convergence guarantees (Sec. 3.1; p.2-3). This affects technical soundness and generalization confidence.\n  - The ACI score combines multiple heuristics ($w\\_{\\text{mean}} = 0.4$, $w\\_{\\text{var}} = 0.05$, $w\\_{\\text{peak}} = 0.8$, $\\gamma = 0.05$) without principled derivation or sensitivity analysis (Appendix A.2; p.13). Optimal hyperparameter selection lacks justification.\n  - No theoretical characterization of when collaborative decomposition outperforms independent expert compression or conditions under which low-rank structure emerges after removing sparse components.\n- **Incomplete ablation coverage and factor isolation**\n  - Roles of batch normalization, initialization schemes, and learning-rate warm-up are acknowledged but not isolated via controlled experiments (Sec. 3.4; Sec. 4.2 warm-up note; p.4,7). This impacts experimental rigor and reproducibility.\n  - Layer-wise parameter budget allocation uses quadratic programming with smoothness constraints but details are relegated to the appendix (Appendix A.3; p.13-14). The main text lacks a clear explanation of the budget determination process.\n  - Analysis of activation sparsity across different layers/experts is limited to two examples (Figures 6-7; Appendix A.4; p.14-15). Broader empirical validation would strengthen generality claims.\n- **Mathematical formulation clarity and notation consistency**\n  - The collaborative unit formulation $Y = g \\sum_{j} (\\sigma(x \\cdot W_{\\text{gate},j,:}) \\odot (x \\cdot W_{\\text{up},j,:})) W_{\\text{down},:,j}$ (Sec. 3.2; p.4) could benefit from explicit dimensionality annotations and a clearer distinction between element-wise and matrix operations.\n  - The Ridge regression objective $J(B) = \\|Y - H_c B^T\\| + \\lambda \\|B\\|$ uses an unspecified norm, and the regularization parameter λ=1e-3 lacks justification (Sec. 3.4; Appendix A.2; p.6,13).\n  - Response magnitude statistics require explicit definitions for computation points (post-BN/pre-ReLU) and aggregation procedures (Sec. 3.4; p.5). Current descriptions are informal.\n- **Reproducibility and resource reporting gaps**\n  - Implementation uses 128 Wikitext2 samples truncated to 2048 tokens on NVIDIA A800 GPUs (Sec. 4.1; Appendix A.2; p.7,13), but memory footprints, training wall-clock times, and exact hardware specs per model/depth are not reported, limiting adoption planning.\n  - Efficiency analysis (Table 5) reports FLOPs and throughput for only 60% sparsity on Mixtral (Sec. 4.4; p.8). Comprehensive efficiency metrics across models and compression ratios would strengthen practical utility claims."}, "questions": {"value": "- **Strengthen theoretical framework**\n  - Provide formal approximation error bounds for the sparse-plus-low-rank decomposition, showing the reconstruction error $\\epsilon$ as a function of sparsity ratio $s$ and rank $r$. Include proof sketches in the main text with full derivations in the appendix.\n  - Conduct a sensitivity analysis for the ACI hyperparameters $(w\\_{\\text{mean}}, w\\_{\\text{var}}, w\\_{\\text{peak}}, \\gamma)$ across different models and datasets. Provide guidelines or adaptive selection procedures based on model characteristics.\n  - Develop theoretical conditions that characterize when collaborative decomposition outperforms independent compression, possibly via an analysis of weight coupling through intermediate activations.\n- **Expand ablation studies**\n  - Execute systematic ablations isolating batch normalization (BN-on vs. BN-off), initialization schemes (Xavier vs. He vs. default), and learning-rate warm-up across multiple depths and models (analogous to Sec. 3.4; Sec. 4.2; p.4,7).\n  - Provide layer-wise analyses for projection shortcut options (A/B/C) showing gradient flow or activation statistics to explain where projections materially impact performance (extending Table 3 analysis; p.6).\n  - Conduct necessity/sufficiency tests: plain nets with BN, identity shortcuts selectively removed, and alternative decomposition strategies to validate the attribution of gains to the collaborative framework.\n- **Clarify mathematical formulations**\n  - Add explicit dimensionality annotations to all equations (e.g., specify $x \\in \\mathbb{R}^{1 \\times d}$, $W\\_{\\text{gate},j,:} \\in \\mathbb{R}^{1 \\times d}$, output dimensions). Include operator precedence diagrams for the residual block activation order (enhancing Fig. 2; Sec. 3.2; p.3).\n  - Specify the norm types in the ridge regression objective ($L\\_2$ is assumed but not stated) and provide principled selection criteria for $\\lambda$ based on model scale or data characteristics (Sec. 3.4; p.6).\n  - Formalize the response magnitude statistics: define computation points, aggregation across layers/batches, and report summary tables with confidence intervals (clarifying Fig. 7; Sec. 4.2; p.8).\n- **Enhance reproducibility reporting**\n  - Report memory usage (peak GPU memory, parameter storage), training time (wall-clock per epoch/iteration), and detailed hardware specifications for each model/depth configuration (supplementing Table 1; Sec. 3.4; p.4-6).\n  - Expand the efficiency analysis (Table 5) to cover all compression ratios (20%, 40%, 60%) across all models with metrics including FLOPs, throughput, latency, and memory footprint."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eHcxXERSRr", "forum": "5SQbPbCU6P", "replyto": "5SQbPbCU6P", "signatures": ["ICLR.cc/2026/Conference/Submission20465/Reviewer_FPVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20465/Reviewer_FPVr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865586372, "cdate": 1761865586372, "tmdate": 1762933907909, "mdate": 1762933907909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **RS-MoE**, a post-training compression framework tailored to Mixture-of-Experts (MoE) LLMs. The core idea is to jointly exploit MoE structure inside each expert by coupling three matrices around the SwiGLU intermediate (the “collaborative unit”): the rows of \\(W_{\\text{up}}\\) and \\(W_{\\text{gate}}\\) and the corresponding column of \\(W_{\\text{down}}\\). For each index \\(j\\), RS-MoE assigns high-importance dimensions to a sparse branch and compresses the rest via a low-rank branch, then uses a small ridge-regression fit to reduce residual error. Dimensional importance is estimated by ACI (Anomalous Contribution Integration), combining routed activation statistics (mean/variance/peaks) with a proxy for downstream influence. Finally, RS-MoE performs mutual-information–guided layer allocation, using an MI estimator to distribute sparsity non-uniformly across depth. Experiments on several MoE models (e.g., Mixtral, DeepSeek-MoE, Qwen-MoE) show that RS-MoE improves perplexity/zero-shot accuracy versus expert-level pruning/merging baselines and reports some throughput/FLOPs trade-offs under certain settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **MoE-aware design.** The collaborative-unit view ties parameters that co-determine each expert dimension, encouraging **aligned** sparse/low-rank decisions and helping preserve expert specialization.  \n- **Importance beyond magnitude.** ACI uses routed activation statistics and a downstream proxy to rank dimensions, typically outperforming magnitude-only criteria in retention quality.  \n- **Complementary decompositions.** High-importance dimensions remain sparse; the remainder uses activation-aware SVD, with **ridge** to absorb truncation errors—often superior to pure pruning or pure low-rank.  \n- **Non-uniform depth allocation.** MI-guided scheduling assigns sparsity where layers are more redundant, beating uniform or naive schedules in most cases.  \n- **Small calibration footprint.** Uses a modest number of calibration samples to collect activations/routing, showing reasonable stability to sample count."}, "weaknesses": {"value": "1. **MoE-specific leverage vs. “applying old tricks.”** It is not yet clear what intrinsically MoE-specific insight drives the gains beyond standard sparse/low-rank compression. The paper claims index-aligned coupling across \\(W_{\\text{up}}, W_{\\text{gate}}, W_{\\text{down}}\\), but the causal link to preserving expert specialization (vs. independent per-matrix pruning) is insufficiently isolated. Without ablations that remove/disable the MoE coupling, the method risks looking like a generic compressor “ported” to MoE.\n\n2. **Runtime benefits are not convincingly established.** Storage/compression metrics and FLOPs proxies are provided, but **end-to-end speedups**  are limited or model-specific. Without comprehensive latency/tokens-per-second results across multiple MoE models and sparsity settings, the efficiency claim remains tentative.\n\n3. **Ablation granularity and causal attribution.** The contributions of (i) MoE index coupling, (ii) the importance metric’s components, (iii) low-rank rank selection, and (iv) MI-guided layer allocation are not disentangled. It is hard to tell which component actually matters, and whether the MoE-specific parts—rather than generic sparse/low-rank—drive the gains."}, "questions": {"value": "1. **ACI ablations:** Break down the contributions of inner-activation statistics vs. downstream-influence components. How sensitive is performance to the weighting of these terms?  \n\n2. **MI estimator cost/stability:** What is the training time and variance of the MI estimator across runs? Is there a simpler heuristic (e.g., depth-based) that approximates MI enough to avoid training it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VMJPJKIRrO", "forum": "5SQbPbCU6P", "replyto": "5SQbPbCU6P", "signatures": ["ICLR.cc/2026/Conference/Submission20465/Reviewer_YZg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20465/Reviewer_YZg1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975248893, "cdate": 1761975248893, "tmdate": 1762933907167, "mdate": 1762933907167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel MoE model decomposition method, where model weights are treated in a more fine-grained way. It first identifies model weights that can produce high activation values. The important part is preserved and only the less important part is being decomposed. The proposed method also use mutual information for allocating compression ratios in different layers. Extensive experiments are carried out to demonstrate the superiority of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n\n2. The experiments carried out in this paper are extensive and comprehensive, which involves multiple common Moe models under different compression ratios. But the evaluation part is also limited, details refer to Weaknesses below."}, "weaknesses": {"value": "1. Lack of novelty and also discussion about two important previous works. The key findings presented in this paper are originally comes from *LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation* (ICML'23) and *SoLA: Leveraging Soft Activation Sparsity and Low-Rank Decomposition for Large Language Model Compression* (AAAI'25). Authors seem missing these two works, and should clarify the novelty compared with these two papers (except for adapting to MoE models).\n\n2. Limited evaluation. The experiments involved in this paper don't include any generative tasks. Authors are suggested to add more experiments about generative tasks, such as arithmetic reasoning and summarization.\n\n3. Need efficiency improvement evaluation. The proposed method might reproduce too many small matrix and thus cause significant performance degradation during the generation. Authors need to evaluate the generation performance to demonstrate it won't incur severe performance issues."}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "npfMbvOxov", "forum": "5SQbPbCU6P", "replyto": "5SQbPbCU6P", "signatures": ["ICLR.cc/2026/Conference/Submission20465/Reviewer_DdvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20465/Reviewer_DdvH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024198243, "cdate": 1762024198243, "tmdate": 1762933906495, "mdate": 1762933906495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}