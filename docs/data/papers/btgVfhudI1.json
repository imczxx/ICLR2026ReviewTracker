{"id": "btgVfhudI1", "number": 17447, "cdate": 1758276214615, "mdate": 1759897174566, "content": {"title": "Short Window Attention Enables Long-Term Memorization", "abstract": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers. \n\nA counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval. \n\nThe issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.", "tldr": "We evidence the counter-intuitive fact that, in hybrid architectures, shorter sliding windows lead to better length-extrapolation. We introduce a training procedure that allows the hybrids to perform well on both short and long-context tasks.", "keywords": ["hybrids", "xLSTM", "SWA", "memory", "attention", "long-context", "architecture-design", "LLM", "stochastic"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f353c86505b56b1bb1e3a2bf14f652cbc9c23a19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper empirically evaluates how the window size of sliding window attention affects the long-context performance, when mixing sliding window attention with xLSTM linear RNN layers in a hybrid architecture. The authors find that short window attention encourages the model to better train xLSTM, leading to better long-context performance. However, short window attention may lead to worse short-context performance. The authors then propose a simple strategy to randomize the window size in training, achieving the best of both worlds."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, clear, and easy to follow. The paper provides an interesting and practically insightful observation that short window attention leads to better long-context performance in hybrid models mixing window attentions and RNN layers.\n\n2. The authors provide a simple adaption to train a model with good short-context and long-context performance."}, "weaknesses": {"value": "1. The paper would be stronger if it evaluates more than one types of RNN layers to show that the findings are robust against different RNN architectures.\n\n2. The findings are based on models without fine-tuning with long-context data. It is unclear whether the results continue to hold if the models are further fine-tuned."}, "questions": {"value": "1. Could the authors comment on the generalizability of the observation to other RNN architectures?\n\n2. Could the authors comment on whether the results continue to hold if the models are further fine-tuned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IaF1WrGNUy", "forum": "btgVfhudI1", "replyto": "btgVfhudI1", "signatures": ["ICLR.cc/2026/Conference/Submission17447/Reviewer_bBjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17447/Reviewer_bBjL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717762203, "cdate": 1761717762203, "tmdate": 1762927337312, "mdate": 1762927337312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SWAX, a hybrid model that alternates sliding‑window softmax attention (SWA) with xLSTM (linear RNN) layers, arguing that short attention windows during training force the model to use the RNN path as true long‑term memory. Empirically, for 1.4B and 7B models trained on 150B tokens at 16k sequence length, the authors find: (i) shorter windows (e.g., 128) yield better long‑context retrieval on RULER Needle‑in‑a‑Haystack (NIAH) than longer windows (e.g., 2048) when extrapolating up to 131k tokens, and (ii) a stochastic window schedule preserves long‑context gains while recovering short‑context quality at test time with a 2048 window."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, counter‑intuitive empirical result: In hybrids, short windows train better long‑term memory than long windows. The heat‑map in Fig. 6 (p.8) plus the degradation curves in Fig. 5 (p.7) make the effect easy to verify.\n-  The stochastic window + brief annealing yields near‑best short‑context results while keeping long‑context robustness. This is a practical recipe others can adopt with minimal code churn."}, "weaknesses": {"value": "- Long‑context evaluation is narrow: RULER NIAH is synthetic. The paper lacks real‑task long‑context evaluations.\n- On the short‑context suite, the Transformer baseline still leads on average. The paper should contextualize this gap with wall‑clock throughput and memory at inference.\n- Design space under‑explored: The hybrid is fixed to a 1:1 SWA:xLSTM interleave and xLSTM only. It would be more valuable if more results on other setup could be added."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L17halcx5j", "forum": "btgVfhudI1", "replyto": "btgVfhudI1", "signatures": ["ICLR.cc/2026/Conference/Submission17447/Reviewer_SVZm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17447/Reviewer_SVZm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893460023, "cdate": 1761893460023, "tmdate": 1762927336558, "mdate": 1762927336558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies hybrid models that interleave sliding-window softmax attention (SWA) with linear RNN layers (xLSTM/mLSTM) in a 1:1 pattern. The core, counter-intuitive claim is that short SWA windows improve long-context recall because they push the model to train its recurrent memory instead of relying on local softmax; conversely, long windows hurt length extrapolation. The authors also propose stochastic window training—randomly switching between short and long windows during pretraining with an anneal near the end—to regain short-context quality without giving up the long-context gains. Empirically, they evaluate mainly on RULER “needle-in-a-haystack” tasks and a suite of short-context benchmarks;"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Clear, testable capacity-allocation hypothesis. Long windows allow SWA “absorb” the learning signal; short windows force the xLSTM to learn long-term memory. The paper probes this across window sizes and model scales.\n\n2) Simple, practical training recipe. Stochastic window training with a short end-of-training anneal; task-agnostic and effective at 1.4B and 7B.\n\n3) Comprehensive comparisons. Side-by-side results for pure (xLSTM, SWA) vs. hybrid models, compute-aware reporting (FLOPs/token; Table 1), and stringent length-extrapolation stress tests (RULER NIAH up to 131k)."}, "weaknesses": {"value": "1) The pretraining mix is described only at a high level (“mostly web and code”); perplexity is reported on a code subset, but sources, filters, licenses, and exact proportions are not disclosed. This reduces reproducibility and external validity.\n\n2) RULER NIAH—primarily a needle-retrieval test—is the centerpiece, but broader long-context reasoning is underrepresented (multi-hop QA over long documents, extended chain-of-thought, codebase-level tasks). The generality of the findings remains unclear. A stronger case would include evaluations across varied tasks, e.g., Babilong and long open-QA benchmarks (HotpotQA and MuSiQue in full-wiki mode, LongBench).\n\n3) The experiments are exclusively based on the xLSTM as the linear RNN component. A brief discussion or experiment on whether this phenomenon holds for other popular models, including neural memory (for example, Titans[1]), would significantly increase the generalizability and impact of the findings.\n\n[1] Ali Behrouz and Peilin Zhong and Vahab S. Mirrokni. Titans: Learning to Memorize at Test Time, arXiv:2501.00663"}, "questions": {"value": "1) How many random seeds were run per configuration? Please report mean ± standard deviation across seeds for the key metrics in Table 1 and Figures 5–8 (and include the per-seed results in an appendix or supplementary file). If only a single seed was used for any result, please state this explicitly.\n\n2) Could you specify the full optimization setup: the optimizer used (e.g., AdamW), exact β₁/β₂ (and ε) values, weight-decay coefficient, gradient-clipping norm and threshold, all dropout rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4shqrUL5ZH", "forum": "btgVfhudI1", "replyto": "btgVfhudI1", "signatures": ["ICLR.cc/2026/Conference/Submission17447/Reviewer_pAFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17447/Reviewer_pAFQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986732450, "cdate": 1761986732450, "tmdate": 1762927334980, "mdate": 1762927334980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **SWAX**, a hybrid that interleaves sliding-window softmax attention (SWA) and xLSTM layers. Core claims: (i) **shorter** SWA windows surprisingly yield **better long-context recall** because they force the RNN path to learn long-term memory; (ii) a **stochastic window** training schedule (occasionally using a small window such as 128, otherwise 2048, with an anneal in the last 10% of training) recovers short-context performance while preserving long-context extrapolation. Evaluations use a 1.4B and 7B model trained on 150B tokens at 16k sequence length; long-context performance focuses on RULER NIAH, with downstream short-context benchmarks reported."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The stochastic window schedule (sampling 128 vs 2048; annealing off in the last 10% of training) is simple, inexpensive to try, and consistently improves length extrapolation without sacrificing short-context quality at both 1.4B and 7B scales. This is well demonstrated in Figs. 7–8 and Table 2."}, "weaknesses": {"value": "1. **Limited and unbalanced evaluation of “long-context” ability.**\n   The central claim rests mostly on **RULER Needle-in-a-Haystack (NIAH)** accuracy; other long-context behaviors (QA, summarization, codebase-level reasoning, tool-use traces, multi-hop retrieval) are not evaluated. As a result, it’s unclear whether the observed gains generalize beyond synthetic token recall. The paper itself emphasizes RULER NIAH and heatmaps across sequence lengths but lacks a broader long-context suite. This weakens external validity of the main claim.  \n\n2. **Confounds in the “short vs long window” story and insufficient controls.**\n   The paper argues that **longer** windows under-train the RNN path and therefore **hurt** extrapolation; however, compute is not cleanly equalized across variants (Table 1 shows **FLOPs/token** rising with window size), and the training schedule/data mix may interact with window length (e.g., document length distribution skewed to code). Stronger controls (fixed total FLOPs, matched optimization schedules, varied data mixes, and tests at matched effective receptive fields) are needed to isolate causality. As is, the causal interpretation (under-training the RNN) is plausible but not conclusively established.  \n\n3. **Novelty is incremental relative to prior hybrid work; framing oversells.**\n   Alternating xLSTM and local attention is now common; the **new ingredient** here is mainly the **stochastic window schedule** and the empirical observation that small windows train the long-term path better. The paper acknowledges closely related hybrids and prior window-size analyses (e.g., De et al.), and even positions the method as akin to **dropout** on the attention mechanism. Without broader/better-controlled evaluations or theoretical support, the contribution feels like a useful training tip rather than a substantial architectural advance."}, "questions": {"value": "* Line 075: “**hybrids architecture**” → “**hybrid architectures**.” \n* Line 198: “In **constrast**” → “In **contrast**.” \n* Line 233:  “we also **train evaluate** models at the 7B parameter scale.” → “train **and** evaluate.” \n* Table/figure captions could be clearer (e.g., units/averaging in Fig. 6 heatmap; spell out metrics). \n* **Add broader long-context tasks** (e.g., LongBench-style QA/summarization/code comprehension) to test whether the NIAH gains translate to practical workloads. \n* **Tighter causal tests**: (i) equalize total compute across window settings; (ii) vary document-length distribution; (iii) report how gradients/attention mass shift between SWA and xLSTM under different windows; (iv) include ablations on the anneal schedule (you mention some in Appendix B—bring the most informative ones into the main paper)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B8lgNr14oZ", "forum": "btgVfhudI1", "replyto": "btgVfhudI1", "signatures": ["ICLR.cc/2026/Conference/Submission17447/Reviewer_4vk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17447/Reviewer_4vk4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147242835, "cdate": 1762147242835, "tmdate": 1762927334387, "mdate": 1762927334387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}