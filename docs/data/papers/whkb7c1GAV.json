{"id": "whkb7c1GAV", "number": 23860, "cdate": 1758349411617, "mdate": 1759896793619, "content": {"title": "Teaching RAG to Play Fair: Assessing and Mitigating Encoder-Only PLM Algorithmic Bias", "abstract": "Retrieval-Augmented Generation (RAG) reduces hallucinations in large language models (LLMs) by retrieving relevant external documents. Central to this process are encoder-only pre-trained language models (PLMs), which map queries and candidate passages into semantic vectors for retrieval. While most fairness research in RAG has focused on biases in generated text or corpora, the encoder’s role as the retrieval bottleneck remains underexplored. In this work, we systematically assess and mitigate representation-level bias in encoder-only PLMs used within RAG systems. We first diagnose bias localization using intrinsic metrics (Sentence Embedding Association Tests and probing classifiers), and show that demographic information is systematically encoded in mid-to-upper layers. We then evaluate whether intrinsic scores correlate with extrinsic disparities measured by statistical parity and equalized odds on the BBQ benchmark, finding moderate-to-strong correlations that establish intrinsic diagnostics as valid predictors of real-world unfairness. Finally, we benchmark lightweight debiasing methods—Low-Rank Adaptation (LoRA), WiSE-FT partial fine-tuning, and targeted attention-head masking—and integrate them into a modular fairness-aware framework. Our results demonstrate that these interventions meaningfully reduce bias with minimal degradation to retrieval quality, highlighting a path towards fairer, representation-aware RAG systems.", "tldr": "We study fairness in RAG encoders, showing biases in PLMs, validating intrinsic metrics as predictors of retrieval disparities, and demonstrating that lightweight debiasing (LoRA, WiSE-FT, masking) reduces bias with minimal quality loss.", "keywords": ["Retrieval-Augmented Generation", "Algorithmic Fairness", "Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/923d5b1382d977a6fddf8cf8bf08ddf4614c61df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper claims to study representational bias in encoder-only models used in RAG systems and to propose lightweight debiasing methods (LoRA, WiSE-FT, attention-head masking) combined with a fairness-regularized contrastive objective. The authors evaluate on BBQ and Wikipedia data and report small improvements in fairness metrics (SPD, EOD) with minimal accuracy loss."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper does touch on a relevant and timely topic—the fairness of encoder components in retrieval-augmented generation systems, which remains relatively underexplored compared to bias in generation. The authors make a commendable attempt to evaluate both intrinsic and extrinsic bias, and their focus on lightweight, practical debiasing strategies such as LoRA and WiSE-FT reflects awareness of computational constraints in real-world applications."}, "weaknesses": {"value": "The paper rehashes well-known methods (SEAT, linear probing, SPD/EOD, LoRA, WiSE-FT) with no genuine innovation.\n\nThe supposed “framework” is just a combination of existing components glued together.\n\nThere is no theoretical contribution, no conceptual advance, and the experimental setup is entirely incremental.\n\nThey use SEAT on BBQ, which is inappropriate. SEAT was designed for word/sentence association, not contextualized QA templates."}, "questions": {"value": "Why don't you have an abstract for your paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L0Sca4lTuw", "forum": "whkb7c1GAV", "replyto": "whkb7c1GAV", "signatures": ["ICLR.cc/2026/Conference/Submission23860/Reviewer_T8p2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23860/Reviewer_T8p2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624760818, "cdate": 1761624760818, "tmdate": 1762942833834, "mdate": 1762942833834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to identify and mitigate fairness issues in Retrieval-Augmented Generation systems by exploring four key research questions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluating and mitigating fairness in the encoder component of RAG systems is an important and timely research direction.\n\n2. The paper shows an effort to progress from surface-level observations to deeper analytical questions, which reflects thoughtful research design."}, "weaknesses": {"value": "1. Writing quality and formatting issues.\n\nThe paper’s overall presentation requires substantial improvement. Important sections such as the abstract and related work are missing, and several paragraphs are too short or disconnected, making the paper difficult to follow. The research questions and corresponding results are presented far apart, which disrupts the logical flow and makes it hard for readers to trace the narrative.\nSome sentences are incomplete or unclear (e.g., line 343). I highly recommend a thorough proofreading and restructuring to enhance readability and coherence.\n\n2. Missing sections and results.\n\nThe abstract and related work sections are missing, which are essential components of an academic paper. There also appear to be missing tables or results. For example, the experimental results corresponding to RQ1 are not clearly presented (line 341). Please double-check that all results are properly included and labeled.\n\n3. Limited model and dataset coverage.\n\nThe experiments are conducted on only one backbone model and two datasets, which limits the generalizability of the findings. Expanding the evaluation to additional models or domains would strengthen the conclusions.\n\n4. Unclear connection between research questions.\n\nIt would be beneficial to clarify how the findings from RQ1 and RQ2 inform or contribute to the design and interpretation of RQ3 and RQ4. Establishing such connections would make the paper’s structure more coherent and logical.\n\n5. Limited novelty.\n\nThe proposed mitigation strategies appear to be combinations or adaptations of existing methods, with only modest improvements in performance. Highlighting the unique contribution or insight of your approach—beyond integration—would make the paper more impactful."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HT9RkOWT93", "forum": "whkb7c1GAV", "replyto": "whkb7c1GAV", "signatures": ["ICLR.cc/2026/Conference/Submission23860/Reviewer_7m3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23860/Reviewer_7m3A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849024666, "cdate": 1761849024666, "tmdate": 1762942833622, "mdate": 1762942833622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates fairness issues in encoder-only pre-trained language models (PLMs) that serve as retrievers within RAG systems. The authors identify representational bias at the encoder level (measured via SEAT and probing classifiers), examine its correlation with downstream fairness metrics (SPD, EOD) on the BBQ dataset, and explore lightweight debiasing techniques, LoRA, WiSE-FT, and attention head masking, combined with a fairness-aware contrastive objective. Results suggest that such methods can modestly improve fairness without substantial performance loss."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Important direction: Fairness in RAG pipelines is underexplored, and studying encoder-level bias rather than output bias is a meaningful perspective.\n\n2. Structured methodology: The paper is organized around clear research questions (RQ1–RQ4), with distinct experimental stages (diagnosis, correlation, mitigation).\n\n3. Multi-level analysis: Combining intrinsic metrics (SEAT/probes) with extrinsic ones (SPD/EOD) is methodologically coherent."}, "weaknesses": {"value": "1. Misleading framing (not a true RAG study): Despite the title, this paper does not evaluate fairness in retrieval-augmented generation. All analyses are limited to encoder embeddings, without a generator component or end-to-end RAG evaluation. Thus, the contribution concerns bias in static sentence representations, a topic extensively explored. The framing overstates novelty and scope.\n\n2. Outdated model and weak baselines: The only model studied is `all-MiniLM-L6-v2`, a small and obsolete encoder. No comparisons are made to contemporary retrievers such as E5, BM25, or T5-Encoder, nor to fairness-aware variants (e.g., Fair-SimCSE, Debias-BERT, or Controlling the Embedder). Without such baselines, the fairness gains (SPD ↓ from 0.135 → 0.107) are marginal and not meaningful.\n\n3. Limited novelty of methods: The “contrastive-fairness” objective is a direct variant of SimCSE with an added group-parity regularizer, and the composition of LoRA + WiSE-FT + masking lacks theoretical motivation. The framework is essentially a combination of existing fine-tuning tricks rather than a principled new approach.\n\n4. Insufficient experimental rigor: The paper uses only BBQ and Wikipedia subsets, omitting standard retrieval or fairness benchmarks (e.g., TREC-Fairness, FairRank. Statistical analyses are weak, p-values in Table 1 are non-significant, yet interpreted as meaningful.\n\n5. Lack of causal or interpretive analysis: The probing results indicate where bias appears but not why. The work offers no interpretation of linguistic or semantic factors behind layer-wise bias, nor any causal explanation of how encoder bias propagates through retrieval.\n\n7. Missing related work and comparative context: The paper fails to situate itself among the growing literature on fairness in RAG. Key related works include:\n    * Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems\n    * No free lunch: Retrieval-augmented generation undermines fairness in llms, even for vigilant users\n    * The other side of the coin: Exploring fairness in retrieval-augmented generation\n    * Gender Encoding Patterns in Pretrained Language Model Representations\n    * Measuring the Fairness Gap Between Retrieval and Generation in RAG Systems using a Cognitive Complexity Framework\n    * Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs\n    * Evaluating the Effect of Retrieval Augmentation on Social Biases\n    * ReFaRAG: Re-ranking for Bias Mitigation in Retrieval-Augmented Generation* \n    * Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers\n\n    All of these directly address bias across retrieval + generation pipelines or propose re-ranking-based mitigation. The current paper neither compares with nor acknowledges most of them, making its contribution unclear and outdated.\n\n8. Poor paper structure and presentation: The layout deviates from standard ICLR format: 1) Missing abstract, leaving readers without a concise overview. 2) No dedicated Related Work section; prior studies are scattered through the introduction. 3) The Methodology and Experiment sections are merged and difficult to follow, with unclear transitions and no architectural diagram. 4) Tables and formulas appear abruptly; figures (e.g., bias heatmaps) are discussed only in the appendix. These issues severely hurt readability and perceived professionalism."}, "questions": {"value": "I have listed all key concerns and clarifying questions within the Weaknesses section. Authors are expected to respond to those points， especially regarding 1) model and baseline choices, 2) missing related work, 3) evaluation validity, and 4) paper organization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VzYU2Hbfw0", "forum": "whkb7c1GAV", "replyto": "whkb7c1GAV", "signatures": ["ICLR.cc/2026/Conference/Submission23860/Reviewer_7WHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23860/Reviewer_7WHM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887707208, "cdate": 1761887707208, "tmdate": 1762942833383, "mdate": 1762942833383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates bias in retrieval-augmented generation (RAG) systems by first diagnosing intrinsic bias in retriever encoders and demonstrating its correlation with downstream unfairness. It then proposes lightweight adaptation methods—LoRA, WiSE-FT, and Head Masking—to effectively mitigate encoder bias without degrading retrieval accuracy. Extensive experiments show these approaches achieve fairness comparable to full fine-tuning at only 1% of the parameter cost, highlighting their practicality for scalable bias correction."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe paper is logically well-structured, progressing smoothly from diagnosing encoder-level bias to analyzing RAG-level bias and finally exploring effective mitigation strategies.  \n2.\tThe proposed encoder-level metric, FairnessScore, shows a strong correlation with downstream fairness in RAG, suggesting it can serve as an efficient proxy indicator for early detection of bias and improved evaluation efficiency."}, "weaknesses": {"value": "1.\tWriting and structure issues:\n\n+ The paper lacks both an abstract and a Related Work section, which are essential for contextualizing the study.\n\n+ The Introduction mainly provides background information but does not clearly articulate the research problem or outline the logical flow of the paper (even though Section 2 touches on it).\n\n+ RQ4 should be merged with RQ3, and the experiments in RQ4—which are valuable—should be moved into the main body rather than left as a separate section.\n\n+ Lines 83–89 describe only one model and should not use itemized formatting (e.g., itemize). Moreover, this model description does not warrant such length in the main text.\n\n+ Section 5.1 Findings should be removed since it contains only one subsection; the same issue may occur elsewhere and should be checked.\n\n+ Reduce the use of subsubsections and consider using the \\paragraph{} command instead to save space for more important content.\n\n+ Equations (1) and (2) are missing periods at the end; please check punctuation after all equations.\n\n2.\tExperimental design and analysis:\n\n+ The choice of evaluation models is too limited; models such as GTE, BGE, Qwen3-Embedding, and NV-Embed should be included for broader comparison.\n\n+ In Table 1, it would be helpful to include fairness results for random or unbiased groups as a control, to contextualize what a fairness score around 0.5 represents.\n\n+ In RQ3, the study only explores several lightweight tuning strategies; this limits novelty. A stronger contribution would involve investigating where the unfairness originates (e.g., data, loss design, or model architecture) and designing mitigation accordingly.\n\n+ Beyond Table 2, the authors should also report RAG-level performance results to confirm that the proposed methods do not harm end-to-end retrieval quality."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ftbiNHjU8K", "forum": "whkb7c1GAV", "replyto": "whkb7c1GAV", "signatures": ["ICLR.cc/2026/Conference/Submission23860/Reviewer_2rLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23860/Reviewer_2rLM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901984502, "cdate": 1761901984502, "tmdate": 1762942833104, "mdate": 1762942833104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}