{"id": "t5uLZSRjhF", "number": 8520, "cdate": 1758089127352, "mdate": 1759897778724, "content": {"title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing", "abstract": "Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size.  This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs to achieve rapid convergence.We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to the vanilla dLLMs like LLaDA and Dream, the acceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable output quality.", "tldr": "", "keywords": ["Diffusion Large Language Models", "Discrete Diffusion Models", "Inference Acceleration", "KV Cache", "AR-Diffusion Hybrid"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f17e677cee754760d24e016681085ae6e41a7050.pdf", "supplementary_material": "/attachment/25f42b34a5c7a7a482404237ed30cf73ffa6b9b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Discrete Diffusion Forcing (D2F), a framework for faster text generation with diffusion-based large language models. D2F combines asymmetric distillation from a bidirectional teacher with block-wise causal decoding to enable partial KV caching. The authors report substantial inference speedups over both prior diffusion LLMs and standard autoregressive models while maintaining competitive accuracy on reasoning and coding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear motivation: Tackles the critical limitation of diffusion LLMs, slow inference, and provides a coherent framework aimed at bridging the efficiency gap with autoregressive models.\n\nTechnical integration: The combination of block-wise causal decoding, diffusion distillation, and pipelined inference is well-engineered and practically significant.\n\nStrong empirical results: Shows large speedups with minimal accuracy loss.\n\nAccessible open-source implementation: The authors promise to open source the code."}, "weaknesses": {"value": "Limited novelty. Most structural components (block-wise causal diffusion, caching, block parallelism) are drawn from prior work. I understand the novelty comes from the asymmetric distillation and the generation of later blocks without fully generated earlier blocks?\n\nInsufficient Justification for Asymmetric Distillation. The paper justifies distillation primarily as a way to reduce training costs. However, it doesn't provide an ablation comparing its asymmetric method to a \"from-scratch\" D2F model (even on a smaller scale) or a simpler \"standard\" distillation. This makes it hard to isolate how relevant this distillation mechanism is."}, "questions": {"value": "KV caching explanation. One source of speedups is the use of block-wise KV caching. Without the strictly causal generation over blocks some blocks are getting unmasked with previous blocks now fully unmasked. How is the KV caching implemented here? Or is KV caching only used for fully unmasked blocks? This is related to the ablations in the Appendix (table 5 and 7) which show that the pipelined decoding leads to 2-3x speedups.\n\nThe paper introduces a form of asymmetric self-distillation to train the model. This is one of the methodological contributions of the paper. It is unclear to me how much this is needed. Does this have any benefits over training the model from scratch (apart from training cost)? Is the asymmetric component needed? You could imagine training with the teacher using exactly the same setup as the student (standard distillation); that could perform similarly? (I understand the teacher is fully bidirectional, but should be able to handle such task as well.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5R04nyCaTI", "forum": "t5uLZSRjhF", "replyto": "t5uLZSRjhF", "signatures": ["ICLR.cc/2026/Conference/Submission8520/Reviewer_gxgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8520/Reviewer_gxgg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852183708, "cdate": 1761852183708, "tmdate": 1762920385060, "mdate": 1762920385060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"Discrete Diffusion Forcing\" (D2F), a framework to make diffusion LLMs (dLLMs) faster than autoregressive (AR) LLMs. D2F reformulates the dLLM as an AR-diffusion hybrid, which solves the core problem of dLLMs not being able to use a standard KV cache. The method uses asymmetric distillation to train a block-wise causal student dLLM to mimic a bidirectional teacher, using a monotonically increasing noise schedule. This enables a \"pipelined parallel decoding\" algorithm during inference, where future blocks can be processed before previous blocks are fully finished, all while using an exact KV cache. The authors report significant speedups (up to 2.5x vs. LLaMA3, 50x vs. vanilla dLLMs) with comparable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant Milestone: The paper achieves \"faster-than-AR\" inference with an open-source dLLM, a significant milestone. The reported speedups (2.5x vs. LLaMA3, 50x vs. LLaDA) are extremely impressive.\n\n2. Effective Training Strategy: The \"asymmetric distillation\" with a \"monotonically increasing mask schedule\" is a clever adaptation of Diffusion Forcing to the discrete domain. It effectively trains the model to predict from an incomplete prefix, enabling the parallel pipeline.\n\n3. Solves the KV Cache Problem: The method reframes the dLLM as a block-wise causal model, allowing the use of a standard, exact KV cache. This is a fundamental improvement over prior work that relied on approximate caching.\n\n4. Strong Ablation Studies: The ablations clearly validate the design, showing that the parallel pipeline adds significant speed over caching alone, the structured noise schedule is critical, and the gains are not just an artifact of the data or tuning."}, "weaknesses": {"value": "1. Inconsistent/Confusing Performance Claims: The paper's headline claim of \"faster-than-AR\" performance is made confusing by seemingly inconsistent numbers across the text and figures. This makes the exact performance trade-off difficult to assess.\n\n- LLaMA3 Baseline: In Figure 2, the LLaMA3-Instruct-8B baseline (star) is plotted with a GSM8K score of ~77. However, the text in Section 5.3 states its score is 70.1. This is a significant discrepancy.\n\n- D2F Performance: The paper reports multiple different performance points for its own model. Figure 1 claims 119.9 TPS (which corresponds to a score of ~75 in Figure 2). But Section 5.3 reports a different operating point of 150.9 TPS and a 71.2 score.\n\n- This inconsistency makes it difficult to definitively conclude if D2F is \"faster and comparable,\" \"faster and slightly worse,\" or \"much faster and slightly better\" than its key AR competitor.\n\n2. Inference Complexity: The proposed \"pipelined parallel decoding\" (Algorithm 2) introduces significant new complexity to the inference process. It requires careful tuning of three new, interacting hyperparameters ($\\tau_{add}$, $\\tau_{act}$, $\\tau_{conf}$) in addition to the block size, which could be a barrier to practical adoption and tuning compared to the simplicity of standard AR decoding.\n\n3. Incremental Novelty: The paper is transparent that its core idea is an \"extension of DF (Diffusion Forcing)\" (Chen et al., 2024a) and \"connects to CausVid\" (Yin et al., 2025). While the adaptation to the discrete domain is novel and the engineering is strong, the fundamental concept of forcing a model to predict from a noisy prefix via distillation is not entirely new. This makes the contribution more of a highly successful and non-trivial adaptation than a completely new paradigm."}, "questions": {"value": "1. Could the authors please clarify the performance of the LLaMA3-Instruct-8B baseline on GSM8K? Figure 2 implies a score of ~77, but the text in Section 5.3 states 70.1. Which number should be used for comparison, and why is there a discrepancy?\n\n2. Similarly, could the authors provide a single, clear {TPS, Score} pair for the D2F-Dream-Base-7B model on GSM8K that represents the main claim? The numbers in Figure 1, Figure 2, and Section 5.3 all seem to refer to different operating points, making a direct comparison difficult.\n\n3. The inference pipeline (Alg. 2) seems complex. How sensitive is the model's performance (both speed and quality) to the choice of $\\tau_{add}$, $\\tau_{act}$, and $\\tau_{conf}$? Table 3 suggests they interact, but is there a \"default\" set of parameters that works well across most tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Byzn0K4P9", "forum": "t5uLZSRjhF", "replyto": "t5uLZSRjhF", "signatures": ["ICLR.cc/2026/Conference/Submission8520/Reviewer_WETF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8520/Reviewer_WETF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974653891, "cdate": 1761974653891, "tmdate": 1762920384257, "mdate": 1762920384257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes discrete diffusion forcing that critically enables decoding of the new block to start even before the previous blocks are fully decoded. This unlocks the parallelization not only within a single block but also across blocks. By further inheriting KV-cache brought by block decoding, D2F achieves promising speedup without sacrificing much sample quality. Experiments on math and coding benchmarks over two models demonstrate the effectiveness of the proposed approach in terms of achieving more speedup while maintaining the accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a critical problem in discrete diffusion model: inference acceleration. D2F unlocks parallelization even across different blocks, which is a very important feature to significantly enhance the speed.\n\n2. The method includes a distillation training for D2F and a customized inference procedure. The distillation helps mitigate the bias in block diffusion that requires the previous blocks to be fully decoded.\n\n3. The presentation is clear and the proposed approach is clean and very easy to follow."}, "weaknesses": {"value": "1. The technical novelty is somewhat bounded since the work is an adaptation of previous diffusion forcing literature (especially video diffusion) to discrete diffusion models. The concern is not significant though, given the promising empirical performance.\n\n2. Several experiment results that are key to compare the accuracy-efficiency frontier and understand the design of the proposed approach are missing. See Q1 and Q2 for more details."}, "questions": {"value": "1. It would be more helpful to elucidate the speedup by showing the accuracy-speedup curves for the proposed approach and the baselines, instead of only showing one single point on the curve (the numbers in the table). i.e, plot the curve of Fast-dLLM and dLLM-Cache, if applicable.\n\n2. How is the accuracy-efficiency tradeoff of Block Diffusion [1]? This can be investigated by doing block diffusion distillation and sweep over different numbers of decoding steps at inference time. This is important for ablating the necessary of diffusion forcing distillation over naive block diffusion distillation.\n\n\n[1] Arriola et al. Block diffusion: Interpolating between autoregressive and diffusion language models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QbTznkFmqv", "forum": "t5uLZSRjhF", "replyto": "t5uLZSRjhF", "signatures": ["ICLR.cc/2026/Conference/Submission8520/Reviewer_QtKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8520/Reviewer_QtKg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982950725, "cdate": 1761982950725, "tmdate": 1762920383875, "mdate": 1762920383875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Discrete Diffusion Forcing (D2F). At inference time, the framework combines the strengths of diffusion LLMs (dLLMs) and autoregressive LLMs (AR LLMs). It adopts an AR‑like blockwise sequential generation scheme to exploit the KV cache while preserving parallel decoding across blocks in dLLMs, so later tokens can be predicted without waiting for the previous block to finish. This hybrid of AR and diffusion substantially accelerates dLLM inference. During training, the method performs asymmetric distillation from a pretrained dLLM teacher with standard bidirectional attention to a student that has only a causally restricted view. Experiments on multiple benchmarks show that D2F significantly speeds up dLLM inference beyond AR LLMs while maintaining accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The work introduces an original hybrid paradigm that enables KV‑cache friendly, AR‑style generation while retaining dLLMs’ cross‑block parallelism, and it provides a tailored asymmetric distillation procedure to train the model.\n2) According to the paper, D2F yields the first open‑source dLLMs that surpass state‑of‑the‑art AR LLMs in inference speed, and achieves more than 10× speedup on some benchmarks over dLLM baselines without D2F.\n3) The experimental study is comprehensive, with comparisons against strong AR LLM and dLLM baselines on common benchmarks, along with ablations on key hyperparameters."}, "weaknesses": {"value": "1) Training relies on a pretrained dLLM as the teacher, which may limit scaling to stronger future D2F variants. In addition, D2F does not accelerate training, so the compute cost remains substantial.\n2) The main figure (Figure 3) is information‑sparse; a clearer depiction of the asymmetric distillation would improve readability. Table 3 could be half‑width, since the current layout leaves excessive white space."}, "questions": {"value": "1) Can the main experiments include error bars to establish statistical significance?\n2) Tables 1 and 2 show large variation in speedup across different benchmarks and different dLLM base models for D2F, for example 52.9× versus 4.3×. What explains this variance? Does it indicate sensitivity to model architecture or to the data type used at inference time?\n3) How sensitive is D2F to generation length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P8P0f78oVF", "forum": "t5uLZSRjhF", "replyto": "t5uLZSRjhF", "signatures": ["ICLR.cc/2026/Conference/Submission8520/Reviewer_au9N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8520/Reviewer_au9N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998816134, "cdate": 1761998816134, "tmdate": 1762920383295, "mdate": 1762920383295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}