{"id": "KPgtXpr7OX", "number": 7353, "cdate": 1758017401383, "mdate": 1763105550577, "content": {"title": "RainDiff: End to End Precipitation Nowcasting Via Token-wise Attention Diffusion", "abstract": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a token-wise attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.", "tldr": "", "keywords": ["AI for Weather; Precipitation Nowcasting; AI for Science; Diffusion Model"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1963ddbd5cea71aaf5f8778e74d8eaf7b269fe7f.pdf", "supplementary_material": "/attachment/94ca362130e6a61a8d14ed1bc34fdc097a9cc409.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents RainDiff, an end-to-end diffusion-based framework for precipitation nowcasting that directly models radar sequences in the pixel space without using latent autoencoders. To address the computational bottleneck of full-resolution attention, the authors propose a Token-wise Attention (TWA) mechanism that achieves global dependency modeling with linear complexity. In addition, a Post-attention (PA) module is introduced after the spatio-temporal encoder to mitigate gradient vanishing and enhance long-term temporal consistency. Experiments conducted on four radar datasets demonstrate that RainDiff achieves stable and competitive performance compared to existing deterministic and diffusion-based baselines, improving both forecast accuracy and spatial coherence in long-horizon precipitation prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Clear motivation and well-defined goal. The paper clearly identifies the computational and architectural limitations of previous diffusion-based nowcasting models, and motivates the need for a fully end-to-end framework operating in pixel space.\n\n+ Effective architectural design. The proposed Token-wise Attention (TWA) efficiently captures global spatial dependencies with linear complexity, making high-resolution attention feasible without latent compression. The introduction of the Post-attention (PA) module provides an effective way to stabilize gradients and improve temporal consistency in long-horizon predictions.\n\n+ Comprehensive experiments. The paper includes extensive evaluations on four radar datasets with solid baselines, ablation studies, and visualization analyses, providing convincing empirical evidence of the method’s effectiveness."}, "weaknesses": {"value": "+ Lack of theoretical depth. The paper introduces Token-wise Attention (TWA), but does not provide a rigorous mathematical analysis demonstrating that TWA is a simplified form of attention with equivalent representational properties. In its current form, TWA behaves more like a global aggregation mechanism than a true attention mechanism, as it lacks token-to-token interaction and selective dependency modeling.\n+ Poor figure and visualization quality. The presentation of figures and tables in the paper is suboptimal and does not conform to standard academic conventions.  In Table 1, the authors highlight their own model in bold font, which is uncommon in scholarly publications.  The table layout also fails to clearly reflect the relative performance advantage of the proposed method.  In Table 6, the proposed RainDiff model is not included at all, resulting in an incomplete performance comparison. Several visualization figures contain too many elements within a single image, which affects the fluency of reading and comprehension."}, "questions": {"value": "+ Could the authors provide a more rigorous theoretical justification for Token-wise Attention (TWA)?  Specifically, under what assumptions can TWA be viewed as a simplified or approximate form of standard self-attention, and what kinds of dependencies might be lost due to the global aggregation design?\n+ How does TWA differ from previously proposed linear or low-rank attention mechanisms?  Is there any empirical or theoretical comparison that highlights its unique behavior?\n+ While TWA reduces quadratic complexity, how does its runtime and memory usage compare with other linear attention models in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RC9uj31W0D", "forum": "KPgtXpr7OX", "replyto": "KPgtXpr7OX", "signatures": ["ICLR.cc/2026/Conference/Submission7353/Reviewer_Urr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7353/Reviewer_Urr3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405421172, "cdate": 1761405421172, "tmdate": 1762919490112, "mdate": 1762919490112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Reviewers and Area Chairs, we have decided to withdraw our paper. We are very grateful for the time and expertise you invested in reviewing our submission."}}, "id": "X3APVsCyFV", "forum": "KPgtXpr7OX", "replyto": "KPgtXpr7OX", "signatures": ["ICLR.cc/2026/Conference/Submission7353/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7353/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763105549825, "cdate": 1763105549825, "tmdate": 1763105549825, "mdate": 1763105549825, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RainDiff, an end-to-end diffusion framework for precipitation nowcasting. It retains the deterministic–probabilistic hybrid design of DiffCast, combining a mean-field deterministic predictor with a residual diffusion model under a joint objective. The key claimed novelties are: (1) Token-Wise Attention (TWA) for linear-complexity global token aggregation, and (2) Post-Attention (PA) applied after the spatio-temporal encoder to enhance gradient stability. The method is evaluated on Shanghai, SEVIR, MeteoNet, and CIKM datasets and shows small but consistent improvements in CSI/HSS scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Reproducible and consistent experimental setup across four datasets. \nS2. Clear motivation to remove autoencoder dependency for cross-dataset generalization. \nS3. Logical architecture design with good readability and structured methodology. \nS4. Empirical improvements are directionally consistent and align with design goals."}, "weaknesses": {"value": "W1. Limited methodological originality beyond architectural refinement of DiffCast. \nW2. No theoretical or runtime analysis differentiating TWA from existing linear attentions (Performer, SwiftFormer). \nW3. Efficiency claims (speed, memory) are unsupported. \nW4. Qualitative and ablation studies are restricted to a single dataset.\nW5. No statistical significance or confidence intervals for reported metrics. \nW6. Figures and equations need better clarity and consistency"}, "questions": {"value": "1. Clarification is needed on how the proposed Token-Wise Attention mathematically differs from existing efficient-attention mechanisms in terms of formulation and information flow.\n2. Quantitative evidence on FLOPs, memory usage, and inference speed would help verify the claimed computational efficiency of the new modules.\n3. Since the reported CSI and HSS improvements are relatively small, statistical validation across multiple runs would strengthen the credibility of the results.\n4. The claimed gradient-stability benefit of post-Attention would be more convincing with supporting analyses of gradient norms or training dynamics.\n5. Visual and ablation results are shown only for the Shanghai dataset. Extending these evaluations to other datasets would demonstrate generalization.\n6. A comparison against other efficient-attention variants implemented under the same setting would clarify whether the improvements arise from true methodological novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T3z2iXmIg4", "forum": "KPgtXpr7OX", "replyto": "KPgtXpr7OX", "signatures": ["ICLR.cc/2026/Conference/Submission7353/Reviewer_rZXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7353/Reviewer_rZXK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681605723, "cdate": 1761681605723, "tmdate": 1762919489465, "mdate": 1762919489465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an end-to-end diffusion framework RainDiff for precipitation nowcasting, which integrates Token-wise Attention into U-Net and spatio-temporal encoder without latent autoencoders, and introduces Post-attention to optimize denoising. Experiments on 4 datasets show RainDiff outperforms baselines in various metrics results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes Token-wise Attention, enabling full-resolution self-attention in pixel space, outperforming state-of-the-art baselines in multiple metrics on 4 datasets.\n2. This work effectively designs a hybrid framework (deterministic + stochastic diffusion modules) and Post-attention to mitigate gradient attenuation."}, "weaknesses": {"value": "1. The precipitation process is constrained by laws of atmospheric dynamics and thermodynamics, yet the hybrid framework proposed in this paper is entirely data-driven and does not incorporate any physical priors. Although this issue is mentioned in the limitations section, it may still lead to physically unreasonable results.\n2. The paper only theoretically analyzes the optimization of time complexity, while lacking comparative analysis at the experimental level—such as the actual consumption of computing resources and time during the training and generation processes.\n3. In precipitation nowcasting, the forecasting requirements and difficulty vary significantly across different lead times. For instance, short lead times focus on accurately capturing local details, while long lead times may prioritize stably modeling global dynamics. However, RainDiff lacks an analysis of the differences in its advantages under different lead times. For example, it is unclear whether the improvement of TWA on local details in short lead times is comparable to its effect on modeling global dependencies in long lead times.\n4. As a probabilistic forecasting model, RainDiff lacks an analysis of the model's stability and its ability to control randomness (including handling outliers), ."}, "questions": {"value": "The questions here are related to the three points described as weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "umoqBMYHmM", "forum": "KPgtXpr7OX", "replyto": "KPgtXpr7OX", "signatures": ["ICLR.cc/2026/Conference/Submission7353/Reviewer_3389"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7353/Reviewer_3389"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806376698, "cdate": 1761806376698, "tmdate": 1762919488999, "mdate": 1762919488999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an end-to-end precipitation nowcasting/downscaling model that combines a deterministic predictor with a diffusion model operating on residuals. The key architectural element is a token-wise attention block that claims to reduce quadratic complexity while preserving long-range temporal dependencies. The method is evaluated on standard radar/climate datasets with metrics such as CSI/HSS (and perceptual scores), showing improvements over recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Quality/results: Solid quantitative gains on standard metrics (CSI/HSS and perceptual proxies). If robust, these are practically relevant.\n\n- Clarity of task framing: Modeling residual stochasticity on top of a coarse predictor is appropriate for precipitation. This is consistent with prior residual video diffusion works.\n\n- Engineering: The end-to-end pipeline appears implementable and computationally careful."}, "weaknesses": {"value": "- No long-range dependency metric: Paper mentions lack of attn in Diffcast leads to long range dependency issues. However, DiffCast does have temporal attn, maybe a metric which explicitly captures long range dependency would have been ideal, something correlation based.\n\n- Multi-loss design not justified: Prior residual diffusion works often use a single diffusion loss end-to-end, avoiding extra hyperparameters (like gamma). The deterministic modules naturally learn coarse features without having to rely on explicit loss. Single end-to-end diffusion loss is usually sufficient.\n\n- Attention novelty/efficiency unclear. TWA reduces quadratic cost, but there is no head-to-head vs SwiftFormer (efficient additive), AFT (element-wise/global-summary), or linear/IO-aware variants (Linformer, Performer, Nyströmformer, FlashAttention). Wall-clock and memory comparisons seem to be missing.\n\n- Limited probabilistic evaluation: Calibration/CRPS missing, usually needed to support uncertainty claims (common in diffusion-for-weather).\n\n- Compute & latency not reported: No throughput/VRAM/sampling-step analysis relative to latent diffusion or efficient attention baselines."}, "questions": {"value": "- Your text says DiffCast omits self-attention at high-resolution layers for tractability, not attention in general. Can you specify which attention forms you claim are absent in DiffCast (spatial vs temporal), and map RainDiff’s attention placement vs DiffCast layer-by-layer to make the distinction precise?\n\n- Are tokens strictly per-pixel (n=h×w) at every scale, or do you patch/subsample anywhere? What positional encodings (if any) are used inside TWA at different resolutions\n\n- You argue DiffCast produces uncontrolled randomness at air-mass boundaries. How did you quantify “boundary stability”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0c2zw9DwR7", "forum": "KPgtXpr7OX", "replyto": "KPgtXpr7OX", "signatures": ["ICLR.cc/2026/Conference/Submission7353/Reviewer_AEhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7353/Reviewer_AEhz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812810560, "cdate": 1761812810560, "tmdate": 1762919488425, "mdate": 1762919488425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}