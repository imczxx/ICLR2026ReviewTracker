{"id": "i6nuapVUDZ", "number": 20480, "cdate": 1758306640044, "mdate": 1759896975619, "content": {"title": "Fast and Expressive Multi-Token Prediction with Probabilistic Circuits", "abstract": "Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming _independence_ between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the _joint_ distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously elucidate the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between verifier and draft LLMs.", "tldr": "", "keywords": ["multi-token prediction", "probabilistic circuits", "speculative decoding", "efficient LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bef6120ff72711f80b402190675702184f24548d.pdf", "supplementary_material": "/attachment/d90c29f53d7af62e0d9a70cc23632a61591e1b87.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MTPC, a probabilistic-circuit-based framework for multi-token prediction (MTP) in large language models. Unlike traditional MTP approaches that assume independence among future tokens, MTPC flexibly encodes joint token distributions through diverse probabilistic circuit architectures, generalizing models such as mixture models, HMMs, and tensor networks. Applied to byte-level LLMs like EvaByte, MTPC, combined with speculative decoding, substantially accelerates generation while preserving the verifier model’s performance. The study systematically explores the trade-off between expressiveness and latency, showing that appropriate architectural and parameter-sharing choices yield efficient, expressive, and consistent multi-token generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical novelty: The paper provides an interesting and elegant theoretical formulation of multi-token prediction using probabilistic circuits, enriching understanding of the expressiveness–latency trade-off.\n\n2. Significant performance gains: Experimental results demonstrate strong acceleration and efficiency improvements while maintaining model quality, highlighting the practical impact of the proposed approach."}, "weaknesses": {"value": "1. Lack of comparison with related methods: The paper does not clearly distinguish MTPC from tree-based speculative decoding approaches, nor does it discuss relevant prior work, which weakens its positioning in the broader MTP literature. [1]https://arxiv.org/abs/2402.12374 [2] https://arxiv.org/abs/2305.09781 [3] https://arxiv.org/abs/2401.10774\n\n2. Limited model applicability: The experiments focus mainly on byte-level LLMs, with no evaluation or discussion on generalizing MTPC to mainstream models such as LLaMA, Qwen, or DeepSeek, leaving its scalability and universality uncertain."}, "questions": {"value": "In the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qaLog0i5MC", "forum": "i6nuapVUDZ", "replyto": "i6nuapVUDZ", "signatures": ["ICLR.cc/2026/Conference/Submission20480/Reviewer_VKHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20480/Reviewer_VKHG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897166939, "cdate": 1761897166939, "tmdate": 1762933918232, "mdate": 1762933918232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MTPC, a family of multi-token prediction (MTP) heads built from probabilistic circuits (FF, CP, HMM, BTree) that model joint distributions over future tokens and plug into a shared-backbone self-speculative decoding setup. The framework lets one trade off expressiveness (acceptance rate) vs latency by (i) choosing the PC architecture and (ii) selecting how many layers the draft/verifier shares or separates. On EvaByte (byte-level LLM), MTPC improves throughput over AR and over fully factorized MTP while guaranteeing AR quality under speculative decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces MTPC, a multi-token prediction framework built on probabilistic circuits, which overcomes the independence assumptions of prior MTP methods. This allows MTPC to model joint token dependencies more effectively than factorized or tensor-decomposition-based approaches.\n\n2. The paper rigorously studies the trade-offs between acceptance rate and generation latency across different PC architectures and different levels of layer sharing. This provides a clear and interpretable design space for controlling speed–quality trade-offs.\n\n3. The framework is evaluated on EvaByte, where MTPC demonstrates substantial throughput improvements, for example, ×5.47 over autoregressive decoding and ×1.22 over MTP models with independence assumptions, while maintaining output quality. The experiment highlights practical deployment viability in real LLM inference settings."}, "weaknesses": {"value": "1. Experiments focus on a single 6.5B byte-level model (EvaByte) and one SFT mixture (Tülu-3). It would strengthen claims to show transfer to a subword LLM (to decouple gains from byte vocabularies) and to other additional datasets/domains.\n\n2. While the loss and discounting are described, ablations on optimization sensitivity (γ, window overlap, head depth/width) are limited. Providing more ablation studies would strengthen the paper.\n\n3. The paper emphasizes latency but gives fewer numbers on memory vs n and r for different PCs (esp. BTree with higher ranks). Besides, this paper introduces a verifier that consumes an additional memory footprint. Therefore, a detailed memory footprint plot will help the audience understand the memory consumption of this paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gNLagz9tsx", "forum": "i6nuapVUDZ", "replyto": "i6nuapVUDZ", "signatures": ["ICLR.cc/2026/Conference/Submission20480/Reviewer_azfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20480/Reviewer_azfz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939894282, "cdate": 1761939894282, "tmdate": 1762933917298, "mdate": 1762933917298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MTPC, a framework for MTP in LLMs based on probabilistic circuits. Existing MTP methods assume independence between future tokens, sacrificing expressiveness and leading to implausible outputs. MTPC addresses this by parameterizing joint distributions over token windows using PC architectures that encode hierarchical mixture models. The framework encompasses fully factorized models (FF), canonical polyadic decompositions (CP), and introduces novel hidden Markov model (HMM) and binary tree (BTree) factorizations for MTP. Combined with speculative decoding, MTPC guarantees retention of the original autoregressive LLM's quality. The authors identify two key trade-offs: (1) PC architecture choice affecting expressiveness vs. latency, and (2) number of LoRA layers shared between draft and verifier models. Experiments retrofitting EvaByte (a 6.5B byte-level LLM) demonstrate 5.47× speedup over autoregressive generation and 1.22× speedup over independence-based MTP, with BTree achieving optimal throughput for n=16 tokens and 2 LoRA layers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. MTPC provides a unified probabilistic circuit framework that systematically navigates MTP design space, introducing novel HMM and BTree architectures with BTree achieving optimal throughput by parallelizing latent sampling while maintaining high acceptance rates.\n2. The paper rigorously examines trade-offs across PC architecture selection (FF/CP/HMM/BTree) and partial layer sharing via LoRA (0-4 layers), revealing device-specific optimal configurations through systematic ablations across mixture components, window sizes, and GPU types.\n3. MTPC uses speculative decoding to provably match autoregressive quality while achieving 5.47x speedups, outperforming provided baselines."}, "weaknesses": {"value": "1. All experiments focus exclusively on EvaByte (6.5B byte-level model with v=320), without validation on subword-level LLMs where vocabularies are 300× larger or across different model families/sizes, limiting claims about scalability.\n2. Key design decisions including inhomogeneous HMMs, identity matrix initialization, and why BTree outperforms CP lack theoretical justification beyond empirical validation, with no analysis of when specific architectures excel for different prompt characteristics.\n3. The paper omits comparisons with recent MTP methods like Hydra and Eagle that introduce sequential dependencies, dismisses Basharin's KL loss without thorough evaluation, and lacks validation on standard speculative decoding benchmarks."}, "questions": {"value": "1. Have authors evaluated MTPC on subword-level LLMs with large vocabularies (v≥100k), and how do the memory/computational costs of CP/HMM scale compared to FF in such settings?\n2. Can authors provide theoretical or empirical guidelines for when to choose BTree vs. HMM vs. CP based on prompt characteristics, sequence lengths, or task requirements beyond throughput measurements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGgrXUvIVi", "forum": "i6nuapVUDZ", "replyto": "i6nuapVUDZ", "signatures": ["ICLR.cc/2026/Conference/Submission20480/Reviewer_jubx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20480/Reviewer_jubx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969290732, "cdate": 1761969290732, "tmdate": 1762933916782, "mdate": 1762933916782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces multi-token PCs (MTPCs). The main idea is to movie beyond fully-factorized and simple mixture models in the context of multi-token prediction for speculative decoding. The authors evaluate MTPC on EvaByte, a byte-level LLM and observe that MTPC increases the throughput of EvaByte by 1.22x compared to the less expressive MTP speculative decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I found the paper to overall be well-written, aside from a few nitpicks that I've highlighted in my questions below.\n\n- The paper offers a general, principled framework that encompasses several of the previous works.\n\n- By exploiting connections to previous work, the authors manage to increase the expressiveness of the drafters while minimizing the latency for an overall improved throughput of 1.22x"}, "weaknesses": {"value": "- The paper deals with byte-level LLMs which in my opinion greatly limits its scope as it's hard to draw strong conclusion about its performance on sub-word LLMs that are a lot more commonly used by the community.\n\n- The paper details the requirement to train the MTPC which by the authors' description is a very arduous process, and could therefore\nlimit adoptability of the proposed approach."}, "questions": {"value": "- The authors mention that \"MTPC guarantees that they match the quality of an AR LLM via speculative decoding, exactly for greedy decoding, or in expectation for sampling\". Are the authors making the claim that the output of MTPC follows the AR LLM distribution? If so, isn't that a standard assumption in speculative decoding approaches? Is the \"in expectation for sampling\" a weakening of that assumption?\n\n- The authors mention \"repurposing\" and/or \"retrofitting\" EvaByte, but my understanding is that the language modeling component is largely left unchanged?\n\n- I find it a bit confusing how *speculative decoding* is separated from the *fully-factorized* and *canonical polyadic factorization* in section 2, since it is my understanding that the latter two are a means to realizing the former.\n\n- I believe the parameterization of the PC with an LLM bears great resemblance to [1], which should be mentioned.\n\n- I am a bit confused by paragraph 293-303. Is the implication that the model being used, EvaByte, is used with n=1 to recover a STP model? Are all the experimental results reported using greedy decoding with EvaByte? If so, it would've been useful to expand more upon the greedy speculative decoding paper by Stern et. al to show how one can guarantee argmax consistency with speculative decoding (which is not specific to MTPC)\n\n- Referencing your conclusion, similar to the work of Zhang et. al regarding integrating constraints during generation, [2] offers a way to do so without training an HMM, which might integrate nicely with your framework.\n\nReferences:\n\n[1] Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, & Antonio Vergari. Semantic Probabilistic Layers for Neuro-Symbolic Learning. NeurIPS 2022.\n[2] Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck. Controllable Generation via Locally Constrained Resampling. In ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tDCYkQXojT", "forum": "i6nuapVUDZ", "replyto": "i6nuapVUDZ", "signatures": ["ICLR.cc/2026/Conference/Submission20480/Reviewer_EgHu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20480/Reviewer_EgHu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291209538, "cdate": 1762291209538, "tmdate": 1762933916157, "mdate": 1762933916157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}