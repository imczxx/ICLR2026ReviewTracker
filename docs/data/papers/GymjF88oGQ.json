{"id": "GymjF88oGQ", "number": 97, "cdate": 1756728728759, "mdate": 1759898275955, "content": {"title": "The Pensieve Paradigm: Stateful Language Models with Learned Memory Management", "abstract": "In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve—mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory.\n\nThis work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manipulate their own state. We equip our model with a suite of tools, such as dynamic indexing, context pruning, and note-taking, and train it to actively manage this loop. By learning to dynamically construct its own context, our model breaks free from the architectural prison of a fixed window. The results are prominent: our state-management approach decouples performance from context window size, delivering strong accuracy and sustainability under extremely long contexts with linear inference cost. We demonstrate this by showing StateLM reliably retrieves a \"needle\" from a 1-million-token haystack, a task far beyond the reach of conventional models. On practical document QA tasks from NovelQA and $\\infty$Bench, StateLM outperforms strong instruct baselines  while using only 1/4 of their active context. An ablation further shows that our curated training pipeline is more effective for learning memory management than agent-like prompting. Together, these results mark a shift from passive predictors to state-aware systems where reasoning becomes a stateful and manageable process.", "tldr": "", "keywords": ["LLM", "memory management"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d411b45856f6dfaf3ae0c24c5b9aa995014326ba.pdf", "supplementary_material": "/attachment/bceafcddb1daa855bd0be813fc8c88bb16a1e0ff.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces **The Pensieve Paradigm**, a framework that equips an LLM with tools to dynamically manage its own context and reasoning process. The proposed system, called **StateLM**, allows the model to actively construct, prune, and update its working memory through operations such as dynamic indexing, note-taking, and context deletion. This enables it to go beyond the predefined context length and handle extremely long sequences efficiently.\n\nThe authors propose a data generation pipeline, and employ SFT to train models from the Qwen3 family. Empirical results show that StateLM achieves strong performance in both synthetic and real-world tasks, consistently outperforming the baselines, while using only a fraction of the context length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written, and the main ideas are presented clearly.\n- I believe that the problem that the authors tackle is both interesting, and useful, particularly in the context of agentic systems. In many cases, agents are required to handle huge contexts (for instance large code repos); therefore, the proposed paradigm could be leveraged to both improve the performance and make agentic systems more efficient.\n- The paper has solid amount of empirical results that showcase the strength of the proposed method, covering both synthetic and real-world settings."}, "weaknesses": {"value": "- As the authors also mention, the current set of tools is predefined and fixed. While this appears sufficient for the evaluated tasks (Needle-in-a-Haystack and Long Document QA), it may prove inadequate in more complex or dynamic scenarios\n- In Section 4.1, I think that the comparison with the baseline may not be entirely fair. The high accuracy achieved by the proposed approach on extremely long contexts is indeed interesting;  however, the poor performance of the baseline models in the cases where the context length is exceeded is not surprising. Perhaps, a better baseline could be to use a simple sliding window approach, where the size of the window is close to the context size of the model.\n- Although the authors note this as well, I believe that testing RL approaches would be reasonable in this setting, since there has been evidence that RL works well in similar settings(e.g [1]). \n\n[1] Feng, Jiazhan, et al. \"Retool: Reinforcement learning for strategic tool use in llms.\" arXiv preprint arXiv:2504.11536 (2025)."}, "questions": {"value": "- In Figure 5, why does the inference time of  Qwen3-8B (baseline) decrease as the context length increases? Is it due to truncation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8W9BCOmgZN", "forum": "GymjF88oGQ", "replyto": "GymjF88oGQ", "signatures": ["ICLR.cc/2026/Conference/Submission97/Reviewer_xaus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission97/Reviewer_xaus"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission97/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640630022, "cdate": 1761640630022, "tmdate": 1762915449568, "mdate": 1762915449568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Pensieve/StateLM reframes long-context modeling as learned memory management. The LLM executes a tool-use loop ( e.g., analyzeText, buildIndex, searchEngine) to construct and prune its own working context rather than passively consuming a fixed window. StateLM is trained on Claude Sonnet 4–generated trajectories filtered for outcome and behavior on NovelQA and NarrativeQA. On Needle-in-a-Haystack (NIAH), StateLM outperforms Qwen-3 instruct baselines (4B/8B). On NovelQA and InfiniteBench, a 32K-context StateLM surpasses a 128K instruct baseline and shows scaling trends with higher inference-time compute. A prompt-only agent underperforms StateLM, supporting the claim that these behaviors should be learned, not merely prompted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The data curation pipeline is carefully designed. multi-stage filtering and process-mode classification (search vs. scan) produce cleaner trajectories for training.\n- SLM w/o search greatly outperforms baseline by a large margin especially after 256K tokens.\n- On real-world tasks like NovelQA and InfiniteBench, the results are impressive where SLM with short context (32K) can achieve better performance than instruct model with context of 128K token\n- Good writing. The description of StateLM and experiments are clear and easy to follow."}, "weaknesses": {"value": "- It is not clear how StateLM materially differs from prior work (e.g., A-Mem, SCM, Dynamic Cheatsheet). The claim of “not a fixed workflow loop” does not really establish novelty, as this function has been supported by agentic toolkit like Anthropic’s Model Context Protocol (MCP) and also has been explored by prior work.\n- The training trajectories come from Sonnet-4, which along with many open-source agents already can decide which tools to use given context. As presented, the contribution is largely policy learning over a fixed toolset (index/search/read/note/delete), rather than a new memory paradigm.\n- Ablations are limited: comparisons are mainly against a prompt-only baseline. There are no per-tool ablations (e.g., removing deleteContext) and no robustness analysis under noisy/failed tool calls.\n- All results are based on only two models Qwen3 4B/8B of the same family. It is not clear if it generalizes well to other families.\n- StateLM appears to use substantially more inference-time interaction/compute than single-pass baselines, making the comparison potentially not apples-to-apples.\n\nReferences:\n\n```\n@article{wang2024openhands,\n  title={Openhands: An open platform for ai software developers as generalist agents},\n  author={Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},\n  journal={arXiv preprint arXiv:2407.16741},\n  year={2024}\n}\n```\n\n```\n@article{xu2025mem,\n  title={A-mem: Agentic memory for llm agents},\n  author={Xu, Wujiang and Mei, Kai and Gao, Hang and Tan, Juntao and Liang, Zujie and Zhang, Yongfeng},\n  journal={arXiv preprint arXiv:2502.12110},\n  year={2025}\n}\n```\n\n```\n@article{yu2025memagent,\n  title={MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent},\n  author={Yu, Hongli and Chen, Tinghong and Feng, Jiangtao and Chen, Jiangjie and Dai, Weinan and Yu, Qiying and Zhang, Ya-Qin and Ma, Wei-Ying and Liu, Jingjing and Wang, Mingxuan and others},\n  journal={arXiv preprint arXiv:2507.02259},\n  year={2025}\n}\n```\n\n```\n@article{wang2023enhancing,\n  title={Enhancing large language model with self-controlled memory framework},\n  author={Wang, Bing and Liang, Xinnian and Yang, Jian and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},\n  journal={arXiv preprint arXiv:2304.13343},\n  year={2023}\n}\n```"}, "questions": {"value": "1. Do you report a baseline using Qwen3 + MCP tooling for different tasks? Since Qwen3 family natively support building AI agents with MCP protocols, the prompt-only ablation may underserve tool use capability of instruct model.\n2. Do you provide error analysis for NIAH, NovelQA, and InfiniteBench for both StateLM and baselines (instruct and prompt-based)?\n3. What is the impact of each tool (e.g., removing deleteContext) on accuracy? Can you provide further ablation analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xFlDZ1tmXK", "forum": "GymjF88oGQ", "replyto": "GymjF88oGQ", "signatures": ["ICLR.cc/2026/Conference/Submission97/Reviewer_GVg6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission97/Reviewer_GVg6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission97/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945031850, "cdate": 1761945031850, "tmdate": 1762915449392, "mdate": 1762915449392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents StateLM, a stateful, memory-augmented LLM agent supervisedly trained to\nautonomously manage its own context through a (predefined) set of tools. Instead of relying on a\nhuman-defined workflow, the model learns, via SFT on author-curated reasoning trajectories, to\ndecide when to perform indexing, note-taking, searching, and pruning. Experiments on long-\ncontext QA benchmarks, compared with Qwen3-Instruct models, demonstrate substantial gains\nin both efficiency and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "● The problem addressed in this paper is crucial: transitioning from stateless LLMs to a\nstateful paradigm enables long-term reasoning, multi-turn dialogue memory, and\ncross-session continuity.\n● The paper is well-written, and the case study in Section 3 provides an intuitive and\neffective way to illustrate the Pensieve paradigm.\n● The “model as the wizard” framing, i.e., pushing the model toward fully autonomous\ndecision-making about when (and, potentially in future work, how) to manage its own\ncontext, is an appealing selling point that makes the work conceptually engaging.\nHowever, as discussed below in the weaknesses, it remains to be seen whether this\nsetting can match the performance of prior semi-automated workflows."}, "weaknesses": {"value": "1. The paper aims at a meaningful goal of achieving a fully automated workflow, since\nheuristic and human-defined pipelines may not fully unlock the capability of LLMs.\nHowever, the framework still relies on manually defined tools, making it essentially\nsemi-automated. Given that prior work (e.g., Memory-R1) also trains models to learn\nwhat memory operations to perform, the main difference here seems to lie in when\nthose operations are triggered. Memory-R1 updates memory after each turn, which is\na natural and reasonable design, while this work lets the model decide the timing in\nan end-to-end way.\n○ The point is that whether this flexibility is actually an advantage is not obvious. \nI would like to see stronger empirical evidence, for\nexample, the proposed method triggers memory operations much less\nfrequently and therefore achieves higher efficiency, better utility, or broader\ngeneralization across question types/domains to justify this design choice.\n2. **Limited benchmark and model scope:** Evaluation is conducted only on a single\nbase model (Qwen3-Instruct) and two document-based QA datasets (NovelQA and\ninfiniteBench En.MC split). Broader long-term or multi-session benchmarks such as\nLoCoMo, MSC, LongMemEval, or RULER (not necessarily all) should be included to\ntest richer memory behaviors.\n3. **Lack of scalability and generalization tests:** The model is trained on the\n*PublicDomain* split of NovelQA and mainly evaluated on the *Copyright* split. In\nTable 3, its gains diminish on $\\infty$Bench, particularly for larger backbones (e.g.,\nQwen3-8B). Additional experiments on cross-domain or OOD settings are needed to\nassess the proposed method’s applicability, especially given its reliance on formatted\ntraining.\n4. **Missing baselines:**\n○ Since the method equips the LLM with external memory, a fair comparison\nshould include the same base model with (i) direct function-call memory\naccess and (ii) MCP-based memory access (e.g.,\nhttps://github.com/doobidoo/mcp-memory-service), to see whether the base\nmodel (without SFT on reasoning traces) can already use memory when\ngiven access, and how much extra benefit the proposed framework actually\nprovides.\n○ Comparisons with prior memory-management methods, whether retrieval-\naugmented (RAG) or agent-based methods such as those evaluated in Mem0\nand Memory-R1, are necessary; For cost reasons, even a direct evaluation\nwithin their setups would make the results more informative."}, "questions": {"value": "5. What are the specific requirements for the “high-quality, good-behavior, expert\nreasoning trajectories”?\n6. Since the generation of expert reasoning trajectories is effectively a distillation\nprocess from Claude-Sonnet-4, it would be informative to include Claude-Sonnet-4’s\nown performance as a baseline in the Experimental section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dr6SiUGOMV", "forum": "GymjF88oGQ", "replyto": "GymjF88oGQ", "signatures": ["ICLR.cc/2026/Conference/Submission97/Reviewer_qeFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission97/Reviewer_qeFc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission97/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955994093, "cdate": 1761955994093, "tmdate": 1762915449286, "mdate": 1762915449286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Stateful Language Models (StateLM or SLM) is a class of foundation models that are equipped with tools (including: dynamic indexing, context pruning, note taking) to manipulate their state in a reasoning loop which dynamically (and automatically) updates their context. A StateLM (i) can retrieve a needle in 1 million-token haystack (ii) in empirical results over practical QA benchmarks it performs better than strong instruct baselines using a fourth of their active context and (iii) is superior in learning to manage memory than agent-like prompting.\n\nSLM's reasoning trajectory consists of a series of actions (thoughts and acts (tool invocations)) and states (optionally modified contexts and responses from tools): tools can do tasks like analyze text, summarize or search through it or even update (e.g. delete) context. SLM is trained over trajectories for handling questions of types involving either locating or understanding text (search or scan types): each trajectory consists of steps (training samples) where given the history up to some step, SLM is trained to predict next step's thought and action. Interestingly, SLM's performance cannot be matched by models that have access to tools and are prompted to follow the context management process in SLM."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This is a simple and novel idea: the model becomes active, inspects its current memory/state and accordingly constructs the context to operate on using pre-defined tools.\n\n- No pressing need for the user-in-the-middle role of building prompts conditioned on a manually inspected state (automation).\n\n- Clean guidelines for training, set of orthogonal tools well-defined.\n\n- Performance on long-context recall and QA benchmarks are impressive."}, "weaknesses": {"value": "- Critical requirement for the availability of a strong LLM for the generation of training samples (in particular for process-mode classification)\n- The set of tools is given, is generic enough but it certainly cannot fit any question handling."}, "questions": {"value": "- Are there any thoughts for automatically building the set of tools most amenable to particular question types?\n\n- Can the succession of the particular tools invoked drive the classification of questions answered into finer-grained classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pQ56bGnGwK", "forum": "GymjF88oGQ", "replyto": "GymjF88oGQ", "signatures": ["ICLR.cc/2026/Conference/Submission97/Reviewer_vbKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission97/Reviewer_vbKg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission97/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978557423, "cdate": 1761978557423, "tmdate": 1762915449172, "mdate": 1762915449172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}