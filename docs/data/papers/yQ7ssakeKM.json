{"id": "yQ7ssakeKM", "number": 18991, "cdate": 1758292594483, "mdate": 1759897068718, "content": {"title": "Learning From the Past with Cascading Eligibility Traces", "abstract": "Animals often receive information about errors and rewards after significant delays. In some cases these delays are fixed aspects of neural processing or sensory feedback, for example, there is typically a delay of tens to hundreds of milliseconds between motor actions and visual feedback. The standard approach to handling delays in models of synaptic plasticity is to use eligibility traces. However, standard eligibility traces that decay exponentially mix together any events that happen during the delay, presenting a problem for any credit assignment signal that occurs with a significant delay. Here, we show that eligibility traces formed by a state-space model, inspired by a cascade of biochemical reactions, can provide a temporally precise memory for handling credit assignment at arbitrary delays. We demonstrate that these cascading eligibility traces (CETs) work for credit assignment at behavioral time-scales, ranging from seconds to minutes. As well, we can use CETs to handle extremely slow retrograde signals, as have been found in retrograde axonal signaling. These results demonstrate that CETs can provide an excellent basis for modeling synaptic plasticity.", "tldr": "A cascade of synaptic eligibility traces can account for delays in biological credit assignment", "keywords": ["biological credit assignment", "eligibility traces", "synaptic plasticity", "computational neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86f2302a5d0f843bf8c3d791585d9df9b162f41a.pdf", "supplementary_material": "/attachment/540e501b8f073ba2c7e6f0b9994f3bd4e0c67814.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Cascade Eligibility Trace (CET), a novel mechanism for the temporal credit assignment problem in deep neural networks, motivated by biological plausibility. The core idea replaces the standard exponential decay of classic eligibility traces (ETs) with a cascade of $n$ coupled differential equations. The CET's primary function is to generate a memory kernel that is centered around the time of the error signal, rather than that of the sensory input. This allows learning systems to accurately assign credit for rewards or feedback signals that arrive significantly after the causal neural activity. This is achieved by effectively substituting the standard exponential kernel of ETs with a Gamma-like kernel which peaks at a given delay. The authors conduct extensive benchmarking against the classical ET across various tasks (both supervised and RL) and datasets (e.g. MNIST, LunarLander), systematically demonstrating the computational advantages of the CET paradigm. The work is well-written, with a clear and compelling narrative."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- ⁠The new formulation is clear and straightforward, addressing with clarity the weaknesses of previous models. Recasting eligibility as a Gamma-shaped temporal window with tunable peak and width is a principled generalization beyond two-exponential composites.\n- Theoretical derivations for both continuous-time and exact discrete-time update. The parameters $(n, \\alpha)$ have a nice interpretation as \"delay\" and \"precision\".\n- ⁠The benchmarking is extensive, and most of the results are quite solid, demonstrating  a clear advantage w.r.t. classic ET, with systematic sweeps over delays and CET orders. Author provide gradient-alignment analysis to connect performance with optimization geometry.\n- ⁠The application with accumulating delays is interesting from both a computational and a neuroscientific perspective."}, "weaknesses": {"value": "- While the work strongly underlines the method’s biological plausibility, benchmarks focus on accuracy on classification and reinforcement learning tasks, but misses benchmarking against real biological data (e.g. neural predictivity metrics (e.g. Yamins et al., 2014; Yamins & DiCarlo, 2016). I’d like to see how a model trained with CET  on a image classification task (e.g. CIFAR10) compares to the same model trained with ET or backpropagation in their neural predictivity.\n- The paper argues that two-ET composites produce broad windows and thus underperform for long delays. However, no optimized two-exponential (or multi-exponential) baseline is reported in the main text under the same tuning budget as CETs. Author could for example include a two-ET case with grid-searched $(\\gamma_P,\\gamma_D)$.\n- CETs increase state size linearly in $n$, which results in per-step computational overhead. The paper should quantify the relevance of such overhead for example by comparing it with ET (e.g. FLOPs/memory). This might become relevant for example in the long-delay experiments that rely on sparsification.\n- In stacked-delay experiments, the $\\delta$-computation assumes access to $f’(⋅)$ at the appropriate delayed postsynaptic state (a soma-side memory). \n-  It seems to me that one of the key advantages of CET is built on the assumption that the variable delays are clustered. As shown in Section I in the Appendix, the benefit of CET w.r.t. ET performances decreased as the variance increased and the delay distribution became closer to uniform. The solution suggested (Perturbation-Based Learning) violates the principle of locality, which is foundational to biological functioning. This method requires a synapse to access, store, and compare the global loss from multiple non-local, trial-level executions."}, "questions": {"value": "1. How sensitive are results to the trade-off $T=(n-1)/\\alpha$? For fixed $T$, what is the optimal $n$ given compute/memory budgets? Please provide heatmaps of performance and cosine similarity over $(n,\\alpha)$ at one or two T values.\n2. In stacked delays, how would you maintain the postsynaptic derivative at the soma? Could an analogous CET run there, and do you expect additional degradation?\n3. What happens when the critic is also trained with delayed signals (e.g., delayed TD errors)? Does CET still yield stable learning?\n4. How does CET compare with *tuned* two-exponential kernels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vbhKK5mrRd", "forum": "yQ7ssakeKM", "replyto": "yQ7ssakeKM", "signatures": ["ICLR.cc/2026/Conference/Submission18991/Reviewer_bZyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18991/Reviewer_bZyU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734733274, "cdate": 1761734733274, "tmdate": 1762931044494, "mdate": 1762931044494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cascading Eligibility Traces (CETs), a generalization of classical eligibility traces for handling delayed credit assignment in neural networks. The authors frame CETs as a state-space model inspired by biochemical cascades, which creates a temporally precise memory trace that peaks at a specific, non-zero delay. Their mechanism overcomes several of the limitations of the exponential decays in classic ETs. The paper provides extensive empirical evidence showing that CETs enable effective learning in both supervised and reinforcement learning settings with delays ranging from seconds to minutes, and connects these findings to the viability of slow retrograde axonal signaling for credit assignment in neuroscience."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, and the theoretical derivations are clear and appear to be correct. The paper provides a solid and well-motivated extension of prior theory on ETs.\n\n1. The formulation of CETs as a state-space model is mathematically sound and provides a powerful generalization of a classic learning mechanism. Showing that a standard eligibility trace is equivalent to a CET with a single state (n=1) makes this work a direct and solid extension of prior work. The model's parameters (n and alpha) offer intuitive control over the precision and timing of the memory trace.\n\n2. They demonstrate the superiority of CETs over standard ETs across a diverse set of non-trivial supervised and RL tasks. As task complexity and feedback delay increase, the performance gap between higher-order CETs and standard ETs widens significantly. The included gradient alignment analysis provides a convincing explanation for these performance gains. I really appreciate this level of benchmarking, which is not common in basic computational neuro work.\n\n3. Modeling learning with minute-long, stacking delays is, to my knowledge, novel and impactful. It provides a plausible computational mechanism by which slow chemical signals, such as retrograde messengers, could effectively transmit credit information."}, "weaknesses": {"value": "1. The retrograde experiments on visual tasks rely on a salience signal to sparsify memory storage, which may be a critical component for making learning tractable over minute-long delays. If my understanding is correct, this assumes the existence of an oracle-like signal that tells the synapse which events are important and thus which memories to form, which seems to be solving part of the most difficult part of the temporal credit assignment problem (i.e. what to remember) before the delayed error signal arrives.\n\n2. The results clearly show that more states are better. However, this benefit presumably comes at a cost. It would strengthen the paper to include a brief discussion on the computational complexity of the CET mechanism as a function of the number of states n."}, "questions": {"value": "1. Could the authors expand on the biological plausibility of the salience signal mechanism in the main text?  A brief discussion on how a salience signal could be computed and propagated quickly enough to gate the slow memory formation process would be helpful for readers to understand the complete proposed system.\n\n2. The theoretical derivations could possibly benefit from including the inverse Laplace transform. The kernel g(t) of the convolution in Eq 5 is the inverse Laplace transform of the function G(s)=(s+alpha)^-n. This interpretation could provide a deeper understanding of why and how decodability improves with higher n. See [Masset, Tano et al Nature 2025] for an example of using the inverse laplace transform framework to understand what kind of information is decodable from a multi-timescale system."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LcFWVmfjf6", "forum": "yQ7ssakeKM", "replyto": "yQ7ssakeKM", "signatures": ["ICLR.cc/2026/Conference/Submission18991/Reviewer_7gVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18991/Reviewer_7gVv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812865694, "cdate": 1761812865694, "tmdate": 1762931044102, "mdate": 1762931044102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new mechanisms for supporting temporal credit assignment in biological networks when reinforcement or error signals arrive with significant delays.The authors propose what they term as “cascading eligibility traces” (CETs), i.e., eligibility traces defined in terms of a state-space model inspired by cascading biochemical reactions. Motivated by the observation that biological systems often process error or reward signals with fixed but significant delays, the authors propose CETs to obtain temporally specific memory of past network activity, that can be matched to delayed credit signals  at behavioral time-scales ranging from seconds to minutes.\n\nThey demonstrate the proposed mechanism in a supervised experimental setting on visual classification tasks employing the MNIST/CIFAR-10 datasets (and the TinyImageNet) with delays up to seconds, where all network layers receive the same delayed error signal, on reinforcement learing tasks (namely CartPole, LunarLander, and MinAtar/SpaceInvaders), and in a retrograde signalling setting on reinforcement learning experiments, where delays stack across layers up to minutes, mimicking very slow biochemical backpropagation.\n\nThey show that increasing CET order systematically improves performance and gradient alignment when compared to standard eligibility traces (whose performance deteriorates for delayed larger than 4 seconds), but does not fully close the gap to true backpropagation for long-delay settings, where the gradient alignment stops to improve.   The authors further demonstrate results against classical eligibility traces, and provide in the supplement additional analyses that study the robustness to variable values and unknown delays, provide ablation studies on cascade order, study gradient alignment across all network layers, and application to recurrent and spiking (leaky integrate and fire) networks.\nOverall the paper provides a well-motivated, biologically grounded extension of eligibility traces, however I find that the strongest biological claim depends on extra assumptions (see below)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly formulated and biologically motivated problem of wanting to model a mechanism for temporal credit assignment that is both temporally fine tuned, but also provides temporal extent to be able to account for delayed error or reinforcement signals with delays up to seconds.\n- The authors provide systematic experiments both in supervised and reinforcement learning settings.\n- To understand the limits in performance for long delays they study how the weight updates align with the true gradients obtained by backpropagation."}, "weaknesses": {"value": "- There is no comparison with the eligibility trace formalisms that consider different traces for potentiation and depression (He et al., 2015; Huertas et al., 2016)\n- The authors assume that in the retrograde setting  the delayed error has access to the current synaptic weights and to $f’(x^\\top w)$ at the time of the original activation. This does some heavy lifting for the biological plausibility of the proposed mechanism for delayed learning since it requires that post-synaptic derivatives at the right delayed time, in order for CETs to match them to the right pre-synaptic trace.\n- As I understand it, the experiments are a slightly fine-tuned to make the proposed mechanism look good. For example, in the visual experiment the authors consider the time dimension to be the batch dimension and then convolve over it with the CET kernel. This makes the temporal credit-assignment problem much easier than in a genuine online  non-i.i.d. input stream. Later they need salience-based sparsification (top $1.25$% losses) to make the long-delay setting train at all. This sparsification strongly biases what is remembered and is itself a nontrivial extra mechanism, but it’s introduced as an implementation detail rather than a core assumption."}, "questions": {"value": "## Questions\n\n- See weaknesses\n- In the retrograde experiments, how is the postsynaptic derivative at $t-T$ stored and retrieved?\n- How would you select the optimal CET order and decay parameters?\n- In Fig. 5 the authors show that gradient alignment saturates even for high CET orders at long delays. What do you think is the limiting factor and how would you overcome this?\n- have you tried the framework on visual experiments with non-i.i.d. input streams?\n\n\n## Comments\n\n\n- In the MNIST/CIFAR experiments, the authors state that in the CIFAR dataset the performance of the classical eligibility traces deteriorates at any increasing delay value, while for the MNIST there is some robustness wrt to delays. They conclude that this indicates that “more complex visual tasks are less robust to imprecise time resolution” (lines 256-257). However I am not sure whether one can immediately state this as conclusion. The fact is that the two experiments use widely different architectures (MLP vs CNN)  \n> We use a 3-layer MLP (input→512 →512 →10) for MNIST and a small CNN with 3 convolutional layers (input →32 →64 →128) and two linear layers (512 →10) for CIFAR-10.\n\nFor the MNIST dataset the 512-512 architecture is probably on the generous side of the required size spectrum, and it can probably reach very low error even if the updates are a bit noisy or slightly wrong. The CIFAR model has moderate size for the dataset, and thus needs accurate gradients to get decent accuracy. Corrupting the gradients with temporal smearing results in performance that deteriorates right away with increasing delays in the CIFAR model, but this is not directly related to the fact that the task is complex, but rather because (probably) the model is at its expressivity threshold. While I understand that more complex tasks need larger networks, I would say that to disambiguate whether the drop in performance with increasing delays for the simple eligibility traces is directly related to the complexity of the task or the expressivity of the network for the given task, one should perform the same experiment for increasingly more/less expressive architectures and observe how results change. **I am not asking authors to perform these experiments**, I just mention this as a comment for their argument in lines 256-257. Also I don’t have good intuition on how the difference in the architecture MLP vs CNN could influence the results, but would be grateful if the authors could comment on this.\n\n- The idea of the cascading eligibility traces conceptually reminds me of the much earlier and much more primitive idea of temporal filterbank employed in [1] that shape the effective learning window. I think it would be interesting if the authors could connect their  more conceptually advanced idea to this earlier work.\n\n\n## Minor\n\n\n- the authors start several sentences the text with the phase “As well,…” which reads a bit ackward and from my side is the first time I encounter this phrase at the start of a sentence.\n- In Figure 3 in vertical axis I would propose to make the axis increasing as you go up in the figure (pot the origin at the  lower left)\n\n---\n## References\n\n[1] Porr, Bernd, and Florentin Wörgötter. \"Learning with “relevance”: using a third factor to stabilize Hebbian learning.\" Neural computation 19.10 (2007): 2694-2719."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N2JbNRJU0v", "forum": "yQ7ssakeKM", "replyto": "yQ7ssakeKM", "signatures": ["ICLR.cc/2026/Conference/Submission18991/Reviewer_kyjJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18991/Reviewer_kyjJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997710988, "cdate": 1761997710988, "tmdate": 1762931043822, "mdate": 1762931043822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores state space models as a model for cascading eligibility traces (CETs) to tackle the credit assignment problem of exponentially decaying eligibility traces.\nTuning the decay parameter of their CET lets them decide the time delay at which the CET peaks and assigns maximal credit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong performance with long delays, in some cases even comparable to backpropagation\n\n- CETs always outperform ETs\n\n - Strong performance on very long delays if input to CETs is zeroed for timesteps with low loss\n\n - Extensive experiments\n\n- Source code provided for reproducibility"}, "weaknesses": {"value": "Since I am not very familiar with the related work literature, I cannot assess the novelty of the work"}, "questions": {"value": "Does alpha need to be hand-tuned, or could it be learned if one thinks of a neural network-based application?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mMDCiNlZFU", "forum": "yQ7ssakeKM", "replyto": "yQ7ssakeKM", "signatures": ["ICLR.cc/2026/Conference/Submission18991/Reviewer_SqB5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18991/Reviewer_SqB5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762356281486, "cdate": 1762356281486, "tmdate": 1762931043426, "mdate": 1762931043426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}