{"id": "bpD4wXVFXV", "number": 4252, "cdate": 1757647287073, "mdate": 1763526019798, "content": {"title": "UAVDB: Point-Guided Masks for UAV Detection and Segmentation", "abstract": "The widespread use of Unmanned Aerial Vehicles (UAVs) in surveillance, security, and airspace monitoring demands accurate and scalable detection methods. Progress, however, is limited by the lack of large-scale, high-resolution datasets with precise yet cost-efficient annotations. To address these challenges, we present UAVDB, a benchmark dataset for UAV detection and segmentation, built through a point-guided weak supervision pipeline. UAVDB leverages trajectory point annotations and RGB video frames from a multi-view drone tracking dataset captured by fixed cameras. We introduce Patch Intensity Convergence (PIC), a lightweight annotation method that converts trajectory points into high-fidelity bounding boxes, eliminating manual labeling while maintaining accurate spatial localization. From these boxes, we further derive instance segmentation masks using SAM2, enabling rich multi-task annotations with minimal supervision. UAVDB captures UAVs across diverse scales, ranging from clearly visible objects to nearly single-pixel instances, under challenging conditions. Additionally, PIC is lightweight and readily pluggable into other point-guided scenarios, making it easy to scale up dataset generation across various domains. We quantitatively show that PIC outperforms existing annotation techniques in IoU accuracy and efficiency. Finally, we benchmark several state-of-the-art (SOTA) YOLO detectors on UAVDB, establishing strong baselines for future research. UAVDB and all associated tools will be publicly released to accelerate point-guided detection and segmentation research.", "tldr": "We present UAVDB, a benchmark for UAV detection and segmentation with point-guided PIC and SAM2, enabling accurate multi-task labels without manual effort and providing strong baselines.", "keywords": ["UAV detection", "weak supervision", "point-guided annotation", "instance segmentation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/827188e433563733ac6d96b7620219fb0f1a84e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper describes a new layer of annotations on top the the UAV tacking dataset of Li et al. (2020). These annotations are created from point annotations stemming from the 2020 dataset by using a point-based object detector (the proposed Point Intensity Convergence, PIC) and SAM2 to turn the bounding boxes into segmentation masks.\nThe proposed PIC is a heuristic for finding a good bounding box size assuming that: (1) the central point of the BB is given, (2) the BB is square and (3) the object is darker than the surroundings. It consists of starting with some fixed initial size and increasing the size a fix step at a time. The average pixel intensity is measured and an elbow in the intensity is detected when the (positive) change in intensity between one step and the next becomes smaller than a fixed threshold."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed approach is simple and easy to grasp. \n- The paper is also easy to read.\n- The proposed PIC is computationally very efficient."}, "weaknesses": {"value": "1. The proposed dataset builds heavily on the dataset of Li at al. (2020), making the contribution from the dataset perspective very limited.\n2. The main contribution is thus PIC. The proposed method is a simple heuristic that is very efficient to compute, but that is hardly a general method and is not sufficiently evaluated. More specifically:\n- PIC is compared against alternative method on a human-annotated dataset. There are insufficient details about this dataset to assess it’s qualities. This human-annotated benchmark should be one of the central pillars of the paper, since it is the only evaluation of the main contribution of the paper.\n- PIC’s simplicity also means it will fail to generalize when the main (unwritten) assumptions don’t hold, such as when the object is brighter than the background (thermal imagery? UAV’s with lights? Night scene?) or the desired BB is not square.\n- There are, at least, three important hyperparameters: init size, step size and threshold, but I couldn’t find any sensitivity analysis in the results.\n3. In the introduction and related works there are some general statements about the limitations of existing datasets (lines 74 to 76) and methods (lines 168 to 170), without pointing out specific limitations of each of the cited works. A table clearly showing the pros and cons of each work would be very useful here.\n4. Overall, I doubt this work fulfills the publication standards detailed in the ICLR reviewer guidelines: there is no novel idea being proposed, and the paper would be of interest to a very small fraction of the ICLR readership. Given the limited contribution, both in terms of datasets and methods, I would suggest to find a suitable workshop to submit this work to."}, "questions": {"value": "I have no outstanding question for the authors. However, I would strongly suggest them to address weaknesses 2 and 3 (hyperparameter sensitivity study and better positioning wrt sota) before submitting to another venue, be it a workshop at an ML conference or other."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u1uL5DgEoq", "forum": "bpD4wXVFXV", "replyto": "bpD4wXVFXV", "signatures": ["ICLR.cc/2026/Conference/Submission4252/Reviewer_zpq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4252/Reviewer_zpq3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580722807, "cdate": 1761580722807, "tmdate": 1762917255275, "mdate": 1762917255275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for the constructive reviews. I am withdrawing the paper at this time and will revise the methodology and experiments accordingly for a future submission."}}, "id": "fC7rovvdDU", "forum": "bpD4wXVFXV", "replyto": "bpD4wXVFXV", "signatures": ["ICLR.cc/2026/Conference/Submission4252/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4252/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763526018855, "cdate": 1763526018855, "tmdate": 1763526018855, "mdate": 1763526018855, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UAVDB, a high-resolution dataset for UAV detection and segmentation built upon an existing multi-view drone-tracking video collection. It proposes a lightweight automatic annotation method called Patch Intensity Convergence (PIC), which generates bounding boxes from trajectory points and uses SAM2 to produce instance masks. The authors benchmark several YOLO-series detectors to evaluate detection and segmentation performance under this weakly supervised labeling pipeline. Overall, the work focuses on automated annotation and benchmark construction for UAV perception tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is clearly written, with logical structure and high-quality figures that effectively illustrate the proposed pipeline.\n\n2.The proposed automatic annotation pipeline (PIC + SAM2) is simple, fast, and reproducible, offering practical value for reducing manual labeling cost in UAV datasets.\n\n3.The work provides a unified baseline for UAV detection under weak supervision and can serve as a useful engineering reference for future data construction research."}, "weaknesses": {"value": "1.Limited novelty: The core algorithm, Patch Intensity Convergence (PIC), is a simple heuristic based on iterative patch expansion and mean intensity convergence. It lacks theoretical justification or methodological innovation and is conceptually similar to classic region-growing and thresholding techniques.\n\n2.Dataset originality is low: UAVDB is mainly derived from an existing dataset (Li et al., 2020) with automatically generated annotations rather than new data collection. Thus, the dataset’s contribution lies more in re-labeling than in creating new sensing modalities or tasks.\n\n3.Dependence on external models: The use of SAM2 for mask generation offloads much of the contribution to a pretrained foundation model, reducing the authors’ own technical novelty.\n\n4.Limited experimental depth: Although many YOLO versions are benchmarked, the evaluation focuses only on detection accuracy and speed. There is no cross-dataset validation, robustness analysis, or ablation to show how PIC behaves under different lighting or motion conditions.\n\n5.Lack of theoretical or practical insight: The paper does not explain why the proposed intensity-based convergence is superior conceptually, nor how it might generalize to other weak-supervision or multimodal UAV scenarios."}, "questions": {"value": "1.Could the authors further explain the theoretical motivation behind Patch Intensity Convergence? Why does the intensity convergence threshold effectively delineate UAV boundaries? Have alternative local statistics (e.g., variance, gradient, or texture features) been explored instead of mean intensity?\n\n2.How is PIC fundamentally different from classic region-growing or adaptive thresholding techniques? Could the authors provide mathematical analysis or comparative experiments to better highlight its uniqueness?\n\n3.Since UAVDB is largely derived from Li et al. (2020), how do the authors plan to strengthen its independent contribution? Are there plans to incorporate multimodal data (e.g., infrared, depth) or temporal annotations in future versions?\n\n4.Given that the pipeline heavily relies on SAM2 for mask generation, how would the overall performance change if SAM2 were replaced by a smaller or non-foundation model?\n\n5.Have the authors tested this automatic annotation pipeline in non-UAV domains (e.g., traffic surveillance or small-object detection)? Demonstrating cross-domain generalization would significantly increase the paper’s impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P8gBzmMAfU", "forum": "bpD4wXVFXV", "replyto": "bpD4wXVFXV", "signatures": ["ICLR.cc/2026/Conference/Submission4252/Reviewer_Z4yc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4252/Reviewer_Z4yc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847551025, "cdate": 1761847551025, "tmdate": 1762917254443, "mdate": 1762917254443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UAVDB, a new benchmark dataset for UAV detection and segmentation, designed to address the limitations of existing datasets for long-range aerial surveillance. This paper proposes Patch Intensity Convergence (PIC) to generate high-quality bounding boxes from trajectory points, which are then used to prompt SAM2 for creating instance segmentation masks. The utility of UAVDB and the PIC method is validated through comprehensive benchmarks using multiple state-of-the-art YOLO detectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-organized and easy to understand. \n\n2. The proposed PIC method offers a training-free, computationally cheap, and effective way to convert simple point annotations into high-fidelity bounding boxes, dramatically reducing annotation cost.\n\n3. The paper provides a very thorough evaluation, benchmarking a wide array of YOLO models for object detection and instance segmentation, establishing strong baselines for future research on this dataset."}, "weaknesses": {"value": "（1）The core innovation is limited, as the work primarily repurposes an existing dataset and relies heavily on existing models like SAM2 for mask generation, while the resulting dataset's annotation quality and reliability remain unvalidated against human annotations.\n（2）The proposed PIC method requires manually set hyperparameters, and the lack of a sensitivity analysis or an adaptive mechanism raises concerns about its robustness and generalizability to other domains.\n（3）A critical limitation is the absence of comparative experiments against recent point-supervised or weakly-supervised detection methods, making it difficult to assess the true advantage of the PIC approach.\n（4）The UAVDB dataset is constructed from a single source, which potentially limits the diversity of scenarios and UAV types compared to incorporating multiple public datasets.\n（5）The segmentation pipeline depends entirely on SAM2, whose known limitations on very small objects could introduce systematic errors into the instance masks without proper validation or correction.\n（6）The experimental evaluation focuses solely on static image tasks, failing to leverage or demonstrate the dataset's potential for video-based temporal modeling or tracking applications."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xfFYHtXJJ8", "forum": "bpD4wXVFXV", "replyto": "bpD4wXVFXV", "signatures": ["ICLR.cc/2026/Conference/Submission4252/Reviewer_nyzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4252/Reviewer_nyzK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007383764, "cdate": 1762007383764, "tmdate": 1762917253880, "mdate": 1762917253880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Manuscript Withdrawal Notice"}, "comment": {"value": "Thank you for the time, effort, and constructive feedback. \n\nAfter carefully reflecting on the comments, I recognize that several aspects of the current submission can indeed be further strengthened. These include the dependence on SAM2 without sufficient validation, the absence of comparisons with recent point- or weakly-supervised methods, and the heavy reliance on an existing dataset with limited novelty. \n\nGiven these limitations, I acknowledge that the work does not yet meet the standard expected for this venue. Therefore, I have decided to withdraw the paper and will continue improving the methodology, evaluation, and dataset construction before preparing a future resubmission. \n\nThank you again for you review."}}, "id": "J9VT7GThaO", "forum": "bpD4wXVFXV", "replyto": "bpD4wXVFXV", "signatures": ["ICLR.cc/2026/Conference/Submission4252/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4252/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4252/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763525931546, "cdate": 1763525931546, "tmdate": 1763525931546, "mdate": 1763525931546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}