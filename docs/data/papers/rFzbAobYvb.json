{"id": "rFzbAobYvb", "number": 22887, "cdate": 1758336715908, "mdate": 1759896841252, "content": {"title": "PB²: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning", "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising ap-\nproach for learning behaviors from human feedback without predefined reward\nfunctions. However, current PbRL methods face a critical challenge in effectively\nexploring the preference space, often converging prematurely to suboptimal policies\nthat satisfy only a narrow subset of human preferences. In this work, we identify\nand address this preference exploration problem through population-based meth-\nods. We demonstrate that maintaining a diverse population of agents enables more\ncomprehensive exploration of the preference landscape compared to single-agent\napproaches. Crucially, this diversity improves reward model learning by generating\npreference queries with clearly distinguishable behaviors, a key factor in real-world\nscenarios where humans must easily differentiate between options to provide mean-\ningful feedback. Our experiments reveal that current methods may fail by getting\nstuck in local optima, requiring excessive feedback, or degrading significantly when\nhuman evaluators make errors on similar trajectories, a realistic scenario often\noverlooked by methods relying on perfect oracle teachers. Our population-based\napproach demonstrates robust performance when teachers mislabel similar trajec-\ntory segments and shows significantly enhanced preference exploration capabilities,\nparticularly in environments with complex reward landscapes.", "tldr": "", "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning", "Preference-based Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef2040f4c1337a84059d7d9299b64faef40be647.pdf", "supplementary_material": "/attachment/8652adba7dcacb4441fa28a6d53b644c402c169b.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PB², a population-based preference-based reinforcement learning (PbRL) framework designed to enhance exploration in the preference space. By training a diverse population of policies guided by a performance-constrained diversity bonus, PB² enables the generation of more informative and distinguishable preference queries, leading to better reward model learning and improved robustness against noisy human feedback. Experimental results demonstrate superiority over several state-of-the-art PbRL methods across various benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is clearly written and well-structured, the idea is intuitive.\n\n2. The paper provides an insightful analysis of exploration deficiencies in existing PbRL methods, particularly highlighting how ensemble-based and single-policy approaches fail to capture sufficient behavioral diversity."}, "weaknesses": {"value": "1. The paper lacks a formal theoretical analysis to support the claim that the proposed method effectively mitigates exploration challenges or addresses uncertainty, limiting the depth of its empirical contributions.\n\n2. The proposed approach introduces substantial computational overhead due to the maintenance and training of multiple policy networks and a discriminator, which may hinder scalability to more complex or real-time environments.\n\n3. The method depends on several critical hyperparameters (e.g., the diversity coefficient $\\lambda$ and performance threshold $\\alpha$) that are not adaptively tuned. This reliance may require domain-specific calibration, potentially limiting the method’s generalizability and ease of deployment in practice.\n\n4. The set of baseline comparisons, while representative, could be further expanded to include more recent or diverse PbRL methods, including RIME [1], direct alignment methods like CPL [2].\n\n[1] Cheng J, Xiong G, Dai X, et al. Rime: Robust preference-based reinforcement learning with noisy preferences. ICML 2024.\n\n[2] Hejna J, Rafailov R, Sikchi H, et al. Contrastive preference learning: learning from human feedback without rl. ICLR 2024."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9wAvA7pO4c", "forum": "rFzbAobYvb", "replyto": "rFzbAobYvb", "signatures": ["ICLR.cc/2026/Conference/Submission22887/Reviewer_adkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22887/Reviewer_adkb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640069539, "cdate": 1761640069539, "tmdate": 1762942426484, "mdate": 1762942426484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PB2 attempts to solve the core problem of preference space exploration. The authors point out that existing PbRL methods often suffer from premature policy convergence to suboptimal local optima. PB2 (Population-Based Preference-Based RL) is a population-based method. Its core idea is to explicitly maintain a population of agents with diverse behaviors, compelling the population to generate a series of high-return and behaviorally distinguishable trajectories to achieve efficient exploration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "• The paper accurately identifies a critical flaw in existing PbRL methods: query similarity. This is a very significant problem in real-world applications.    \n• The introduction of a similarity threshold, $\\epsilon$, to simulate human inconsistency when facing similar trajectories is highly practical. The experiments demonstrate that PB2's robustness in high-noise (high $\\epsilon$) environments far exceeds that of baseline methods.    \n• The experiments in the maze environment are very intuitive, showing how PB2 leverages its population diversity to successfully explore and find the globally optimal path."}, "weaknesses": {"value": "• Compared to single-agent methods like QPA, PB2's implementation complexity and the number of hyperparameters are significantly increased. It requires maintaining N policy networks, N corresponding Q-value networks, and an additional discriminator network $q_{\\psi}$. This leads to higher computational and memory overhead.        \n• In the grand scheme, PB2 can be seen as a diversity-based exploration strategy. I am convinced that it performs better in toy examples. However, in the DMC experiments, the variance and mean of most methods have severe overlap, making it difficult to discern a clear performance advantage for PB2.         \n• Appendix A.3 mentions several \"tricks\" to \"ensure stable and effective diversity guidance.\" However, traditional methods like PEBBLE and QPA already perform stably without much tuning, suggesting that this added complex design exacerbates training instability."}, "questions": {"value": "1. I am curious, compared to PEBBLE and RUNE, how would PB2 perform if it were used only as an auxiliary exploration strategy for them?     \n2. Does the PB2 mechanism allow me to drastically improve exploration efficiency and final performance by simply increasing the population size (e.g., to N=20)? Or would the discriminator's training become the new bottleneck at that point? Furthermore, in environments like DMC, is exploration itself truly the bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uiFxVsw4hY", "forum": "rFzbAobYvb", "replyto": "rFzbAobYvb", "signatures": ["ICLR.cc/2026/Conference/Submission22887/Reviewer_yKcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22887/Reviewer_yKcr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719885035, "cdate": 1761719885035, "tmdate": 1762942426238, "mdate": 1762942426238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PB² proposes a population-based method for PbRL to address insufficient exploration of the preference space and query ambiguity. The approach trains an anchor policy to exploit the current learned reward and multiple diverse policies encouraged via a discriminator-based mutual-information bonus under a performance constraint, yielding more distinguishable trajectories for preference queries. Experiments across navigation and DMControl tasks show improved feedback efficiency, greater robustness to noisy or inconsistent human labels, and the ability to escape local optima compared to single-agent baselines and a naïve posterior-sampling ensemble."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Maintaining behavioral diversity during the query stage can markedly increase the discriminability and stability of human comparisons, thus learning rewards more effectively and reducing feedback needs. The central problem addressed by this paper is indeed important.\n\nThe discriminator’s mutual-information objective directly optimizes for “distinguishable behaviors”, which aligns with the goal of making human comparisons easier and provides a principled mechanism for boosting information content and robustness. The design is reasonable and clear."}, "weaknesses": {"value": "1. As acknowledged by the authors, the computationalcost of the design is significant, which may limit practical usability.  \n2. Other relevant approaches deserve discussion. For example, PPE [1] manages data/distribution-side coverage and evaluation reliability; and PPE appears to report stronger results than the present method. The authors should provide fuller theoretical analysis and empirical comparisons. The SENIOR paper [2] is also highly relevant.  \n3. The mathematical presentation lacks clarity and rigor in several places, with notable inconsistencies that should be carefully checked, including but not limited to:  \n   a) **Algorithm 1 vs. main equations**: Algorithm 1 Line 12 uses  $ r_\\phi(\\tau) + \\lambda \\cdot q_\\psi(i), $   whereas the main method and Eqs. (3)/(4) use the **log-probability** conditioned on trajectories/states, e.g.,   $ \\log q_\\psi(i \\mid \\tau). $ \n   b) **Granularity of \\(q_\\psi\\) inputs**: Algorithm 1 does not specify whether \\(q_\\psi\\) takes entire trajectories \\(\\tau\\) or single states \\(s\\); Algorithm 2 uses a state-based information-gain form,   $ \\log q_\\psi(i \\mid s) - \\log p(i). $   The paper should unify the convention in the main text and annotate Eq. (4) accordingly.  \n4. There are typos and minor writing issues: “one of the main claim” → “one of the main claims”; “We also the naive implementation …” (incomplete sentence); “collect diverse trajectory” → “collect diverse trajectories.” These should be carefully corrected.  \n5. Stronger Bayesian query selection/uncertainty-modeling baselines could and should be compared against the proposed method.\n\n\nThe method’s performance advantages appear limited relative to its higher computational cost, which raises concerns about practical utility. The paper would benefit from more comprehensive validation to demonstrate effectiveness. In parallel, the manuscript requires careful, rigorous proofreading to resolve notational inconsistencies and typographical issues.\n\n\n[1] Zhu, Y., ... . (2024). Optimizing reward models with proximal policy exploration in preference-based reinforcement learning. In NeurIPS 2024 Workshop on Behavioral Machine Learning. \n\n[2] Ni, H., ... . (2025). SENIOR: Efficient query selection and preference-guided exploration in preference-based reinforcement learning. arXiv preprint arXiv:2506.14648."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U8Fw3FLQjk", "forum": "rFzbAobYvb", "replyto": "rFzbAobYvb", "signatures": ["ICLR.cc/2026/Conference/Submission22887/Reviewer_wJQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22887/Reviewer_wJQR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805975915, "cdate": 1761805975915, "tmdate": 1762942425844, "mdate": 1762942425844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PB², a novel population-based framework for preference-based reinforcement learning (PbRL) aimed at addressing the lack of behavioral diversity during user feedback collection. Traditional single-policy PbRL methods often converge to local minima in the preference space, limiting exploration and leading to suboptimal alignment with human preferences.\n\nPB² tackles this by training multiple distinct policies simultaneously, each encouraged to explore different behavioral modes through an explicit diversity bonus. These diverse policies generate varied trajectories, which are then compared using human preference feedback to train a shared reward model. A discriminator module maintains population diversity while ensuring that learned behaviors remain consistent with user preferences.\n\nExperimental results across DMControl locomotion and navigation tasks show that PB² produces more diverse and distinguishable behaviors, improves reward learning efficiency, and remains robust under noisy or inconsistent feedback. Additionally, the paper reveals that neural ensemble models fail to capture preference uncertainty effectively, offering little improvement over deterministic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of separating a reference policy and a diverse policy to serve as different purposes is insightful and reasonably novel in preference-based RL literature. \n2. The use of discriminator to differentiate between anchor and diverse incorporates the idea of adversarial learning into preference-based RL algorithms.\n3. Experimental results demonstrate performance gains in both sample and feedback efficiency in different types of teachers, which are more aligned with real world scenarios."}, "weaknesses": {"value": "1. Although the use of discriminator is different from previous work that only focuses on single-agent preference-based RL algorithms, are there implementation and experimental challenges as a result?"}, "questions": {"value": "1. Will the policy used for evaluation the same as the reference policy $\\pi_{\\text{ref}}$? Is it possible to somehow combine reference policy and diverse policy to see what are resulting behaviors, since exploration may also benefit a task-specific policy?\n2. It seems that choice of $\\alpha$ is based on heuristic, i.e. ideally the diversity-based exploration should contain meaningful behavior at least some portion of anchor policy. How sensitive is performance with different values of $\\alpha$?\n3. Is the discriminator trained to maximize mutual information between all trajectories collected by the diverse policy? Or to distinguish between the anchor policy and the remaining from diverse policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1jDWu3MJuW", "forum": "rFzbAobYvb", "replyto": "rFzbAobYvb", "signatures": ["ICLR.cc/2026/Conference/Submission22887/Reviewer_UaLC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22887/Reviewer_UaLC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162446181, "cdate": 1762162446181, "tmdate": 1762942425655, "mdate": 1762942425655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}