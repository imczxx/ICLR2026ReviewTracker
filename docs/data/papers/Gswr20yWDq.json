{"id": "Gswr20yWDq", "number": 19518, "cdate": 1758296918294, "mdate": 1763063955173, "content": {"title": "Addressing Missing and Noisy Modalities in One Solution: Unified Modality-Quality Framework for Low-quality Multimodal Data", "abstract": "Multimodal data encountered in real-world scenarios are typically of low quality, with noisy modalities and missing modalities being typical forms that severely hinder model performance and robustness. However, prior works often handle noisy and missing modalities separately. In contrast, we jointly address missing and noisy modalities to enhance model robustness in low-quality data scenarios. We regard both noisy and missing modalities as a unified low-quality modality problem, and propose a unified modality-quality (UMQ) framework to enhance low-quality representations for multimodal affective computing. Firstly, we train a quality estimator with explicit supervised signals via a rank-guided training strategy that compares the relative quality of different representations by adding a ranking constraint, avoiding training noise caused by inaccurate absolute quality labels. Then, a quality enhancer for each modality is constructed, which uses the sample-specific information provided by other modalities and the modality-specific information provided by the defined modality baseline representation to enhance the quality of unimodal representations. Finally, we propose a quality-aware mixture-of-experts module with particular routing mechanism to enable multiple modality-quality problems to be addressed more specifically. UMQ consistently outperforms state-of-the-art baselines on multiple datasets under the settings of complete, missing, and noisy modalities.", "tldr": "We propose a unified modality-quality framework to jointly address missing and noisy modalities for multimodal affective computing.", "keywords": ["Multimodal Affective Computing", "Noisy Modality", "Missing Modality", "Low-quality Multimodal Data"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f15fb388276fab8360e549dba3403694eba19243.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper jointly tackles robustness to both missing and noisy modalities in multimodal affective computing (MAC). It proposes a Unified Modality-Quality (UMQ) framework with three key pieces: \n- (i) a quality estimator trained using explicit anchors and a rank-guided pairwise loss so the model learns ordinal quality without relying on brittle absolute labels; \n- (ii) a quality enhancer that “decouples” each unimodal feature into sample-specific and modality-specific parts;\n- (iii) a Modality-Quality-aware Mixture-of-Experts (MQ-MoE) with routing and regularizers that encourages consistent expert assignment under similar quality configurations.\n\nEmpirically, UMQ matches or exceeds recent robust baselines under complete inputs, outperforms methods like MMIN, GCNet, IMDer, MoMKE under missing modalities (varying missing rates), improves over C-MIB and Multimodal Boosting under noisy modalities (Gaussian corruption of features)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality / Idea quality**. Casting both missing and noisy modalities as \"low-quality\" and learning an ordinal quality estimator with explicit anchors plus rank-guided training is thoughtful; it avoids brittle absolute labels and fits naturally with routing.\n- **Architecture design**. The decoupling into sample-specific vs. modality-specific subspaces and the quality enhancer that borrows cross-modal sample-specific information are intuitive and empirically helpful via illustrated experiments.\n- **Empirical breadth**. Competitive or SOTA results are shown (i) in complete-modality; (ii) for missing modalities across a range of missing rates; (iii) for noisy modalities, indicating task generality."}, "weaknesses": {"value": "- **Concerns about high-quality anchors**.  The \"highest-quality\" anchor uses a low unimodal predictive loss threshold. This might not be a robust indicator, as it may risk quality to reflect ease of the task label for that modality, or potentially rewarding label-leakage or majority cues rather than modality fidelity.\n- **Noise realism**. The paper primarily uses Gaussian feature corruption and treats missing as \"extreme noise\". In fact, degradation could come from misalignment (e.g. between audio and video signal – which is quite popular in MAC applications), video frame drop, … \n- **Potential misuse of Mutual Information terminology**. I believe the constraint presented is a distance/similarity regularizer, not an MI estimator/variational bound. The authors should clarify this terminology usage.\n- **Limited applications**. While some ideas are interesting and parts of the architecture are cleverly designed, the authors choose to apply the method on MAC applications. This is somewhat restrictive for the applicability of the method. I suggest the Authors add an adaptation or provide guidance to extend the pipeline to other tasks."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CPMZyjMoty", "forum": "Gswr20yWDq", "replyto": "Gswr20yWDq", "signatures": ["ICLR.cc/2026/Conference/Submission19518/Reviewer_qgnE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19518/Reviewer_qgnE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951558934, "cdate": 1761951558934, "tmdate": 1762931411575, "mdate": 1762931411575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "CmOeTTaqkQ", "forum": "Gswr20yWDq", "replyto": "Gswr20yWDq", "signatures": ["ICLR.cc/2026/Conference/Submission19518/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19518/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763063954392, "cdate": 1763063954392, "tmdate": 1763063954392, "mdate": 1763063954392, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempted to propose a unified framework for handling both noisy and missing modalities in multimodal learning. The proposed unified modality-quality (UMQ) archives this goal by the following three modules: quality estimator, quality enhancer, and modality-quality-aware mixture of experts. The proposed approach is evaluated on several multimodality learning tasks that combine information from 3 input modalities, including visual, acoustics, and language."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. UMQ outperforms the compared methods on the benchmarks.\n\n2. The concept of developing a unified approach to handle both noisy and missing modalities is technically sound and addresses an important problem"}, "weaknesses": {"value": "1. Overall, although the proposed framework reports state-of-the-art performance, it appears rather complex, with multiple tightly coupled components and hyperparameters. Beyond the reported results, it remains unclear whether this work meaningfully advances the field or stimulates further discussion within the research community.\n\n2. The design of the UMQ framework is not theoretically grounded. Its design lacks an in-depth mathematical justification or toy-example simulation on how and why each component is important for the final framework.\n\n3. I had a hard time going through the content of this paper and have several comments on the presentation of the paper: 1) The text within the figures is overly dense and too small to read clearly. 2) The paper is difficult to follow overall, especially in the algorithm section, where equations are presented in a cluttered manner and frequently referenced back to the main text. This disrupts the reading flow and makes comprehension challenging."}, "questions": {"value": "- Line 218: Since this is a learnable parameter, what ensures that the bias embedding term effectively compensates for the error? Are there any experiments or simulations that validate this claim?\n- How are the predefined hyperparameters $\\eta$, $m$, and $\\gamma$ (line 292) selected? More generally, how many hyperparameters does the proposed framework involve in total, and what strategy is used for their selection?\n- What is the exact implementation of the quality enhancer EH operator in Eq. 8?\n- What is R in Eq. 10?\n- In Eq. 17, what is the difference between L_m^{est} and L_m?\n- In Eq. 20, how is $\\alpha_m$ constructed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Smo1Gb4wSp", "forum": "Gswr20yWDq", "replyto": "Gswr20yWDq", "signatures": ["ICLR.cc/2026/Conference/Submission19518/Reviewer_ogxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19518/Reviewer_ogxu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156430517, "cdate": 1762156430517, "tmdate": 1762931411192, "mdate": 1762931411192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Unified Modality-Quality (UMQ) framework comprising three key components that jointly address the challenges of missing and noisy modalities in multimodal affective computing. Extensive experiments on five benchmark datasets—CMU-MOSI, CMU-MOSEI, CH-SIMS, UR-FUNNY, and MUStARD—demonstrate that UMQ consistently outperforms prior methods under both complete and degraded modality conditions."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive experiments: The authors evaluate the approach under multiple conditions (complete, missing, and noisy modalities) and across several datasets.\n\n2. Ablation studies and visualizations: They carefully analyze the effect of each component (estimator, enhancer, MQ-MoE) and visualize the improvement qualitatively."}, "weaknesses": {"value": "Major:\n\n1. The manuscript is poorly written, with awkward phrasing and overly long sentences (e.g., in the abstract) that obscure the main ideas. Clarity is a major problem throughout. \n\n2. The paper lacks a formal definition or analysis showing why missing modalities can be treated as a subclass of noisy modalities from an information-theoretic or probabilistic perspective.\n\n3. Besides manually added noise, can the proposed method also handle naturally occurring ones, such as those caused by poor devices?\n\n\nMinor:\n\nMultiple formulas are stacked together (e.g., 2–5) and explained with excessive text, while key model construction details are placed in the appendix. This structure obscures the main idea."}, "questions": {"value": "Please think carefully about the issues mentioned in the **weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vwWVZNIgra", "forum": "Gswr20yWDq", "replyto": "Gswr20yWDq", "signatures": ["ICLR.cc/2026/Conference/Submission19518/Reviewer_U1S7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19518/Reviewer_U1S7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762364515749, "cdate": 1762364515749, "tmdate": 1762931410746, "mdate": 1762931410746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}