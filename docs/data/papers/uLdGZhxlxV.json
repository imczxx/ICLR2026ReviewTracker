{"id": "uLdGZhxlxV", "number": 7958, "cdate": 1758046162228, "mdate": 1759897819868, "content": {"title": "Gradient Inversion Attacks Beyond SGD", "abstract": "Gradient Inversion Attack (GIA) poses a significant threat to federated learning, enabling adversaries to reconstruct private training data from the information shared during training. Prior research has predominantly focused on the vanilla SGD, where the server or an eavesdropper can directly observe true gradients. In practical deployments, however, models may be trained with adaptive optimizers (e.g., Adam, RMSProp, and AdaGrad), for which the observable signal is not raw gradients but momentum-based parameter updates. This setting remains underexplored and undermines traditional gradient-matching strategies, which struggle to recover labels and images from non-gradient updates. To address this gap, this paper explores attacks tailored to modern adaptive optimizers. We present an analytical rule for recovering labels from optimizer updates and propose an update-matching objective that optimizes dummy inputs to reproduce the observed updates. The proposed approach is general and can be directly applied to various optimizers such as Adam, AdaGrad, and RMSProp. Furthermore, we find that, despite being introduced for adaptive optimizers, the proposed objective function also yields stronger attacks in the standard SGD setting. Experiments on datasets such as ImageNet and PACS highlight the effectiveness of our method over existing gradient matching techniques.", "tldr": "This paper presents new gradient inversion attacks tailored to optimizers beyond SGD.", "keywords": ["Gradient Inversion Attack", "Federated Learning", "Privacy Leakage", "Adaptive Optimizer"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd7a543e4722070b8963885311dca55b448a31dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates an interesting problem in federated learning, where the authors propose the first attack approach towards FL systems with various optimizers, such as Adam. To this end, this paper designs an analytical rule for recovering labels from real optimizer updates and proposes an update-matching objective that optimizes dummy inputs to match the observed updates. Extensive experiments show that the proposed strategy exhibits great efficacy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper discusses an important scenario beyond simply using SGD, which fills the blank in this field.\n2. The authors propose two novel techniques, i.e., an analytical rule for label recovery from optimizer updates and an update-matching optimization objective, which effectively recover private images from gradients.\n3. The authors provide sufficient experiments and reveal the superiority of the proposed attack.\n4. The analytical method for label extraction is correct and interesting."}, "weaknesses": {"value": "1. From my view, the major contributions of this paper lie in the novel label extraction technique, as the so-called update-matching objective is an intuitive extension of the previous loss function. \nWhile I appreciate the derivation of this label inference technique, I do think the authors should reorganize this work to emphasize their core contributions.\n\n2. Moreover, why don't the authors combine the proposed techniques with existing approaches? I believe the optimization objective and label inference can be effectively combined with existing gradient inversion methods, such as GIAS and GIFD. Considering the proposed mechanisms as a plug-and-play strategy for transferring existing methods to non-SGD optimization settings would better highlight your contributions.\n\n3. The authors refer to Figures 1 and 2 in Sec. Introduction. However, there is a large gap between the text and the figure, affecting the readability of the manuscript."}, "questions": {"value": "I'm curious about the performance of generative attacks (e.g., GIFD) combined with the proposed methods under the Adam optimizer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jTMCl24d4k", "forum": "uLdGZhxlxV", "replyto": "uLdGZhxlxV", "signatures": ["ICLR.cc/2026/Conference/Submission7958/Reviewer_CajD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7958/Reviewer_CajD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600404515, "cdate": 1760600404515, "tmdate": 1762919974654, "mdate": 1762919974654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a significant gap in the field of Gradient Inversion Attacks (GIAs) by developing a method effective against Federated Learning (FL) systems that use adaptive optimizers (like Adam, RMSProp, AdaGrad), whereas prior attacks primarily worked only with the vanilla Stochastic Gradient Descent (SGD) optimizer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper extends the threat model of Gradient Inversion Attacks to the realistic and prevalent scenario where clients use adaptive local optimizers.\n2. By introducing a method to analytically recover labels from optimizer updates and a powerful update-matching objective for image reconstruction, the authors demonstrate that privacy risks in FL are more severe than previously thought, as adaptive optimizers alone do not provide sufficient protection.\n3. This paper highlights a need for developing new, robust defense mechanisms tailored to secure FL systems that employ modern optimization techniques."}, "weaknesses": {"value": "1. The attack's success is predicated on a very specific and powerful threat model that may not always hold in practice. The attack requires the attacker to know the exact initial state (e.g., $m_{t-1}$, $v_{t-1}$) of the client's optimizer at the start of the local training round. While the paper justifies this by citing FL algorithms that synchronize this state, many practical and privacy-preserving implementations do not. If a client uses a locally maintained, non-reset state unknown to the server, the entire analytical label recovery method, which is the cornerstone of the attack, would fail.\n2. The attack is demonstrated on a single local update step. In real FL, clients typically perform multiple local steps before sending an update. It is unclear how the attack would perform when the observed update is an aggregate of multiple optimization steps, which would significantly obfuscate the relationship between the final update and the data from the first step.\n3. All main experiments are conducted on ResNet-18. It is unclear how the attack would fare against architectures without a traditional fully-connected final layer (e.g., Transformers using a linear projection, or models with heavy use of GroupNorm) where the derived gradient constraints may not hold.\n4. The attack is exclusively validated on image data. A discussion on the potential applicability to other modalities like text (where labels might be sequences) or tabular data is absent."}, "questions": {"value": "1. How would your attack perform in a more challenging and realistic scenario where the local optimizer state is not reset and synchronized by the server at the beginning of each round, but is instead maintained privately by the client? Would the attack still be feasible without precise knowledge of $m_{t-1}$ and $v_{t-1}$?\n2. The attack is demonstrated for a single local training step. What is the expected degradation in reconstruction quality if a client performs K>1 local steps, and the server only observes the aggregated parameter update? Could your method be extended to this more common FL setting?\n3. The label recovery method in the \"post-initial\" phase relies on the observation that ambiguous gradient elements cluster in specific rows. Can you provide a theoretical intuition or more rigorous empirical analysis for why this clustering occurs? Is this a general phenomenon or something dependent on the specific dataset and model?\n4. The \"initial phase\" label recovery critically depends on Approximation 1 (Inter-class Low Entanglement). Can you quantify the sensitivity of your attack to the validity of this approximation? How does the reconstruction quality degrade as the model moves away from this \"initial\" state and the approximation becomes less valid?\n5. You mention secure aggregation as a defense, but do not show results. Have you tested your attack in a setting where updates from even a small number of clients (e.g., 2-10) are aggregated? Furthermore, how does your method perform against other standard defenses like the addition of differential privacy noise or aggressive gradient compression?\n6. Your gradient constraints are derived for a standard fully-connected final layer. Can your label recovery method be directly applied to models with a different final layer structure, such as a Vision Transformer (ViT) which uses a single linear layer on the [CLS] token? If not, what modifications would be required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ubPQCUN4bm", "forum": "uLdGZhxlxV", "replyto": "uLdGZhxlxV", "signatures": ["ICLR.cc/2026/Conference/Submission7958/Reviewer_KHeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7958/Reviewer_KHeN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442352348, "cdate": 1761442352348, "tmdate": 1762919974304, "mdate": 1762919974304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel Gradient Inversion Attack (GIA) tailored for federated learning (FL) that uses adaptive optimizers (e.g., Adam), where only momentum-based updates, not raw gradients, are accessible. The method employs an update-matching objective for image reconstruction and an analytical rule to recover labels from the observed updates. Experiments on ImageNet and PACS show it significantly outperforms existing gradient-matching techniques, even achieving stronger attacks in the traditional SGD setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a gap in GIAs by successfully targeting FL systems that use modern adaptive optimizers like Adam. Prior GIAs largely assumed direct access to raw SGD gradients, but this work shows how to attack the more complex momentum-based updates.\n\n2. The proposed method, which utilizes an update-matching objective and analytical label recovery, significantly outperforms previous GIAs. It achieves notably higher PSNR and SSIM scores, resulting in reconstructions that closely resemble the original private data.\n\n3. The \"update-matching\" objective is robust, generalizing effectively across various adaptive optimizers (RMSProp, Adagrad, etc.). Furthermore, the new objective function provides a stronger attack, even surpassing traditional gradient-matching attacks in the standard SGD setting."}, "weaknesses": {"value": "1. The paper's overall contribution is trivial, as the image optimization objective (Eq. 3) is an incremental change, substituting gradients with optimizer updates in an established cosine similarity loss similar to IG (Geiping et al., 2020). Moreover, the analytical label recovery relies on existing techniques, drawing foundational principles from GIAs (Eq. 6) and assumptions (Approximation 1 and Assumption 1) previously proposed in iLRG (Ma et al., 2023) and GradientInv (Yin et al., 2021), respectively. Consequently, the main technical novelty is confined to the specific derivation that infers the final-layer gradient information exclusively from the Adam-like optimizer's update parameters.\n\n2. The paper overstates its first challenge (Objective Function) since the optimizer update $\\mathcal{U}(\\nabla\\theta, s)$ shares the same dimensionality as the raw gradient $\\nabla\\theta$ and model parameters $\\theta$. Although it is not the exact gradient $\\nabla\\theta$, $\\mathcal{U}(\\nabla\\theta, s)$ differs only by a further step of computation. Hence, it is very straightforward for gradient matching.\n\n3. The attack is significantly constrained as its high efficacy is primarily limited to the initial training phase, with performance severely degrading thereafter. Furthermore, the analytical label recovery for this critical early phase relies on highly idealized constraints, such as the Inter-class Low Entanglement Approximation and Non-negative Activation Assumption."}, "questions": {"value": "1. Given that label recovery is core to the attack, what is the quantitative accuracy of the inferred labels across various model architectures and datasets used in your experiments?\n\n2. Assumption 1 only holds for ReLU and Sigmoid. How can the analytical label recovery method be adapted to accommodate other activation functions, such as Tanh or GeLU?\n\n3. If clients perform multiple local training updates before sending the final aggregation, how does this process of local averaging affect the overall effectiveness of the proposed attack?\n\n4. The theory primarily derives from the Adam update rule (Eq. 4). How does a differing mechanism in other adaptive optimizers affect the generalizability and theoretical soundness of the label recovery process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HG7SYy1jns", "forum": "uLdGZhxlxV", "replyto": "uLdGZhxlxV", "signatures": ["ICLR.cc/2026/Conference/Submission7958/Reviewer_1dEy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7958/Reviewer_1dEy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644833602, "cdate": 1761644833602, "tmdate": 1762919973898, "mdate": 1762919973898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper broadens Gradient Inversion Attacks to the realistic case where the adversary observes parameter updates rather than raw gradients. It provides an analytic label-recovery step from updates plus a general update-matching loss built from the optimizer rule, achieving sizable reconstruction gains and even outperforming gradient-matching in standard SGD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A clean reframing of the threat model covering adaptive optimizers used in practice.\n\n2. Strong empirical lift across optimizers/datasets, including large-scale cases."}, "weaknesses": {"value": "1. Reliance on optimizer state knowledge. The approach assumes access to optimizer state each round like moments and variance. In many federated settings, such state may be hidden. Even small mismatches can degrade label algebra or update alignment, potentially reducing attack reliability.\n\n2. Uncertain robustness under defenses. Schemes like secure aggregation, compression, randomized quantization, or DP noise could distort updates and optimizer states. The paper shows strong results without such defenses, but doesnâ€™t quantify how much distortion is needed to break the analytic step."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OOj8nwEEvG", "forum": "uLdGZhxlxV", "replyto": "uLdGZhxlxV", "signatures": ["ICLR.cc/2026/Conference/Submission7958/Reviewer_5Jp8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7958/Reviewer_5Jp8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977251862, "cdate": 1761977251862, "tmdate": 1762919973383, "mdate": 1762919973383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}