{"id": "xjPtzMoqVP", "number": 5169, "cdate": 1757859775305, "mdate": 1762920339804, "content": {"title": "CORAL: Correspondence Alignment For Controllable Person Image Generation", "abstract": "Virtual Try-On (VTON) aims to outfit a person with a specific garment from paired\nperson and garment images. Recent diffusion-based approaches show promising\nresults but still struggle to preserve fine-grained details such as logos, patterns, and\ntextures. We suggest these failures come from inaccurate query–key matching in\nattention maps. To analyze this, we introduce a correspondence evaluation frame-\nwork that extracts dense correspondences from attention maps and evaluates them\nwith pseudo ground-truth matches. Using this framework, we analyze a simple\nDiT-based baseline and observe that its attention maps in most layers fail to cap-\nture reliable semantic correspondences. We then propose CORAL, a lightweight\nregularization strategy with two components: correspondence loss, which cor-\nrects where each query attends by aligning it with reliable external matches, and\nentropy loss, which sharpens attention for more confident matching. CORAL\nimproves person–garment alignment in our baseline and can be applied to other\ndiffusion-based pipelines without architectural changes.", "tldr": "", "keywords": ["virtual try-on", "dense matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/feae4d1436a99d44c0adeb5044acaf855f93c7de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CORAL, a regularization strategy to improve person–garment alignment in diffusion-based virtual try-on. The core idea is to (1) align attention-based correspondences between person queries and garment keys using pseudo ground truth from DINOv3, and (2) sharpen attention with an entropy minimization loss so matches are more localized. The authors first analyze a DiT-based baseline by extracting dense correspondences directly from multi-modal attention and evaluating them with a PCK metric built from DINOv3 matches, finding weak and diffuse query–key alignment across layers/timesteps. CORAL then adds a correspondence loss (soft-argmax L2 to DINOv3 matches) and an entropy loss across layers. On VITON-HD and DressCode, CORAL improves FID/KID and preserves fine details like small logos and textures over a DiT baseline trained with standard diffusion losses. The method requires no architectural changes and is trained with LoRA on FLUX.1-dev."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Frames VTON as attention-level person–garment correspondence within DiTs, rather than only improving U-Net warping or photometric supervision. The DINOv3-distilled correspondence objective at attention level plus an explicit entropy regularizer is a neat combination that targets the failure mode (diffuse attention) head-on. \n2. Presents a diagnostic framework: extracts attention correspondences across layers/timesteps and evaluates them against DINOv3-based pseudo-GT via PCK, revealing systematic weaknesses in the baseline and motivating the loss design. Figures show layer/timestep grids and quantitative PCK trends. \n3. The losses are clearly specified (soft-argmax coordinate expectation; Shannon entropy over attention; total loss with λ’s), and training/inference details (datasets, metrics, LoRA/FLUX.1-dev setup) are provided. \n4. Empirical gains over the baseline are consistent on two datasets across paired/unpaired settings (FID/KID, with SSIM/LPIPS in paired).\n5. If attention alignment is indeed a key bottleneck in diffusion VTON, a plug-and-play regularizer that materially sharpens and aligns attention could be widely adopted across DiT-style pipelines and potentially beyond.."}, "weaknesses": {"value": "1. Missing SOTA comparison table. The quantitative table compares only Baseline vs Baseline+CORAL, with no head-to-head against contemporary VTON systems (e.g., StableVITON, IDM-VTON, Leffa, CATVTON, etc.). For ICLR, a comprehensive table across standard metrics/datasets is expected. This omission makes it hard to judge real-world significance beyond the authors’ baseline. (See Table 1: only two rows per dataset.)\n2. Figure polish and presentation quality. Many figures (attention maps, correspondence grids, qualitative visuals) look rough and under-annotated: axis labels, scale bars, consistent color maps, and high-resolution crops are needed for ICLR standards. Some composites are cramped; captions could better specify layers/timesteps/heads and exact visualization procedures. (E.g., multi-panel correspondence visualizations and qualitative comparisons.)\n3. Ablation/detail gaps. Loss-weight sensitivity (λ_corr, λ_ent) is not explored; the text notes empirical choices but no curves or robustness ranges. \n4. Overhead analysis is missing: how much training cost (DINOv3 feature extraction, added losses) and inference cost (if any) does CORAL add? The analysis reports layer/timestep behavior, but a systematic entropy profile (before/after, per layer) and PCK@α curves would strengthen the case.\n5. Only VITON-HD and DressCode are reported; no in-the-wild or cross-domain robustness tests."}, "questions": {"value": "1. Please add a full comparison table on VITON-HD and DressCode versus recent methods cited in your related work, reporting FID/KID and standard paired metrics (SSIM/LPIPS). If possible, include per-category (upper/lower/dress) breakdowns on DressCode. \n2. Overhead and practicality. What is the training compute/time overhead of CORAL (DINOv3 feature extraction + extra losses) relative to the baseline? Is there any inference-time cost? A table with wall-clock and GPU memory would clarify practicality. \n3. Loss behavior and stability. Provide λ_corr/λ_ent sensitivity (e.g., grid or sweep) and report how attention entropy and PCK change.\nDo you observe failure modes (e.g., over-sharp/peaky attention that latches onto wrong keys)? Any mitigation (temperature scaling, head dropout)?\n4. Analysis depth. Please include layer-wise entropy curves (before vs after) and PCK@α curves to complement the qualitative grids.\nCan you show failure cases (repetitive patterns, text, specular materials) with attention overlays and discuss why CORAL fails/succeeds? \n5. Suggestions for rebuttal. Substantially upgrade figure quality (resolution, consistent layouts, readable colormaps/legends, clearer captions; include high-res zooms on logos/patterns and side-by-side with SOTA)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KsVWJeADB1", "forum": "xjPtzMoqVP", "replyto": "xjPtzMoqVP", "signatures": ["ICLR.cc/2026/Conference/Submission5169/Reviewer_awEs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5169/Reviewer_awEs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754312662, "cdate": 1761754312662, "tmdate": 1762917927720, "mdate": 1762917927720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Y9kLSPufdc", "forum": "xjPtzMoqVP", "replyto": "xjPtzMoqVP", "signatures": ["ICLR.cc/2026/Conference/Submission5169/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5169/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920337878, "cdate": 1762920337878, "tmdate": 1762920337878, "mdate": 1762920337878, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the virtual try-on task, which aims to synthesize a person wearing a specific garment, given an identity image of the person and an image of the target clothing. The authors specifically focus on establishing the correspondence between the human body and the clothes. To this end, they propose aligning the attention map of the generation model with a pseudo ground truth derived using DINOv3. Furthermore, they introduce an attention entropy loss to regularize the sharpness of the attention map, thereby encouraging it to focus more precisely on the corresponding region."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The rationale for this work is clear and well-justified. \n\nThe paper is clearly presented and well-structured. \n\nThe empirical results demonstrate a strong performance."}, "weaknesses": {"value": "1. Lack of sufficient comparison with previous virtual try-on methods. All results appear to be conducted against a simple baseline. Since many other virtual try-on methods exist, a broader comparative evaluation is necessary.\n\n2. Lack of sufficient discussion regarding previous correspondence alignment methods. Papers such as [1] and [2] also discuss the utilization of correspondence. The authors should provide a fair comparison or discussion of how their method relates to these existing approaches.\n\n[1] Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment. ECCV 2024.\n\n [2] INCORPORATING VISUAL CORRESPONDENCE INTO DIFFUSION MODEL FOR VIRTUAL TRY-ON. ICLR 2025."}, "questions": {"value": "1. The discussion of related work and previous methods is insufficient. (Please refer to the **Weakness** section.)\n2. Figure 4 is counter-intuitive and requires clarification: Please explain why the seemingly suboptimal correspondence in panel (c) (compares with (b)) leads to superior final wrapped results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OIW6dh118F", "forum": "xjPtzMoqVP", "replyto": "xjPtzMoqVP", "signatures": ["ICLR.cc/2026/Conference/Submission5169/Reviewer_soKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5169/Reviewer_soKg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982589399, "cdate": 1761982589399, "tmdate": 1762917927108, "mdate": 1762917927108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the research of Virtual Try-On (VTON) in the field of controllable person image generation. It identifies a critical limitation of existing diffusion model-based VTON methods: their inability to preserve fine-grained clothing details (e.g., logos, patterns, textures). The root cause of this issue is pinpointed as inaccurate query-key matching in attention maps. To address this problem, the paper makes two core contributions: first, it proposes a \"correspondence evaluation framework\" that extracts dense correspondences from attention maps and conducts evaluation using pseudo-ground-truth matching; second, it introduces the CORAL lightweight regularization strategy, which consists of a \"correspondence loss\" (aligning query attention with reliable external matches) and an \"entropy loss\" (sharpening attention to improve matching confidence). Notably, the CORAL strategy can be adapted to existing diffusion models without modifying their architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed methods exhibit strong logical coherence: the evaluation framework identifies attention misalignment, while the CORAL strategy directly addresses this issue through targeted loss functions. This end-to-end problem-solution pipeline ensures the methods are theoretically sound and well-aligned with the research goal."}, "weaknesses": {"value": "The core idea of \"correspondence learning\" in this paper bears notable similarities to the method proposed in Cross-domain Correspondence Learning for Exemplar-based Image Translation. However, the paper fails to explicitly highlight the essential differences between the two works.\n\nThis paper heavily lacks comparison with state-of-the-art works in virtual try-on field"}, "questions": {"value": "No detailed ablation study are provided？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8x3yGCxmm", "forum": "xjPtzMoqVP", "replyto": "xjPtzMoqVP", "signatures": ["ICLR.cc/2026/Conference/Submission5169/Reviewer_D2C4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5169/Reviewer_D2C4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010971935, "cdate": 1762010971935, "tmdate": 1762917926862, "mdate": 1762917926862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}