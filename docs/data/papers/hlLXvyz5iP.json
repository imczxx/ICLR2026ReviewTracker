{"id": "hlLXvyz5iP", "number": 3000, "cdate": 1757314755491, "mdate": 1763719539526, "content": {"title": "Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning", "abstract": "Recent breakthroughs in reasoning language models have significantly advanced text-based reasoning. On the other hand, Multi-modal Large Language Models (MLLMs) still lag behind, hindered by their outdated internal LLMs. Upgrading these is often prohibitively expensive, as it requires complete vision-language alignment retraining which is costly. To address this issue, we introduce Perception-Reasoning Decoupling,  which modularizes the MLLM’s reasoning component and makes it easily replaceable. This approach redefines the MLLM's role to convert multi-modal inputs into detailed textual outputs that can be processed by any powerful, external, text-only LLM reasoners. To align the MLLM's perceptual output with the final reasoning task, we propose a novel reinforcement learning algorithm called Visual Perception Optimization (VPO). VPO rewards the MLLM based on the correctness of answers generated by the external reasoner to produce faithful and query-relevant captions. Together, this decoupling pipeline and VPO form our Reasoning-Aligned PerceptIon Decoupling (RAPID) approach. Empirical results show that RAPID achieves significant performance gains on multi-modal reasoning benchmarks. Crucially, RAPID enables a novel inference-time scaling paradigm: Once trained with VPO, the MLLM can be paired with any state-of-the-art LLM reasoner for consistent performance improvement without retraining. The implementation of our method is available at: https://anonymous.4open.science/r/RAPID2-80CD/.", "tldr": "", "keywords": ["MLLM", "multi-modal", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98f5a72e84122afdc5b62508713b6eb25464c950.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RAPID, a method to decouple the perception and reasoning capabilities of multi-modal large language models (MLLMs). Compared to prior methods which aim at improving reasoning of MLLMs through end to end reinforcement learning based approaches, RAPID essentially separates the process into two stages; 1. perception: where the MLLM is asked to generate high quality captions describing an image and 2. reasoning, where the captions are fed to a stronger language model to answer the question. They also propose VPO, a policy gradient method to improve the MLLMs capabilities to produce high quality captions by assigning rewards based on a stronger LLM which takes as input the captions and generates the answer. The method shows strong performance on most multi-modal benchmarks, beating existing approaches. They also perform detailed ablations of the different components of their pipeline. Another advantage of RAPID is that it enables multiple LLMs to be used for the same MLLM backbone and allows for inference time scaling where a more powerful LLM can be used to generate solutions as test time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an important technique of decoupling perception of MLLMs from reasoning, which allows for modularization and more customizable design choices \n2. The paper introduces VPO, a method to improve the perception and captioning capabilities of MLLMs which further helps improve downstream performance on several multi-modal benchmarks by 3-4%\n3. The various components like decoupling, GRPO, VPO, caption penalty are ablated well which should help other researchers with their design choices while training multi-modal language models with policy gradient methods. The authors also ablate other aspects such as prompts being fed to the MLLM and the LLM and the types of LLMs used during training and inference, providing valuable insights.  \n4. The technique allows for the same MLLM backbone to be used with several LLMs and also allows for inference time compute scaling by using a more powerful LLM."}, "weaknesses": {"value": "1. In Table 1, the paper lacks comparison of using existing MLLM baselines as the perception module with an LLM (Qwen3-8B/GPT-OSS-120B) as the reasoner. While it is clear that RAPID helps improve performance of Qwen2.5-VL, the table lacks performance of just using the decoupling strategy with the other MLLM baselines. For instance, if we look at Table 2 A vs B, we see that there is a 5.5% boost just from the decoupling. It will be good to include similar performance metrics for the other baselines. \n2. Adding on to point 1, RAPID is only tested with Qwen2.5-VL as the MLLM backbone. While it is clear that RAPID generalizes to different reasoning LLMs, it will be good to show how well RAPID performs with some of the other MLLM backbones like InternVL or Ovis2 to shed light on its generalizability. \n3. Since one of the major claims is that VPO helps improve quality of captions/textual outputs, it will be good to compare how VPO performs against some of the other baselines like OmniCaptioner or MM-Eureka-7B. Currently the authors only include captioning win rates before and after VPO on the same MLLM backbone in Figure 11.\n4. One drawback is that while VPO helps improve the overall performance and the reasoning of the decoupled system, looking at Table 2 (rows H vs I), the reasoning capabilities of the underlying MLLM (using just GRPO vs GRPO + VPO) seems to decrease, which is not great."}, "questions": {"value": "Please refer to the weaknesses section for questions. Additionally, \n\n1. The LLM is kept frozen throughout the process, did you try to fine-tune the LLM as well either jointly or in a separate stage? Would that help align the LLM to better answer questions based on the image descriptions? \n2. did you try any experiments where the same MLLM backbone was used as the reasoner as well? I wonder if training the same backbone for both captioning and reasoning + answering would allow for some transferability in learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QuJ5DXG8qg", "forum": "hlLXvyz5iP", "replyto": "hlLXvyz5iP", "signatures": ["ICLR.cc/2026/Conference/Submission3000/Reviewer_izU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3000/Reviewer_izU6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760764927257, "cdate": 1760764927257, "tmdate": 1762916489818, "mdate": 1762916489818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAPID (Reasoning-Aligned PerceptIon Decoupling), a two-stage pipeline for multi-modal reasoning that (1) repurposes an MLLM into a perception module that emits rich, query-aware text (caption + tentative solution), and (2) passes that text to a separate, stronger text-only LLM for the actual reasoning. Empirically, RAPID yields large gains on seven vision-math/logic benchmarks (e.g., MathVista, MathVision, MathVerse, MMMU, WeMath, DynaMath, LogicVista), enables inference-time scaling by swapping in stronger LLM reasoners without re-training the MLLM, and preserves general abilities on non-reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline diagrams and prompt templates (referenced figures/appendices) make it easy to follow the two-stage flow and what exactly is optimized.\n2. Thorough ablations: perception variants (none / cap / qcap / sol / cap+sol / qcap+sol), with and without VPO/GRPO, with/without penalties, and different LLMs for training vs. inference.\n3. VPO is a neat twist on GRPO for caption supervision by outcome: the reward comes from a downstream verifier (the reasoner’s answer correctness), not from caption n-grams or human preferences."}, "weaknesses": {"value": "1. Comparisons to verification-augmented or tool-enabled LMMs (e.g., visual verification modules, external OCR/detection tools) are missing. This matters because a strong verifier might reduce drift without multi-turn agent interaction.\n2. The method introduces two RL phases (GRPO then VPO) plus group rollouts and external LLM calls for rewards. While appendices mention batch sizes/steps, the wall-clock/cost, GPU hours, and reasoner-call counts per step are not surfaced prominently in the main text. This matters for practical adoption.\n3. The hasCap(·) check is prompt-based. This heuristic may be gamed (e.g., hiding solutions within “caption-like” text) or fail across domains/styles. A small quantitative audit of false positives/negatives (and downstream impact) would be valuable."}, "questions": {"value": "1. Does RAPID extend to multi-image and video inputs without architectural changes?\n2. You find R1-7B best for training rewards and Qwen3-8B strong for inference. Could you elaborate on the caption-length & difficulty balance hypothesis (Fig. 9) with controlled experiments (e.g., length-controlled rewards)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "36MfbZWf76", "forum": "hlLXvyz5iP", "replyto": "hlLXvyz5iP", "signatures": ["ICLR.cc/2026/Conference/Submission3000/Reviewer_wDxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3000/Reviewer_wDxG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654935048, "cdate": 1761654935048, "tmdate": 1762916489604, "mdate": 1762916489604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VPO, which rewards the MLLM based on the correctness of answers generated by the external reasoner. They use a caption penalty to make sure the MLLM focus on the visual understanding, while the LLM mainly serve as a reasoner. Overall, this framework seems to be simple yet effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes training a visual captioner using the accuracy of LLM responses as a reward signal. Although combining VLM-based captioning with LLM reasoning has become common, to the best of my knowledge, introducing RL for training a visual captioner is novel. Therefore, the work demonstrates strong originality.\n\n2. The experiments are very solid, particularly as the conclusions are validated across models of multiple scales, with comprehensive ablation studies provided.\n\n3. The paper is easy to understand, and the formulations related to RL training are presented with rigor.\n\n4. The literature review is complete, and the code has been open-sourced."}, "weaknesses": {"value": "The main limitation of the paper is that it does not provide a sufficiently detailed justification for the necessity of adopting a VLM captioner + LLM reasoner framework. In fact, many existing open-source and closed-source MLLMs still rely on unified reasoning, and the unified model appears to be an important trend in the development of large models. \n\nThe authors could elaborate on their perspective regarding the future direction of large models: whether they believe the field will move toward unified architectures or expert-agent-based systems. If the explanation is convincing, I would be willing to raise my score to 10, as this is already a very solid piece of work."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vUKPYcZid0", "forum": "hlLXvyz5iP", "replyto": "hlLXvyz5iP", "signatures": ["ICLR.cc/2026/Conference/Submission3000/Reviewer_DHha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3000/Reviewer_DHha"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705110381, "cdate": 1761705110381, "tmdate": 1762916489375, "mdate": 1762916489375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAPID, a method to enhance the reasoning of Multi-modal Large Language Models (MLLMs) whose internal language models are often outdated and expensive to upgrade. The approach decouples the system, using the MLLM for perception to generate detailed textual descriptions from images, which are then fed to a separate, powerful text-only LLM for reasoning. A novel reinforcement learning algorithm, Visual Perception Optimization (VPO), is used to train the MLLM to produce captions that are optimized for the final reasoning task. This \"plug-and-play\" design allows for easy upgrading of the reasoning LLM, enabling performance improvements without costly retraining of the vision components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well-written and logically structured, making the core ideas easy to understand. The figures are clear and effectively illustrate the main concepts.\n\n2. The motivation is straightforward."}, "weaknesses": {"value": "1. This paper lacks novelty. The paper presents a relatively straightforward idea. It uses a multimodal large language model (MLLM) to describe an input image and then relies on a separate language model for textual reasoning. This setup mainly combines existing components rather than introducing a new methodological contribution. The multimodal stage performs description rather than genuine reasoning, which limits the conceptual depth of the approach.\n\n2. The proposed pipeline does not offer clear benefits in performance or efficiency. For example, when using Qwen2.5-VL-7B as the captioner and Qwen3-8B as the reasoner, the results reported in Table 1 remain below those of the single multimodal model Qwen3-8B-Instruct without reasoning. The method also requires roughly twice as many model parameters, which increases computational cost without providing noticeable performance gains.\n\n3. The decoupled architecture is vulnerable to error accumulation. If the perception stage produces an inaccurate or incomplete description, the reasoning model has no way to recover from that mistake. This makes the overall system unstable and limits its reliability compared with end-to-end multimodal reasoning approaches."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JIdJznn2W8", "forum": "hlLXvyz5iP", "replyto": "hlLXvyz5iP", "signatures": ["ICLR.cc/2026/Conference/Submission3000/Reviewer_cXzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3000/Reviewer_cXzG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921663869, "cdate": 1761921663869, "tmdate": 1762916489192, "mdate": 1762916489192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}