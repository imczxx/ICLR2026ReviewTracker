{"id": "6jIc8m38gj", "number": 22669, "cdate": 1758334264818, "mdate": 1759896853269, "content": {"title": "Causal Interpretation of Neural Network Computations with Contribution Decomposition (CODEC)", "abstract": "Understanding how neural networks transform inputs into outputs is crucial for interpreting and manipulating their behavior. Most existing approaches analyze internal representations by identifying hidden-layer activation patterns correlated with human-interpretable concepts. Here we take a direct approach to examine how hidden neurons act to drive network outputs. We introduce CODEC ($\\textbf{Co}$ntribution $\\textbf{Dec}$omposition), a method that uses sparse autoencoders to decompose network behavior into sparse motifs of hidden-neuron contributions, revealing causal processes that cannot be determined by analyzing activations alone. Applying CODEC to benchmark image-classification networks, we find that contributions grow in sparsity and dimensionality across layers and, unexpectedly, that they progressively decorrelate positive and negative effects on outputs. We further show that decomposing contributions into sparse modes enables greater control and interpretation of intermediate layers, supporting both causal manipulations of network output and human-interpretable visualizations of distinct image components that combine to drive that output. Finally, by analyzing state-of-the-art models of retinal activity, we demonstrate that CODEC uncovers combinatorial actions of model interneurons and identifies the sources of dynamic receptive fields. Overall, CODEC provides a rich and interpretable framework for understanding how nonlinear computations evolve across hierarchical layers, establishing contribution modes as an informative unit of analysis for mechanistic insights into artificial neural networks.", "tldr": "Using a novel famework borrowed from a century of neuroscience experiments, we gain insight into, and causal control over, intermediate layers of benchmark image-classification networks.", "keywords": ["mechanistic interpretibility", "neuroscience", "xai", "ai safety", "tool-development"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aae51d100fac22bd620ebef607c8c807dc0634fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CODEC, an interpretability framework for studying causal contributions of hidden units in artificial neural networks.  In this paper, this is performed by estimating the contribution of hidden units to an output (with ActGrad or Integrated Gradients, for instance), decomposing these contributions into interpretable “modes” with spare autoencoders (SAEs), and then studying the role of different network units (or channels) associated with the learned modes via visualization and/or ablation experiments.  The authors apply this framework to reveal interpretable patterns in CNN image classification models and models of human retinal activity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- To my knowledge, this is the first paper to apply SAEs to contribution matrices instead of model activations.  The authors argue, and methodologically demonstrate, that this can help reveal causal contributions of hidden units that are not revealed through similar analyses on model activations.\n- The proposed method is flexible: architecture agnostic, can be adapted to different contribution targets, does not require access to the model’s original training data  (although, see weakness 1). This is a valuable feature of mechanistic interpretability methods.\n- Applying CODEC beyond traditional image classification models, the authors reveal interesting insights into casual operations in models of retinal activity.  This type of interpretability for neuroscience models could be quite valuable for NeuroAI researchers seeking to better understand the brain with models that have representationally aligned neural activity."}, "weaknesses": {"value": "- Authors suggest the generality of CODEC to other architectures and tasks but do not provide evidence of this working.  Although logically, it should, it would be beneficial if the authors could substantiate this claim with small experiments on other tasks and/or modalities.\n- Human interpretability of modes are not guaranteed and their interpretations are often subjective.  Although in CODEC analyses the authors identify modes that are correlated with class outputs, it is not always the case that SAE modes are interpretable.  The interpretability of the causal contribution of a unit is hinged upon SAE modes being interpretable.\n- There is a lack of quantitative evaluation of the coverage (how many classes are represented by distinct contribution modes) or specificity (how uniquely those modes map to a given class).  While the “black widow” and “dog” (appendix Figure 11) examples are illustrative, a broader analysis across all ImageNet categories (or even semantic combination of categories (e.g., “contains animal”),  would clarify whether CODEC consistently identifies compact causal modes, or if such patterns are limited to a subset of classes."}, "questions": {"value": "- How would you position this work alongside other mechanistic interpretability methods based on causal analysis (e.g., Activation Patching [1])?\n- The paper highlights the advantages of performing causal analysis on unit contributions instead of activations.  Are there any scenarios where you would expect analysis on the activation space to be more insightful?  When is it a better choice to use CODEC over an alternative, activation-based approach?\n- What is the coverage and specificity of CODEC to a broader set of ImageNet classes (see weakness 3)?\n\n\n[1] Kevin Meng et al. *Locating and Editing Factual Associations in GPT*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "axwNLeegPy", "forum": "6jIc8m38gj", "replyto": "6jIc8m38gj", "signatures": ["ICLR.cc/2026/Conference/Submission22669/Reviewer_Jrdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22669/Reviewer_Jrdp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761255485886, "cdate": 1761255485886, "tmdate": 1762942328638, "mdate": 1762942328638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CODEC (Contribution Decomposition), a method for interpreting neural networks by analyzing how hidden neurons contribute to network outputs rather than just their activation patterns. The approach uses gradient-based attribution methods (Integrated Gradients, ActGrad) to compute neuron contributions, then applies sparse autoencoders to decompose these contributions into interpretable modes. The authors apply CODEC to ResNet-50 on ImageNet and to models of retinal neural circuits, demonstrating that contribution-based analysis enables better network control and reveals computational structure invisible to activation-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths \n\n* The paper presents a novel and conceptually appealing perspective by focusing on how hidden neurons contribute to network outputs rather than merely analyzing their activation patterns. This shift is well-motivated from neuroscience principles, where understanding neural function requires examining both receptive fields (input sensitivity) and projective fields (output effects). The theoretical framework is rigorous, with complete input space decompositions properly derived for ActGrad, InputGrad, and Integrated Gradients, providing a mathematically sound foundation for the contribution analysis. \n\n* One particularly compelling aspect of this work is its dual application to both artificial and biological networks. I find the application to retinal neurons especially relevant and valuable, as it represents a novel translation of topics typically associated with mechanistic interpretability of artificial neural networks to models of biological systems. This demonstrates the breadth and potential impact of the CODEC framework beyond standard machine learning interpretability tasks, bridging computational neuroscience and AI interpretability in a meaningful way.\n\n* The paper provides causal validation through ablation and preservation experiments, which convincingly demonstrate that the discovered modes capture functionally relevant channels. These experiments show that targeting specific channels identified by CODEC can selectively eliminate or preserve classification performance for target classes while leaving other classes largely unaffected, providing evidence beyond mere correlation that the method identifies causally important computational pathways.\n\n* The findings themselves are also scientifically interesting. The progressive decorrelation of excitatory and inhibitory effects across network layers is a novel observation that provides insight into how neural networks organize computations hierarchically. Similarly, the observation that contributions become increasingly sparse and high-dimensional through the network offers new perspectives on how feature processing evolves with depth, findings that would not be apparent from activation analysis alone."}, "weaknesses": {"value": "## Weaknesses \n\n* While the paper acknowledges sensitivity to hyperparameters including hidden layer size and regularization, this issue is not systematically studied or addressed. The authors mention that these parameters \"may require tuning for different architectures\" but provide no guidance on how to perform this tuning, no ablation studies exploring the sensitivity, and no principled approach for selecting appropriate values. This is particularly problematic for the number of modes (k), which appears to be a critical hyperparameter that determines the granularity of the decomposition. Without systematic investigation or clear selection criteria, practitioners would struggle to apply CODEC to new problems.\n\n* The paper lacks any analysis of computational costs, which is a significant omission given that the method involves multiple backward passes for Integrated Gradients integration steps, and additional computation for the sparse autoencoder training and decomposition. Readers cannot assess the practical feasibility of applying CODEC to larger models or compare its computational requirements to alternative interpretability methods. This is especially important given that the authors suggest the method could scale to large language models, yet provide no evidence or analysis of whether this is computationally tractable.\n\n* The choice of contribution targets—specifically the sum of top-k logits and entropy—lacks thorough justification. While these are reasonable choices, the paper does not explain why these particular targets are optimal, whether other targets might be more informative for different analysis goals, or how sensitive the results are to this choice. The single contribution target formulation is motivated primarily by avoiding intractable 3D decompositions, which is a practical consideration but may limit the method's ability to capture class-specific computational pathways.\n\n* Regarding methodological details, the sparse autoencoder training procedure is underspecified in ways that would make reproduction difficult. Critical details such as the optimizer used, learning rate schedules, convergence criteria, weight initialization schemes, and the specific form of sparsity regularization are either missing or relegated to brief mentions. The paper states that results will be made available as supplementary materials, but the main text should provide sufficient detail for readers to understand and evaluate the approach. The lack of these details makes it difficult to assess whether the discovered modes are stable features of the networks or artifacts of particular training procedures.\n\n\n### Minor Details \n\n* From a presentation standpoint, Figure 8 is particularly difficult to read. The figure attempts to convey complex information about retinal neural network analysis, but most of the labels are too small to be legible, and the overall layout is cramped. I would suggest either significantly enlarging this figure, possibly splitting it across multiple subfigures, or substantially reducing the caption length to allow more space for the visual content.\n\n* Fig 1 is wrongly referenced multiple times: Line 237; Line 262, Line 269\n\n* Missing Fig reference in line 305"}, "questions": {"value": "## Questions\n\n* A critical question for practical application of CODEC concerns hyperparameter selection. Can you provide systematic guidance for choosing the number of modes (k), the appropriate sparsity penalties, and other key hyperparameters? Currently, the paper acknowledges that these choices matter but offers no principled approach for setting them. It would be valuable to understand whether there are data-driven methods for selecting k (perhaps based on reconstruction error curves, or information-theoretic criteria), how sensitive the discovered modes are to different sparsity penalty values, and whether certain hyperparameter configurations consistently work well across different network architectures or layers. Without such guidance, practitioners would need to perform extensive manual tuning, which limits the method's accessibility and reproducibility.\n\n* Another important consideration is the stability of the discovered modes. Are the modes stable across different random seeds in the autoencoder training, across different training runs of the original neural network, or across slight architectural variations? This is crucial for understanding whether CODEC identifies fundamental computational structures in the network or whether it is sensitive to arbitrary training details. For instance, if you train ResNet-50 multiple times with different random initializations, do you recover similar modes? If you slightly modify the architecture (for example, changing the number of channels in a layer or using a different residual connection pattern), do the modes remain interpretable and consistent? Understanding this stability would help establish whether modes represent robust computational motifs or are more ephemeral features of particular network instances.\n\n* Finally, to my understanding, the current approach assumes that contributions can be decomposed as linear combinations of modes, which may be a limiting assumption. What about interactions between modes? In practice, neural network computations often involve complex, nonlinear interactions between features, where the effect of one feature depends on the presence or absence of others. The linear decomposition might miss these interaction effects entirely. Have you considered approaches that could capture mode interactions, such as examining joint activation patterns of multiple modes, or using more sophisticated decomposition methods that allow for multiplicative or other nonlinear interactions? Understanding the limitations of the linear assumption and whether important computational structure is being missed would strengthen the interpretation of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jl2Sb69G31", "forum": "6jIc8m38gj", "replyto": "6jIc8m38gj", "signatures": ["ICLR.cc/2026/Conference/Submission22669/Reviewer_MSQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22669/Reviewer_MSQk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570954359, "cdate": 1761570954359, "tmdate": 1762942328407, "mdate": 1762942328407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a two-stage pipeline that first computes per-channel contribution scores to a scalar target using gradient-based attribution, then trains a sparse autoencoder over these contribution vectors to discover sparse \"modes\". The authors claim these modes are sparser than activations, align with classes, enable selective output control via targeted channel masking, and support channel-level visualizations. Experiments are on ResNet-50 and a small retinal CNN."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Originality: Clear formulation that swaps activations for contributions before SAE, producing sparse modes that are easy to manipulate\n* Quality: Targeted masking yields selective effects for classes inside ResNet-50 and the retina case study shows external face validity\n* Clarity: The method is explained step by step, and the visualization procedure is easy to follow\n* Significance: The pipeline could be a practical analysis and editing tool for CNNs and may assist mechanistic probing in constrained settings"}, "weaknesses": {"value": "* Modern literature already demonstrates SAE-driven feature discovery and steering in ViTs and vision-language models. The submission does not compare against these systems or articulate why contribution-SAE is preferable.\n* All main results are in CNNs. There is no validation on ViTs, attention heads, MLP neurons, or token features, which is where much of the community focuses today\n* The experiments show interventional control under channel masking in one backbone but do not establish model-invariant causal mechanisms - so the causal language is not entirely warranted\n* Emphasis on top-k logits or entropy may conflate classes. Per-logit or contrastive targets would better test class specificity\n* No systematic ablation is provided over dictionary width , sparsity penalty, or thresholds. Stability of mode discovery is also unclear\n* The \"century of neuroscience\" narrative should be replaced by a concise, current context with concrete recent citations"}, "questions": {"value": "* How does the method extend to ViTs and transformer units? Please report a compact ViT-B experiment at a few layers and compare contribution-SAE against activation-SAE or token features for steering and selectivity.\n* Can you evaluate per-logit and contrastive targets and report how mode sparsity, stability, and class selectivity change?\n* Please add SAE ablations over dictionary size, L1 strength, thresholding, and signed modeling, with stability metrics for mode-class correlation and intervention effect sizes\n* If you select modes across several layers, can you achieve stronger or more selective control than single-layer masking?\n* For the retina study, what specific new biological hypothesis or prediction follows from your modes that prior retinal CNN work did not already suggest?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iCZXDS48CL", "forum": "6jIc8m38gj", "replyto": "6jIc8m38gj", "signatures": ["ICLR.cc/2026/Conference/Submission22669/Reviewer_aiv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22669/Reviewer_aiv4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869120762, "cdate": 1761869120762, "tmdate": 1762942328224, "mdate": 1762942328224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}