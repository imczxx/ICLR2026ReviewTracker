{"id": "oM69q4817x", "number": 15753, "cdate": 1758254861797, "mdate": 1762927910954, "content": {"title": "Protein-SE(3): Unified Framework and Comprehensive Benchmark for SE(3)-based Protein Structure Design", "abstract": "SE(3)-based generative models have shown great promise in protein geometry modeling and effective structure design. However, the field currently lacks a pipeline to support consistent re-training and fair comparison across different methods. In this paper, we propose Protein-SE(3), a unified framework accompanied by the comprehensive benchmark for SE(3)-based protein design. Protein-SE(3) integrates recent advanced methods, supports diverse evaluation metrics and also develops a mathematical decoupling toolkit. Specifically, state-of-the-art generative models for typical protein design tasks (unconditional generation and motif scaffolding), from multiple perspectives like DDPM (Genie1 and Genie2), Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and FrameFlow) are systematically incorporated into our framework. All methods are re-trained on identical datasets and evaluated with consistent metrics, ensuring fair and reproducible comparison. Furthermore, our proposed decoupling toolkit abstracts the mathematical foundations of generative models, facilitating rapid prototyping of future algorithms without reliance on explicit protein structures. Taken together, Protein-SE(3) establishes a standardized foundation for the advancing research field of SE(3)-based protein design.", "tldr": "", "keywords": ["Protein Structure Design", "Unified Training Framework", "Mathematical Decoupling", "Comprehensive Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/53a96e33e382ade9be4c61d818ea2960fd9602ea.pdf", "supplementary_material": "/attachment/0d2214cf15e5d4281660fd99a079b35fad602268.zip"}, "replies": [{"content": {"summary": {"value": "The manuscript presents Protein-SE(3), a unified framework complemented by a comprehensive benchmark for evaluating SE(3)-equivariant protein design models. The framework integrates state-of-the-art methodologies, standardizes diverse evaluation metrics, and introduces a novel mathematical decoupling toolkit. A key feature is the re-training of all integrated methods on identical datasets and their evaluation using consistent metrics, which significantly promotes fairness and reproducibility in this rapidly evolving field. The mathematical toolkit aims to establish a foundational understanding of the generative processes, facilitating algorithm prototyping without an explicit reliance on protein structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Significant Contribution to Standardization and Reproducibility**: The development of a unified framework and comprehensive benchmark for SE(3)-based protein design is a substantial service to the community. By standardizing training data, implementation details, and evaluation metrics, the authors have created an important resource that enables rigorous, fair, and transparent comparisons among competing methodologies.\n\n2. **Mathematical Decoupling Toolkit**: The introduction of a mathematical decoupling toolkit is a technical contribution. This toolkit provides a clear, principled foundation that articulates the underlying mathematical differences between generative models."}, "weaknesses": {"value": "1. **Need for Enhanced Data Visualization**: The core comparison results presented in Table 1 are difficult to interpret solely in tabular form. Supplementing this data with appropriate visual figures is necessary to effectively communicate the relative performance, elucidate trends across different protein lengths, and enhance the overall clarity and impact of the empirical findings."}, "questions": {"value": "1. Could the authors include a figure representation of the results in Table 1? This visualization should effectively illustrate the performance differences (or similarities) among the various methods, particularly emphasizing how performance metrics trend or vary as a function of protein length or sequence complexity.\n\n2. In Appendix B, the paper draws a conclusion that \"SE(3)-based approaches merit greater attention and exploration.\" This statement is based on comparisons that include models like Proteina, DPLM-2, and MultiFlow, which, as noted, were trained with different architectures, proprietary training data, and potentially distinct task formulations. Could the authors rephrase the conclusion to be more appropriately guarded?\n\n3. There are some typos in the caption of Table 1, please check and correct them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IkFcXtsQED", "forum": "oM69q4817x", "replyto": "oM69q4817x", "signatures": ["ICLR.cc/2026/Conference/Submission15753/Reviewer_krsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15753/Reviewer_krsJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760949431689, "cdate": 1760949431689, "tmdate": 1762925987196, "mdate": 1762925987196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "X70YfH2Tz4", "forum": "oM69q4817x", "replyto": "oM69q4817x", "signatures": ["ICLR.cc/2026/Conference/Submission15753/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15753/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927909567, "cdate": 1762927909567, "tmdate": 1762927909567, "mdate": 1762927909567, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Protein-SE(3), a unified benchmark for SE(3)-based protein design models like Genie 1-2, FrameDiff, FoldFlow, and FrameFlow. All baselines are trained and evaluated on same dataset to ensure fair and consistent comparison."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed framework facilitates re-training and evaluation of various SE(3)-based baseline models by other researchers."}, "weaknesses": {"value": "## Limited contribution\nThe proposed benchmark is incomplete, as it only includes SE(3)-based models. Given the diversity of protein design models, models beyond SE(3)-based category should also be included, not left out for future work. \n\nMoreover, since the primary contribution of this work is benchmarking, the experimental evaluation should have been more expensive -- e.g., reporting results across multiple data splits and/or random seeds. These details are not mentioned in the paper. \n\nFinally, the usefulness of the proposed mathematical decoupling toolkit is questionable, as it trains MLP-based flow/diffusion models on simple toy synthetic data, without demonstrating any clear practical application. \n\n## Overstated claims\n\nSection 8 seems to make overstated claims. The authors assert that their decoupling toolkit demonstrates superior efficiency of flow matching in $\\mathbb{R}^3$ and $SO(3)$ spaces; however, this conclusion is based solely on toy experiments under limited conditions.\n\n## Unclear motivation \n\nThe introduction lacks clarity regarding the motivation and novelty of the benchmark. While the authors state that existing benchmarks “overlook consistent re-training and fair comparison,” no concrete examples or analyses are provided to illustrate how or why current benchmarks fail in these aspects.\n\n### Typos\n- Line 84 incomplete sentence\n- Line 137 \"rotation angel\"\n- Line 377 \"To abstract and abstract\""}, "questions": {"value": "Could the authors clarify in what ways previous benchmarks have “overlooked consistent re-training and fair comparison,” and explain how this gap motivated the development of Protein-SE(3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vbIGHB8184", "forum": "oM69q4817x", "replyto": "oM69q4817x", "signatures": ["ICLR.cc/2026/Conference/Submission15753/Reviewer_5vXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15753/Reviewer_5vXG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463718020, "cdate": 1761463718020, "tmdate": 1762925986812, "mdate": 1762925986812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a codebase that unifies 6 prior frame based protein design generative models. The authors re-train all 6 models on the same dataset and evaluate them on both unconditional generation and motif scaffolding using standard quality, novelty and diversity metrics enabling a fair comparison between the underlying generative methodologies in each case. Toy datasets on translations and rotations are also provided to gain insights into generative processes on those modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "I think this codebase will be a valuable resource for further research. Due to its fixed training set and evaluation code, it will make it very easy for researchers to add in their own method and have it be immediately comparable with all previous frame-based generative models. The inclusion of the toy datasets for debugging is also nice as this is often a part of model development that researchers usually have to implement themselves.\n\nThe metrics used to evaluate seem comprehensive to me, covering sample quality, diversity and novelty in addition to measuring secondary structure content which is a metric that often differentiates methods."}, "weaknesses": {"value": "The motif scaffolding benchmark setup seems quite non-standard with the authors measuring scTM and MotifRMSD values. Previous motif scaffolding bechmarks have used the idea of 'successful designs' that meet certain motif RMSD and scRMSD thresholds and then counting the number of unique and successful designs. I think the authors could benefit from simply integrating a recent motif scaffolding benchmark such as MotifBench.\n\nThe plots for secondary structure are nice however it would be ideal to have a quantitative metric with which to compare models rather than having to eyeball different plots. Perhaps the distributional difference to the PDB could be computed.\n\nI don't know why the authors restricted themselves to only frame based models. The codebase could become a training harness for all types of protein generative models which share the same training dataset and evaluation protocol. This seems like a missed opportunity.\n\nSadly I think that ICLR may not be the correct venue for this work since in terms of originality and significance there is not much new in this work and there are not many new insights. I do however believe the codebase itself is very valuable to the community."}, "questions": {"value": "I wonder if the toy datasets on rotations could be very difficult to learn. It seems to me that a 3D object has been placed in euler angle space in order to create the dataset which seems like quite an unnatural way to generate rotations to learn over and could end up with points near singularities in rotation space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9geUTh2UCM", "forum": "oM69q4817x", "replyto": "oM69q4817x", "signatures": ["ICLR.cc/2026/Conference/Submission15753/Reviewer_PZ7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15753/Reviewer_PZ7B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877165643, "cdate": 1761877165643, "tmdate": 1762925986335, "mdate": 1762925986335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework to benchmark protein generative models, including a training set that can be used to retrain the models for fair comparison"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**Originality**: It is probably not a bad idea to do a fair comparison of these algorithms, with the same training data. \n\n**Clarity**: The figures are well presented"}, "weaknesses": {"value": "- There are multiple things about this paper I do not understand. First of all, I do not understand the focus on SE(3) methods. It seems to me like the central claim of the paper is to build a pipeline for evaluating generative models for proteins, including training them all on the same data. But why the focus on SE(3). I see no reason why a method that is not based on SE(3) could not be put through the same pipeline. \n- While I think there is value in retraining these models on the same data, to evaluate them, I also do not think it is entirely fair. After all, anyone who has ever developed a model, knows that model choices (architecture, hyperparameters, etc.) are made based on the data at hand. So, retraining these models on a (very small) dataset, that most of them were not designed for, does not necessarily measure their performance. Furthermore, my feeling is that ultimately, what matters is how good the model is at designing proteins, even if that performance is driven by a specific dataset. An example of this are folding models. AlphaFold3 beats any other public reimplementation, because it was trained on more data, even if the implementation are largely the same.\n- The training dataset of ~19k proteins is exceedingly small. Some of these models were trained on datasets that were 10x larger. Therefore, performance, particularly in terms of generalization, is never going to be good. \n- The models benchmarked are very out of date: There is a FoldFlow2, but the paper seems to use the much older FoldFlow. Likewise with RFDiffusion2. There is also a family of models that do all-atom design (La Proteina, ProtParpadelle, Latent-X) that are not included. \n- The quality benchmarks do inverse folding generating 8 sequences with ProteinMPNN, then refolding with ESM-Fold. It seems to me like the number of sequences, and inverse and forward models, should present the user multiple options, and a good benchmark would also ablate over those.\n- The authors present DDPM and score-matching as distinct modeling paradigms. However, these are mathematically equivalent in the continuous-time limit, as established by Song et al. (2020). DDPMs can be viewed as a discrete implementation of score-based diffusion, differing mainly in the parameterization of the reverse process (stochastic vs. deterministic formulations)\n- There are some very important citations missing, such as Lipman's flow matching paper, which is mind boggling in a paper that talks a lot about flow matching. \n- I found section 7 extremely confusing, and out of place. Is it just trying to academically study the differences between DDPM, score-matching and flow matching, unrelated to proteins? If so, there are many available resources that do a very good job at this."}, "questions": {"value": "Aside from the weaknesses highlighted above, there are multiple typos in the text. For example: \n\n- Line 50: \"Backend by Pytorch Lightning\" \n- Line 86: \"Based on the and Wasserstein\"\n- Line 136: \"Rotation angel\"\n- Table 1 caption: \"scTM ¿ 0.5\"\n- Line 271: \"flow-matching based methods (FrameFlow and Foldflow) demonstrates\"\n- Line 272: \"Novelty is also an essential metric to evaluates\"\n- Line 276\" \"shows a decline trend\"\n- Line 377: \"To abstract and abstract\"\n\nTerms like Wasserstein, FoldFlow and RFDiffusion also do not consistently use uppercase"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yOcvfnv9UO", "forum": "oM69q4817x", "replyto": "oM69q4817x", "signatures": ["ICLR.cc/2026/Conference/Submission15753/Reviewer_YRLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15753/Reviewer_YRLi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908946589, "cdate": 1761908946589, "tmdate": 1762925985869, "mdate": 1762925985869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}