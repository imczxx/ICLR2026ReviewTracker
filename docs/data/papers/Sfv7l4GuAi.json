{"id": "Sfv7l4GuAi", "number": 13846, "cdate": 1758223593068, "mdate": 1759897408875, "content": {"title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling", "abstract": "While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation.", "tldr": "This work shows a 2B autoregressive model with beam search generates better compositional images than a 12B diffusion model, proving architecture trumps scale for efficient inference-time search.", "keywords": ["Autoregressive Models", "Inference-Time Scaling", "Beam Search", "Image Generation", "Compositional Generation", "Verifiers", "Computational Efficiency"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1de78e2e59cb111a02b563702faaedd153b50b17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores using more capable search algorithms, primarily beam search (and also GTO, Greedy Token Optimization), on autoregressive image generative models, focusing on Infinity. It analyzes several verifier choices (e.g., ImageReward, LLaVA) and shows that beam search outperforms naive random search. It also compares to the scaling of a continuous diffusion model (FLUX.1-dev)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Using stronger search (beam search/GTO) for autoregressive image generators is a sensible direction.\n\n2. The idea feels general and likely applicable across architectures. This reads like an initial step toward a useful approach.\n\n3. There are helpful ablations on diversity (generation temperature) and dynamic budget allocation, presented in the paper."}, "weaknesses": {"value": "**Major:** \n\n1. The paper makes general claims about discrete-token AR, but all experiments are on Infinity. Other AR families (e.g., Janus) are not tested, limiting the scope and generality of the claims.\n\n2. L.245 argues that random search has an undesirable logarithmic growth and motivates beam-like algorithms. Then it should be shown that beam actually scales differently, not just that it wins at some budgets.\n\n3. To me, comparing Infinity (AR) to FLUX (continuous diffusion) is hard to interpret. NFE isn't a fair cross-architecture proxy, and the comparison mixes different models and different search algorithms. If we assume matched compute, how do the two paradigms scale under random search only? Right now it's hard to tell whether gains come from the model or the search.\n\n**Minor:**\n\n1. In Fig.1, FLUX is shown at very few points (only two). The curve could saturate below Infinity or keep rising. more NFE points are needed to understand FLUX's trend, whether it is scaling better or worse than Infinity.\n\n2. Methods like beam search can get stuck in undesirable subtrees. A short limitations discussion for the proposed methods in the context of AR image generative models would help.\n\n\n3. GenEval and T2I-CompBench++ use compositional but simple prompts. A benchmark with longer, more detailed prompts (e.g., DPG-Bench) would provide valuable insights."}, "questions": {"value": "1.  Is \"LLaVA + Random\" in Tab. 4 the same as \"LLaVA\" in Tab. 3? Some category scores differ, making the comparison to ImageReward a bit difficult. If they are the same, are results averaged over multiple runs or single-shot?\n\n2. In Tab. 4, beam search uses fewer NFEs than random, thus the scores are close. If you match NFEs, how big is the gap between the search algorithms on any of the benchmarks (e.g. T2I-CompBench++)?\n\n3. Did the higher-temperature images show visible characteristics (lower quality or higher diversity)? Not required, but a small visualization of beam paths would help a reader. Also, since ImageReward underweights spatial correctness, why not using LLaVA here to test whether increased diversity helps spatial tasks (regarding the claim in L.417)?\n\nPlease also address the points raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qajlfd42wo", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Reviewer_mwtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Reviewer_mwtS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812050330, "cdate": 1761812050330, "tmdate": 1762924371201, "mdate": 1762924371201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the problem of Test-Time Scaling (TTS) for image generation, comparing Autoregressive (AR) models and Diffusion models. The authors compare three TTS strategies—random search, greedy token optimization, and beam search—along with different verifiers, evaluating their performance on TTS. The conclusion is that employing beam search with a composite verifier can enable AR-based TTS to surpass large-scale diffusion models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper addresses an interesting topic: the comparison between AR and Diffusion models for TTS, and the potential advantages of AR.\n2.  The paper is generally clearly written and provides reasonably thorough experiments on TTS for AR models."}, "weaknesses": {"value": "1.  The experiments are conducted exclusively on VAR (Infinity). However, VAR represents just one specific instance of autoregressive image generation; other representative paradigms include LlamaGen and MaskGIT. The authors need to provide experimental results across different autoregressive paradigms to robustly support their claims.\n2.  The experiments lack sufficient baselines. VAR-TTS shares a similar objective with this work, yet the authors do not provide comparative results. Furthermore, only one Diffusion model and one specific TTS strategy are included as baselines. These results are too limited to definitively claim that AR outperforms Diffusion for TTS. To substantiate the authors' viewpoint, experiments are needed to demonstrate that TTS on AR models, particularly with the proposed beam search, generally outperforms Diffusion models.\n3.  Regarding the reporting of experimental results, the authors should report the total per-image generation time, inclusive of verification, to serve as a practical reference metric, rather than reporting only the NFE.\n4.  A common challenge in TTS for image generation is that the verifier rewards obtained during the generation process can be inaccurate, while full verification only after complete generation is time-consuming. The authors need to clarify how this issue is addressed (or acknowledged if not resolved) in their framework.\n5.  Beyond experiments, the paper also lacks a detailed theoretical or principled analysis explaining why TTS on AR models would be superior to Diffusion models.\n\nIn summary, the claim that TTS on AR models surpasses Diffusion models is interesting but strong. While the problem is intriguing, the authors need to provide more comprehensive evidence to support this conclusion. If the authors' response adequately addresses these points, I would be inclined to increase my score.\n\nref:  TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation"}, "questions": {"value": "Refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2e9zP5cgHh", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Reviewer_CgR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Reviewer_CgR8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900143868, "cdate": 1761900143868, "tmdate": 1762924370875, "mdate": 1762924370875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new test-time search method, beam search, to enhance autoregressive T2I models. The paper demonstrates the effectiveness and scalability of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed test-time beam search can greatly improve the pretrained model's performance, demonstrated by comprehensive experiments.\n2. The paper introduces multiple verifiers that consider different perspectives on generation quality.\n3. The paper shows that three of the proposed verifiers exhibit logarithmic scaling.\n4. The paper is well-written."}, "weaknesses": {"value": "1. The methodological contribution is somewhat limited, but I believe the extensive experiments in this paper compensate for this limitation.\n2. All the experiments for the proposed beam search are conducted using Infinity. It would be better if the authors could provide additional results with other autoregressive models, as this would better demonstrate the generalizability of the proposed method."}, "questions": {"value": "1. To my understanding, the ablation study of w (parallel number) and c (candidate number) is reflected in the number of images and NFEs. Am I correct?\n2. In Table 3, the “Ensemble” is not the best-performing approach. Have the authors tried different ways to ensemble the verifiers, such as adjusting their weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LoT8yDaHBj", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Reviewer_XMLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Reviewer_XMLy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964966015, "cdate": 1761964966015, "tmdate": 1762924370035, "mdate": 1762924370035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper dives into inference-time scaling through search for image generation. \nThe key finding here is that while recent attempts to apply search to diffusion models have not been particularly successful, this *is* incredibly successful for autoregressive models. Here, the authors show that beam search significantly improves image generation, enabling their 2B autoregressive model to surpass a much larger 12B diffusion model on common benchmarks. \nThis paper work suggests that architecture (or architectural compatibility with search?), rather than just scale, is critical for inference-time optimization in visual generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's core claim is clear and well supported by experimental results, showing that a model's compatibility with search can decisively overcome a 6x parameter deficit against SOTA diffusion models. Analysis showcases trade-offs between various varifiers, and breaks out comparisons across different capabilities within aggregate benchmarks."}, "weaknesses": {"value": "The main efficiency metric uses \"Number of Function Evaluations\" (NFEs), but the paper says that NFEs for an autoregressive model and a diffusion model \"are not directly comparable in FLOPs\" which is a potentially noteable caveat. An NFE for one of 13 generation steps is not the same efficiency as for the noising denoising steps in a diffusion model. A more direct efficiency comparison may be needed.\n- The method relies on an external verifier model to guide the search, and this verifier can be a massive bottleneck - the paper shows the best verifier for complex reasoning (LLaVA-OneVision) is 36x slower and requires 9x more GPU memory than the 3 lightweight alternatives. This means the total inference cost (generation + verification) could be substantially higher than just running the 12B diffusion model, even if the generation NFEs are lower.\n\nThe autoregressive vision model used here, Infinity, was chosen because it is a state-of-the-art autoregressive model but also because it  'fundamentally differs from traditional autoregressive image generation' (L140). The scale-wise generation reduces the number of tokens generated and makes the model more appropriate for beam search than other AR models which are not compared here. This to me suggests some light reframing of the claims that VAR models beat etc and that it is fundamentally a property of AR models vs specific instanitations of them that permit such improvements through search, unless additional comparisons can be added."}, "questions": {"value": "How general is this to all AR models? The AR model used, Infinity, seems particularly well-suited for this approach because it generates in 13 progressive scales. It would be great to make clearer through comparison or clearer discussion what benefits could hold for any discrete AR model vs are special properties of multi-scale generation. The paper's conclusion may be over-generalizing from a specific AR architecture.\n\nThe paper notes that optimizing for one verifier can hurt other metrics, for example optimizing for aesthetics can hurt prompt adherence. The paper explores different verifiers, but are there other ways to mitigate this? For example, guiding search by other scores or internal model probabilities in addition to just the external verifier?\n\nSystematic ablations are mentioned in the abstract, but I may have missed ablations within the paper - could you please clarify?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQh6ZbKrwE", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Reviewer_VaT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Reviewer_VaT6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985525702, "cdate": 1761985525702, "tmdate": 1762924369226, "mdate": 1762924369226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes an existing autoregressive image generation model (which does next _scale_ prediction on images), and applies various inference time search techniques to it, most notably using beam search. The authors show how beam search outperforms other things like greedy decoding or rejection sampling. Then the authors compare this to existing inference time search techniques for diffusion models and show that much smaller AR model can beat a larger diffusion model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a valuable empirical comparison of different search strategies within the context of scale-wise autoregressive models. It evaluates random search, greedy decoding, and beam search, demonstrating clearly that beam search provides the best trade-off between computational cost and performance.\n\n1. The paper also provides evidence that guided search can fix specific, challenging compositional errors. The figures (e.g., Figure 1 and Appendix A) provide examples where the baseline model fails on spatial relations (\"giraffe on the right of a wallet\"), object counts (\"six keys\"), or attribute binding (\"a green rose and a blue tulip\"), while the beam-search-guided model produces the correct image."}, "weaknesses": {"value": "1. The related work section doesn't capture the full space of diffusion model inference time scaling. For instance, the authors mention \"In contrast [to diffusion models], language models benefit consistently from ... reward-model guidance\". This claim is not necessarily true, Black et. al. 2024, Fan et. al. 2024 both have shown that you can consistently benefit diffusion models with reward-model guidance.\n\n1. The paper's central claim of superiority over diffusion models  rests heavily on a comparison against the findings of Ma et al. (2025) , which reported limited benefits for search in continuous spaces. This is potentially a \"strawman\" representation of inference-time scaling for diffusion, and perhaps not the strongest baseline. Further, training data differences between the Infinity model and the Flux model tested could have been a contributing factor to the result, and decorrelating this was not done.\n\n1. Applying beam-search to an existing image generation model is not particularly non-obvious or challenging. While the empirical evaluations and results are indeed interesting, it feels like this paper is more suited at a workshop-level.\n\n1. Without a direct comparison of FLOPs or at least wall-clock time for generating a single high-quality image, the claim that the 2B AR model is more efficient than the 12B diffusion model is unsubstantiated.\n\n1. There may be some overclaiming in the title, I think the paper makes a specific claim about \"hierarchical scale-wise AR models,\" not AR models as a class."}, "questions": {"value": "1. Your central performance comparison is between the 2B Infinity model and the 12B FLUX.1-dev model. How did you account for potential differences in their respective training datasets?\n\n1. Given that beam search is a standard, well-established algorithm for autoregressive sequences, what do you consider the primary novel technical contribution of this work, beyond the (albeit interesting) empirical finding that it works well on a hierarchical image model?\n\n1. The title makes a very broad claim about \"Visual Autoregressive Models\" as a class. However, your method's tractability relies entirely on the Infinity model's specific \"next-scale prediction,\" which has only 13 sequential decision points. How would your findings apply to traditional raster-scan AR models, where beam search would be computationally infeasible? Shouldn't the paper's claims be scoped more precisely to hierarchical or scale-wise AR models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fwtQmeGfhw", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Reviewer_B9N1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Reviewer_B9N1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992664485, "cdate": 1761992664485, "tmdate": 1762924368582, "mdate": 1762924368582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary"}, "comment": {"value": "We thank the reviewers for their valuable feedback. We provide a comprehensive (XMLy) and thorough (CgR8) empirical comparison of search strategies with clear and well-supported results (VaT6) showing that architectural compatibility with search can overcome a 6× parameter deficit (VaT6). We address an interesting topic (CgR8) with a sensible direction (mwtS) about VAR versus diffusion models for test-time scaling. The paper provides valuable verifier analysis (XMLy, VaT6) showing trade-offs across compositional tasks, along with evidence that guided search fixes specific compositional errors (B9N1). The systematic ablations (mwtS) and presentation quality were recognized positively, with one reviewer scoring presentation as excellent (mwtS: 4/4) and others noting the paper is well-written (XMLy, CgR8). We will address all reviewer concerns in the revised version."}}, "id": "emKO7YIe91", "forum": "Sfv7l4GuAi", "replyto": "Sfv7l4GuAi", "signatures": ["ICLR.cc/2026/Conference/Submission13846/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13846/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission13846/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668463049, "cdate": 1763668463049, "tmdate": 1763668463049, "mdate": 1763668463049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}