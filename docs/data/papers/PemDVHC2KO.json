{"id": "PemDVHC2KO", "number": 19057, "cdate": 1758293176283, "mdate": 1759897063500, "content": {"title": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time", "abstract": "As the knowledge landscape evolves and large language models (LLMs) become increasingly widespread, there is a growing need to keep these models updated with current events. While existing benchmarks assess general factual recall, few studies explore how LLMs retain knowledge over time or across different regions. To address these gaps, we present the Timely Events Benchmark (TiEBe) - a dataset of over 23,000 question-answer pairs centered on notable global and regional events, spanning more than 10 years of events, 23 regions, and 13 languages. TiEBe leverages structured retrospective data from Wikipedia to identify notable events through time. These events are then used to construct a benchmark to evaluate LLMs' understanding of global and regional developments, grounded in factual evidence beyond Wikipedia itself. Our results reveal significant geographic disparities in factual recall, emphasizing the need for more balanced global representation in LLM training. We also observe a Pearson correlation of more than 0.7 between models' performance in TiEBe and various countries' socioeconomic indicators, such as HDI. In addition, we examine the impact of language on factual recall by posing questions in the native language of the region where each event occurred, uncovering substantial performance gaps for low-resource languages.", "tldr": "TiEBe benchmarks time-sensitive knowledge in LLMs with 23k QA over 10+ years, 23 regions, and 13 languages, revealing large geographic and language gaps and >0.7 correlation with certain socioeconomic indicators.", "keywords": ["LLM factual recall", "geographic disparities", "low-resource languages", "event-based QA"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1ca1c9095f4596f8089385ba64445c9b68697d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- Proposes TiEBe (Timely Events Benchmark), a large-scale multilingual and multi-regional benchmark to evaluate LLMs’ ability to recall real-world events over time.\n\n- Covers 23 regions, 13 languages, and ~23K QA pairs spanning 2015–2025, focusing on temporal and geographic diversity.\n\n- Provides an evaluation of various LLMs (e.g., GPT-4, Claude, Gemini, etc.), revealing strong temporal decay and regional/language disparities in factual accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Timely and challenging idea: Capturing time-varying world knowledge in a benchmark is both important and difficult; this work makes a well-designed and meaningful attempt to address that gap.\n\n- Broad multilingual and geographic coverage: The dataset spans many countries and includes both English and native-language questions, enabling diverse and fair evaluation across linguistic and regional contexts.\n\n- Clear presentation: The paper is well-structured and easy to follow, with intuitive explanations of data construction and evaluation results."}, "weaknesses": {"value": "While the main idea is valid and meaningful, the experimental depth and amount of analysis leveraging the dataset remain limited.\n\n**W1. Lack of dataset validation**\n\n- The paper does not clearly describe any human validation or systematic quality control for the automatically generated QA pairs, making it difficult to assess dataset reliability. Possible issues such as overlap between questions or hallucinated answers may exist.\n- Similarly, the translation process across multiple languages requires validation to ensure consistency and accuracy.\n- A deeper analysis of *what kinds* of time-varying information appear in the dataset and how those reflect real-world dynamics would help clarify what the benchmark actually measures.\n\n**W2. Concerns on evaluation setting**\n\n- The current evaluation does not appear to provide the *time period or contextual timestamp* of each question when prompting the model. However, many factual questions (e.g., *“Who is the president of [country]?”*) have time-dependent answers that can change year by year.\n- Without explicitly anchoring the time context, model errors may reflect ambiguity in the question rather than a genuine knowledge gap.\n\n**W3. Limited evaluation depth**\n\n- Although the benchmark focuses on *time-dependent knowledge*, the analysis does not fully exploit this aspect. Temporal variance is observed but not deeply explored (e.g., how recall changes across time windows or event types).\n- As mentioned in W2, certain facts can change over time (e.g., political positions or economic indicators). Analyzing such cases could provide valuable insight into how models handle *conflicting or updated knowledge* during training, i.e., whether new information overwrites or coexists with older facts.\n- Moreover, the results in Section 4.2 are not particularly surprising, likely because the evaluated models (mostly released in 2023–2024) have limited exposure to post-2023 factual data.\n- Instead of only reporting numerical scores, the paper could highlight *what we can learn* from these temporal gaps, for example, how LLMs adapt (or fail to adapt) to evolving world knowledge and where the main bottlenecks lie.\n\n**W4. Insufficient explanation in Section 4.3**\n\n- The discussion on performance drops in *Tok Pisin* and *Amharic* seems inconsistent with the reported results: models appear to perform worse even when queried in English.\n- A few qualitative examples or error analyses are needed to clarify and support the authors’ interpretation."}, "questions": {"value": "Revealed in the Weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0PXzgrUHnv", "forum": "PemDVHC2KO", "replyto": "PemDVHC2KO", "signatures": ["ICLR.cc/2026/Conference/Submission19057/Reviewer_dvRr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19057/Reviewer_dvRr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991429575, "cdate": 1761991429575, "tmdate": 1762931090168, "mdate": 1762931090168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper works on the challenge of evaluating how LLMs recall factual knowledge about world events across time and geographic regions. The authors introduce the Timely Events Benchmark (TiEBE), a dataset with over 23,000 question-answer pairs spanning ten years, 23 regions, and 13 languages.  Key experimental results from evaluating nine different LLMs show significant geographic disparities in factual recall. Performance is strongly correlated with socioeconomic indicators like a country's Gross Domestic Product (GDP) and Human Development Index (HDI), with Spearman correlations exceeding 0.7."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper presents a scalable method for benchmark creation by using Wikipedia retrospective pages to identify notable events, ensuring a structured and continuously updatable data source for temporal analysis.\n2.  Experimental evaluation is extensive, testing nine different open-source and commercial LLMs. This provides a broad and representative assessment of current model capabilities and their shared limitations."}, "weaknesses": {"value": "### About Method\n\n1. The paper only validates LLM-as-judge reliability but lacks human evaluation of the generated QA pairs themselves. LLM-generated questions may contain factual errors or inconsistencies. Recommend conducting human evaluation on a sample to assess quality metrics like factual consistency and answerability.\n2.  GPT-4o achieved 91% consistency versus DeepSeek-V3's 88.5%, yet the paper chose the lower-performing DeepSeek-V3 as judge without explanation (cost, API availability, etc.). Should clarify the rationale or use the better-performing model.\n3. The limitation section mentions contamination risks but lacks depth. Since the dataset uses Wikipedia content, models likely encountered it during pre-training, potentially testing memorization rather than reasoning. Need deeper analysis and mitigation strategies.\n\n### About Experiment\n\n1.  The evaluation relies heavily on DeepSeek-V3 as a judge, yet the authors' own analysis indicates that GPT-4o has higher agreement with humans. The authors should justify the choice of the slightly inferior judge, and include a sensitivity analysis using GPT-4o as the judge to verify the robustness of the core conclusions to this choice.\n2.  For a new benchmark paper, the experimental section critically overlooks the issue of data contamination. The authors should conduct a preliminary contamination analysis, for instance, by searching for TiEBE test samples in the training data of open-source models, to quantify potential train-test overlap; otherwise, it is difficult to discern whether models are performing factual recall or simple pattern matching.\n3.  The paper lacks a qualitative analysis of the models' specific failure cases. It is recommended to add a section that presents and analyzes typical incorrect answers from models across different regions, languages, or time periods, which would provide deeper insights into the models' capability boundaries than accuracy scores alone."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jZLnVmja1X", "forum": "PemDVHC2KO", "replyto": "PemDVHC2KO", "signatures": ["ICLR.cc/2026/Conference/Submission19057/Reviewer_HVLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19057/Reviewer_HVLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132554231, "cdate": 1762132554231, "tmdate": 1762931089578, "mdate": 1762931089578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TiEBe (Timely Events Benchmark), a new dataset with over 23,000 question-answer pairs designed to evaluate an LLM's factual recall of notable world events across time, geography, and language. Covering 10 years and 23 global areas, the benchmark exposes substantial geographic inequalities in model performance, with factual accuracy showing a strong positive correlation (Spearman > 0.7) with socioeconomic metrics. The results further reveal that although models achieve reasonable accuracy in many languages, their performance deteriorates significantly on low-resource languages like Tok Pisin and Amharic, highlighting persistent systemic biases in current LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is grounded in strong motivation and tackles an important and relevant problem. Building a benchmark to evaluate time-sensitive world knowledge in LLMs fills a major gap in current evaluation practices, making this contribution meaningful and well-justified."}, "weaknesses": {"value": "1. Unclear justification for using only DeepSeek-V3: The rationale behind selecting DeepSeek-V3 as the sole model for generating questions, translations, and evaluations is not well supported. Although the authors claim the model performs adequately, their own results (Table 2) show that GPT-4o aligns more closely with human judgments. Relying exclusively on one model—especially a less optimal one—seems unjustified. A more credible design would validate outputs across multiple models or through an ensemble approach.\n2. No secondary validation of generated data: The paper does not describe any additional quality control step to verify the QA pairs generated by DeepSeek-V3. Without screening for hallucinations, factual errors, or model-induced biases, the reliability and correctness of the dataset remain questionable.\n3. Limited novelty in the analysis: Although the dataset itself is useful, the analytical results offer little novelty beyond what is already obvious.\n4. Conclusions are obvious and potentially problematic: The main findings—models perform worse on low-resource languages and better for countries with stronger AI ecosystems—are expected. Moreover, drawing direct correlations with socioeconomic metrics like GDP or HDI is oversimplified. It ignores the primary factor: disparity in available training data. This kind of interpretation risks reinforcing harmful biases or socioeconomic stereotypes.\n5. Overly binary evaluation method: The authors mention that the LLM judge is often stricter than human annotators, yet they still rely on a binary Correct/Incorrect scoring scheme. This is inadequate. A minor temporal mismatch (e.g., a correct answer from a different year) is not comparable to a completely wrong answer or refusal to answer. A more fine-grained evaluation scale is needed to reflect these distinctions."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVcuP6O2zx", "forum": "PemDVHC2KO", "replyto": "PemDVHC2KO", "signatures": ["ICLR.cc/2026/Conference/Submission19057/Reviewer_CCW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19057/Reviewer_CCW7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343584611, "cdate": 1762343584611, "tmdate": 1762931088820, "mdate": 1762931088820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a dataset of automatically generated questions about notable global and regional event that are generated from wikipedia articles. The authors prompt the model to generate questions from different countries wikipedia entry snapshots taken from different time periods, spanning a decade. The authors then evaluate LMs on their constructed dataset, demonstrating trends in models being able to answer questions better for events from some countries more than others (e.g., models answer questions about US events better than those about Indonesia) and models tend to struggle with facts from most recent years."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work presents a large dataset of questions about events from different countries and times. One distinction from prior works establishing similar datasets is the inclusion of questions in different languages reflecting the country the question pertains to."}, "weaknesses": {"value": "1. Limited discussion of related work, both in the related work section and in the paper as a whole. In particular, lost of prior work has explored the impact of temporal or geographical context on wikipedia-based factoid QA [1]. Other works have looked at both individually [2, 3]. Additionally other works have developed other methods of synthetically identifying such temporally dependent QA pairs or facts from Wikipedia pages or other related resources like wikidata [4, 5]. Other works have looked into the impact of socioeconomic factors on LM behavior or performance. Given the overlap in task, methods, and findings, such works should be discussed, compared against, or evaluated on.\n\n[1] SituatedQA: Incorporating Extra-Linguistic Contexts into QA\nMichael J.Q. Zhang, and Eunsol Choi.\nEMNLP 2021\n\n[2] Visually Grounded Reasoning across Languages and Cultures\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, Desmond Elliott\nEMNLP 2021\n\n[3] Time Waits for No One! Analysis and Challenges of Temporal Misalignment\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, Noah A. Smith\nNAACL 2022\n\n[4] A Dataset for Answering Time-Sensitive Questions\nWenhu Chen, Xinyi Wang, William Yang Wang\nNEURIPS 2021\n\n[5] Time-Aware Language Models as Temporal Knowledge Bases\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, William W. Cohen\nTACL 2021\n\n[6] Whose Opinions Do Language Models Reflect?\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto\n\n\n2. The constructed QA pairs"}, "questions": {"value": "Are all the all the results on non-English languages from models that have been trained on the target languages? Or have the developers claimed that model should support the languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8FOByIhnFl", "forum": "PemDVHC2KO", "replyto": "PemDVHC2KO", "signatures": ["ICLR.cc/2026/Conference/Submission19057/Reviewer_mRZh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19057/Reviewer_mRZh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762541874358, "cdate": 1762541874358, "tmdate": 1762931088422, "mdate": 1762931088422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}