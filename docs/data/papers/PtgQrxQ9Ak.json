{"id": "PtgQrxQ9Ak", "number": 14289, "cdate": 1758232042983, "mdate": 1763557046310, "content": {"title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance", "abstract": "Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce **AReUReDi** (**A**nnealed **Re**ctified **U**pdates for **Re**fining **Di**screte Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.", "tldr": "AReUReDi refines discrete flows with annealed updates to guarantee Pareto-optimal multi-objective sequence generation, with strong performance on peptide and SMILES design.", "keywords": ["multi-objective optimization", "discrete flows", "Pareto optimality", "biomolecular sequence design", "therapeutic peptides", "generative modeling"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82903bd8fc582f49e882a6be0fc68529744f882e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers the multi-objective reward guidance for the discrete flow models. The proposed approach has three main components: (1) Rectified Discrete Flow (ReDi) from (Yoo et al., 25) which is the unconditional transition kernel; (2) the Tchebycheff scalarization that transforms a multi-objective problem into a single-objective task; (3) the balancing function technique from [Informed proposals for local MCMC in discrete spaces, (Zanella, 17)] to formulate a locally balanced proposal to have a higher acceptance rate in the MH step. Various experiments are conducted on biological tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation is clear. The components used to derive the proposed method is clearly described. \nI am not an expert on the biological tasks considered in the experiment section of this paper, but the empirical validations in Table 2 seem to suggest the proposed approach has advantages over the included baselines."}, "weaknesses": {"value": "The strategy proposed seems to be direct combination of existing methods. It is hard to say what is the insight one can get from this submission. \n\nSince there are several existing approaches for reward guidance for discrete state, e.g. DSearch [Dynamic Search for Inference-Time Alignment in Diffusion Models (Li et. al, 25)], comparing the proposed approach with these methods (the reward should be replaced by the output of the Tchebycheff scalarization) would enhance the paper.\n\nThe content in section 2.3 and the beginning of section 3.2 overlap quite a lot. No need to define the scalarized reward twice."}, "questions": {"value": "Please see the comments in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p543ogSQGe", "forum": "PtgQrxQ9Ak", "replyto": "PtgQrxQ9Ak", "signatures": ["ICLR.cc/2026/Conference/Submission14289/Reviewer_qjSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14289/Reviewer_qjSa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841865409, "cdate": 1761841865409, "tmdate": 1762924737654, "mdate": 1762924737654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a multi-objective guidance method (AReUReDi) for discrete flow models, enabling simultaneous optimization of multiple desired properties in biomolecule sequence design. AReUReDi performs reward-tilted sampling, using a pretrained rectified discrete flow model as prior, and a Tchebycheff scalarized reward function that pushes the Pareto front of multiple objectives. The guided sampling is done using a Metropolis-Hasting algorithm, with carefully designed locally balanced proposals built upon the pretrained flow model. The capability of AReUReDi is validated on the generation of wild-type peptide sequences and chemically-modified peptide SMILES, both guided by multiple therapeutic objectives. Experimental results show the effectiveness of the proposed method in steering generation toward multiple properties, compared to established multi-objective optimization baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses an important, underexplored problem: cleanly steering discrete flow models with multiple objectives, with clear potential for real-world impact.\n- The introduced AReUReDi uses Tchebycheff scalarization as a guidance potential for sampling in discrete state spaces, automatically balancing tradeoffs and pushing toward Pareto-optimal solutions.\n- Extensive experiments with promising results, demonstrating effective multi-objective control and practical applicability."}, "weaknesses": {"value": "- While the Metropolis-Hasting algorithm with a token flipping proposal guarantees an optimal stationary solution, the convergence speed could be slow and the sampling cost may become substantial as the dimensionality scales. A theoretical guarantee or an empirical justification regarding convergence or mixing properties of the sampler would provide more insight.\n- The method is evaluated only on in-silico benchmarks, leaving open whether learned policies exploit metric proxies rather than true utility. \n- While AReUReDi is framed as sampling from the reward-tilted distribution $p_1(x)\\exp(\\eta S_w(x))$, the paper does not analyze whether samples remain consistent with the prior $p_1(x)$. An ablation study on using a completely uninformed prior versus using the discrete flow model $p_1(x)$ may help illustrate the necessity of a prior as reference."}, "questions": {"value": "- How are the weight vectors for the Tchebycheff scalarization chosen in practice? Does the proposed method heavily rely on careful selection of these weights?\n- Could the authors comment more on existing steering methods [1-4] that work for discrete diffusion/flow model? Is Tchebycheff scalarization directly applicable to these guiding methods as well?\n- It is stated that \"a monotonicity constraint that accepts only token updates that increase the weighted sum of the current objective score\" is employed during Metropolis-Hasting. I view this as a trick to accelerate the slow convergence of Metropolis Hasting, but does this trick breaks down the theoretical guarantee of convergence at the same time?\n\n[1] Li et al. \"Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding\", arXiv: 2408.08252.\n\n[2] Nisonoff et al. \"Unlocking guidance for discrete state-space diffusion and flow models\", ICLR 2025.\n\n[3] Chu et al. \"Split Gibbs Discrete Diffusion Posterior Sampling\", NeurIPS 2025.\n\n[4] Venkatraman et al. \"Amortizing intractable inference in diffusion models for vision, language, and control\", NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fs1Jmye8CW", "forum": "PtgQrxQ9Ak", "replyto": "PtgQrxQ9Ak", "signatures": ["ICLR.cc/2026/Conference/Submission14289/Reviewer_KW2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14289/Reviewer_KW2p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982905882, "cdate": 1761982905882, "tmdate": 1762924737313, "mdate": 1762924737313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a new framework designed for multi-objective optimization (MOO) in discrete sequence generation, with a focus on biomolecular design. The core idea is to extend Rectified Discrete Flows (ReDi), a generative model for discrete data, with multi-objective guidance.\n\nThe method combines three key components:\n1.  Tchebycheff Scalarization: To combine multiple, conflicting objectives ($S_{\\omega} (x) = \\min_{n} \\omega_n \\tilde{s}_n(x)$) into a single scalar reward.\n2.  Annealed Guidance: A simulated annealing-like schedule for the guidance strength ($\\eta_t$) to balance exploration and exploitation.\n3.  Locally Balanced Proposals & Metropolis-Hastings (MH) Updates: A theoretically-grounded MCMC sampling procedure designed to make the ReDi model's sampling process converge to the target multi-objective distribution ($\\pi_{\\eta_t, \\omega} (x) \\propto p_1(x) \\exp(\\eta_t S_{\\omega} (x))$).\n\nThe authors provide theoretical guarantees that AReUReDi converges to the Pareto front with full coverage. Empirically, they demonstrate that AReUReDi can successfully generate peptide and SMILES sequences that optimize for up to five properties (e.g., affinity, solubility, half-life), outperforming evolutionary and diffusion-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a critical and high-impact problem: the de novo design of discrete biological sequences (like peptides) that must satisfy multiple, conflicting therapeutic properties.\n\nThe core idea of integrating on-the-fly MOO guidance directly into the sampling process of a discrete flow model (ReDi) is an elegant and well-motivated approach.\n\nThe paper presents strong experimental results, particularly in Table 2, where AReUReDi shows significant improvements over the PepTune baseline on several metrics.\n\nThe paper is well-written, logically structured, and the theoretical motivation is clearly explained."}, "weaknesses": {"value": "1. The proposed theoretical framework is a combination of several well-established, existing components: Tchebycheff scalarization (a standard MOO technique), annealed guidance (from simulated annealing), and Metropolis-Hastings sampling (a standard MCMC method). While applying this stack to Rectified Discrete Flows is new, the conceptual contribution of this specific combination is limited.\n\n2. There is a fundamental contradiction between the paper's theory and its experimental execution. The theory (Sec 3, App A) proposes an MCMC sampler (MH) that must accept worse-scoring states to escape local optima. However, all experiments (Sec 4) use a \"monotonicity constraint,\" which is a greedy search that only accepts improvements.\n\n3. This greedy search algorithm invalidates all theoretical guarantees of Pareto convergence proven in Appendix A. The paper's own ablation (Table 6) confirms this, showing the theoretical MH method (\"w/o constraints\") performs poorly, while the greedy version (\"w/ constraints\") is the one that achieves the strong results. This strongly suggests the core theoretical contribution is not practically viable as proposed."}, "questions": {"value": "1.  Could you clarify the significant discrepancy between the theoretical MH-based sampler and the greedy algorithm (using the \"monotonicity constraint\") applied in all experiments? Given this constraint, how should we interpret the paper's theoretical claims of convergence?\n\n2.  The half-life score model (App E.3) was fine-tuned on only 105 samples. Given this extremely small dataset, could you provide more evidence or justification for this model's reliability? How can we be sure the strong half-life results are not an artifact of overfitting to this low-data proxy model?\n\n3.  What was the rationale for framing the SMILES task as a refinement (starting from $p_1$ samples) rather than a full generation task (starting from $p_0$)? This is a valid task, but it is a different and less difficult problem. How does the method perform in the full generation ($p_0$ start) setting for SMILES?\n\n4.  Could you provide results showing how the generated sequences change when the Tchebycheff weights ($\\omega$) are varied? This is essential to demonstrate that the method can indeed explore the Pareto front."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nx59wRy2Om", "forum": "PtgQrxQ9Ak", "replyto": "PtgQrxQ9Ak", "signatures": ["ICLR.cc/2026/Conference/Submission14289/Reviewer_D3no"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14289/Reviewer_D3no"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987897153, "cdate": 1761987897153, "tmdate": 1762924736911, "mdate": 1762924736911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose AReUReDi, an optimization algorithm to bias sampling towards Pareto optimal states while preserving distributional invariance. Experiments on peptide and SMILES sequence generation demonstrate the effectiveness of AReUReDi."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem statement and the method are clearly written.\n- The authors provide theoretical guarantees that AReUReDi preserves distributional invariance and converges to the Pareto front."}, "weaknesses": {"value": "- The method seems to be a direct adaptation of the Metropolis-Hastings algorithm to the ReDi model. Could the authors clarify the novelty of AReUReDi?\n- Ablation studies on the computational cost of AReUReDi are missing. It's helpful to show how the performance of AReUReDi scales as the computation increases.\n- In Table 3, AReUReDi takes significantly longer than PepTune+DPLM. Therefore, a naive improvement of PepTune+DPLM under the same computational cost would be generating multiple samples from PepTune+DPLM (to keep the total time the same as AReUReDi) and utilizing best-of-N on top of these samples. This should offer a fair comparison between PepTune+DPLM and AReUReDi.\n- The experimental results lack comparison to baseline methods. All the tables and figures, except for table 3, only present the performance of AReUReDi or its comparison with the pretrained model without any guidance. Even in table 3, only PepTune and other very traditional MOO methods are compared with. The comparison with other reward-guided sampling baselines or RL fine-tuning baselines is missing. Although these guidance-based methods are not designed for Pareto optimization, they can be naively adapted to achieve multi-objective optimization by enforcing certain weights to balance these objectives into a single aggregated objective. Also, how does the model perform compared with [1]?\n- Why does the author select ReDi as the base generative model, instead of the more commonly used discrete diffusion or flow matching methods, like MDLM and discrete flow matching? A comparison of these base models' performance would be helpful.\n\n[1] Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design. NeurIPS 2025."}, "questions": {"value": "Please refer to the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppotjemnzT", "forum": "PtgQrxQ9Ak", "replyto": "PtgQrxQ9Ak", "signatures": ["ICLR.cc/2026/Conference/Submission14289/Reviewer_CUw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14289/Reviewer_CUw1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989682263, "cdate": 1761989682263, "tmdate": 1762924736476, "mdate": 1762924736476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}