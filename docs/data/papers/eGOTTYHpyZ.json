{"id": "eGOTTYHpyZ", "number": 20362, "cdate": 1758305126245, "mdate": 1759896981753, "content": {"title": "The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models", "abstract": "Membership inference attacks (MIAs) have emerged as the standard tool for evaluating the privacy risks of AI models. However, state-of-the-art attacks require training numerous, often computationally expensive, reference models, limiting their practicality. We present a novel approach for estimating model-level vulnerability, TPR at low FPR, to membership inference attacks without requiring reference models. Empirical analysis shows loss distributions to asymmetric and heavy-tailed and suggest that most points at risk from MIAs to have moved from the tail (high-loss region) to the head (low-loss region) of the distribution. We leverage this insight to propose a method to estimate model-level vulnerability from the training and testing distribution alone: using the absence of outliers from the high-loss region as a predictor of the risk. We evaluate our method, the TNR of a simple loss attack, across a wide range of architectures and datasets and show it to accurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We also show our method to outperform both low-cost (few reference models) attacks such as RMIA and other measures of distribution difference. We finally evaluate the use of non-linear function to evaluate risk and show the approach to be promising to evaluate the risk in large-language models.", "tldr": "We introduce a novel method to predict a model's vulnerability to SOTA membership inference attacks without the need for expensive reference model-based approaches while maintaining accurate risk assessment across various architectures and datasets.", "keywords": ["membership inference attacks", "privacy", "deep learning", "memorization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bf3460eaee17d2a8e3e20f03f32438ad3948b5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new metric to approximate the vulnerability of a model to SOTA MIAs without the need to train additional reference models. As empirical analysis shows, loss distribution on training data points tends to be long-tailed. Their proposed metric is based on the idea that during training, atypical training points tend to move from high-loss (tail) to low-loss (head) of the loss distribution as a consequence of memorisation. Identifying such samples can then provide an estimate of MIA vulnerability. The proposed metric estimates the true negative rate (TNR) of a model at a fixed FPR for an attack to account for such tail-to-head records."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach does not rely on reference models and is computationally feasible.\n- The new metric is tested against a diverse set of models (including LLMs) and image classification datasets.\n- The estimated MIA vulnerability using the new metric is compared against MIA vulnerability estimates using other SOTA metrics in the literature, such as LT-IQR.\n- Authors(s) use the proposed metric to estimate MIA vulnerability for SOTA MIAs such as LiRA and RMIA (with 64 shadow models), thereby providing empirical evidence supporting the validity of their method."}, "weaknesses": {"value": "- The metric appears to be sensitive to the choice of pre-set threshold used to compute LOSS TNR at fixed FPR as demonstrated by Eq (4). It is possible that the choice of threshold changes depending on the choice of datasets/ models. Furthermore, it is unclear whether the author(s) vary the threshold for different experimental settings. Nor is it clear if their proposed metric is robust to the choice of threshold.\n- In Line 344, the authors conjecture \"An exponential fit would imply that as LOSS TNR increases, member identification becomes easier as the model memorises more difficult samples...\". But they provide no empirical evidence to support this. As a reviewer, I would appreciate a plot (if feasible) equivalent of Table 3. Furthermore, it is not specified in the paper if the results shown in Table 3 pertain to a specific training setting or it is generalizable to other datasets/models.\n- The figures/ tables in the paper lack the necessary information to clearly convey the author(s)' intended message. They are supposed to be self-contained with as little need to refer to text in the paper as possible:\n    - Table 1: Does \"Ours\" in the table refer to the Loss TNR at fixed FPR? Can you make it clear in the caption or the table?\n    - Table 2: Same as Table 1. It lacks the context to interpret the contents of the table.\n    - Table 3: Same as Table 1. It lacks the context to interpret the contents of the table.\n- In all figures, you use TNR@FNR instead of TNR@FPR.\n- Line 335: I suppose you mean “that a linear model may not be the most appropriate given the task at hand.”\n- Figure 5's caption says, \"The LIRA TPR@FPR LOSS as a linear function of the LOSS AUC evaluated on LLMs.\" Assuming this refers to the subfigure on the right, the metric in the subfigure on the x-axis is LOSS TNR@FNR and not the LOSS AUC as the caption suggests. It will cause confusion about the effect/ claim the author(s) intend to convey using the figure."}, "questions": {"value": "**Questions**: If the author(s) can address the weaknesses detailed above, I would be amenable to revising my initial assessment.\n\n**Suggestions**: \nThe author(s) provide ample evidence (in Section 4, Hypothesis) to support their proposed metric, but there is a lack of empirical evidence that would be necessary to solidify their argument, as detailed among the weaknesses. A bigger issue with the paper is its poor presentation. The author(s) need to improve their presentation, which, in its current version, makes the paper a rather difficult read. I have detailed some of the presentation issues in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQX2GXvkVe", "forum": "eGOTTYHpyZ", "replyto": "eGOTTYHpyZ", "signatures": ["ICLR.cc/2026/Conference/Submission20362/Reviewer_7a13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20362/Reviewer_7a13"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466646067, "cdate": 1761466646067, "tmdate": 1762933816776, "mdate": 1762933816776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for estimating model-level vulnerability to membership inference attacks without training reference models. The key idea is that the absence of high-loss (tail) samples in the training loss distribution correlates with the model’s susceptibility to MIAs. The authors empirically show that the TNR of a simple loss-based attack can predict the TPR of LiRA at low false positive rates. The approach aims to offer a low-cost privacy risk estimation method suitable for large or resource-limited settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical results cover multiple datasets and architectures, demonstrating correlation between the proposed metric (LOSS TNR) and LiRA’s TPR at FPR.\n\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "- The paper focuses on model-level vulnerability estimation, but it is unclear what the real-world application scenario of such a metric is. In privacy evaluation, MIAs are primarily defined as worst-case, sample-level privacy breaches (determining whether a particular record was in training), as evidenced by Carnili et al.. A model-level average metric offers little actionable guidance: practitioners either need to test specific data records (for auditing) or evaluate defense mechanisms under realistic attack settings. The proposed metric seems to produce a correlation measure with LiRA or RMIA, but it is unclear how this would be used in practice. The paper does not provide any concrete use cases or deployment scenarios.\n\n- The technical novelty of the work is also limited. The core contribution amounts to computing the True Negative Rate (TNR) of a standard loss-based attack. This idea lacks theoretical grounding or statistical justification. The absence of analytical insights weakens the contribution, making it less suitable for a top-tier venue such as ICLR.\n\n- The evaluation is restricted to LiRA and RMIA, both of which rely on output-distribution differences. However, many MIAs operate under different assumptions: Reference-calibrated or label-only attacks (e.g., He et al., 2024; Ye et al., 2022) rely on label confidence or query perturbations, not continuous loss values. It remains unclear whether the proposed estimator generalizes to those attack families.\n\n- Finally, although the authors compare their approach with metrics like LT-IQR AUC and train-test gap, these baselines are not designed for model-level vulnerability estimation. As a result, the comparison does not convincingly demonstrate the proposed method’s superiority or distinct advantages."}, "questions": {"value": "- What are the real-world application scenarios where a model-level vulnerability metric is practically useful for privacy evaluation?\n\n- What theoretical or statistical justification supports using the TNR of a simple loss attack as a valid estimator of MIA vulnerability?\n\n- Does the proposed method generalize beyond LiRA and RMIA to other attack types, such as label-only or reference-calibrated MIAs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "INUb8s2v50", "forum": "eGOTTYHpyZ", "replyto": "eGOTTYHpyZ", "signatures": ["ICLR.cc/2026/Conference/Submission20362/Reviewer_r89d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20362/Reviewer_r89d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897393932, "cdate": 1761897393932, "tmdate": 1762933816370, "mdate": 1762933816370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method to estimate a model’s vulnerability to membership inference attacks without training any reference models. It is shown that samples most vulnerable to LiRA are the ones that were moved from the high-loss tail of the distribution to the low-loss region during training. By analyzing the loss distributions, the MIA vulnerability can be predicted. The approach outperforms SOTA methods such as RMIA in predicting LiRA’s overall success rate, while requiring no reference models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed method is way more efficient than previous approaches.\n- The method is a very good and efficient indicator to approximate vulnerability to MIAs after training a model.\n- The paper was very easy to read and to follow.\n- With an adaptation of the LOSS TNR to the LOSS AUC, the method can even be applied to LLMs."}, "weaknesses": {"value": "- While LiRA and RMIA are computationally more demanding, these attacks can be used to predict membership for individual samples. The proposed method cannot predict membership for individual samples, but only estimates the vulnerability to membership inference attacks on a model level.\n\nMisc:\n- In line 82, the sentence seems to be incomplete and has \"achieve\" two times within the sentence.\n- In line 260, \"Appendix\" and the closing brackets are missing."}, "questions": {"value": "Q1: Is it somehow possible to extend this approach to allow for sample-level membership predictions?  \nQ2: Why use the LOSS AUC only for LLMs? Did you also try it for other \"traditional\" models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tb4lcHzBU6", "forum": "eGOTTYHpyZ", "replyto": "eGOTTYHpyZ", "signatures": ["ICLR.cc/2026/Conference/Submission20362/Reviewer_Lyxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20362/Reviewer_Lyxz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928563493, "cdate": 1761928563493, "tmdate": 1762933815926, "mdate": 1762933815926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propses a low cost method to identify points that will be vulnerable to privacy leakage. The method works by tracking the loss of the points in the training set and comparing to the test set. The author further conduct experiments on LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes solid contributions. It substantially reduces the computational cost of estimating membership inference vulnerability by removing the need for reference models. It also establishes a strong empirical relationship between true MIA performance and its proposed proxy (the LOSS TNR metric), demonstrating that simple loss-based statistics can reliably estimate privacy risk. Finally, it conducts extensive experiments across diverse architectures and datasets, reinforcing the robustness and generalizability of its findings."}, "weaknesses": {"value": "My biggest concern is with the limited scale of the image experiments. The tails tend to disappear when the generalization gap is low. For example, finetuning a large transformer models (like ViT) on CIFAR datasets. I think this represents an important case for the authors to consider."}, "questions": {"value": "above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2NjiBeRsZC", "forum": "eGOTTYHpyZ", "replyto": "eGOTTYHpyZ", "signatures": ["ICLR.cc/2026/Conference/Submission20362/Reviewer_e8BK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20362/Reviewer_e8BK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968697336, "cdate": 1761968697336, "tmdate": 1762933815592, "mdate": 1762933815592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}