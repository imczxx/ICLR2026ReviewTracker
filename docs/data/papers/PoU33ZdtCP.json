{"id": "PoU33ZdtCP", "number": 12373, "cdate": 1758207350831, "mdate": 1759897514077, "content": {"title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "abstract": "While text-to-3D generation has attracted growing interest, existing methods often struggle to produce 3D assets that align well with human preferences. Current preference alignment techniques for 3D content typically rely on hardly-collected preference-paired multi-view 2D images to train 2D reward models, when then guide 3D generation — leading to geometric artifacts due to their inherent 2D bias. To address these limitations, we construct 3D-MeshPref, the first large-scale unpaired 3D preference dataset, featuring diverse 3D meshes annotated by a large language model and refined by human evaluators. We then develop RewardCS, the first reward model trained directly on unpaired 3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling effective learning of human-aligned 3D geometric preferences without requiring paired comparisons. Building on this, we propose DreamCS, a unified framework that integrates RewardCS into text-to-3D pipelines — enhancing both implicit and explicit 3D generation with human preference feedback. Extensive experiments show DreamCS outperforms prior methods, producing 3D assets that are both geometrically faithful and human-preferred. Code and models will be released publicly.", "tldr": "We propose DreamCS, a text-to-3D generation framework that aligns with human preferences using a 3D reward model trained on unpaired preference data.", "keywords": ["Preference Alignment in 3D", "Preference Alignment", "Human Preference Alignment", "3D Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55f8933bdad4637af15e947e7b8a12fbbefda52e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a systematic framework to address to the challenges of aligning optimization-based 3D generation with human preferences. In particular, the authors firstly construct 3D-MeshPref, an unpaired 3D preference dataset. They then train a reward model, RewardCS, on the dataset to predict the human preferences given a mesh. Building on the reward model, the authors propose DreamCS, which integrates RewardCS into score distillation-based optimization. Experiments on GPTEval3D benchmark demonstrate the improvements in geometric accuracy, human preference alignment, and reduction of Janus artifacts across major baselines such as DreamFusion, MVDream and Magic3D."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Novel unpaired reward learning: the paper introduces the first 3D reward model capable of learning from unpaired preference data. The CS-divergence–based loss is an elegant theoretical contribution that bypasses the need for expensive paired supervision, which is often a bottleneck in 3D learning tasks.\n* Effective reward model integration: DreamCS integrates the RewardCS model smoothly into various 3D pipelines through thoughtful system design, including differentiable meshization and adaptive mesh fusion. This indicates solid engineering and practical feasibility.\n* Systematic method design: to address the mentioned challenges, the authors constructed a new dataset, trained a new reward model, and integrated the reward model to 3D generation pipelines, forming a coherent and well-structured workflow.\n* Solid theoretical foundation: the theoretical justification for using CS divergence is well presented, with rigorous proof of its asymptotic equivalence to paired supervision. This adds credibility to the learning mechanism."}, "weaknesses": {"value": "* Limited exploration of efficiency: there is little discussion on the additional computational overhead introduced by RewardCS integration and the adaptive mesh fusion step.\n* Confusing experiment labels: the short titles for the experiments on different baselines are confusing. For example, in line 1, column 3 of Fig. 5, does \"+RewardCS\" mean \"DF+RewardCS\" or \"DF+Reward3D+RewardCS\"?\n* Potential quality drop: the mesh faces produced by DMTet could be very dense. However, the proposed Adaptive Mesh Fusion (AMF) fixes the mesh faces to match \"256 non-overlapping patches, each with 64 faces\". The conversion from dense faces might lead to a loss of geometry details. The authors should provide the results before and after the AMF operation for a clearer demonstration.\n* Non-superior metrics compared to baseline: In Table 1, the IR scores of pure RewardCS integration lag behind those of pure Reward3D integration by a clear margin."}, "questions": {"value": "* The paper only conducts experiments of integrating RewardCS to SDS-based method, have the authors considered try the reward model to those more advanced diffusion guidance algorithm (e.g., ProlificDreamer[1], LucidDreamer[2], DreamCraft3D[3], etc)?\n\n[1] Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. NeurIPS 2023.\n\n[2] Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. CVPR 2024.\n\n[3] Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. ICLR 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f7BP90xjrQ", "forum": "PoU33ZdtCP", "replyto": "PoU33ZdtCP", "signatures": ["ICLR.cc/2026/Conference/Submission12373/Reviewer_697y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12373/Reviewer_697y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739033386, "cdate": 1761739033386, "tmdate": 1762923280222, "mdate": 1762923280222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DreamCS, a framework that improves text-to-3D generation by incorporating 3D geometric preference signals. To overcome the limitations of existing 2D reward models, which often cause geometric artifacts, the authors make three key contributions:\n\n1. 3D-MeshPref. A large-scale, unpaired 3D preference dataset of text-mesh pairs with scores from an LLM and human verification.\n2. RewardCS. A 3D reward model trained on this unpaired data using a novel Cauchy-Schwarz divergence objective, which learns to distinguish mesh quality at a distribution level without direct pairwise comparisons.\n2. DreamCS framework. A method that integrates RewardCS into text-to-3D pipelines via differentiable meshization, adaptive mesh fusion, and progressive reward guidance.\n\nExperiments show that DreamCS enhances geometric quality and consistency, reducing artifacts like Janus faces and outperforming 2D-reward-based methods on 3D-specific metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed RewardCS model directly tackles the fundamental issue of 2D bias in existing text-to-3D preference alignment methods, which leads to geometric artifacts like the Janus problem. The 3D-geometric aware model also bypasses the need for hard-to-collect paired preference data.\n2. The use of Cauchy-Schwarz divergence for unpaired preference learning is both effective in practice and supported by a solid theoretical proof of its equivalence to paired learning.\n3. The method is shown to be compatible with diverse state-of-the-art generators (both implicit and explicit), greatly enhancing its utility and impact."}, "weaknesses": {"value": "1. The entire DreamCS framework is built upon the increasingly dated Score Distillation Sampling (SDS) paradigm, which is slow, optimization-based, and prone to artifacts. The field is rapidly moving towards fast, feed-forward text-to-3D generators (e.g., Trellis). A more forward-looking and potentially more effective approach for preference alignment would be to directly fine-tune these feed-forward models using human preferences, rather than adding a complex reward guidance mechanism to a slow, legacy SDS framework. The proposed method, while clever, addresses a problem that may soon be obsolete.\n2. The differentiable meshization and reward guidance introduce non-trivial computational cost compared to vanilla pipelines, as acknowledged in the appendix. While a trade-off for quality, this limits accessibility and practical iteration speed.\n3. The quality of the 3D-MeshPref dataset and the RewardCS model is contingent on the external components used (e.g., MeshAnythingV2, Llama-Mesh). Any biases or limitations in these models are directly inherited and not ablated, potentially skewing the learned notion of quality."}, "questions": {"value": "1. Can the proposed RewardCS be effectively adapted to fine-tune pretrained, feed-forward text-to-3D models? If so, what would be the proposed mechanism? and do you anticipate it would yield greater performance and efficiency gains compared to guiding slow SDS-based optimization?\n2. Given the reliance on Llama-Mesh for scoring 3D-MeshPref, how did you quantify or control for potential biases or errors in its automated assessments before human refinement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "woOfVP5gsG", "forum": "PoU33ZdtCP", "replyto": "PoU33ZdtCP", "signatures": ["ICLR.cc/2026/Conference/Submission12373/Reviewer_pQXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12373/Reviewer_pQXb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746466782, "cdate": 1761746466782, "tmdate": 1762923279866, "mdate": 1762923279866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a fundamental limitation in text-to-3D synthesis: the prevalence of geometric artifacts, such as the Janus face problem, which arise from an over-reliance on 2D view-dependent supervision. The authors posit that these flaws are an inevitable consequence of using 2D reward signals to enforce 3D consistency and propose a novel framework that learns and applies 3D-native geometric preferences directly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper correctly diagnoses a key failure mode of 2D preference signals in 3D generation and proposes a principled 3D reward to address it.\n* The paper provides a formal mathematical justification, establishing the asymptotic equivalence between the proposed unpaired objective (using CS divergence) and traditional paired supervision, which instills high confidence in the method's soundness.\n* Building 3D‑MeshPref at 30k+ meshes with human‑verified thresholds, is a non‑trivial engineering contribution likely to be useful beyond this paper.\n* The method is successfully integrated and tested on a diverse set of backbones."}, "weaknesses": {"value": "* A primary concern is the use of the \"GA\" (3D Geometry-Asset Alignment Reward) metric. The authors state (Section 4, Appendix F.1) that this metric is \"based on RewardCS\" and \"derived from RewardCS.\" Using a variant of their own proposed model as a key evaluation metric creates a significant risk of \"metric-method coupling,\" where the metric may be inherently biased to favor the architecture and training objective of the method being tested. This potential bias makes the \"GA\" scores in Table 1 less reliable for fairly comparing DreamCS against other methods.\n* While the creation of 3D-MeshPref is a notable contribution, its reliance on Llama-Mesh for initial scoring is a potential weakness. Any intrinsic biases within Llama-Mesh are likely to be inherited by RewardCS. The paper's \"human refinement\" process is insufficiently detailed to confirm whether this risk is mitigated, creating the potential for a model-centric feedback loop where RewardCS learns Llama-Mesh's preferences rather than genuine human ones.\n* Based on the paper's own analysis (Appendix G.2, Table 6), the added generation complexity results in a notable slowdown. For a 20,000-step generation, the baseline MVDream takes ~8.7 hours (at 0.64 it/s), while the proposed DreamCS (128-res) takes ~11.1 hours (at 0.50 it/s). This represents a ~27.6% increase in generation time. While the paper provides iteration-per-second metrics, a more direct comparison of wall-clock time, GPU memory usage, and throughput against the 2D-guided baselines would strengthen the analysis.\n* While the paper compares its methodology against 2D-guided preference methods (Reward3D, DreamDPO), this is not sufficient to fully support the claim of mitigating the Janus problem. The paper claims to reduce Janus artifacts but only compares against baselines and other preference methods. The argument would be much stronger if a direct comparison was provided against methods specifically designed to solve the Janus problem, such as MVControl [1], MT3D [2], or DreamControl [3].\n* The Cauchy-Schwarz divergence estimator (Eq. 3) used for training the reward model has a quadratic computational complexity of O(m²+n²+mn) with respect to the preferred (m) and dispreferred (n) batch sizes, as it relies on pairwise kernel sums. This can make the reward model training computationally intensive, potentially limiting its scalability or practical adoption.\n\n[1] Li, Zhiqi, et al. \"Controllable text-to-3D generation via surface-aligned Gaussian splatting.\" 2025 International Conference on 3D Vision (3DV). IEEE, 2025.\n\n[2] Nath, Utkarsh, et al. \"Deep Geometric Moments Promote Shape Consistency in Text-to-3D Generation.\" 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025.\n\n[3] Huang, Tianyu, et al. \"Dreamcontrol: Control-based text-to-3d generation with 3d self-prior.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qoZsrafCFK", "forum": "PoU33ZdtCP", "replyto": "PoU33ZdtCP", "signatures": ["ICLR.cc/2026/Conference/Submission12373/Reviewer_cTX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12373/Reviewer_cTX5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937399495, "cdate": 1761937399495, "tmdate": 1762923279435, "mdate": 1762923279435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the high cost of paired multi-view preference datasets and geometric artifacts in text-to-3D generation. The authors create 3D-MeshPref, an unpaired dataset of 30,000+ text-mesh-score triplets labeled via Llama-Mesh. They propose RewardCS, a reward model that encodes 3D geometry and text to predict preference scores, trained with Cauchy-Schwarz divergence to separate preferred and dispreferred meshes in latent space without requiring paired supervision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The Cauchy-Schwarz divergence training approach for unpaired preference data offers a potentially generalizable framework applicable beyond 3D generation tasks.\n- 3D-MeshPref provides a human-verified dataset of diverse unpaired 3D meshes, which may be a good community resource that helps reduces dependence on expensive paired annotations.\n- The differentiable meshization pipeline enables end-to-end optimization with geometry-aware supervision, demonstrating technical soundness in integrating 3D reward signals into existing text-to-3D frameworks."}, "weaknesses": {"value": "- The CS divergence receives extensive theoretical treatment (Appendix B) but lacks empirical validation. No ablations demonstrate performance degradation without this loss, no clustering baselines justify its necessity, and Table 4's λ variations don't compare against removing the term entirely. The mathematical formalism appears to add complexity without proven practical benefit. I ask the authors to further elaborate upon this point.\n- Table 1 exposes a critical flaw: RewardCS underperforms Reward3D on ImageReward (DreamFusion: -0.21 vs 1.71) and only improves when combined with it (1.89). This pattern repeats across backbones, contradicting the claim that 3D supervision addresses 2D methods' limitations. I believe the method functions as a supplement rather than a standalone solution: please elaborate on this point.\n- MeshMAE embeddings are trained purely for geometric reconstruction, encoding shape but not color, material, or style. Cross-attention thus lacks features to evaluate stylistic prompts.\n- The experimental results seem to only show marginal improvements over baseline method despite increased computation overhead."}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vigiWSYvZe", "forum": "PoU33ZdtCP", "replyto": "PoU33ZdtCP", "signatures": ["ICLR.cc/2026/Conference/Submission12373/Reviewer_c45f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12373/Reviewer_c45f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055254815, "cdate": 1762055254815, "tmdate": 1762923279114, "mdate": 1762923279114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}