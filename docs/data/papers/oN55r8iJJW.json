{"id": "oN55r8iJJW", "number": 1282, "cdate": 1756868962191, "mdate": 1763706359309, "content": {"title": "SAM-Veteran: An MLLM-Based Human-like SAM Agent for Reasoning Segmentation", "abstract": "Significant progress has been made in reasoning segmentation by combining multi-modal large language models (MLLMs) with the Segment Anything Model (SAM): the former excel in reasoning and vision–language alignment, while the latter offers powerful pixel-level understanding. However, current paradigms fall short in exploiting SAM’s strengths, especially the ability to support iterative mask refinement by interactive segmentation, a process that human users can naturally perform. To bridge this gap, we introduce  **SAM-Veteran**, an experienced mask-aware SAM agent capable of emulating human interaction with SAM via a reasoning-driven segmentation workflow that integrates (i) generating bounding boxes given image–query pairs for SAM input, (ii) proposing refinement points based on SAM-generated masks, and (iii) adaptively terminating the process. Aiming for this goal, we propose a multi-task reinforcement learning framework based on Group Relative Policy Optimization (GRPO), which enhances the MLLM’s abilities in textual grounding and mask comprehension. Furthermore, we introduce a dynamic sampling strategy tailored for generating both boxes and points to stabilize training. Extensive experiments across diverse datasets show that SAM-Veteran achieves human-like interaction with SAM and establishes new state-of-the-art performance on both in-domain and out-of-domain benchmarks.", "tldr": "", "keywords": ["reasoning segmentation", "multi-modal large language model", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/325fce6b918232cbb5037f2ccb0da6ae777bf3ab.pdf", "supplementary_material": "/attachment/35d1ecca7e37c51648a39922d6be89dd290b5a9f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SAM-Veteran, a reasoning-driven segmentation framework that enables human-like interaction with the Segment Anything Model (SAM). The method integrates a multi-modal large language model (MLLM) with SAM to perform iterative segmentation through three key steps: generating bounding boxes from image–query pairs, proposing refinement points based on SAM-generated masks, and adaptively terminating the process. To train the SAM-Veteran, diverse reward functions are utilized. Extensive experiments demonstrate that SAM-Veteran achieves state-of-the-art performance across both in-domain and out-of-domain datasets, effectively leveraging SAM’s interactive segmentation strengths."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents the first unified framework that integrates bounding box generation, iterative mask refinement, and adaptive termination into a single reasoning-driven segmentation process.\n- It introduces a well-designed multi-task reinforcement learning framework to effectively train the SAM-Veteran workflow, enhancing both textual grounding and mask comprehension.\n- The proposed method achieves state-of-the-art performance on both in-domain and out-of-domain datasets, demonstrating strong generalization and robustness."}, "weaknesses": {"value": "- Recently, in the reasoning segmentation task, several datasets beyond ReasonSeg have been proposed. (MUSE [1] for multi-target cases and MMR [2] for part-level reasoning). It would be interesting to see whether SAM-Veteran demonstrates good generalization performance across these diverse reasoning segmentation scenarios.\n\n[1] Ren, Zhongwei, et al. \"Pixellm: Pixel reasoning with large multimodal model.\" CVPR  2024.\n\n[2] Jang, Donggon, et al. \"Mmr: A large-scale benchmark dataset for multi-target and multi-granularity reasoning segmentation.\" ICLR 2025."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ctjzbIUU91", "forum": "oN55r8iJJW", "replyto": "oN55r8iJJW", "signatures": ["ICLR.cc/2026/Conference/Submission1282/Reviewer_fRUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1282/Reviewer_fRUK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100310225, "cdate": 1761100310225, "tmdate": 1762915726799, "mdate": 1762915726799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SAM-Veteran, an MLLM-based agent that mimics human interaction with Segment Anything Model, enabling a complete “box generation to iterative refinement to adaptive termination” reasoning segmentation workflow. Built on a multi-task RL framework using Group Relative Policy Optimization, the method enhances textual grounding and mask comprehension, achieving new state-of-the-art performance and strong cross-domain generalisation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive experiments. The authors conduct extensive evaluations across multiple in-domain and out-of-domain datasets, providing solid empirical evidence for their claims.\n\n2. Clear visualisations. The workflow and results are illustrated with intuitive and well-structured figures, making it easy to understand the model’s behaviour.\n\n3. Well-organised and easy to follow. The paper is logically structured, with each component of the method introduced and justified clearly.\n\n4. Strong motivation and validation. The motivation is well grounded, and the proposed design is effectively validated through both quantitative results and qualitative analysis."}, "weaknesses": {"value": "1. Missing related works. Some relevant literature, such as recent works on task-generic promptable segmentation[1][2] and Grounding SAM[3], is not discussed or compared in the related work section. This weakens the contextual positioning of the contribution.\n\n2. Insufficient hyperparameter analysis. Many components involve manually set hyperparameters (e.g., the threshold of R_{iou}^B=\n0.4), but the paper does not explain how these values were chosen or their sensitivity.\n\n3. Potential training complexity and stability issues . The framework integrates multiple tasks simultaneously, which may lead to excessive complexity. The paper does not provide sufficient analysis on training stability or convergence behaviour.\n\n[1] Hu, Jian, et al. \"Relax image-specific prompt requirement in sam: A single generic prompt for segmenting camouflaged objects.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 11. 2024.\n\n[2] Tang, Lv, et al. \"Chain of visual perception: Harnessing multimodal large language models for zero-shot camouflaged object detection.\" Proceedings of the 32nd ACM international conference on multimedia. 2024.\n\n[3] Ren, Tianhe, et al. \"Grounded sam: Assembling open-world models for diverse visual tasks.\" arXiv preprint arXiv:2401.14159 (2024)."}, "questions": {"value": "1. Training stability. Given the multi-task RL design, how stable is training in practice? Are there signs of task interference or convergence issues?\n\n2. Generalisation & scalability. How well does the method scale with more refinement steps or larger models, and can it generalise to broader segmentation scenarios beyond the tested datasets?\n\n3. Hyperparameters & ablation. Many hyperparameters are manually set, but their selection process and sensitivity are unclear. Could the authors provide ablation or sensitivity analyses to justify these choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nQ2MVS9SgK", "forum": "oN55r8iJJW", "replyto": "oN55r8iJJW", "signatures": ["ICLR.cc/2026/Conference/Submission1282/Reviewer_yz1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1282/Reviewer_yz1X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379720317, "cdate": 1761379720317, "tmdate": 1762915726491, "mdate": 1762915726491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SAM-Veteran as an MLLM-based solution for reasoning segmentation, of which the key feature is to emulate the human SAM users to refine the mask iteratively. Instead of relying on supervised fine-tuning of the MLLM, SAM-Veteran is built on top of the current RL-based training paradigms. The paper proposes to divide the iterative reasoning segmentation task into three sub-tasks and design reward models to help train the MLLM on each of those sub-tasks. The proposed method shows strong results on several reasoning segmentation benchmarks compared to prior SFT and RL-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, well-organized, and easy to follow. In general, the proposed method shows noticeable improvement over the prior RL-based methods and SFT-based methods, and is closer to actual human interaction with the SAM-like segmentation models. The paper includes a thorough experiment for analyzing the contribution of each design choice, quantitative results, and qualitative visualizations to clearly show the strength and potential failing cases of the proposed method."}, "weaknesses": {"value": "The motivation for having an auxiliary task is clear; however, the improvement is not significant, as shown in Table 4. Meanwhile, this means that there is room for improvement in task 2. A better reward design or pipeline for mask comprehension is needed. This weakness, as the paper points out, is linked to the limited ability of MLLM to understand images with masks, and adding a specific type of color mask on the visual input directly will inevitably compromise the valuable information contained in the target region."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f8xh8lvTNf", "forum": "oN55r8iJJW", "replyto": "oN55r8iJJW", "signatures": ["ICLR.cc/2026/Conference/Submission1282/Reviewer_GtqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1282/Reviewer_GtqV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460577392, "cdate": 1761460577392, "tmdate": 1762915726359, "mdate": 1762915726359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAM-Veteran, a multi-modal large language model (MLLM)-based agent designed to emulate human-like interaction with the Segment Anything Model (SAM) for reasoning segmentation. The key contributions include:  \n\n- A **multi-task reinforcement learning (RL) framework** that trains the MLLM to generate bounding boxes, refine masks iteratively via points, and adaptively terminate the process.  \n- A **dynamic sampling strategy** to stabilize training by diversifying box/point generation.  \n- State-of-the-art performance on in-domain (RefCOCO) and out-of-domain (ReasonSeg) benchmarks, demonstrating improved generalization over existing supervised fine-tuning (SFT) and RL-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**: The work creatively combines MLLMs with SAM’s interactive segmentation capabilities, addressing a gap in prior RL-based methods that neglect iterative refinement. The integration of three RL tasks (grounding, mask comprehension, and auxiliary) to mimic human-SAM interaction is novel.  \n**Quality**: The experiments are thorough, including ablation studies on reward design, multi-task training, and dynamic sampling. The comparison with SFT and RL baselines (e.g., Seg-Zero, SAM-R1) validates the framework’s effectiveness.  \n**Clarity**: The paper is well-structured, with clear task formulations (e.g., MDP definitions) and visualizations of the workflow. The appendices provide implementation details, prompts, and failure case analysis.  \n**Significance**: The work advances MLLM-based segmentation by enabling human-like SAM usage, which could inspire future research on interactive vision-language systems. The code and configuration details (Figure 6) enhance reproducibility."}, "weaknesses": {"value": "**Limited comparison with SegAgent**: While SegAgent (Zhu et al., 2025b) is mentioned, the paper does not quantitatively compare SAM-Veteran against it, despite both addressing iterative refinement. This omission weakens the claim of novelty.  \n\n**Inference time:** The paper does not provide inference time comparisons with baseline methods, which is critical for evaluating practical deployment feasibility. Additionally, detailed training resource requirements (e.g., GPU hours, memory consumption) are not explicitly reported, limiting reproducibility assessments and computational cost analysis for researchers with constrained resources.\n\n**SAM dependency**: The framework heavily relies on SAM for mask generation and reward computation. The impact of SAM’s inherent limitations (e.g., failure modes on fine-grained objects) on SAM-Veteran’s performance is not discussed.  \n**User study absence**: While the paper emphasizes \"human-like\" behavior, no user study evaluates whether the termination policy or refinement steps align with human preferences."}, "questions": {"value": "**Q1**: How does SAM-Veteran compare to SegAgent in terms of refinement steps and termination accuracy? The authors should include a direct comparison to clarify their method’s advantages.  \n**Q2**: Could the inference time and training requirement be provided? This would potentially enhance the feasibility of real-world deployment.\n\n**Q3**: The paper states that SAM-Veteran avoids \"catastrophic forgetting\" seen in SFT methods. Is there empirical evidence (e.g., performance on general MLLM benchmarks) to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gaSNJaVHeS", "forum": "oN55r8iJJW", "replyto": "oN55r8iJJW", "signatures": ["ICLR.cc/2026/Conference/Submission1282/Reviewer_yXBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1282/Reviewer_yXBo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807437033, "cdate": 1761807437033, "tmdate": 1762915726190, "mdate": 1762915726190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their valuable comments and suggestions.\nWe have carefully addressed each concern point by point, and the corresponding revisions have been incorporated into the updated manuscript. In addition, we provide further visualizations of our method in the supplementary materials.\n\nWe hope that our responses satisfactorily resolve the reviewers’ concerns. If any additional clarification is needed, we would be glad to provide further details."}}, "id": "yQL5evq077", "forum": "oN55r8iJJW", "replyto": "oN55r8iJJW", "signatures": ["ICLR.cc/2026/Conference/Submission1282/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1282/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission1282/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763696258564, "cdate": 1763696258564, "tmdate": 1763696258564, "mdate": 1763696258564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}