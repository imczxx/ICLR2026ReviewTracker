{"id": "APgSIyWZT7", "number": 5096, "cdate": 1757847208506, "mdate": 1759897994752, "content": {"title": "Functional Equivalence in Attention: A Comprehensive Study with Applications to  Linear Mode Connectivity", "abstract": "The parameter space of neural networks serves as a surrogate for the underlying function class; however, the mapping is inherently non-injective, as revealed by functional equivalence, wherein distinct parameter configurations yield identical input-output behaviors. While this phenomenon has been analyzed in classical architectures such as fully connected and convolutional networks, the increasing complexity of modern designs, particularly attention-based models, presents new and significant challenges. Prior analyses of multihead attention have been largely restricted to the vanilla formulation, thereby neglecting crucial components such as positional encodings that fundamentally alter architectural symmetries and render earlier results inapplicable. In this work, we undertake a formal study of functional equivalence in Transformers with positional encodings. Focusing on the two most widely used variants--sinusoidal and rotary--we demonstrate that sinusoidal encodings preserve the equivalence structure of vanilla attention, whereas rotary encodings significantly reduce the associated symmetry group, thereby enhancing expressivity. This theoretical insight offers a principled explanation for the growing prominence of RoPE in practice. Furthermore, we extend our analysis to investigate how positional encodings influence the phenomenon of linear mode connectivity (LMC). By introducing an alignment algorithm, we empirically validate the presence and variability of LMC across a wide range of Transformer configurations, datasets, and modalities, demonstrating that the type of positional encoding plays a decisive role in shaping the connectivity of solutions.", "tldr": "This work studies the functional equivalence in Transformers with positional encodings", "keywords": ["functional equivalence", "attention mechanism", "positional encoding"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73832323596ba713290aff2ec9b2eeb0872ab34e.pdf", "supplementary_material": "/attachment/2f38111866b9a260a2e1ea5a9cd5d00b0dc268db.zip"}, "replies": [{"content": {"summary": {"value": "The paper explores the linear mode connectivity of transformers using positional encodings. To do so, the paper explores the functional equivalence of these architectures by characterising the symmetry groups of the parameters of the attention mechanism. By then developing algorithms to align the parameters of two transformer models, the authors empirically validate the presence of linear mode connectivity in these models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical Contribution: The authors provide a strong theoretical characterisation of the symmetry group of attention with positional encodings.\n2. Practical Application: The authors develop methods to facilitate the exploration of linear mode connectivity for transformers with positional encodings.\n3. Empirical Validation: The authors utilise their methods in an array of settings."}, "weaknesses": {"value": "1. Limited Experiments in the Main Text: The paper presents limited results, with Tables 1 and 2 providing minimal information. \n2. No Practical Suggestions as a Result of Findings: A study into what architectures or training hyperparameters result in LMC would be beneficial for informing the training of these models. With the proposed tools, it seems within scope to perform these experiments and provide valuable contributions to the community. \n3. Presentation: There are various formatting and presentation issues. For example, line 208 appears to be missing a section of text. The paper reads as an overly condensed version of a much longer paper (which it is). This affects how the problem is introduced and the clarity of the paper's details."}, "questions": {"value": "1. How does the theory account for causal masks? How does this influence the study of the text-based datasets, since the corresponding transformers implement causal masks?\n2. Could you provide greater details as to why obtaining LMC is desirable?\n3. How do architectural design and training hyperparameters influence the extent to which LMC is observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CChnzXopZG", "forum": "APgSIyWZT7", "replyto": "APgSIyWZT7", "signatures": ["ICLR.cc/2026/Conference/Submission5096/Reviewer_NSy3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5096/Reviewer_NSy3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761397155530, "cdate": 1761397155530, "tmdate": 1762917868475, "mdate": 1762917868475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigated how positional encodings affect the symmetry group of multihead attention. The authors showed that sinusoidal encodings preserve the equivalence structure of MHA, whereas rotary encodings restrict the symmetry group. In addition, they characterized equivalence under RoPE and proposed a two-stage matching algorithm to align attention parameters. Extensive experiments across vision and language models investigate how PE affects LMC under various re-initialization schemes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The characterization of the reduced symmetry group $G_{RoPE}$ is mathematically clean and novel, providing the first rigorous treatment of functional equivalence in attention with rotary encodings. Additionally, the authors extend the functional equivalence analysis to general multihead attentions.\n\n- The proposed matching approach adapts the weight-matching idea to proposed symmetry groups. The authors also proposed gradient-based optimization methods to achieve weight matching.\n\n- The experiments span a wide range, testing four re-initialization strategies and validating both theoretical predictions and alignment behavior. Ablation tables are detailed and convincing."}, "weaknesses": {"value": "- Although the main contribution of the equivalence analysis is based on RoPE, the experiments only involve old models such as BERT and ViT. It would be helpful if more experiments could be conducted on models that are naturally equipped with RoPE, such as Llama3 and Mistral model series, for a larger real-world impact.\n\n- Some baselines on weight matching for Transformers are missing, such as [1,2]. \n\n- It is mentioned in the abstract that rotary encodings reduce the associated symmetry group, thereby enhancing expressivity. However, there is no further support for the connections between symmetry group sizes and expressivities in the following sections.\n\n- The writing can be further improved:\n    - The introduction section consists of multiple introductions of related work, but the connection between different subsections can be strengthened.\n    - It would be helpful to add more intuition about the theorems in the main text or a simple diagram in Section 4.\n\n[1] Imfeld M, Graldi J, Giordano M, et al. Transformer Fusion with Optimal Transport[C]//The Twelfth International Conference on Learning Representations.\n\n[2] Zhang B, Zheng Z, Chen Z, et al. Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion[C]//Forty-second International Conference on Machine Learning."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LosOx4uifA", "forum": "APgSIyWZT7", "replyto": "APgSIyWZT7", "signatures": ["ICLR.cc/2026/Conference/Submission5096/Reviewer_Xznn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5096/Reviewer_Xznn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882441489, "cdate": 1761882441489, "tmdate": 1762917868081, "mdate": 1762917868081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses the symmetry group that occurs in the weight space of the self-attention component, with a focus on attention with rotary-positional encodings. It extends the ideas from previous work [1] by including the positional encodings and proposes a data-independent algorithm for aligning networks inspired by another previous work [2]. Finally, the authors use the proposed alignment method to study linear mode connectivity in a few Transformer-based architectures (BERT, GPT, ViT) and across initialisation schemes (single layer/whole model reinitialized) and observe LLC in many of the studied settings contrary to naively looking for LLC without aligning.\n\n[1] Hoang V. Tran, Thieu Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant neural functional networks for transformers. In The Thirteenth International Conference on Learning Representations, ICLR, 2025.\n[2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation of the paper is very good. The problem and prerequisites are well outlined and the main result clearly presented. I especially appreciate the shortened versions of the proofs in the appendix before stating the whole proof.\n2. For my understanding, finding a symmetry group that takes the embedding into account is novel.\n3. Proposing an algorithm that allows for studying LLC in Transformer can be useful for the LMC community.\n4. The experiments cover many models, dataset and initialization settings, and I appreciate that the authors conduct the ablation study for their algorithm."}, "weaknesses": {"value": "1. Other than naively measuring the LMC (interpolation between points as in figure 2) without any alignment, the work does not compare how the LMC behaves under other alignment methods, for example the one used in [3]\n2. Accounting for the rotary positional embeddings in the method is not ablated. By that I mean checking how LMC behaves (similar result as in figure 2) in a model that uses RoPE but while using the $GL(d_h)$ symmetry group.\n3. I believe that in line 265 in equation 15 it should be $x_m^\\top A_i^{m,n}x_n$ inside the softmax instead of  $x_mA_i^{m,n}x_n^\\top$. Unless the authors assume that $x_m$ and $x_n$ are row-vectors. In that case it should be clearly stated but ideally reconsidered, as column-vectors are more canonical. Similarly in many places in the appendix, for instance in lines 1421-1423.\n4. The definition of attention with RoPE is missing in line 208.\n\nMinor:\n1. In line 179, the authors say that it is a standard assumption that $d=d_h$. I believe the standard implementation assumption is $d=h*d_h$. Can the authors substantiate their claim with any reference?\n2. What does “this” refer to in line 296, when saying “this theory”.\n3. m-dashes are not used properly. For example in line 427 there is a dash instead of an m-dash before “specifically”, and possibly also in line 420 there are n-dashes instead of m-dashes.\n\n[3] Alexander Theus, Alessandro Cabodi, Sotiris Anagnostidis, Antonio Orvieto, Sidak Pal Singh, and Valentina Boeva. Generalized linear mode connectivity for transformers."}, "questions": {"value": "1. Can you explain more in detail, how does it happen that the positional encoding applied to the input does not change the symmetry group of the attention?\n2. Have you ablated/investigated the choice of the Frobenius matrix norm in the matching method described in section 5? Does it matter for the method if I choose spectral or nuclear norm instead? Would the results from Figure 2 (especially 2c, where the LMC is not ideal) change under a different norm?\n3. How does the alignment method presented in this paper compare experimentally to the alignment method from [3]? Can you repeat (at least part of) your interpolation experiments (like the one in figure 2) for the method from [3]?\n4. In section 6.1 the lack of LMC for models with causal attention is painted as a limitation of architecture. Isn’t it a limitation of the proposed alignment method? In the end, it does not consider masking. If so, I believe it should be clearly stated.\n5. This work proposes to align the models by transforming the weight matrices linearly along the head dimension $d_h$. Previous work [3] did the alignment along the embedding dimension $d$ (equation 18 in [3]). Shouldn’t we ideally align both dimensions before interpolating to check for LMC?\n\n[3] Alexander Theus, Alessandro Cabodi, Sotiris Anagnostidis, Antonio Orvieto, Sidak Pal Singh, and Valentina Boeva. Generalized linear mode connectivity for transformers."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "I would like to flag this submission as potentially violating the submission policy. Submission 5098 presents the same theoretical result on the characterisation of the symmetry of attention with widely used PE, and I have reasons to believe (see below) that both papers share authors. The papers use the theoretical result in different application contexts (LMC and efficient Transformer training) but they both claim that the theoretical result and its discussion are one of their main contributions.\n\nThe reason I believe that the papers share authors and claim the same contributions are the following parts of the papers:\n* In “Contribution” section of submission 5096 in lines 110-117 we read:\n>>2. In Section 3, we analyze how positional encodings alter the internal structure of attention. We focus primarily on the most widely used encodings, Absolute PE and Relative PE. In particular, we study sinusoidal PE as a representative of APE and rotary PE as a representative of RPE, and show why results from the vanilla case do not extend directly to these settings. \n>> 3. In Section 4, we present the main result of the paper, which characterizes the full symmetry of attention with widely used positional encodings. This characterization underlies the matching algorithm for Multihead Attention described in Section 5.\n\nMeanwhile, in “Contribution” section of submission 5098 in lines 98-102 we read:\n>> 2. In Section 3, we analyze how positional encodings alter the internal structure of attention. We focus primarily on the most widely used encodings, Absolute PE and Relative PE. In particular, we study sinusoidal PE as a representative of APE and rotary PE as a representative of RPE, and show why results from the vanilla case do not extend directly to these settings. We then present our finding that fully characterizes the symmetry of attention with widely used PE.\n* The symmetry group under RoPE both papers introduce is the same, just with slightly different notation, see lines 219-247 in 5096 and lines 249-278 in 5098.\n* The main theoretical result regarding multihead attention with RoPE is the same in both papers with just slightly altered exposition, see lines 308-320 in 5096 (Theorem 4.2) and lines 279-289 in 5098 (Theorem 3.2).\n* Both papers derive their theorem as a consequence of a more general result. Submission 5096 states it lines 289-292 (Theorem 4.1) while submission 5098 in lines 895-903 in the appendix (Theorem B.1). The proofs of both theorems also look strikingly similar (see lines 1499-1527 in submission 5096 and lines 905-933 in submission 5098)."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Itv74uMix", "forum": "APgSIyWZT7", "replyto": "APgSIyWZT7", "signatures": ["ICLR.cc/2026/Conference/Submission5096/Reviewer_5bft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5096/Reviewer_5bft"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922992306, "cdate": 1761922992306, "tmdate": 1762917867624, "mdate": 1762917867624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates functional equivalence in transformers, specifically examining how different positional encodings affect the symmetry structure and linear mode connectivity (LMC). The authors provide a theoretical analysis of functional equivalence for multihead attention with different types of positional encoding, particularly focusing on sinusoidal and rotary encodings. The authors show that sinusoidal encodings preserve the symmetry group of the vanilla attention, while RoPE shrinks the symmetry group, hence enhancing expressivity. The paper introduces an alignment algorithm leveraging these symmetries and empirically validates LMC across various transformer configurations and datasets. The experiments indicate that while LMC consistently emerges in models with encoders given appropriate alignment, it fails in decoder-only models for large-scale tasks, due to the restricted information flow in causal attention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** The paper addresses a theoretical gap by characterizing functional equivalence in attention mechanisms with positional encodings. This improves our theoretical understanding of how positional encoding plays into transformers’ performance, enabling developing tools that can lead to future improvements. \n\n**S2.** The theoretical analysis is comprehensive, presenting formal characterizations of symmetry groups for different types of positional encodings and connecting these to practical implications for expressivity.\n\n**S3.** In particular, the analysis offers a principled explanation for RoPE’s popularity. Shrinking the symmetry group offers a neat theoretical story for RoPE’s empirical success due to enhanced expressivity.\n\n**S4.** The proposed alignment algorithm is well-designed and theoretically grounded, decomposing the matching problem into discrete head permutation and continuous parameter transformation stages, with specialized solutions for different symmetry groups.\n\n**S5.** The broad experimental sweep offers extensive empirical evidence with 17 datasets and 3 modalities, and careful control experiments that connect theory to practice."}, "weaknesses": {"value": "**W1.** The authors could motivate their study better. The paper falls short in clearly motivating how studying LMC in attention mechanisms leads to practical implications. While they establish theoretical connections between positional encodings and symmetry groups, the paper doesn't clearly explain how these insights could inform better model design, training strategies, etc. \n\n**W2.** The conceptual flow could be improved. The paper often alternates long theoretical details with heavy notation before clarifying the high-level idea, making it hard to follow on first read. Ideally, a high-level motivation and argument before theoretical details for explaining where they come from, and after them for clarifying what they imply, helps readers follow the paper more easily.\n\n**W3.** The paper attributes the LMC failure in decoder-only models to causal attention, but, to my understanding, the authors do not provide any targeted experiment on this that can isolate that factor. \n\n**W4.** I appreciate the theoretical contribution of the paper, and would like to emphasize that this last weakness is not major in my opinion, but for a better applicable and practically-relevant demonstration, one should consider running the experiments on more modern/larger transformers. The presented experiments are mostly on models that are currently effectively toy models and not used in SOTA pipelines (e.g., small ViT, GPT-2, BERT, etc).  That being said, I don’t want to be the reviewer who asks for “larger experiments”, and I’m not asking for that, though, I presume the authors would agree that larger scales would provide more detailed and relevant demonstrations."}, "questions": {"value": "**Q1.** Can you provide examples of how understanding the symmetry structure of groups of different positional encodings could guide architectural design choices or training strategies?\n\n**Q2.** Have you considered developing specialized alignment methods for causal attention? Or can you provide theoretical/conceptual insights (high-level arguments would suffice in my opinion) into why LMC fails with causal attention?\n\n**Q3.** How does the computational cost of the alignment algorithm scale with model size? Any practical limitations when applying it to modern large-scale transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VhTrR2l9fi", "forum": "APgSIyWZT7", "replyto": "APgSIyWZT7", "signatures": ["ICLR.cc/2026/Conference/Submission5096/Reviewer_dS1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5096/Reviewer_dS1c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926963947, "cdate": 1761926963947, "tmdate": 1762917867208, "mdate": 1762917867208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}