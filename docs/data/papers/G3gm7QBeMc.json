{"id": "G3gm7QBeMc", "number": 20095, "cdate": 1758302419611, "mdate": 1759897002156, "content": {"title": "Bayesian Robust Cooperative Multi-Agent Reinforcement Learning Against Unknown Adversaries", "abstract": "We consider the problem of robustness against adversarial attacks in cooperative multi-agent reinforcement learning (c-MARL) at deployment time, where agents can face an adversary with an unknown objective. We address the uncertainty about the adversarial objective by proposing a Bayesian Dec-POMDP game model with a continuum of adversarial types, corresponding to distinct attack objectives. To compute a perfect Bayesian equilibrium (PBE) of the game, we introduce a novel partitioning scheme of  adversarial policies based on their performance against a reference c-MARL policy. This allows us to cast the problem as finding a PBE in a finite-type Bayesian game. To compute the adversarial policies, we introduce the concept of an externally constrained reinforcement learning problem and present a provably convergent algorithm for solving it. Building on this, we propose to use a simultaneous gradient update scheme to obtain robust Bayesian c-MARL policies. Experiments on diverse benchmarks show that our approach, called BATPAL, outperforms state-of-the-art baselines under a wide variety of attack strategies, highlighting its robustness and adaptiveness.", "tldr": "A Bayesian approach for coping with unseen attacks in c-MARL", "keywords": ["multi-agent reinforcement learning", "adversarial attacks", "Bayesian games", "robust RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4dc4ee42751e458c6ea9cd67512d087aac26a1d.pdf", "supplementary_material": "/attachment/27d5d33ef58bb06fea48f4fea5cd5ace022b3456.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a method to optimize against Bayesian adverrsaries with several unknown types that corresponds to different reward reductions. The authors initially assume a continuous type space, then discretize it to some representitive type spaces, thus learning a more flexible policy against attackers. Experiments show higher robustness of their methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper solves the problem of continuous type in MARL with Bayesian attackers. While previous works solves the problem with 0-1 type, the paper expands the problem to continuous space and give solid theoretical derivations.\n\n2. Technically, the authors propose BATPAL to gain robustness against varying attackers. While experiments are not highly extensive, it sufficiently verifies the claim of users."}, "weaknesses": {"value": "1. only one adversary is considered. This is probably inherited from EIR-MAPPO, but this remains a limitation nonetheless. Since the attackers are less severe, maybe we can consider multiple attackers?\n\n2. A very simple method of controlling the severity of attack is by a linear combination of a benign policy and an adversarial policy, as shown in [1]. Why we need to learn a constrained policy as shown in Section 4? I'm not asking for additional experiments, just curious about the design since current method is quite complex both theoretically and empirically, and I wonder if a simple interpolation of policies can solve most problems.\n\n[1] Action Robust Reinforcement Learning and Applications in Continuous Control.\n\n3. I do not fully understand Eqn. 7. Since it is proposed to solve (4), and in (4) the problem is formulated as a min-max game over a shared value function, then why we have two POMDPs? My understanding is we need to minimize reward while constraining it in a  certain range, but in this way it is a single constrained POMDP.\n\n4. Do there exist an exact threat model of adversaries and a clear definition of adversaries? I see some definitions in Sec 4.1 but am curious of how  to formally determine a \"ground truth\" type given an attack policy. As also claimed by authors, two attack policies can vary in parameter and behaviors, while a single defense policy may not be effective for both.\n\n5. In Proposition 3.2 the importance of generating diverse adversaries are proposed. However, in Eqn. 7, 8 and 9, I wonder which term corresponds to diversity. I assume it is related to \\theta_1 and \\theta_2 in Proposition 3.2? but numbers are different. What refers to 0 and 1 in subscripts with bracket?\n\n6. Clarity. This is not a big issue, but I would recommend using bullet points or explicit subsections to improve the organization of the paper. Currently some important informations and settings are hard to find in a glance.\n\n7.  Proposition 3.2 seems to be the difference between (1) and (2)? Guess this is a typo."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O2CCoJELen", "forum": "G3gm7QBeMc", "replyto": "G3gm7QBeMc", "signatures": ["ICLR.cc/2026/Conference/Submission20095/Reviewer_JzM1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20095/Reviewer_JzM1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760519720684, "cdate": 1760519720684, "tmdate": 1762932990926, "mdate": 1762932990926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies execution-time robustness in cooperative multi-agent reinforcement learning under unknown adversarial objectives. The authors formulate the problem as a Bayesian Dec-POMDP with a continuous adversarial type space and make it tractable by discretizing adversarial types based on performance degradation (severity) relative to a reference policy. The resulting framework seeks a Perfect Bayesian Equilibrium (PBE), where the cooperative policy conditions on beliefs over adversarial types. To generate representative adversaries, the paper proposes an Externally Constrained RL formulation solved via a log-barrier gradient method with convergence guarantees and introduces a practical EC-PPO variant for stability. A severity-dependent regret bound theoretically shows that finer partitions reduce regret and improve adaptivity. Empirical evaluations on SMAC, LBF, and MPE-Spread demonstrate consistent performance gains over baselines across both seen and unseen adversarial conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual novelty and clarity of the discretized Bayesian game:\nThe paper presents a novel Bayesian framework that models adversarial uncertainty as a continuous type space and discretizes it based on the degree of performance degradation (severity). Here, severity is defined as the reduction in performance relative to a fixed cooperative reference policy, which allows the continuous adversarial type space to be transformed into a computationally tractable finite set of types, formulated as a Bayesian Dec-POMDP $\\hat{M}_B$ (Eq. 2). The equilibrium concept of this discretized game is defined as a Perfect Bayesian Equilibrium (PBE), where the cooperative policy is conditioned on beliefs over adversarial types and adaptively responds to different levels of attack severity (Eq. 4). This formulation clearly captures cooperative–adversarial interactions under adversarial uncertainty, enabling tractable reasoning over continuous uncertainty without relying on manually defined attacker classes.\n\n2. Technical contributions for adversary synthesis:\nThe paper introduces an Externally Constrained Reinforcement Learning formulation for training adversarial policies. Each adversary aims to minimize the cooperative team’s return while satisfying the severity constraint defined over the partitioned type space. To address this constrained optimization problem, the authors propose a log-barrier policy-gradient method and theoretically prove convergence to a KKT point under mild assumptions. Furthermore, to ensure practical stability, they propose the EC-PPO algorithm, which incorporates PPO-style clipping to alleviate instability near constraint boundaries. The algorithm performs simultaneous gradient updates between the cooperative agents and adversaries, allowing adversaries to act as approximate minimizing oracles while the cooperative policy progressively adapts to them.\n\n3. Theoretical and Empirical Insights into Severity-Based Partitioning:\nThe paper proposes a severity-based partitioning of the adversarial type space to enhance the diversity of attack behaviors and provides both theoretical and empirical justifications for this design. This approach demonstrates that considering multiple severity partitions, rather than training a single worst-case (max–min) adversarial policy, leads to lower regret and higher adaptability, as mathematically established in the theoretical analysis. Such partitioning enables the model to learn representative adversarial policies for each severity level, thereby maintaining training stability and convergence while capturing a wider spectrum of adversarial strategies. Furthermore, experimental results presented in Section 5 (Figure 2) reveal that the most severe attacks do not always cause the greatest performance degradation. This finding suggests that the impact of adversarial behavior cannot be fully explained by attack strength alone, as attacks of different severity levels may exert distinct strategic effects. In particular, in the LBF, MPE-Spread, and SMAC-2s3z environments, moderate-intensity attacks were observed to disrupt cooperative balance more severely than the strongest attacks, which is consistent with the theoretical analysis. Overall, the proposed severity-based partitioning effectively handles continuous uncertainty in adversarial intensity and provides a principled means to achieve quantitative robustness across a diverse range of attack strengths."}, "weaknesses": {"value": "1. Limited threat model and multi-agent realism:\nThe paper assumes a restricted threat model in which at most one victim agent is attacked per episode. While this setting enables analytical simplification (Sec. 3), it may not fully capture the diversity of adversarial scenarios that can arise in realistic multi-agent environments. For instance, the EGA mechanism in ROMANCE [1] generates a wide range of attack policies through evolutionary processes, and the Wolfpack attack in WALL [2] dynamically targets multiple agents across timesteps, reflecting more complex and coordinated adversarial behaviors inherent to multi-agent systems. To better account for such multi-agent adversarial settings, interactive and cooperative attack characteristics beyond those of single-agent scenarios may need to be further considered. The proposed method provides meaningful diversity by learning various single-victim attacks with different severities; however, this diversity appears to focus mainly on the single-agent perspective and might not fully capture the broader robustness challenges that could emerge from simultaneous or cooperative multi-agent adversarial interactions. Including a discussion or empirical extension on how the proposed Bayesian partitioning framework could be adapted or evaluated under these richer multi-agent adversarial scenarios would help clarify the scope and enhance the practical relevance of the work.\n\n2. Limited diversity of experimental scenarios:\nThe experimental evaluation appears to be conducted on a relatively limited set of scenarios — one environment each for LBF and MPE, and two scenarios for SMAC. Compared to recent studies on robust MARL, which often consider a broader range of tasks or larger-scale multi-agent systems, the current evaluation setup may provide only a partial view of the proposed method’s generality. Expanding the experiments to include additional and more diverse cooperative environments (e.g., larger SMAC maps, alternative LBF configurations, or other benchmark multi-agent settings) would further enhance the empirical validation and offer a more comprehensive assessment of the method’s robustness and scalability.\n\n3. Computational cost and scalability with respect to severity partitioning:\nThe proposed approach maintains a separate adversarial policy for each severity level K, suggesting that computational complexity may increase as K grows. In the appendix, results are reported only for K = 3, 4, 5, and the authors also note that training becomes more demanding for larger K. In more complex environments, finer partitioning could require learning multiple diverse adversarial policies, which may introduce scalability challenges in terms of training time and computational resources. Providing quantitative measurements of the computational cost analysis would enhance the empirical transparency and credibility of the study.\n\n\n[1] Yuan, L., Zhang, Z., Xue, K., Yin, H., Chen, F., Guan, C., Li, L., Qian, C., and Yu, Y. Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 11753–11762, 2023.\n[2] Lee, Sunwoo, et al. \"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning.\" International conference on machine learning. PMLR, 2025."}, "questions": {"value": "Minor notation and typographical issues:\n\n1.\tIn Equation (12), it is unclear what the subscript T in S_T specifically denotes. Given the surrounding context, it appears that this might be a typographical error and that the intended notation is S_t, representing the state at timestep t. Clarifying whether T refers to a specific terminal time or if it is indeed a typo would help prevent confusion.\n\n2.\tIn Appendix Table 1, the entry labeled “SAMC-2s3z” seems to contain a typographical error; it should likely read “SMAC-2s3z.” Correcting this minor issue would improve the overall consistency of the presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yr3pw4LDi3", "forum": "G3gm7QBeMc", "replyto": "G3gm7QBeMc", "signatures": ["ICLR.cc/2026/Conference/Submission20095/Reviewer_GewT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20095/Reviewer_GewT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809581823, "cdate": 1761809581823, "tmdate": 1762932990217, "mdate": 1762932990217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical problem of robustness against unknown adversaries in cooperative Multi-Agent Reinforcement Learning (c-MARL). The authors propose BATPAL, a novel method that models the problem as a Bayesian Dec-POMDP with a continuum of adversarial types. To make the problem tractable, they introduce a partitioning scheme for the adversarial type space based on the severity of an attack, measured by its performance against a reference policy. This discretization allows them to frame the problem as finding a Perfect Bayesian Equilibrium in a finite-type game. Empirical results on several benchmarks demonstrate that BATPAL outperforms state-of-the-art baselines against a variety of attack strategies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Problem Formulation: The Bayesian formulation to capture uncertainty over adversarial objectives is a relevant and timely direction for robust MARL, moving beyond the standard worst-case adversary assumption.\n\n- Comprehensive Evaluation: The experimental section is thorough, using multiple diverse environments (SMAC, MPE, LBF) and a wide array of adversarial policies, including unseen dynamic adversaries, to evaluate the proposed method."}, "weaknesses": {"value": "- Limited Theoretical Novelty and Justification for Partitioning: While the Bayesian game model is a good framing, the core partitioning mechanism, while intuitive, lacks a strong theoretical justification. Proposition 3.2 provides a bound on KL divergence, but this bound can be very loose in practice, and it is not conclusively demonstrated that this partitioning is the optimal or most efficient way to discretize the type space. The choice feels more heuristic than principled, and the theoretical benefits (mitigating local optima, ensuring diversity) are stated but not rigorously proven.\n\n- High Complexity and Insufficient Ablation: The proposed method is significantly more complex than the baselines, requiring a pre-trained reference policy, K separate adversarial networks, a belief network, and a two-timescale update. This complexity raises concerns about scalability and computational cost. A major weakness is the lack of a proper ablation study. It is unclear how much each component (e.g., the Bayesian belief, the specific partitioning, the EC-PPO component) contributes to the final performance. For instance, would a simpler ensemble of adversaries trained with different fixed rewards perform similarly? The gains could be attributed more to exposure to a diverse set of adversaries during training rather than the specific Bayesian partitioning scheme.\n\n- Technical Presentation and Proofs: The presentation of the externally-constrained RL algorithm and its convergence proof is highly technical and complex. While Proposition 4.2 claims convergence, the assumptions (e.g., perfect critics, bounded derivatives, a strictly feasible starting point) are very strong and often not met in deep RL practice. The subsequent switch to a more practical EC-PPO variant, while understandable, undermines the theoretical guarantees provided earlier. This creates a gap between the theory and the practical implementation."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OvV8U1NUl", "forum": "G3gm7QBeMc", "replyto": "G3gm7QBeMc", "signatures": ["ICLR.cc/2026/Conference/Submission20095/Reviewer_oTvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20095/Reviewer_oTvg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902069700, "cdate": 1761902069700, "tmdate": 1762932989703, "mdate": 1762932989703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BATPAL, a Bayesian framework for making c-MARL robust against unknown adversarial behaviors. Instead of assuming a single worst-case adversary, the authors model uncertainty through a Bayesian Dec-POMDP with a range of adversarial types. The authors introduce an externally constrained RL formulation with the EC-PPO algorithm to learn representative adversarial policies, and use simultaneous gradient updates to train robust cooperative agents toward a Perfect Bayesian Equilibrium."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is extending standard MARL robustness beyond worst-case adversaries, which could potentially provide a new problem setup.\n- The paper provides a theoretical analysis, such as regret bounds and a convergence proof for EC-PPO."}, "weaknesses": {"value": "- Even though the method is theoretically sound,  the computational cost of training multiple adversarial policies (one per severity level and victim) could potentially be prohibitively large for with increasing agent team size.\n- There seems to missing ablation such as the performance of the proposed method with and without the belief network, and how does the method performs with different under different accuracy of the blief network?"}, "questions": {"value": "- The paper uses 4 levels of severity. How does it affect the performance of the proposed method if we use a different number of severity levels?\n- How does the EC PPO compare to other constrained RL algorithms? Are they any evaluation specifically on the model?\n- What is the real-world applicability of the proposed method and what are the real-world use cases for this type of method where you can assume"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q4ZBI9ULop", "forum": "G3gm7QBeMc", "replyto": "G3gm7QBeMc", "signatures": ["ICLR.cc/2026/Conference/Submission20095/Reviewer_PTA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20095/Reviewer_PTA9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982823552, "cdate": 1761982823552, "tmdate": 1762932988999, "mdate": 1762932988999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}