{"id": "5bJZtzTFYy", "number": 10882, "cdate": 1758184024138, "mdate": 1759897622716, "content": {"title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching", "abstract": "Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\\times$ speedup with comparable visual quality.", "tldr": "", "keywords": ["Diffusion Model", "Video Generation", "Cache"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e0192a1e782461886402bb2ceddb0afc43be30b.pdf", "supplementary_material": "/attachment/861d91ec5fff84330b7332b787c836ba658374b1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes BWCache, a training-free acceleration method for DiT-based video diffusion. It caches and reuses block-level features across timesteps and triggers reuse with a similarity indicator computed from the mean relative L1 distance between adjacent-timestep block features. To avoid latent drift, it adds periodic recomputation with a reuse interval. Experiments on Open-Sora, Open-Sora-Plan, Latte, Wan 2.1, and HunyuanVideo report up to 2.6× speedup with comparable visual quality, along with scaling results under multi-GPU DSP and ablations on the indicator threshold and reuse interval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The mechanism is training-free, compatible with a range of DiT video generators, and easy to integrate at inference time.\n\n2. The use of a block-wise mean relative L1 indicator to decide reuse is principled and aligns with the observed per-timestep dynamics.\n\n3. Results span multiple DiT-based models and include comparisons to ∆-DiT, T-GATE, PAB, and TeaCache, plus scaling behavior with DSP and ablations over threshold δ and reuse interval R.\n\n4. The paper gives quantitative and qualitative evidence that BWCache sustains visual fidelity at meaningful acceleration levels."}, "weaknesses": {"value": "1. The comparison with related work in Table 1 is not sufficiently comprehensive. Several recent and relevant caching-based acceleration methods, such as FasterCache and TaylorSeer, are missing from the comparison.\n\n2. The appendix shows that the proposed method increases memory consumption compared with the original base model. It should also include the memory usage of other representative cache-based methods to demonstrate the superiority of the proposed approach."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nzg5tKkkpF", "forum": "5bJZtzTFYy", "replyto": "5bJZtzTFYy", "signatures": ["ICLR.cc/2026/Conference/Submission10882/Reviewer_wT4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10882/Reviewer_wT4r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461617692, "cdate": 1761461617692, "tmdate": 1762922093161, "mdate": 1762922093161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BWCache, a training-free acceleration framework for Diffusion Transformer (DiT)–based video diffusion models. Instead of modifying the model architecture, BWCache reuses intermediate block-level features across diffusion timesteps when the change between adjacent blocks is below a similarity threshold δ. A periodic recomputation strategy mitigates latent drift.\n\nComprehensive experiments across five representative video diffusion models show that BWCache achieves up to 2.6× speed-up while maintaining comparable or slightly better perceptual quality compared to TeaCache and PAB. Statistical analyses and ablations (threshold $\\delta$, reuse interval, GPU scaling) reinforce the empirical soundness.\n\nWhile the acceleration results are impressive, the approach incurs non-negligible memory overhead due to feature caching, and its stability across architectures and motion intensities remains insufficiently explored. Nevertheless, the work offers a practical plug-and-play contribution that meaningfully advances inference-time efficiency for large-scale DiT-based video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "BWCache demonstrates outstanding performance in **redundancy granularity**, **cross-scenario adaptability**, and **quality–efficiency balance**, providing a precise and general acceleration framework tailored for redundant computation in DiT-based video generation models.\n\n1. **Focusing on Module-Level Redundancy, Breaking the Granularity Limitation of Traditional Caching**  \n   Existing caching methods (e.g., TeaCache, PAB) typically operate at the *timestep level* or *attention-head level*: the former risks losing critical timestep information, while the latter incurs high computational overhead and limited acceleration:\n   - **Key discovery of module-level redundancy:** By analyzing DiT model computation time distribution, the authors find that DiT modules account for over 70% of total inference latency, and that module features exhibit a *U-shaped variation pattern* across diffusion timesteps — large differences in early (high-noise) and late (low-noise) stages (requiring computation), but small differences in mid-stages (moderate noise, high redundancy, suitable for caching).  \n   - **Relative L1 distance as a quantitative criterion:** They define the *relative L1 distance* between module features and aggregate it into  \n     $ \\text{ARL1}(t) $ — the sum of all module-wise relative L1 values — as the redundancy measure. When  \n     $ \\text{ARL1}(t)/N < \\delta $ (δ = threshold), cached features are reused, ensuring that computation is skipped *only* when true redundancy exists, preventing blind reuse and quality degradation.  \n   This design focuses on the *most time-consuming components* (modules), guaranteeing both substantial acceleration and controlled quality degradation — the key to BWCache’s superior performance.\n\n2. **Consistent Cross-Model Acceleration with Negligible Quality Loss**  \n   Across five mainstream video generation models (Open-Sora, Open-Sora-Plan, Latte, Wan 2.1, and HunyuanVideo), BWCache achieves a strong balance between *speed* and *quality*, without requiring retraining:  \n   - **Single-GPU scenarios:**  \n     - On *HunyuanVideo* (129 frames, 544 P), BWCache achieves 2.6× speed-up (latency 1122 s → 433 s), with VBench 82.48% (higher than baseline 82.29%), LPIPS 0.0794 (better than PAB 0.1045 and TeaCache 0.1630), and PSNR 29.91.  \n     - On *Open-Sora*, at 1.61× acceleration, LPIPS 0.0879 (close to the original), SSIM 0.8854, and PSNR 27.05 — all significantly outperforming baselines like Δ-DiT and T-GATE.  \n   - **Multi-GPU scaling:**  \n     With *Dynamic Sequence Parallelism (DSP)*, BWCache reaches 17.2× acceleration on 8 A800 GPUs for Open-Sora (204 frames, 480 P) (latency 11.1 s vs 190.2 s original), with near-linear scaling, enabling large-scale deployment.  \n   - **High-resolution / long-video adaptability:**  \n     On *Wan 2.1* (720 P), BWCache delivers 2× speed-up (latency 912 s → 457 s) with VBench 81.99% (only 0.18% lower than original), while TeaCache at 1.41× acceleration shows more noticeable quality loss."}, "weaknesses": {"value": "Despite its impressive performance, BWCache still has room for improvement in **memory overhead** and **adaptability to dynamic scenes**.\n\n1. **Module Caching Increases Memory Consumption — Limited for Single-GPU High-Resolution Scenarios**  \n   BWCache caches the *output features of each DiT module*, leading to noticeable GPU memory growth, especially for large models and high-resolution videos:  \n   - Appendix D.4 reports that on Wan 2.1 (81 frames, 720 P), BWCache uses 80 607 MiB vs 73 955 MiB for the baseline (+9%), and on HunyuanVideo (129 frames, 720 P) +8%.  \n   - To reduce memory pressure, the authors compute caching metrics on “50% module sampling”, which lowers memory cost but slightly reduces redundancy detection accuracy (e.g., 10% sampling yields LPIPS 0.7901 vs 0.0782 for 50% sampling).  \n   This limits BWCache’s applicability for single-GPU, memory-constrained environments (e.g., edge devices or high-resolution video generation).\n\n2. **Insufficient Redundancy Detection Accuracy in Highly Dynamic Scenes**  \n   The “U-shaped feature variation” assumption holds well for static scenes (e.g., “beach sunset”) but weakens in highly dynamic cases (e.g., “car chase,” “crowded street”):  \n   - In dynamic scenes (Prompt 2: “bus approaching traffic light”), module-level differences remain larger, with mid-stage redundant steps 30–40% fewer than in static scenes, leading to lower acceleration (e.g., on Open-Sora-Plan, TeaCache achieves 4.41× while BWCache only 2.24×, Table 1).  \n   - No dynamic-aware adaptation is proposed: the method lacks an *adaptive threshold $\\delta$* based on scene dynamics (e.g., using smaller δ for fast-changing content), resulting in suboptimal speed-quality trade-offs in dynamic video generation."}, "questions": {"value": "1. Are spatial and temporal DiT blocks cached jointly or independently, and did you observe temporal drift in high-motion clips\n2. Can you provide results on highly dynamic or compositional scenes (human motion, fast parallax) to test stability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u7oAKN8VZx", "forum": "5bJZtzTFYy", "replyto": "5bJZtzTFYy", "signatures": ["ICLR.cc/2026/Conference/Submission10882/Reviewer_RYjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10882/Reviewer_RYjh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903965085, "cdate": 1761903965085, "tmdate": 1762922092719, "mdate": 1762922092719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BWCache, a training-free inference acceleration method for DiT-based video generation. The work is motivated by an empirical analysis showing that DiT block computations are the main latency bottleneck and exhibit a U-shaped variation pattern, implying high redundancy in intermediate steps. BWCache exploits this by dynamically caching and reusing features at the block level. A similarity indicator (average relative L1 distance) determines when to reuse features, while safeguards like periodic recomputation and forced computation in the final steps ensure high visual fidelity. The method is validated across five state-of-the-art models, demonstrating significant speedups while consistently outperforming other acceleration baselines in preserving video quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is built on a clear, insightful, and well-visualized analysis (Figure 4). The identification of the U-shaped variation pattern in DiT blocks is a strong finding that provides a direct and logical justification for the entire method's design.\n2. The method is more sophisticated than several competitors. It correctly identifies a suitable *granularity* (DiT blocks) and, more importantly, implements a dynamic *trigger* (the similarity indicator) for caching. This, combined with the fidelity-preserving strategies (final-step recomputation), allows it to effectively balance acceleration and quality.\n3. The experimental validation is thorough. Testing on five different, large-scale video DiTs and comparing against multiple relevant baselines demonstrates the method's general applicability. The scalability tests (multi-GPU, video length) and the direct ablation against naive step reduction (Table 3) make the performance claims very convincing."}, "weaknesses": {"value": "1. Notably, the trade-off of speed for memory is a weakness. By caching DiT block features, the method's GPU memory footprint increases, as acknowledged in the appendix (Appendix D.4). For very large models that already push the limits of high-end GPUs (e.g., Wan 2.1 using ~74GB), this additional overhead (to ~81GB) could make the method unusable on memory-constrained hardware.\n2. The method's effectiveness depends on manually tuning two key hyperparameters: the similarity threshold $\\delta$ and the recomputation interval $R$. The ablation studies show that these values control a sensitive trade-off between speed and quality. While the authors provide default values, these may not be optimal for all models or use cases, and the paper does not offer a strategy for setting them automatically.\n3. A critical implementation detail is hidden in the appendix (D.4). For memory-heavy models (Wan 2.1, HunyuanVideo), the similarity indicator is not calculated using *all* blocks; instead, a *subset* (e.g., 50%) is used to save memory. The main paper does not mention this, and the appendix does not specify *how* this subset is chosen (e.g., first 50%, random, etc.), making the indicator's true behavior and robustness on these models unclear.\n4. The fidelity safeguards are static: always recompute the *last $k/2$ steps* and reuse for a *fixed interval $R$*. The \"U-shape\" analysis suggests that the transition from stable to variable features (the right side of the \"U\") is gradual. A more dynamic approach, perhaps by monitoring the L1 distance to decide when to *stop* caching (rather than a fixed $k/2$ rule), might offer an even better trade-off."}, "questions": {"value": "What is the underlying theoretical explanation for the observed U-shaped pattern in DiT block feature variations? Is this pattern consistent across different architectures and training methodologies, and how does it relate to the diffusion process dynamics? I would appreciate it if the authors could discuss this with previous related works (e.g., [1])\n\n[1] Ma, Xuran, et al. \"Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pbTkJE4CFL", "forum": "5bJZtzTFYy", "replyto": "5bJZtzTFYy", "signatures": ["ICLR.cc/2026/Conference/Submission10882/Reviewer_41nC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10882/Reviewer_41nC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975195549, "cdate": 1761975195549, "tmdate": 1762922092222, "mdate": 1762922092222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BWCache, a training-free acceleration technique for Diffusion Transformer video generation. It starts with the observation that DiT blocks dominate latency and many intermediate timesteps present redundant computation. BWCache caches block outputs across timesteps and reuses them based on a similarity indicator. It also incorporates periodic recomputation to prevent latent drift. Experiments across multiple backbone models show up to 2.6× speedup while maintaining competitive quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- High Practicality and Scalability. As a training-free method, it can be integrated into existing DiT pipelines without costly fine-tuning. \n- The BWCache design is well-motivated by a clear empirical analysis of DiT block behavior. The Aggregated Relative L1 analysis in Figure 4 provides a strong justification for the caching strategy.\n- The method achieves a superior balance between inference speed and visual quality. It delivers speedups while maintaining comparable or even superior visual fidelity."}, "weaknesses": {"value": "- The paper's claim to be the first method to exploit block-wise temporal stability in DiTs appears to be overstated. There is some highly relevant prior work which also investigates temporal redundancy in DiTs but not discussed in the paper. The authors must situate their work relative to it, differentiate their contribution, and (if possible) provide a comparative analysis.\n1. Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints\n\n- In Table 1, TeaCache achieves a 4.41x speedup on Open-Sora-Plan. The authors attribute it to \"significant variation in block features during the early inference stages,\" but provide no data or analysis to support this claim. Why does the block-wise indicator in Eq. 7 fail in this specific scenario while TeaCache's simpler timestep-level caching succeeds? \n-  The entire caching mechanism depends on a manually-set similarity threshold in Eq. 7, which appears empirical. The paper provides no theoretical grounding for this choice, nor does it analyze its stability or the risk of error accumulation from repeated cache reuse. The ablation in Table 4 shows a predictable quality/speed trade-off but fails to explain how a user should set $\\delta$ for a new model or task.\n- The computational cost of the cache indicator is not considered. BWCache requires calculating the L1 distance for all blocks at every step to decide whether to cache. This overhead is never measured. While this cost may be negligible on large models, it could substantially diminish on smaller models, in edge-computing scenarios, or on hardware with lower memory bandwidth."}, "questions": {"value": "The paper's validation is limited exclusively to text-to-video generation. Have you explore the core \"U-shaped\" ARL1 pattern holds in other settings, like class-conditional generation, image generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M89iKTnKmm", "forum": "5bJZtzTFYy", "replyto": "5bJZtzTFYy", "signatures": ["ICLR.cc/2026/Conference/Submission10882/Reviewer_CKpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10882/Reviewer_CKpW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762277557869, "cdate": 1762277557869, "tmdate": 1762922091775, "mdate": 1762922091775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}