{"id": "qK43vBvOZ7", "number": 9017, "cdate": 1758107369837, "mdate": 1759897747930, "content": {"title": "When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs", "abstract": "Layer pruning has emerged as a widely adopted technique for improving the efficiency of large language models (LLMs). Although existing methods demonstrate strong performance retention on general knowledge tasks, their effect on long-chain reasoning, a more brittle yet crucial capability, remains largely unexplored.  In this work, we study the impact of layer pruning on long-chain reasoning through the lens of test-time scaling, a key mechanism in modern LLMs that enables strong reasoning capacity by allocating more computation at inference time. With extensive experiments, we demonstrate that pruning even one or two layers can severely impair test-time scaling, with performance collapsing drastically on long reasoning benchmarks even when performance on knowledge-intensive and shallow reasoning tasks remains stable. Furthermore, we find that standard supervised fine-tuning remedies fail to recover lost test-time scaling once it has deteriorated. Through in-depth analyses, we identify the mechanisms underlying this fragility of test-time scaling and highlight the fundamental risks of applying layer pruning to reasoning-intensive LLMs. These findings call for a rethinking of layer pruning strategies and provide insights for developing methods that preserve the robustness of reasoning.", "tldr": "We systematically study the effect of layer pruning on test-time scaling, unveiling its negative influence on test-time scaling, which is hard to be repaired by standard SFT.", "keywords": ["LLMs", "layer pruning", "test-time scaling", "model compression", "long-context reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac1e059ded931c17efcfabfa15fd1376c64cb7a7.pdf", "supplementary_material": "/attachment/a2d298c2dc362dbf8ab4f6807b014de895cf2a39.zip"}, "replies": [{"content": {"summary": {"value": "The paper experimentally demonstrates that Layer Pruning, a mainstream technique for improving model efficiency, causes LLMs' performance to collapse on complex reasoning tasks reliant on long sequential Chains-of-Thought. Crucially, standard supervised fine-tuning (such as LoRA or full-parameter fine-tuning) cannot effectively recover this lost test-time scaling capability. Through mechanistic analysis, the authors attribute the performance degradation to structural damage in the model, resulting in an increase in redundant loops, a decrease in reasoning trajectory diversity, and a weakening of self-reflection capability within the reasoning paths. The work calls for future efforts to explore hybrid strategies that balance efficiency and robustness, ensuring that pruning preserves both performance and reasoning depth."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates a degree of originality by proposing a novel and critical problem formulationâ€”investigating the impact of pruning on \"Test-Time Scaling\" capability. This is a question that integrates efficiency research with reasoning research, establishing a necessary new evaluation criterion for model compression.\n\n2. The authors provide both quantitative and qualitative analyses to explain the cause of the performance collapse, with ample experimentation and clear, compelling evidence to support their findings.\n\n\n3. The paper proves that standard recovery techniques, such as LoRA and full-parameter fine-tuning, are largely ineffective in recovering the lost test-time scaling capability caused by pruning. These results are significant for both academic research and practical engineering."}, "weaknesses": {"value": "1. The paper only validates training-free pruning techniques and does not incorporate mainstream training-based pruning methods. This makes it impossible to verify whether such methods can alleviate the fragility of test-time scaling. While the paper makes certain contributions, the generalizability of its conclusions is limited.\n\n2. The paper explicitly states that it studies both sequential and parallel test-time scaling. However, most of the content and core analysis focus on sequential scaling. Regarding the performance collapse mechanism of parallel scaling methods, the paper's analysis is relatively weak and fails to provide in-depth mechanistic insights like those for sequential scaling.\n\n3. The s1K-1.1 dataset used for fine-tuning is not introduced, and only this single dataset is employed in the fine-tuning experiments. Consequently, the conclusion that 'fine-tuning has limited effect' is questionable."}, "questions": {"value": "1. As mentioned in the Weaknesses, if more complex methods such as training-based pruning methods are used, can the damage to test-time scaling ability be effectively alleviated?\n\n2. Since the effect of standard fine-tuning is limited, does there exist or is it considered to design a customized fine-tuning scheme targeting reasoning trajectory loss? Can it repair the structural damage caused by pruning from a mechanistic perspective?\n\n3. It is suggested that the authors supplement some comparison charts of the model evaluation indicators before and after fine-tuning in Section 4, so as to intuitively demonstrate the limitations of supervised fine-tuning methods in restoring the reasoning ability of models after layer pruning.\n\n4. Is there a mistake in the introduction of ShortGPT in Appendix B? As far as I know, a lower Block Influence (BI) score indicates a higher cosine similarity between two layers, which means that the layer has minimal transformation on the hidden state and low importance, so it can be pruned with limited performance loss [1]. If so, please correct it; if not, please ignore this comment.\n\n[1] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. ACL Findings, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s2XIWsHbJu", "forum": "qK43vBvOZ7", "replyto": "qK43vBvOZ7", "signatures": ["ICLR.cc/2026/Conference/Submission9017/Reviewer_NA8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9017/Reviewer_NA8t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735152139, "cdate": 1761735152139, "tmdate": 1762920741577, "mdate": 1762920741577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of layer pruning on the test-time scaling capability of Large Language Models for long-chain reasoning tasks. Through experiments, the authors demonstrate that pruning even 1-2 layers severely impairs sequential test-time scaling, despite stability on knowledge-intensive tasks. Parallel scaling is also harmed by direct pruning methods but preserved by merging-based pruning. Additionally, supervised fine-tuning fails to recover the degraded test-time scaling. These findings provide fresh insights into building lightweight reasoning models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The multiple conclusions identified offer clear reference value for lightweighting reasoning LLMs and for on-device deployment.  \n2) The experiment covers a diverse spectrum of lightweighting techniques.\n3) The work also supplies explicit qualitative and quantitative case analyses for the discovered phenomena."}, "weaknesses": {"value": "1) Experiments have only been conducted on models with fewer than 10B parameters; results would be more convincing if larger-scale models were also included.  \n2) When exploring supervised fine-tuning as a recovery remedy, incorporating the dominant RL recipes used in current reasoning-model training would further complete the findings of this work."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Thw0xwzqh9", "forum": "qK43vBvOZ7", "replyto": "qK43vBvOZ7", "signatures": ["ICLR.cc/2026/Conference/Submission9017/Reviewer_kqHT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9017/Reviewer_kqHT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909005872, "cdate": 1761909005872, "tmdate": 1762920740782, "mdate": 1762920740782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows a finding that layer pruning methods that work well for non-reasoning models might not work as effectively for reasoning models. Also, the authors show that SFT training is not sufficient to recover the performance after pruning. The authors further try SFT training but this also does not recover the performance drop occuring from layer pruning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The observation that layer pruning does not work effectively for reasoning models (which has become the de-facto model we adopt in the community for experiments) is timely and important."}, "weaknesses": {"value": "1. This paper presents negative results but the explanations or experimental setting to analyze the negative results are limited. For example, the observation that \"most layers play a non-trivial role in enabling test-time scaling\" is very interesting, but the underlying explanation for whether that is not the case for \"non-reasoning models\" or what is the reason behind that is very limited.\n\n2. As a follow-up of 1, I think there should be trends of non-reasoning models on the same experimental setting for Figure 2,3,4. A very simple way to do this would be to turn off the reasoning mode on Qwen3-8B and check if the trends differ after applying the pruning methods or Qwen2.5-7B-Instruct (which is the base model for s1.1).\n\n3. For the qualitative example in Section 5.1 (Figure 5), could setting a higher temperature or applying repetition penalty mitigate this issue? Related to 1, there is insufficient explanation of why this repetition is happening to reasoning models versus non-reasoning models and what is the mechanistical or other reason behind this."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8MVSwbEq4h", "forum": "qK43vBvOZ7", "replyto": "qK43vBvOZ7", "signatures": ["ICLR.cc/2026/Conference/Submission9017/Reviewer_W7Vc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9017/Reviewer_W7Vc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976368804, "cdate": 1761976368804, "tmdate": 1762920740259, "mdate": 1762920740259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates on the effect of layer pruning in LLM long chain reasoning, showing that pruning even 1 or 2 layers significantly impair the performance in test-time scaling. The experiments cover different models, datasets, pruning methods and evaluation metrics, and the results are basically consistent. Furthermore, the authors show that SFT after pruning cannot recover the original performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The experiments contain sufficient ablations, covering different models, datasets, pruning methods and evaluation metrics. The overall conclusions are consistent and can well support the main claim that layer pruning hurts the long chain reasoning performance."}, "weaknesses": {"value": "The main conclusion of the paper is simple and rather intuitive. The layer pruning method would surely decrease the performance since the model loses part of its parameters and face OOD problems compared with training. One could naturally expect these results without experiments, and the conclusions are known. The paper does not provide new interesting results, nor the solution to address the problem. \n\nMoreover, layer pruning itself is not a practically meaningful method from my perspective. Large scale pretraining / post-training aims to improve the reasoning performance, while minimum pruning would severely hurt the performance, which deviates from the original target. Why would people need layer pruning anyways? It is not a principled way in any sense. Even in terms of efficiency, pruning 1 or 2 layers would only bring marginal acceleration, while other methods such as distillation or quantization would significantly improve the efficiency without sacrificing much performance. Unfortunately, the main result of the paper lies in the natural consequence of layer pruning, which is deemed to hurt the performance, while the authors fail to provide any theoretical results, nor any successful methodologies to avoid the performance drop."}, "questions": {"value": "* Since the performance degrade is natural, can you theoretically characterize the phenomenon? Note that contents in Section 5 are mostly case studies and heuristics, not theoretically grounded.\n* Can you provide any practical methods to mitigate the issue? Did you try out other SFT configurations? The current setting seems not convincing (only SFT on s1K seems insufficient).\n* Did you try out more models? Do you have intuitions on the slight difference in performance under various settings?\n* Can you provide any convincing reasons why layer pruning is worth studying? Since all methods hurt the performance, what would be the next step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HyDeptCVch", "forum": "qK43vBvOZ7", "replyto": "qK43vBvOZ7", "signatures": ["ICLR.cc/2026/Conference/Submission9017/Reviewer_PSGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9017/Reviewer_PSGU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983917715, "cdate": 1761983917715, "tmdate": 1762920739639, "mdate": 1762920739639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}