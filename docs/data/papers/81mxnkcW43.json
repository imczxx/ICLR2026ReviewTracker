{"id": "81mxnkcW43", "number": 12966, "cdate": 1758212226082, "mdate": 1763718038030, "content": {"title": "SPARD: Defending Harmful Fine-Tuning Attack via Safety Projection with Relevance–Diversity Data Selection", "abstract": "Fine-tuning large language models often undermines their safety alignment, a problem further amplified by harmful fine-tuning attacks in which adversarial data removes safeguards and induces unsafe behaviors. \nWe propose **SPARD**, a defense framework that integrates **S**afety-**P**rojected **A**lternating optimization with **R**elevance-**D**iversity aware data selection. \nSPARD employs SPAG, which optimizes alternatively between utility updates and explicit safety projections with a set of safe data to enforce safety constraints.\nTo curate safe data, we introduce a Relevance–Diversity Determinantal Point Process to select compact safe data, balancing task relevance and safety coverage. \nExperiments on GSM8K and OpenBookQA under four harmful fine-tuning attacks demonstrate that SPARD consistently achieves the lowest average attack success rates, substantially outperforming state-of-the-art defense methods, while maintaining high task accuracy.", "tldr": "", "keywords": ["Large Language Models; Harmful Finetuning Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6cc723b64fd9d790c37ba479210c5f0106cd4c0e.pdf", "supplementary_material": "/attachment/15b893951a76e4efac445fce32913a09157c72c4.zip"}, "replies": [{"content": {"summary": {"value": "A safety-projected Alternating optimization with relevance-diversity aware data selection scheme is proposed to address harmful fine-tuning attacks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is extremely well written. I can understand the solution very easily. \n\n2. Two components, i.e., safety data selection and alternating safety projection is proposed. \n\n3. Compared to a similar work, Lisa, the optimization problem is transferred from a multi-task loss to a constraint problem, and this problem is solved by a projected gradient method. This transformation is crucial and might inspire newer ideas on design defense.  To my best knowledge, this is the first (among a few concurrent work) to explore such projection methods. It is a solid work in my understanding. \n\n4. The projection problem in Eq. (3) and the resulted projection step in (4) is elegant and makes perfect sense to me."}, "weaknesses": {"value": "1. More discussion on penalty and constraint-based problem formulation.\n\n* Line 3, Page 3, I think Lisa [1] and SafeGrad [2] both explore the penalty problem formulation. I suggest the authors to add them into the citation with (Bianchi et al., 2023). \n\n*  I can't agree the statement \"Unlike penalty-based approaches, which require careful tuning of λ and offer no guarantee of feasibility, SPAG yields a closed-form projection step that enforces the constraint up to the first order.\"  The constraint problem and the penalty problem are sort of identical in some sense. For the constraint problem, you also need to carefully tune $\\tau$, which is identical to tune $\\lambda$ in the penalty problem. Please consider to remove such an unjustified statement, which is biased towards penalty methods. I tend to believe that the two methods are just two alternative way to express the same fundamental problem (trade-off between two losses), but we should not be biased towards one alternative only by simply looking at problem formulation. But I do feel that studying the constraint-based alternative is important as it gives alternative ways to design new methods and potentially these new methods can give better empirical performance.   \n\n[1] Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack\n\n2. A concurrent work should be discussed.  Stemming from the gradient conflict for the penalty problem, SafeGrad[2] derives a very similar projection method. However, because the two update rule are derived from different problems, it appears to me the fundamental projection rule seems to be not identical. I suggest the authors to:\n\n* Discuss the similarity and difference between SafeGrad and SPAG. \n* Do an experiment to compare SafeGrad and SPAG empirically. I think this is interesting because the two projection rule are derived based on different problems (penalty/constraint) and let's see which method works better empirically.  \n\n[2] SafeGrad: Gradient Surgery for Safe LLM Fine-Tuning\n\n3. The finding of \"Relevance between safety data and fine-tuning data Improves Safety\" has already been covered by several existing literature but the authors did not cite and discuss them. I urge the authors to properly credit the following works, otherwise it will vitiate our research community.\n\n* This finding is first concurrently covered by [3][4] in two ICLR2025 submissions.  Then it is also covered by [5].  Particularly, a highly relevant work [3] also discusses similarity and diversity metric. They use a similar similarity and diversity metric to measure the  a  subset of dataset.  However, this paper lacks proper credit to [3] given the relevance of these two papers. I strongly suggest the authors to properly credit [3]. Otherwise I can't recommend acceptance of this paper.   Also, could you discuss whether you have some novelty contribution over [3]? If you have, could you perform experiments to show that how your method compare against [3]? \n\n* With that said, it seems that  the finding \"Notably, the curve also shows that ASR rises again (to 16.6%) when the selected samples are too similar to the fine-tuning data \" is new to me. \n\n* A recent work [6] explores the safety sample curation problem. They explore an optimization-based solution for curate safety data. The solutions are not in the same direction with the  cosine similarity criterion explored in [3][4]. I suggest the authors compare with [6] to see which safety data curation method is better.   \n\n[3] Your Task May Vary: A Systematic Understanding of Alignment and Safety Degradation when Fine-tuning LLMs  (ICLR2025 submission)\n\n[4] Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models (ICLR2025 submission)\n\n[5]  When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment\n\n[6] Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning\n\n4. In addition to the above highly relevant papers, there are many more papers on harmful fine-tuning that are not discussed in this paper:\n\nScaling Trends for Data Poisoning in LLMs\n\nUnleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models\n\nVirus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation \n\nNo, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data\n\nBenign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety \n\nYour Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents \n\nEliciting Harmful Capabilities by Fine-Tuning on Safeguarded Outputs\n\nDeep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs\n\nVaccine: Perturbation-aware alignment for large language model aginst harmful fine-tuning\n\nTamper-Resistant Safeguards for Open-Weight LLMs\n\nBooster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation\n\nTargeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation \n\nSelf-Destructive Language Model\n\nCTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning\n\nVulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning\n\nLoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning\n\nTowards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning\n\nAntibody: Strengthening Defense Against Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Gradient Influence\n\nSEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection\n\nSafety alignment should be made more than just a few tokens deep\n\nSaLoRA: Safety-Alignment Preserved Low-Rank Adaptation\n\nBeware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs \n\nShape it Up! Restoring LLM Safety during Finetuning \n\nMitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization \n\nRefusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation \n\nAsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin\n\nDefending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment\n\nGradShield: Alignment Preserving Finetuning\n\nA Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space \n\nDetecting Instruction Fine-tuning Attack on Language Models with Influence Function\n\nAntidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning \n\nLocking Down the Finetuned LLMs Safety \n\nPanacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation\n\nSafe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets \n\nNavigating the safety landscape: Measuring risks in finetuning large language models\n\nESTIMATING WORST-CASE FRONTIER RISKS OF OPEN-WEIGHT LLMS\n\nDetecting Adversarial Fine-tuning with Auditing Agents\n\nFundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models\n\nWhen Style Breaks Safety: Defending Language Models Against Superficial Style Alignment \n\nThere may be more relevant works (I just list above some more recent work), and I suggest the authors to read and discuss all of the relevant works when writing the paper."}, "questions": {"value": "1. When limiting the projection step size $\\alpha$ with  (Schulman et al., 2015), will the projection still make sure that constraint in (3) strictly holds?  Did you have some results for ablation when we does not limit the step size $\\alpha$ with $\\eta_{safe}$?\n\nPlease address the concerns and feel free to leave me a comment in the rebuttal phase. I enjoy reading this paper overall, although I have serious concern on your Section 3.2, which do not credit properly on [3]. I will consider adjusting my score based on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OhasGpXkq6", "forum": "81mxnkcW43", "replyto": "81mxnkcW43", "signatures": ["ICLR.cc/2026/Conference/Submission12966/Reviewer_41kS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12966/Reviewer_41kS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761253521580, "cdate": 1761253521580, "tmdate": 1762923721928, "mdate": 1762923721928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework named SPARD to defend LLMs against harmful fine-tuning attacks. The proposed solution consists of two key components: (1) Safety-Projected Alternating Gradient (SPAG). This is a principled optimization strategy. Instead of using safety data as a soft penalty (which is common in other methods), SPAG formulates the problem as a safety-constrained optimization. It alternates between a standard utility update on the fine-tuning data and an explicit, closed-form projection step that pushes the model parameters back into a safe region defined by a curated set of safe data. (2) Relevance-Diversity Data Selection: the authors show that safety data must be both relevant to the downstream task and diverse. To achieve this balance, they propose a Relevance-Diversity Determinantal Point Process (DPP) to incorporate both a task-relevance quality score and an intrinsic diversity measure. Experiments conducted on two LLMs (Qwen-2.5-7B and LLaMA-3.2-3B) , two downstream tasks (GSM8K and OpenBookQA) , and four different harmful attack datasets demonstrate that SPARD consistently achieves the lowest ASR and HS. Notably, it does this while maintaining high task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The combination of SPAG and Relevance–Diversity DPP is a novel and principled solution to address harmful fine-tuning attacks. \n- SPARD consistently outperforms baseline methods across multiple datasets and model architectures.\n- The paper provides extensive experiments, sensitivity analyses, and comparisons with baselines, demonstrating the effectiveness and generalizability of SPARD."}, "weaknesses": {"value": "- [Minor] Details on generating the embeddings are not directly available until Section 4.1, making people slightly confused at the beginning. \n- Would the cost of applying DPP increase with the dataset size as it seems to compute pairwise similarity? how would it scale to larger datasets if that's the case? \n- Computational cost: if the threshold in Algorithm 1 is passed, then a second backward is required. This will impose greater computational cost and longer running time. It would be great if the authors can provide statistics on how frequent this will trigger. \n- The method's success appears to be highly dependent on several new hyperparameters that must be carefully tuned, but the paper provides limited guidance on how to set them universally."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kH6BgrqVEF", "forum": "81mxnkcW43", "replyto": "81mxnkcW43", "signatures": ["ICLR.cc/2026/Conference/Submission12966/Reviewer_pm8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12966/Reviewer_pm8L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703157752, "cdate": 1761703157752, "tmdate": 1762923721508, "mdate": 1762923721508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SPARD, a safety projection approach based on relevance-based diversity-aware data selection. The  relevance-based diversity-aware data selection is directly achieved by using the existing DDPs approach (Determinantal Point Processes) which ranks the top subset that contains both individually informative and mutually diverse data items/elements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of utilizing the existing DDPs approach to provide the relevance-based diversity-aware data selection is simple and interesting. \n\nThe safety projected alternating gradient (SPAG) is an extension of Bianchi et al 2023's approach, aiming to improve the tuning efforts of setting the average safety loss parameter and the penalty parameter. \n\nThe idea of extension is to perform fine-tuning on the fine-tuning dataset first (i.e., utility driven update) and then perform projection to encode the updated parameters into the half-space defined by C^+, which satisfies safety constraints. \n\nThe paper provides experimental comparison using two LLMs: Qwen-2.5-7B-instruct and LLaMA-3.2-3B-instruct and compared to three existing safety guardrail methods in addition to the SFT baseline."}, "weaknesses": {"value": "The paper can benefit from providing clear elaboration on the following aspects.\n\n(1) Although the idea of utilizing the existing DDPs approach to provide the relevance-based diversity-aware data selection is simple and interesting, given that the quality of selected subsets by DPPs is based on the balancing of the two criteria: individually informative and mutually diverse, the paper should provide discussion to elaborate on how these two criteria are semantically measured since informative is context relevant and mutually diverse is also context driven (e.g., agreement diversity or disagreement diversity ... ) and why DDPs based  data selection will help mitigating harmful fine-tuning.\n\n(2) The proposed approach relies on task-relevant safe samples to provide safety constraints. It is a very strong assumption. A discussion on how the proposed approach responds when the task relevant safe samples are of varying quality and volume. \n\n(3) The experimental results could benefit from more detailed discussion to elaborate on the boundary cases. For example, Table 1 has shown that there are two out of four datasets the proposed approach did not outperform existing safety guardrail methods, like LISA. The intuitions behind the proposed approach vs LISA (Huang et. al. 2024b) and vs. SafeInstr (Bianchi et.al 2023) should be provided. \n\n(4) Similar questions also apply to Table 2, Table 3 and Table 4. \n\n(5) Can you use Figure 6 style of comparison on GSM8K to show those cases where SPARD performance is weaker compared to existing methods and analyze why."}, "questions": {"value": "See the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FE85xDoP6I", "forum": "81mxnkcW43", "replyto": "81mxnkcW43", "signatures": ["ICLR.cc/2026/Conference/Submission12966/Reviewer_BprZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12966/Reviewer_BprZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921267467, "cdate": 1761921267467, "tmdate": 1762923721075, "mdate": 1762923721075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPARD, a defense framework against harmful fine-tuning attacks on aligned large language models (LLMs). The framework combines two complementary ideas:\n(1) Safety-Projected Alternating Gradient (SPAG) — an optimization procedure that alternates between utility-driven fine-tuning steps and explicit safety projection steps to enforce safety constraints in closed form;\n(2) Relevance–Diversity Determinantal Point Process (DPP) — a principled method to select a compact subset of “safe” data that balances task relevance and behavioral diversity.\n\nExperiments on GSM8K and OpenBookQA under multiple harmful fine-tuning settings (BeaverTails, LatHarmful, etc.) demonstrate that SPARD significantly reduces attack success rate (ASR) while maintaining downstream utility. The method consistently outperforms strong baselines such as SafeInstr, SafeLoRA, and Lisa."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The combination of task relevance and diversity within a determinantal point process framework is well-motivated. It improves upon prior ad-hoc or random selection strategies, and empirical analysis (Figure 2) convincingly shows that both factors matter for robust defense.\n2. The mathematical formulation of SPAG (Eq. 2–4) and its derivation (Appendix A) are precise and well-presented. The trust-region stabilization strategy further enhances practical robustness."}, "weaknesses": {"value": "- The proposed Safety-Projected Alternating Gradient (SPAG) method is presented as a novel optimization framework. However, its core mechanism is well-established in classical constrained optimization literature (e.g., projected gradient descent, proximal updates, or trust-region projection). While the adaptation to LLM safety alignment is interesting and practically meaningful, the methodological novelty of SPAG itself appears limited. The paper would benefit from a clearer distinction between conceptual novelty (application to harmful fine-tuning) and methodological novelty (new optimization formulation).\n- Although the DPP-based selection is implemented with a greedy approximation, the paper does not provide a clear discussion or analysis of its computational efficiency."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SrvBb0N9SQ", "forum": "81mxnkcW43", "replyto": "81mxnkcW43", "signatures": ["ICLR.cc/2026/Conference/Submission12966/Reviewer_Jg8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12966/Reviewer_Jg8Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762266031237, "cdate": 1762266031237, "tmdate": 1762923720577, "mdate": 1762923720577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}