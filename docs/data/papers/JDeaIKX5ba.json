{"id": "JDeaIKX5ba", "number": 16422, "cdate": 1758264396416, "mdate": 1759897241568, "content": {"title": "Axiomatization of Concept CNN Explanations", "abstract": "Concept-based explanations for convolutional neural networks (CNNs) offer human-interpretable insights into the decision-making processes of artificial intelligence (AI) models. In contrast to attribution-based methods, which primarily highlight salient pixels, concept-based approaches capture higher-level semantic features, thereby elucidating not only where the model looked but also what it saw. Despite their promise, the absence of rigorous axiomatic foundations has impeded systematic evaluation, comparison, and compliance, limiting their broader adoption. This paper presents a conceptual axiomatic framework, derived from the principles of explanation logic, for evaluating the faithfulness of concept-based explanations in CNN-driven image classification. We propose a novel set of axioms that formalize essential criteria for trustworthy explanations and establish a quantitative methodology for their evaluation. Extensive experiments conducted in both ideal and adversarial settings, across diverse model architectures, demonstrate the necessity and validity of these axioms. Our findings contribute to the development of reliable, interpretable, and trustworthy explainable artificial intelligence (XAI) frameworks, with particular relevance to high-stakes domains where transparent decision-making is crucial.", "tldr": "The paper proposes a set of rules for evaluating concept-based CNNs guided by prototypes.", "keywords": ["Concept", "Explainability", "CNN", "Trustworthiness", "Axiom"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/feb7d01e832bf171d13c70584bcf2e818f6bb27f.pdf", "supplementary_material": "/attachment/4c3ff32947f4428819d5365b3bc6c07aecda8400.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a set of axiomatic foundations for evaluating and comparing concept-based explanations. Among these foundations, the authors include interpretability, relevance, coherence, fidelity, and sanity. They evaluate two concept explainers: CRAFT and ICE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper attempts to **formalize key concepts** involved in evaluating concept-based explainers, which is highly relevant and valuable to the field.\n- The authors present their propositions through **multiple complementary approaches**, including **diagrams** (especially Figure 3) and **mathematical formulations**, which improve understanding."}, "weaknesses": {"value": "- **Alignment between concepts and human beliefs:** As you mentioned, humans expect explanations to align with their intuition; however, how can we be sure that this alignment actually occurs? An explanation might differ from human expectations while still being faithful to the model. Moreover, we should be cautious not to **reinforce human biases** through explanations.\n- **Existant lack of quantitative evaluation:** You say there is a lack of quantitative evaluation. However, other existing studies provide such evaluations, for example, deletion and insertion [1], SIC and AIC [2]. Can you discuss why these metrics are not suitable?\n- **Order of presentation:** It is somewhat confusing to see the proposed explanation (e.g., Figure 3) before the problem statement. Presenting the problem first would help readers follow the logical flow more easily.\n- **Novelty of Section 2.2:** In Section 2.2, you present formal principles and conceptual bases. However, these ideas are not entirely new—other works have discussed similar principles. Could you cite relevant prior works mentioning each of these principles?\n- **Experiments:** \n    - In the experiments, the two evaluated methods exhibit very similar behavior according to your axioms you propose. Does this indicate that both methods perform well, or that the metric is too **permissive**?\n    - Could you include an example of a **poor concept explainer** according to your metric?\n    - When performing **adversarial attacks**, do the results reflect non-robustness of the **model** or of the **explanations**?\n    - How are the explainers comparable if they are applied to **different models** (you seem to use a different model for each explainer)?\n    - What would be the impact of **changing the model**, or of **explaining a biased model**?\n\n[1] Covert, I., Lundberg, S., & Lee, S. I. (2021). Explaining by  removing: A unified framework for model explanation. Journal of Machine  Learning Research, 22(209), 1-90.\n\n[2] Kapishnikov A., Bolukbasi T., Vi'egas F., Terry M. XRAI: Better Attributions Through Regions. ICCV. 2019."}, "questions": {"value": "- There is a **typo in row 040**.\n- I suggest including all explanations **within the main text** rather than in **footnotes**. Using footnotes reduces readability, and integrating the explanations directly may also help you use page space more efficiently.\n- I included some questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KHNOwqBNr6", "forum": "JDeaIKX5ba", "replyto": "JDeaIKX5ba", "signatures": ["ICLR.cc/2026/Conference/Submission16422/Reviewer_LnWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16422/Reviewer_LnWZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900638234, "cdate": 1761900638234, "tmdate": 1762926542649, "mdate": 1762926542649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an axiomatic framework for quantitative evaluation of concept-based explanations. The authors motivate and list a rich list of axioms that concept-based explanations ought to satisfy, like Faithfulness and Causality. Nonetheless, the crucial aspect of how these axioms are operationalized remains unclear to me."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of the paper is solid: most works in XAI are qualitative, lacking a principled, quantitative, and reproducible set of evaluation criteria. Also, the approach adopted by the authors is rather formal, which is generally a pro when formalizing evaluation criteria. Nonetheless, I'm left with several major concerns outlined below."}, "weaknesses": {"value": "- **W1:** Clarity issues in the mathematical notation:\n\n   - The notation of the concept-based explainer $\\hat{f}$ should be improved. Why the use of the set notation instead of function notation? How are its encoder and decoder defined?\n\n   - What does $\\\\{ C(A), C(A') \\\\}_i$ represent? Why is the set indexed by $i$ since it never appears inside the set?\n\n   - What does $\\mathcal{G}^n$ represent? It is never explicitly defined.\n\n   - How is $\\mathcal{H}$ defined? Authors say it is *the space of human-recognizable concepts*, but this comes with several issues. First, humans do not share a universal set of recognizable concepts (e.g., domain experts vs laypeople). Second, how is this mapping defined, i.e., how does the output of $\\iota(s)$ look like? Third, the study acknowledges that it does not use any human study, leaving the question of how this space of human-recognizable concepts is defined/validated/computed.\n\n\n   - Class questions $\\mathcal{Q}$ are defined too abstractly. Some concrete examples would help the reader to contextualize it.\n\nIn general, authors define a rather overwhelming quantity of notation whose definition is scattered between the sections, making it very challenging to reconcile the use and the definitions of symbols. A table of symbols might help in this case, but I feel that the authors could simplify the text a lot by getting rid of superfluous notation.  \n\n**Edit:** I just realized the authors already provide a table of symbols in the supplemental, which is, however, uploaded as a separate file rather than an Appendix of the main PDF. Nonetheless, I suggest that the authors further add the domain and co-domain of all symbols (when applicable), together with a richer description of the meaning of the symbol.\n\n\n- **W2:** Unclear semantic of *Principle of Causality*: \n\n   - The sentence *intervening in the presence of a concept must alter the output distribution, ensuring causal relevance* is unclear. In particular, what does it mean to *intervene in the presence of a concept*? Does it mean intervening in the specific concept? Or to intervene on features unrelated to that concept? Furthermore, the semantics of $do(s=0)$ should be made explicit: what is the effect of such an operation on the input image/model?\n\n   - It is unclear what authors intend for *causal factors* and *spurious correlations*. Do these notions refer to the data generative process? If yes, this seems to clash with the *Principle of Faithfulness*, as if the model learned spurious features in the first place, a faithful explanation should depict such spurious features. \n\n\n- **W3:** Unclear usage of some words, which are often conflated in XAI literature, and that lack context:\n\n   - Unclear usage of the word *reasonable* in Section 2.3. *Reasonable* is an ambiguous word, and it is never contextualized, nor defined explicitly. In particular, when is a set of ground-truth laterals \"reasonable\"?\n\n   - A similar argument can be made for *sufficient*. In particular, what does *sufficient explanations* mean in Lemma 1? It seems it is the only part of the text where this word appears.\n\n\n\n- **W4:** Figure 4 is introduced abruptly, and the experimental setting is not described in detail. In particular, it is not clear how the axioms are operationalized and how the Figure should be interpreted.\n\n- **W5:** In line 351, the authors refer to Axiom $g_{10}$, but such an axiom is not present in the list of axioms reported in Section 2.3.\n\n- **W6:** The motivation behind Sec. 3.2 is unclear. I suggest that the authors better contextualize this analysis by writing a small introductory paragraph explaining the rationale behind the incoming analysis. \n\n- **W7:** Lemma 1 and Theorem 1 are introduced abruptly, and a proper description of the premises and the consequences is lacking. For instance, such results seem not to be used ever again in the text. \n\n\n**Minors**\n\n- **W8:** There are some missing references (l. 40), and citations are always reported with *\\citet* even when the citation does not play an active role in the sentence. Please, switch to *\\citep* as indicated in the submission guidelines."}, "questions": {"value": "Q1: How are the proposed axioms actually evaluated when it comes to empirical validation? I think this is key to the paper, and the fact that this question is unclear from the text highlights that major clarifications are required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l6b07g6R95", "forum": "JDeaIKX5ba", "replyto": "JDeaIKX5ba", "signatures": ["ICLR.cc/2026/Conference/Submission16422/Reviewer_WvQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16422/Reviewer_WvQw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911020589, "cdate": 1761911020589, "tmdate": 1762926542283, "mdate": 1762926542283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of robust evaluation of concept-based explanations in Convolutional Neural Networks (CNNs). The authors propose a unified axiomatic framework that formalizes five key desiderata: human alignment, causality, consistency, faithfulness, and the explanation of inner model representations. This framework translates these principles into corresponding axioms. The proposed framework consider a generic concept-based interpretability method that encodes and decodes concepts from CNNs, specifically at the penultimate layer, and then quantitatively evaluates the verification of each axiom. To demonstrate its application, the paper presents quantitative examples using two interpretability methods (CRAFT and ICE) across two different backbone architectures (ResNet50 and Inceptionv3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Significance:** The axiomatization of explainability frameworks is a highly significant contribution towards establishing a common and rigorous evaluation ground for concept-based explanations.\n- **Clarity:** The paper's discussion is generally clear and well-structured, with the accompanying figures effectively aiding the understanding of the proposed framework.\n- **Quality:** The definitions and notations used throughout the framework provide a strong and rigorous grounding for the proposed axioms and principles."}, "weaknesses": {"value": "- **Clarity/Quality:** There are insufficient details regarding the computation of the various $g_i$ ​ functions, which are linked to evaluating each axiom. The paper provides mainly descriptive statements rather than proper mathematical definitions or algorithmic steps for their calculation. This lack of clarity makes it difficult to assess the practical relevance, reproducibility, and whether these functions accurately evaluate their corresponding axiom and property. Furthermore, the text mentions $g_2, g_3, g_4 \\in \\{0,1\\}$, yet experiment show values $\\in [0,1]$, which needs clarification.\n- **Clarity:** The purpose and key takeaways from the experiments presented in Section 3.2, i.e. those involving adversarial inputs, are not clearly articulated. It is difficult to understand what are specific insights or conclusions from these experiments, either regarding the framework or the interpretability methods.\n- **Clarity:** There are several notational inconsistencies and what appear to be typos that hinder understanding. For instance, the meaning of the ⊨ symbol in Axiom 2 is unclear and not defined. Additionally, the functions $g_i$​ are introduced as indexed by $i \\in \\{1,\\dots, 5\\}$ (corresponding to the five axioms), yet the text sometimes refers to $g_i$​ with indices greater than 5 (e.g., $g_9$​ and $g_{10}$​ in lines 351, 947, and 1117). This is confusing, as it is then not clear which actual $g_i$ should be understood.  \n- **Quality:** A lot of proofs for the lemmas and theorems behind the principles are missing, even from the supplementary material. For instance, Lemma 1 and Theorem 1, presented in the main text, do not have proofs. This significantly impacts the quality and the ability to fully assess the technical correctness."}, "questions": {"value": "- Could you please provide a mathematical definition or pseudocode for each of the $g_i$​ functions, detailing exactly how they are computed from the model's concepts and outputs? This is crucial for understanding how each axiom is quantitatively evaluated.\n- Please provide the missing proofs for all lemmas and theorems, either within the main paper (if space permits) or in a comprehensive appendix, to allow for a thorough verification of the framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9IMTrEnZzJ", "forum": "JDeaIKX5ba", "replyto": "JDeaIKX5ba", "signatures": ["ICLR.cc/2026/Conference/Submission16422/Reviewer_Qaxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16422/Reviewer_Qaxw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917601265, "cdate": 1761917601265, "tmdate": 1762926541919, "mdate": 1762926541919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper Axiomatization of Concept CNN Explanations introduces a formal framework for evaluating concept-based CNN explanations. It defines five axiom Interpretability, Relevance, Coherence, Fidelity, and Sanity to ensure transparency and trust. These axioms quantitatively assess explainer faithfulness beyond pixel attribution. Experiments using CRAFT and ICE explainers on ResNet and Inception models validate robustness, including under adversarial attacks. The framework establishes a model-agnostic, quantitative basis for reliable, interpretable AI, advancing transparency and trust in explainable deep learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is interesting and applicable to the border audience \n\n2. The paper introduces a methodologically novel approach by defining explainability through an axiomatic system. Instead of relying solely on empirical visualizations, it establishes structured, measurable criteria that can be systematically tested across models.\n\n3. The framework thoughtfully integrates causal reasoning and internal representation theory, ensuring that explanations are not merely correlative but causally meaningful. This adds theoretical depth to the relationship between CNN activations and conceptual understanding.\n\n4. The paper has shown the detailed experiments over the various CNN architecture for the adversarial condition and cross-model investigation"}, "weaknesses": {"value": "1. I find the architecture are novel but most of the axioms (faithfulness, causality, stability, human alignment) restate existing XAI principles without new theorems or guarantees; similar axiomatic treatments already exist (e.g., Amgoud & Ben-Naim, 2022).\n\n2. Earlier work defined completeness and game-theoretic importance (ConceptSHAP), offering stronger notions than “minimal sufficiency” here. TCAV/ACE already quantify concept relevance; CRAFT adds importance via Sobol indices. The paper doesn’t position against these formally. Prototype families (PIP-Net, ProtoPNet line) explicitly target human-aligned concepts; their evaluation checklists could be contrasted to the proposed axioms. Also, the recent CBM advances (V2C-CBM, AAAI’25) build vision-aligned concept vocabularies; your axioms aren’t tested on (or mapped to) CBMs. Please justify the paper's key contributions\n\n3. The proposed approach are claims model agnostic: Extend to ViTs and non-ImageNet domains (medical, satellite) to support “model-agnostic” claims. Surveys flag cross-domain gaps\n\n4. How this explanability is aligned with the human, is there some study?\n\n\n**Reference:**\n\nAxiomatic XAI: Amgoud & Ben-Naim (IJCAI’22)\n\nConcept completeness / ConceptSHAP: Yeh et al., 2019\n\nTCAV / ACE (automatic discovery): Kim et al., 2018; Ghorbani et al., 2019\n\nCRAFT (recursive, Sobol importance): Fel et al., 2023\n\nPrototype methods: PIP-Net (CVPR’23)\n\nCBMs (current): V2C-CBM (AAAI’25)"}, "questions": {"value": "Please refer to the weakness section and justify the novelty. The claim for the model agnostic is not justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SES15tv76E", "forum": "JDeaIKX5ba", "replyto": "JDeaIKX5ba", "signatures": ["ICLR.cc/2026/Conference/Submission16422/Reviewer_FZpG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16422/Reviewer_FZpG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034236172, "cdate": 1762034236172, "tmdate": 1762926541308, "mdate": 1762926541308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes to the literature on the evaluation of TCAV class explanations by presenting a set of axioms that express desirable qualities these explanations should satisfy, along with a quantitative methodology for explanation evaluation. The proposed axioms are related to: (1) human-alignment, stating that explanations should focus on high-level and simple concepts without unnecessary complexity; (2) relevance, stating that explanations must refer to causally relevant concepts and not correlations (so they should not include irrelevant concepts, such as background); (3) consistency or robustness, stating that explanations should be stable under small perturbations of the input; (4) faithfulness, stating that explanations should preserve the DNN’s decision logic; and (5) representativeness, stating that explanations should relate to the DNN’s internal representation. These axioms are implemented through metrics described in the appendix. The paper compares two methods in this area using the proposed evaluation framework, finding that both achieve comparable and reasonable scores, and studies the degradation of metrics under adversarial conditions, highlighting the need for more robust explainers."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating XAI methods is a highly important topic in the field. Contributions in this direction can help improve the area and accelerate progress. The potential significance and adoption of a paper like this one is high.\n-"}, "weaknesses": {"value": "- **Clarity and informativeness of the main text**: The clarity of the paper can be significantly improved. Multiple factors impact readability. Given that the paper aims to provide a theoretical foundation, its rigor and clarity should be high. Namely:\n   - The fact that this paper focuses on (or assumes it works for) TCAV, class explanations, and explanations of a given form (i.e., prototypes + concept weights + concept set) is not mentioned in either the abstract or the introduction. From the abstract, the reader expects a more general axiomatization for concept-based explanations. As a result, there is a mismatch between the claimed contribution (concept-based explanations in general) and the proposed system, which is specifically for TCAV class explanations. This information should be stated as assumption at the beginning, given the theoretical nature of this paper. Conversely, in the current manuscript, it is mentioned only in footnotes and in the appendix.\n   - There is excessive use of adjectives, which hinders readability.\n   - Citations do not use the proper format and are sometimes overly abundant relative to the text, especially in section 4.1 and related work, which have extremely short sentences and many citations.\n   - Some important definitions are relegated to footnotes (e.g., footnote 5 on page 3 for the set of possible explanations or footnote 1 on page 2).\n   - Some important information is reported only in the appendix and not mentioned in the main text. For example, there is no indication of how the scores for each axiom are computed. This information is reported in the appendix, but since the whole experimental section discusses these values, they should be introduced in the main text. I understand that the limitations in space make giving the full mathematical derivation in the main text impractical, but the main text should at least cite and briefly describe the metrics used to compute the values.\n   - (Minor) I found it very confusing that the paper uses different words for principles and axioms (the implementations of the principles). Using so many different terms makes the text harder to read\n\n\n- **Debatable Running Example and Axioms**: Some of the axioms and the running example are debatable given the evidence provided in literature. While it is acceptable for the authors to argue for their position, it should be clearly stated that it's their point of view and alternative evidence found in the literature should be cited and discussed. In this direction, strong claims such as “logical necessities” should be avoided. Often, the textual description seems to assume a direct connection between the input and explanation spaces, without considering the model’s decision space. Some examples of these problems are the following:\n   - The running example repeatedly claims that the background concept should not be included by any explainer because it is irrelevant. However, there is significant evidence in the literature that models do use background as a discriminative concept for predictions across classes (e.g., the LIME paper), or that CNNs can solve tasks by only analyzing small portions of the background in images. Should an explainer omit the background concept in those cases? If the running example assumes the toy model relies only on specific concepts, this should be made explicit to avoid confusion.\n   - The axioms regarding robustness against adversarial attack (and consistency) are debatable and discussed extensively in the literature. A small change in the input (e.g., via adversarial attack) can produce a large shift in the network’s internal response (i.e., neuron activations) even if the final prediction remains unchanged, as shown in adversarial attack research. If an explanation is faithful according to the axiom, it should reflect such changes in the decision process, because different concepts may now be considered important. The axiom about robustness contradicts this evidence, and this tension is not discussed in the paper.\n- **Lack of contextualization with respect to literature on explanation evaluation**: There is a large body of literature offering metrics, datasets, and approaches for evaluating TCAV. Some are cited and used in the appendix. However, the paper currently lacks proper contextualization of its axioms and metrics within the current literature in the main text. For instance, what is the relationship between alternative quantitative metrics proposed in prior work and the proposed axioms? What roles do datasets from existing literature have in the current framework? What do current metrics lack that this framework provides?  Some of the proposed axioms are highly related with metrics used in literature. Which metric is completely new and which one is from the literature? These questions remain unanswered. This problem is also evident in the related work section, where different types of explanations are covered, but there is no discussion on the multiple evaluation paradigms and metrics used in TCAV research.\n- **Limited discussion for the experimental section**: There is very little discussion about the experiments. For example, in the cross-model investigation (section 3.1), the only comment on the analyzed explainer is: *“CRAFT slightly outperforms ICE in Fidelity and Relevance while both methods maintain high Coherence and consistent adherence to the axiomatic framework.”* No insights or analysis are provided that are retrievable through the frameworks presented in the text. Section 4.1 includes some information, but it is difficult to extract due to its placement among citations and repetition of principles already addressed in previous sections. Overall, the framework’s application does not seem to provide new perspectives or deeper insights about the analyzed explainers."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fCE77t3IS4", "forum": "JDeaIKX5ba", "replyto": "JDeaIKX5ba", "signatures": ["ICLR.cc/2026/Conference/Submission16422/Reviewer_N4GF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16422/Reviewer_N4GF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144538525, "cdate": 1762144538525, "tmdate": 1762926540584, "mdate": 1762926540584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}