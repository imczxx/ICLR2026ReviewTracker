{"id": "uqT7TAhwrm", "number": 17083, "cdate": 1758271952391, "mdate": 1759897199454, "content": {"title": "DTP: Delta-Guided Two Stage Pruning for Mamba-based Multimodal Large Language Models", "abstract": "Multimodal large language models built on the Mamba architecture offer efficiency advantages, yet remain hampered by redundant visual tokens that inflate inference cost, with the prefill stage accounting for the majority of total inference time. We introduce Delta-guided Two stage Pruning (DTP), a method that progressively reduces token redundancy through selective pruning at early layer and complete pruning at late layer. Unlike Transformer-oriented pruning methods, our approach derives token importance directly from Mamba’s internal parameters. The statistical distribution of these importance scores, combined with implicit attention patterns, then provides the basis for determining both the pruning layers and the tokens to be removed. Extensive evaluation across diverse benchmarks demonstrates that DTP reduces computation by nearly 50\\% while preserving task performance more effectively than existing pruning methods under the same reduction setting. Beyond efficiency, our analysis reveals previously underexplored behaviors of visual tokens within Mamba layers, suggesting a principled perspective for designing future pruning techniques in Mamba-based Multimodal Large Language Models.", "tldr": "", "keywords": ["Mamba", "Multimodal Large Language Models", "Token Pruning", "Efficiency", "Interpretability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c98ec43f33bf823d8a47079e808e4409144deb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper targets the inference-efficiency bottleneck of Mamba-based Multimodal Large Language Models (MLLMs) and proposes **Delta-guided Two-stage Pruning (DTP)**: early **top-k** retention of visual tokens and late **complete removal**, performed entirely at inference without retraining. Guided by the $\\Delta_t$ based token importance, the authors select **layer 15** for selective pruning and **layer 45** for complete pruning. Experiments on **Cobra** and **RoboMamba** indicate that DTP can reduce FLOPs by **approximately 50%** while maintaining competitive accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Architecture-aligned pruning signal:** Avoids modifying model structure or additional training; the target layers are determined via forward passes only.\n2. **Two-stage design:** Preserves sufficient early visual information while removing later-layer redundancy, substantially improving efficiency."}, "weaknesses": {"value": "1. **Methodology flaws:**\n   a. The 15/45 choices hinge on layer-wise standard deviation of $\\Delta_t$ -derived token importance computed on a **calibration dataset**; the authors mention a **VQAv2 subset** but disclose neither its size nor sampling strategy. Please detail these and validate on **multiple, distinct calibration datasets**.\n   b. A natural **ablation by perturbing the pruning layers** is missing: plots show a $Std_\\ell$ valley near **layer 15** and another around **layers 30–35**, yet no experiments/discussion evaluate these alternatives.\n   c. Reporting the **post-pruning** standard-deviation profiles would further substantiate the effectiveness of the proposed selection.\n2. **Experimental clarity and completeness:**\n   a. In **Table 5**, clarify the comparison between *complete pruning* and *disable complete pruning*: in the latter, what pruning rate is used, or is it the **vanilla** model?\n   b. It is recommended that the authors provide experimental results with different pruning rates to demonstrate the effectiveness of their approach; the main results only include (r=0.9) and (r=0.5).\n   c. Beyond **FLOPs**, include **wall-clock latency** (with **prefill/decoding** breakdown) and **memory footprint**; the two-stage paradigm may affect prefill and decode differently even under the same global token compression.\n   d. The authors should provide details such as the sampling parameters of the specific experiments to improve the reproducibility of the paper."}, "questions": {"value": "Refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DoSiYKaTy8", "forum": "uqT7TAhwrm", "replyto": "uqT7TAhwrm", "signatures": ["ICLR.cc/2026/Conference/Submission17083/Reviewer_pdx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17083/Reviewer_pdx2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381326560, "cdate": 1761381326560, "tmdate": 1762927089295, "mdate": 1762927089295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DTP (Delta-guided Two stage Pruning), a novel and effective token pruning framework designed for Mamba-based Multimodal Large Language Models (MLLMs) to reduce the high computational cost of the prefill stage, which is dominated by a large number of visual tokens. The core idea is to leverage an internal, input-dependent parameter of the Mamba architecture, ∆t, to estimate the importance of each visual token without requiring any retraining. The proposed DTP method employs a two-stage strategy, beginning with selective pruning at an early layer (the 15th), where a portion of the least important visual tokens are discarded. This is followed by complete pruning at a late layer (the 45th), where all remaining visual tokens are removed, a decision justified by the observation that their contributions become negligible in deeper layers as implicit attention patterns diminish. Extensive experiments on two Mamba-based MLLMs (Cobra and RoboMamba) demonstrate that DTP can reduce computation (FLOPs) by nearly 50% while incurring minimal performance degradation, significantly outperforming adapted Transformer-based pruning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a relevant and important problem: the inference inefficiency of Mamba-based MLLMs. It is intrinsically tied to the Mamba architecture by using the ∆t parameter for importance scoring, a concept not applicable to Transformers.\n- The design choices, particularly the selection of the 15th and 45th layers for pruning, are not made ad-hoc. They are convincingly supported by a careful analysis of the model's internal state, which adds a layer of interpretability and principle to the method.\n- The method achieves a remarkable balance between computational reduction and performance preservation. A nearly 50% reduction in FLOPs with an average performance drop of less than 1 point (on the Cobra model) is a very strong result. The comprehensive ablation studies further solidify the paper's claims."}, "weaknesses": {"value": "- The specific layers for pruning (15th and 45th) were empirically identified for the Cobra and RoboMamba models, which appear to have around 64 layers. It is unclear how these specific layer indices would generalize to Mamba-based models of different depths (e.g., a 48-layer or 96-layer model). While the methodology for finding these layers (analyzing the standard deviation of ∆t) seems general, the paper could be strengthened by framing it as a general \"recipe\" and discussing how it would apply to other architectures.\n- The evaluation relies exclusively on FLOPs as a metric for computational cost. While FLOPs are a good hardware-agnostic proxy, they do not always translate linearly to wall-clock speedup due to factors like memory access patterns and GPU kernel optimizations. Including actual latency measurements ( ms/token or similar) would provide a more practical and complete picture of the efficiency gains.\n- The experiments show results for keep ratios r of 0.9 and 0.5. A more detailed analysis of the trade-off between performance and the keep ratio r would be beneficial. A plot showing how the average performance gracefully degrades as r is decreased would give readers a better sense of the method's sensitivity to this hyperparameter.\n- It is recommended to include a comparison and discussion with these methods [1-4].\n\n[1] Arif, Kazi Hasan Ibn, et al. \"HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 2. 2025. \\\n[2] Xing, Long, et al. \"Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction.\" arXiv preprint arXiv:2410.17247 (2024).  \\\n[3] Wen, Zichen, et al. \"Stop looking for important tokens in multimodal language models: Duplication matters more.\" arXiv preprint arXiv:2502.11494 (2025).  \\\n[4] Ye, Weihao, et al. \"Fit and prune: Fast and training-free visual token pruning for multi-modal large language models.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 21. 2025."}, "questions": {"value": "- Regarding the choice of the 15th and 45th layers: Could you elaborate on the generality of this finding? If a researcher were to apply DTP to a new Mamba-based MLLM with a different number of layers, should they re-run the standard deviation analysis to find the new optimal \"early\" and \"late\" layers? Is there a rule of thumb, e.g., pruning at \\~25% and \\~70% of the model's depth?\n- The paper's efficiency claims are based on FLOPs reduction. Have you conducted any experiments to measure the actual wall-clock inference speedup (e.g., in terms of tokens/second or total latency)? This would be a valuable addition to confirm the practical benefits of DTP.\n- How sensitive is the DTP framework to the choice of the early-stage keep ratio r? Could you provide a brief analysis or a curve illustrating the performance-computation trade-off as r is varied continuously between, for example, 0.5 and 1.0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mzaSqrY8Dt", "forum": "uqT7TAhwrm", "replyto": "uqT7TAhwrm", "signatures": ["ICLR.cc/2026/Conference/Submission17083/Reviewer_oGU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17083/Reviewer_oGU1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497451032, "cdate": 1761497451032, "tmdate": 1762927088577, "mdate": 1762927088577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Delta-guided Two Stage Pruning (DTP), a pruning framework designed for Mamba-based multimodal large language models (MLLMs). The method estimates token importance using the Mamba-specific internal parameter ∆t, enabling selective pruning in early layers and complete pruning in late layers. Experiments on Cobra and RoboMamba demonstrate that DTP can reduce FLOPs by nearly 50% with minimal accuracy degradation. The study further analyzes implicit attention patterns in Mamba, providing insight into token dynamics across layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a new pruning paradigm specifically designed for Mamba-based models rather than Transformer-based ones.\n- Experiments are comprehensive, covering both Cobra and RoboMamba across multiple benchmarks.\n- The two-stage pruning strategy is intuitively reasonable and empirically validated.\n- Provides interesting analysis of implicit attention patterns in Mamba, offering new perspectives on token behavior."}, "weaknesses": {"value": "- Limited theoretical justification for ∆t as a universal token importance indicator; the claim is mostly empirical.\n- The computational overhead of computing ∆t during inference is not discussed; clarity on latency gain beyond FLOPs would be beneficial.\n- Comparison with non-pruning efficiency techniques (e.g., token merging, KV cache optimization) is missing.\n- Some figures and analysis (e.g., implicit attention) could be better interpreted to help readers grasp the practical implications.\n\nIf authors address my concerns, I will consider raising my score."}, "questions": {"value": "- Can ∆t be efficiently extracted in real inference pipelines without additional latency?\n\n- How sensitive is DTP to the choice of pruning layers (15th and 45th)? Would adaptive layer selection improve performance?\n\n- Could the ∆t-based importance be combined with other statistics (e.g., gradient-based) to further enhance robustness?\n\n- How does DTP perform under extremely high pruning ratios (>60%)?\n\n- Are the observations about implicit attention patterns consistent across all Mamba-based architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G73kaGLE5h", "forum": "uqT7TAhwrm", "replyto": "uqT7TAhwrm", "signatures": ["ICLR.cc/2026/Conference/Submission17083/Reviewer_BGBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17083/Reviewer_BGBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801836068, "cdate": 1761801836068, "tmdate": 1762927088149, "mdate": 1762927088149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Delta-guided Two Stage Pruning (DTP), a pruning framework designed for Mamba-based multimodal large language models (MLLMs). The method estimates token importance using the Mamba-specific internal parameter ∆t, enabling selective pruning in early layers and complete pruning in late layers. Experiments on Cobra and RoboMamba demonstrate that DTP can reduce FLOPs by nearly 50% with minimal accuracy degradation. The study further analyzes implicit attention patterns in Mamba, providing insight into token dynamics across layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a new pruning paradigm specifically designed for Mamba-based models rather than Transformer-based ones.\n- Experiments are comprehensive, covering both Cobra and RoboMamba across multiple benchmarks.\n- The two-stage pruning strategy is intuitively reasonable and empirically validated.\n- Provides interesting analysis of implicit attention patterns in Mamba, offering new perspectives on token behavior."}, "weaknesses": {"value": "- Limited theoretical justification for ∆t as a universal token importance indicator; the claim is mostly empirical.\n- The computational overhead of computing ∆t during inference is not discussed; clarity on latency gain beyond FLOPs would be beneficial.\n- Comparison with non-pruning efficiency techniques (e.g., token merging, KV cache optimization) is missing.\n- Some figures and analysis (e.g., implicit attention) could be better interpreted.\n\nIf authors address my concerns, I will consider raising my score."}, "questions": {"value": "- Can ∆t be efficiently extracted in real inference pipelines without additional latency?\n\n- How sensitive is DTP to the choice of pruning layers (15th and 45th)? Would adaptive layer selection improve performance?\n\n- Could the ∆t-based importance be combined with other statistics (e.g., gradient-based) to further enhance robustness?\n\n- How does DTP perform under extremely high pruning ratios (>60%)?\n\n- Are the observations about implicit attention patterns consistent across all Mamba-based architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G73kaGLE5h", "forum": "uqT7TAhwrm", "replyto": "uqT7TAhwrm", "signatures": ["ICLR.cc/2026/Conference/Submission17083/Reviewer_BGBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17083/Reviewer_BGBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801836068, "cdate": 1761801836068, "tmdate": 1763605665469, "mdate": 1763605665469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Delta-guided Two-stage Pruning (DTP), a training-free framework for visual token pruning in Mamba-based multimodal large language models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[1] Adaptation of pruning for Mamba’s state-space mechanism, clearly distinct from Transformer-specific attention-based approaches.   \n[2] Two-stage pruning strategy is empirically well-motivated by variance and implicit-attention statistics.   \n[3] Extensive ablations validating design choices and demonstrating robustness.   \n[4] The method performs pruning during inference without any retraining or fine-tuning, which makes it practically deployable."}, "weaknesses": {"value": "[1] The layer selection heuristic (15th & 45th) is empirical; a more formal justification or adaptive strategy could strengthen generality.   \n[2] The analysis depth of implicit attention remains qualitative; quantitative correlation with pruning behavior would improve rigor.   \n[3] While FLOPs reduction is well-documented, real-world inference latency (wall-clock time) is not reported.   \n[4] Limited baselines are included. There are many works that should be considered for comparison, e.g. PyramidKV [a], VL-cache [b], etc.  \n[5] Novelty is limited, where the methods/motivation are borrowed from the transformer-based research work. \n\n\n[a] Cai, Zefan, et al. \"Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling.\" arXiv preprint arXiv:2406.02069 (2024).   \n[b]   Tu, Dezhan, et al. \"VL-cache: Sparsity and modality-aware KV cache compression for vision-language model inference acceleration.\" arXiv preprint arXiv:2410.23317 (2024)."}, "questions": {"value": "How were the 15th and 45th layers chosen for selective and complete pruning? Were these empirically optimal for all models or do they depend on model depth or dataset characteristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jQDa8exdnk", "forum": "uqT7TAhwrm", "replyto": "uqT7TAhwrm", "signatures": ["ICLR.cc/2026/Conference/Submission17083/Reviewer_K7EU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17083/Reviewer_K7EU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897363530, "cdate": 1761897363530, "tmdate": 1762927087862, "mdate": 1762927087862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}