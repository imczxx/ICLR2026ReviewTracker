{"id": "RawXXTYZCw", "number": 21092, "cdate": 1758313679657, "mdate": 1759896942433, "content": {"title": "Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints", "abstract": "This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods—requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel penalty-based framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(\\delta, \\epsilon)$-Goldstein stationary points using $\\Theta(\\delta^{-1}\\epsilon^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.", "tldr": "", "keywords": ["stochastic bilevel optimization", "linear constraints", "first-order methods", "penalty method", "hypergradient approximation", "stochastic oracles", "bias–variance analysis", "Goldstein stationarity", "nonsmooth optimization", "convergence guarantees", "SFO complexity", "Lipschitz & strong convexity", "KKT/duality", "variance reduction (SVRG)", "meta-learning", "hyperparameter optimization", "reinforcement learning", "constrained lower-level problem", "perturbed gradient", "penalty Lagrangian"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/153d07cd10866ebf75178196ba5219321d813864.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper establishes a theoretical milestone by bridging the gap between linear constrained and stochastic bilevel optimization, delivering a purely first-order, provably convergent algorithm. It lays the groundwork for extending efficient bilevel solvers to more realistic, noisy, and constrained ML applications like meta-learning, RL, and data reweighting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a finite-time stochastic convergence with linear constraints and first-order access.\n\n2. This paper also has strong theoretical grounding (bias/variance analysis and Goldstein stationarity), and the proposed method has superior scalability for high-dimensional problems."}, "weaknesses": {"value": "1. This paper announces that it provides the first finite-time convergence guarantees. However, there are several works about constraints in bilevel optimization, such as Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions. Can the author provide some comparison?\n\n2. It looks like the Assumption 3.1 (ii) asks lower-level $g$ to be strongly convex and also have a bounded gradient. Can the author verify this assumption?\n\n3. This paper also employed additional assumptions compared to other bi-level works. Such as Assumption 3.1 (iii) and Assumption 3.2. \n\n4. The paper is not well organized and is hard to read. Such as $\\lambda^*(x)$ in line 151 is used before defined."}, "questions": {"value": "1. Why is Assumption 3.2 necessary? In traditional bilevel optimization, this condition typically appears as a lemma rather than an assumption. Could the authors clarify what specific difficulty prevents deriving a similar lemma in the constrained setting?\n\n2. Please compare the role and strength of this assumption with those used in other bilevel optimization works, particularly in constrained bilevel formulations.\n\n3. Does the proposed problem have any practical applications? The current experiments appear overly simplified and resemble toy examples, which raises concerns about the real-world relevance of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eqLtaUTtUi", "forum": "RawXXTYZCw", "replyto": "RawXXTYZCw", "signatures": ["ICLR.cc/2026/Conference/Submission21092/Reviewer_WVrC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21092/Reviewer_WVrC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705284088, "cdate": 1761705284088, "tmdate": 1762941249078, "mdate": 1762941249078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies stochastic bilevel optimization with linearly constrained lower-level (LL) problems, a setting where no prior work provides finite-time guarantees. The authors propose F2CSA (Fully First-order Constrained Stochastic Approximation)—a fully first-order method requiring only noisy gradients from the upper- and lower-level objectives. The key idea is to construct a stochastic inexact hypergradient oracle via a smoothed Lagrangian/penalty formulation with scaling parameters $\\alpha_1 = \\alpha^{-2}$ and $\\alpha_2 = \\alpha^{-4}$, together with inexact primal–dual LL solves.\nThey prove that the oracle has bias $O(\\alpha)$ and variance $O(1/N_g)$, and that when used in a clipped nonsmooth outer loop, the algorithm converges to a $(\\delta,\\epsilon)$-Goldstein stationary point with total complexity $\\tilde{O}(\\delta^{-1}\\epsilon^{-5})$—the first finite-time result for this class."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First finite-time guarantee for stochastic bilevel problems with linearly constrained LL subproblems, using a fully first-order method.\n2. The presentation is clear."}, "weaknesses": {"value": "1. The experimental evaluation is limited; additional large-scale experiments would be valuable to demonstrate the method’s scalability and practical relevance.\n2. The LICQ assumption appears somewhat strong. Could the authors consider relaxing it to a weaker constraint qualification, or provide more discussion on why this assumption is essential for the current analysis?"}, "questions": {"value": "1. What other stationarity notions (beyond Goldstein stationary points) have been adopted in prior literature? A more comprehensive literature review on alternative stationarity metrics would strengthen the paper’s context.\n2. I wonder whether variance-reduction or momentum techniques could further improve the theoretical complexity bounds within the proposed framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fKA2l4wP2W", "forum": "RawXXTYZCw", "replyto": "RawXXTYZCw", "signatures": ["ICLR.cc/2026/Conference/Submission21092/Reviewer_7guu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21092/Reviewer_7guu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861582923, "cdate": 1761861582923, "tmdate": 1762941248541, "mdate": 1762941248541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the linearly constrained stochastic bilevel optimization problem with first-order methods. The authors propose the algorithm F2CSA and provide convergence analysis."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The first-order methods for the constraint bilevel optimization problem have not been fully studied before.\n2. The authors provide proof sketches for better understanding."}, "weaknesses": {"value": "**There are lots of presentation problems that I doubt the correctness of the proof.**\n1. In line 191, there is an incomplete sentence.\n2. There is no explanation of Algorithm 1 before Remark 4.1. Therefore, there are a lot of undefined notations in it.\n3. There is no update rule for $\\tilde{\\lambda}(x)$.\n4. For the stochastic algorithm, are the authors sure that we can get $\\\\|\\\\tilde{y}^\\ast(x)-y^*(x)\\\\|\\\\leq\\mathcal{O}(\\delta)$ rather than  in the expectation form with samples ($\\mathbb{E}[\\\\|...\\\\|]\\leq...$ with some samples $\\xi$)? The same problem exists for $\\lambda$.\n5. In line 5 of Algorithm 1, what does \"$\\\\|\\leq\\delta$\" mean? \n6. In line 246, $\\alpha\\geq\\frac{2C_f}{\\mu}$. This means that $\\alpha$ is at a constant order. However, later, $\\alpha$ is set to the $\\epsilon$ order so that the algorithm converges. This contradiction makes me highly doubt the correctness of the proof.\n7. In Lemma 4.3, $L_{H,y}$ and $L_{H,\\lambda}$ are not defined, and the formulation is not given either. \n8. For this linear constraint BO problem, I do not see how $h(x,y)$ impacts the convergence.\n\nThere are more problems that I have not listed yet. At this point, I believe the paper is far from ready."}, "questions": {"value": "Please check the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PcO9Vjoa3Q", "forum": "RawXXTYZCw", "replyto": "RawXXTYZCw", "signatures": ["ICLR.cc/2026/Conference/Submission21092/Reviewer_m2gH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21092/Reviewer_m2gH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926179555, "cdate": 1761926179555, "tmdate": 1762941247912, "mdate": 1762941247912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}