{"id": "BCir2oVTVj", "number": 12901, "cdate": 1758211430337, "mdate": 1763745570909, "content": {"title": "Symmetric Sinkhorn Diffusion Operators", "abstract": "Smoothing a signal based on local neighborhoods is a core operation in machine learning and geometry processing. On well-structured domains such as vector spaces and manifolds, the Laplace operator derived from differential geometry offers a principled approach to smoothing via heat diffusion, with strong theoretical guarantees. However, constructing such Laplacians requires a carefully defined domain structure, which is not always available. Most practitioners thus rely on simple convolution kernels and message-passing layers, which are biased against the boundaries of the domain.\n\nWe bridge this gap by introducing a broad class of *smoothing operators*, derived from general similarity or adjacency matrices, and demonstrate that they can be normalized into *diffusion-like operators* that inherit desirable properties from Laplacians. Our approach relies on a symmetric variant of the Sinkhorn algorithm, which rescales positive smoothing operators to match the structural behavior of heat diffusion.\n\nThis construction enables Laplacian-like smoothing and processing of irregular data such as point clouds, sparse voxel grids or mixture of Gaussians. We show that the resulting operators not only approximate heat diffusion but also retain spectral information from the Laplacian itself, with applications to shape analysis and matching.", "tldr": "This work presents a method to compute heat diffusion-like operators from similarity matrices using optimal transport, applicable to generic unstructured data such as point clouds, voxel soups or gaussian splats.", "keywords": ["Laplacian-free diffusion", "Sinkhorn normalization", "Mass-preserving diffusion operator", "heat kernel approximation", "geometric data analyis", "learning on unstructured data", "similarity kernels", "spectral methods", "geometry processing", "Laplace–Beltrami", "manifold learning", "gaussian splatting", "optimal transport"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f8aea276d7975e1f9bf85a00c48d6a8cd25a2d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a unified framework for constructing diffusion-like operators on diverse data modalities, ranging from geometric domains such as meshes, point clouds, and voxel grids to more abstract settings like Gaussian mixtures and graphs. The approach is based on a symmetric Sinkhorn normalization of a positive kernel, yielding a mass-preserving, symmetric, and positive operator that behaves analogously to a Laplace-type diffusion without requiring an explicit manifold or mesh structure. This formulation generalizes classical heat diffusion to unstructured data while retaining desirable spectral and stability properties. The authors provide theoretical analysis of the symmetric Sinkhorn scaling, including convergence guarantees and connections to bi-stochastic Laplacian normalization. They also present a practical, GPU-accelerated implementation that integrates efficiently with modern deep learning toolkits. Experiments span multiple representations and tasks such as spectral analysis, diffusion of signals on shapes, and 3D shape correspondence, demonstrating consistency across modalities and competitive runtime scaling. The method is positioned as a general-purpose, geometry-agnostic building block for differentiable diffusion and attention-based architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a conceptually elegant and well-formulated approach to defining diffusion operators on arbitrary data domains through symmetric Sinkhorn normalization. Its key strength lies in the 'unification': a single construction that consistently yields mass-preserving, symmetric, and positive diffusion-like operators across diverse modalities, including meshes, point clouds, volumetric grids, Gaussian mixtures, and graphs.\n\nThe theoretical presentation is clear and rigorous, with well-defined axioms, proofs of existence and convergence for the symmetric Sinkhorn scaling, and insightful connections to bi-stochastic Laplacian theory. A practical GPU implementation integrates smoothly with differentiable programming enviromnents, emphasizing scalability.\n\nEmpirically, the paper demonstrates qualitative and quantitative behavior across multiple tasks, such as spectral analysis, shape diffusion, and correspondence learning. Runtime comparisons are included."}, "weaknesses": {"value": "While the paper is theoretically solid and clearly presented, its empirical scope is somewhat narrow relative to the breadth of its claims. Most demonstrations focus on proof-of-concept tasks that illustrate the behavior of the proposed operator rather than establishing concrete advantages over strong Laplacian-based or diffusion-network baselines. As a result, the practical impact of the method (especially in settings where Laplacian discretizations are already available or efficient!) remains somewhat speculative. A more systematic comparison on downstream learning or signal-processing tasks would strengthen the case for real-world applicatons.\n\nAlthough the authors provide convergence guarantees for the symmetric Sinkhorn scaling, the broader connection between the proposed operator and established continuous LB theory is discussed only qualitatively. Readers interested in manifold consistency or spectral convergence may find the theoretical positioning somewhat ambiguous. Similarly, while the symmetric Sinkhorn normalization is conceptually neat, its novelty relative to prior work on bi-stochastic Laplacian normalization could be articulated more sharply.\n\nFinally, the experimental section could benefit from clearer quantitative evidence of robustness, e.g., under sampling noise, geometric deformation, or varying kernel bandwidths to support the claimed generality across modalities. The applications showcased are visually convincing but not always deeply analyzed in terms of quantitative metrics or ablation study."}, "questions": {"value": "The experimental results convincingly illustrate consistency across data modalities but remain limited in terms of downstream validation. Are there concrete scenarios, such as non-manifold geometries, corrupted meshes, or learning-based applications operating with point clouds in very high dimensions (think of learned latent spaces) where the symmetric Sinkhorn formulation offers measurable advantages over classical Laplacian or graph-diffusion approaches (see e.g. \"Latent functional maps\" for an example)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yFMdAxRiUN", "forum": "BCir2oVTVj", "replyto": "BCir2oVTVj", "signatures": ["ICLR.cc/2026/Conference/Submission12901/Reviewer_ETdd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12901/Reviewer_ETdd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466466663, "cdate": 1761466466663, "tmdate": 1762923681639, "mdate": 1762923681639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Symmetric Sinkhorn Diffusion Operators, a new method for normalizing a family of “discrete diffusion operators” – which includes graph Laplacian variants. By extending Sinkhorn scaling to include a mass matrix (M), the proposed normalization provides the first graph-Laplacian that is mass preserving and does not experience ringing artifacts. By the Sinkhorn approach, it is also bi-stochastic. The flexibility of choosing M allows for adaptation across different data representations (graphs, point clouds, voxel grids, Gaussian mixtures). Experiments show it is spectrally similar to the original graph-Laplacians (exponential), effective for mass-preserving interpolation, and can be incorporated to NNs, specifically DiffusionNet (Sharp et al) for improved gains."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "•\tMain: A new flavor for the graph-Laplacian is proposed with several desired properties (see below). It can be plugged wherever graph-Laplacians appear, in manifold learning and even in specialized NN architectures (e.g. DiffusionNet).\n\n•\tClear mathematical formulation: The paper rigorously defines diffusion operators and provides formal guarantees (Theorems 4.1–4.2) ensuring convergence and symmetry under a discrete measure.\n\n•\tElegant unification: The approach unifies multiple normalizations (row, symmetric, spectral) under a single operator framework that generalizes diffusion to arbitrary discrete measures.\n\n•\tMass preservation and bi-stochastic symmetry: The operator Q adresses a long-standing trade-off in graph-Laplacian normalization, providing an operator that is symmetric stochastic - and also preserves mass.\n\n•\tImplementation simplicity: Algorithm 1 (symmetric Sinkhorn normalization) is GPU-friendly, requires only matvecs with (S), and converges in a few iterations.\n\n•\tCross-domain generality: Through the choice of M, The same procedure works on diverse data structures: graphs, voxels, Gaussian mixtures. \n\n•\tSolid connection and extension of continuous theory:  Including an extension of current theory, including fixed-scale limits, and generalizing to a family of smoothing operators.\n\n•\tNumerical demonstrations: Mass preservation as well as cross-data types spectral consistency is clearly shown."}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "1.\tHow does Algorithm 1 compare numerically to standard bistochastic Sinkhorn scaling? Is it any different? Would the incorporation of M affect it in terms of convergence rate and stability? \n\n2.\tHow sensitive is the method to errors in the estimated mass matrix (M) (e.g., when PCL sampling density estimation is noisy due to finite samples)? I would gladly raise my score if error bounds derivation + empirical demonstrations would be in place.\n\n3.\tFrom your experiments - are there guidelines for M construction when transitioning between data types (e.g. mesh to PCL), that keep spectra consistent between data types?\n\n4.\tPlease add at least one more example for spectral consistency (Fig. 3) and shape interpolation (Fig. 5). Appendix is fine.\n\n5.\tPlease report limitations of the method. For instance - mass preservation might be restrictive in some cases: Consider the shape deformations presented in [1]. Such extreme deformations cannot accommodate mass preservation while keeping the shape intact. Another limitation is the fact that we do not know exactly the relation between Q's spectra and the graph-Laplacian's.\n\nWriting:\n\n1.\tCould the authors clarify their positioning w.r.t. the prior work on Sinkhorn scaling for Laplacians and kernels (Marshall & Coifman, 2019; Wormell & Reich, 2021; Cheng & Landa, 2024)?\n\n2.\tAlthough it is clear by observation - please state clearly in the text how mass preservation (or any other desired property) is shown in the numerical experiments. \n\n3.\tPlease justify the use of DiffusionNet for correspondence - as it would be more natural to start with classification or segmentation.\n\n4.\tThe authors may wish to mention iterative flow formulations where mass preservation is achieved by alternating between applying a Laplacian operator and a mass-normalization step. A prominent example is cMCF [2] (\"area-normalized\"). \n\n[1] Brokman et. al 2024 \"Spectral Total-Variation Processing of Shapes - Theory and Applications\" ACM Transactions on Graphics (TOG)\n\n[2] Kazhdan et. al 2012 \"Can mean-curvature flow be modified to be non-singular?\" Computer Graphics Forum, Vol. 31."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMlr6Eon73", "forum": "BCir2oVTVj", "replyto": "BCir2oVTVj", "signatures": ["ICLR.cc/2026/Conference/Submission12901/Reviewer_s2Wz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12901/Reviewer_s2Wz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748586463, "cdate": 1761748586463, "tmdate": 1762923681267, "mdate": 1762923681267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach to generalize heat diffusion operators to unstructured geometric data by transforming symmetric similarity matrices into mass-preserving diffusion operators using a symmetric variant of the Sinkhorn algorithm. Some of the writing is not clear, including the gaps they bridge. Visualizations are good. Experimental results and comparisons to other method are less convincing, appear to yield marginal improvements. Some aspects of the experiments are not clear as well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThorough theoretical analysis.\n2.\tSufficient number of visual experiments in both main paper and appendix\n3.\tNice looking visualizations"}, "weaknesses": {"value": "1.     Editing and clarity, not clear what gapped are exactly bridged. Some parts may be too heavily written with LLM's.\n2.     Hard to see a significant novelty presented here, the field is very mature and heavily researched.\n3.     The authors claim \"Most practitioners thus rely on simple convolution kernels and message-passing layers, which are biased against the boundaries of the domain.” However, it is not biased if Neumann boundary conditions are taken under consideration.\n4.\tRuntime results compare between GPU method and CPU method, unfair comparison. \n5.\tWorst quantitative results than competitors\n6.\tLack of quantitative comparisons\n7.\tTable 1: left is right and vice versa (remark)\n8.\tDistinction between structured and unstructured domain is unclear\n9.\tNo quantitative evaluation of the results of figures 4, 5, just visual. There are qualitative metrics that can be applied.\n10.\tIn figure 3 – the font of the graph is too small and not visible (remark)"}, "questions": {"value": "1.\tCan you add quantitative aspect to your visual results, more than general intuition?\n2.\tCan you explain why did you compare yourself to these specific methods in the visual experiments?\n3.\tCan you implement your method on CPU or other methods on GPU for fair runtime comparison?\n4.\tSplits your article to known and the innovative parts\n5.\tCite the most relevant references related to the mentioned methods \n6.\tFind an example to demonstrate the superior of your method"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dSi04bCu6o", "forum": "BCir2oVTVj", "replyto": "BCir2oVTVj", "signatures": ["ICLR.cc/2026/Conference/Submission12901/Reviewer_bESy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12901/Reviewer_bESy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909311619, "cdate": 1761909311619, "tmdate": 1762923680827, "mdate": 1762923680827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper characterizes key properties of Laplacian and diffusion operators which make them useful for various tasks in geometry processing, in a general formalism based on the idea of smoothing. It then proposed a Sinkhorn-algorithm-based way of computing operators with these properties. This replaces convolution-like operations which can produce artifacts at boundaries - or when used with unstructured representations of geometric objects - with alternatives that can perform better. It also does not require computing or truncating Laplacian eigenfunctions on a mesh. The authors study the functional limit of their proposed approach, and prove convergence in a certain limit as the point cloud approximates the true continuous geometry. They show experimentally that the resulting operators can be used for pose interpolation and other shape analysis tasks in computational geometry."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a good paper. Its strengths include:\n* **Comprehensive and well-presented low-level details in the writing.** The authors introduce the properties they want, and develop a clean algorithm for computing operators that satisfy those properties.\n* **Presenting the discrete case, which is simpler, first.** This is a good structure, as it allows the reader to gradually ease in to the material, before heavier formalism is introduced.\n* **Characterizing continuum limits, and not just graph and mesh based operations.** This is important, as it shows that the operators are principled and will not degenerate under mesh refinement.\n* **High-quality figures.** As is often the case in a paper at the intersection of machine learning and computer graphics, the authors take the actual graphics seriously.\n* **Fast convergence of the proposed method.** The authors are able to get results in as few as 5-10 Sinkhorn iterations, which is at least one order of magnitude less than I would have expected.\n* **Flexible downstream use.** This includes the ability to recover Laplacian eigenmaps as shown in Fig.3, and interpolation use such as in Fig.5.\n* **Inclusion of actual wall-clock runtime comparisons.** Comparisons like this are important in practice, because they illustrate in precise terms how long the techniques used take to work, and speed can be very important for various geometry processing applications.\n\nPlease also note that, since my qualifications for reviewing this paper rest mainly on using similar technical tools for completely different purposes, I am unable to evaluate the novelty of this work within geometry processing. I therefore defer evaluations on this aspect to other reviewers and would be interested to know their views in discussion."}, "weaknesses": {"value": "The main issues include:\n* **Confusing medium-level structure.** This paper could have been written in the following form: Section 1 - intro, Section 2 - background and prior work, Section 3 - the problem, Section 3.1 - new proposed operators on graph, Section 3.2 - new proposed operators in general, Section 3.3 - computation, Section 4 - results. Instead, the authors first present that they do on a graph, then present it in general, without cleanly and clearly stating what is the problem in a manner that is separate from stating what is the method.  While I ultimately understood that the problem at hand is \"develop a characterization of smoothing operators that behave well even in the presence of boundaries or unstructured representations of geometry, and make them computationally tractable\", it would be good to have a reminder at the beginning of Section 3 and Section 4 that this is the goal, otherwise it is unclear where the formalism is headed.\n* **Please remind readers what is a Metzler matrix**. I've written several papers about using Laplacian eigenmaps for certain machine learning purposes, and know the abstract machinery well, but did not recognize this specific technical term. I suspect many other readers may miss it too.\n* **The two-dimensional plots presented are far too small and are therefore not accessibility friendly or readable on paper.** I cannot read what is in Figure 3(e) or Figure 3(j) because the font is tiny. Please redo this figure to make its fonts the same size as the surrounding text."}, "questions": {"value": "Two key questions:\n* **Do you have any idea why Sinkhorn is so fast in this setting?** This is much faster than uses in optimal transport that I have worked with, which is typically at minimum hundreds of iterations.\n* **I am confused about Table 1.** This shows that the two Q-DiffNet variants, which are the authors' proposal, perform better on the SHREC19 benchmark, but worse on FAUST and SCALE. I have many questions: (1) Why are there no error bars - are all the methods deterministic? (2) How much of a difference does 2.1 vs. 1.6 on FAUST make? (3) How strong is the absolute performance - what kind of score would be considered \"solved\" for these benchmarks? (4) If the proposed method is weaker than baselines on this benchmark, is there some other characteristic which makes it desirable anyway, such as being faster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jJnWwnw5tL", "forum": "BCir2oVTVj", "replyto": "BCir2oVTVj", "signatures": ["ICLR.cc/2026/Conference/Submission12901/Reviewer_dJDG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12901/Reviewer_dJDG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762358518124, "cdate": 1762358518124, "tmdate": 1762923680505, "mdate": 1762923680505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}