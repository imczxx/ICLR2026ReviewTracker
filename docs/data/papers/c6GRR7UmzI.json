{"id": "c6GRR7UmzI", "number": 6374, "cdate": 1757975264707, "mdate": 1759897918753, "content": {"title": "TriGuardFL: Triple-Step Byzantine-Robust Federated Learning against Model Poisoning Attacks", "abstract": "Federated learning's (FL) distributed architecture is promising, yet it is vulnerable to model poisoning attacks that degrade global model accuracy. Existing defense strategies typically compare the locally updated gradients of clients and exclude or down-weight those exhibiting substantial deviations. However, these strategies may become ineffective when the clients’ datasets are heterogeneous. In this paper, we propose TriGuardFL, a novel triple-step defense framework that robustly discriminates malicious actors from benign and non-IID clients. First, we employ a cosine-similarity-based filter to identify suspicious clients. Second, a fine-grained secondary evaluation assesses their performance using a small class-stratified dataset. By analyzing class-wise performance differences, it can discern whether a divergent update stems from a malicious attack or data heterogeneity. Finally, a Bayesian reputation model is integrated to manage the uncertainty of detection and enhance the long-term robustness. Extensive case studies on two benchmark datasets and three representative model poisoning attacks demonstrate that TriGuardFL outperforms existing methods in mitigating the impact of model poisoning attacks.", "tldr": "", "keywords": ["Federated Learning; Byzantine-Robust"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ee0267d1f3cf26d9ea5972b8ecff6ed3805d34a.pdf", "supplementary_material": "/attachment/0d1d1a4a11f5419f20abbfc405a6225ed9e7ea7c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TriGuardFL, a three-step defense framework for Byzantine-robust federated learning (FL). It addresses a challenge in FL—distinguishing between malicious and benign clients under non-IID data. The method integrates (1) cosine-similarity filtering, (2) class-wise statistical testing using a small clean dataset, and (3) a Bayesian reputation model for long-term robustness. Experiments on Fashion-MNIST and CIFAR-10 with several CNN architectures show improvements over baselines such as DeFL, FLTrust, and Multi-Krum."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper identifies a limitation of existing Byzantine-robust FL methods under non-IID conditions.\n2. The triple-step design is intuitive, each step compensates for the others’ weaknesses.\n3. This paper uses multiple datasets, architectures, and attack types (Min-Max, Min-Sum, LIE) under both full and partial knowledge settings."}, "weaknesses": {"value": "1. Each step (similarity filtering, validation via small dataset, Bayesian weighting) has been explored before in isolation. The paper lacks strong conceptual unification beyond “combining three steps.”\n2. The assumption that the server owns a few-shot clean dataset breaks the FL privacy model and reduces practicality for real-world deployment.\n3. Only image classification benchmarks are used; no large-scale or cross-domain experiments (e.g., NLP or medical data).\n4. Mathematical analysis mostly restates known FedAvg convergence bounds (Li et al., 2019) with small extensions for malicious clients—little theoretical innovation.\n5. The paper only considers classical attacks (Min-Sum, Min-Max, LIE). It ignores modern adaptive attacks like gradient-sign inversion, backdoor or stealthy attacks that exploit cosine-based detection."}, "questions": {"value": "1. How much each of the three steps contributes individually to the robustness gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BTuuvqBqkC", "forum": "c6GRR7UmzI", "replyto": "c6GRR7UmzI", "signatures": ["ICLR.cc/2026/Conference/Submission6374/Reviewer_wGaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6374/Reviewer_wGaJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760987679924, "cdate": 1760987679924, "tmdate": 1762918663614, "mdate": 1762918663614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes TriGuardFLhat to discriminate malicious actors from benign and non-IID clients. It first filters potential attackers using cosine similarity, and then perform a statistical significance test. Finally, it employs a dynamic Bayesian reputation system to track client behavior over time, using this reputation score to weight model aggregation and perform long-term client selection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The work tackles a key but difficult challenge in FL, i.e., distinguishing malicious clients from benign outliers (non-IID).\n\n- The three-step detection/filter is logical and comprehensive.\n\n- The introduction of a Bayesian reputation system with a discount factor provides adaptability."}, "weaknesses": {"value": "- There is a fatal logic vulnerability that the a simple adaptive attacker can mimic the non-IID client to pass the test. The experiments also fail to test against adaptive attacks.\n\n- The reputation system (Step 3) blindly trusts the flawed detector (Step 2), and will reward successful attackers by boosting their reputation scores, making the system actively counter-productive.\n\n- Step 3 applies a zero-weight filter to any client, including benign ones, which means a significant increase in false positives.\n\n- A clean few-shot server dataset covering all classes is an extremely strong prerequisite that is impractical in many FL settings and undermines privacy principles.\n\n- For Sec 4.2 ROBUSTNESS ANALYSIS, it is wrong as it does not prove the effectiveness of the defense itself."}, "questions": {"value": "- In Step 2, a single t-test comparing the two groups $C_1$ v.s. $C_2$ yields only one p-value. Does this imply that multiple different tests are performed?\n\n- How can the server obtain knowledge of the full class space of all clients to build $D'$ without violating FL privacy norms? How about a client introduces a novel class unknown to the server?\n\n- Is there any design to defend against an attacker who intentionally spoofs a non-IID client, or maybe clients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dy3L74iygQ", "forum": "c6GRR7UmzI", "replyto": "c6GRR7UmzI", "signatures": ["ICLR.cc/2026/Conference/Submission6374/Reviewer_3doy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6374/Reviewer_3doy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832049767, "cdate": 1761832049767, "tmdate": 1762918663168, "mdate": 1762918663168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TriGuardFL, a three-step defense framework for Byzantine-robust federated learning under non-IID settings. In Step 1, the server detects potentially malicious clients using cosine similarity between each client’s update and the aggregated global model. In Step 2, a class-wise evaluation is performed on a small server-side dataset using a $t$-test to distinguish benign non-IID clients from adversarial ones. In Step 3, a Bayesian reputation update is used to assign lower weights to low-reputation clients in future aggregation rounds."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper addresses an important problem: improving the robustness of federated learning when client data are heterogeneous.\n+ The class-wise evaluation step is designed to reduce false positives caused by label distribution imbalance.\n+ The Bayesian reputation mechanism adds temporal adaptivity to client weighting.\n+ The experimental results show some improvement in robustness across multiple attack settings."}, "weaknesses": {"value": "- The proposed framework mainly integrates ideas that already exist in prior Byzantine-robust FL studies rather than introducing a substantially new method.\nStep 1 is very similar to the cosine-similarity-based filtering in FLTrust, where the server compares each client’s update direction with a trusted reference model. The claimed link to FLDetector is inaccurate, as FLDetector focuses on temporal consistency checks across rounds rather than per-round similarity comparison. Step 2 follows the general idea of client evaluation as in DeFL, although DeFL uses gradient-norm metrics instead of $t$-tests. Step 3 conceptually matches reputation- or trust-based FL frameworks, which track client reliability over time using reputation-weighted aggregation or trust propagation, as discussed in surveys on Trustworthy FL. Overall, the contribution mainly combines known components into a single framework without introducing a clear algorithmic or theoretical innovation.\n- In Section 3.1, cosine similarity is defined using $\\nabla F_i(w_{K_i,t})$. In standard FL, the server cannot compute client gradients and only receives updated weights $w_{K_i,t}$. Therefore, the feasibility of Step 1 is unclear.\n- The approach assumes that the server holds a small labeled dataset that includes all classes. This assumption weakens the privacy guarantees of FL and may not hold in realistic deployments.\n- The design of the class-wise $t$-test is not well explained. There are no details about sample sizes, independence assumptions, or correction for multiple comparisons. The justification for using loss differences as the test statistic is also missing.\n- The convergence proof relies on $\\mu$-strong convexity and $L$-smoothness, which are not valid for deep CNNs. This makes the theoretical analysis largely symbolic and not directly applicable to the experimental models.\n- The evaluation is incomplete and lacks analysis depth. There are no experiments on targeted, backdoor, or adaptive attacks, even though the paper claims general Byzantine robustness. No ablation studies are provided for key hyperparameters ($\\delta_1$, $\\delta_2$, $\\varepsilon$, $r$, $T_1$) or for the effect of the server dataset size $|D'|$. Some baseline methods perform equally well or even better in certain cases, which raises doubts about the claimed advantage of TriGuardFL.\n- Several notation and presentation problems reduce clarity. The paper inconsistently uses $\\gamma_t$ and $\\gamma_{i,t}$ in the update equation. The parameter $\\delta_2$ appears in Algorithm 1 but is never defined, and its relation to the “Significance Level = 0.001” in Table 1 is unclear. The text also switches between “parameters” and “gradients”, which is inconsistent with the mathematical formulation in Step 1.\n- The formatting of tables is inconsistent. Table 1 has its caption above the table, while Tables 2 and 3 have captions below. The caption placement should follow a consistent format."}, "questions": {"value": "1.Which part of TriGuardFL is genuinely novel beyond the existing methods such as FLTrust and DeFL?\n\n2.How is $\\nabla F_i(w_{K_i,t})$ obtained if the server does not have access to local client data?\n\n3.What is the size and class coverage of the server dataset $D'$? How does performance vary if $D'$ is incomplete?\n\n4.How is the $t$-test validated when the sample size is small?\n\n5.Can adaptive or backdoor attackers exploit the $t$-test mechanism to evade detection?\n\n6.Please provide ablation and sensitivity results for $\\delta_1$, $\\delta_2$, $\\varepsilon$, $r$, and $T_1$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WsJAqSt0Ak", "forum": "c6GRR7UmzI", "replyto": "c6GRR7UmzI", "signatures": ["ICLR.cc/2026/Conference/Submission6374/Reviewer_a1FK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6374/Reviewer_a1FK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873498209, "cdate": 1761873498209, "tmdate": 1762918662689, "mdate": 1762918662689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key vulnerability in Federated Learning (FL): standard defenses against model poisoning often fail in non-IID (heterogeneous) settings, as they cannot distinguish between malicious updates and the natural, harmless deviations from clients with different data distributions.\n\nThe authors propose TriGuardFL, a novel triple-step defense framework designed to solve this specific problem. First, it uses a cosine-similarity-based filter to identify a broad list of \"suspicious\" clients. Second, it performs a fine-grained evaluation on these suspicious clients using a small, class-stratified dataset held by the server. By analyzing per-class performance, it can discern if a deviation is from a malicious attack (which tends to degrade performance uniformly) or a benign non-IID client (who may perform poorly on some classes but well on others). Finally, it integrates a Bayesian reputation model to track client behavior over time, which manages detection uncertainty and enhances long-term robustness.\n\nExtensive experiments on Fashion-MNIST and CIFAR-10 show that TriGuardFL outperforms existing state-of-the-art defenses like DeFL, FLTrust, and Multi-Krum, particularly in non-IID settings where others fail in terms of the average rank of the defense against these attacks. The authors acknowledge that the method's reliance on a clean server-side dataset is a limitation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work identifies a critical vulnerability in federated learning: the difficulty of distinguishing between genuinely malicious behavior and the natural, divergent updates from benign clients in a non-IID setting. They propose a novel heuristic for this problem, based on the insight that malicious updates tend to degrade performance uniformly, while benign non-IID clients will exhibit high variance in their per-class performance.\n- The multi-stage detection architecture is logical. It uses a broad, low-cost filter (cosine similarity) to create a \"suspicious\" list and applies a more expensive, fine-grained analysis only to that list. The third step is necessary for long-term stability and to tolerate potential detection errors.\n- The integrated Bayesian reputation model is a standard approach that suits the setting well, using a Beta distribution to manage uncertainty  rather than proposing an overly complex new system. The reputation system handles practical details well, such as using a \"hard filter\" to set a client's aggregation weight to zero if flagged as malicious in the current round , and implementing \"long-term gatekeeping\" to remove consistently low-reputation clients.\n- The reputation system handles practical details well, such as using a \"hard filter\" to set a client's aggregation weight to zero if flagged as malicious in the current round , and implementing \"long-term gatekeeping\" to remove consistently low-reputation clients.\n- While the experiments use two benchmark datasets , they are evaluated across a diverse set of five different network architectures, including LeNet, AlexNet, VGG11, VGG16, and ResNet18, which strengthens the claims of its effectiveness ."}, "weaknesses": {"value": "- The paper's biggest weakness is that it does not evaluate against an adaptive adversary. The core of the defense relies on the Step 2 heuristic that malicious clients degrade performance uniformly, while benign non-IID clients show high per-class variance. An adaptive attacker could easily fool this by crafting a malicious update that performs very well on one arbitrary class, thereby disguising itself as a benign non-IID client. This lack of stress-testing means the central idea of the paper is not fully validated.\n- The first step, cosine-based shortlisting, is vague. Equation 4 compares the client gradient $\\nabla F_i(w_{i,t}^K)$ with a global gradient $\\nabla F(w_t')$. It is not clear how this global gradient is computed. This first step also suffers from a \"chicken-and-egg\" problem. It uses an \"initial aggregation\" $w_t'$ as the reference for similarity. If this reference is already poisoned by attackers, the whole defense could break, as malicious clients might appear \"similar\" to the poisoned average. It also relies on a hard-coded threshold ($\\delta_1$), which is a brittle defense mechanism.\n- The paper lacks a clear analysis of false positives. While it mentions the reputation system tolerates false negatives , it provides no evidence that benign non-IID clients are not incorrectly flagged and eventually \"starved\" or removed by the long-term gatekeeping mechanism .\n- The experimental diversity is limited. Instead of using four different complex models (VGG11, VGG16, ResNet18) on the single CIFAR-10 dataset, the paper would have been more convincing if it had demonstrated its effectiveness on more diverse data modalities, such as text.\n- The attack scenario tested represents a relatively weak threat. The experiments use a 12.5% malicious ratio (4 of 32 clients), but only sample 50% of clients per round (16 clients). This means, on average, only two attackers are active in any given round. The paper does not convince that the defense is robust against higher, more realistic proportions of attackers. Even when 64 clients were simulated, the fraction of malicious clients was still 12.5% which is low. \n- The main results table (Table 2) reports only test loss, not accuracy. Loss is a less intuitive metric for performance. Furthermore, while TriGuardFL wins on average loss score, it does not consistently outperform all other defenses in every individual scenario. For example, on the partial knowledge attack, TriGuardFL wins only 3 out of 15 times. \n- The design of the Step 2 filter seems to reward high variance, which may not be desirable. A client is deemed benign if its \"good\" class performance is significantly different from its \"bad\" class performance, which is a strange and potentially exploitable proxy for \"benign-ness.\"\n- The three-stage process, especially the per-class, per-client analysis in Step 2 , introduces significant computational overhead for the server, which is never measured or discussed."}, "questions": {"value": "- Given that the Step 2 filter is the key innovation but seems easily fooled by an adaptive adversary, how can the defense be modified to handle a smart adversary?\n- Since the Step 1 filter uses the (potentially poisoned) initial aggregate $w_t'$ as its reference, how can this stage be modified so that the defense stays robust even when the reference is poisoned? Some prior defense techniques [1] handle this.\n\n[1]: Sharma, Atul, et al. \"Flair: Defense against model poisoning attack in federated learning.\" Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G9mv6bWN3A", "forum": "c6GRR7UmzI", "replyto": "c6GRR7UmzI", "signatures": ["ICLR.cc/2026/Conference/Submission6374/Reviewer_c5iV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6374/Reviewer_c5iV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942422379, "cdate": 1761942422379, "tmdate": 1762918662250, "mdate": 1762918662250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key vulnerability in Federated Learning (FL): standard defenses against model poisoning often fail in non-IID (heterogeneous) settings, as they cannot distinguish between malicious updates and the natural, harmless deviations from clients with different data distributions.\n\nThe authors propose TriGuardFL, a novel triple-step defense framework designed to solve this specific problem. First, it uses a cosine-similarity-based filter to identify a broad list of \"suspicious\" clients. Second, it performs a fine-grained evaluation on these suspicious clients using a small, class-stratified dataset held by the server. By analyzing per-class performance, it can discern if a deviation is from a malicious attack (which tends to degrade performance uniformly) or a benign non-IID client (who may perform poorly on some classes but well on others). Finally, it integrates a Bayesian reputation model to track client behavior over time, which manages detection uncertainty and enhances long-term robustness.\n\nExtensive experiments on Fashion-MNIST and CIFAR-10 show that TriGuardFL outperforms existing state-of-the-art defenses like DeFL, FLTrust, and Multi-Krum, particularly in non-IID settings where others fail in terms of the average rank of the defense against these attacks. The authors acknowledge that the method's reliance on a clean server-side dataset is a limitation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work identifies a critical vulnerability in federated learning: the difficulty of distinguishing between genuinely malicious behavior and the natural, divergent updates from benign clients in a non-IID setting. They propose a novel heuristic for this problem, based on the insight that malicious updates tend to degrade performance uniformly, while benign non-IID clients will exhibit high variance in their per-class performance.\n- The multi-stage detection architecture is logical. It uses a broad, low-cost filter (cosine similarity) to create a \"suspicious\" list and applies a more expensive, fine-grained analysis only to that list. The third step is necessary for long-term stability and to tolerate potential detection errors.\n- The integrated Bayesian reputation model is a standard approach that suits the setting well, using a Beta distribution to manage uncertainty  rather than proposing an overly complex new system. \n- The reputation system handles practical details well, such as using a \"hard filter\" to set a client's aggregation weight to zero if flagged as malicious in the current round , and implementing \"long-term gatekeeping\" to remove consistently low-reputation clients.\n- While the experiments use two benchmark datasets , they are evaluated across a diverse set of five different network architectures, including LeNet, AlexNet, VGG11, VGG16, and ResNet18, which strengthens the claims of its effectiveness ."}, "weaknesses": {"value": "- The paper's biggest weakness is that it does not evaluate against an adaptive adversary. The core of the defense relies on the Step 2 heuristic that malicious clients degrade performance uniformly, while benign non-IID clients show high per-class variance. An adaptive attacker could easily fool this by crafting a malicious update that performs very well on one arbitrary class, thereby disguising itself as a benign non-IID client. This lack of stress-testing means the central idea of the paper is not fully validated.\n- The first step, cosine-based shortlisting, is vague. Equation 4 compares the client gradient $\\nabla F_i(w_{i,t}^K)$ with a global gradient $\\nabla F(w_t')$. It is not clear how this global gradient is computed. This first step also suffers from a \"chicken-and-egg\" problem. It uses an \"initial aggregation\" $w_t'$ as the reference for similarity. If this reference is already poisoned by attackers, the whole defense could break, as malicious clients might appear \"similar\" to the poisoned average. It also relies on a hard-coded threshold ($\\delta_1$), which is a brittle defense mechanism.\n- The paper lacks a clear analysis of false positives. While it mentions the reputation system tolerates false negatives , it provides no evidence that benign non-IID clients are not incorrectly flagged and eventually \"starved\" or removed by the long-term gatekeeping mechanism .\n- The experimental diversity is limited. Instead of using four different complex models (VGG11, VGG16, ResNet18) on the single CIFAR-10 dataset, the paper would have been more convincing if it had demonstrated its effectiveness on more diverse data modalities, such as text.\n- The attack scenario tested represents a relatively weak threat. The experiments use a 12.5% malicious ratio (4 of 32 clients), but only sample 50% of clients per round (16 clients). This means, on average, only two attackers are active in any given round. The paper does not convince that the defense is robust against higher, more realistic proportions of attackers. Even when 64 clients were simulated, the fraction of malicious clients was still 12.5% which is low. \n- The main results table (Table 2) reports only test loss, not accuracy. Loss is a less intuitive metric for performance. Furthermore, while TriGuardFL wins on average loss score, it does not consistently outperform all other defenses in every individual scenario. For example, on the partial knowledge attack, TriGuardFL wins only 3 out of 15 times. \n- The design of the Step 2 filter seems to reward high variance, which may not be desirable. A client is deemed benign if its \"good\" class performance is significantly different from its \"bad\" class performance, which is a strange and potentially exploitable proxy for \"benign-ness.\"\n- The three-stage process, especially the per-class, per-client analysis in Step 2 , introduces significant computational overhead for the server, which is never measured or discussed."}, "questions": {"value": "- Given that the Step 2 filter is the key innovation but seems easily fooled by an adaptive adversary, how can the defense be modified to handle a smart adversary?\n- Since the Step 1 filter uses the (potentially poisoned) initial aggregate $w_t'$ as its reference, how can this stage be modified so that the defense stays robust even when the reference is poisoned? Some prior defense techniques [1] handle this.\n\n[1]: Sharma, Atul, et al. \"Flair: Defense against model poisoning attack in federated learning.\" Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G9mv6bWN3A", "forum": "c6GRR7UmzI", "replyto": "c6GRR7UmzI", "signatures": ["ICLR.cc/2026/Conference/Submission6374/Reviewer_c5iV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6374/Reviewer_c5iV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942422379, "cdate": 1761942422379, "tmdate": 1763678606641, "mdate": 1763678606641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}