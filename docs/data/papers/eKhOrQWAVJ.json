{"id": "eKhOrQWAVJ", "number": 2382, "cdate": 1757068535986, "mdate": 1759898152131, "content": {"title": "Spatially Guided Training for Vision-Language-Action Model", "abstract": "Large vision–language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce SP-VLA, a dual-system **V**ision–**L**anguage–**A**ction framework that leverages **S**patial **P**riors as a bridge between linguistic instructions and embodiment-specific control. \nintroduce SP-VLA aligns action learning with spatial priors through two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting.\nThis design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, introduce SP-VLA achieves substantial improvements over vanilla VLA, with performance increasing from $66.1{\\rightarrow}84.6$ on Google Robot and from $54.7{\\rightarrow}73.2$ on WidowX, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. We will release code, data, and model checkpoints to support future research.\nSee more visualization results at the anonymous page: https://sp-vla-anonymous.vercel.app", "tldr": "", "keywords": ["Spatial Priors", "Robot Manipulation", "Instruction Following"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/708a1e3c80b0a1ea50c1c231f1d80e8100986ac3.pdf", "supplementary_material": "/attachment/75e14cb6cc4a1b8ba70ac810be0617bbb2c6e609.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SP-VLA, a spatially guided vision–language–action (VLA) model that introduces spatial grounding during both pretraining and post-training. By combining large-scale vision–language grounding data with robot-specific datasets, SP-VLA effectively aligns spatial and action objectives, leading to a robust and generalizable framework for robot learning that achieves significant improvements over vanilla VLA baselines on multiple benchmarks.\n\nOverall, SP-VLA presents a well-motivated and promising approach to integrating spatial priors into general-purpose action representation learning. The reported gains are substantial, and the concept of latent spatial prompting is particularly compelling. However, the paper would benefit from clearer methodological exposition, more detailed attribution of performance gains, and improved consistency in presentation. With these refinements, it could make a strong contribution to the field."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. SP-VLA achieves notable improvements over vanilla VLA across multiple settings.\n2. The discussion around latent spatial prompting is particularly interesting. The authors recognize the difficulty of obtaining high-quality spatial annotations and instead design a lightweight strategy that implicitly guides visual representations. The qualitative motivation and empirical validation for this are compelling.\n3. Stronger transfer to unseen objects and paraphrased language instructions; good long horizon reasoning."}, "weaknesses": {"value": "1. Models such as GR00T and π0 are not cited when first introduced. Similarly, based on context (line 140-145), DiT appears to refer to a Diffusion Transformer but is never expanded upon. Clarifying these early and in figures would improve accessibility for readers new to the area.\n\n\n2. The paper transitions quickly from the framework description to experiments without clearly specifying details like: (a) The training procedure and optimization details (loss functions, objective balancing). (b) Architectural modifications relative to existing VLAs, etc. These are crucial for reproducibility and clarity.\n\n3. There is some ambiguity in experimental setups. For example: In Experiment 3.1, how are training losses weighted between spatial grounding and action data for the vanilla co-training baseline? In the RefCOCO-g perception task, what specific perception task is used, and how is vanilla VLA (without a dedicated perception head) evaluated on it?\n\n4. SP-VLA includes multiple architectural components (spatial prompting, latent planner, diffusion-based actor, hierarchical reasoning). However, the paper attributes most gains to spatial grounding. It would strengthen the claims to discuss or experimentally isolate the contributions of each module. For example, how would performance change if the diffusion actor were replaced with a simpler transformer or MLP policy?\n\n5. The paper does not clearly describe what forms of spatial prompts are used or whether they are consistent across tasks. Are these limited to the ones shown in Figure 2? Appendix D alludes to an ablation while making some conclusions about spatial prompts, but corresponding experiments seem missing. Given that spatial prompting is the key contribution, these things deserve a more detailed exposition (potentially in the main text).\n\n6. Typographical and presentation issues: \n* “WindowX” appears to be a typo for WidowX.\n* Table 1 could improve readability by highlighting best results (e.g., bold/underline consistently to table 2).\n* While the main text has a good number of visual explanation and diagrams, I think the ones related to spatial prompts, the ‘lightweight’ prompting module, etc. that center around the main contributions of the paper are missing."}, "questions": {"value": "1. Why not directly fine-tune a pretrained VLM that already exhibits strong spatial grounding capabilities, instead of performing an additional spatial grounding pretraining stage? What are the concrete advantages of the proposed approach?\n\n2. In Section 3.1, how are the spatial grounding and action losses weighted in the co-training baseline?\n\n3. In Section 3.1, what specific perception task is used on RefCOCO-g, and how is vanilla VLA evaluated there without a perception head?\n\n4. Could the authors clarify what kinds of spatial prompts are used (point, box, trajectory, etc.), and whether they vary across datasets or tasks?\n\n5. How much do the diffusion transformer and hierarchical planner contribute independently to overall performance? Could similar benefits arise from these architectural components rather than spatial grounding alone?\n\n6. Why do many popular baselines (Tables 2 and 3) underperform the “vanilla VLA” baseline used in this paper? What efforts are made to ensure that these baselines are evaluated under comparable settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a1o1cXypt6", "forum": "eKhOrQWAVJ", "replyto": "eKhOrQWAVJ", "signatures": ["ICLR.cc/2026/Conference/Submission2382/Reviewer_qHTz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2382/Reviewer_qHTz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409016802, "cdate": 1761409016802, "tmdate": 1762916215233, "mdate": 1762916215233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SP-VLA, a Spatially Guided Vision-Language-Action model that introduces latent spatial priors to bridge perception and action. The model conducts Spatial Grounding Pre-training on large-scale real and synthetic data to acquire spatial understanding, and then applies Spatially Guided Action Post-training to maintain such spatial priors during policy learning. It achieves new state-of-the-art results on SimplerEnv, and further validates the approach with a large-scale custom simulation benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Achieves new SOTA performance on the SimplerEnv benchmark.\n- Provides clear analysis of Figure 3, effectively demonstrating how spatial priors benefit action generation.\n- Conducts extensive experiments on a large-scale, custom-built simulation to verify the generality of the approach."}, "weaknesses": {"value": "- In Figure 3(b), Vanilla VLA’s curve may not have saturated. Given its long zero-phase and upward trend, it’s possible that Vanilla VLA could reach similar performance with longer training. Likewise, the Co-training setup doesn’t show a clear drop and might continue improving. I suggest extending training to 80–100 k steps or until convergence for all settings. It necessary to justify whether spatial priors mainly accelerate convergence or are essential for final performance.\n- Moreover, it would be insightful to extend experiments of Figure 3(b) to Google Robot (VM/VA) setting\n- Consider analyzing different scales of spatial prior pretraining data, not only training steps. More interpretability analysis on how spatial priors influence the latent representation would further strengthen the contribution.\n- Related work currently misses several key papers that employ chain-of-thought style or multi-step reasoning in robot learning. Please cite:\n1. Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation\n2. CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models\n3. Efficient Robotic Policy Learning via Latent Space Backward Planning"}, "questions": {"value": "- How exactly is Figure 3(a) evaluated? Is the grounding metric still measured through spatial-ground prompts during Stage 2?\n- Have you tried use co-training and your methods together.\n- Why does the Spatially Guided Action Training curve in Figure 3(a) continue to improve, even though Stage 2 seemingly lacks explicit grounding supervision? This behavior seems counter-intuitive and deserves clarification.\n- Do the authors plan to release the custom simulation benchmark used in large-scale experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NBCx6YWD5t", "forum": "eKhOrQWAVJ", "replyto": "eKhOrQWAVJ", "signatures": ["ICLR.cc/2026/Conference/Submission2382/Reviewer_BHsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2382/Reviewer_BHsq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912376485, "cdate": 1761912376485, "tmdate": 1762916215059, "mdate": 1762916215059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a novel vision-language-action model, SP-VLA, that uses a two-stage training approach to ground action learning in spatial reasoning: 1) a pre-training stage, in which large-scale internet vision-language corpora and robot-specific datasets are formatted into a unified Question-Answer template and used to train a VLM, and 2) a post-training stage, in which a prompt that queries the spatial relationships between objects is appended to the task instruction and the VLM is trained to predict actions. The experiments evaluate SP-VLA on SimplerEnv, Libero, a new simulated benchmark based on IsaacSim, and short-horizon and long-horizon pick and place tasks in a real-world set-up. SP-VLA performs favorably compared to many VLAs on SimplerEnv, and is mainly compared to $\\pi_{0}$ and GR00T N1.5 on the other benchmarks. The ablations show that SP-VLA relies on both training stages for its best performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- SP-VLA explicitly introduces a spatial grounding training phase and includes a loss over spatial metrics (box, point, trace).\n- Strong results on multiple simulated and real-world benchmarks show that SP-VLA sets a new SOTA on SimplerEnv, Libero and the constructed benchmarks."}, "weaknesses": {"value": "- Unclear whether the SOTA performance is due to the proposed training method or due to the VLM backbone. The comparisons between SP-VLA, $$pi_{0}$$ and GR00T N1.5 in Section 3 are fair if they have seen the same data during pre-training, but if that's not the case then it's possible the improvement comes from the strength of the VLM backbone, not necessarily the proposed training method. \n- The details on the experiments and the losses are not very clear – see the questions below."}, "questions": {"value": "l. 147-148 What are the outputs of the model in symbolic form? What is the loss during each stage?\nl. 188 + l. 196 How do you get the spatial prompt? + Relatedly, can you explain the lightweight latent spatial prompting strategy?\nl. 339 Are all the models in Tables 2 and 3 trained on the same amount/set of data?\nl. 372 \"benchmarks\" <-- do you mean tasks?\nl. 376 \"post-pretraining\" Is this different from the post-training phase, i.e. stage 2, of SP-VLA?\nRegarding experiments in section 3: Did you finetune $$pi_{0}$$ and GR00T N1.5 on the tele-operation demonstrations before inference?\nTable 4: The abbreviations are confusing, can you clarify them?\nl. 1106 What prompt do you use to rephrase the instruction? Can you give an example of a rephrased prompt?\nSection E.3: l. 1129 \"unseen object position\" Does that mean that the seen object is in a position unseen during training, or that an unseen object is placed in an unseen position? \"unseen instruction\" Does that mean it was rephrased or is it a novel task? l. 1161 Are positions of objects and containers fixed across trials or across models?\nl. 1258 - 1260 How do the buttons correspond to the correct answer?\nl. 1267 \"be prompted\" Is there a prompt from the user to tell the robot to continue to the next subtask? Also, are the subtask instructions generated by the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wimWq5idbQ", "forum": "eKhOrQWAVJ", "replyto": "eKhOrQWAVJ", "signatures": ["ICLR.cc/2026/Conference/Submission2382/Reviewer_QqBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2382/Reviewer_QqBv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965226366, "cdate": 1761965226366, "tmdate": 1762916214617, "mdate": 1762916214617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposs SP-VLA, using spatial prior to bridge the language space and action space. The SP-VLA has two stages, the first train the VLM with image, language, and spatial priors like bbox, points or trajectories. The second stage use the latent generated in the first stage to train a DiT policy for control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper is good, which using spatial prior to pretrain the vlm, then further tune the vlm for downstream tasks, which compare with mainstream vlm which focus on daily question or specific QA task, spatial information seems more important for control and robotic task.\n\n2. The paper deliver thourough experiments and ablations to claim the spatial priors both in pretraining and post-training will help the embodied control."}, "weaknesses": {"value": "1. The idea of use spatial priors to pretrain or post train or cotrain a vlm is not novel and actually used by many recent papers like , so i do not think the proposed spatial prior will help both pretraining and post training is novel enough. \n\n2. In the experiment design, the author try to verify with spatial prior in pretraining or post training, the SP-VLA can perform better than other fundational VLAs, but it seems the compared method like pi and openvla, these paper does not claim they use specific priors, The author might need to compare with more related work as mentioned in 1.\n\n\n[1] Wen, Chuan, et al. \"Any-point trajectory modeling for policy learning.\" \n[2] Niu, D., Sharma, Y., Biamby, G., Quenum, J., Bai, Y., Shi, B., ... & Herzig, R. (2024). Llarva: Vision-action instruction tuning enhances robot learning."}, "questions": {"value": "See weakness part, and furthermore, small questions:\n1. for the real world experiment, section 3.4, it seems all task is pick and place. It is not clear clarified what is the experiment setting? Did the author use standard DROID? If so, why the pi-zero is so bad on unseen object, with my own experience, at least pi-zero should have better results than what record in Table 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mG9IJPMnSI", "forum": "eKhOrQWAVJ", "replyto": "eKhOrQWAVJ", "signatures": ["ICLR.cc/2026/Conference/Submission2382/Reviewer_uJaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2382/Reviewer_uJaW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976875329, "cdate": 1761976875329, "tmdate": 1762916214483, "mdate": 1762916214483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SP-VLA, a dual-system VLA framework that integrates action learning with spatial priors. The framework consists of two components: Sys2, a VLM (Qwen2.5-vl-3B), and Sys1, a DiT-based diffusion model. A Q-former module links the two systems through cross-attention, compressing Sys2’s tokens into a fixed-length representation that is then injected into Sys1.\n\nTraining occurs in two stages: spatial grounding, where the VLM acquires spatial priors (points, boxes, and trajectories) from a large dataset, and spatially guided action post-training. Experimental results in both simulated and real-world environments demonstrate that SP-VLA outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-organized and clearly written, making it easy to follow. The authors provide extensive implementation details to support reproducibility and include numerous illustrative examples that enhance understanding. The experiments are reasonable and solid."}, "weaknesses": {"value": "The introduction of spatial priors to enhance VLA performance is conceptually intuitive and has been explored extensively in prior research (e.g., LLaRA [1], TraceVLA [2], Magma [3], among others). Similar strategies, such as dataset curation via object detection, have also been commonly adopted. Moreover, the dual-system design has been previously discussed in works like TinyVLA [4] and $\\pi_0$ [5]. Consequently, the paper’s contribution appears to offer limited novelty relative to existing literature.\n\nNonetheless, the work provides substantial implementation details and practical engineering insights, which could be valuable to the research community. Therefore, I recommend a weak acceptance.\n\nReference\n\n1. Li, X., Mata, C., Park, J., Kahatapitiya, K., Jang, Y. S., Shang, J., ... & Ryoo, M. S. (2024). Llara: Supercharging robot learning data for vision-language policy. arXiv preprint arXiv:2406.20095.\n\n2. Zheng, R., Liang, Y., Huang, S., Gao, J., Daumé III, H., Kolobov, A., ... & Yang, J. (2024). Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345.\n\n3. Yang, J., Tan, R., Wu, Q., Zheng, R., Peng, B., Liang, Y., ... & Gao, J. (2025). Magma: A foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 14203-14214).\n\n4. Wen, J., Zhu, Y., Li, J., Zhu, M., Tang, Z., Wu, K., ... & Tang, J. (2025). Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters.\n\n5. Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., ... & Zhilinsky, U. (2024). $\\pi_0 $: A Vision-Language-Action Flow Model for General Robot Control. arXiv preprint arXiv:2410.24164."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUnkCgfptI", "forum": "eKhOrQWAVJ", "replyto": "eKhOrQWAVJ", "signatures": ["ICLR.cc/2026/Conference/Submission2382/Reviewer_MEHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2382/Reviewer_MEHz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993087776, "cdate": 1761993087776, "tmdate": 1762916214349, "mdate": 1762916214349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}