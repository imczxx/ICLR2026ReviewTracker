{"id": "s1avhccyvg", "number": 22146, "cdate": 1758326724812, "mdate": 1763702633875, "content": {"title": "Which Eigenvectors Do Graph Transformers Need for Node Classification?", "abstract": "Graph transformers have emerged as powerful tools for modeling complex graph-structured data, offering the ability to capture long-range dependencies beyond the graph adjacency. Yet their performance on node classification often lags behind that of message passing and spectral graph networks. Unlike these methods, graph transformers require additional mechanisms to inject structural information. In this work, we focus on Laplacian positional encodings, which use eigenvectors of the graph Laplacian to provide node-level positional information. Existing methods select eigenvectors using data-agnostic heuristics, assuming one-size-fits-all rules suffice.\nIn contrast, we show that the spectral distribution of class information is graph-specific. To address this, we introduce Broaden the Spectrum (BTS), a novel, intuitive, and data-driven algorithm for selecting subsets of Laplacian eigenvectors for node classification.\nOur method is grounded in theory: we characterize the structure of optimal attention matrices for classification and show, in a simplified setting, how BTS naturally emerges as the eigenvector selection rule for achieving such attention matrices. When evaluated with standard graph transformer architectures, it delivers substantial performance gains across a wide range of node classification benchmarks. Our work shows that the performance of graph transformers on node classification has been held back by the choice of positional encodings and can be improved by employing a broader, well-chosen set of Laplacian eigenvectors.", "tldr": "We present a principled approach for selecting Laplacian eigenvectors to boost performance of standard graph transformers on node classification.", "keywords": ["Graph Transformers", "Node classification", "Position Encoding", "Graph Laplacian", "Heterophily"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb664fd9a09b35a388aa79464e7475f499c90f06.pdf", "supplementary_material": "/attachment/0d7fa90666ae1b454ddea55db0817fd712ebd8dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper examines the positional encoding effectiveness of Laplacian matrix eigenvectors in graph transformers, with a particular focus on node classification tasks. It introduces an Energy Spectral Density metric derived from class labels and uses this metric to identify the top-_k_ eigenvectors for encoding. The proposed approach is integrated into several existing graph transformer architectures, leading to consistent improvements in node classification performance across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ESD metric and corresponding BTS method are simple, intuitive, and easily adaptable to a wide range of graph transformer models.\n2. The paper offers a theoretical analysis of the rationale behind BTS, elucidating its effectiveness in the context of node classification tasks.\n3. The experimental evaluation is thorough, including extensive ablation studies that validate the efficacy of the proposed BTS method."}, "weaknesses": {"value": "1. The first one lies in BTS’s reliance on full eigen-decomposition, which incurs higher computational complexity compared to previous methods that select only the lowest or highest top-_k_ eigenvectors. This substantially restricts its scalability to large-scale graphs.\n2. Both the BTS method and its theoretical analysis are limited to node classification task, posing considerable challenges when extending to other tasks such as link prediction or graph-level prediction.\n3. The assumptions underlying the theoretical analysis are overly restrictive—particularly the formulation  $X = Y M_X + \\sigma N $—which does not accurately reflect real-world conditions, where node features typically incorporate structural and neighborhood information.\n4. The experimental section omits several important baselines, including PolyFormer [1] and SpecFormer [2].\n5. Moreover, the Chameleon and Squirrel datasets used in the experiments exhibit substantial edge overlap, results should be reported on their filtered versions in [3].\n6. (Minor) The graph should be denoted as \"an undirected graph\" in Definition 2.1.\n\n[1] Ma J, He M, Wei Z. Polyformer: Scalable node-wise filters via polynomial graph transformer[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024: 2118-2129.\n\n[2] Bo D, Shi C, Wang L, et al. Specformer: Spectral graph neural networks meet transformers[J]. arXiv preprint arXiv:2303.01028, 2023.\n\n[3] Platonov O, Kuznedelev D, Diskin M, et al. A critical look at the evaluation of GNNs under heterophily: Are we really making progress?[J]. arXiv preprint arXiv:2302.11640, 2023."}, "questions": {"value": "1. Please begin by responding to the Weaknesses part.\n2. The baselines PolyFormer and SpecFormer should be included for comparison in the experimental evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "boY8J2saJA", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Reviewer_3HAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Reviewer_3HAs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690764768, "cdate": 1760690764768, "tmdate": 1762942088649, "mdate": 1762942088649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the positional encoding effectiveness of Laplacian matrix eigenvectors in graph transformers, with a particular focus on node classification tasks. It introduces an Energy Spectral Density metric derived from class labels and uses this metric to identify the top-_k_ eigenvectors for encoding. The proposed approach is integrated into several existing graph transformer architectures, leading to consistent improvements in node classification performance across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ESD metric and corresponding BTS method are simple, intuitive, and easily adaptable to a wide range of graph transformer models.\n2. The paper offers a theoretical analysis of the rationale behind BTS, elucidating its effectiveness in the context of node classification tasks.\n3. The experimental evaluation is thorough, including extensive ablation studies that validate the efficacy of the proposed BTS method."}, "weaknesses": {"value": "1. The first one lies in BTS’s reliance on full eigen-decomposition, which incurs higher computational complexity compared to previous methods that select only the lowest or highest top-_k_ eigenvectors. This substantially restricts its scalability to large-scale graphs.\n2. Both the BTS method and its theoretical analysis are limited to node classification task, posing considerable challenges when extending to other tasks such as link prediction or graph-level prediction.\n3. The assumptions underlying the theoretical analysis are overly restrictive—particularly the formulation  $X = Y M_X + \\sigma N $—which does not accurately reflect real-world conditions, where node features typically incorporate structural and neighborhood information.\n4. The experimental section omits several important baselines, including PolyFormer [1] and SpecFormer [2].\n5. Moreover, the Chameleon and Squirrel datasets used in the experiments exhibit substantial edge overlap, results should be reported on their filtered versions in [3].\n6. (Minor) The graph should be denoted as \"an undirected graph\" in Definition 2.1.\n\n[1] Ma J, He M, Wei Z. Polyformer: Scalable node-wise filters via polynomial graph transformer[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024: 2118-2129.\n\n[2] Bo D, Shi C, Wang L, et al. Specformer: Spectral graph neural networks meet transformers[J]. arXiv preprint arXiv:2303.01028, 2023.\n\n[3] Platonov O, Kuznedelev D, Diskin M, et al. A critical look at the evaluation of GNNs under heterophily: Are we really making progress?[J]. arXiv preprint arXiv:2302.11640, 2023."}, "questions": {"value": "1. Please begin by responding to the Weaknesses part.\n2. The baselines PolyFormer and SpecFormer should be included for comparison in the experimental evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "boY8J2saJA", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Reviewer_3HAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Reviewer_3HAs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690764768, "cdate": 1760690764768, "tmdate": 1763716769185, "mdate": 1763716769185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Broaden the Spectrum (BTS), a data-driven method for selecting Laplacian eigenvectors as positional encodings (PEs) in Graph Transformers (GTs) for node classification. Existing GTs underperform on node classification due to their reliance on data-agnostic heuristics which ignore the graph-specific spectral distribution of class information. The main contributions of this paper are: (1) BTS Algorithm: A lightweight, task-aware method that selects eigenvectors most aligned with class labels using Energy Spectral Density (ESD). (2) Theoretical Justification: Shows that the optimal attention matrix for classification has a class-wise block structure, and that BTS-selected eigenvectors best approximate this structure. (3) Empirical Validation: Extensive results demonstrate significant performance gains across homophilic, heterophilic, and long-range benchmarks using standard GT architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tNovel & Principled Method: BTS is intuitive and theoretically grounded, bridging graph signal processing and transformer architectures, and is a simple yet effective method for improving GTs. Moreover, it is a plug-in module and can easily be applied into different backbones.\n\n(2)\tSolid Theoretical Analysis: It proves that optimal attention matrices for classification should have class-block structure.\n\n(3)\tStrong Empirical Results: Extensive experiment result, including large gains on challenging datasets; consistent improvements across multiple architectures and task types (homophily, heterophily, long-range)."}, "weaknesses": {"value": "(1)\tModerate novelty: This work can be seen as an incremental work on traditional positional encoding methods by adding ESD before selection of eigenvectors.\n\n(2)\tLarge experimental searching space: According to Tab. 12, hyper-parameter searching space seems to be extremely large, which diminishing reproducibility of the results.\n\n(3)\tClarification issue of motivations: Statement of data-agnostic methods seems ambiguous. (see Q.1, Q. 2)\n\n(4)\tSome issues and concerns of experimental parts: See Q. 3."}, "questions": {"value": "(1)\tAuthor states that current positional encoding methods are data-agnostic and proposed BTS is a data-adaptive method. However, calculation of eigenvector is still required in Algorithm 1, which I think is still related to graph data (structure). Does data-agnostic mean feature-free? This part should be discussed or clarified to avoid misunderstanding. Moreover, now that your method is data-adaptive, it is suggested to add comparison with other learnable positional encoding methods like LSPE [1].\n\n(2)\tFor definition of ESD, it seems that it can be regarded as a preprocessing procedure which is static in essence. Then what is the difference between BTS and static positional encoding methods? \n\n(3)\tPlease list computation cost and GPU consumption of BST and its comparison with LPE or other methods. It seems that ESD calculation is time-consuming, especially you claim that BST is “lightweight”. Moreover, what about its scalability on large-scale graphs?\n\n[1] Dwivedi V P, Luu A T, Laurent T, et al. Graph neural networks with learnable structural and positional representations[J]. arXiv preprint arXiv:2110.07875, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xiib3Zd2sk", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Reviewer_Ay4a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Reviewer_Ay4a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528348836, "cdate": 1761528348836, "tmdate": 1762942087953, "mdate": 1762942087953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines why graph transformers often underperform on node classification tasks, identifying the choice of eigenvectors for Laplacian positional encodings as a key factor. To address this, the authors introduce Broaden the Spectrum (BTS), a data-driven approach that selects eigenvectors based on their alignment with class label energy. Theoretical analysis explains how BTS promotes attention matrices with class-aligned block structures, and extensive experiments on homophilic, heterophilic, and long-range benchmarks demonstrate that BTS significantly outperforms common eigenvector selection heuristics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Figure 1 offers a clear illustration of class-label energy distributions, highlighting the importance of adaptive spectrum selection.\n\n2. The theoretical analysis helps to understand the method’s underlying principles.\n\n3. Experiments across diverse graph types, particularly heterophilic and long-range datasets, demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. Additional related works [1-2] on adaptive or alternative positional encodings in graph transformers should be discussed to provide a more comprehensive context.\n\n2. The proposed use of label-aligned spectral energy for positional encoding selection relies heavily on labeled data, which may not always be readily available.\n\n3. While the paper focuses on node classification, it would be valuable to explore whether the proposed approach can generalize to graph-level classification tasks.\n\n[1] Park, Wonpyo, et al. \"Grpe: Relative positional encoding for graph transformer.\" arXiv preprint arXiv:2201.12787 (2022).\n\n[2] Li, Chenyang, et al. \"DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification.\" arXiv preprint arXiv:2505.17660 (2025)."}, "questions": {"value": "How does the proposed method perform under label-scarce settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4SAQeDZJKJ", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Reviewer_m6Ny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Reviewer_m6Ny"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965527084, "cdate": 1761965527084, "tmdate": 1762942087566, "mdate": 1762942087566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study how the selection of Laplacian eigenvectors as positional encodings influences graph transformer performance in node classification. The authors propose Broaden the Spectrum (BTS) to selects eigenvectors according to their class-label energy spectral density (ESD). They show that the optimal attention matrix has a class-wise block structure and that high-label-energy eigenvectors best approximate it. Experiments demonstrate consistent large performance gains on heterophilic and long-range benchmarks, across several graph transformer architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Introducing adaptive frequency selection meaningfully advances positional encoding in graph transformers. \n\n2. The model is simple, effective, and supported by theoretical analysis."}, "weaknesses": {"value": "1. While early graph transformers relied heavily on Laplacian eigenvectors to incorporate graph topology, recent work has demonstrated that structural biases can also be introduced through GNNs [1,2,3] and attention masks[4]. Thus, the statements \"graph\ntransformers require explicit positional encodings to inject structural information\" in Abstract and \"transformers rely on positional encodings (PEs) to inject structural information\" in Introduction may introduce some misleading understanding. \n\n\n2. While the proposed BTS is theoretically grounded in the concept of class-label energy spectral density (ESD), this dependence constrains its applicability to supervised settings. Therefore, the impact of label quantity on performance needs to be studied. \n\n3. The explanation of the “class-wise block structure” is somewhat ambiguous. Intuitively, the optimal attention matrix is purely block-diagonal form (where the diagonal blocks are non-zero and the off-diagonal blocks are zero). In Figure 2, the empirical pattern shows substantial inter-class attention, which seems inconsistent with the intuitive clustering.\n\n4. The paper omits comparison and discussion with several state-of-the-art transformer-based graph models, including Polynormer [1], CoBFormer [2], DualFormer [3], and Gradformer [4], which represent the latest advances in structural bias integration. This omission substantially weakens the empirical credibility of the paper’s claimed contributions. \n\n[1] Polynomial-Expressive Graph Transformer in Linear Time, in ICLR 24. \n\n[2] Less is More: on the Over-Globalizing Problem in Graph Transformers, In ICML 24.\n\n[3] DUALFormer: Dual Graph Transformer, in ICLR 25.\n\n[4] Gradformer: Graph Transformer with Exponential Decay, in IJCAI 24."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K1rJCSnshT", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Reviewer_wJCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Reviewer_wJCD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976366170, "cdate": 1761976366170, "tmdate": 1762942087197, "mdate": 1762942087197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal Response (1/2)"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. We are pleased that the core motivations, theoretical insights, and empirical strengths of our work were broadly recognized across all reviews.\n\n* **Clear Motivation & Core Insight:** Reviewers noted that “*introducing adaptive frequency selection meaningfully advances positional encoding in graph transformers.*” (wJCD and Ay4a), and that “*Figure 1 offers a clear illustration of class-label energy distributions, highlighting the importance of adaptive spectrum selection.*” (m6Ny)  \n* **Theoretical Contributions:** Reviewers noted that our analysis “*helps to understand the method’s underlying principles*” (m6Ny), it “*proves that optimal attention matrices for classification should have class-block structure*” (Ay4a), and that our theory “*elucidates its effectiveness in the context of node classification tasks*” (3HAs).  \n* **Simplicity, Practicality & Adaptability of BTS:** Reviewers highlighted that BTS is “*simple, effective*” (wJCD), “*intuitive and theoretically grounded \\[…\\] and can easily be applied into different backbones*” (Ay4a), and “*simple, intuitive, and easily adaptable to a wide range of graph transformer models*” (3HAs).  \n* **Strong Empirical Results:** Reviewers emphasized our “*strong gains on heterophilic and long-range datasets*” (m6Ny), praised the “*extensive experiment result, including large gains on challenging datasets*” (Ay4a), and commended the “*extensive ablation studies that validate the efficacy of the proposed BTS method*” (3HAs).\n\n\\\nWe take this opportunity to address some concerns shared by multiple reviewers:\n\n**1. Performance study under label-scarce setting:** To address reviewer wJCD and m6Ny’s concerns about the performance of our method in label scarce settings, we conducted an additional experiment in which we artificially restrict the number of training labels. **This analysis has been added to Appendix B in our updated manuscript**. (Results also in table R1A in our response to reviewer wJCD)\n\nAs expected, the test accuracy of both models decreases as the number of training labels is reduced. However, GT-BTS still consistently outperforms the baseline GT across all label budgets, showing that even in label-scarce regimes, selecting eigenvectors using BTS remains more effective than the classical top-k selection.\n\n\\\n**2. Extended Baseline Comparisons:** As requested by reviewers wJCD and 3HAs, we now include comparisons with four recent state-of-the-art models: **PolyFormer**, **CoBFormer**, and **Polynormer** (graph transformers), and **SpecFormer** (a spectral GNN). Because the literature does not report results on all of our datasets, we reran these baselines under our standardized training pipeline and hyperparameter-tuning setup to ensure a fair and consistent comparison. These new results have been added to **Tables 1 and 2 (page 7\\) and Figure 3 (page 8\\) of the updated manuscript** (highlighted in blue).\n\nBased on **average rank** (lower is better), we observe the following trends:\n\n- **Heterophilic datasets:** `GraphGPS-BTS < NAGphormer-BTS < Polynormer < GT-BTS < GraphGPS < … `\n- **Homophilic datasets:** `PolyFormer < GraphGPS-BTS < NAGphormer-BTS < SpecFormer < GT-BTS < …` \n- **Overall:** `GraphGPS-BTS < PolyFormer < NAGphormer-BTS < Polynormer < GT-BTS < …`\n\nGraphGPS goes from being ranked 6th overall to 1st, when equipped with BTS. Notably, GT, the simplest transformer in our study, improves dramatically, going from being ranked 12th overall to 5th. Overall, the BTS variants of GT, NAGphormer, and GraphGPS not only outperform their respective base models but also remain highly competitive with recent state-of-the-art transformer and spectral approaches. \n\n\\\n**3. On graph classification and other tasks:** Several reviewers note that our analysis does not directly extend to tasks such as graph classification or link prediction. We would like to clarify the motivation and scope of our work. We focus specifically on **node classification**, because this is the setting where graph transformers have historically struggled to match the performance of MPNNs and spectral GNNs \\[1, 2\\]. \n\nOur goal is to demonstrate that graph transformers, particularly LPE-based ones, have been held back primarily by *how* eigenvectors were selected, and that with a principled selection strategy, they can be highly competitive in the node-classification regime.\n\nOther tasks pose different challenges for injecting positional or structural information, and have been addressed extensively in prior work \\[3, 4\\]. Our contribution is therefore focused and complementary: we provide a theoretical framework and a practical solution tailored to node classification, the regime where graph transformers have been most disadvantaged. There is also strong precedent for this focus, as several prior works have studied only node classification for similar reasons \\[5, 6, 7, 8, 9, 10\\]."}}, "id": "uFgGClki09", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704852639, "cdate": 1763704852639, "tmdate": 1763704852639, "mdate": 1763704852639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal Response (1/2)"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. We are pleased that the core motivations, theoretical insights, and empirical strengths of our work were broadly recognized across all reviews.\n\n* **Clear Motivation & Core Insight:** Reviewers noted that “*introducing adaptive frequency selection meaningfully advances positional encoding in graph transformers.*” (wJCD and Ay4a), and that “*Figure 1 offers a clear illustration of class-label energy distributions, highlighting the importance of adaptive spectrum selection.*” (m6Ny)  \n* **Theoretical Contributions:** Reviewers noted that our analysis “*helps to understand the method’s underlying principles*” (m6Ny), it “*proves that optimal attention matrices for classification should have class-block structure*” (Ay4a), and that our theory “*elucidates its effectiveness in the context of node classification tasks*” (3HAs).  \n* **Simplicity, Practicality & Adaptability of BTS:** Reviewers highlighted that BTS is “*simple, effective*” (wJCD), “*intuitive and theoretically grounded \\[…\\] and can easily be applied into different backbones*” (Ay4a), and “*simple, intuitive, and easily adaptable to a wide range of graph transformer models*” (3HAs).  \n* **Strong Empirical Results:** Reviewers emphasized our “*strong gains on heterophilic and long-range datasets*” (m6Ny), praised the “*extensive experiment result, including large gains on challenging datasets*” (Ay4a), and commended the “*extensive ablation studies that validate the efficacy of the proposed BTS method*” (3HAs).\n\n\\\nWe take this opportunity to address some concerns shared by multiple reviewers:\n\n**1. Performance study under label-scarce setting:** To address reviewer wJCD and m6Ny’s concerns about the performance of our method in label scarce settings, we conducted an additional experiment in which we artificially restrict the number of training labels. **This analysis has been added to Appendix B in our updated manuscript**. (Results also in table R1A in our response to reviewer wJCD)\n\nAs expected, the test accuracy of both models decreases as the number of training labels is reduced. However, GT-BTS still consistently outperforms the baseline GT across all label budgets, showing that even in label-scarce regimes, selecting eigenvectors using BTS remains more effective than the classical top-k selection.\n\n\\\n**2. Extended Baseline Comparisons:** As requested by reviewers wJCD and 3HAs, we now include comparisons with four recent state-of-the-art models: PolyFormer, CoBFormer, and Polynormer (graph transformers), and SpecFormer (a spectral GNN). Because the literature does not report results on all of our datasets, we reran these baselines under our standardized training pipeline and hyperparameter-tuning setup to ensure a fair and consistent comparison. These **new results have been added to Tables 1 and 2 (page 7\\) and Figure 3 (page 8\\) of the updated manuscript** (highlighted in blue).\n\nBased on average rank (lower is better), we observe the following trends:\n\n- **Heterophilic datasets:** `GraphGPS-BTS < NAGphormer-BTS < Polynormer < GT-BTS < GraphGPS < … `\n- **Homophilic datasets:** `PolyFormer < GraphGPS-BTS < NAGphormer-BTS < SpecFormer < GT-BTS < …` \n- **Overall:** `GraphGPS-BTS < PolyFormer < NAGphormer-BTS < Polynormer < GT-BTS < …`\n\nGraphGPS goes from being ranked 6th overall to 1st, when equipped with BTS. Notably, GT, the simplest transformer in our study, improves dramatically, going from being ranked 12th overall to 5th. Overall, the BTS variants of GT, NAGphormer, and GraphGPS not only outperform their respective base models but also remain highly competitive with recent state-of-the-art transformer and spectral approaches. \n\n\\\n**3. On graph classification and other tasks:** Several reviewers note that our analysis does not directly extend to tasks such as graph classification or link prediction. We would like to clarify the motivation and scope of our work. We focus specifically on **node classification**, because this is the setting where graph transformers have historically struggled to match the performance of MPNNs and spectral GNNs \\[1, 2\\]. \n\nOur goal is to demonstrate that graph transformers, particularly LPE-based ones, have been held back primarily by *how* eigenvectors were selected, and that with a principled selection strategy, they can be highly competitive in the node-classification regime.\n\nOther tasks pose different challenges for injecting positional or structural information, and have been addressed extensively in prior work \\[3, 4\\]. Our contribution is therefore focused and complementary: we provide a theoretical framework and a practical solution tailored to node classification, the regime where graph transformers have been most disadvantaged. There is also strong precedent for this focus, as several prior works have studied only node classification for similar reasons \\[5, 6, 7, 8, 9, 10\\]."}}, "id": "uFgGClki09", "forum": "s1avhccyvg", "replyto": "s1avhccyvg", "signatures": ["ICLR.cc/2026/Conference/Submission22146/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22146/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission22146/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704852639, "cdate": 1763704852639, "tmdate": 1763748002919, "mdate": 1763748002919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}