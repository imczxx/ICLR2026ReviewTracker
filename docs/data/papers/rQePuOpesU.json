{"id": "rQePuOpesU", "number": 13069, "cdate": 1758213257640, "mdate": 1759897467431, "content": {"title": "AutoFold: Ultra-fast Protein Generation via Autoregressive Contact Graph Generation", "abstract": "Generative models for protein design, particularly diffusion and flow matching approaches, are powerful but computationally expensive, with slow sampling times that hinder high-throughput applications. We introduce AutoFold, an ultra-fast autoregressive model that generates proteins via a sparse graph representation of their structure. Instead of generating continuous coordinates directly, AutoFold learns a contact graph of the backbone structure. A Vector Quantized Variational Autoencoder is trained to discretize contacting inter-residue geometric features, creating a graph representation with single edge labels invariant to SE(3) transformations. This representation can be decoded to reconstruct a backbone structure with high fidelity. We then train an autoregressive model to generate these graphs, further incorporating amino acid sequence into node attributes. Our trained model can be seamlessly used for both unconditional generation and motif scaffolding. Our results demonstrate that AutoFold achieves performance comparable to state-of-the-art methods while accelerating sampling by over an order of magnitude. By shifting generation from continuous coordinates to discrete graphs, AutoFold opens the door to high-throughput, large-scale protein design applications.", "tldr": "We propose a graph-based autoregressive model for protein sequence and structure co-generation", "keywords": ["protein design", "motif scaffolding", "graph generative models", "autoregressive models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c2e18f5b19f645f525e5c6a03add503381a81e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces AutoFlow, a model for co-design of protein backbones and sequences. The main idea behind the model is to train an autoencoder for protein backbones and sequences, and perform generation autoregressively in the latent space of the autoencoder. By doing this, the model can enforce SE(3) equivariance, and generates proteins with lower inference times than methods that work on 3D space. The authors show results that are competitive with other existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**: The idea of doing co-design in the latent space of an autoencoder, instead of in 3D space, is a welcome change from the large and ever increasing amount of models that run a generative model on atomic coordinates. \n\n**Quality**: The model performance is comparable to that of the models they compare against (La Proteina, Protpardelle, etc.) despite the lower generation time\n\n**Clarity**: The paper is very well written and presented. The introduction does a good review of the existing literature. The model is described clearly, and figure 1 is a great aid. Results figures and tables are well laid out. The honesty about the weaknesses of the model (e.g. producing too many alpha helices) is very appreciated."}, "weaknesses": {"value": "- Perhaps not a weakness, as much as something that needs to be clarified, but it is not obvious to me what the advantage of faster inference time is for protein design (though I admit there could be advantages I am not aware of). What I mean is, when it comes to protein folding, for example, faster inference means that one could do virtual screening. However (and again, I am ready to be corrected here), my feeling is that if I was going to do in silico design of a protein to perform some function, inference time is not really a bottleneck (especially compared to other testing and trials the designed protein would need to undergo to be useful), as long as the protein I design folds correctly and does what I want it to do. Of course, less inference time means one could scale design to bigger proteins, but aside from that, it is unclear to me what the advantage of the faster inference time would be for practical drug design. Once more, I would like to stress that there are probably applications I am not thinking about here, I would just like to encourage the authors to add some more focus on why this is something that helps drug design.\n- Somewhat related to the point above, but the motivation for the choice of codebook size and distance threshold for the autoencoder is not at all clear. Figure 2 clearly shows that (as expected) larger numbers lead to better performance. Therefore, it would be good to know if the reason is that these settings reach some threshold memory usage, or inference time. Otherwise, why not use settings that lead to better performance? \n- It would have been good to see ca omparison in designability to structure-only models (e.g. RFDiffusion, Foldflow) and also a comparison in sequence quality to sequence only models (e.g. ESM). \n- Some of the results require more detailed explanation (see questions below)"}, "questions": {"value": "- In section 4.3, the authors discuss potential applications in unconditional design and motif scaffolding. Could the approach used for motif scaffolding also be used for binder design? It would be good to add a comment on this. \n- In section 5, experimental setup, it is not clear how the split is done. Is it using mmseqs clustering, for example? \n- In the same section, why is the model trained on alphafold structures only, and no PDB structures are included? \n- In section 5.1, it would be good to understand why the performance on CATH (which should in principle be more out of distribution) is seemingly better than on AFDB \n- In the same section, in figure 2, it would be good to see for which values in the x-axis, the performance plateaus (it seems to continue to get better for the values shown). \n- Another comment to that section and figure, if that is possible, it would be much more useful to view a 2D grid of performance as a function of distance threshold and codebook. \n- Related to a point brought up in the weaknesses section, but the motivation to settle for codebook size 256 and distance threshold 8 is not at all explained. \n- In section 5.2, sequence quality is evaluated through FID of ESM2 embeddings. There exist alternatives to FID that claim to be better (such as Feature Likelihood Divergence, PQMass) it would be interesting to see results on those as well if possible\n- In table 1, the smaller Autofold seems to do better in some categories. It would be good to get an explanation for this behaviour. \n- Similarly, there is no comment about the big gap in designability compared to La Proteina\n- There is also no discussion (as far as I can see) about the choice of parameters for generation, such as temperature, and how they affect these results. \nIn table 2, the results for AutoFold are much better with a distance threshold of 9. This once again raises the question of why 8 was chosen."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3mhSKtRhAt", "forum": "rQePuOpesU", "replyto": "rQePuOpesU", "signatures": ["ICLR.cc/2026/Conference/Submission13069/Reviewer_pLGR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13069/Reviewer_pLGR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831792656, "cdate": 1761831792656, "tmdate": 1762923797433, "mdate": 1762923797433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AutoFold is an autoregressive model for protein sequence and structure generation that operates by generating a sparse, discrete contact graph representation instead of continuous 3D coordinates. It uses a Vector Quantized Variational Autoencoder to encode protein backbone geometry into graph edges and then trains an autoregressive transformer to sequentially generate these graphs, incorporating sequence information as node features. This method enables co-generation and motif scaffolding at speeds over an order of magnitude faster than the leading diffusion and flow matching models while maintaining competitive design quality and diversity. The framework naturally supports zero-shot motif scaffolding and large-scale protein design, though it shows bias toward alpha-helical structures due to its graph representation choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel architecture and use of the VQ-VAE to encode backbone structure. \n- Promising scaling of the transformer decoder compared to prior work.\n- Superior generation speed.\n-Ability to do conditional tasks without finetuning as well as simple sequence co-conditioning coupling\n- Orders of magnitude better novelty (which oddly is not emphasized)"}, "weaknesses": {"value": "- Given that AutoFold is autoregressive and it randomly generate 500 proteins for AutoFold models without explicit length controls, what is the distribution of the lengths used in the benchark for Table 1? Shorter proteins are typically much easier to co-design so the performance many be better or worse when normalized for length.\n- 90% helix and nearly no beta sheet is a major limitation. Especially when ESMFold designability has a strong bias towards helicity.\n- Not the most fair motif benchmark. La Proteina and Protpardelle do all atom motif scaffolding whereas the backbone only motif tasks used would be a better comparison to prior backbone methods benchmarked in Proteina [1] Table 5 albeit AutoFold has a sequence component. Also the motif tasks by design have a length requirement.\n- Missing comparison to MultiFlow https://arxiv.org/abs/2402.04997"}, "questions": {"value": "- Can a length based comparison be made by some amount of oversampling filtering, and binning to evaluate the first K proteins that are close to the length binds of the prior diffusion and flow models?\n- The novelty is orders of magnitude better than every prior method. Any reason as to why? Given the samples are 90% helix it seems odd to see such strong AFDB novelty which is known to be helix saturated.\n- How are the length requirements respected for the motif benchmarking"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7nosk3T2z0", "forum": "rQePuOpesU", "replyto": "rQePuOpesU", "signatures": ["ICLR.cc/2026/Conference/Submission13069/Reviewer_Cd7Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13069/Reviewer_Cd7Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979991691, "cdate": 1761979991691, "tmdate": 1762923796980, "mdate": 1762923796980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoFold, a novel two-stage method for de novo protein structure and sequence co-generation. The core idea is to move from continuous coordinate generation (common in diffusion models) to a discrete, autoregressive process. In the first stage, a VQ-VAE is trained to learn a discrete \"vocabulary\" of geometric features, effectively tokenizing a protein's 3D structure into a sparse latent contact graph. In the second stage, an autoregressive model (based on the AutoGraph framework) is trained to generate these graphs, with amino acid sequences treated as node attributes. The authors claim this approach achieves strong performance on par with state-of-the-art (SOTA) models for unconditional generation and motif scaffolding, while being over an order of magnitude faster to sample."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary strength of this paper is its novel approach. Reframing the protein generation problem as an autoregressive task on a discretized, latent contact graph is an interesting idea that moves away from the dominant diffusion/flow-matching paradigm.\n2. The model demonstrates impressive sampling speed, reportedly orders of magnitude faster than most baselines. This speed, combined with competitive performance on co-designability and novelty, makes it a promising direction for high-throughput design.\n3. The two-stage approach (VQ-VAE for representation, AutoGraph for generation) is a clever way to simplify the generation problem, making it tractable for a powerful sequence-based model. The ability to perform zero-shot motif scaffolding is also an advantage."}, "weaknesses": {"value": "Despite its strengths, the paper suffers from several significant weaknesses, primarily concerning methodological clarity and the experimental setup.\n1. The paper is not self-contained, particularly regarding the core autoregressive mechanism in Section 4.2. The authors state it is built based on an existing framework AutoGraph and refer readers to the original paper. However, this graph generation step is central to the paper's contribution, and its application to the protein domain is non-trivial. The paper would be much stronger if it expanded this section with more detail (moving specifics to an appendix if needed) to explain how the graph is \"flattened\", how the stochastic ordering is generated, and how validity is ensured.\n2. The model is termed \"autoregressive,\" but it does not generate the protein in its natural sequence order. Instead, it uses a stochastic graph traversal order, which appears to treat the protein as a permutation-invariant contact graph. This is a strong, and perhaps incorrect, inductive bias, as proteins are fundamentally not permutation-invariant; their properties are tied to the N-to-C terminal sequence. This choice needs to be explicitly justified and discussed.\n\nExperimental Concerns:\n1. The VQ-VAE's reconstruction RMSD of >0.6A (per Fig. 2, ~0.8A for the chosen 256 codebook size) seems significantly higher than the fidelity of other models (e.g., La-Proteina's VAE at ~0.12A). While \"sub-angstrom\" is good for folding, this level of reconstruction error in the VAE stage may be a weak link.\n2. The primary comparison in Table 1 is potentially unfair. AutoFold is length-unconstrained, while the baselines are length-conditioned. It is known that generating longer proteins is more difficult. Without seeing the length distribution of AutoFold's generated samples, it is impossible to know if it is \"hacking\" the metrics by primarily generating shorter, easier proteins. The fact that all sample visualizations appear short is concerning. The authors must provide this length distribution for a fair comparison.\n3. Several generative models, such as DPLM-2 and MultiFlow, are absent from the table.\n4. The \"unique co-designability\" metric is confusing. Based on the definition, it is equivalent to the number of designable clusters which should be an integer, but the values in Table 1 are not integers. This should be clarified.\n5. The model's strong overrepresentation of alpha-helices (~90%) is a major weakness, which the authors acknowledge. This suggests the model is not capturing the full diversity of protein structures, and it's unclear if this is due to the methodology (e.g., the VQ-VAE representation) or the sampling scheme.\n6. While the model is clearly fast, the claim of \"over an order of magnitude\" faster sampling seems to apply only to older diffusion models (like RFDiffusion), not the SOTA flow-matching model La-Proteina. The speedup over La-Proteina (per Fig. 3) is closer to 3x, which is still good but not an order of magnitude."}, "questions": {"value": "To address the weaknesses above, I have the following questions:\n\n1. **On Graph Generation** (Sec 4.2): Could you please elaborate on the graph flattening process?\n- What is the specific meaning of the '<' and '>' tokens in the sequence in Figure 1?\n- Why are node indices (e.g., node '3') generated multiple times in the example sequence?\n- How exactly is the stochastic ordering generated during training?\n- During inference, how do you ensure the generated token sequence is syntactically valid and decodes to a valid graph?\n- How is the graph sparsity (i.e., the total number of nodes and edges) controlled or learned during generation?\n2. **On Model Choice**: Why use a stochastic graph ordering, which implies permutation invariance, instead of the protein's canonical N-to-C sequence order? Given that this choice seems to discard crucial biological information, what is the authors' interpretation of its impact? What advantages does this specific \"autoregressive\" formulation have over any-order generation models like diffusion?\n3. **On Experimental Results**:\n- Have you investigated the source of the strong alpha-helical bias? Is it a consequence of the low-temperature sampling, or is it an inherent bias of the VQ-VAE representation, which may find it easier to tokenize helices?\n- Your model shows higher co-designability than PMPNN-8 designability, a result hardly seen in previous models. What is your interpretation of this? Is it possible the model is \"hacking\" this metric by overrepresenting specific residue types that are \"easy\" to co-design (which might also explain the alpha-helix bias)?\n- The model achieves both good co-designability and high novelty. This is an excellent result. Do you have an interpretation for why this method is particularly good at finding novel, valid designs?\n\nIn general, I think this paper presents a good, novel idea with promising results. However, the paper is currently held back by a significant lack of methodological detail and a few concerning issues in the experimental setup (length bias, alpha-helix bias, metric clarity). I'm happy to discuss more during the rebuttal process and increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E68hAkZnZm", "forum": "rQePuOpesU", "replyto": "rQePuOpesU", "signatures": ["ICLR.cc/2026/Conference/Submission13069/Reviewer_dGv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13069/Reviewer_dGv6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762444363128, "cdate": 1762444363128, "tmdate": 1762923796677, "mdate": 1762923796677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a two-stage de novo protein generation pipeline that first trains a VQVAE on protein frames, and then uses the resulting codebook to autoregressively generate 3D protein backbone structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The codebooks in VQ-VAEs are usually unstable/erratic to train but AutoFold's reconstruction low recon-RMSD of 0.7 angstrom is noteworthy.\n\n2. The generation speed of proteins with AutoFold is fast compared to existing SOTA methods, which could be useful in high-throughput design pipelines."}, "weaknesses": {"value": "1. The baselines all operate at an all-atom resolution, generating the sidechain atoms/identities as well. AutoFold only operates on the backbone resolution (CA, C, N, O), which makes the modeling task much easier. The community has moved on from backbone-only generation for quite some time now. Could the authors comment on efforts / potential extensions to convert it into an all-atom model?\n\n2. Usually, there is a huge trade-off between Designability (% foldable by ESMFold) and Novelty metrics. If a protein design method generates a very unique backbone, it means P-MPNN might have some difficulty producing a sequence that's likely to fold into it. This error further propagates to ESMFold that may predict a 3D structure that has low self-consistency with the generated backbone. This means it's less likely to meet the designability threshold of scRMSD<2 angstroms despite being very novel. I think this might be happening here: for the best-performing AutoFold model with a good balance of scores, the all-designability metric (50.4) is rather low compared to current SOTA, La Proteina (72%), but has higher novelty (0.718 vs 0.571 respectively). It's difficult to offer an apples-to-apples comparison between models when this is the case. \n\n3. The community is more concerned with the Designability and Novelty metrics being high than inference speed. La Proteina-like methods are already pretty fast for large sequence lengths, and AutoFold is not that much faster, especially for proteins of size ~800. Also, given the autoregressive nature of AutoFold, I'm skeptical if AR-style generation is really superior than diffusion/flow-style generation."}, "questions": {"value": "1. I'm curious to see the % of secondary structural elements like alpha helices and beta sheets. The issue with autoregressive protein design models (eg: FoldingDiff [1]) is that they have this tendency to generate multiple helices because they are more well-represented than loops and beta sheets. Perhaps a figure like Figure 3 in [FoldFlow2's paper](https://arxiv.org/abs/2405.20313) would be useful in showing us AutoFold's local structural diversity as well.\n\n2. All of these protein design methods (including AutoFold) are trained on different subsets of the PDB/AFDB, which makes the comparison problem even harder since some methods are more capable than others in designing novel structures. \n\n3. The authors mention they have a sequence length cut-off of 256 residues. **To convince me further, I'd be interested in seeing how this method scales to larger proteins > 300 residues** (if time permits). Authors mention their method can be scaled up to such lengths, but there's no evidence of this. Furthermore, La Proteina trains an all-atom model upto 896 residues (understandable, given that it's NVIDIA and they might have the compute resources) and still performs really well. Authors provide Figure 3 detailing the inference speed for increasing lengths. I'd want to also see how designability and other metrics differ as AutoFold is scaled up to increasing lengths. A figure of seq-len vs average designability (vs La Proteina) would be very convincing.\n\n[1] Protein structure generation via folding diffusion. Wu et al. (2022)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lOQHOn2hho", "forum": "rQePuOpesU", "replyto": "rQePuOpesU", "signatures": ["ICLR.cc/2026/Conference/Submission13069/Reviewer_NzJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13069/Reviewer_NzJQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762458537901, "cdate": 1762458537901, "tmdate": 1762923796382, "mdate": 1762923796382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoFold, a novel autoregressive generative model for de novo protein design that addresses the computational bottlenecks of existing diffusion and flow-matching approaches. By representing protein backbones as sparse, discrete contact graphs learned via a VQ-VAE (achieving ~0.8 Å RMSD reconstruction fidelity), and then generating these graphs autoregressively using a transformer-based framework adapted from AutoGraph, the method enables ultra-fast sampling (over an order of magnitude faster than baselines like RFDiffusion). It supports unconditional co-generation of sequence and structure, as well as zero-shot motif scaffolding. Evaluations on AFDB-derived datasets show competitive co-designability (~72%), diversity, and novelty compared to state-of-the-art models like La-Proteina, though with a noted bias toward alpha-helical structures. The authors plan to release code, models, and protein graph datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Efficiency Gains: The reported speedups (e.g., 100x faster than RFDiffusion on H100 GPUs) are compelling for high-throughput applications. By flattening graphs into sequences, the model scales well with transformer architectures, and the use of constrained decoding ensures valid outputs.]\n2. Strong Results: On unconditional generation, AutoFold-m achieves higher unique co-designability (46.4%) and better sequence FID (~9.07) than baselines, with good novelty against PDB/AFDB. For motif scaffolding, it solves 18/20 tasks in the benchmark, outperforming La-Proteina on flexible-length tasks (102 vs. 79 unique successes)"}, "weaknesses": {"value": "The strong preference for alpha-helices (~90% content) limits diversity, potentially due to the VQ-VAE's codebook favoring frequent local patterns. While acknowledged, more analysis (e.g., codebook utilization histograms or β-sheet-specific reconstructions) could strengthen the discussion. Comparisons to curated datasets (as in La-Proteina) might reveal if this is data- or model-induced.\n\nThe authors investigate the impact of graph sparsity by fixing the codebook size to 256 and varying the distance threshold. A fuller exploration of threshold impacts on generation complexity and speed would be useful."}, "questions": {"value": "The unconditional generation results show a strong bias toward alpha-helical structures (~90% helix content). What steps could mitigate this for more diverse topologies? Fuller exploration on what's the root cause of the strong preference would be interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tdioHIoPqX", "forum": "rQePuOpesU", "replyto": "rQePuOpesU", "signatures": ["ICLR.cc/2026/Conference/Submission13069/Reviewer_jUAk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13069/Reviewer_jUAk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762471123065, "cdate": 1762471123065, "tmdate": 1762923795810, "mdate": 1762923795810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}