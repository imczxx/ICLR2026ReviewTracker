{"id": "EVZMZQogUm", "number": 5787, "cdate": 1757935185598, "mdate": 1763710168809, "content": {"title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better", "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: Can we leverage existing small pretrained models to accelerate the training of larger models? In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6× speedup with nearly 5% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10x fewer parameters than the target model.", "tldr": "We propose a Late-to-Early Training (LET) method that lets LLMs explicitly learn later knowledge in earlier steps and earlier layers, leading to faster and improved LLM training.", "keywords": ["Large Language Models", "Efficient Training", "Representation Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38c2d921e19565d594a52646a2e93fd20bc5c426.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors address the problem of utilizing existing pre-trained smaller models to train larger models. To do this they propose to use KD, which would be an architecture agnostic form of upcycling. Extensive results show improved training using this technique. The main novelty lies in guiding the earlier layers using the late layers of the smaller pre-trained model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using smaller pre-trained models to accelerate training larger models is a very interesting and relevant problem statement. The proposed approach of using reverse distillation from early to late layers makes a lot of sense and the authors demonstrate is very effective. I don't see any issues with the experimental section or the theory, which is relatively thorough and clear.\n\nThe results are shown on large scale models (which is important for this problem statement) and show clear and strong empirical results. The application of KD to this problem is new and interesting."}, "weaknesses": {"value": "Unfortunately, the authors claim of being the first to leverage smaller pre-trained models to train larger LLMs is not true. There is a relatively new technique called upcycling, which has been used extensively in industry and academia [1]. I would really encourage the authors to include a thorough discussion on these works and related techniques. As far as I know, the authors approach for using reverse knowledge distillation is indeed novel, but the problem statement itself is not. Finally, I do understand that the proposed KD technique makes upcycle architecture agnostic and this is a benefit which should be highlighted in this work over prior upcycle techniques.\n\nKD has been shown to be more data efficient than training a model from scratch [2,3,4,5,6]. I would encourage the authors to add this to the discussion and analysis for *why* LET is able accelerate training the larger LLMs.\n\nIn summary, I think the paper is missing further analysis with respect to prior works in the KD literature and the novelty of the problem statement is a bit overclaimed. Including a more thorough related work discussion to prior upcycle techniques would be good. If the authors make this these changes I would be very happy to increase my score.\n\n[1] Scaling Laws for Upcycling Mixture-of-Experts Language Models. PMLR 2025\n\n[2] Understanding the Role of the Projector in Knowledge Distillation. AAAI 2024\n\n[3] Training data-efficient image transformers & distillation through attention. PMLR 2021\n\n[4] VkD : Improving Knowledge Distillation using Orthogonal Projections. CVPR 2024\n\n[5] Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. CVPR 2022\n\n[6] DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. CVPR 2022"}, "questions": {"value": "Although only a small modification, it would be interesting to see the results using layer norm and a smooth l1 or logsum loss [2]. These are shown to be more effective for distillation and I am curious if these results extend to the authors proposed setting here. In general, fitting this work into the recent knowledge distillation literature would really strengthen the submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tbFBCpFLhC", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Reviewer_srWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Reviewer_srWB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300959037, "cdate": 1761300959037, "tmdate": 1762918261391, "mdate": 1762918261391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the desire to re-use smaller open-source LLMs to train larger LLMs efficiently, the authors propose a new method (LET) where a smaller model guides a larger model in order to achieve better performance earlier, reversing the original size relation between student and teacher.  While previous works proposed similar strategies with smaller teachers, the scaling size between the small teacher and the large student has remained small so far. In this work, the authors are able to train 10x larger models by introducing two key modifications: 1) the small teacher is used to align early layer representations of the larger student with its later layers (rather than the logits), 2) this alignment guidance needs to be stronger at the beginning of training and subsides as the larger model gets more capable later in training. The methods is thoroughly tested experimentally."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The writing is very clear\n* The experiments are comprehensive with well designed ablation studies\n* The reported performance of the method is significant ( \"[the method] exceeds the baseline’s average performance while requiring less than 67% of the training steps even with 10× smaller model\")\n* The method does not require architectural compatibility between the student and the teacher"}, "weaknesses": {"value": "* While I am not seeing this as a significant weakness (because of the detailed experimental evidences) the proposed method is lacking theoretical backing."}, "questions": {"value": "* Can the authors comment on theoretical reasons that could underpinne this method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cldFNK3lpp", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Reviewer_dPjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Reviewer_dPjE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444411502, "cdate": 1761444411502, "tmdate": 1762918261015, "mdate": 1762918261015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the desire to re-use smaller open-source LLMs to train larger LLMs efficiently, the authors propose a new method (LET) where a smaller model guides a larger model in order to achieve better performance earlier, reversing the original size relation between student and teacher.  While previous works proposed similar strategies with smaller teachers, the scaling size between the small teacher and the large student has remained small so far. In this work, the authors are able to train 10x larger models by introducing two key modifications: 1) the small teacher is used to align early layer representations of the larger student with its later layers (rather than the logits), 2) this alignment guidance needs to be stronger at the beginning of training and subsides as the larger model gets more capable later in training. The methods is thoroughly tested experimentally."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The writing is very clear\n* The experiments are comprehensive with well designed ablation studies\n* The reported performance of the method is significant ( \"[the method] exceeds the baseline’s average performance while requiring less than 67% of the training steps even with 10× smaller model\")\n* The method does not require architectural compatibility between the student and the teacher"}, "weaknesses": {"value": "* While I am not seeing this as a significant weakness (because of the detailed experimental evidences) the proposed method is lacking theoretical backing."}, "questions": {"value": "* Can the authors comment on theoretical reasons that could underpinne this method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cldFNK3lpp", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Reviewer_dPjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Reviewer_dPjE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444411502, "cdate": 1761444411502, "tmdate": 1763732633727, "mdate": 1763732633727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The **Late-to-Early Training (LET)** paper proposes a new pre-training method that uses small, existing models to accelerate the training and improve the performance of new, larger models . Its core mechanism involves two parts:\n\n1. **Late-to-Early-Layer (L2E) Learning:** It aligns the internal representations from the **final (late) layer** of the small teacher model with an **early layer** of the large student model .\n2. **Late-to-Early-Step (L2S) Learning:** This alignment acts as a temporary guide during the **early training steps** only, fading to zero at a set point (Sstop) .\n\nThis L2E + L2S design prevents the small teacher from bottlenecking the larger model, allowing it to serve as a \"bootstrap\" rather than a \"ceiling\" . Experiments show this method achieves up to **1.6x faster** convergence while also yielding **higher final accuracy** than standard training, even when the teacher model is 10x smaller ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper demonstrates that knowledge distillation (KD) onto a large model is possible using a teacher model that is 10x smaller. By applying KD only during the initial phase of pre-training, not every step, the computational cost does not persist throughout the entire training process . The paper presents results showing that this method achieves higher performance compared to standard training without knowledge distillation ."}, "weaknesses": {"value": "- **Insufficient Baseline Comparisons:** \nThe paper compares against **standard training** and **RKD**, but omits head-to-head evaluations with **large-teacher, logits-based KD**, strong **offline KD** pipelines, and recent **data-selection / model-growth** accelerators. Adding **wall-clock–normalized** and **peak-VRAM–normalized** comparisons to these families would more clearly position LET.\n- **Size of the L2E Advantage:**\n    \n    While Figures 3–4 **suggest** L2E > L2M/L2L, the **visual gaps appear modest**. Please report **exact end-of-training deltas, variance, confidence intervals,** and (where feasible) **repeat runs** to rule out noise. In early steps, L2M/L2L sometimes exceed L2E; clarifying **why early-layer alignment should win eventually** (with theory-backed or empirical ablations) would strengthen the claim.\n    \n- **Relation to Offline KD:**\n    \n    Prior **offline KD** reports (e.g., MiniPLM, ICLR 2025) claim **2.2×** speedups against their baselines, whereas LET reports **1.6×** against standard training. Unlike LET’s **dual-model forward** in early steps, offline KD typically adds **no per-step training overhead** (though it may incur preprocessing cost). A **fair, wall-clock** comparison—controlling for hardware, batch size, and token budgets—would clarify the net efficiency trade-offs.\n    \n- **Training Overhead Transparency:**\n    \n    LET requires **co-loading teacher and student** and **forwarding both** during the early phase, introducing **VRAM pressure** and a **throughput hit** (the paper notes roughly **~8%** slower throughput). Please include **end-to-end wall-clock curves**, **tokens/sec**, **peak VRAM**, and **batch-size regressions** across teacher sizes to show that faster convergence outweighs this overhead.\n    \n- **Heuristic Dimension Alignment:**\n    \n    To resolve teacher–student hidden-size mismatch, the method uses **1-D linear interpolation** of teacher hidden states followed by **cosine-similarity alignment**. The paper should justify what **semantics are preserved** by this interpolation and compare against stronger baselines (e.g., **learned linear projections**, **CCA/Procrustes**, or **adapter heads**) and **tokenizer-mismatch** settings to demonstrate robustness."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wpxAotI6n1", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Reviewer_ec8P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Reviewer_ec8P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963070477, "cdate": 1761963070477, "tmdate": 1762918259651, "mdate": 1762918259651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Late-to-Early Training (LET), which leverages small pre-trained models to accelerate the pre-training of larger  models.\nThe authors pose the practical question of whether existing small pre-trained models can guide and speed up the early learning of larger target models.\n\nThe core idea of LET is to use representations from the late layers of a pre-trained model to guide the early layers of the target model during early training steps.\nThe method consists of two mechanisms: Late-to-Early Step Learning and Late-to-Early Layer Learning.\nThese mechanisms aim to accelerate training convergence and improve both language modeling capability and downstream task performance.\nThe contributions are summarized in three points.\nFirst, the study formulates the previously underexplored problem of generally accelerating the pre-training of much larger LLMs (e.g., 10×) using small pre-trained models.\nSecond, it proposes the LET paradigm with the two mechanisms above and states that LET is architecture-agnostic.\nThird, it provides extensive experiments showing that LET achieves faster training and superior downstream performance compared to standard training.\n\nThe experiments evaluate 1.4B- and 7B-parameter models using perplexity on The Pile and accuracy on nine downstream tasks: HellaSwag, WinoGrande, LAMBADA, OpenBookQA, ARC-easy, ARC-challenge, PIQA, SciQ, and BoolQ.\nFor the 1.4B model, teachers such as OPT-125M, Pythia-160M, and SmolLM-135M yield consistent gains, with up to 1.6× training speedup on The Pile and nearly 5% improvement in downstream accuracy; for the 7B model, using Llama-3.2-1B as the teacher also provides faster training and higher final performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The core mechanisms are clear (late-to-early-step / late-to-early-layer).\nIt formalizes two mechanisms, using a teacher’s late-layer representations to guide a student’s early layers, and applying this guidance only in early training steps with a decaying schedule, yielding a reproducible training recipe.\n\n* The approach is architecture-agnostic and effective with small teachers.\nBecause alignment is performed on hidden states, the method imposes minimal architectural constraints and remains effective even when the teacher is 10× smaller than the target model, thereby increasing practical reusability of open pretrained assets.\n\n* It demonstrates robustness across teacher families and sizes.\nUsing heterogeneous small teachers (e.g., OPT-125M, Pythia-160M), LET consistently accelerates convergence and improves accuracy, indicating method-level robustness beyond a single family.\n\n* It demonstrates practical impact under constrained compute.\nUnder identical token budgets, LET-1.4B surpasses a baseline 3B model in downstream performance, highlighting the advantage of better training dynamics rather than brute-force scaling."}, "weaknesses": {"value": "* There is a dependence on teacher quality.\nAlthough LET works with small teachers, using weak or domain-mismatched teachers may inject harmful biases into the early layers, potentially leading to negative distillation effects.\nIt would be better to also discuss the situations in which the proposed method does not work well.\n\n* The breadth and strictness of baselines could be improved.\nWhile several baselines are covered, more stringent comparisons under identical token/compute/data budgets with the latest pretraining acceleration approaches (e.g., strong online distillation or growth strategies) would further solidify the claim of superiority.\n\n* Theoretical grounding is limited.\nThe paper would benefit from deeper analysis of why late-to-early (in both depth and time) works, e.g., representation geometry, optimization landscape smoothing, or gradient noise reduction, perhaps via simplified models or convergence sketches."}, "questions": {"value": "* Reductions in convergence steps do not automatically guarantee total wall-clock or cost gains once teacher feature extraction, caching, and (in distributed setups) communication overheads are included. \nCan authors provide such information?\n\n* Evidence at 1.4B/7B is promising, but it remains unclear how late -> early alignment behaves for tens of billions to hundreds of billions of parameters, especially under architectural mismatches (e.g., LayerNorm variants, depth discrepancies).\nWhat could the authors add regarding this point? (this does not mean to perform the experiments of large models)\n\n* Finer-grained ablations would be valuable.\nMore exhaustive studies disentangling late-to-early-step vs. late-to-early-layer effects, which late teacher layer(s) to use, the student’s matched layer(s).\n\nI am willing to update the overall scores when the authors clearly answer my concerns and questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FOpwvc6QU8", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Reviewer_4pPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Reviewer_4pPU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998294642, "cdate": 1761998294642, "tmdate": 1762918259004, "mdate": 1762918259004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Appreciation to all Reviewers"}, "comment": {"value": "### Dear Reviewers,\n\nWe would like to extend our sincerest gratitude to all of you for your time and dedicated effort in reviewing our manuscript. We deeply appreciate your insightful comments and constructive suggestions.\n\nWe are encouraged to see that the reviewers recognize the **comprehensive and well-designed experiments** (Reviewers 4pPU, dPjE, srWB), the **novelty of LET** (Reviewer srWB), the **practical significance** of our method LET, including its architecture-agnostic design, effectiveness even with small model alignment, and robustness across diverse families (Reviewers 4pPU, ec8P, dPjE), and the **clarity of our presentation** (Reviewers 4pPU, ec8P, dPjE, srWB).\n\nIn response to your valuable feedback, we have made the following revisions to our manuscript:\n\n- Added comparison experiments with SALT in Table 1\n- Conducted failure mode analysis\n- Performed fine-grained ablation studies on layer selection\n- Provided detailed analysis of throughput, wall-clock time, and peak VRAM\n- Conducted in-depth investigation of alignment methods, including CCA, logsum, and tokenizer-mismatch settings\n- Expanded the discussion of prior works\n\nWe have given careful consideration to all the comments raised and have responded to each of them in the clear and detailed response below. To facilitate your review of the revised manuscript, we have highlighted all major revisions and newly added experimental results in blue.\n\nWe believe these revisions have substantially strengthened the paper and effectively addressed your concerns. We sincerely look forward to your feedback and welcome any further discussion.\n\n\nBest regards,\n\nSubmission 5787 Authors"}}, "id": "dBf1fMIQso", "forum": "EVZMZQogUm", "replyto": "EVZMZQogUm", "signatures": ["ICLR.cc/2026/Conference/Submission5787/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5787/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission5787/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720044377, "cdate": 1763720044377, "tmdate": 1763720044377, "mdate": 1763720044377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}