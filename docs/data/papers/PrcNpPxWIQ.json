{"id": "PrcNpPxWIQ", "number": 9467, "cdate": 1758123524036, "mdate": 1763716879775, "content": {"title": "GenFlow: Constrained Long-Form Text Generation via Adaptive Workflow Optimization", "abstract": "Large Language Models (LLMs) exhibit strong abilities in generating coherent human-like text, yet producing long-form content that satisfies complex constraints remains challenging. Existing approaches either extend generation length through large curated datasets, as in LongWriter, or structure outputs via cognitive-inspired hierarchical planning, as in CogWriter, but often struggle to balance coherence, semantic fidelity, and explicit requirements. In this work, we propose GenFlow, an adaptive framework for constrained long-form text generation. It decomposes writing objectives into constraint-aware sub-plans, uses adaptive decision-making and reward filtering to retain high-quality plans, and optimizes both local and global generation. By embedding constraints directly into the workflow, GenFlow ensures consistency while adapting to evolving requirements. Experimental results on the Qwen2.5 series demonstrate that GenFlow outperforms GPT-4o-mini and CogWriter baselines in constraint satisfaction, coherence, and overall quality.", "tldr": "GenFlow is an adaptive framework for constrained long-form text generation that outperforms GPT-4o-mini and CogWriter in constraint satisfaction, coherence, and quality.", "keywords": ["LLM", "Constrained Long-Form Text Generation", "Agentic WorkFlow"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79506ac58b374dcced4acf58c6faa6c19be5d2f5.pdf", "supplementary_material": "/attachment/63222babfa3755c8c9ca3c0ad8dd25ff6abb6e9a.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors proposes a framework \"GenFlow\" for constrained long-from text generation. In this framework, adaptive decision-making and reward filtering is used to retain high quality plan and generations. According to the experiments on LongGenBench-16k, Qwen2.5 series with GenFlow outperforms GPT-4o-mini and other baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem motivation and improvements:\nThe idea of reframing long-form generation as an adaptive workflow optimization problem is interesting. It used RL-style optimization paradigms to improve constrain satisfaction. And in the experiment, large improvements are shown in LongGenBench-16k.\n\n2. use of reward signals and DPO:\nThe combination of rollout-based reward estimation and Direct Preference Optimization (DPO) for aligning hierarchical sub-plans is technically well-motivated and grounded in recent progress in reinforcement learning for LLMs.\n\n3. Unified integration of constraints.\nGenFlow embeds constraint-awareness at every stage—planning, filtering, and generation—resulting in a coherent system architecture that’s both modular and interpretable."}, "weaknesses": {"value": "1. One key metric (\"Main Task Completion Rate\") is missing. In the work, only constrained metric is reported. There is no any related metric to check whether designated subtasks are completed in sequence. It is hard to conclude the proposed method is better.\n\n2. In the experiment, only one model family (\"Qwen2.5\") is used. It is also hard to conclude whether the method is good. It is better to add experiments for other models (Llama, mistrial etc).\n\n3. The reward is described abstractly. Details about weighting between constraint satisfaction, coherence, and fluency are missing, which limits reproducibility. And how reliable of these rewards are unclear.\n\n4. Some figures are hard to read. In some figures, for example, Figure 8, some lines are hard to be distinguished or read."}, "questions": {"value": "1. Reward composition:\nHow exactly is the final reward computed? What are the relative weights between constraint satisfaction, coherence, and fidelity?\n\n2. Human Judgement\nHave you conducted or considered pairwise human evaluations to validate that GenFlow’s outputs are indeed more coherent or faithful?\n\n3. \nHow is “feedback” operationalized? Is it gradient-based (learned) or heuristic (e.g., reward prompts)? Clarifying this would help readers understand training dynamics.\n\n4.  What are static vs. Dynamic Filtering? There is no detailed description for two settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CtdNeYBzkx", "forum": "PrcNpPxWIQ", "replyto": "PrcNpPxWIQ", "signatures": ["ICLR.cc/2026/Conference/Submission9467/Reviewer_MtBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9467/Reviewer_MtBy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609973971, "cdate": 1761609973971, "tmdate": 1762921057349, "mdate": 1762921057349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GenFlow, a workflow-centric framework for constrained long-form generation. \n\nIt builds constraint-aware hierarchical plans, applies a binary relevance filter over six criteria, and uses rollout-based rewards plus DPO to guide both planning and segment-level generation. \n\nExperiments predominantly use Qwen2.5 and compare against CogWriter and GPT-4o-mini variants on tasks inspired by LongGenBench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Pipeline view** (planning → filtering → reward-guided generation) is a coherent design and maps well to practical systems.\n* **Segment-level training signal** (rollouts + DPO) is a plausible way to align local segments with global constraints."}, "weaknesses": {"value": "* **Theory is under-specified.** Propositions 1–3 read as tautologies (e.g., “the probability of invalid plans → 0 as revisions iterate”) without a formal model, explicit assumptions, or a convergence argument. The appendix proofs are difficult to follow.\n\n* **Problem statement near Eq. (4) is unclear.** Define all symbols (plans, sub-plans, local constraints/contexts) precisely and state the generative process and independence assumptions before presenting the factorization.\n\n* **Eq. (10) is not a probability.** As written, it evaluates to {0,1}. If you intend a probability, model each criterion as a Bernoulli with parameter (p_k) (and justify independence) so (\\Pr(\\delta_i=1)=\\prod_k p_k); otherwise, rewrite as a joint indicator.\n\n* **Figure 2 label.** Clarify what “ConFlow” refers to (typo for GenFlow or a distinct method?).\n\n* **Citations and formatting.**\n\n  * Use `\\citep{}` (author–year parenthetical) instead of `\\cite{}` throughout; only the first paragraph of the Introduction may warrant `\\citet{}` for narrative flow.\n  * Fix mismatches and duplicates: Brown et al. (2021) NeurIPS should not have an ICASSP-style DOI; Flower & Hayes (1981) duplicates (1981a/1981b) with odd URLs—merge and cite correctly; OPT (Zhang et al., 2022) needs a consistent PMLR volume/pages/URL. Ensure all entries have the correct venue, year, DOI/URL, and consistent style."}, "questions": {"value": "The manuscript requires a thorough proofreading. Once that’s done, I’m willing to give constructive feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TKX4LjjDDi", "forum": "PrcNpPxWIQ", "replyto": "PrcNpPxWIQ", "signatures": ["ICLR.cc/2026/Conference/Submission9467/Reviewer_PFkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9467/Reviewer_PFkG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890268084, "cdate": 1761890268084, "tmdate": 1762921057079, "mdate": 1762921057079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Adaptive Workflow Optimization framework, named GenFlow, for the task of constrained long-form text generation. The framework decomposes the generation objective into three stages: Hierarchical Planning, Adaptive Workflow Execution, and Reward-based Filtering and Decision Making. Specifically, Hierarchical Planning is responsible for breaking down the generation goal into multiple candidate sub-plans, ensuring they satisfy the given constraints and context. Adaptive Workflow Execution carries out the generated plans, evaluates their quality, and makes optimization adjustments based on predefined thresholds. Finally, Reward-based Filtering and Decision Making ranks and filters the various sub-plans to select the optimal solution for the final output.\n\nThe paper validates and tests the proposed method on the LongGenBench-16K dataset. The results demonstrate that, compared to the Cogwriter method, GenFlow achieves significant advantages across the Qwen2.5 Instruct series of models, including the 0.5B, 1.5B, and 7B parameter sizes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper proposes a novel adaptive framework, GenFlow, for long-form text generation. It utilizes a Global Planning module to ensure the generated plans satisfy global constraints, while a binary relevance filtering mechanism enforces semantic quality, effectively complementing structural and constraint validation.\n\n2.  The authors introduce an end-to-end Direct Preference Optimization (DPO) strategy capable of simultaneously optimizing both the planner and the generator."}, "weaknesses": {"value": "1.  The writing of this paper requires improvement, as it lacks rigor and logical flow:\n\n\n1.1 Citation formatting is inconsistent, which hinders readability. For instance, the citation `\"......builds on Cognitive Writing Theory Flower & Hayes (1981b),......\"` (Line 051) should be formatted as `\"......builds on Cognitive Writing Theory (Flower & Hayes, 1981b),......\"`. This issue occurs multiple times throughout the paper, significantly impacting the reading experience.\n\n\n1.2 In Lines 035-043, the authors directly introduce the proposed method, then later discuss the background, and finally reiterate the method again. This structure creates a confusing and illogical flow.\n\n\n1.3 Figure 2 mentions three methods: `ConFlow`, `Long Writer`, and `CogWriter`. However, only the latter two are explained; `ConFlow` is neither introduced nor cited. Is this a typographical error, and should `ConFlow` actually be `GenFlow`?\n\n\n1.4 The meanings of many variables in the formulas are not explained. This includes, but is not limited to, the variable `x` in the formula around Line 132, and `T_{single}`, `T_{range}`, `T_{periodic}` around Line 186, etc.\n\n\n1.5 Figure 3 is blurry and appears to contain errors. In the bottom-left \"Main workFlow\" section, \"Range Instruction\" appears twice. According to the description in Line 186, one of these should likely be \"Single\"?\n\n\n1.6 Based on the example in Figure 3, the overall method comprises both an inference process and a training process. However, the writing in Sections 4.1-4.3 not only fails to align with Figure 3 but is also logically obscure and difficult to follow. The methodology description should ideally correspond to the figure.\n\n2.  The method lacks necessary details:\n\n\n2.1 The calculation of the Reward used for the end-to-end DPO is not provided.\n\n\n2.2 Implementation details are insufficient. Appendix G does not offer significantly more detail than the \"Implementation Details\" section (Lines 314-319) in the main text.\n\n3.  The experimental setup is problematic:\n\n\n3.1 The number of baselines is insufficient. Besides Cogwriter, the authors also mention Long Writer (Line 88) earlier in the paper. Why is no comparison conducted with Long Writer?\n\n\n3.2 Where do the experimental results for the `gpt-4o-mini + Cogwriter` method in Table 1 (Accuracy One 0.4918, Accuracy Range 0.4026, Accuracy Periodic 0.2386) originate from? Why are these results significantly lower than those reported in the original Cogwriter paper's Table 1 (which reports scores of 0.80, 0.76, and 0.67, respectively)? The authors should provide a further explanation for this discrepancy.\n\n\n3.3 The experimental comparisons are unfair. Firstly, Cogwriter is a training-free method, whereas the proposed GenFlow is trained on the LongGenBench-16K dataset. Secondly, Cogwriter has not been experimentally validated on the Qwen2.5 series of models. Did the authors make any adaptations when replicating the Cogwriter method for these models? Finally, Cogwriter was tested on the Llama-3.1-8B-Instruct model, which has a parameter size similar to the Qwen2.5-7B model used in this paper. To ensure a fairer comparison, the authors should supplement their results with experiments conducted on the Llama-3.1-8B-Instruct model."}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sFmRwUND7x", "forum": "PrcNpPxWIQ", "replyto": "PrcNpPxWIQ", "signatures": ["ICLR.cc/2026/Conference/Submission9467/Reviewer_fuJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9467/Reviewer_fuJw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971929475, "cdate": 1761971929475, "tmdate": 1762921056625, "mdate": 1762921056625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GenFlow, an adaptive workflow optimization framework designed for constrained long-form text generation. It addresses the challenge of creating long, coherent content that satisfies multiple complex constraints by integrating planning, decision-making, generation, and feedback into a unified loop."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work integrates planning and generation in a unified, adaptive workflow.\n2. The methodology boosting constraint satisfaction for most settings, especially for complex Range and Periodic constraints.\n3. Outperforms baselines (GPT-4o-mini, CogWriter) in accuracy, especially when combined with larger base models (Qwen2.5-7B)"}, "weaknesses": {"value": "1. The hierarchical planning and reward optimization processes introduce additional computational overhead and longer generation times. The inherent increase in latency limits suitability for real-time applications.\n2. The paper names LongWriter as a primary related work and even distinguishes GenFlow's approach from it, but LongWriter is omitted from the quantitative comparison tables (Tables 1 and 2). This leaves the core claim—that GenFlow outperforms data-centric approaches—unverified by the provided experimental evidence.\n3. The primary evaluation is focused entirely on instruction following accuracy (Single, Range, Periodic). While this is essential for constraints, it neglects quantification of subjective metrics mentioned in the abstract, such as overall coherence, semantic fidelity, and quality, which are typically measured using human evaluation or complementary linguistic metrics (e.g., perplexity, fluency scores).\n4. The entire training and evaluation are performed on a synthetic dataset (LongGenBench-16K) , which is entirely generated using LLMs. This creates a closed loop of LLM-generated tasks and LLM-trained solutions, raising questions about the generalizability of the results to real-world writing tasks and human-authored constraints.\n5. In the ablation study (Table 2), the \"+ Planning only\" method resulted in an Average Accuracy (0.3643) lower than the Base Model (0.401). This suggests that the hierarchical planning strategy alone is detrimental or confusing to the base model, and the significant gains are almost entirely attributed to the Generation module's DPO refinement, which contradicts the paper's emphasis on constraint-aware planning being the key innovation."}, "questions": {"value": "Suggestion: Did you use \\citet{} instead of \\cite{} or \\citep{}? The citations in the paper are not in parentheses, interrupting the flow of the writing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jHQJcIIo9S", "forum": "PrcNpPxWIQ", "replyto": "PrcNpPxWIQ", "signatures": ["ICLR.cc/2026/Conference/Submission9467/Reviewer_4hDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9467/Reviewer_4hDn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148938689, "cdate": 1762148938689, "tmdate": 1762921056359, "mdate": 1762921056359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}