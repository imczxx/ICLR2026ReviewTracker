{"id": "LF9bRCLHMW", "number": 5351, "cdate": 1757903452171, "mdate": 1759897980130, "content": {"title": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning", "abstract": "When thinking with images, humans rarely rely on a single glance: they revisit visual information repeatedly during reasoning. However, existing models typically process images only once and thereafter generate reasoning entirely in text, lacking mechanisms to re-access or ground inference in visual representations. We empirically confirm this: as reasoning chains lengthen, models progressively lose focus on relevant regions. In response, we introduce v1, a lightweight extension that enables active visual referencing through a simple point-and-copy approach. This allows the model to identify relevant image patches and copy their embeddings back into the reasoning stream, ensuring that evolving hypotheses remain grounded in perceptual evidence. Crucially, our pointing strategy lets the MLLM directly select image patches using their semantic representations as keys, keeping perceptual evidence embedded in the same space as the model’s reasoning. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Across various multimodal mathematical reasoning benchmarks, v1 consistently outperforms comparable baselines, establishing dynamic visual access based on point-and-copy as a practical mechanism for grounded reasoning. We will release the model checkpoint and data.", "tldr": "We present v1, a novel multimodal reasoning framework that dynamically revisits and grounds visual information during inference, outperforming static image processing approaches on mathematical reasoning benchmarks.", "keywords": ["Large Language Models", "Multimodal Reasoning", "Visual Grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf46b4205b7b608df3300d28b51dde2df8332189.pdf", "supplementary_material": "/attachment/5c3c3dcb4dd1c1297cc993ca0460f2280849a7a6.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a lightweight extension that explicit re-access region in the input images to avoid visual groundting decay, which helps MLLM to do complex visual reasoning tasks. Their experiments show good performance on visual-math problems and they also claim the training data will be released."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed methods sound reasonable and it is a lightweight extension that can be easily used on many MLLM.\n2. The section 3 provides a solid support for the motivation of the methods. \n3. The authors provide dataset to finetune its model. Also, the generation method is interesting and inspired."}, "weaknesses": {"value": "1. The Figure 1 seems to sometimes invisible or incorrectly rendering. Sometimes, the region 2z-15 is too large and covering surrounding text. Please test your image in different devices.\n2. Table 1 is too far from the related text.\n3. [**MAJOR**] The ablation study in Table 2 is not significant. Specifically, the coord-method improve the backbone by 8.3% while your pointing method improve the w/o pointer version by 9.2%. The difference is not significant to show the benefits of your pointing comparing to the coord.\n\nIf the point 3 has been solved, I will raise my score to 6. If some of the following questions can be answered reasonably, I will further raise my score."}, "questions": {"value": "1. One of the benifit of the method is that it is a lightweight extension and can be used with many existing pretrained MLLM. Another way that has been used in GPT-4o is to let MLLM to write some code to operate the images and put the new images into the context again. The latter one shows better flexible because it can do more on the original images rather than only point the original region. Can the authors provide more comparison between these two methods? \n2. Also, the authors claim the \"coordinate\" method will \"fail in cases where relevant visual cues are abstract or not spatially localized\". Can the authors provide some experiments to show the better localizing ability of the point method?\n3. The experiments in section 3 can be better explained. First, it is possible that the useful information has been extracted and reprensented in the text in the early stage of the generation, so that it is not necessary to revisit the images. Then, in Figure 3(b), 0.8 is not a significant low value, which raise a question that whether the decay will influnce the performance. I believe the author should provide more evidence about the relation between the decay and the performance.\n4. Using cross-attention to ground the region is an interesting idea but it requires some experiments to support its effectiveness. Is there any post-examination or double-check to support its alignment with human intuition or \"correct\" useful region?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ptxzVLRWIk", "forum": "LF9bRCLHMW", "replyto": "LF9bRCLHMW", "signatures": ["ICLR.cc/2026/Conference/Submission5351/Reviewer_db5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5351/Reviewer_db5f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574861739, "cdate": 1761574861739, "tmdate": 1762918020185, "mdate": 1762918020185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning\" proposes a novel mechanism to enhance multimodal large language models (MLLMs) by enabling them to re-access visual information during reasoning. Inspired by how humans repeatedly revisit visual stimuli while thinking, the authors design a point-and-copy module that allows the model to identify relevant image patches and inject their embeddings back into the reasoning stream.\n\nTo train this capability, they build v1g, a dataset containing 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments across several multimodal mathematical reasoning benchmarks show that v1 achieves strong performance gains over comparable baselines, demonstrating the potential of dynamic visual access for grounded reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Conceptually inspired and technically elegant:\n\nThe paper draws inspiration from human problem-solving processes and introduces a clever point-and-copy mechanism that allows re-referencing of visual regions without introducing additional vocabulary tokens.\n\n### 2. Insightful analysis of visual grounding behavior:\n\nThe authors identify and analyze issues such as attention degradation and mismatch in visual token importance during long reasoning chains. This motivates the design of mechanisms that preserve visual grounding, making the argumentation coherent and compelling.\n\n### 3. Strong empirical results and valuable dataset:\n\nThrough rigorous experiments, v1 demonstrates clear improvements on multimodal mathematical reasoning benchmarks. The proposed v1g dataset is likely to be a useful resource for future research on multimodal reasoning and grounding."}, "weaknesses": {"value": "### 1. Dependence on external text-trace generation:\n\nThe SFT data pipeline relies on an external model (Gemini) to produce text-based traces. This is fine, but it remains unclear how reliably v1 can autonomously propose and execute such traces after training.\n\n**Question:** Is there a quantitative evaluation of the success rate and reliability of detect-call usage during inference?\n\n### 2. Lack of clarity in visual trace generation:\n\nSection 4.3 states that v1’s visual traces are derived via heuristic post-processing of cross-attention maps, yet the details of this algorithm are not provided.\n\n**Suggestion:** Include pseudo-code or a concise algorithmic description in the main text (possibly condensed from Appendix E) along with statistical analysis of the heuristic’s stability.\n\n### 3. Ambiguity in model composition:\n\nIt is unclear whether the visual traces during inference are extracted by v1 itself or by a separate pretrained model (e.g., Qwen).\n\n**Question:** How many models are actually involved in the inference loop, and are all functionalities integrated within v1 after training?\n\n### 4. Limited interpretability discussion:\n\nThe ablation study on “How does v1 utilize pointed visual regions?” lacks depth.\n\n**Suggestion:** A more detailed explanation would enhance understanding of how v1 internally uses the pointed regions to support reasoning.\n\n### 5. Training procedure justification:\n\nThe choice of training for five epochs deviates from common MLLM practice (typically 1–2 epochs).\n\n**Question:** Could the authors provide additional insight into training dynamics and the rationale behind this choice?"}, "questions": {"value": "See the Weaknesses part for details.\n\n**Minor Issues:**\n\nFigures 1 and 2 embedded in the PDF fail to render correctly in Safari and some other browsers. The authors are encouraged to check compatibility or provide rasterized alternatives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0YLUJRjTZ2", "forum": "LF9bRCLHMW", "replyto": "LF9bRCLHMW", "signatures": ["ICLR.cc/2026/Conference/Submission5351/Reviewer_2iuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5351/Reviewer_2iuN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813427538, "cdate": 1761813427538, "tmdate": 1762918019748, "mdate": 1762918019748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical issue in vision-language models (VLMs): the diminishing influence of visual tokens as the chain-of-thought (CoT) lengthens. To mitigate this, the authors introduce a novel training dataset, v1g, and a resulting model, v1. The v1g dataset is constructed from VQA samples, where CoT sequences are augmented by interleaving relevant visual tokens, copied directly from the input, to reinforce visual grounding. For instance, a reasoning step like \"query z\" is explicitly followed by the visual tokens corresponding to object \"z\". The authors fine-tune a base VLM on V1G to obtain the V1 model. Experimental results on benchmarks, including MathVista, MathVision, and MathVerse, demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel training dataset named v1g to address a critical issue in VLMs: the diminishing influence of visual tokens as the chain-of-thought (CoT) length increases.\n\n2. By fine-tuning VLMs on v1g, the authors obtain the v1 model. Experimental results on three mathematical VQA benchmarks demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The evaluation of the proposed method is currently limited to mathematical VQA tasks. Its performance on other domains remains unclear, and further experiments on general VQA benchmarks are needed to assess the generalization capability of both the dataset and the fine-tuned VLM.\n\n2. Does the reuse of visual tokens in the chain-of-thought introduce additional computational overhead compared to text-only CoT? A quantitative comparison of computational costs between the proposed method and the baseline would help clarify this practical concern."}, "questions": {"value": "1. How does the method generalize to other VQA domains?\n\n2. What is the computational overhead compared to baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CN6NbwCzYh", "forum": "LF9bRCLHMW", "replyto": "LF9bRCLHMW", "signatures": ["ICLR.cc/2026/Conference/Submission5351/Reviewer_zMQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5351/Reviewer_zMQY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015136216, "cdate": 1762015136216, "tmdate": 1762918019463, "mdate": 1762918019463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MLLMs often lose visual grounding as reasoning unfolds, since they process images only once before generating purely textual inferences. This paper proposes v1, a lightweight extension that enables active visual referencing through a simple point-and-copy mechanism. v1 allows the model to dynamically select relevant image patches and copy their embeddings into the reasoning stream, keeping inference grounded in perceptual evidence. To train this ability, the authors construct v1g, a dataset of 300 K multimodal reasoning traces with interleaved visual-grounding annotations. Evaluated on three multimodal mathematical reasoning benchmarks, v1 consistently surpasses comparable baselines, particularly on tasks requiring fine-grained visual grounding. These results demonstrate that dynamic visual access via point-and-copy offers an effective and efficient mechanism for grounded multimodal reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)\tThe paper identifies a concrete weakness in current MLLMs, the visual grounding decay problem during reasoning, and provides empirical evidence to substantiate this observation.\n2)\tThe proposed point-and-copy approach offers a simple yet elegant solution to dynamically re-access visual representations during reasoning, conceptually bridging textual reasoning and visual perception.\n3)\tThe method introduces minimal additional parameters (two linear heads), making it easy to integrate into existing MLLMs without significant computational or architectural overhead."}, "weaknesses": {"value": "1)\tEmpirical validation is confined to MathVista/MathVision/MathVerse. Broader domains (charts beyond math, documents, VQA, OCR-heavy tasks) are not evaluated, limiting claims of generality. Consider adding non-math benchmarks.\n2)\tAlthough the method is said to be generally compatible, experiments are instantiated only on Qwen2.5-VL-7B; cross-backbone results (e.g., InternVL/LLaVA variants) would strengthen the case for portability.\n3)\tThe paper offers only brief training descriptions, making it difficult to assess reproducibility and generalization. More details on optimization settings, schedule, and potential use of reinforcement-style methods (e.g., R1-type reasoning training) would help clarify robustness and strengthen confidence in the reported results.\n4)\tIn Figure 3(b), although the ratio of attention to salient regions decreases as generation progresses, the overall attention level remains relatively high. It is unclear how the authors interpret this result: does a high absolute ratio still indicate grounding decay, or could it reflect stable focus on visual areas? \n5)\tIn Figure 3(a), the decrease of attention on visual tokens with longer generation steps seems natural for autoregressive reasoning, where attention gradually shifts from perception to internal memory. The key question is not whether decay occurs, but how fast it happens. The paper interprets the observed decline as evidence of model deficiency, yet does not justify why the speed of attention decay indicates unreasonable behavior. A more principled analysis or comparison across models with different decay rates would make this argument more convincing.\n6)\tSwapping Figures 2 and 3 would improve narrative clarity: the paper should first show the attention-decay problem (Fig. 3) before introducing the proposed point-and-copy solution (Fig. 2)."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nUEgoOyWKd", "forum": "LF9bRCLHMW", "replyto": "LF9bRCLHMW", "signatures": ["ICLR.cc/2026/Conference/Submission5351/Reviewer_7bZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5351/Reviewer_7bZK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139584603, "cdate": 1762139584603, "tmdate": 1762918019146, "mdate": 1762918019146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}