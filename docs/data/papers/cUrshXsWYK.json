{"id": "cUrshXsWYK", "number": 25406, "cdate": 1758367750076, "mdate": 1759896722220, "content": {"title": "MARINA-P: Superior Performance in Nonsmooth Federated Optimization with Adaptive Stepsizes", "abstract": "Non-smooth communication-efficient federated optimization remains largely unexplored theoretically, despite its importance in machine learning applications. We consider a setup focusing on optimizing downlink communication by improving state-of-the-art schemes like EF21-P [Gruntkowska et al., 2023] and MARINA-P [Gruntkowska et al., 2024] in the non-smooth convex setting. Our key contributions include extending the non-smooth convex theory of EF21-P from single-node to distributed settings and generalizing MARINA-P to non-smooth convex optimization. For both algorithms, we prove optimal $\\mathcal{O}(1/\\sqrt{T})$ convergence rates under standard assumptions and establish matching communication complexity bounds with classical subgradient methods. We provide theoretical guarantees under constant, decreasing, and adaptive (Polyak-type) stepsizes. Our experiments demonstrate MARINA-P‚Äôs superior performance with correlated compressors in both smooth non-convex and non-smooth convex settings. This work presents the first theoretical analysis of distributed non-smooth optimization with server-to-worker compression, including a comprehensive analysis for various stepsize schemes.", "tldr": "We extend MARINA-P and EF21-P to non-smooth distributed optimization, introduce adaptive stepsizes, and show MARINA-P with permutation compressors outperforms EF21-P in non-smooth settings", "keywords": ["Federated Learning", "Communication-efficient non-smooth optimization", "Adaptive Stepsizes"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe0d57f0d21aa1b22be55d3ee6383abccc106cb7.pdf", "supplementary_material": "/attachment/51668adc2048872493c6c3f4296b75aae17e00fb.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the distributed optimization problem under server-to-worker (s2w) compression with error feedback. This paper focuses on Lipschitz continuous objectives. The authors analyze two algorithms: EF21-P, which employs server-side compression, and MARINA-P, a loopless variant that uses server-side unbiased quantizers. Theoretical analyses are provided for three types of stepsize: constant, decreasing, and Polyak stepsize."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theorems are written clearly and backed by reasonable proofs and analysis.\n2. The synthetic experiment is well-defined for illustrations of the theoretical results."}, "weaknesses": {"value": "1. The paper only considers s2w compression, while w2s communication is typically considered much more expensive than s2w broadcast [1,2]. It seems to make more sense to consider w2s or bidirectional compression. The current setting is a bit less interesting.\n\n2. The main contribution is mostly an extension of prior works from the single-node to distributed settings. However, since the paper assumes the bounded gradient assumption, the typical challenge of the data heterogeneity issue (when considering smooth objectives) disappears. Therefore, it seems extending the results to the multiple client setting is natural, and the overall contribution is a bit limited to me. \n\n3. The scale of the experiment is a bit too small to show the superiority of the algorithms compared with other algorithms.\n\n[1] EF21: A new, simpler, theoretically better, and practically faster error feedback, Neurips 2021. \n\n[2] EF21 with Bells & Whistles: Six Algorithmic Extensions of Modern Error Feedback. JMLR 2025."}, "questions": {"value": "1. Could the authors clarify why MARINA-P is analyzed under unbiased compression rather than the more general contractive compression setting?\n\n\nP.S. The reference: Error feedback for smooth and nonsmooth convex optimization with constant, decreasing, and polyak stepsizes, seems broken."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L8ms3f9Mf8", "forum": "cUrshXsWYK", "replyto": "cUrshXsWYK", "signatures": ["ICLR.cc/2026/Conference/Submission25406/Reviewer_vakc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25406/Reviewer_vakc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779999601, "cdate": 1761779999601, "tmdate": 1762943423345, "mdate": 1762943423345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends EF21-P and MARINA-P to non-smooth convex federated optimization with server-to-worker compression and adaptive (Polyak) stepsizes. It establishes optimal convergence rates of $O(1/\\sqrt{T})$ for constant and Polyak stepsizes, and $O(\\log T/\\sqrt{T})$ for decreasing ones, matching the best-known subgradient bounds. Experiments show that MARINA-P performs competitively and maintains strong communication efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides the first theoretical analysis of communication-efficient federated optimization for non-smooth convex problems with s2w compression.\n\n2. Achieves optimal convergence rates, extending EF21-P and MARINA-P frameworks with adaptive stepsizes.\n\n3. Experiments, though limited, confirm robust performance and communication savings."}, "weaknesses": {"value": "1. While the theoretical extension to non-smooth settings is novel, the core algorithmic structure (EF21-P and MARINA-P) is largely inherited from prior works. The contribution lies primarily in analysis, rather than introducing fundamentally new mechanisms.\n\n2. The experiments are not extensive enough to fully substantiate claims of ‚Äúsuperior performance.‚Äù For example, no ablation is presented on key parameters such as compression aggressiveness $(\\alpha, \\omega)$ or probability $p$, and the tasks tested are relatively simple.\n\n3. The paper emphasizes theory but does not address how these results translate to real-world federated systems, where non-smooth objectives (e.g., L1 regularization or hinge losses) are used in large-scale training. Practical scalability, stability under high heterogeneity, and wall-clock communication savings are not discussed.\n\n4. The introduction justifies the non-smooth setting generally, but it could better articulate why optimizing s2w compression in non-smooth regimes matters in practice, especially since upload and download speeds are often asymmetric in FL deployments.\n\n5. The paper‚Äôs experiments primarily replicate setups from earlier MARINA or EF21 works, and the benefit of correlated compressors, though noted, is not deeply analyzed."}, "questions": {"value": "1. Could the authors include experiments varying the compression ratio or probability $p$ to empirically demonstrate how communication complexity scales?\n\n2. The theoretical results assume convex objectives; can the authors comment on the extension to non-convex but Lipschitz objectives (e.g., ReLU networks)?\n\n3. Are there practical datasets (e.g., text or healthcare) where the proposed methods show meaningful speedups? A more applied experiment could strengthen the motivation.\n\n4. How does MARINA-P behave under client heterogeneity or unbalanced data distributions? Some robustness evaluation would be valuable.\n\n5. Could the authors clarify whether adaptive Polyak stepsizes can be used without exact knowledge of $f(x^*)$ in practice, and how sensitive the results are to errors in estimating this value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sZUBUKDEmq", "forum": "cUrshXsWYK", "replyto": "cUrshXsWYK", "signatures": ["ICLR.cc/2026/Conference/Submission25406/Reviewer_oeeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25406/Reviewer_oeeX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951767598, "cdate": 1761951767598, "tmdate": 1762943423085, "mdate": 1762943423085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the EF21-P and MARINA-P methods to the federated convex nonsmooth problems. For both methods, the authors explore three step sizes: constant, Polyak-type, and decreasing step sizes. They provide theoretical analysis and empirical evidence for both algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors present the first theoretical analysis of the federated convex nonsmooth optimization problem with server-to-worker compression.\n\n* The paper is well-written, and the presentation is clear."}, "weaknesses": {"value": "* I feel that the technical novelty of this paper is limited. Both federated smooth optimization and convex nonsmooth optimization are well-studied areas, and I do not see what specific technical challenges arise in the case of federated convex nonsmooth optimization.\n\n* The constant and Polyak-type stepsize are based on the $x^*$, which is the optimal value of the objective function.\n\n* The numerical experiments are weak. In particular, the empirical experiments are conducted on synthetic nonsmooth functions. How about machine learning experiments with real-world datasets?"}, "questions": {"value": "* In related work, the authors claim that recent work with decreasing stepsize can achieve a convergence rate without the log factors on convex nonsmooth problems (lines 123-124). Can the author extend these techniques to the federated setting to achieve an optimal rate without log factors? Is the log factor due to the use of the last iterate instead of the average iterate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rWL7myhsYq", "forum": "cUrshXsWYK", "replyto": "cUrshXsWYK", "signatures": ["ICLR.cc/2026/Conference/Submission25406/Reviewer_To1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25406/Reviewer_To1B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762755156065, "cdate": 1762755156065, "tmdate": 1762943422897, "mdate": 1762943422897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of the optimization problem for nonsmooth federated learning.\nThey build on existing algorithms EF21-P and MARINA-P for nonsmooth settings.\nFor EF21-P, they extend it from single-node to a distributed version. Whereas for MARINA-P, they use subgradients to generalize it to nonsmooth optimization.\nBoth algorithms achieve optimal $O(1/\\sqrt{T})$ rate. They also study rates under different step size schemes (constant, decreasing and Polyak)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Both improved algorithms work in distributed nonsmooth settings, achieving optimal rates (with the caveat that it requires either knowing the total number of iterations ùëá in advance for constant stepsize, or knowing the optimal value ùëì(ùë•*) for Polyak stepsize),\nFor EP21-P, the communication complexity matches classical distributed subgradient methods.\n2. Three different stepsize schemes are studied."}, "weaknesses": {"value": "1. Lack of experiments section in the main text: while in both abstract and contribution section, it is said that experiments show MARINA-P has superior performance, there is no experiment section in the main text. There's an experiment section in the appendix; only synthetic dataset is considered and I do not see comparison with other distributed FL optimization algorithms.\n2. Presentation issues: since the author states that MARINA-P has superior performance, the extension of EF21-P seems to be a minor contribution. Whereas for MARINA-P, the only modification the authors made to the algorithm is to use subgradients instead of gradients.\n3. Incremental contribution: both algorithms heavily build on previous works, the extensions and the analysis seem relatively standard. It is hard to see whether there are technical contributions in the analysis (related to the presentation issues above)."}, "questions": {"value": "1. Is the experiment section supposed to be in the main text, and do you have a comparison with other Fed ML algorithms?\n2. What are the technical difficulties when generalizing those two algorithms? Can you summarize the technical contributions in your analysis if there are any?\n3. For communication complexity of MARINA-P, you wrote in page 9.\n> A notable feature of our complexity result is its independence from\nthe number of workers ùëõ in the non-smooth setting ‚Äì a known phenomenon in subgradient methods\nI have trouble understanding why this is a \"feature\". Does this suggest parallelization provides no benefits, similar what you mentioned in page 7\n4. In Algo 1, is line 12 a duplicate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LU2Hzb61Vx", "forum": "cUrshXsWYK", "replyto": "cUrshXsWYK", "signatures": ["ICLR.cc/2026/Conference/Submission25406/Reviewer_da6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25406/Reviewer_da6R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762761064139, "cdate": 1762761064139, "tmdate": 1762943422756, "mdate": 1762943422756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}