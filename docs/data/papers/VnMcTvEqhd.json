{"id": "VnMcTvEqhd", "number": 5420, "cdate": 1757908124997, "mdate": 1759897976447, "content": {"title": "Self-Tooling Agent: Dynamically Extending Agent Capabilities through Scientific Tool Synthesis and Invocation", "abstract": "Tools are essential for defining an agent's capabilities, yet a fundamental challenge remains: general-purpose agents lack expert tools, while specialized scientific agents rely on manually-crafted toolsets that are expensive to build and do not generalize across domains. This tool creation bottleneck limits agent adaptability and performance on novel tasks. To address this challenge, we introduce the Self-Tooling Agent (STA), an agentic framework where the policy LLM learns to dynamically arbitrate between invoking existing tools and synthesizing new, specialized ones as needed. Specifically, the training dataset is generated by reverse-engineering contexts from expert tools sourced from multiple scientific agents, while a dynamic, interactive environment provides a sand-boxed space for tool execution and registration. The framework trains the policy LLM using a two-stage process: supervised fine-tuning is used for syntax learning, while reinforcement learning with a principled, multi-component reward function optimizes the LLM's strategic decision-making. Extensive evaluations on a diverse suite of benchmarks, from complex scientific QA to standard function-calling leaderboards, demonstrate that the proposed STA significantly outperforms baselines that rely on fixed toolsets, including specialized agents and powerful proprietary models. This work establishes that empowering an agent to autonomously expand its own capabilities is a critical step towards creating more adaptable and resourceful scientific agents.", "tldr": "", "keywords": ["Agent", "Tool use", "Reinforcement learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0a436adc372ac7045361b6fa6d41c09d7129d28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Self-Tooling Agent (STA), a framework that lets LLM agents create their own tools on the fly instead of depending only on pre-built ones. STA generally teaches an agent when to generate a new specialized tool (like a Python function) and when to use an existing one, using supervised learning and reinforcement learning with carefully designed rewards. In experiments, STA outperformed both general and specialized agents across several scientific and tool-use benchmarks, showing it can adapt and expand its capabilities dynamically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper frames the tool use / tool creation as a decision-making problem, which is a quite novel perspective. The reward design is also novel to consider redundant creation of tool, etc. It would make the paper better if the reward design can be more fine-grained and tailored to “when to use / create tool”. Currently it’s still through an “indirectly” to reflect this.\n\n- The motivation and writing is clear, even though the experiment part is too tight and should be left more space to further discuss the insights and findings."}, "weaknesses": {"value": "- The current tool creation and tool call seems very disentangled. From the example given, tool creation’s media is still code, so the tool created is python code-oriented tasks like drawing, calculation, synthesize of existing tool etc. But the tool use is about science domain expert tool, which makes its use case quite different from that of tool creation. From my view this makes the cognitive challenge to the model lowered and undermines the value of the work.\n\n- If tool creation’s purpose is to write code to wrap several “atomic” tools into a “compound” tool, what’s the difference of it and just using the “atomic” tool (originally provided scientific tool) one by one? The tool creation here then did not really create anything new, so it’s not making sense to call it so.\n\n- I think your work is very related to the line of tool creation works including LATM, CREATOR, Alita, etc. They worth more discussion than the current general agent. Also, some works related to tool use decisions / efficiency can be discussed, as basically your paper is training the model to make tool use and tool creation decisions. \n\n- STA with only Sci tool cannot beat Biotin with Sci tool on DbQA and SeqQA, which necessitates further exploration and discussion: is the method proposed really working well? Or it’s just because the “tuning” of instructions that make the model achieve a “cherry-pick” good result. Also the results need error bar / parallel evaluations.\n\n- The whole analysis part of this paper is too shallow and lacks insights. For example, BFCL is just a tool calling benchmark, is the model showing tool creation capability in this kind of benchmark? How the model did that? Is tool creation in benchmark like this really leads to improvement? The ablation should not be just on reward, but more from the intrinsic method level what is contributing to success. Error analysis should also be presented.\n\n- The claim of tool use / tool creation decision could be put into a broader context rather than just scientific domain. Also, the benchmarks like BFCL is not scientific domain, which make the scope of the paper a little bit questionable. Seems the author wants to focus on science agent but also needs to use dataset other than science domain to show the model’s advantages."}, "questions": {"value": "- Can the created tools be reused across tasks?\n\n- Is the RL rollout multi turn? Is the reward design all contributing to final reward instead of dense turn reward?\n\n- Other questions see the section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eTPZ5q9K9k", "forum": "VnMcTvEqhd", "replyto": "VnMcTvEqhd", "signatures": ["ICLR.cc/2026/Conference/Submission5420/Reviewer_LtVT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5420/Reviewer_LtVT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709982127, "cdate": 1761709982127, "tmdate": 1762918051417, "mdate": 1762918051417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a new framework of self tooling agent, where they trained a new model to decide whether to use an existing tool and synthesize a new tool to solve tasks. The authors trained  the model with multi component reward for tooling decisions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the work is well established, where the authors are motivated by that tools might be available for domain specific tasks, so they want to study whether agents could synthesize tools given existing tools.\n- The authors performed comprehensive evaluation on three benchmarks, HLE, DbQA, and SeqQA.\n- The training pipeline is well-documented."}, "weaknesses": {"value": "- One main motivation the authors discussed is that manually-created toolsets lack generalizability across domains, but the authors didn't discuss whether their proposed approach could improve generalizability and didn't perform any experiment to demonstrate this.\n- The gain on HLE benchmark is small, but the authors didn't explain why this is the case. \n- The authors didn't control for the type of tools when comparing STA to Qwen3-8B. They should conduct an experiment where they constraint STA to only use web tools and then compare performance gains of STA.\n- In Table 2, the authors didn't compare their models to the baseline Qwen3-8B performance on DbQA and SeqQA.\n- Missing citation: LLaMA Factory is not cited in the paper. \n- Missing related work on agent skill induction (e.g. Wang, Zora Zhiruo, et al. \"Inducing programmatic skills for agentic tasks.\" arXiv preprint arXiv:2504.06821 (2025).)"}, "questions": {"value": "- How generalizable is the approach proposed in the paper compared to existing approaches such as manually curating tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ohHpHpnwJy", "forum": "VnMcTvEqhd", "replyto": "VnMcTvEqhd", "signatures": ["ICLR.cc/2026/Conference/Submission5420/Reviewer_VGq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5420/Reviewer_VGq1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963841152, "cdate": 1761963841152, "tmdate": 1762918051147, "mdate": 1762918051147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Self-Tooling Agent (STA), a framework that enables an agent to both invoke existing tools and synthesize new ones on the fly during task execution. It unifies tool use and creation within a single action space, employing SFT to teach syntax for tool generation, followed by RL to optimize tool-calling use and decisions."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The two-stage training approach—using SFT to teach syntax for tool generation and invocation, followed by RL to optimize tool-calling decisions—is reasonable.\n- The paper is well-presented, clearly structured, and easy to follow.\n- The unified action space for both tool creation and invocation is a novel and interesting design choice."}, "weaknesses": {"value": "1. **Lack of Key Ablation Studies:** A major concern is the absence of ablation studies to support core claims. The paper argues that the unified Generation-Invocation Action Space is central to its effectiveness, yet provides no ablation to verify this. Additionally, ablating the contributions of the SFT and RL stages would strengthen the analysis.\n\n2. **Missing Qualitative Analysis:** The paper would benefit from qualitative insights, such as examples of the types of tools generated and how the agent’s behavior differs from conventional action spaces.\n\n3. **Incomplete Related Work:** The paper overlooks recent advances in self-evolving agents via tool creation, for example, Alita (Qiu et al., 2025) and Darwin Godel Machine (Zhang et al., 2025)."}, "questions": {"value": "- Is the RL training conducted on the same benchmarks used for testing? A comparison between the proposed method and a pure RL baseline trained directly on the benchmarks would help assess the effectiveness of the two-stage approach.\n\n- In Tables 1 and 2, what does \"Tool Type\" refer to for the STA method? Since tool creation is integrated into the action space, it is unclear why a separate tool type specification is needed.\n\n- Could the dataset description be more specific? What is the size of the SFT dataset? How exactly does the reverse-engineering process work for generating SFT examples from expert tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yXFSnszWPs", "forum": "VnMcTvEqhd", "replyto": "VnMcTvEqhd", "signatures": ["ICLR.cc/2026/Conference/Submission5420/Reviewer_aQoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5420/Reviewer_aQoa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986749959, "cdate": 1761986749959, "tmdate": 1762918050680, "mdate": 1762918050680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of self-tooling agents that can dynamically create new tools as needed. Rather than relying on predefined tool sets, the agent analyzes task requirements, designs and implements specialized tools, then uses them to complete tasks. The paper presents a complete pipeline of tool creation-validation-usage and demonstrates advantages on multiple complex tasks requiring specialized tools."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel concept**: Self-tooling represents a fresh and meaningful idea, significantly enhancing agent flexibility and adaptability\n2. **Complete system**: Forms a full cycle from tool need identification through creation, validation, usage, to reuse - comprehensively designed\n3. **Extensive experiments**: Broad experimental coverage across task types, with ablation studies analyzing component contributions convincingly\n4. **Multi-dimensional analysis**: Examines not just success rates but tool quality, efficiency, reusability, and other dimensions\n5. **High practical value**: Demonstrates application potential in real complex tasks including data analysis and system operations\n6. **Good reproducibility**: Provides detailed implementation specifics and examples facilitating reproduction and extension"}, "weaknesses": {"value": "1. **Security concerns**: Automatically generated code serving as tools poses security risks. While sandbox mechanisms are mentioned, security protection discussion is insufficient. Malicious or buggy tools could cause harm\n2. **Computational overhead**: Creating new tools requires additional inference and testing, potentially inefficient for simple tasks. Cost-benefit tradeoff analysis is lacking\n3. **Validation limitations**: Tool correctness verification relies primarily on test cases, which may have insufficient coverage. Edge cases in complex tools are difficult to thoroughly test\n4. **Quality control issues**: Generated tool quality depends on LLM capabilities - how is consistency ensured? Some examples show suboptimal tool quality\n5. **Simplistic reuse mechanism**: Tool reuse mainly relies on semantic similarity, with insufficient consideration for scenarios requiring composition or modification of existing tools"}, "questions": {"value": "1. How can generated tools be adequately security-audited? What are the potential risks and mitigation strategies?\n2. What determines when to create new tools versus using existing ones? What's the decision policy?\n3. When created tools contain bugs, can the agent automatically debug and fix them, or must it regenerate from scratch?\n4. What's the strategy for managing the tool library? How do you prevent unbounded growth?\n5. Could the self-tooling concept extend to other types of \"resource\" creation, such as data structures or configuration files?\n6. Compared to traditional tool learning (learning from human demonstrations), what unique advantages does self-tooling offer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WlbJ9ab5Ay", "forum": "VnMcTvEqhd", "replyto": "VnMcTvEqhd", "signatures": ["ICLR.cc/2026/Conference/Submission5420/Reviewer_FjDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5420/Reviewer_FjDz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998331976, "cdate": 1761998331976, "tmdate": 1762918050258, "mdate": 1762918050258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I apologize for any confusion in the earlier review. I have now carefully re-examined the paper and updated the comments, and based on the revised assessment, I think the original score does not need to be changed.\nThe paper proposes the Self-Tooling Agent (STA), an agentic framework in which a policy LLM learns to either call an existing tool or synthesize a new tool on the fly for scientific tasks. The paper argues that learning a unified “generate-or-invoke” policy yields more adaptable scientific agents than systems with fixed toolsets. The experimental results on the HLE, DbQA, and SeqQA subsets support the argument."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Explicitly treats the agent’s toolset as evolving, and frames “create a new tool vs. call an existing one” as a central RL decision. This is a natural next step beyond classical tool-use work like Toolformer, CRAFT, Voyager, and Alita.\nThe use of a sandboxed environment (mini-swe-agent–style) with bash access, Docker/Python runtime, and dynamic registration of newly generated tools is technically significant and closer to real research workflows than static function-calling benchmarks.\nA single policy learns end-to-end when to synthesize vs. reuse in a realistic scientific environment. That feels like a meaningful conceptual step."}, "weaknesses": {"value": "Although involving the tool creation and reuse capability in the RL process is an interesting idea, this paper does not compare the method to any relevant previous work, including CRAFT, Voyager, ToolGen, LLMs as Tool Makers (LATM), and Alita.\nIn the formalism, new tools expand F_t within an episode; it is unclear whether tools persist across episodes/tasks (as in LATM’s functional cache) or are purely ephemeral. This affects the “self-expanding” narrative.\nThe paper doesn’t deeply analyze how often the agent decides to generate a tool vs. call an existing one, nor under what circumstances."}, "questions": {"value": "With respect to the environment details, what exact libraries and datasets are available? How is time/compute bounded for generated tools? How are unsafe operations constrained?\nGRPO is mentioned, but details like the number of updates, trajectory length, sampling strategy, reward scaling/normalization, and KL regularization (if any) are not provided. These are important to reproduce RL training.\nDoes STA over-generate tools early in training and later regularize to a smaller tool-creation rate?\nDo new tools materially improve performance across multiple steps in an episode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WlbJ9ab5Ay", "forum": "VnMcTvEqhd", "replyto": "VnMcTvEqhd", "signatures": ["ICLR.cc/2026/Conference/Submission5420/Reviewer_FjDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5420/Reviewer_FjDz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998331976, "cdate": 1761998331976, "tmdate": 1763655866532, "mdate": 1763655866532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}