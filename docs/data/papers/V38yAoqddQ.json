{"id": "V38yAoqddQ", "number": 20805, "cdate": 1758310414494, "mdate": 1759896957731, "content": {"title": "Decoupling Positional and Symbolic Attention in Transformers", "abstract": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. \nRecently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general  definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them.  \nWe apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use.  \nFinally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.", "tldr": "", "keywords": ["Transformers architecture", "positional encodings", "Transformers theory", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54dfcffc59dd2a6e0f6c41b9e4a5f3ca3c407178.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors formalise what it means for a head to behave positionally or symbolically. They then measure and study in detail both mathematically and empirically how positional and symbolical heads function. The paper focuses particularly on RoPE and performs interesting experiments to support their claims."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I found the paper to be insightful. The formalisation of positional / symbolic heads using invariance and equivariance is clever and the resulting analysis is very interesting. I found Section 5 to be a good sanity check of the theory and intuition. \n\nI find this work to be a really strong and useful deep dive into RoPE consolidating and improving our existing understanding of how RoPE functions and how internal frequencies are used. As RoPE is the current most popular LLM PE, I find this paper to be on an important topic."}, "weaknesses": {"value": "The main weakness I see is that the paper is dense at times especially in notation. It might be useful to add a small summary at the end of the sections to help convey what the authors believe should be the main message. This is mostly a minor suggestion to help with readability."}, "questions": {"value": "I was wondering if the authors have an opinion given their analysis on the proposed method of p-RoPE (Barbero et al. 2024) where the p% lowest frequencies are set to spin to 0 to construct robust symbolic channels? \n\nOn a related note, do the authors have an idea of how the study could point towards possibly improving RoPE or for example the understanding of the choice of the wavelength hyperparameter. This is particularly important as many frontier models have been choosing different wavelengths for RoPE to deal with long context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IFfdqY1HEU", "forum": "V38yAoqddQ", "replyto": "V38yAoqddQ", "signatures": ["ICLR.cc/2026/Conference/Submission20805/Reviewer_Fhmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20805/Reviewer_Fhmh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671252046, "cdate": 1761671252046, "tmdate": 1762935497300, "mdate": 1762935497300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Transformers decouple positional and symbolic information in attention heads. It provides formal definitions for positional and symbolic behavior and a theorem showing they are mutually exclusive unless the attention pattern is uniform. The authors introduce a metric to quantify the amount of positional and symbolic information, using it to analyze RoPE models. The main finding is that low-frequency angles from RoPE are used for symbolic tasks, while high-frequency angles are used for positional tasks. This is demonstrated by training a toy model in canonical positional and symbolic tasks. Moreover, this metric is also applied to pre-trained models, showing that it can be used to decouple positional and symbolic behavior in attention heads."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The main takeaway is very interesting: attention heads use RoPE frequency as a tool to implement either positional or symbolic functions. \n- The paper has originality in providing a theoretical analysis of the problem.\n- The choice of training toy models trained in canonical tasks is a clear way to test the positional and symbolic functions of attention heads.\n- The experiments are done in multiple pre-trained LLMs, which shows how generalizable the proposed metric is."}, "weaknesses": {"value": "- Some of the claims lack statistical tests:\n    - The claim that early layers are positional and late layers are symbolic (Lines 268-269) is based on visual inspection, not a statistical test.  \n    - The \"negatively correlated\" claim (Lines 271-272) also needs a proper correlation test with a p-value.  \n- The theory is on logits, but the metric uses softmax weights. The paper doesn't discuss this switch. Softmax is a nonlinear function and has a relativistic behavior, ie, when analyzing a token $d$ attention to another token $s$, the logit value itself is not the only important component, but how large this logit is in comparison to the other tokens $j \\leq d, j \\neq s$. I wonder if this property would somehow impact the analysis.\n- The usage of blocks is not motivated. Clearly, it is important for efficiency, but this is not discussed in the paper, which makes the transitions between paragraphs in Section 3.3 a bit rough to read. Block sizes were not discussed either."}, "questions": {"value": "**Questions:**\n1. What are the limitations of basing the theory on logits but the empirical metric on softmax weights?  \n2. Why use blocks? Is it just for efficiency? What's the impact of this approximation?  \n3. What is the model's accuracy on the binding task? Are you only analyzing instances where the model was correct?\n4. Can you add statistical tests (e.g., correlation) for the claims in Lines 268-269 (layer depth) and Lines 271-272 (negative correlation)?  \n\n**Suggestions:**\n- \"Fine-Tuning Enhances Existing Mechanisms\" (https://arxiv.org/abs/2402.14811) is a relevant paper that could be cited. They identify the circuit that solves entity tracking as having heads that handle positional information and others that handle content information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hA4c89uPtD", "forum": "V38yAoqddQ", "replyto": "V38yAoqddQ", "signatures": ["ICLR.cc/2026/Conference/Submission20805/Reviewer_tJZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20805/Reviewer_tJZ7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856333214, "cdate": 1761856333214, "tmdate": 1762935491478, "mdate": 1762935491478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to study the positional and symbolic properties of the Rotary Positional Encodings (RoPE). To this end, the authors provide formal definitions of the two properties and introduce a metric to assess the extent to which a given head behaves positionally or symbolically on a given input, which enables analyzing the positional and symbolic behaviors of a given model at different levels of granularity, and also visualizing of the positional-symbolic profile of the model.\n\nFirst, they apply the metric to the transformer-basd LLM, Gemma-2m,  and reveal that RoPE with a high frequency tends to be very positional while a low frequency corresponds to symbolic patterns. \n\nSecond, they introduce two tasks, Index and retrieval,  that can be solved by pure positional (symbolic) headsusing a 1â€“layer attention-only transformer. Empirical studies show that a head can only act positionally when it uses fairly large frequencies. If we force it to use lower frequencies, its performance drops.Similarly, attention heads can perform well on intrinsically symbolic tasks if given access to low frequencies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This work provides a formal definition of positional and symbolic behavior of attentions and introduces metrics to quantify the two scores for LLMs, which would be useful for following studies\n* They provide both theoretical analyses and empirical studies to support their findings. Extensive experiments have been conducted, including multiple LLMs, studies across real LLMs and toy transformers.  \n* The presented tasks, index and retrieval, are interesting, which offer a doable way to study the functions of positional and symbolic properties"}, "weaknesses": {"value": "* Some key related papers are not discussed in the current manuscript, which strongly undermine the novelty and contribution of this work. \n[1] and [2] have already discussed the separation of positional and semantic encodings. More importantly, [3] reveals the attention weights in transformers can be divided into two components: positional weights and semantic weights. It further introduces two properties, locality and symmetry, to measure the two behaviors. Basically, a high (or low) locality corresponds to positional (or symbolic) behaviors. In other words, the locality here can depict the positional-symbolic profile of a language model, i.e., a high locality is aligned with the RoPE with a high frequency. However, the authors did not discuss the main distinction of this work compared to prior studies, which undermines the contribution of this work.\n\n* The presentation requires further improments. (1) This work aims to study and interpret the patterns of RoPE, but the authors do not provide a basic background in the preliminary section while they did present the background of the transformer architecture. I would suggest introducing the basic concepts of RoPE, e.g., what is the frequency. (2) I appreciate that they provide dense figures, but some figures are hard to understand. The lack of clear titles and axis texts lead to a bad readability (see questions below).\n\n* They have conducted extensive theoretical analyses and empirical studies, but it is not easy for me to derive the main conclusions from this manuscript, and it is quite vague about how to use their findings to improve the current RoPE \n\n\n\n[1] Constituency Parsing with a Self-Attentive Encoder. ACL 2018\n  \n[2] Rethinking Positional Encoding in Language Pre-training. ICLR 2021 \n\n[3] The Locality and Symmetry of Positional Encodings. EMNLP 2023 Findings"}, "questions": {"value": "* the key concept of frequency is not introduced in the preliminary, which hampers the understanding of the subsequent sections. I would suggest introducing the basic background of the RoPE in section 2.\n* in line 145, *\"its logit sequence is invariant under permutation of the key vectors.\"* I would like to confirm if the key vectors or input vectors (with regard to $x$) are shuffled? I asked this since self-attention uses the input vectors as key, query, and value. I am confused by performing the permutaion of only the key vectors. \n* some figures are confusing. What are the score, frequency IDs, and the two line plots in Figure 1C? What is the number, e.g, 1->32, in Figure E? What are the three points in Figure F?\n* In Figure 3A, why a low frequency results in a U-shaped acc patterns? Why an encoder with a low frequency can deal with indexes of the beginnig positions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QEe4yrXmaO", "forum": "V38yAoqddQ", "replyto": "V38yAoqddQ", "signatures": ["ICLR.cc/2026/Conference/Submission20805/Reviewer_uiez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20805/Reviewer_uiez"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863902826, "cdate": 1761863902826, "tmdate": 1762935474615, "mdate": 1762935474615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors formalize definition of positional and symbolic heads, define metrics, canonical tasks, prove several theorems and perform analysis of several open weight and toy models.  Characterization of a model by its positional-symbolic profile is a potentially powerful tool to analyze how particular models behave on different tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Theoretical Formalization: The authors provide formal definitions for when an attention head behaves purely positionally (depending only on position, regardless of token value) or purely symbolically (depending only on the token value, regardless of position) and prove several theorems.\n\nIntroduce a novel metric is introduced to quantify the extent to which any given attention head exhibits positional or symbolic behavior on a specific input. This metric allows for granular analysis, creating a positional-symbolic profile for the entire model, visualized in the positional-symbolic plane.\n\nApplying the metric to real-world LLMs (GEMMA-2, QWEN-2, and LLAMA-3 families) reveals a sharp correspondence between RoPE frequencies and attention head behavior.\n\nVery interesting finding about high frequencies in RoPE"}, "weaknesses": {"value": "Analysis done on limited set of tasks for real world LLMs ( binding task for real models)\nIt is not clear how reliable and computationally expensive metric in practice"}, "questions": {"value": "Is score input dependent? Could the head act symbolically on one sequence and positionally on another?\nHow the score is affected by block boundaries?\nAre you considering only swaps but not all permutations of input sequence? How does this affect score?\nHow expensive is calculation of such scores in practice?\n\nline 389. nit: Frecuency ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WevcmbKyTr", "forum": "V38yAoqddQ", "replyto": "V38yAoqddQ", "signatures": ["ICLR.cc/2026/Conference/Submission20805/Reviewer_x2bi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20805/Reviewer_x2bi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986427367, "cdate": 1761986427367, "tmdate": 1762935451631, "mdate": 1762935451631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}