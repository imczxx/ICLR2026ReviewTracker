{"id": "z8xjWmyQSZ", "number": 23765, "cdate": 1758348151754, "mdate": 1759896798377, "content": {"title": "Strong Correlations Induce Cause Only Predictions in Transformer Training", "abstract": "We revisit when Transformers can prioritize causes over spurious effects by viewing the problem through data correlation strength and the implicit regularization of gradient descent. We identify a phenomenon called Correlation Crowding-Out (CCO) arising from the training dynamics of Transformers. Specifically, under strongly correlated causal features, gradient descent filters out spurious cues and converges to a predictor that relies almost exclusively on the causes. Theoretically, using a simplified Transformer model trained on data from a minimal causal chain, we introduce a Dominant-coordinate condition that characterizes when CCO arises and explain its mechanism as a coupling of ''occupation'' and ''crowding-out''.  ''Occupation'' denotes to rapid growth of weights aligned with the dominant causal direction while non-dominant directions remain small. ''Crowding-out'' denotes to attention logits align with separation directions favoring the causal branch, suppressing descendants. We provide convergence guarantees for both the optimization trajectory and generalization. Our empirical results on simulated and real examples across various tasks including vision and natural language demonstrate the procedure. Together, these results show that, under suitable conditions, standard training alone can induce cause only prediction.", "tldr": "", "keywords": ["Implicit bias", "Transformers", "Optimization", "Causal robustness prediction"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c6a75ba65a9399bb262659aa9e4ff59b52f78ac.pdf", "supplementary_material": "/attachment/c9baacfd7f97206566f5ac878e0e0b63f9dc6c9c.pdf"}, "replies": [{"content": {"summary": {"value": "This manuscript identifies a phenomenon termed Correlation Crowding-Out (CCO), whereby gradient descent on Transformers under a dominance gap in correlation causes the network to rely solely on causal features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptually elegant, with positions causal invariance as an implicit-bias phenomenon.\n2. Theoretical development (Dominant-Coordinate Condition) is clear and connects optimization dynamics to causality."}, "weaknesses": {"value": "1. The “cause-only” notion is defined in correlation space, not interventionally -- terminological overreach.\n2. Limited experimental scope: small-scale ViT toy data only.\n3. Mathematical exposition skips proofs for convergence guarantees.\n4. Does not quantify how large the dominance gap must be in realistic data."}, "questions": {"value": "1. Include experiments with mixed-strength spurious features to test CCO boundary conditions.\n2. Discuss links to implicit-bias literature (max-margin GD) quantitatively.\n3. Clarify how “cause” vs “effect” features are labeled in synthetic datasets.\n4. Consider renaming to “dominant-feature selection” to avoid causal over-interpretation.\n5. Although the concept is interesting, it still needs to extend CCO tests to large language models (e.g., BERT for sentiment analysis) to determine whether the two-phase dynamics (occupation -> crowding-out) generalize beyond Vision Transformers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uKAi3m4N1c", "forum": "z8xjWmyQSZ", "replyto": "z8xjWmyQSZ", "signatures": ["ICLR.cc/2026/Conference/Submission23765/Reviewer_SPKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23765/Reviewer_SPKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450458677, "cdate": 1761450458677, "tmdate": 1762942797045, "mdate": 1762942797045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the phenomenon that they name CCO (correlations crowding out) in transformers. Under the hypothesis of strongly correlated causal features, the authors show that simplified transformer architectures, trained with GD, can filter out spurious dependencies in the data and synchronize with the causal directions. The analysis of the learning dynamics leads to the observation of two phases: \"occupation,\" in which there is a rapid growth of the weight in the causal direction, and the \"crowding out,\" in which the attention aligns its logits to favor the causal features and suppress the spurious ones.\nThe theoretical results are accompanied by simulations on both synthetic and realistic data."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The concept of CCO is interesting and the setting of the theoretical analysis, albeit simplified, correctly captures the main ingredients for the study of CCO in transformers trained with GD.  \n- The two-phases unveiled are both non-trivial and interestingly link causal data structure to architectural properties of self-attention.  \n- The proofs of the theorems are convoluted and contain non-standard ideas.  \n- Causal dominance seems to be a natural requirement satisfied in concrete cases."}, "weaknesses": {"value": "- The transformer model is rather simplified with respect to practice.  \n- The training algorithm is also greatly simplified, with attention and $w$ learned at different moments. It is not clear from the theorems whether this fundamentally shapes the two-phase behavior described, or whether the two phases would persist also in the case of a standard training protocol. This weakness is partially compensated by Section 6.1, in which we see simulations with standard GD show the same phenomenology.  \n- Figure 4a is hard to see, and it is not clear what point it makes.  \n\n#### Minor remarks\n- There is a mistake in Example 25: in line 1660 the coefficient of $x_1$ is 1.  \n- In line 284 the index should be $t+1$, not $t$.  \n- The notation is sometimes confusing; for instance, why do you introduce $\\tilde v$ if it is equal to $q$?"}, "questions": {"value": "- I wonder how much of the result has to do with _causality_ and how much with _dominant correlation_. Since you do not perform invariance study with interventions, how exactly does the fact that $x$ \"causes\" $y$ impact the theorems? In a specular setting with dominant correlations between $y$ and $z$, and weaker ones between $x$ and $y$, could we see a specular result, or would the inverted causal dependencies forbid it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sI7hVR3HBG", "forum": "z8xjWmyQSZ", "replyto": "z8xjWmyQSZ", "signatures": ["ICLR.cc/2026/Conference/Submission23765/Reviewer_9kRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23765/Reviewer_9kRu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664676044, "cdate": 1761664676044, "tmdate": 1762942796548, "mdate": 1762942796548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose to inspect a phenomenon called Correlation Crowding-Out (CCO), which arises during transformers training where, under a precise set of assumptions, transformers learn to exclude spurious features in favor of the true causal factors as training progresses. The authors argue that CCO arises because weights aligned with causal features grow and stabilize during training.\n\nMore generally, a two-phase process is assumed where the models quickly adapts to the true causal features (while still remaining attention to spurious links). The authors provide a detailed theoretical analysis, providing bounds for CCO effects for gradient descent training on transformers. Experiments on synthetic tabular-, visual- and natural language data are provided to support the theory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The research question is interesting as only a limited number of works analyze the conditions under which transformers learn to distinguish between causal and spurious features. The authors present an extensive theoretical analysis on the proposed problem and, to the best of my knowledge, are the first to provide such a formal analysis on the theory of causal transformer learning.\n\nThe paper is generally well motivated and structured. The overall claims and theory seem reasonable and is well laid out. The presented theorems and proofs seem to be generally plausible and consistent, although I did not check them in detail.\n\nTheoretical results are supported by experiments on simulated tabular-, vision- and natural language data and cover a wide range of applications, demonstrating the robustness of the proposed theory under a distinct range domains."}, "weaknesses": {"value": "1) **Causal Inference.** In unintervened settings (in the Pearlian Causal or Bayesian Network sense) it should be perfectly fine to infer y from z, assuming an equal quality of the links x->y and y->z. Given that the authors assume x->y to be a linear relation and y->z to be a more complex Lipschitz function, x->y should generally be expected to be easier to learn. Particularly the initially described \"occupation\" phase might simply be a byproduct of the different link qualities. I would like to suggest to at least mention this aspect on the possible validity of causal and anti-causal inference in the paper.\n2) **Experimental Alignment.** To test their theory on image data, the authors create a synthetic waterbirds-like dataset. In their setup, the learned foreground and background features, however, differ in quality - \"object-like\" versus \"scenery\". This difference in feature quality could in theory be a cause of the transformer focusing in the foreground feature, artificially introducing the \"occupation\" effect. A more sound setup would be to analyze the evolution of attention on images containing two features of the same type and/or complexity, e.g. two bird of different species in the image. While the crowding-out theory still yields important insights on the evolution of feature and attention, I am unsure whether the initial motivation of shifting attention actually aligns with the described problem setup. \n3) **Intuition on Theory.** The authors employ an extensive theoretical analysis under assumptions that are not further justified. For example, the moment and boundedness conditions (Eq. 3 - it would help referencing equations, if the authors could add numbers beyond the first two equations) and following sup-bounds are no further elaborated. I suspect that the particular setup is a key assumption to why a model should prefer x->y against predicting z->y. Generally, I the authors should discuss more clearly and extensively when the conditions necessary for CCO arise and can actually hold in real-world scenarios.\n4) **Choice of the Model.** The authors do not elaborate why the particular two-key attention architecture is chosen over a standard transformer with concatenated features. The paper could be improved by stating whether this particular architecture is required to obtain the observed results and how it would generalize to larger models. Similarly, it not described whether the value vector $\\tilde{v}$ assigned to $q^t$ is a concatenation of $v_x$ and $v_z$ for both attention products $l_{x/z}$ or whether they are fed separately into $l_x$ and $l_z$. \n\n\n\n**Missing references to the appendix:** The proofs in the appendix should be referenced in the main text. Also Fig.2 is un-referenced.\n\n**Typos:** \n\n- Line 20 and 22: \"to\" -> \"the\"\n- Line 47: GD is abbreviated as GD before the mention in line 57.\n- Line 179: \"feature\"->\"features\"\n- Line 188: \"exhibit\" -> \"exhibits\"\n- Line 205: \"dominance\" -> \"dominant\"\n- Line 206: \"feature\" -> \"features\"\n- Line 225: \"with noise\" is repeated.\n- Line 294: \"a diagonal\" is repeated.\n- Line 306: \"preserves\" -> \"preserving\"\n- Line 478: \"phenomenon CCO for Transformers\" -> \"phenomenon for transformers training dynamics called CCO\" or something similar."}, "questions": {"value": "My questions primarily regard the mentioned weaknesses:\n\n1) Could the authors further elaborate on the insufficiency of anti-causal predictions in the chosen scenario? Under which scenarios would and z->y prediction become invalid, and how does this reflect in the made assumptions and data generation?\n2) Could the authors comment on the possibility of the observed occupation effects being due to varying feature quality or link complexity?\n3) Which assumptions with regard to the model and data generating process do the authors make in their paper? Are these assumptions expected to be observed in real-world scenarios and how do the results generalize beyond the made assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HqXHPowLsl", "forum": "z8xjWmyQSZ", "replyto": "z8xjWmyQSZ", "signatures": ["ICLR.cc/2026/Conference/Submission23765/Reviewer_DtiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23765/Reviewer_DtiM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744918933, "cdate": 1761744918933, "tmdate": 1762942796247, "mdate": 1762942796247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes how transformers learn differently strong signals in data. They define causal signals as the target signals that dominate in data, and verify in a stripped down transformer that the dominating signal is learned over the training duration, edging away the spurious signal. In three experiments, they verify that the transformer architecture indeed ends up learning the stronger, causally relevant signal from the data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This reviewer is not deeply familiar with the transformer-theory literature, but the theoretical framing is potentially original and useful. (Perhaps other reviewers/AC with deeper familiarity of the topic could confirm whether similar derivations already exist)\n\n- The paper poses an original and conceptually stimulating hypothesis: that under strong causal alignment, gradient descent in Transformers may implicitly suppress spurious correlations.\n\n- The mathematical formulation is clean and logically structured. The \"Dominant-Coordinate Condition\" and the theoretical two-phase training dynamics are novel in how they connect implicit bias to causal learning.\n\n- The analysis isolates the attention mechanism and offers a mechanistic interpretation (\"occupation\" and \"crowding-out\") that could inspire further study.\n\n- The clarity of exposition is commendable. Even though the setup is theoretical, the derivations are readable and well-motivated.\n\n- The authors attempted to empirically verify their theory, including both simulated and real tasks.\n\n- Conceptually, this paper asks a meaningful question that sits at the intersection of optimization dynamics, causal inference, and representation learning - a very relevant direction."}, "weaknesses": {"value": "1.) Architectural realism.\nThe experiments and the two-key, gated-query setup differ substantially from classical self-attention. It remains unclear whether the observed CCO mechanism holds in standard multi-token Transformers where Q, K, V are interdependent.\n\n2.) Experimental limitations and questionable causal setup.\nThe empirical evidence is far too weak to substantiate the claims.\n\nThe toy experiments are overly simple and serve mainly as visual confirmation of the theory rather than genuine empirical validation. The NLP experiment has no quantifiable way of measuring spuriousness that is in the data.\n\nIn the Waterbirds experiment, the authors treat the background as the causal feature and the bird type as spurious. This is opposite to the conventional bias/shortcut setup. The background is typically the spurious cue, as it is the easier signal. This inversion undermines the causal argument and makes the results hard to interpret. To be credible, the experiment should make the bird type the causal target and the background the bias.\n\nEven more problematic is the bias strength of 70%. This is far too weak to demonstrate spurious correlation effects. At 70%, even simple architectures easily learn the causal signal.\n\n3.) Replication evidence (my own test).\nTo prove my point that even simple architectures learn the causal signal, I reproduced a small experiment following the waterbirds setup in Sagawa et al. [1]. Using the CUB and Places datasets, I balanced the waterbird/landbird classes (so that accuracy equals balanced accuracy) and applied a 70% background bias. I used the bird type as the label and the background as the spurious attribute, which aligns with standard fairness and bias-mitigation literature. I trained a ResNet-18 under standard ERM and also with the GDRO method [1]. The training set contains the 70% bias, while the test set is perfectly balanced/unbiased. Training on biased data and testing on unbiased data is a reliable method to verify whether methods have learned the causally relevant features [1,2,3,...]. Results, rounded:\n\nOracle (ResNet-18 trained on unbiased data): 96% accuracy\n\nResNet-18 ERM (trained on biased = 70%): 95% accuracy\n\nGDRO (ResNet-18 backbone, trained on biased = 70%): 95% accuracy\n\nThese results show that with such weak correlation strength, even a plain CNN (ResNet) is not misled by the bias and the model simply learns the dominant causal signal. Thus, the authors' experiment cannot reveal anything unique about transformer dynamics. To show a genuine robustness effect, they should raise the bias correlation to ≥ 90% or 95%, as commonly done in prior work [1, 2, 3]. Furthermore, the authors must report quantitative numbers and not only qualitative visual cues.\n\n4.) Lack of baselines.\nThe paper does not compare against CNNs, MLPs, or other baselines. Without this, one cannot tell whether CCO is a transformer-specific property or simply an instance of correlation dominance common to most architectures.\n\n5.) Triviality of the claim.\nThe theoretical result essentially formalizes the intuitive statement that if one signal is much stronger, gradient descent will focus on it. This is not necessarily \"causal learning\". It's correlation bias under strong dominance. Similar statements can likely be derived for CNNs and other architectures. The true open question is, whether transformers produce more causally stable predictions than other networks, or whether their implicit bias provides tighter robustness bounds under stronger spurious dependencies.\n\nIn short, while the theory is elegant, the experiments do not convincingly demonstrate anything beyond the trivial: when one feature is much more predictive, the model will use it. As shown above, even a ResNet-18 attained these results out of the box if the causal signal significantly dominates (as given by the authors experiments). Whether transformers are better for robust learning than other relevant baselines is a very interesting question. As is, this paper is not publication ready, as the claims cannot be proven beyond the trivial. I'm open to discussing my points during the rebuttal.\n\nReferences (as cited in review):\n[1] Sagawa et al. \"Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization.\" arXiv 1911.08731 (2019).\n[2] Makar et al. \"Causally Motivated Shortcut Removal Using Auxiliary Labels.\" AISTATS 2022.\n[3] Liu et al. \"Just Train Twice: Improving Group Robustness Without Training Group Information.\" ICML 2021."}, "questions": {"value": "- Can the authors provide fair experiments with waterbirds (stronger bias, i.e. 90-95%) and use ResNet and other modern architectures as baselines?\n\n- Can the authors provide even more experiments with realistic data, in which the bias is explicitly known (the NLP dataset is not suitable, as far as I understand, as spuriousness is not labeled or controllable)?\n\n- Do the authors think that transformers are substantially different to CNNs when it comes to picking up the stronger signal in data? Any evidence for this, be it theoretical or empirical if yes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PFYORj0B83", "forum": "z8xjWmyQSZ", "replyto": "z8xjWmyQSZ", "signatures": ["ICLR.cc/2026/Conference/Submission23765/Reviewer_hNry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23765/Reviewer_hNry"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892087487, "cdate": 1761892087487, "tmdate": 1762942795922, "mdate": 1762942795922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is trying to answer a very specific, surprisingly under-asked question:\nWhen can a vanilla Transformer, trained by plain ERM + gradient descent on one environment, end up using only the causal feature and ignore the spurious ones — without IRM, DRO, group labels, or any of the usual causal tricks?\n\nTheir answer is: it happens when the causal signal is uniformly stronger than every competing spurious signal — and gradient descent in a Transformer has an implicit bias that pushes attention to that dominant causal direction. They call this phenomenon Correlation Crowding-Out (CCO)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Positioning CCO as the “mirror regime” of shortcut learning (spurious wins early vs causal wins early) is neat.\n\n- The occupation → crowding-out story is crisp and gives an intuitive training-dynamics explanation. Theorem 1 cleanly proves the two-phase story—occupation then crowding-out.  The training-time schedule and resulting bounds are explicit.\n\n- The Frisch–Waugh–Lovell construction shows population least squares retains a nonzero spurious coefficient even with a dominant causal feature—so the cause-only outcome is due to optimization + attention geometry, not merely stronger correlations.\n\n- The two-token, two-branch attention setup with a squared-parameter head is cleverly chosen: simple enough for precise proofs,\nstill recognizably “Transformer-like”, and directly exposes the competition between causal vs descendant branches."}, "weaknesses": {"value": "- Your main theoretical result (Theorem 1) relies on a three-stage schedule where you alternately freeze the head and the gate. Can you clarify whether your informal claims about “standard training alone” are meant to apply to simultaneous GD on all parameters?\nDo you have either (a) a theoretical argument, or (b) empirical evidence, that joint updates (no freezing) still exhibit the same occupation → crowding-out dynamics and cause-only behavior?\nIf not, could you re-scope the claim or discuss when the staged schedule is a realistic approximation to how Transformers are trained in practice?\n\n- All formal results appear to be proved for the squared-parameter head  $\\hat{y} = h^\\top w^{\\odot 2}$, while the difference-of-squares generalization $\\hat{y} = h^\\top \\bigl(w^{\\odot 2} - v^{\\odot 2}\\bigr)$ is only mentioned briefly.\nCan you clarify whether Theorem 1 and Theorem 2 actually extend to the  difference-of-squares parameterization (and hence to a general signed linear head)? If they do extend, could you outline the additional arguments needed to handle the dynamics of both $w$ and $v$ and possible cancellations between $w_j^2$ and $v_j^2$? If they do not, it would be helpful to state explicitly that the current guarantees\nare restricted to the squared-only head.\n\n- Your Dominant-Coordinate Condition combines (i) a uniform population dominance gap and (ii) per-sample margin and sign-stability assumptions on the dominant causal coordinate. How realistic do you believe these assumptions are in typical high-dimensional settings with multiple causal parents, mixed-sign effects, or occasional sign flips?\nAre there weaker or more invariant conditions (e.g., dominance in norm, or dominance on average but not per-sample) under which you conjecture CCO would still hold?\nCould you comment on how sensitive your mechanism is to mild violations of sign-stability or margin—for example, do small fractions of “bad” samples already break the crowding-out behavior?\n\n- In Theorem 2, the lower bound on $p_{T^*}$ is written as $p_{T^\\*} \\ge 1/d^2$, which seems inconsistent with the training result $p_{T^\\* i} \\geq 1 - 1/d^2$ and  with the textual claim that the gate ``prefers'' the causal branch. Should the bound in Theorem 2 instead be $p_{T^\\*} \\geq 1 - 1/d^2$?"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Jdcg5ksooK", "forum": "z8xjWmyQSZ", "replyto": "z8xjWmyQSZ", "signatures": ["ICLR.cc/2026/Conference/Submission23765/Reviewer_1yaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23765/Reviewer_1yaN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317863948, "cdate": 1762317863948, "tmdate": 1762942795442, "mdate": 1762942795442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}