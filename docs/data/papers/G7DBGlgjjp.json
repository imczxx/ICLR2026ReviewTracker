{"id": "G7DBGlgjjp", "number": 11934, "cdate": 1758204768475, "mdate": 1763661924045, "content": {"title": "Uni-DPO: A Unified Paradigm for Dynamic Preference Optimization of LLMs", "abstract": "Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning difficulty, leading to suboptimal data utilization and final performance. To address this challenge, we propose **Uni-DPO**, a unified dynamic preference optimization paradigm that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Uni-DPO enables more effective utilization of training data and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Uni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Uni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on Arena-Hard. On mathematical reasoning and multimodal tasks, Uni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be made publicly available.", "tldr": "", "keywords": ["Deep Learning", "Large Language Model", "Preference Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eeb89c8cb6c90d371841be1ec1a23155a3897d44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper develops what’s called a “unified dynamic preference approach”, UNI-DPO, aiming to improve extend the basic DPO. Some key elements of UNI-DPO include: two weighting factors, w_qual and w_perf, which apply, respectively, to the preference pair (y_win, y_loss) and to the current policy \\pi_\\theta; the former takes into account data quality, and the latter measures how the policy aligns with the pair. Furthermore, a “calibrated negative log-likelihood loss” (c-NLL) term is added to the training objective, to amplify the policy’s confidence on y_win. These are elaborated in \\S3.2, 3.3. and 3.4."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the approach is to dynamically adjust each sample’s contribution based on both data quality and the model’s learning performance. This effectively incorporates ideas like advantage-based schemes in RL and thereby improves training efficiency and overall performance."}, "weaknesses": {"value": "The weight w_qual defined in (5) is a bit simplistic in view of the complexity in defining (or capturing) what is data quality. This is also partially acknowledged by the paper (at the end)."}, "questions": {"value": "Can the author(s) comment on how does the dual weighting scheme in UNI-DPO compare with other approaches to preference alignment and reward optimization, and specifically the weighting factors motivated by importance sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zlNejAFO58", "forum": "G7DBGlgjjp", "replyto": "G7DBGlgjjp", "signatures": ["ICLR.cc/2026/Conference/Submission11934/Reviewer_HSvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11934/Reviewer_HSvf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600806390, "cdate": 1761600806390, "tmdate": 1762922937704, "mdate": 1762922937704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Uni-DPO, which injects two jointly learned adaptive weights into single-pass offline training: 1) a quality weight derived from external score margins that up-weights high-confidence preference pairs, and 2) a performance weight computed from the current policy margin that down-weights already well-fitted samples while emphasizing those that remain hard. Additionally, to counteract DPO’s tendency to suppress the absolute likelihood of preferred responses, the authors devise a calibrated negative log-likelihood term that selectively reinforces difficult yet high-quality positive examples. The resulting objective preserves the simplicity of DPO but enables gradient allocation at a fine-grained level that accounts for both sample difficulty and data quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper derives the gradient coefficient of Uni-DPO in closed form, explicitly integrating two modulation factors, including quality weight and performance weight, to provide a principled explanation for online sample re-weighting.\n- Uni-DPO preserves the simplicity of single-stage offline training without introducing additional reward models or iterative sampling overhead. With only two learnable weights and a calibrated loss, it can achieve consistent performance improvements across diverse benchmarks."}, "weaknesses": {"value": "- Although the paper proposes a \"unified dynamic weighting paradigm\", its core idea essentially combines a quality-aware weight with a performance-aware weight. Such sample reweighting concepts have already been extensively studied in machine learning and RLHF literature, including focal loss, curriculum learning, and advantage reweighting. Therefore, the conceptual novelty of Uni-DPO appears somewhat incremental rather than fundamentally new.\n\n- A major concern lies in the lack of essential baselines. While the paper focuses on DPO-based alignment, it would be important to include comparisons with more recent RL-based alignment methods beyond PPO, since several advanced RL algorithms have recently demonstrated improved stability and performance in preference optimization. Moreover, to my knowledge, there already exist studies on data selection and adaptive weighting for DPO that explicitly aim to exploit heterogeneous data quality during training. Including these approaches as baselines would provide a fairer and more convincing empirical validation.\n\n- Although Uni-DPO claims to \"outperform SimPO\", its improvement may come from the weighting strategy rather than a new optimization formulation. In contrast, SimPO already removes the reference model and introduces length normalization, while Uni-DPO still relies on the log-probability ratio between the policy and the reference model. This raises questions about whether the claimed superiority originates from a fundamentally different principle or just an additional heuristic weighting scheme.\n\n- Uni-DPO is only evaluated on textual understanding and mathematical reasoning tasks, while multimodal experiments are mentioned in the abstract but not presented in the results section. The lack of evaluations on other important aspects such as dialogue safety, factual consistency, or instruction robustness weakens the \"Unified\" claim. Additionally, the models used for evaluation (e.g., LLaMA-3-8B, Gemma-2-9B) appear to be relatively early versions; including results on more recent model releases would strengthen the credibility and contemporaneity of the paper."}, "questions": {"value": "Some suggestions here.\n\n- The right-hand side of Figure 3 does not clearly or intuitively convey the core idea of the proposed method. Its meaning only becomes apparent after reading the entire paper and integrating multiple sections. It is recommended to redesign or annotate the figure to make the conceptual distinction between DPO and Uni-DPO more self-explanatory and visually interpretable.\n\n- It would be helpful to include training curve comparisons (e.g., loss or evaluation score versus training steps) to further illustrate the differences in learning efficiency among Uni-DPO, DPO, and SimPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jApo14ObVN", "forum": "G7DBGlgjjp", "replyto": "G7DBGlgjjp", "signatures": ["ICLR.cc/2026/Conference/Submission11934/Reviewer_Azer"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11934/Reviewer_Azer"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830492019, "cdate": 1761830492019, "tmdate": 1762922937298, "mdate": 1762922937298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Uni-DPO for fine-tuning large language models with human feedback that dynamically reweights preference training pairs based on each pair’s quality and the model’s evolving performance. This addresses the limitation of standard DPO, which treats all feedback equally. By explicitly emphasizing high-quality, challenging preference pairs during training, Uni-DPO enables more effective use of data, which in turn yields better alignment performance across diverse benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly identifies that standard DPO treats all preference pairs uniformly, which underutilizes high-quality feedback and fails to adapt to varying task difficulty. Uni-DPO tackles this by adaptive sample weighting, allowing the model to focus on informative training examples and thus improving learning efficiency. This insight targets an important problem in RLHF.\n2.\tThe quality aware weight prioritizes pairs with larger expert score margins, while the performance aware weight, inspired by focal loss, shifts focus toward underfitted examples. This addresses observed mismatches between external score margins and the model’s reward margin.\n3.\tUni-DPO shows consistent improvements on diverse tasks. Experiments show that Uni-DPO consistently outperforms vanilla DPO and SimPO baselines on multiple language understanding benchmarks, mathematical reasoning datasets, and multimodal tasks."}, "weaknesses": {"value": "1.\tA notable concern is that Uni-DPO requires a “quality score” for each preference pair, often obtained via human annotation or a powerful proxy model like GPT-4. This introduces an extra dependency on external evaluators (in effect, a form of reward signal), partially undermining the simplicity of the reward model-free DPO paradigm. If these quality scores are noisy, biased, or unavailable, it’s unclear how well the method would perform. The authors themselves acknowledge that training data quality is critical and better methods for estimating it are needed. In scenarios without reliable prior scores, the applicability of Uni-DPO could be limited or require additional effort (e.g. training a proxy reward model), which diminishes its practicality.\n\n2.\tThe contribution of Uni-DPO, though useful, can be seen as a combination of existing ideas in preference optimization rather than an entirely novel invention. The method builds on known techniques: using an auxiliary NLL loss to counter DPO’s bias (as in Pal et al., 2024), applying length normalization to remove sequence-length bias, and adapting focal loss from classification to reweight easy vs hard examples. The quality-based data filtering is conceptually akin to curriculum learning or prioritizing high-confidence preferences, which have precedents in the literature, although not in the exact form used here. While the unification of these components into one framework is valuable, the paper does not derive fundamentally new theory beyond this integration, nor introduce a wholly new training paradigm. It primarily refines DPO with well-motivated heuristics. \n\n3.\tBy focusing training on “hard” and high-quality examples, there is a risk, which is not deeply explored in the paper, of overfitting or bias if these weights are mis-calibrated. It would be better to introduce a calibrated performance weight to avoid instability, indicating the approach required careful handling to train reliably. It would be good to know how consistently training converges under Uni-DPO versus standard DPO. For instance, do some runs diverge or get stuck if the thresholds are set poorly?\n\n4.\tWhile c-NLL is proposed to counteract DPO’s preference suppression bias, the paper does not explore its impact on diversity or other alignment metrics. It also does not evaluate fairness or bias trade offs introduced by weighting high quality data more heavily."}, "questions": {"value": "1. Could Uni-DPO be combined with reward model–based RLHF methods (e.g., PPO) to further improve alignment? How would the dual perspective weighting interact with explicit reward signals?\n2. How sensitive is Uni-DPO to the source and accuracy of the quality scores $w_{qual}$? For example, if one uses a weaker judge model or noisy human preference ratings to derive the score margin, does Uni-DPO still outperform vanilla DPO by a large margin?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8iJAlkQjA6", "forum": "G7DBGlgjjp", "replyto": "G7DBGlgjjp", "signatures": ["ICLR.cc/2026/Conference/Submission11934/Reviewer_5qbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11934/Reviewer_5qbs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980026127, "cdate": 1761980026127, "tmdate": 1762922936927, "mdate": 1762922936927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Uni-DPO, a “dual-perspective” extension of Direct Preference Optimization that dynamically reweights each preference pair by (i) a quality-aware weight $w_{\\text{qual}}$ estimated from external expert scores and (ii) a performance-based weight $w_{\\text{perf}}$ that emphasizes pairs the model currently fails to fit; it also adds a calibrated NLL term for difficult, high-quality positives. The stated goal is to correct DPO’s uniform treatment of pairs and better utilize data during preference learning for text, math, and multimodal LLMs. Empirically, the authors report consistent gains over SFT, DPO, and SimPO on instruction-following (AlpacaEval2, Arena-Hard, IFEval, SedarEval), math (GSM8K, MATH, MinervaMath, etc.), and multimodal suites, and claim Gemma-2-9B-it+Uni-DPO surpasses Claude 3 Opus on Arena-Hard."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and formulation: The paper explicitly identifies DPO's uniform-pair limiattions and proposes a dual weighting + c-NLL objective.\n- Empirical gains across domains: Reported improvements over SFT/DPO/SimPO on text, meth and multimodal data depicts the strength of Uni-DPO\n- Ablations: The paper shows that removing each of the component degrades performance, empirically supoprting the necessity of each component."}, "weaknesses": {"value": "- Lack of transparency and reproducibility of the scalar quality score $w_{\\text{qual}}$.\n    - The core of Uni-DPO's contribution, $w_{\\text{qual}}$, depends on external scalar scores produced by \"expert evaluators\" (e.g., GPT-4). However, the exact procedure is not disclosed. Since $w_{\\text{qual}}$ is an important component of Uni-DPO, the procedure of obtaining this score should be thoroughly described in the paper.\n- Insufficient engagement with existing sample-wise weighting preference optimization (PO) literature.\n    - There are no related work section regarding the prior methods that apply per-sample or instance-wise weighting in PO[a,b,c,d]. This weakens the novelty and makes it unclear whether if Uni-DPO has any overlaps between the related works.\n- Potential unfairness from applying Length Normalization (LN) only to Uni-DPO.\n    - The paper notes that LN is used when computing rewards or scores for Uni-DPO, but not for baseline method such as DPO in Table 1. As LN can boost the performance regardless of the underlying loss, the fairness of the baseline is questionable.\n- External scorer dependence and fairness\n    - The $w_{\\text{qual}}$ depends on strong proprietary LLM scorers, which hinders the fairness against baselines trained with weaker/cheaper labels. How is using these external strong models justified?\n- If these weaknesses are taken care of, I am willing to raise my score.\n\n[a] Reward Difference Optimization For Sample Reweighting In Offline RLHF, ACL, 2024\n\n[b] Enhancing RLHF with Weighted Preference Optimization, EMNLP, 2024\n\n[c] MWPO: Enhancing LLMs Performance through\nMulti-Weight Preference Strength and Length Optimization, ACL, 2025\n\n[d] Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts, ArXiv, 2025"}, "questions": {"value": "- Described in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rgMSsbFxMO", "forum": "G7DBGlgjjp", "replyto": "G7DBGlgjjp", "signatures": ["ICLR.cc/2026/Conference/Submission11934/Reviewer_uYa8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11934/Reviewer_uYa8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996443323, "cdate": 1761996443323, "tmdate": 1762922936535, "mdate": 1762922936535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}