{"id": "d0gvsym66h", "number": 9519, "cdate": 1758125858939, "mdate": 1759897714711, "content": {"title": "NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction", "abstract": "Nucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. We introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative sequence models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to catalyze progress in nucleic-acid modeling and to support downstream applications in nucleotide molecular design, synthetic biology, and biochemistry. Our code is available at https://anonymous.4open.science/r/NABench-20CB.", "tldr": "We present a large-scale evaluation of nucleotide foundation models across diverse tasks, providing deeper insights into nucleic acid fitness prediction.", "keywords": ["Foundation model", "Fitness prediction", "Nucleic acid"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da275053f45744c5156ff81cc628f0fa636d8909.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed, NABench, a large-scale, standardized benchmark to fairly compare nucleotide foundation models (NFMs) on sequence-to-fitness prediction tasks across DNA and RNA. It aggregates 162 assays and ~2.6M mutated sequences spanning mRNA, tRNA, ribozymes, aptamers, enhancers, promoters, and exons, surpassing prior fitness benchmarks in scale and diversity. The authors reprocessed >110 experiments with a unified pipeline (quality control, read trimming/merging, deduping, clustering) to produce consistent datasets and splits with rich metadata. NABench evaluates 29 representative models (BERT-like, GPT-like, Hyena-like, and others) under zero-shot, few-shot, supervised, and transfer-learning regimes. They evaluate these models on four complementary metrics (Spearman correlation, NDCG, AUC, and MCC) that provide both ranking fidelity and top-variant discovery power. Across DMS tasks, no single model dominates in zero-shot, while BERT-style encoders often lead once supervision is introduced. Results highlight a notable gap between random and position-contiguous splits, exposing challenges in out-of-distribution generalization across mutational regions. For SELEX with assays with random sequence pools, zero-shot performance is weak across the board, but supervision helps, underscoring limitations of current NFMs on synthetic sequence spaces. NABench, released with code and data, establishes reproducible baselines and clear failure modes to guide future, possibly structure-aware, nucleotide modeling and design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- NABench aggregates 162 assays and ~2.6M mutants across DNA and RNA, much larger than prior fitness benchmarks, giving statistically robust comparisons. The authors also release code and data with a clear evaluation recipe, supporting community adoption and extension. \n- The benchmark spans seven nucleic-acid families (mRNA, tRNA, aptamers, ribozymes; DNA enhancers, promoters, exons), supporting conclusions that generalize across molecule types and assay designs. \n- Over 110 raw experiments are reprocessed with a consistent and unified workflow (QC, trimming/merging, deduplication, clustering), reducing dataset bias and making results comparable.\n- Models are tested under zero-shot, few-shot, supervised, and transfer-learning settings, capturing both intrinsic knowledge and supervised learning efficiency.\n- The authors evaluate 29 representative NFMs in a common framework, enabling fair head-to-head comparisons.\n- The metrics are complementary and capture correlation, high-score sequence prioritization, and classification power, aligning evaluation with real experimental goals.\n- Both random and contiguous splits are used, explicitly testing interpolation vs. extrapolation and revealing generalization gaps that matter in practice. \n- The paper analyzes architecture- and data-specific performance (e.g., Hyena/Evo strengths in zero-shot; BERT-style gains with supervision) and highlights efficiency–performance trade-offs for long sequences."}, "weaknesses": {"value": "I will discuss them together with questions."}, "questions": {"value": "I appreciate the huge amount of time the authors have spent on collecting, curating, processing and harmonizing this big dataset. This effort for sure has the potential to become one of the main benchmarks in evaluating nucleic acid foundation models. However, I have some questions and reservations, which I will list below. \n\n- I get the feeling that the manuscript and data preparation is a bit rushed and does not have the quality and consistency needed for such a big task. I downloaded the provided supplementary data and randomly picked two datasets: Kolm et al. (2020), which a SELEX experiment for finding aptamers against bacterial growth and Baeza-Centurion et al. (2020), which is a DMS for studying impacts of mutations on alternative mRNA splicing. I did not find any data for Kolm and found this just one sequence `GGGAAATTTCCCCCCAAAGGGAAATTTCCCTTTAAAAAACCCTTTTTTGGGCCCTTTGGGTTTGGGGGGTTTTTTGGGTTTGGGTTTCCCTTTCCCCCCTTTGGGCCCTTTTTTCCCTTTCCCCCCCCCGGGAAATTTTTTCCCTTTAAAGGGTTTAAAAAATTTTTTGGGTTTTTTTTTGGGGGGGGG` for FAS exon 6 experiment. I am not sure what this is, does not map anywhere, and is different from the version in the original paper: `TGTCCAATGTTCCAACCTACAGGATCCAGATCTAACTTGCTGTGGTTGTGTCTCCTGCTTCTCCCGATTCTAGTAATTGTTTGGGGTAAGTTCTTGCTTTGTTCAAACTGCAGATTGAAATAACTTGGGAAGTAG`, which included a doped version of FAS exon 6 and a few bases from the flanking introns. \n- Overall, performance metrics are pretty low and random-like. The drastic performance drop across the board from “Random CV” to \"Contiguous CV” indicates that more sophisticated data splits are needed, and some of these models are simply cheating.\n- Contiguous (position-blocked) splits, potentially useful for OOD testing, aren’t meaningful for SELEX and thus aren’t used there, limiting extrapolation tests on that large slice of the benchmark. Why not use sequence similarity metrics or clustering them and then splitting the folds by cluster/max similarity? \n- Authors note some sequences may exist in pretraining corpora; while labels are unseen (so “zero-shot” stands), any overlap could advantage certain models and complicate fairness claims. \n- Zero-shot performance on SELEX is uniformly poor and essentially random (Spearman correlation ≲ 0.1; mean AUC < 0.6), limiting conclusions about model priors on synthetic libraries without supervision. This is in part an issue with how SELEX experiments are binarized and evaluated. In a successful SELEX experiment (where the initial random pool has some functional molecules), there will be many good sequences in the final round, so assigning the top 10% as positives and the rest as negatives might not be the best strategy. \n- AUC/MCC are computed by labeling only the top 10% sequences as positives; that fixed threshold is convenient but somewhat arbitrary and may not reflect assay-specific distributions. What if 50% of sequences are high-scoring and functional? I think a more sophisticated strategy should be used, e.g., fitting a GMM.\n- Zero-shot “fitness” is simply the average of the embedding vector, which is architecture-agnostic but arguably crude. In DMS data for example, most sequences might be embedded into a very small space (many mutations are not consequential, hence should not change anything). Maybe the authors can run PCA on embeddings from the unlabelled samples and use the top principal component, or train a small VAE on them. \n- The assay mix is heavily skewed (e.g., 94 SELEX aptamer assays and 2M out of 2.6M mutants vs far fewer per other families), which could bias aggregate rankings and hide family-specific strengths. On the same note, why even use aptamers? They are recognition/binding molecules, meaning that their “structure” is crucial, and this benchmark is ignoring them altogether.  \n- SELEX sequences are clustered (CD-HIT at 95%) partly to keep embedding runtime “reasonable,” which could merge near-neighbors and alter abundance/label granularity. \n- The benchmark explicitly focuses on sequence-only NFMs and defers structure-aware evaluation to future work, so it can’t yet assess models that leverage RNA/DNA 2D/3D structure or inverse folding.\n- Lastly, the paper's presentation needs some reworking. There are too many figures and too many tiny panels within them and they are not super helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7SLh5fdnCv", "forum": "d0gvsym66h", "replyto": "d0gvsym66h", "signatures": ["ICLR.cc/2026/Conference/Submission9519/Reviewer_qYsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9519/Reviewer_qYsy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789986474, "cdate": 1761789986474, "tmdate": 1762921087286, "mdate": 1762921087286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NABench, a large-scale, systematic benchmark for nucleotide fitness prediction , addressing the lack of standardized evaluation for Nucleotide Foundation Models. The benchmark aggregates 162 high-throughput assays, curating 2.6 million mutated sequences across diverse DNA and RNA families. NABench rigorously evaluates 29 representative NFMs, categorized by architecture, under a unified suite of four task settings: zero-shot, few-shot, supervised, and transfer learning. The results quantify performance heterogeneity, demonstrating that Hyena architectures excel in zero-shot DMS tasks , whereas BERT-like models show superior performance in supervised and few-shot settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Scale and Diversity: NABench is the largest benchmark in this domain, comprising 2.6M mutants and 162 assays , making it 8x larger than the previous SOTA (RNAGym). It covers both DNA and RNA and includes two fundamentally different data types: DMS and SELEX.\n\n- Comprehensive Evaluation: The benchmark tests 29 models and defines four rigorous evaluation paradigms, which is critical for a holistic understanding of model capabilities.\n\n- Rigorous Supervised Splits: The supervised setting wisely employs two data splitting strategies: \"Random Cross-Validation\" and \"Contiguous Cross-Validation\", the latter being more relevant for realistic scientific discovery.\n\n- Clear Insights: The results provide clear, actionable insights: Hyena/Evo models are optimal for zero-shot tasks , while BERT-like models are better for feature extraction. It also highlights the universal failure of current models on zero-shot SELEX prediction."}, "weaknesses": {"value": "- Lack of Structure: The benchmark focuses exclusively on sequence-only models. It explicitly omits hybrid models that incorporate nucleic acid structure , which is a growing and important area of research.\n\n- Limited Transfer Learning Analysis: Although transfer learning is listed as one of the four main evaluation settings, its analysis in the main paper is very brief. The transfer learning results for DMS tasks are missing entirely from the main results.\n\n- Oversimplified Supervised Probe: The supervised and few-shot evaluations use only a ridge regression probe. While good for testing static embedding quality, this may under-estimate models that would perform better with a more complex probe or full-model fine-tuning.\n\n- SELEX Task Failure: The near-random performance of all models on zero-shot SELEX is a key finding, but also suggests the task may be outside the scope of what current pretraining paradigms can achieve."}, "questions": {"value": "- Choice of Supervised Probe: Why did you choose a linear ridge regression probe  instead of a non-linear MLP probe or full-model fine-tuning? Could this choice systematically favor models whose embeddings are more linearly separable (like BERT )?\n\n- SELEX Generalization Gap: Does the failure on zero-shot SELEX imply that pretraining on \"natural\" sequences  fails to generalize to \"synthetic\" ones? Does this suggest \"fitness prediction\" is two separate tasks?\n\n- DMS Transfer Learning Results: What were the key findings for the transfer learning task on DMS datasets? Why were these results omitted from the main paper?\n\n- Embedding Strategy Sensitivity: You used different embedding strategies for different architectures. How sensitive are the model rankings to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "poFiv1SWuB", "forum": "d0gvsym66h", "replyto": "d0gvsym66h", "signatures": ["ICLR.cc/2026/Conference/Submission9519/Reviewer_rEPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9519/Reviewer_rEPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894244207, "cdate": 1761894244207, "tmdate": 1762921086861, "mdate": 1762921086861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents NABench, a large-scale benchmark for evaluating nucleotide foundation models in DNA and RNA fitness prediction. NABench compiles 162 assays with over 2.6 million mutated sequences from Deep Mutational Scanning and SELEX experiments. It standardizes data processing, train–test splits, and evaluation metrics to ensure consistent and fair comparison.\n\nThe benchmark assesses 29 models across four evaluation settings: zero-shot, few-shot, supervised, and transfer learning, using metrics such as Spearman correlation and AUC. Results show that no single model performs best in all regimes. State-space models perform better in zero-shot prediction, while BERT-style models perform better in supervised and few-shot conditions. The benchmark exposes clear generalization gaps between natural and synthetic data and provides a public platform for reproducible evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Substantially larger and more diverse dataset\nNABench contains 2.6 million sequences from 162 assays, making it around eight times larger than previous nucleotide benchmarks such as RNAGym. It covers both DNA and RNA tasks and spans diverse experimental types, providing a richer and more representative evaluation corpus.\n\n2. Broader model coverage\nThe benchmark includes 29 models across seven architecture families, such as BERT, GPT, Hyena, and Evo variants. This extensive coverage allows systematic comparison between transformer-based and state-space designs, offering the most comprehensive model-level evaluation in the domain to date."}, "weaknesses": {"value": "1. Clarification on dataset source.\nThe paper states that NABench integrates experimental measurements from Deep Mutational Scanning (DMS). However, according to Section 3.2, the dataset is derived from MaveDB, which broadly contains Multiplexed Assays of Variant Effect (MAVE), including both DMS and SELEX experiments. The authors should clarify whether they extracted only the DMS subset or used the full MaveDB collection. If the latter, they should revise phrasing such as “NABench integrates an extensive collection of experimental measurements from deep mutational scanning (DMS)” to accurately reflect the broader MAVE scope\n\n\n2. Embedding extraction for Evo models.\nIn Section 3.4.1, the authors note that Evo models “output sequence-level embeddings” and are used directly. However, Evo-2 (referenced in their baseline table) is trained using an autoregressive approach based on the StripedHyena 2 architecture. The Evo-2 paper emphasizes that middle-layer embeddings better capture fitness-related features, while final-layer embeddings mainly encode autoregressive objectives. The authors should verify whether the Evo embeddings used in NABench come from the last layer or an intermediate layer, and consider aligning their extraction strategy with the Evo-2 paper’s recommendation (i.e., use middle layers for fitness prediction, or use last layers consistently across AR-style models)"}, "questions": {"value": "1. **DNA–RNA conversion**\n   Please clarify how DNA and RNA sequences were handled for model input. Were DNA sequences converted (T→U) for RNA models or vice versa?\n\n2. **Contiguous cross-validation**\n   More details are needed on how contiguous CV was defined.\n\n3. **Species breakdown**\n   Do the datasets include viral, prokaryotic, and eukaryotic sequences? Any performance breakdown across these groups would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WYjIysujLj", "forum": "d0gvsym66h", "replyto": "d0gvsym66h", "signatures": ["ICLR.cc/2026/Conference/Submission9519/Reviewer_TTvY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9519/Reviewer_TTvY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935367280, "cdate": 1761935367280, "tmdate": 1762921086589, "mdate": 1762921086589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents NABench, a large-scale benchmark for evaluating nucleotide foundation models in DNA and RNA fitness prediction. NABench compiles 162 assays with over 2.6 million mutated sequences from Deep Mutational Scanning and SELEX experiments. It standardizes data processing, train–test splits, and evaluation metrics to ensure consistent and fair comparison.\n\nThe benchmark assesses 29 models across four evaluation settings: zero-shot, few-shot, supervised, and transfer learning, using metrics such as Spearman correlation and AUC. Results show that no single model performs best in all regimes. State-space models perform better in zero-shot prediction, while BERT-style models perform better in supervised and few-shot conditions. The benchmark exposes clear generalization gaps between natural and synthetic data and provides a public platform for reproducible evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Substantially larger and more diverse dataset\nNABench contains 2.6 million sequences from 162 assays, making it around eight times larger than previous nucleotide benchmarks such as RNAGym. It covers both DNA and RNA tasks and spans diverse experimental types, providing a richer and more representative evaluation corpus.\n\n2. Broader model coverage\nThe benchmark includes 29 models across seven architecture families, such as BERT, GPT, Hyena, and Evo variants. This extensive coverage allows systematic comparison between transformer-based and state-space designs, offering the most comprehensive model-level evaluation in the domain to date."}, "weaknesses": {"value": "1. Clarification on dataset source.\nThe paper states that NABench integrates experimental measurements from Deep Mutational Scanning (DMS). However, according to Section 3.2, the dataset is derived from MaveDB, which broadly contains Multiplexed Assays of Variant Effect (MAVE), including both DMS and SELEX experiments. The authors should clarify whether they extracted only the DMS subset or used the full MaveDB collection. If the latter, they should revise phrasing such as “NABench integrates an extensive collection of experimental measurements from deep mutational scanning (DMS)” to accurately reflect the broader MAVE scope\n\n\n2. Embedding extraction for Evo models.\nIn Section 3.4.1, the authors note that Evo models “output sequence-level embeddings” and are used directly. However, Evo-2 (referenced in their baseline table) is trained using an autoregressive approach based on the StripedHyena 2 architecture. The Evo-2 paper emphasizes that middle-layer embeddings better capture fitness-related features, while final-layer embeddings mainly encode autoregressive objectives. The authors should verify whether the Evo embeddings used in NABench come from the last layer or an intermediate layer, and consider aligning their extraction strategy with the Evo-2 paper’s recommendation (i.e., use middle layers for fitness prediction, or use last layers consistently across AR-style models)"}, "questions": {"value": "1. **DNA–RNA conversion**\n   Please clarify how DNA and RNA sequences were handled for model input. Were DNA sequences converted (T→U) for RNA models or vice versa?\n\n2. **Contiguous cross-validation**\n   More details are needed on how contiguous CV was defined.\n\n3. **Species breakdown**\n   Do the datasets include viral, prokaryotic, and eukaryotic sequences? Any performance breakdown across these groups would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WYjIysujLj", "forum": "d0gvsym66h", "replyto": "d0gvsym66h", "signatures": ["ICLR.cc/2026/Conference/Submission9519/Reviewer_TTvY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9519/Reviewer_TTvY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935367280, "cdate": 1761935367280, "tmdate": 1763501084767, "mdate": 1763501084767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors have undertaken a significant effort in curating a large-scale benchmark (162 assays, 2.6M sequences, 29 models) to systematize the evaluation of nucleotide foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The scale of the data curated in this benchmark is the core strength."}, "weaknesses": {"value": "However, I think this is a recycle from the previous NeurIPS Dataset and Benchmark track to ICLR, which immediately raised my eyebrow. After a careful read of this paper, I found the evaluation protocols have strong, unjustified biases. The analysis, while broad, remains superficial in key areas. This leads to my concerns about the conclusions, which may be artifacts of the benchmark's design rather than fundamental properties of the models being tested."}, "questions": {"value": "1. Your zero-shot evaluation protocol (Section 3.4.3) predicts fitness by \"computing the mean of the variant's embedding vector\". This is an extremely simplistic linear probe that implicitly assumes the models' pre-trained embedding spaces are already perfectly aligned with the downstream task of fitness prediction. Why was this naive approach chosen over more established and powerful zero-shot inference methods for language models, such as using masked token probabilities or pseudo-log-likelihoods? Does this not create a significant evaluation bias that conflates the quality of a model's representations with the arbitrary suitability of a mean-pooling probe?\n\n2. The transfer learning analysis (Section 3.6) is presented as a key evaluation setting but is arguably the most superficial. The analysis is limited to a single correlation matrix (Figure 11 in Appendix), with the primary conclusion: \"assays under the same experiment have higher correlation\". This is a nearly tautological finding. Why does the benchmark not investigate more challenging and scientifically interesting transfer learning scenarios, such as transferring from DMS to SELEX, from DNA to RNA tasks, or from common, data-rich families to rare, data-poor ones, which would truly test the \"transferable principles of nucleic acid biology\" you claim to probe?\n\n3. In Section 3.4.1, you mentioned that you use different, hand-picked strategies for extracting sequence-level embeddings from different model architectures (e.g., concatenating <cls> and mean-pooling for BERT, using the last hidden state for GPT). These are heuristics, not principled methods! How can you ensure that the performance differences reported are not simply artifacts of these arbitrary and potentially suboptimal embedding choices? Did you perform any ablation studies to demonstrate that your conclusions are robust to different embedding extraction strategies for each model?\n\n4. The benchmark evaluates an impressive 29 foundation models but appears to lack any non-foundation model baselines. A robust benchmark must ground its findings by comparing against strong, simpler alternatives. For example, how do these multi-billion parameter models compare against a well-tuned CNNs trained from scratch on a one-hot encoding of the sequence, or even simpler k-mer based regression models (e.g., a kernel ridge regression)?\n\n5. The paper correctly identifies that DMS (local mutations on a known scaffold) and SELEX (discovery of functional sequences from a random pool) are fundamentally different tasks. Indeed, your results show that model performance is 'significantly' different between them, with zero-shot performance on SELEX being near-random (Lines 424-428). Given this profound difference, why are results from both tasks aggregated into single \"overall performance\" metrics and rankings (e.g., Figure 2a)? Doesn't this aggregation obscure more than it reveals, by averaging performance over tasks that test completely different scientific capabilities (interpolation vs. extrapolation; local vs. global search)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tSYbqJJ7ri", "forum": "d0gvsym66h", "replyto": "d0gvsym66h", "signatures": ["ICLR.cc/2026/Conference/Submission9519/Reviewer_EUvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9519/Reviewer_EUvA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158871302, "cdate": 1762158871302, "tmdate": 1762921086250, "mdate": 1762921086250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}