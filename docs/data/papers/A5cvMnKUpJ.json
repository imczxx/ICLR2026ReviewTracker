{"id": "A5cvMnKUpJ", "number": 15486, "cdate": 1758251808829, "mdate": 1759897303781, "content": {"title": "Large Language Models Could Be Rote Learners", "abstract": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework that reformulates MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).", "tldr": "", "keywords": ["Interpretability and Analysis of Models for NLP", "Cognitive Modeling", "Generation", "Language Modeling", "Question Answering"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fba763ea3158368db888e92b72e1902195c3f2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates \"benchmark contamination\" in LLMs, distinguishing between genuinely learned capabilities and simple \"rote memorization.\"\nThe authors find that LLMs often perform worse on perfectly memorized questions from benchmarks like MMLU compared to non-memorized ones. This suggests superficial memorization doesn't equal understanding.\nTo address this, they propose TrinEval. This framework reformulates multiple-choice questions into a \"knowledge entity, attribute, and context\" trinity. This breaks the exact phrasing models might have memorized while keeping the core problem intact.\nExperiments show TrinEval effectively isolates genuine capability. Results indicate open-source LLMs rote-memorize about 20.5% of MMLU knowledge points, while truly mastering only 19.6%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Perspective: Instead of just trying to remove contaminated data, it studies how contamination (memorization) actually affects performance, and discovers a counterintuitive finding thatmemorized questions often have lower accuracy.\n- Practical Solution (TrinEval): The proposed reformulation method provides a concrete way to evaluate models more fairly, even if they have seen the test data."}, "weaknesses": {"value": "- Limited Scope: The study focuses almost entirely on multiple-choice questions in MMLU. It's unclear how well this applies to open-ended or reasoning tasks.\n- Dependence on LLMs for Reformulation: The TrinEval framework relies on LLMs to perform the question reformulation. This raises questions about whether the reformulation process itself introduces any biases from the model used."}, "questions": {"value": "- The core finding that LLMs perform worse on memorized MCQs is fascinating. Do you have a hypothesis as to why this occurs?\n- Have you considered applying the TrinEval methodology to other types of evaluation benchmarks beyond MCQs, such as those for mathematical reasoning (GSM8K, MATH) or code generation (HumanEval, LCB)? How might the \"knowledge-centric trinity\" be adapted for these different formats?\n- The analogy to human STM and LTM is insightful. Do your findings suggest that specific training strategies or architectural changes could encourage LLMs to develop more LTM-like knowledge representations and rely less on STM-like rote memorization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hbURmkpqEJ", "forum": "A5cvMnKUpJ", "replyto": "A5cvMnKUpJ", "signatures": ["ICLR.cc/2026/Conference/Submission15486/Reviewer_rvEY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15486/Reviewer_rvEY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811575425, "cdate": 1761811575425, "tmdate": 1762925776606, "mdate": 1762925776606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a fresh perspective on benchmark contamination. Specifically, the study analyzes model performance under varying levels of memorization and uncovers a finding: LLMs perform worse on questions they have memorized than on unseen ones, revealing the coexistence of two distinct learning processes—rote memorization and genuine capability learning. To disentangle these phenomena, the authors propose TrinEval, an innovative evaluation framework that reformulates MCQs into a trinity format, effectively reducing the impact of memorization while preserving the assessment of true knowledge understanding. Experimental results demonstrate that TrinEval successfully mitigates memorization effects and provides a more accurate evaluation of model capability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper offers a novel perspective on benchmark contamination, revealing through experiments that LLMs actually perform worse on questions they have memorized than on unseen ones.\n\n2. The authors further propose TrinEval, an innovative evaluation framework that reformulates multiple-choice questions into a knowledge-centric trinity structure, effectively mitigating the influence of memorization while preserving the evaluation of genuine knowledge understanding.\n\n3. Finally, experimental results demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. Unclear contribution. In Section 3.2, the authors use an existing method to quantify LLM memorization, which I do not consider a major contribution of the paper. The proposed metric, Min-K%, is only one of many possible ways to measure memorization—other approaches such as Min-K++%, perplexity, and others could also be employed. I suggest moving the “Quantifying LLM Memorization” section into the experimental part and comparing multiple metrics to avoid overclaiming the contribution.\n\n2. Limited scope. This work focuses exclusively on MCQ-style benchmarks. Although the authors mention this limitation in the paper, I believe that for a venue like ICLR, analyzing only MCQs is insufficient. Extending or at least discussing applicability to other evaluation formats would strengthen the paper.\n\n3. Limited evaluation setup.In Figure 2, the authors evaluate only on a single benchmark and three relatively small models. This setup limits the generalizability of the results. For example, would the conclusions still hold for larger-scale LLMs? I remain skeptical.\n\n4. Ambiguity in the definition of “memorization.” I find the paper’s definition of memorization somewhat unclear. For instance, in Figure 1, memorization seems to be determined solely by matching options. Could it also involve matching questions and answers instead? Moreover, what if we modify certain conditions or phrasing in the question—would the model still produce the same answer? This aspect deserves further clarification and empirical analysis."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vK2g4diBHz", "forum": "A5cvMnKUpJ", "replyto": "A5cvMnKUpJ", "signatures": ["ICLR.cc/2026/Conference/Submission15486/Reviewer_Nv1h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15486/Reviewer_Nv1h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898879314, "cdate": 1761898879314, "tmdate": 1762925775949, "mdate": 1762925775949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the superficial memorization problem in LLM MCQ evaluation. Specifically, this paper first analyzes model performance under different memorization conditions and observes an interesting finding: LLMs perform worse on memorized MCQs than on non-memorized ones. This may due to the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. This paper then propose TrinEval, a novel\nevaluation framework that reformulates MCQs into an alternative trinity format to reduce memorization affect. The authors conduct extensive evaluation using TrinEval and find common LLMs may memorize by rote 20.5% of knowledge points."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates the important and interesting issues of evaluation leakage and whether models truly understand knowledge, which represents a meaningful and interesting direction. Assessing whether models genuinely comprehend knowledge is important for the development of LLMs.\n2. The finding that LLMs perform worse on memorized MCQs than on non-memorized ones is interesting. It may inform further reflection on how LLMs actually understand and utilize knowledge, highlighting the need for further investigation into the underlying mechanisms.\n3. The paper also conducts extensive experiments using TrinEval, revealing several interesting insights that could inform future development of LLMs."}, "weaknesses": {"value": "1. The paper mentions a limitation of the preliminary investigation: the binary classification of MCQs as either memorized or non-memorized oversimplifies the nuances of memorization. However, it remains somewhat unclear how TrinEval effectively addresses these limitations.\n2. The presentation of the paper is a bit confusing. TrinEval is an evaluation method, but its role and contribution could be made clearer. When the paper claims that TrinEval can reduce memorization, does it mean that this evaluation better measures the model’s true knowledge understanding, or that it simply avoids the effects of memorization?\n3. The categorization of model knowledge into rote memorization and genuine capability learning seems to lack sufficient justification. Are there potentially other categories? Moreover, is rote memorization necessarily something to avoid? The ability to memorize knowledge is also a form of capability. It would strengthen the paper if the authors could further clarify the boundary between knowledge storage and knowledge utilization in LLMs."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TtRfFAnwzi", "forum": "A5cvMnKUpJ", "replyto": "A5cvMnKUpJ", "signatures": ["ICLR.cc/2026/Conference/Submission15486/Reviewer_KGaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15486/Reviewer_KGaV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924025529, "cdate": 1761924025529, "tmdate": 1762925775550, "mdate": 1762925775550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of benchmark contamination on the evaluation of LLMs in MCQ tasks. The authors propose TrinEval, an evaluation framework that can disentangle memorization and genuine understanding by reformulating MCQs into triplets of entity, attribute, and context asking the model to answer the reformulated questions. Experiments reveal that TrinEval can effectively reduce memorization and that LLMs could be rote learners (they fail to answer the question with high memorization)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper disentangles memorization and understanding by rephrasing the MCQs, and the experiments are shown to be effective.\n\nThe findings are particularly insightful, revealing that Large Language Models (LLMs) often struggle with questions they appear to have memorized, which highlights a critical limitation in those models."}, "weaknesses": {"value": "The methodology of using other language models to reformulate MCQs could introduce verbal biases inherent to those models. The high sum of output probabilities might suggest a direct, non-reflective answering process instead of the memorization, which could undermine the conclusiveness of the results as a measure of true understanding. And the method of rephrasing the questions to decouple memorizing and reasoning is widely used, which may restrict the novelty of the paper.\n\nCertain claims in the paper are not sufficiently justified. For instance, the assertion in lines 165-166 that memorized questions are \"relatively simple\" because they are absent from MMLU-Pro needs further support. It is crucial to consider whether the training data cutoff date for the evaluated models precedes the creation of the MMLU-Pro benchmark, as this could be a confounding factor.\n\nThe scope of models evaluated is limited, with most being released in 2023. This may restrict the generalizability of the findings to more recent and advanced models."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pMQXxEFMoV", "forum": "A5cvMnKUpJ", "replyto": "A5cvMnKUpJ", "signatures": ["ICLR.cc/2026/Conference/Submission15486/Reviewer_qCn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15486/Reviewer_qCn1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988926450, "cdate": 1761988926450, "tmdate": 1762925775095, "mdate": 1762925775095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}