{"id": "hYojmwkXwQ", "number": 15889, "cdate": 1758256642561, "mdate": 1759897275129, "content": {"title": "Score-Based Density Estimation from Pairwise Comparisons", "abstract": "We study density estimation from pairwise comparisons, motivated by expert knowledge elicitation and learning from human feedback. We relate the unobserved target density to a tempered winner density (marginal density of preferred choices), learning the winner's score via score-matching. This allows estimating the target by `de-tempering' the estimated winner density's score. We prove that the score vectors of the belief and the winner density are collinear, linked by a position-dependent tempering field. We give analytical formulas for this field and propose an estimator for it under the Bradley-Terry model. Using a diffusion model trained on tempered samples generated via score-scaled annealed Langevin dynamics, we can learn complex multivariate belief densities of simulated experts, from only hundreds to thousands of pairwise comparisons.", "tldr": "We show how to estimate densities solely from pairwise comparisons. We establish a relationship between the target density and tempered density of the preferred choices, and provide a score-based method that recovers the target density.", "keywords": ["score-based methods", "pairwise comparisons", "density estimation", "elicitation", "random utility models", "tempering"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a289fd557496fa8e06d3468fa5f50f40a2e6678e.pdf", "supplementary_material": "/attachment/bd0f4a3b9c8028fe22818f68ea06adefae651c06.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a method to sample from a distribution given only independent paired samples from an arbitrary distribution and a ranking of pairs of points namely $(x,x',I[U(x)>U(x')])$  where $U$ is some *random* utility function defined as $U(x)=\\log p(x)+W$ with $W$ a known noise distribution. It achieves this in two stages: firstly, it learns the score marginal distribution of the sample with highest utility ($p_w(x)$) via denoised score matching, second it learns the transformation $\\tau(x)=\\frac{\\nabla\\log p(x)}{\\nabla\\log p_{w}(x)}$ in order to obtain an estimate for the data distributions score. The paper derives a closed form for $\\tau$ as a function of the density ratio when $W$ follows a Gumbel or Exponential distribution as a function of the belief density ratio, $p(x)/p(x')$ for any $x,x'$ pair."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "*  Well written with clear introductions to each of methods used.\n\n* Tackles an interesting and challenging problem where only pairwise ranks can be used to learn a multivariate distribution.\n\n* Good mix of synthetic illustrative experiments and real world experiments outside of the simplifying assumptions of the paper.\n\n* Provides theoretical justification explanation of  existing method for density estimation with ranked comparisons giving theoretical lower bounds on accuracy of prior work."}, "weaknesses": {"value": "* Estimation of the transformation $\\tau$ is non-trivial and relies upon estimating the belief density ratio $r(x,x')=p(x)/p(x')$. Once the belief density ratio has been estimated you already have access to the unnormalised density (for arbitrary fixed $x'$ $p(x)=C\\cdot r(x,x'))$ for all $x$) and the score of the distribution ($s(x)=\\nabla\\log p(x)=\\lim_{h\\rightarrow 0} \\log r(x,x+h)/h$). As such it is not clear that the requirement to learn the MWD and then the tempering field $\\tau$ from $r$ is strictly necessary.\n\n* Estimation of $r$ and by consequence $\\tau$ is dependent upon the distribution of $W$ being known and from one of two distributional families. This should be made clear in the paper earlier stages of the paper. (It might also be good to highlight that the problem is unidentifiable if $x\\succ x'$ is taken as the deterministic density rankings as this is fundamental to the approach and without this the equation on line 79 doesn't seem to make sense as $x\\succ x'$ is deterministic.)\n\n* It is unclear how the quality of the method depends upon the accuracy of the assumption on $W$, both in terms of the parameter $s$ and the assumption of the distributional family (namely Gumbel). It would be good to have experiments exploring synthetic data in a more reasonable miss-specified setting.\n\n* The score matching using both winning and paired samples seems slightly under-explored, I appreciate the nice illustration of the improvement from including the paired samples but is there any further justification as opposed to only utilising samples from the distribution you are learning. It seems to me that for this to hold we would reasonably need the neural network to learn that the masking variable represents the marginalisation of the distribution in order for paired samples to meaningfully help learn the MWD.\n\n* (This is more a comment) Algorithms B.1 and B.2 should be in the main body if possible and clarification that the likelihood on line 342 comes from the assumption that $W\\~ \\text{Gumbel}(0,s)$.\n\n* Small mistake I believe $\\mathbb{P}(x\\succ x')=F_{W(x')-W(x)}(u(x)-u(x'))$ not $F_{W(x)-W(x')}(u(x)-u(x'))$ (although I could be mistaken.) It's also maybe worth highlighting that this probability is what you mean by the choice distribution conditional on $\\mathcal{C}$.\n\n\nHighlight early on that $x\\succ x'$ is random with some fixed noise distribution known up to a parametric family, as this is necessary for the identifiability of the problem."}, "questions": {"value": "* I am unclear of the relationship between 3.1 and the rest of the paper. I appreciate that when $\\tau$ is constant you then get the relationship you establish later on ($\\nabla \\log p=\\tau\\nabla\\log q$) however the later work does not work under the assumption that $\\tau$ is constant.\n\n* Can you use the score at the end to produce an explicit density estimate or does it exclusively give a score function and thereby a means of sampling from a distribution.\n\n* Is there any way to use the score estimate to give a new proposal distribution say $\\lambda'$ which is closer to $p$?\n\n* Were other choices for estimation of $s_\\theta$ explored such as normalising flows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gOTdnaO92e", "forum": "hYojmwkXwQ", "replyto": "hYojmwkXwQ", "signatures": ["ICLR.cc/2026/Conference/Submission15889/Reviewer_zzGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15889/Reviewer_zzGP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760624771045, "cdate": 1760624771045, "tmdate": 1762926105656, "mdate": 1762926105656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We are pleased the reviewers both understood the paper well and had a positive overall impression. We thank them for their time, effort, and constructive comments, which helped improve both the clarity and soundness of the paper.\n\nThe promised experiments are completed, and their results are summarized in the rebuttal. We are now working on integrating the revisions in the full paper, which we will reupload in a few days."}}, "id": "IrpHew8VPX", "forum": "hYojmwkXwQ", "replyto": "hYojmwkXwQ", "signatures": ["ICLR.cc/2026/Conference/Submission15889/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15889/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15889/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724409273, "cdate": 1763724409273, "tmdate": 1763724409273, "mdate": 1763724409273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies density estimation from pairwise comparisons: For example, an expert is shown two i.i.d. draws $\\mathbf{x}, \\mathbf{x}'$ from a sampling distribution $\\lambda(\\mathbf{x})$, and the object they prefer is recorded. It is assumed that $\\mathbf{x}$ is preferred over $\\mathbf{x}'$ with probability $\\mathbb{P}(\\mathbf{x} \\succ \\mathbf{x}')$, which is modeled as a random utility model (RUM) with deterministic utility $u(\\mathbf{x}) = \\log p(\\mathbf{x})$. The goal of this work is to infer $p$ from a set of $(\\mathbf{x}, \\mathbf{x}', \\mathbf{x} \\succ \\mathbf{x}')$ triples. Firstly, for two special cases of RUM (Bradley-Terry & exponential noise RUM), the authors (1) provide theoretical results that relate the marginal winner density (MWD) $p_w(\\mathbf{x}) \\propto \\int \\mathbb{P}(\\mathbf{x} \\succ \\mathbf{x}') \\lambda(\\mathbf{x}) \\lambda(\\mathbf{x}') \\, \\mathrm{d}\\mathbf{x}'$ to $p(\\mathbf{x})$ via a position-dependent *tempering field* $\\tau(\\mathbf{x})$ by showing that their scores are collinear, i.e., $\\nabla \\log p(\\mathbf{x}) = \\tau(\\mathbf{x}) \\nabla \\log p_w(\\mathbf{x})$, and (2) provide explicit formulas for $\\tau(\\mathbf{x})$. \nSecondly, the paper builds on these insights by proposing a practical algorithm: The authors train (1) a diffusion model to estimate $\\nabla \\log p_w$ (MWD model), and (2) a second neural network to estimate the density ratio $r_\\theta(\\mathbf{x}, \\mathbf{x}') \\approx p(\\mathbf{x}')/p(\\mathbf{x})$, which is then used to estimate $\\tau(\\mathbf{x})$ (with the MWD model as a proposal distribution).\nLeveraging the relation to the score of $p$ allows the authors to then draw approximate samples from $p$ using score-scaled annealed Langevin dynamics. \nEmpirical findings show that this approach outperforms a previous flow-based method qualitatively and quantitatively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ **Motivation.** The work is well-motivated and tackles an important issue in preference learning. The studied RUM models are relevant in many disciplines.\n+ **Theoretical Results.** The theoretical results are sound, well-justified, novel, and important for this line of research. Moreover, they give rise to novel practical algorithms for estimating $p$ using score-based approaches.\n+ **Presentation.** The main results are presented in a clear way. It was easy to follow the author's main arguments."}, "weaknesses": {"value": "+ **Estimating $\\tau(\\mathbf{x})$** \n\t+ Given a parametric model $r_{\\theta}(\\mathbf{x}, \\mathbf{x}') \\approx p(\\mathbf{x}')/p(\\mathbf{x})$, estimating $\\tau(\\mathbf{x})$ still involves approximating two (possibly high-dimensional) integrals (Eq. 7) for each $\\mathbf{x}$, which is computationally very expensive.  \n\t+ While estimating these integrals with importance-sampled Monte-Carlo (MC) yields unbiased estimates for each of the integrals, the resulting MC estimate for $\\tau(\\mathbf{x})$ is *not* unbiased, since it involves a fraction of the two integrals. The paper would benefit from a more thorough discussion of this estimation procedure. Moreover, claiming that \"the integrals are computed using importance sampling [...]\" (L343-L344) is misleading (\"computed\" should be replaced).\n\t\n+ **Computational cost for sampling from the estimated $p$**\n\t+ In score-scaled annealed Langevin dynamics (ALD), using the scaled score $\\tau(\\mathbf{x}) \\nabla \\log p_w(\\mathbf{x})$ requires estimating $\\tau(\\mathbf{x})$ in each step of ALD. In Appendix C.6, it is stated that for a $d$-dimensional target, $2000d$ importance samples are used to estimate the integrals in Eq. 7. Hence, a single ALD step is at least $2000d$ times more expensive than regular ALD sampling (where the score is obtained by a single forward pass of the network)---while not counting the computational complexity of calling $r_\\theta$ and estimating $\\log p_w(\\mathbf{x})$ with the probability-flow ODE.\n\t+ It is clear that this does not scale well to large $d$, or large networks (neither to large diffusion model, nor large $r_\\theta$).\n\t\n+ **Score collinearity when $\\sigma > 0$**\n\t+ The identity $\\nabla \\log p(\\mathbf{x}) = \\tau(\\mathbf{x}) \\nabla \\log p_w(\\mathbf{x})$ only holds in the *noise-free* case, i.e., $\\sigma = 0$. When replacing $p$, $p_w$ with their Gaussian-smoothed version $p*\\mathcal N(0,\\sigma^2I)$ and $p_w*\\mathcal N(0,\\sigma^2I)$ for some $\\sigma > 0$, this is *not* true anymore. I think this should be stressed more explicitly in the main text.\n\t+ L353-L354 claims that the \"tempering field relation (Eq. 6) is guaranteed to hold in the small-noise limit [...]\". This is misleading, as Eq. 6 only holds when $\\sigma = 0$ exactly, and is only approximate when $\\sigma \\approx 0$.\n\t+ L354: \"Possible mismatch at higher noise scales is not an issue [...]\". This is only true in the ideal case of $L \\to \\infty$. In practical settings, where $L$ is moderate, the scores at higher noise levels will have a large effect on ALD.\n\t\n+ **Hyperparameters** \n\t+ Many of the empirical results seem sensitive to the particular choice of both training and sampling hyperparameters. In Appendix C.7, it seems that $\\epsilon_{\\text{base}}$  is chosen very differently for 2D experiments ($\\epsilon_{\\text{base}} = 7.0$) than in the other experiments ($\\epsilon_{\\text{base}} = 0.15$). This could hint at the fact that the magnitude of $\\tau(\\mathbf{x})$ may be miscalibrated.\n\t\n+ L109: \"The network is trained to predict the score of this kernel, which is typically tractable.\"\n\t+ This is not true: When minimizing the objective in Eq. (1), the network will learn $s_{\\theta}(\\mathbf{x}, \\sigma) \\approx \\nabla_{\\mathbf{x}} \\log p_\\sigma(\\mathbf{x})$, which is the score of $p_\\sigma$, and not of the perturbation kernel $p_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})$.\n\t\n+ Very minor suggestions regarding notation: \n\t+ L163: I suppose $C$ should be a set, so this should read $C = \\\\{\\mathbf{x}, \\mathbf{x}' \\\\}$ instead. \n\t+ L374-375 reads \"2$d$ and 1$d$\" marginals, but $d$ refers to the *particular* dimension of the target (L367), so it should rather read e.g. \"2D and 1D\" (where \"D\" is used as a shorthand for \"dimensional\").\n\t\n+ Typos:\n\t+ L1022, L1052: \"probability ODE\", should probably read \"probability-flow ODE\" \n\t+ L1049: The equation for the Skilling-Hutchinson trace estimator should read $=$, not $\\approx$. The expectation is the exact trace, estimating the expectation with Monte Carlo makes it approximate."}, "questions": {"value": "+ L373 claims that a \"linear MLP score network\" is used. What exactly is meant with \"linear\" here? The used MLP is surely a non-linear function of inputs/parameters.\n+ Have the authors considered training a model to output $\\tau(\\mathbf{x})$ directly, e.g. by regressing many (expensive) Monte Carlo estimates? This would amortize the computational complexity during sampling.\n+ In low dimensions (e.g., $d = 2$), numerical integration algorithms (e.g. quadrature) have much better convergence rates than Monte Carlo. Have the authors tried such methods to estimate $\\tau(\\mathbf{x})$ (instead of importance-sampled Monte Carlo)?\n+ When training the diffusion model on the full joint (Figure C.1 (b)), is the distribution of the model when `joint = False` actually close to the *true* marginal of the joint model? An experiment would be interesting where (1) you sample $(\\mathbf{x}, \\mathbf{x}')$ from the model with `joint = True`and discard $\\mathbf{x}'$ (which is an *exact* sample from the winner marginal), and (2) you sample $\\mathbf{x}$ with `joint = False`, and compare the distributions (e.g. Wasserstein).\n+ Why is it that the diffusion model trained on the full joint (both winners and losers) learns \"better\" winner marginals than the model only trained on winner? I find this surprising."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tl2g2qWX5E", "forum": "hYojmwkXwQ", "replyto": "hYojmwkXwQ", "signatures": ["ICLR.cc/2026/Conference/Submission15889/Reviewer_aZ9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15889/Reviewer_aZ9i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763316985, "cdate": 1761763316985, "tmdate": 1762926105156, "mdate": 1762926105156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies density estimation from pairwise comparisons, linking the unobserved target density to a tempered winner density. By learning the winner’s score through score matching and applying a de-tempering step, the target density can be recovered. The authors prove a collinearity relation between the belief and winner scores, derive a tempering field, and propose an estimator under the Bradley–Terry model. A diffusion-based method using annealed Langevin dynamics demonstrates effective learning of complex belief densities from limited comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Estimating the density function from pairwise comparison is novel.\n2. The motivation and structure of the work are clear.\n3. The theoretical results can show the benefits of the proposed method."}, "weaknesses": {"value": "1. The notation is a little messy. Is there a notation list table to explain the meaning of these items?\n2. Maybe it is better to provide an analysis for the efficiency of the proposed method. There should be some experiment results or some discussions on it.\n3. I recommend to provide a more clear structure of the algorithm in Section 4, which should be the core contribution of your work."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oDfVgPVii9", "forum": "hYojmwkXwQ", "replyto": "hYojmwkXwQ", "signatures": ["ICLR.cc/2026/Conference/Submission15889/Reviewer_qpz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15889/Reviewer_qpz9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946963574, "cdate": 1761946963574, "tmdate": 1762926104706, "mdate": 1762926104706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a density estimation method for recovering the target density from pairwise comparison data. The key contribution is showing that the score of the target density is colinear with the score of the winner’s density, allowing it to be estimated via score matching. The proposed method is evaluated on synthetic data generated from a Random Utility Model (RUM) as well as data derived from large language model (LLM) experts, demonstrating promising performance compared with Mikkola et al. (2024)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem of estimating the density function from expert knowledge is novel in generative model context and underexplored. \n\nThe score factorization in (6) is a natural extension of Mikkola et al. (2024)'s proposal where tau(x) here is a constant. The connection with score matching (Fisher divergence) and generative model is very natural as well. \n\nThe performance reported in Section 5 is a clear improvement over the normalising flow model used in Mikkola et al. (2024). \n\nThe use of LLM to generate expert belief is an efficient way of producing cheap expert comparisons and is a motivating problem setting for the proposed method. \n\nThis paper is reasonably well-written (although I think the problem should be better motivated, see below)."}, "weaknesses": {"value": "The main weakness is the confusion of the setting considered in this paper. In what application, it is useful to estimate the experts beliefe density? For example, if I have experts opinions {x > x'} where x is a hand-written image and experts are asked copmare images based on how much they look like a digit one. In what application, would I want to estimate p(x)? I guess the user would probably be interested in knowing whether the experts consider a specific image x being 1 or not but I cannot see why it is useful to sample from p(x)? \n\nI could not find much motivation on that in the introduction section, and the experiment section is largely based on toy datasets. \n    - In the housing data example, it is unclear what is the \"belief about the distribution of features in the California housing dataset\". Does it mean, the expert belief on whether the data point is in the dataset or not? \n\nThat said, I can see that mathematically, this is an interesting object, as the method seems to estimate the utility function u(x) in RUM model (line 169), which is often unobserved. \n\nIn the experiments, the comparisons are made against Mikkola et al. (2024) using a flow model. However, as a benchmark, the authors should also include a diffusion model with \\tau(x) treated as a constant, in order to compare the performance gained by incorporating the estimated \\tau(x)."}, "questions": {"value": "What is the definition of the expert's belief density? why is it useful to generate sample from it?\n\n4.1, I feel the main estimation objective (i.e., the denoising score matching) should be displayed here, showing how the data distribution is constructed from winners and losers and how exactly they are used in the score matching objective.  The current explanations from line 324 - 328 and B1 is not clear enough. For example, what does \"train sθ(x,x′,σ,joint) using score-matching\" mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EtxWHpKcjl", "forum": "hYojmwkXwQ", "replyto": "hYojmwkXwQ", "signatures": ["ICLR.cc/2026/Conference/Submission15889/Reviewer_NgZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15889/Reviewer_NgZe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985196828, "cdate": 1761985196828, "tmdate": 1762926104092, "mdate": 1762926104092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}