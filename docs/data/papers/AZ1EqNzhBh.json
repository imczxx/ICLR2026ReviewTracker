{"id": "AZ1EqNzhBh", "number": 2612, "cdate": 1757163901398, "mdate": 1759898137657, "content": {"title": "AEON: Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise for Robust Learning", "abstract": "Robust training with noisy labels is a critical challenge in classification because it offers the potential to reduce reliance on costly clean-label datasets. Real-world datasets often contain a mix of in-distribution (ID) and out-of-distribution (OOD) instance-dependent label noise, which is a challenge that is rarely addressed simultaneously by existing methods and is further compounded by the lack of comprehensive benchmarking datasets. Furthermore, many noisy-label learning methods rely on inefficient multistage learning algorithms that explicitly identify noisy samples, which are treated differently from those presumed to be clean. However, this identification typically depends on either oversimplified two-component clustering assumptions (clean vs. noisy) or the arbitrary specification of a noise rate.\nTo address these research gaps, we propose the Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise (AEON) method. The AEON is an efficient one-stage noisy-label learning methodology that adaptively estimates instance-dependent (ID and OOD) label noise rates to enhance robustness in complex noise settings. In addition, we introduced a new benchmark that reflects real-world ID and OOD noise scenarios. Experiments demonstrate that AEON achieves state-of-the-art performance on both synthetic and real-world datasets.", "tldr": "Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise for Robust Learning", "keywords": ["Label noise", "Noise Classification", "Open-Set", "Close-Set"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39c1f3c41af80ff0a4cc812fd4473314c1ff8937.pdf", "supplementary_material": "/attachment/09fd0014baf34e67ab9995911f8928f009de24a7.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates a practical setting of learning with label noise in which instance-dependent noise and out-of-distribution (OOD) noise coexist. To address these challenges, the authors introduce a one-stage method for robust training under label noise. They also propose a new benchmark, ID2Noise, designed to more accurately reflect real-world noise conditions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. It tackles a more challenging and realistic label-noise setting that combines instance-dependent (ID) noise and out-of-distribution (OOD) noise.\n3. It introduces a benchmark dataset covering both ID and OOD label noise, enabling systematic evaluation under practical conditions."}, "weaknesses": {"value": "1. The method is framed as addressing instance-dependent label noise (Definition 1), but the noise rate tied to Eq. (1) aggregates over all training losses; it is dataset-dependent rather than instance-dependent. As written, the approach appears instance-independent instead of instance-dependent (ID).\n\n2. Although the paper aims to handle ID and OOD noise simultaneously, it lacks a comprehensive analysis of how these two noise types jointly affect training. Without such analysis, the contribution reads as a combination of two noise settings rather than a principled treatment of their interaction.\n\n3. Several reported baselines are notably below the original papers. For example, ELR+ is reported at 71.5% on Clothing1M, while the original work reports 74.81%. Given ELR+ is open-sourced and widely validated, these discrepancies need careful explanation.\n\n4. The paper asserts that both ID and OOD noise occur in real-world datasets, yet the evaluation relies on synthetic construction. An empirical study would strengthen this claim—for instance, identifying and presenting concrete OOD examples in datasets like Clothing1M or WebVision."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ncl56O1d7g", "forum": "AZ1EqNzhBh", "replyto": "AZ1EqNzhBh", "signatures": ["ICLR.cc/2026/Conference/Submission2612/Reviewer_K8Qm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2612/Reviewer_K8Qm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761050197600, "cdate": 1761050197600, "tmdate": 1762916304194, "mdate": 1762916304194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address two key limitations in the LNL field:\n\n(1) the lack of unified approaches that handle both ID and OOD label noise, and\n\n(2) the dependence of existing methods on noise-specific or manually tuned hyper-parameters.\n\nThe main idea of the paper is to design a learnable loss function that adaptively estimates the noise ratio in both ID and OOD samples through trainable parameters.\nThrough extensive experiments, the authors demonstrate that their proposed method achieves SOTA performance and that the estimated noise ratios closely approximate the true noise rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose an intuitive yet effective idea and support it with comprehensive experimental evidence.\nThe adaptive weights derived from the estimated noise ratios are well-designed and theoretically sound.\nFurthermore, the ablation studies in the appendix provide sufficient experimental validation for the robustness and rationale behind the weighting mechanism."}, "weaknesses": {"value": "1. Lack of theoretical justificaiton\n\nIn my opinion, the proposed loss function is suited for a well trained model.\nHowever, it remains unclear whether this loss can reliably approximate the true noise ratios during the training process.\nSpecifically, I believe that when the noise rate exceeds a certain threshold, the proposed function may no longer provide a valid estimation, potentially leading to an ill-posed problem.\nFor instance, in a dataset with two binary attributes, whether the noise exceeds 50% can drastically affect which attribute the model learns to classify.\n\n2. Numerous hyperparmeters\n\nThe authors should provide a more detailed sensitivity analysis for the various hyperparameters.\nAlthough they argue that previous OOD aware LNL methods are difficult to train due to parameter dependencies, their own approach also introduces multiple tunable parameters related to noise rate estimation.\nIt would strengthen the paper if the authors could show, either experimentally or theoretically, that these parameters are relatively insensitive to the true noise rate or can adapt automatically.\n\n3. Dependence on contrastive loss\n\nIn addition to the above, the method heavily relies on having a strong vision encoder.\nThe large performance gap between the presence and absence of the contrastive loss in the ablation study supports this point.\nHowever, the authors provide little theoretical or analytical discussion about the role of contrastive loss in their framework.\nSince the method cannot achieve SOTA performance without it (and it appears to be deeply intertwined with the proposed learning scheme) the paper would benefit from additional experiments or theoretical analysis clarifying the relationship between the contrastive loss and the main objective."}, "questions": {"value": "I am impressed that the authors’ approach achieves the best performance on real-world datasets among recent LNL methods. If the authors can address some of my concerns listed under Weaknesses, I would be happy to evaluate this work as a sufficiently strong contribution to the community.\n\nMinor question (you may not answer it):\nIn web search collected datasets (where I believe the proposed method could be particularly effective) the nature of OOD may differ from what the authors assume. Instead of encouraging the model to consider “easy” features (e.g., resolution, grayscale) as OOD, I would like the model to focus on semantic characteristics. Which hyperparameters should be adjusted, and how, to bias the method toward semantic OOD rather than low-level cues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3AsF1NkrH", "forum": "AZ1EqNzhBh", "replyto": "AZ1EqNzhBh", "signatures": ["ICLR.cc/2026/Conference/Submission2612/Reviewer_q2xm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2612/Reviewer_q2xm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839004647, "cdate": 1761839004647, "tmdate": 1762916303998, "mdate": 1762916303998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AEON, a one-stage framework for learning with noisy labels that jointly models in-distribution and out-of-distribution instance-dependent noise. The method adaptively estimates noise rates without explicit sample filtering and demonstrates state-of-the-art performance on both synthetic and real-world benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental validation is thorough, covering multiple variables and validating the method's effectiveness against several baselines.\n2. The paper's focus on a highly realistic yet rarely addressed challenge—the concurrent appearance of instance-dependent in-distribution (ID) and out-of-distribution (OOD) label noise—is particularly relevant;\n3. Paper is generally well writen and easy to follow."}, "weaknesses": {"value": "1. First, I’m a bit unclear about the distinction between the noisy-label learning with a mix of in-distribution (ID) and out-of-distribution (OOD) instance-dependent label noise discussed in this paper and what you refer to as Joint ID and OOD Noise Handling. Is the only difference the consideration of instance-dependent label noise? Furthermore, how does the proposed method specifically address instance-dependent label noise?  \n2. Overall, the proposed method does not present substantial methodological innovation. Most of its design still relies on distinguishing clean and noisy samples based on differences in their loss distributions. Although I understand that the new approach achieves a one-stage framework, it’s not clear why this is advantageous—what makes a one-stage approach better than a two-stage one? From my perspective, the proposed method seems to share similar motivations and basic intuitions with previous approaches, and this distinction is not clearly articulated.  \n3. In addition, the paper’s use of contrastive loss, in-distribution loss, and out-of-distribution loss appears to be straightforward adaptations of existing methods, lacking a principled or theoretically grounded design motivation.\n4. The justification for why minimizing the overall objective in Eq (8) leads to an optimal or correct estimation of the noise parameters ($\\gamma^{id}$ and $\\gamma^{ood}$) is not sufficiently discussed. The paper would be strengthened by a more formal analysis or discussion on the convergence guarantees for these parameters;\n5. The use of convex combinations, such as the weighting by $w_i^{ood}$ in Eq (8) and $\\alpha_{cont}$ in Eq (12), lacks theoretical or analytical justification beyond empirical results. A deeper analysis is needed to explain why these specific formulations are a principled approach to balancing the respective loss components;\n6. The method introduces several key hyperparameters (e.g., $\\lambda$, $m^{id}$, $m^{ood}$) that seem non-trivial to tune. For instance, the margins $m^{id}$ and $m^{ood}$ are based on energy scores, which can vary significantly across different datasets, potentially hindering the method's practical applicability and generalization;\n7. The contrastive loss $\\mathcal{L}_i^{cont}$ in Eq (12) does not appear to differentiate between ID and OOD samples. This implies that OOD instances are also utilized in the supervised component, which seems counter-intuitive."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x2ZNJxysUd", "forum": "AZ1EqNzhBh", "replyto": "AZ1EqNzhBh", "signatures": ["ICLR.cc/2026/Conference/Submission2612/Reviewer_WSLq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2612/Reviewer_WSLq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917867313, "cdate": 1761917867313, "tmdate": 1762916303489, "mdate": 1762916303489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of classification with instance-dependent in-distribution (ID) and out-of-distribution (OOD) label noise. The proposed one-stage method AEON adaptively estimates ID and OOD label noise rates, avoiding the limitations of relying on arbitrary noise rate. The framework incorporates a dual-stream soft-masking mechanism to dynamically evaluate sample reliability and a unified multi-objective training strategy combining supervised, unsupervised, and contrastive learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Designs a soft-masking mechanism, converts noise rates into gradient-optimizable parameters. This mechanism addresses the non-differentiability of hard-thresholding selection, and enables dynamic weight adjustment and collaborative optimization with the model."}, "weaknesses": {"value": "1. The assumption that losses follow a normal distribution is too strong. Numerous studies have shown that losses in noisy datasets typically exhibit a similar bimodal distribution (relatively distinct separation between clean and noisy samples) instead of a normal distribution.\n2. The justification only uses loss as the criterion. Other commonly used criteria in noisy label learning, such as Jensen-Shannon (JS) divergence can be explored.\n3. Theoretical analysis is lacking. No proofs for noise rate estimation error bounds, model convergence, or other key theoretical guarantees are provided.\n4. Comparative experiments fail to include SOTA noisy label learning and OOD detection methods from the last three years."}, "questions": {"value": "1. The definitions of $\\eta^{id}$ (Definition 1) and $\\eta^{ood}$ (Definition 2) are not rigorous. Label mismatch between true and noisy labels includes both ID and OOD noise, leading to an inclusion relationship rather than mutual exclusivity.\n2. Manuscript proofreading needs improvement.  In line 341, \"ciFAIR-100\" should be \"CIFAR-100\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3X9TCtVymD", "forum": "AZ1EqNzhBh", "replyto": "AZ1EqNzhBh", "signatures": ["ICLR.cc/2026/Conference/Submission2612/Reviewer_fyGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2612/Reviewer_fyGH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988484922, "cdate": 1761988484922, "tmdate": 1762916303370, "mdate": 1762916303370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}