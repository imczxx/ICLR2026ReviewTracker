{"id": "cM3gsqEI4K", "number": 3608, "cdate": 1757486203633, "mdate": 1763436968041, "content": {"title": "When LLMs get significantly worse: A statistical approach to detect model degradations", "abstract": "Minimizing the inference cost and latency of foundation models has become a\ncrucial area of research. Optimization approaches include theoretically lossless\nmethods and such without accuracy guarantees like quantization. In all of these\ncases it is crucial to ensure that the model quality did not degrade. However, even\nat temperature zero, model generations are not necessarily robust even to theo-\nretically lossless model optimizations due to numerical errors. We thus require\nstatistical tools to decide whether a finite-sample accuracy deviation is an evi-\ndence of a model’s degradation or whether it can be attributed to (harmless) noise\nin the evaluation. We propose a statistically sound hypothesis testing framework\nbased on McNemar’s test allowing to efficiently detect model degradations, whilst\nguaranteeing a controlled rate of false positives. The crucial insight is that we have\nto confront the model scores on each sample, rather than aggregated on the task\nlevel. Furthermore, we propose three approaches to aggregate accuracy estimates\nacross multiple benchmarks into a single decision. We provide an implementation\non top of the largely adopted open-source LM Evaluation Harness and provide\na case study illustrating that the method correctly flags degraded models, whilst\nnot flagging model optimizations that are provably lossless. We find that with our\ntests even empirical accuracy degradations of 0.3% can be confidently attributed\nto actual degradations rather than noise.", "tldr": "", "keywords": ["LLM", "Benchmarking", "Statistics", "Accuracy"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48656f16613e872228fc76e4fc00cb63e7b95a4d.pdf", "supplementary_material": "/attachment/cbd64ad14ead61be1d92bb29a8eb4a5402c54c19.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates an important problem of testing whether Large Language Models (LLMs) degrade, e.g., after quantization or different serving environments. It proposes to leverage the principled McNemar's test to detect statistically significant performance degradation. On evaluation of open-source datasets, the proposed method is shown to be highly sensitive in detecting model degradation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Targets an important and practical problem of testing LLM degradation.\n- Theoretically grounded approach using McNemar's test for degradation detection.\n- Interesting empirical observations about LLM degradation with the proposed method."}, "weaknesses": {"value": "- Evaluations can be expanded to more models other than LLaMA.\n- Practical implications of the findings regarding regression testing could be further elaborated."}, "questions": {"value": "This is an interesting paper targeting an important problem of testing LLM degradation. The theoretical analysis with McNemar's test is highly encouraging, as it provides a principled way to detect statistically significant degradation. This is a new and useful contribution to the field. However, I am not an expert in this field, especially regarding the theoretical aspects. I would be interested in the following questions:\n\n- The evaluations are primarily on LLaMA models. How well does the proposed method generalize to other open-source LLMs?\n- Will this method be applicable to small models as well, or is it specifically designed for large models?\n- The \"faster regression testing\" part seems interesting. Could you elaborate more on the actionable advice on how to effectively use this method for degradation testing in practice?\n- If we have access to model internals, can we use this method to pinpoint the specific components causing degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "LhCYVYGL45", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359196392, "cdate": 1761359196392, "tmdate": 1762916865750, "mdate": 1762916865750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an important problem of testing whether Large Language Models (LLMs) degrade, e.g., after quantization or different serving environments. It proposes to leverage the principled McNemar's test to detect statistically significant performance degradation. On evaluation of open-source datasets, the proposed method is shown to be highly sensitive in detecting model degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretically grounded approach using McNemar's test for degradation detection.\n- The presentation is clear"}, "weaknesses": {"value": "- Limited importance of the targeted problem\n- The observations have limited practical implications\n- Evaluations can be expanded to more models other than LLaMA.\n- Practical implications of the findings regarding regression testing could be further elaborated."}, "questions": {"value": "This paper targets the problem of testing LLM degradation. Although the use of McNemar's test for the theoretical analysis part is encouraging, this is an application work that applies an existing theory to a new problem. I would be interested in the following questions:\n\n- Could you elaborate on the practical importance of testing LLM degradation?\n- Why is the technical contribution of applying an existing method to a new problem significant?\n- The evaluations are primarily on LLaMA models. How well does the proposed method generalize to other open-source LLMs?\n- Will this method be applicable to small models as well, or is it specifically designed for large models?\n- The \"faster regression testing\" part seems interesting. Could you elaborate more on the actionable advice on how to effectively use this method for degradation testing in practice?\n- If we have access to model internals, can we use this method to pinpoint the specific components causing degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "LhCYVYGL45", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359196392, "cdate": 1761359196392, "tmdate": 1763040655476, "mdate": 1763040655476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an important problem of testing whether Large Language Models (LLMs) degrade, e.g., after quantization or different serving environments. It proposes to leverage the principled McNemar's test to detect statistically significant performance degradation. On evaluation of open-source datasets, the proposed method is shown to be highly sensitive in detecting model degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretically grounded approach using McNemar's test for degradation detection.\n- The presentation is clear"}, "weaknesses": {"value": "- Limited importance of the targeted problem\n- The observations have limited practical implications\n- Evaluations can be expanded to more models other than LLaMA.\n- Practical implications of the findings regarding regression testing could be further elaborated."}, "questions": {"value": "This paper targets the problem of testing LLM degradation. Although the use of McNemar's test for the theoretical analysis part is encouraging, this is an application work that applies an existing theory to a new problem. I would be interested in the following questions:\n\n- Could you elaborate on the practical importance of testing LLM degradation?\n- Why is the technical contribution of applying an existing method to a new problem significant?\n- The evaluations are primarily on LLaMA models. How well does the proposed method generalize to other open-source LLMs?\n- Will this method be applicable to small models as well, or is it specifically designed for large models?\n- The \"faster regression testing\" part seems interesting. Could you elaborate more on the actionable advice on how to effectively use this method for degradation testing in practice?\n- If we have access to model internals, can we use this method to pinpoint the specific components causing degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "LhCYVYGL45", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_Fq3r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359196392, "cdate": 1761359196392, "tmdate": 1763641792913, "mdate": 1763641792913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a statistical framework for detecting significant performance degradations in Large Language Models (LLMs) after optimization techniques such as quantization. The paper adapts a one-sided exact McNemar test and offers three ways to aggregate over multiple benchmarks (pooled, max-drop with Monte-Carlo, Fisher) to statistically determine whether observed accuracy drops are due to true degradation or random noise. The result shows that even small drops (≈0.3%) can be detected as statistically significant and evaluates this on Llama-3.x variants run via lm-eval and vLLM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important gap in LLM evaluation, quantitatively distinguishing real degradations from noise, which is critical for optimization research. The framework is theoretically justified using McNemar’s test, with mathematical backing and variance derivations. The integration into the popular LM Evaluation Harness makes it useful for both academic and industrial research. The experiments cover different Llama models and optimization types (INT4, FP8, KV-FP8). The paper also gives a useful rule-of-thumb for compressing evaluation sets by removing examples that never flip in pilot runs, reducing evaluation cost while preserving signal."}, "weaknesses": {"value": "•\tThe technical tool (McNemar 1947) and Fisher’s method are classical. The paper’s contribution is primarily in adapting these to LLM regression testing, making them one-sided, and packaging aggregation strategies. This is still valuable, but conceptual novelty is limited.\n\n•\tMain empirical claims revolve around Llama-3.x variants and specific serving stacks (vLLM; KV-FP8 issues noted). It is unclear how robust the conclusions are across other model families, in-house evals, or non-binary metrics. Without replication on other models/toolchains, significance to the broader community is limited.\n\n•\tWhile the paper sketches a generalization for non-binary scores, the actual implementation and experiments remain binary. Given how common non-binary metrics are for LLMs, a small worked example (pairwise wins/ties/losses with thresholds) would increase practical utility."}, "questions": {"value": "1.\tThe paper needs to replicate the main findings on at least one other family (e.g., Qwen, Mixtral, DeepSeek) and another serving stack, demonstrating that 0.3–1.0% effects are robust.\n2.\tBeyond adopting the exact one-sided McNemar/binomial test, what is the new statistical contribution? Could the paper include a non-inferiority (TOST) module or sequential design that adapts sample size for power? How robust is the one-sided McNemar test when model scores are not perfectly binary (e.g., tasks with multiple correct answers or graded scoring)?\n3.\tThe paper may run a prospective study showing how flip-focused trimming affects type-I/II error and bias across content strata, and propose guardrails. This directly addresses the validity of Recommendation 1.\n4.\tFor non-binary metrics, please add a summarization or QA experiment (e.g., ROUGE or F1) and explicitly show the reduction to pairwise wins/ties/losses (or thresholds). The paper may also report one-sided p-values, effect sizes, and calibration to validate the procedure and demonstrate its generalizability and broader applicability.\n5.\tCould the paper specify and discuss the exact decision rule for flagging degradation? Does the framework flag when any one-sided aggregated test rejects at α (e.g., 0.05), when (\\hat{\\delta}>2%) or (\\hat{p}*{\\updownarrow}\\ge 5%) (per Dutta et al., 2024), or only when both statistical and practical criteria are met? In the 70B KV-FP8 and 70B w8a8 cases where (\\hat{p}*{\\updownarrow}<5%), the framework still detects a performance degradation. \n6.\tThe manuscript contains several language issues that impede readability (e.g., apostrophe misuse “its/it’s,” misspellings such as “assess”/“further,” incorrect conjunction “correct or false,” article usage “a similar situation,” verb form “build,” spacing/typography around symbols like “X, Y,” a duplicated footnote/URL, and inconsistent dialect choices like “while/whilst”). Could the paper (a) perform a thorough language edit to correct these issues, (b) standardize on a single dialect, and (c) resubmit with tracked edits or a brief editorial checklist summarizing the changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hHuQpGNjRD", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_9xbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_9xbY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952023667, "cdate": 1761952023667, "tmdate": 1762916865462, "mdate": 1762916865462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a statistical framework to decide whether an optimized LLM has truly degraded relative too a baseline, rather than exhibiting harmless eval noise. The core is an exact one-sided McNemar test over per-item success/failure pairs fro the baseline vs optimized. This controls type-1 error and increase power by focusing only on disagreements (flips) between models. They derive theoretical analysis of test power, propose three aggregation methods for multi-task eval (pooled test, max drop test, Fisher's method), and provide implementation for the LM Evaluation Harness. Experiments on Llama-3.1 8B and Llama-3.3 70B with various quantization schemes demonstrate method can detect accuracy degradations as small as 0.3% while correctly not flagging theoretically lossless optimizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- well motivated as it addresses a real and understudied problem of rigorous statistical testing for LLM optimization \n- first to apply McNemar's test to LLM degradation detection\n- I like the practical contribution for LM eval harness which enables immediate adoption\n- very interesting that even a 0.3% degradation can be detected with statistical confidence"}, "weaknesses": {"value": "- limited novelty: this is more about an application and implementation of statistical tests than methodological innovation\n- I think some of the assumptions may not hold for many of the benchmarks today. they make an i.i.d assumption but real benchmarks (like MMLU) often stratify by difficulty, topic, or other factors which may violate independence. Additionally, the binary metric assumption is limiting as plenty of important metrics are continuous. \n- I believe there is a limited scope of evaluation. the authors only evaluate on llama models (3.1, 3.3). no other model families are tested. they also only test quantization optimizations, but pruning, distillation, etc is missing."}, "questions": {"value": "-  Given the three aggregation methods with different trade-offs, how should a practitioner choose?\n- other than McNemar's test to answer if optimized model is statistically significantly worse than baseline, could you also use bootstrap confidence intervals for accuracy differences? or maybe permutation tests or bayesian model comparison (Bayes factors)? How do these compare in terms of power?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LjVZH4PUIX", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_NK8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_NK8t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998098296, "cdate": 1761998098296, "tmdate": 1762916864064, "mdate": 1762916864064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a statistical framework to decide whether an optimized LLM has truly degraded relative too a baseline, rather than exhibiting harmless eval noise. The core is an exact one-sided McNemar test over per-item success/failure pairs fro the baseline vs optimized. This controls type-1 error and increase power by focusing only on disagreements (flips) between models. They derive theoretical analysis of test power, propose three aggregation methods for multi-task eval (pooled test, max drop test, Fisher's method), and provide implementation for the LM Evaluation Harness. Experiments on Llama-3.1 8B and Llama-3.3 70B with various quantization schemes demonstrate method can detect accuracy degradations as small as 0.3% while correctly not flagging theoretically lossless optimizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- well motivated as it addresses a real and understudied problem of rigorous statistical testing for LLM optimization \n- first to apply McNemar's test to LLM degradation detection\n- I like the practical contribution for LM eval harness which enables immediate adoption\n- very interesting that even a 0.3% degradation can be detected with statistical confidence"}, "weaknesses": {"value": "- limited novelty: this is more about an application and implementation of statistical tests than methodological innovation\n- I think some of the assumptions may not hold for many of the benchmarks today. they make an i.i.d assumption but real benchmarks (like MMLU) often stratify by difficulty, topic, or other factors which may violate independence. Additionally, the binary metric assumption is limiting as plenty of important metrics are continuous. \n- I believe there is a limited scope of evaluation. the authors only evaluate on llama models (3.1, 3.3). no other model families are tested. they also only test quantization optimizations, but pruning, distillation, etc is missing."}, "questions": {"value": "-  Given the three aggregation methods with different trade-offs, how should a practitioner choose?\n- other than McNemar's test to answer if optimized model is statistically significantly worse than baseline, could you also use bootstrap confidence intervals for accuracy differences? or maybe permutation tests or bayesian model comparison (Bayes factors)? How do these compare in terms of power?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LjVZH4PUIX", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_NK8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_NK8t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998098296, "cdate": 1761998098296, "tmdate": 1763506626834, "mdate": 1763506626834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors argue that simply comparing aggregated accuracy scores of benchmarks are statistically flawed because it ignores the fact that two model evaluations are performed on the same set. Authors propose the testing framework based on the exact one-sided McNemar test. And further show that the testing framework can confidently flag model degradation as small as 0.3% to be statistically significant."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors challenge a fundamental approach in LLM evaluation benchmark where accuracies may not necessarily show the degradation of LLM capability. Currently the general practice is to consider difference within a certain threshold as statistically insignificant. Authors provide a more principled way to identify the flaws of the convention."}, "weaknesses": {"value": "1. As a work studying variance/randomness, in the main experiment, it might be important for authors to report results across three runs.\n2. Similarly, authors may want to reproduce an empirically lossless variant to rule out the hardware level randomness. (for example, use hf transformers backend and throw in the deterministic flag) Since we don’t necessarily know how the hardware-level randomness interact with the model-level randomness."}, "questions": {"value": "1. Different inference engine backend (hf transformers, vllm, sglang) sometimes show different results on benchmarks. It would be helpful to test on different engines and tell if different engines may yield different results.\n2. The method may be useful for measuring forgetting of language models after finetuning/continual learning. I encourage authors to explore other scenarios where degradation is an important topic (pruning, finetuning, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLGYFDQUg5", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Reviewer_Uko7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Reviewer_Uko7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150810474, "cdate": 1762150810474, "tmdate": 1762916863694, "mdate": 1762916863694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Generalized methods, new experiments and general reply"}, "comment": {"value": "We thank all the reviewers for their feedback and are happy to hear that the reviewers\nagree that a proper statistical treatment of model degradations is an important\nand understudied problem in LLM inference optimization. We appreciate the reviewers' \ncriticism and suggestions and have now addressed them in a revised version.\n\nThe main concerns about our initial submission shared among the reviewers are about    \na/ our work focusing on binary scoring metrics  \nb/ limited scope of evaluation (only evaluating on quantized Llama 3.x models)  \nc/ limited novelty from a statistical perspective (building on the existing McNemar test)\n\nWe addressed points a/ and b/ through further theory and experiments\n and uploaded a revised version with changes highlighted (we will prepare a final and clean version after the paper decision, of course). We briefly discuss\n the new results as well as a general reply to concern c/, and clarify the remaining\n questions of each reviewer in separate replies.\n\nWe hope that this addresses the remaining concerns and are happy to clarify further questions.\n \n## a/ Beyond binary scores (new Appendix 4)\nWe added **Appendix D**, where we introduce equivalents of our initial binary tests, where $p$-values are obtained via \npermutations/bootstrapping. We show that on **binary data those permutation $p$-values are almost exactly the same as the ones \nobtained with our initial binary tests**. Furthermore, the permutation tests natively work with non-binary metrics, like Rouge Scores or F1\n Scores. We added synthetic and real experiments (TruthfulQA) that show that our permutation tests should be used in such cases, rather \n than thresholding scores to binary, or using win-rates in the contingency table (as we recommended for simplicity in our initial submission). \n To reflect this, we also updated the discussion in Section 5, which is now a discussion of the generalization to non-binary scores, \n rather than a discussion of its limitation. \n\nWe will of course also release the code for that generalized test.\n\n## b/ Limited scope of empirical evaluations (Section 4 & Appendix D.4)\nThe reviewers rightly criticized that our evaluations were focused on Llama 3.x models and quantization as optimizations. Furthermore, that we were only testing with vLLM. We have now added experiments on a **Mistral**-Small model (Section 4), which confirmed that less than 1% accuracy drop can be statistically significant (see the w4a16 results added to Table 2). Furthermore, we tested a **pruned and distilled** variant of the Llama-3.1 8B (base) model, also showing a statistically significant accuracy drop. In terms of frameworks, we included an experiment serving the model with the **transformers** library, showing that changing the framework is lossless in this case, as  it should be.\n  \nWhile our main experiments focused on the LM Evaluation Harness, we now also use OpenAI's evaluation suite to investigate **GPT-OSS**. In this case we indeed obtain non-binary scores (because the same sample is evaluated multiple times) and thus directly use our new permutation-based tests, see Appendix D.4. There we also test a **pruned** model with fewer experts in the MoE layers, which however, results in very bad performance.\n\nThese experiments strengthen the conclusions and demonstrate that our framework applies robustly across multiple models and inference stacks. If reviewers have specific cases that they think would be crucial to add, we are happy to take another look at those.\n\n## c/ Limited statistical novelty\n\nThe evaluation of LLMs has two major difficulties. First, a model is usually \nevaluated on\nmultiple benchmark tasks and, second, different models are evaluated with \nexactly the same examples.\nWe are not aware of any out-of-the-box statistical solution for such a scenario. \nFurthermore, in the LLM community, apparently a proper statistical treatment is not known \n(Dutta et al, 2024; Kurtic et al, 2025).\nYou might also want to consider the reviews of Dutta et al, where the programm chairs\nexplicitly ask for a better statistical treatment https://openreview.net/forum?id=QVG7j29Sta&noteId=F8xc48nuZ1 \n\nOur contribution is not proposing a new test, but adapting and integrating classical \nstatistical tools into the specific multi-benchmark, paired-evaluation setting of LLMs,\n for which, to our knowledge, no principled solution existed.\n\nWe think it is crucial to popularize such tools, not only to detect too heavily optimized models, \nbut also to include them in continuous integration pipelines in large open-source \nprojects like vLLM.\n\nIn our paper, we thoroughly credit the historical work of McNemar and Fisher and do not \nclaim to make any groundbreaking contribution in terms of statistical methodology."}}, "id": "qZqFfhaRrA", "forum": "cM3gsqEI4K", "replyto": "cM3gsqEI4K", "signatures": ["ICLR.cc/2026/Conference/Submission3608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3608/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission3608/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763437341321, "cdate": 1763437341321, "tmdate": 1763437341321, "mdate": 1763437341321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}