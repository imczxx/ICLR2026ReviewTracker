{"id": "MAApSY32Z6", "number": 3513, "cdate": 1757454204074, "mdate": 1763564707029, "content": {"title": "MergeTune: Continued Fine-Tuning of Vision-Language Models", "abstract": "Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MergeTune) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MergeTune improves the harmonic mean of CoOp by +5.6\\% on base-novel generalisation without adding parameters. \nOn robust fine-tuning evaluations, the LMC-merged model from MergeTune surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model.", "tldr": "", "keywords": ["Vision-Language Models", "Continue Learning", "Parameter-Efficient Fine-Tuning", "Robust Fine-Tuning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2b14276c5c32b44f68927b2424fac6d21456c02.pdf", "supplementary_material": "/attachment/a834d2bb7433a33ae2d3dbee8e320fac2764f6e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of catastrophic forgetting in fine-tuned vision-language models (VLMs). The authors propose a new paradigm called Continued Fine-Tuning (CFT), which aims to recover pretrained knowledge after a model has already been adapted, rather than just mitigating forgetting during adaptation. Extensive experiments on CLIP are conducted to verify the effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n2. The experiments are comprehensive."}, "weaknesses": {"value": "1. Some assumptions are overly strong and insufficiently discussed. The derivation of the second-order surrogate is not entirely convincing. For example, for smaller models such as ViT-B/16, it is questionable whether the gradients of the trained model are close to zero, as the authors assume. Additionally, the assumption \nùêª1‚âàŒºI appears too strong.\n\n2. Comparing a training-free method with training-based methods seems unfair, as they operate under fundamentally different settings.\n\n3. The performance improvement is limited. The proposed method requires significantly more training time to train a new model, yet the gains on downstream tasks are modest. In the base-to-new setting, it even underperforms compared to ATPrompt.\n\n4. There is no discussion of training time or computational cost. It would be helpful to include an analysis of the training cost of the proposed method.\n\n5. The paper claims that the proposed method is designed for vision-language models, yet only CLIP is evaluated. A broader experimental validation would strengthen this claim."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zvrxXZmafx", "forum": "MAApSY32Z6", "replyto": "MAApSY32Z6", "signatures": ["ICLR.cc/2026/Conference/Submission3513/Reviewer_Poyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3513/Reviewer_Poyv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421653102, "cdate": 1761421653102, "tmdate": 1762916778221, "mdate": 1762916778221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MERGETUNE, a novel *continued fine-tuning* (CFT) framework that recovers pretrained knowledge in vision-language models (VLMs) after standard fine-tuning has already occurred. Unlike prior work that attempts to mitigate forgetting during adaptation, MERGETUNE operates post hoc by leveraging linear mode connectivity (LMC) to find a model that maintains low-loss interpolation paths to both the original zero-shot model (e.g., CLIP) and the fine-tuned model (e.g., CoOp). To avoid the infeasible requirement of replaying the massive pretraining data, the authors introduce a second-order surrogate loss that approximates the LMC constraint for the zero-shot model using a simple ‚Ñì¬≤ regularizer toward the pretrained weights. The method is model-agnostic, requires no architectural changes, and consistently improves performance across multiple VLM adaptation strategies (prompt-based, adapter-based, linear probing, full fine-tuning) and evaluation protocols (base-to-novel, cross-dataset, domain generalization, robust fine-tuning). Notably, MERGETUNE achieves a +5.6% harmonic mean gain over CoOp without adding parameters and outperforms CLIP in all evaluated settings, with further gains possible via ensembling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Introduces a conceptually fresh and practical paradigm‚Äîcontinued fine-tuning‚Äîthat decouples adaptation from knowledge recovery, enabling post hoc enhancement of any existing fine-tuned VLM.  \n(2) Proposes a theoretically grounded yet simple method (MERGETUNE) based on linear mode connectivity, with a clever second-order surrogate that eliminates the need for pretraining data replay‚Äîa major practical bottleneck.  \n(3) Demonstrates consistent and significant improvements across diverse adaptation methods (CoOp, MMA, PromptKD, etc.) and evaluation settings (few-shot, many-shot, OOD robustness), validating the generality of the approach.  \n(4) Provides comprehensive empirical analysis, including ablation studies on hyperparameters, comparisons with training-free merging (TIES, DARE) and ensembling baselines (VRF, Wise-FT), and shows MERGETUNE reduces inference cost while improving performance."}, "weaknesses": {"value": "(1) The surrogate loss assumes the Hessian of the pretraining loss at the zero-shot checkpoint is isotropic (H‚ÇÅ ‚âà ŒºI) and that the gradient is near zero. While common, this may not hold for large-scale VLMs like CLIP trained on noisy web data. Could the authors provide empirical validation of these assumptions (e.g., via Hessian spectrum estimation on a subset) or discuss how violations might affect MERGETUNE‚Äôs performance?  \n(2) In Table 1, MERGETUNE improves CoOp by +5.58% HM but only +0.36% on PromptKD. The paper attributes this to PromptKD already preserving more knowledge. However, is it possible that MERGETUNE‚Äôs ‚Ñì¬≤ regularizer toward CLIP inadvertently conflicts with PromptKD‚Äôs distillation objective, limiting gains? An analysis of the weight trajectory or feature similarity between CLIP, PromptKD, and MERGETUNE-enhanced PromptKD would help clarify this.  \n(3) The method samples a small number of Œ± values (e.g., 5‚Äì10) to approximate the expectation in the LMC loss. How sensitive are results to the number and spacing of these Œ± points? Did the authors experiment with adaptive sampling or importance weighting along the interpolation path to better capture high-loss regions?  \n(4) For adapter-based methods like MMA, which modify model architecture, MERGETUNE is applied only to the linear head (per Section 5.2). This seems inconsistent with the claim of being ‚Äúmodel-agnostic.‚Äù Can MERGETUNE be applied to the full adapter parameters? If not, what architectural constraints limit its applicability, and how does partial application affect knowledge recovery?  \n(5) The continued model is initialized as a convex combination of wÃÉ‚ÇÅ and wÃÉ‚ÇÇ (œÑ ‚àà [0,1]). Figure 3(b) shows optimal œÑ ‚àà [0.3, 0.6], but this requires tuning. In real-world deployment, the ideal œÑ may be unknown. Does MERGETUNE include a practical strategy for selecting œÑ without validation data (e.g., based on fine-tuning loss magnitude or forgetting estimates)?  \n(6) All experiments use CLIP ViT-B/16 (and briefly ViT-B/32). How does MERGETUNE scale to larger backbones (e.g., ViT-L/14) or non-CLIP VLMs (e.g., ALIGN, BLIP)? Given that mode connectivity properties can vary with model size and training data, it would strengthen the paper to show results beyond the standard CLIP setting."}, "questions": {"value": "Please weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3lNFF37zkt", "forum": "MAApSY32Z6", "replyto": "MAApSY32Z6", "signatures": ["ICLR.cc/2026/Conference/Submission3513/Reviewer_uAR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3513/Reviewer_uAR8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625366041, "cdate": 1761625366041, "tmdate": 1762916777897, "mdate": 1762916777897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MERGETUNE, a continued fine-tuning framework for vision-language models such as CLIP. Instead of preventing catastrophic forgetting during adaptation, the authors attempt to recover pre-trained knowledge after fine-tuning has already taken place. The key idea is to exploit linear mode connectivity (LMC) to find new parameters with two low-loss connections to the zero-shot and fine-tuned checkpoints, effectively merging their knowledge. A second-order surrogate term approximates the pretraining loss to avoid data replay from the pretraining task. The method is model-agnostic and post-hoc, requiring no architectural modifications. Experiments on base-to-novel, cross-dataset, domain generalization, and robust fine-tuning benchmarks show consistent improvements across CoOp, KgCoOp, MMA, and PromptKD, achieving state-of-the-art results with minimal overhead. Nevertheless, I am not an expert in VLMs, so the following comments and questions could be based on potentially inaccurate understandings or biased perspectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n1. Introduces a novel post-hoc fine-tuning paradigm focusing on knowledge restoration instead of prevention, offering a new conceptual direction.\n\n2. The approach is simple, elegant, and general, requiring no model modifications and applying to various adaptation methods.\n\n3. Using linear mode connectivity as an explicit optimization objective provides solid geometric intuition and interpretability.\n\n4. Experimental evaluation is extensive and convincing, demonstrating consistent gains across diverse datasets and settings."}, "weaknesses": {"value": "1. The theoretical justification is limited, and it is unclear why continued fine-tuning converges to a mode-connected region.\n\n2. The final objective closely resembles standard $L_2$-regularized fine-tuning, so the novelty might be overstated.\n\n3. The isotropic Hessian assumption in the surrogate loss is strong and unvalidated; its practical effect remains unclear.\n\n4. The paper does not quantify the extra training cost or test scalability on larger or multimodal models."}, "questions": {"value": "1. How sensitive is MERGETUNE to violations of the isotropic curvature assumption ($H_1 \\approx \\mu I$)?\n\n2. What is the additional training or computational overhead compared to standard fine-tuning?\n\n3. Could continued fine-tuning lead to ‚Äúover-merging,‚Äù where downstream adaptation performance is harmed after too many epochs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l066dJiyuz", "forum": "MAApSY32Z6", "replyto": "MAApSY32Z6", "signatures": ["ICLR.cc/2026/Conference/Submission3513/Reviewer_wRZr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3513/Reviewer_wRZr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843293683, "cdate": 1761843293683, "tmdate": 1762916776084, "mdate": 1762916776084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a novel model merging method named Continue fine-tuning, which utilizes the low-loss path phenomenon in the model merging to effectively combine knowledge from the pre-training model and the fine-tuned version, achieving high performance across various tasks, models, and fits well with different tuning methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework shows a strong performance gain compared to the simple model merging baseline\n- Achieving competitive performance with low inference cost\n- Theoretical analysis on the low-loss path phenomenon and its relation with easing catastrophic forgetting is interesting."}, "weaknesses": {"value": "- While overall performance gain looks good, the improvement on the different tuning methods is unstable, sometimes hurts the base performance while largely improves the novel class, and sometimes degrades the novel performance while the base performance becomes better. This indicates the instability of the proposed framework\n- Figure 1 is really unfriendly for the reader. I strongly suggest that the author update the scale for different datasets to emphasize the performance difference between models instead of packing them together..."}, "questions": {"value": "- Does the low-loss path phenomenon always exist, no matter how complex the models are? Since the more recent VLM architecture has become increasingly complex, handling complex real-world tasks that generalize across various tasks, the loss space will become much more complicated. I am curious whether such LMCs discovered at an early age still affect?\n- How is Figure 2 constructed? Is it just an illustration or sampled from the training process? Could the author provide more detail on the illustration part.\n\nI will raise my score once my concerns are solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7bZ9f6jJBB", "forum": "MAApSY32Z6", "replyto": "MAApSY32Z6", "signatures": ["ICLR.cc/2026/Conference/Submission3513/Reviewer_wfKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3513/Reviewer_wfKc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881692500, "cdate": 1761881692500, "tmdate": 1762916775883, "mdate": 1762916775883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}