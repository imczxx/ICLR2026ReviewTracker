{"id": "iE0dWuv6jU", "number": 6634, "cdate": 1757990977757, "mdate": 1759897903769, "content": {"title": "Leveraging Rotation Symmetry for Efficient LoRA Merging in Large Language Models", "abstract": "Merging a large number of low-rank adaptations (LoRAs) is a key technology for enhancing the integration and deployment efficiency of large language models (LLMs). However, this process has long been hindered by the catastrophic \"parameter interference\" problem, which often leads to a sharp decline in model performance after merging. Existing merging methods are vulnerable when dealing with complex conflicts, such as high-rank LoRAs. While the classical rotation alignment approach can enhance robustness, it is difficult to apply due to incompatibility with the LoRA structure and its high computational complexity. To address these challenges, we propose a novel two-stage parameter alignment (TSPA) framework. TSPA fundamentally overcomes the limitations of existing methods through two core strategies: (1) we innovatively design an alignment mechanism within the LoRA low-rank space, which effectively resolves the structural compatibility issue while maintaining functional equivalence; (2) we introduce an alignment paradigm of \"comparison with an average model,\" which reduces computational complexity from quadratic to linear, ensuring the scalability of the method. To guide the alignment process, TSPA further designs two complementary optimization objectives: macro-functional alignment and micro-parameter alignment, and uses Stiefel manifold optimization to solve the problem, steadily maintaining the orthogonality of the rotation matrices during iterations. We conduct experiments on Natural Language Processing (NLP) tasks using models such as Llama-3-8B. The results show that TSPA not only outperforms state-of-the-art (SOTA) baseline methods, including DARE, in terms of average performance across tasks but also demonstrates unique comprehensive advantages: its two-stage design achieves the optimal balance between task capabilities and general knowledge; it exhibits greater robustness than SOTA methods in high-rank and high-interference scenarios; and it shows significant effectiveness in retaining fine-grained functions, such as \"safety capability.\" This work presents a novel and practical framework for efficient, powerful, and stable multi-task model merging.", "tldr": "The paper introduces TSPA, a two-stage parameter alignment framework that effectively performs multi-LoRA merging in LLMs, alleviating parameter interference while outperforming existing methods in robustness and scalability.", "keywords": ["Natural Language Processing", "Large Language Models", "Model Merging", "Low-Rank Adaptation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f104f01194d575c0fae15da9720368038d215d79.pdf", "supplementary_material": "/attachment/16055cd49ae40c4684d85e254e006df6b073e3a2.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates the problem of parameter interference when merging multiple low-rank adaptations (LoRAs) in large language models, which often leads to performance degradation. It proposes a Two-Stage Parameter Alignment (TSPA) framework that applies rotation symmetry to align parameters in low-rank space, addressing structural incompatibility and reducing computational complexity from quadratic to linear. The core contributions include a novel alignment mechanism and a linear-complexity paradigm. Experimental results on models like Llama-3-8B demonstrate that TSPA outperforms state-of-the-art methods in average performance across NLP tasks, robustness in high-rank scenarios, and retention of fine-grained capabilities such as safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an innovative low-rank space alignment mechanism that preserves LoRA structure while maintaining functional equivalence, solving the problem of vulnerability when merging high-rank LoRAs.\n\n2. The paper discusses safety, which is a perspective that mainstream merging methods ignore and is of great significance.\n\n3. It covers comprehensive evaluations across multiple tasks (e.g., Instruction-Following, Math, Code, Safety), different LoRA ranks (e.g., 8, 16, 32), and models (e.g., Llama-3-8B, Qwen3-8B)."}, "weaknesses": {"value": "1. This paper missed several key researchs that are highly related to the motivation. For example, RobustMerge[1] also claims to tackle robustness when merging LoRA. It would be of great benefit to have a comprehensive comparison with it to demonstrate the effectiveness of the proposed method. Other works like KnOTS and LoraHub should also be conducted.\n\n2. The experimental results are not that impressive. The improvements mainly come from safety task.\n\n3. No explicit explanation of O(n) in the introduction. It appears afterwards and the authors should reorganize the order.\n\n4. The figures in Figure 3 are too small to be read. It should be replaced with a clearer one.\n\n5. The method assumes rotation symmetry applies broadly, but no discussion of scenarios where alignment might fail (e.g., non-linear interactions) is included."}, "questions": {"value": "1. It would be better to compare with LoRA-based[1,2] and up-to-date[3,4] merging methods to validate the effectiveness of the proposed method.\n\n2. More experimental results like different types of structure (mllm, etc), different types of lora (dora, etc) are encouraged to further demonstrate the effectiveness of the proposed method.\n \n3. The paper mentions computational efficiency in merging. But I think the complexity mainly comes from the iterative process it introduces in merging. A large number of merging methods like DARE, Ties-merging directly merge multiple models in a post-training paradigm, so the computational efficiency should not be a bottleneck. The author should provide more analysis and experiments, such as quantitative experiments to illustrate the necessity of reducing computational complexity.\n\n4. Despite the proposed TSPA, the technique within each stage like rotation and scaling is not that novel. More explanation and comparison with existing methods should be incorporated.\n\n5. About OOD generalization: How does TSPA scale with the number of LoRAs beyond the tested scenarios?\n\n[1] RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness.\n\n[2] LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition.\n\n[3] EMR-Merging: Tuning-Free High-Performance Model Merging.\n \n[4] Parameter Competition Balancing for Model Merging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XTNqV4kU1O", "forum": "iE0dWuv6jU", "replyto": "iE0dWuv6jU", "signatures": ["ICLR.cc/2026/Conference/Submission6634/Reviewer_ggVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6634/Reviewer_ggVN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760611987051, "cdate": 1760611987051, "tmdate": 1762918952479, "mdate": 1762918952479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of parameter interference in lora merging, which often leads to significant performance degradation. The authors identify that existing rotation alignment methods are inapplicable to lora merging due to structural incompatibility and quadratic complexity. To addaress this, the authors introduces the two-stage parameter alignment (TSPA) framework. The main contributions include an alignment mechanism that operates within the lora low-rank space to preserve its structure, and a `comparison with an average model' paradigm that reduces complexity to linear. Empirical results on Llama-3-8B demonstrate that TSPA outperforms SOTA methods, particularly in high-interference settings like high-rank lora merging."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength of this paper is the adaptation of rotation symmetry for the specific constraints of lora merging. The solution to the structural incompatibility problem by operating directly on the low-rank matrices is effective. \n\n- The `comparison-to-average-model' strategy is interesting, which reduces the computational complexity from quadratic to linear."}, "weaknesses": {"value": "- The paper focuses on the reduction in asymptotic complexity. However, from a practical stand point, some analysis of the wall-clock time and computational overhead of the iterative optimization process, especially in comparison to the non-iterative baselines, would be beneficial.\n\n- The paper uses a simple linear average of the models as the alignment anchor. A discussion on the method's sensitivity to the quality of this initial anchor point could be beneficial, particularly in cases of extreme initial parameter interference.\n\n- An analysis of the model's sensitivity to the hyperparameters is needed to further understand the paper's claims of robustness."}, "questions": {"value": "- Could you provide some comparison of the wall-clock time required for merging using TSPA versus the TIES-Merging and DARE baselines under the experimental settings reported in Table 1?\n\n- The two alignment stages are performed sequentially. What is the rationale for this design choice over a joint optimization of the two objectives?\n\n- How does the alignment process perform if the initial 'average model' target is of very low quality (as in the `Task Arithmetic' case)? Does the optimization procedure reliably converge to a good solution regardless of the anchor point's quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yL9rBczYtr", "forum": "iE0dWuv6jU", "replyto": "iE0dWuv6jU", "signatures": ["ICLR.cc/2026/Conference/Submission6634/Reviewer_L1Rc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6634/Reviewer_L1Rc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760636068104, "cdate": 1760636068104, "tmdate": 1762918952198, "mdate": 1762918952198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework called TSPA to solve a critical problem in LLMs: efficiently merging multiple LoRAs without catastrophic performance degradation. Merging LoRAs is essential for deployment efficiency, but it often fails due to parameter interference, where conflicts between the adapters' parameters sharply decrease the merged model's capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Solves Critical, Previously Unsolved Bottlenecks: The paper successfully addresses two nearly insurmountable obstacles that previously hindered the use of rotation alignment for LoRA merging. It introduces a novel mechanism to perform alignment within the LoRA low-rank space, solving the structural incompatibility issue. Simultaneously, it pioneers an alignment paradigm based on comparison with an average model, which reduces computational complexity from a quadratic growth rate to a scalable linear growth rate.\n- Demonstrated Robustness in High-Interference Scenarios: The TSPA framework shows significantly greater robustness than existing SOTA methods, especially in high-interference scenarios. In experiments with high-rank adapters (specifically, a rank of 32), where SOTA methods experience performance collapse, TSPA maintains robust and outstanding results, proving its superiority in handling complex parameter conflicts.\n- Achieves a Balanced and Comprehensive Performance: The two-stage design, which combines macro-functional alignment (Attention Alignment) and micro-parameter alignment (LoRA Alignment), achieves an optimal balance between task-specific capabilities and general knowledge. Ablation studies confirm that this integrated approach is more comprehensive and powerful than using either alignment stage alone.\n- Superior Retention of Fine-Grained Functionality: Beyond just improving average task performance, TSPA demonstrates a unique advantage in preserving delicate and fragile model functionalities. In all experiments, TSPA consistently outperformed other methods in safety capability evaluations , indicating its alignment strategy is more effective at protecting fine-grained parameter structures vulnerable during merging."}, "weaknesses": {"value": "- `Limited Comparison`: The study restricts its comparison to limited TSPA baselines, omitting a comprehensive analysis against more recent work [1]. Furthermore, it lacks exploration of other LoRA variants [2-4], potentially limiting the method's applicability to the broader PEFT landscape.\n- `Limited Analysis`: The paper's approach of optimizing on the Stiefel manifold to solve complex and constrained problems is heuristic. A deeper investigation into theoretical bounds and performance preservation guarantees is warranted.\n- `Limited Results`: Despite claiming efficient time complexity, the paper presents no supporting experimental results. This omission hinders a clear assessment of its performance-efficiency trade-offs against competing methods.\n- `Limited Open-sourcing`: The open-source contribution is confined to training logs without the corresponding test logs, which offers limited value for verification. A more comprehensive and transparent release is expected.\n\n[1] SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging\n\n[2] When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications\n\n[3] HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning\n\n[4] CoLA: Collaborative Low-Rank Adaptation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FlKtWOwL4c", "forum": "iE0dWuv6jU", "replyto": "iE0dWuv6jU", "signatures": ["ICLR.cc/2026/Conference/Submission6634/Reviewer_QzJ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6634/Reviewer_QzJ7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395144856, "cdate": 1761395144856, "tmdate": 1762918951815, "mdate": 1762918951815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous rotation alignment approaches are used to reduce the conflicts during model merging. However, they are not incompatible with LoRA-trained models and suffer from high computational cost. This paper proposed TSPA, a two-stage framework to align with both the merged weight and the LoRA weights at a macro- and micro-level, respectively. The experiments are conducted on a Llama-3-8B model with diverse NLP tasks. Compared to baselines such as TIES and DARE, the significant performance improvement shows the effectiveness of their method. Ablation studies are also conducted to evaluate their method in different settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is overall well written. The logic is clear and easy to follow.\n- Experimental details are provided, making this paper reproducible.\n- Performance improvement is significant, supporting the effectiveness of their method."}, "weaknesses": {"value": "- The connection between Sec.3.1 and 3.2 is not clear to me. Sec.3.1 discussed the necessity to use parameter alignment when merging models, but at the beginning of Sec.3.2, it is discussed that \"*Parameter space symmetry refers to the property of a set of models with identical functionality but different parameters….*\" Obviously, models trained on different tasks do not satisfy the assumption of “**identical functionality**”. In that case, does the permutation symmetry still hold for those models?\n- I do not quite understand the role of attention alignment and LoRA alignment in the proposed method. If I understand correctly, $W_{Q_i}$ is the learned weight in the $i$-th model. Since attention alignment (Eq.7) is already trying to align the merged weight, under what situation do we still need to do the micro-level alignment in Eq.9? In other words, I think the necessity to have Eq.9 after Eq.7 is not well discussed.\n- Most previous merging methods, especially the ones that are used in this paper, are usually computationally efficient. However, the proposed method introduces an extra overhead that could be large. I suggest that the author compare such computational cost ot even the trade-off to better justify the utility of TSPA. While the results reported in Tab.1 are good, Tab.4 shows that KnOTS + TIES can outperform TSPA with less computation, leading to more concerns.\n- In the abstract, the author discussed that rotation alignment approaches can enhance robustness, but there are no experiments on the robustness of TSPA. Does the robustness only mean less performance degradation during merging?"}, "questions": {"value": "- Fig.2 appears before Fig.1, which is a bit confusing. I think it would be better to relabel these two figures.\n- In Eq.5 and 6, why does the merged model have the form of “entangled” LoRA pieces. That is, a common practice of merging models is $W’ = W + \\sum_i \\lambda_i B_iA_i$, while this paper multiplies those LoRA across all the models. Is there any specific reason to make this assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XulqEoH9i", "forum": "iE0dWuv6jU", "replyto": "iE0dWuv6jU", "signatures": ["ICLR.cc/2026/Conference/Submission6634/Reviewer_XMCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6634/Reviewer_XMCd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542452709, "cdate": 1761542452709, "tmdate": 1762918950241, "mdate": 1762918950241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TSPA, a two-stage parameter alignment framework for merging multiple LoRA adapters in LLMs. The key innovation is performing rotation alignment within the low-rank space while reducing computational complexity from O(n²) to O(n) by comparing with an average model. Experiments on Llama-3-8B show improvements over TIES-Merging and DARE across multiple tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a solid technical contribution by extending rotation symmetry to LoRA structures. The core insight of performing alignment in low-rank space while preserving functional equivalence through paired rotation of A and B matrices is elegant and well-motivated. The reduction from quadratic to linear complexity through the \"compare-with-average\" paradigm is theoretically sound and practically important for scaling to many models.\n\nThe experimental evaluation is reasonably comprehensive, covering multiple merging scenarios (3-4 models), different LoRA configurations (rank 8/16/32), and demonstrating particular strength in preserving safety capabilities. The two-stage design combining macro-level attention alignment with micro-level parameter alignment is conceptually appealing."}, "weaknesses": {"value": "**Theoretical foundation is insufficient.** While the intuition from Figure 1 is clear, the paper lacks rigorous theoretical analysis. There is no proof that rotation alignment guarantees linear mode connectivity in the high-dimensional parameter space of LLMs, nor any convergence analysis for the Stiefel manifold optimization. \n\n**Experimental validation is incomplete and unconvincing.** The most critical issue is the absence of runtime comparisons. The paper claims O(n) complexity but runs 2000 optimization iterations, actual wall-clock time against baselines is never reported. The performance gains are modest (52.41 vs 48.94 average score, ~7% improvement) but without error bars or statistical significance testing across multiple runs. Curiously, Table 1 shows TSPA(Attention) achieves 53.01, higher than the full TSPA at 52.41, suggesting the two-stage combination may actually hurt performance rather than help.\n\n**Limited scope raises generalization concerns.**  There is no comparison with a simple but important baseline: merge-then-finetune. This omission is problematic because directly fine-tuning the merged model on a small dataset might achieve similar results with less complexity."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5eYl2zG3ZL", "forum": "iE0dWuv6jU", "replyto": "iE0dWuv6jU", "signatures": ["ICLR.cc/2026/Conference/Submission6634/Reviewer_AAva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6634/Reviewer_AAva"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199416564, "cdate": 1762199416564, "tmdate": 1762918949656, "mdate": 1762918949656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}