{"id": "Tab9dmIGRg", "number": 21011, "cdate": 1758312770438, "mdate": 1763124573781, "content": {"title": "WildSVG: Towards reliable SVG generation under Real-Word conditions", "abstract": "We introduce SVG extraction, the task of translating specific visual inputs into scalable vector graphics. Existing multimodal models such as StarVector achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. To address this gap, we extend StarVector’s capabilities toward robust vision-to-SVG translation in the wild. A central challenge in this direction is the lack of suitable benchmarks. To fill this need, we develop two complementary datasets: Natural WildSVG, consisting of real-world images paired with SVG annotations, and Synthetic WildSVG, which integrates complex and elaborate SVG designs into real-life scenarios to simulate challenging conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. Building on them, we benchmark StarVector and related models. Our study establishes SVG extraction as a new problem domain, introduces datasets and evaluation protocols for its study, taking initial steps toward extending multimodal LLMs to handle reliable SVG generation in complex, natural scenes.", "tldr": "We present a new dataset and benchmark for a new SVG generation task", "keywords": ["SVG", "VLLM", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f35098ef94fe2341669e3ee2ae1d684076331407.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SVG extraction, the novel task of translating specific visual inputs (such as real-world images) into Scalable Vector Graphics (SVG). Existing multimodal LLMs (e.g., StarVector) perform well when generating SVGs from clean renderings or text but falter under noisy, cluttered, and domain-shifted natural images. To address this, the authors define and formalize SVG extraction as a research problem. Besides, they present WildSVG, the first benchmark dedicated to SVG extraction. They also devise new evaluation protocols and multi-metric analysis (L2, SSIM, LPIPS, DINO). Finally, this paper benchmark a range of state-of-the-art VLLMs on the task, including StarVector, GPT-4/5, Claude Opus, Gemini, and Qwen.\nTheir results highlight the performance ceiling of current models, the semantic vs. fidelity trade-off, and the increased difficulty of SVG extraction in natural settings. The work establishes a foundation for systematic research and development on reliable SVG generation from complex images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper present the first benchmark dedicated to SVG extraction, comprising Natural WildSVG, focusing on real-world images paired with verified SVG annotations, and Synthetic WildSVG, focusing on natural images with synthetically embedded, complex SVGs.\n2. They devise new evaluation protocols and multi-metric analysis. Besides, they benchmark several VLM on this task.\n3. The paper outlines clear future directions and potential integration with multimodal LLM pipelines, encouraging further exploration and dataset expansion."}, "weaknesses": {"value": "1. From my perspective, the proposed task lacks sufficient innovation.\n2. Limited benchmark diversity: Expansion to more diverse SVG types (beyond logos, include pictograms, diagrams, UI elements) could clarify task boundaries and model strengths.\n3. Lack of Editing-based SVG extraction: It is possible to first use image editing to extract the target object into raster image, them vectorize the raster image into SVG.\n4. More Robust Metrics: this paper only consider visual similarity as the metric. However, code compactness and editablity are also important."}, "questions": {"value": "1. Have you or do you plan to fine-tune VLLMs specifically on WildSVG?\n2. How might you further extend the dataset into more SVG types, e.g. diagrams, UI elements?\n3. Beyond the current metrics, have you considered human evaluations on SVG usefulness/quality, or downstream applicability (e.g., re-editing for designers)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "17aeoD9bWT", "forum": "Tab9dmIGRg", "replyto": "Tab9dmIGRg", "signatures": ["ICLR.cc/2026/Conference/Submission21011/Reviewer_ewb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21011/Reviewer_ewb4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903254134, "cdate": 1761903254134, "tmdate": 1762999993475, "mdate": 1762999993475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "DYpM4x8sY4", "forum": "Tab9dmIGRg", "replyto": "Tab9dmIGRg", "signatures": ["ICLR.cc/2026/Conference/Submission21011/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21011/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763124572879, "cdate": 1763124572879, "tmdate": 1763124572879, "mdate": 1763124572879, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces SVG extraction, a task for generating vector graphics from specific elements within real-world images. The authors contribute the WildSVG benchmark, comprising new natural and synthetic datasets, to facilitate research on this challenging problem. They benchmark a suite of modern VLLMs, revealing a significant performance gap and an interesting trade-off between the models' semantic understanding and aesthetic fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The claims are well backed by experiments. Using both one-step and two-step evaluation settings helps disentangle localization from vectorization capabilities. The choice of metrics, covering both pixel-level and semantic similarity, is comprehensive."}, "weaknesses": {"value": "1. The presentation of the paper is extremely poor and looks like it is written in hurry.\n2. The test sets are worryingly small. This raises serious questions about the statistical significance of the reported results and the reliability of the benchmark for distinguishing between top-performing models where score differences are marginal.\n3. The authors astutely identify that most VLLMs cheat by using SVG text primitives instead of drawing shapes. However, the chosen raster-based metrics (DINO, LPIPS, etc.) fail to penalize this behavior and may even reward it, meaning the quantitative results do not fully reflect this important qualitative failure mode."}, "questions": {"value": "1. Given the small test sets, can you provide confidence intervals or a statistical significance analysis for your results? It is difficult to assess whether the reported performance differences between models in Tables 4 and 5 are meaningful.\n2. You suggest that StarVector's failure in the one-step setting is due to being overwhelmed by noise. Could this failure not also be a more fundamental architectural flaw stemming from its training regime, where text and image conditioning were learned separately, leading to weak prompt alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AgZ5yM5QGa", "forum": "Tab9dmIGRg", "replyto": "Tab9dmIGRg", "signatures": ["ICLR.cc/2026/Conference/Submission21011/Reviewer_kBHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21011/Reviewer_kBHm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939007861, "cdate": 1761939007861, "tmdate": 1762999993476, "mdate": 1762999993476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a novel task, SVG extraction that combines vision and structured graphics, which is timely given the rise of multimodal LLMs. The authors introduce WildSVG, the first benchmark for extracting vector graphics from real images. They construct two complementary datasets: Natural WildSVG and Synthetic WildSVG. The evaluation protocol is carefully designed with multiple metrics to capture different aspects of output quality. Overall, framing SVG extraction is a well-motivated problem with clear definitions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors evaluate a wide range of state-of-the-art vision-language models (Qwen, Gemini, Claude, GPT, StarVector, GLM) on both Natural and Synthetic WildSVG test sets.\n\n2. They use four complementary metrics (L2, SSIM for pixel fidelity; LPIPS, DINO for perceptual/semantic similarity), which is an appropriate choice to capture different aspects of the generated SVG."}, "weaknesses": {"value": "1. How often did the VLLM-based SVG matching fail or produce incorrect logo–SVG pairs in Natural WildSVG? Were any manual checks done, and how sensitive are the results to these mismatches?\n\n2. Can you clarify how the “focus prompt” is formulated and used? If the prompt is ambiguous or generic, how does it affect the model’s output?\n\n3. Can you provide more details on the synthetic data creation? How diverse are the embedded SVG contexts (lighting, occlusion, styles)?\n\n4. How do you ensure that the chosen pixel/semantic metrics correlate with true SVG fidelity?"}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ye8ir6jG5p", "forum": "Tab9dmIGRg", "replyto": "Tab9dmIGRg", "signatures": ["ICLR.cc/2026/Conference/Submission21011/Reviewer_9H4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21011/Reviewer_9H4j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995537197, "cdate": 1761995537197, "tmdate": 1762940253672, "mdate": 1762940253672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}