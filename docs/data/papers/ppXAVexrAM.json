{"id": "ppXAVexrAM", "number": 10821, "cdate": 1758182644668, "mdate": 1759897626444, "content": {"title": "ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View", "abstract": "Despite their exceptional generative quality, diffusion models have limited applicability to world modeling tasks, such as novel view generation from sparse inputs. This limitation arises because diffusion models generate outputs in a non-causal manner, often leading to distortions or inconsistencies across views, and making it difficult to incrementally adapt accumulated knowledge to new queries. In contrast, autoregressive (AR) models operate in a causal fashion, generating each token based on all previously generated tokens. In this work, we introduce \\textbf{ARSS}, a novel framework that leverages a GPT-style decoder-only AR model to generate novel views from a single image, conditioned on a predefined camera trajectory. We employ a video tokenizer to map continuous image sequences into discrete tokens and propose a camera encoder that converts camera trajectories into 3D positional guidance. Then to enhance generation quality while preserving the autoregressive structure, we propose a autoregressive transformer module that randomly permutes the spatial order of tokens while maintaining their temporal order. Extensive qualitative and quantitative experiments on public datasets demonstrate that our method performs comparably to, or better than, state-of-the-art view synthesis approaches based on diffusion models. Our code will be released upon paper acceptance.", "tldr": "", "keywords": ["Autoregressive Model", "Novel View Synthesis", "Generative Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31f3fb656111ab5502477817ab463a0dd39d45f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an autoregressive transformer framework for novel-view synthesis conditioned on a predefined camera trajectory. Instead of an image-based tokenizer, the authors use a video-based tokenizer to reduce 3D inconsistencies. They also propose a Plücker-ray autoencoder to tokenize camera trajectories. After tokenizing both images and camera embeddings, a decoder-only transformer generates novel views via next-token prediction. Experiments show the proposed method achieves state-of-the-art results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow.\n\n- The design of the camera-embedding autoencoder is novel and highly useful and could be applied in future work.\n\n- The results look strong and achieve state-of-the-art performance compared to prior work."}, "weaknesses": {"value": "- The authors claim that “diffusion models have limited applicability to world modeling tasks, such as novel view generation from sparse inputs.” However, recent work (e.g., Gen3C, CVPR 2025) demonstrates promising results for novel-view synthesis from sparse inputs. Video-diffusion-based approaches also play an important role in world modeling and achieve competitive performance. Diffusion-based and feed-forward transformer-based methods each have their own strengths for these tasks, so the authors should avoid a biased presentation and instead acknowledge the complementary advantages and trade-offs. \n\n- Using a video-based tokenizer is common in video-diffusion approaches for novel-view synthesis; it is not a novel contribution on its own.\n\n-  Please compare with the latest related work, including transformer-based methods (e.g., Rayzer, ICCV 2025) and recent diffusion-based methods (e.g., Gen3C, ViewCrafter).\n\n-  Novel-view synthesis: If I want a single specific target view, does your model require generating a predefined sequence, or can it directly produce a single image like image-based methods? As I understand it, your model generates views sequentially from input views. If so, generating a target view that is far from the inputs may require many steps and be time-consuming. Please clarify this limitation and discuss trade-offs compared to single-view generation methods.\n\n-  Lack of visualization for temporal/video consistency: For sequences of novel-view images, please provide dynamic video renderings (continuous sequences), not only static frames. I could not find such videos in the supplementary material."}, "questions": {"value": "- Line 316-317 in the paper, has some visualized artifacts.\n\n- Missing some relative reference for novel-view synthetics:\n       Zero-1-to-3: Zero-shot one image to 3d object\n       RayZer: A Self-supervised Large View Synthesis Model\n       Videomv: Consistent multi-view generation based on large video generative model\n       ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis\n\n\n\nAs noted in the Strengths and Weaknesses, I am currently inclined to give a borderline-reject score; I would be willing to raise this if the authors address the timing, baselines, and other main concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are not ethics issues."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bSpUB41nr6", "forum": "ppXAVexrAM", "replyto": "ppXAVexrAM", "signatures": ["ICLR.cc/2026/Conference/Submission10821/Reviewer_xUT2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10821/Reviewer_xUT2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628298292, "cdate": 1761628298292, "tmdate": 1762922026344, "mdate": 1762922026344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARSS, a new framework for novel view synthesis (NVS) from a single input image. The core idea is to move away from diffusion-based models and instead use a GPT-style decoder-only autoregressive (AR) model. The system works by first encoding the input image and a target camera trajectory. It uses a pre-trained video tokenizer to convert image sequences into discrete tokens (to maintain temporal consistency) and a new camera autoencoder to turn camera paths into 3D positional guidance tokens. The model then autoregressively predicts the next visual token, conditioned on all previous tokens and the camera guidance. To help the uni-directional AR model handle 2D image data, the authors propose a hybrid token-shuffling strategy where the spatial order of tokens within a frame is randomized, but the temporal (frame-by-frame) order is kept fixed. The authors claim this method achieves results comparable to or better than state-of-the-art diffusion models on standard NVS benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Token Permutation Strategy: The idea of only permuting the spatial order of tokens while maintaining the temporal sequence is a clever trick. It attempts to get the best of both worlds: adapting the AR model to the bi-directional nature of images without breaking the causal, frame-by-frame generation process.\n\n2. Logical Components: The proposed architecture is well-reasoned. Using a video tokenizer to enforce temporal coherence and a dedicated camera encoder to inject 3D positional guidance makes intuitive sense."}, "weaknesses": {"value": "I have several major concerns that, in my opinion, prevent the paper from being ready for acceptance.\n\n1. Missing Video Results: This is the most significant weakness. The paper tackles a video generation task (synthesizing a sequence of new views) but provides only static images in the figures. The primary justification for using a video tokenizer is to improve temporal consistency, but this claim is impossible to verify without seeing the actual video outputs. Static images can hide critical failures like flickering, temporal drift, or object instability. The authors even note in the appendix that AR models suffer from \"error accumulation,\" and video results are the only way to judge how severe this problem is.\n\n2. Insufficient Evidence for Performance Claims: The paper claims to perform comparably to or better than state-of-the-art methods, but the evidence provided is not convincing. The paper relies on a very small number of selected static images in the figures (e.g., Fig 3, 4, 8). This is not sufficient to demonstrate the general efficiency, robustness, or consistency of the method. Given the claims, a much more comprehensive set of qualitative results is needed to fairly judge the model's performance against strong baselines, especially since the provided quantitative metrics (Table 1) are mixed and do not show a clear win (e.g., SSIM and FID are often worse).\n\n3. Heavy Reliance on Pre-trained Tokenizer: The entire framework is critically dependent on the quality of the \"VidTok\" video tokenizer, which was not designed specifically for this task. The authors admit this in their limitations, stating that \"The generation quality of ARSS is still limited by the quality of tokenizer.\" This makes the contribution feel less fundamental and more like an integration of existing tools, whose own limitations (e.g., handling large view changes) are inherited by the proposed method.\n\n4. Unanalyzed Error Accumulation: The paper mentions the classic AR problem of error accumulation but doesn't analyze it. This is a key potential drawback compared to diffusion models that often generate an entire sequence in parallel. How does the image quality degrade as the camera path gets longer? A quantitative study on this (e.g., plotting PSNR vs. frame number) would be essential for a fair comparison, but it is missing.\n\n5. Unverified Claims of Camera Pose Accuracy: The paper claims (e.g., in the caption for Figure 4) that baseline methods suffer from \"incorrect camera pose alignment.\" However, it provides no quantitative or qualitative proof (like rendering a known 3D object or re-projecting points) to demonstrate that its own method accurately adheres to the provided camera trajectories. The visual results are not sufficient to verify this claim, especially when the camera autoencoder (Sec 3.2.2) is a core part of the method."}, "questions": {"value": "1. How do you interpret the significant gap in FID scores on the ACID dataset, despite the competitive LPIPS scores? Does this point to a lack of diversity or a consistent \"style\" bias introduced by the AR model or tokenizer?\n\n2. Regarding the camera autoencoder: Have you performed any ablation studies on its reconstruction quality? How much geometric precision is lost during this tokenization step, and how sensitive is the final output quality to this loss?\n\n3. You mention error accumulation as a limitation. Have you quantitatively measured this? For example, how does the quality (e.g., PSNR/LPIPS) of the 10th generated frame compare to the 1st generated frame in a sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKZ7420eE5", "forum": "ppXAVexrAM", "replyto": "ppXAVexrAM", "signatures": ["ICLR.cc/2026/Conference/Submission10821/Reviewer_r8gv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10821/Reviewer_r8gv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891146681, "cdate": 1761891146681, "tmdate": 1762922025651, "mdate": 1762922025651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARSS, a  framework that uses a GPT-style, decoder-only autoregressive model to generate novel views from a single image conditioned on a predefined camera trajectory. Unlike previous work, ARSS employs a video tokenizer to convert continuous image sequences into discrete tokens and a camera encoder to translate camera trajectories into 3D positional guidance. To improve generation quality while retaining the autoregressive structure, ARSS includes an autoregressive transformer module that randomly permutes the spatial order of tokens while preserving their temporal order. Extensive experiments show that ARSS performs on par with or better than state-of-the-art diffusion-based view-synthesis methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The camera autoencoder is novel: it encodes camera trajectories into tokens that can be easily integrated into transformer architectures.\n\n2. Video tokenization improves 3D consistency; visualizations show the method recovers higher-fidelity results than prior work.\n\n3. The approach achieves state-of-the-art results—especially compared with diffusion-based view-synthesis methods—in both qualitative and quantitative evaluations.\n\n4. The writing flows well and is reader-friendly."}, "weaknesses": {"value": "The paper is novel and effective; the following points are offered as constructive suggestions for improvement.\n\n1. Autoregressive framework: If I only want to generate a single novel view, does the model need to generate an entire camera trajectory to reach that view, or can it directly synthesize a single target image like LVSM? Please clarify the expected usage and efficiency for single-view generation.\n\n2. 3D consistency: Which component contributes most to 3D consistency — the autoregressive generation scheme or the video tokenization? Please clarify their individual roles and relative importance.\n\n3. Missing references: Please add comparisons or citations for recent related work, e.g., Rayzer (ICCV 2025), Cat4D (CVPR 2025), and Gen3C (CVPR 2025)."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dgsUUyiBK9", "forum": "ppXAVexrAM", "replyto": "ppXAVexrAM", "signatures": ["ICLR.cc/2026/Conference/Submission10821/Reviewer_5asS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10821/Reviewer_5asS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973895803, "cdate": 1761973895803, "tmdate": 1762922025091, "mdate": 1762922025091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an autoregressive framework (ARSS) for novel view synthesis: given a single input image and a specified camera trajectory, the model sequentially generates new views along that trajectory. They discretize images into visual tokens using a video tokenizer and introduce a camera trajectory encoder that maps the sequence of Plücker ray embeddings representing the trajectory into a set of latent camera tokens. These tokens provide 3D geometric guidance for the autoregressive decoder. The approach is compared primarily to diffusion-based view synthesis methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method achieves better or comparable results with the compared baselines\n- The proposed spatial permutation seems to improve the results compared to no or full permutation.\n- Authors provide ablation justifying their design choices."}, "weaknesses": {"value": "Over-generalization and inaccurate claims about related work and novelty:\n- In the abstract, the authors claim that diffusion models are limited for novel view synthesis (NVS) due to their “non-causal” nature. This statement is not well-supported and oversimplifies the role of causality in diffusion-based generation.\n- Throughout the paper, “autoregressive prediction” is used narrowly to refer only to the proposed decoder-only, next-token prediction scheme. The related work section overlooks a large body of causal “next-frame” or “next-view” approaches for video and NVS tasks (e.g., CausVid, ViewCrafter). This results in an incomplete and somewhat misleading positioning of the proposed method’s novelty.\n\t\nLimited qualitative evidence of temporal or view consistency:\n- It is difficult to assess the consistency of the proposed approach from the static figures provided. Including video or interactive visualizations (as supplementary materials) would substantially strengthen the evaluation of temporal and multi-view coherence.\n\nBaselines:\n-  As the method treats NVS as a next-view prediction task, it would have been helpful to include comparisons with video-generation models (e.g. CausVid, VideoCrafter 2) or NVS methods built on video-generation frameworks (e.g. ViewCrafter, SplatDiff, )"}, "questions": {"value": "- Could author provide more details on what per-frame mean in this sentence: \"Since Genwarp (Seo et al.,\n2024) and LVSM (Jin et al., 2024) are not designed for sequence generation, we adapt them by performing per-frame generation\". Is the generation of each frame still conditioned on previous frames?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fciCoydRsz", "forum": "ppXAVexrAM", "replyto": "ppXAVexrAM", "signatures": ["ICLR.cc/2026/Conference/Submission10821/Reviewer_G3wz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10821/Reviewer_G3wz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762353292321, "cdate": 1762353292321, "tmdate": 1762922024597, "mdate": 1762922024597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}