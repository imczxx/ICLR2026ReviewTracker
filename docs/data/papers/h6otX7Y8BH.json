{"id": "h6otX7Y8BH", "number": 23529, "cdate": 1758344995344, "mdate": 1759896810225, "content": {"title": "Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving", "abstract": "Large vision language models exhibit notable limitations on Geometry Problem Solving (GPS) because of their unreliable diagram interpretation and pure natural-language reasoning. A recent line of work mitigates this by using symbolic solvers: the model directly generates a formal program that a geometry solver can execute. However, this direct program generation lacks intermediate reasoning, making the decision process opaque and prone to errors. In this work, we explore a new approach that integrates Chain-of-Thought (CoT) with formal language. The model interleaves natural language reasoning with incremental emission of solver-executable code, producing a hybrid reasoning trace in which critical derivations are expressed in formal language. To teach this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly developed synthetic dataset with automatic formalization and interleaved formal-natural language reasoning, and (2) solver-in-the-loop reinforcement learning that jointly optimizes both the CoT narrative and the resulting program through outcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named GF-Reasoner, achieves up to 15% accuracy improvements on standard GPS benchmarks, surpassing both 7B-scale peers and the much larger model Qwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading symbolic computation to the solver, the generated reasoning traces are noticeably shorter and cleaner. Furthermore, we present a comprehensive analysis of method design choices (e.g., reasoning paradigms, data synthesis strategies, training methodologies, etc.), providing actionable insights for future research.", "tldr": "", "keywords": ["Geometry Problem Solving", "Multimodal LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80c2f93b5dada1bf75ee7f998e576284fadbf4d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the unreliable computation of LVLMs in Geometry Problem Solving (GPS), this paper proposes a \"hybrid reasoning\" framework. This framework combines natural language Chain-of-Thought (CoT) with executable formal code, trained in two stages (SFT+RL) on a new 11K-sample dataset. The resulting GF-Reasoner, based on Qwen2.5-VL-7B, outperforms the larger Qwen2.5-VL-72B and achieves more reliable and concise solution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe method achieves excellent performance and efficiency. The GF-Reasoner (7B) significantly outperforms the 10x larger Qwen2.5-VL-72B by +15.2% (PGPS9K) and +4.8% (UniGEO). This hybrid reasoning approach also reduces the \"computation error\" rate to nearly zero (0.3%) and substantially decreases the \"reasoning error\" rate from 23.0% to 14.3%.\n2.\tThe paper validates an effective \"SFT + solver-in-the-loop RL\" framework, with the RL stage alone improving Pass@1 accuracy by +26.6%. The detailed ablation studies offer valuable insights by analyzing the impact of data synthesis strategies, SFT training epochs, and CoT style."}, "weaknesses": {"value": "1.\tThe core method (CoT + external solver) is a well-established paradigm (e.g., AlphaGeometry, Toolformer, PoT, Logic-RL, ReTool, ToRL, Search-O1). The reliance on an existing solver (PGPSNet's 34 operations) shifts the contribution to data synthesis and an SFT+RL pipeline, rather than methodological innovation.\n2.\tThe framework is highly specialized, confined to a closed set of 34 predefined solver operations. This limits scalability (e.g., cannot \"add auxiliary lines\" per Section F) and lacks essential general tools (image processing, Python, web search, API). The absence of image tools might explain the higher visual perception error (8.0% vs 3.0%) reported in Table 2.\n3.\tThe comparisons are insufficient, omitting recent models (e.g., Qwen3-VL, GLM-4.5V, Gemini 2.5 Pro, GPT5) and relevant visual-driven methods (e.g., \"Thinking with Images\")."}, "questions": {"value": "1.\tGiven that the \"CoT + external solver\" and \"SFT + RL\" paradigms are well-established, what is the core methodological innovation of this paper?\n2.\tTable 2 shows your model's visual perception error is higher (8.0% vs 3.0%). Could this be due to lacking image processing tools (e.g., zoom/crop), and have you considered integrating them?\n3.\tWhat are the advantages of this method over recent visual planning approaches like \"Thinking with Images,\" and can you provide comparisons to justify the complex training pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E9rCDLFOAi", "forum": "h6otX7Y8BH", "replyto": "h6otX7Y8BH", "signatures": ["ICLR.cc/2026/Conference/Submission23529/Reviewer_8pkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23529/Reviewer_8pkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666389583, "cdate": 1761666389583, "tmdate": 1762942700192, "mdate": 1762942700192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a hybrid reasoning approach to solve geometry problems for LVLMs, via combining natural and formal languages. They also curate a dataset of 11K questions and used it in post-training SFT and RL for training purposes. Their model beats the SOTA on relevant benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a nice and well-written paper that tackles an interesting problem that LVLMs face.\n\n2. The hybrid reasoning approach presented in the paper is novel where it combines interpretability and precision of the two core components, i.e., natural language CoT and formal reasoning.\n\n3. Training with SFT and RL shows robust framework and performance boost. Their 7B model outperforms the peers as well as larger model of 72B.\n\n4. The paper contributed to the field by curating a synthetic dataset of 11K questions.\n\n5. Authors conducted extensive experiments including ablation studies."}, "weaknesses": {"value": "1. One major concern that I have is that there are only 34 operators which means if a problem needs a geometry theorem, it is not included in the set.\n\n2. Dataset was essentially distilled from large and strong teacher models, hence model performance is heavily dependent on these models.\n\n3. RM is quite simple (binary) and the model doesn't take partial credit for correct steps rather than final answer."}, "questions": {"value": "1. OOD theorems: what happens if the model encounters a problem that isn't part of the 34 operators? Is the model able to learn how to derive theorems that are not included in the set?\n\n2.  I wonder what happens if the RM generates continuous reward rather than a simple binary where the model can be rewarded partially for a correct step?\n\n3. Is there a mechanism to check if the performance boost is due to the model being distilled from powerful teacher models or it's a natural result of an RL step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mgkFrfYSf4", "forum": "h6otX7Y8BH", "replyto": "h6otX7Y8BH", "signatures": ["ICLR.cc/2026/Conference/Submission23529/Reviewer_2ABk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23529/Reviewer_2ABk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809188401, "cdate": 1761809188401, "tmdate": 1762942699903, "mdate": 1762942699903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GF-Reasoner, a fine-tuned large vision-language model (LVLM) built on Qwen2.5-VL 7B that combines formal language programs with natural language chain-of-thought (CoT) for geometry problem solving. The model alternates between descriptive reasoning and executable formal code. The authors construct a new 11k-sample synthetic dataset with interleaved formal-natural reasoning and employ supervised fine-tuning (SFT) on the synthetic dataset and solver-integrated reinforcement learning (RL), where correctness feedback comes from a geometric solver. GF-Reasoner achieves up to 15% higher accuracy compared to both specialist geometry solvers and much larger LVLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear motivation:** The paper addresses geometry problem solving, which is difficult for the current LVLMS.\n- **Rigorous ablations:** The study isolates the effects of the added elements such as the interleaved CoT, synthesis strategies, and RL. This helps the readers understand the contribution of each elements to the performance gain."}, "weaknesses": {"value": "- **Questionable necessity of the proposed approach**\n    - The paper does not demonstrate that the hybrid approach is fundamentally better than just improving pure natural language reasoning (SFT + RL only on natural language reasoning).\n    - To add, Interleaving CoT is not a novel idea. This is similar to how the ReAct framework works.\n- **The performance comparisons are not particularly fair**\n    - The proposed model has access to formal language specs, while the other baseline models do not have access to those. Thus, it is difficult to completely isolate the factors that lead to the improvement. The simpler conclusion may be that the comparisons measure performance given access to tools, instead of the “bridging of formal language and CoT”.\n- **The analyses lack explanations**\n    - How did the authors classify the error types presented in Table 2 and what are their definitions? Is this an exhaustive list of errors? Who annotated these errors and are there inter-annotator agreement statistics?\n    - Did the authors evaluate the checkpoints after the SFT epochs on the benchmarks beyond the PGPS9K? It is interesting to note that even in the PGPS9K, the 2 epochs SFT initialization achieves higher Accuracy than the others (Figure 6a, left-most points), potentially suggesting overfitting."}, "questions": {"value": "See Weaknesses.\n\nNote that I am not an expert in this research area. I carefully checked the paper for the details; however, there may be prior studies that I am not familiar with which may provide more context to the general contributions of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CEWTZsvhKh", "forum": "h6otX7Y8BH", "replyto": "h6otX7Y8BH", "signatures": ["ICLR.cc/2026/Conference/Submission23529/Reviewer_VSGS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23529/Reviewer_VSGS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866058527, "cdate": 1761866058527, "tmdate": 1762942699618, "mdate": 1762942699618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a post-training strategy to teach Large Vision Language Models (LVLMs) to perform interleaved natural-language reasoning and formal reasoning, benifiting from the flexibility of natural language and the precision of formal language.\nTo do the post-training, authors synthesize an 11K-sample interleaved formal-natural language CoT dataset to conduct fine-tuning, and develop a solver-in-the-loop RL framework to further improve the performance.\nSpecifically, the interleaved formal-natural language CoT requires LVLMs first generate natural language reasoning and then generate symbolic programs step-by-step accordingly.\nAuthors claim that such interleaved and step-by-step program generation is better than direct generating the whole program."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Text, figures, tables are all good presented.\n2. Experiments with various types of baselines and datasets well justify the performance of the proposed framework compared with existing methods."}, "weaknesses": {"value": "1. Appendix B Related Work is not cited in the main text, which is an important part to understand the contribution.\n2. The so-called \"interleaved formal-natural language CoT\" is suspicious. Figure 1 clearly shows that the interleaved CoT involves interleaved natural-language reasoning statement and formal reasoning statement. However, in Appendix D, the so-called interleave CoT is more like a three-stage reasoning process, i.e., step-by-step natural-language reasoning, generating formal statements step-by-step according to the natural-language reasoning steps, and aggregating the formal statements to form a program. This raises the concerns that\n    - The definition of \"interleaved formal-natural language CoT\" is not consistent across the whole paper.\n    - The key innovation is adding a step-by-step formal statement generation between vanilla CoT and program generation, which seems not as interesting as Introduction claims.\n3. Since the motivation of generating interleaved formal-natural language CoT is that directly generating the program is prone to errors, a comparison between the current framework with the one without interleaved formal-natural language CoT (keep fine-tuning, RL, and any other component in the current framework) is needed."}, "questions": {"value": "1. How about the performance without fine-tuning? Can the proposed solver-in-the-loop RL framework work without fine-tuning?\n2. Why the proposed reasoning process is called interleaved formal-natural language reasoning but the proposed dataset is called formal-interleaved CoT dataset (without *natural*) ?\n\n\nTypo:\n1. Line 228-229: $\\hat{r}$ should be $\\hat{z}$\n2. Line 236-237: $z$ should be $x$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YJJWYqMlrP", "forum": "h6otX7Y8BH", "replyto": "h6otX7Y8BH", "signatures": ["ICLR.cc/2026/Conference/Submission23529/Reviewer_m5o1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23529/Reviewer_m5o1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762767543762, "cdate": 1762767543762, "tmdate": 1762942699404, "mdate": 1762942699404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}