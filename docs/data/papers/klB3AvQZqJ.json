{"id": "klB3AvQZqJ", "number": 22357, "cdate": 1758329995072, "mdate": 1759896870482, "content": {"title": "Constraint-Aware Reward Relabeling for Offline Safe Reinforcement Learning", "abstract": "Offline safe reinforcement learning (OSRL) considers the problem of learning reward-maximizing policies for a pre-defined cost constraint from a fixed dataset. This paper proposes a simple and effective approach referred to as Constraint-aware Reward (Re)Labeling (CARL), that can be wrapped around existing offline RL algorithms. CARL is an iterative approach that alternates between two steps for each sampled batch of data to ensure state-action-wise safety constraints. First, update cost evaluation function using an off-policy evaluation procedure. Second, update policy using relabeled rewards (assign large penalty) for state-action pairs which are detected unsafe based on cost estimates. CARL is a minimalist approach, doesn’t introduce any additional hyperparameters, and allows us to leverage strong off-the-shelf offline RL algorithms. Experimental results on the DSRL benchmark tasks demonstrate that CARL reliably enforces safety constraints under small cost budgets, while achieving high rewards.", "tldr": "We introduce a reward relabeling method that enables high-return, constraint-satisfying policies in offline safe RL.", "keywords": ["Offline Safe Reinforcement Learning", "Safe Reinforcement Learning", "Offline Reinforcement Learning", "Reward Shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eff7bc88a76f482d7dc7d5a3932f8dcfc8f6d9c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CARL (Constraint-aware Reward Labeling), a simple offline safe RL method that enforces safety by relabeling rewards with large penalties for unsafe state-action pairs identified through cost estimation. Experiments on DSRL tasks show that it achieves better reward performance while satisfying safety constraints under small cost budgets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple and can be easily integrated into existing offline RL algorithms without introducing new hyperparameters or major architectural changes."}, "weaknesses": {"value": "The proposed method lacks theoretical convergence analysis and does not provide any formal guarantee of safety during test-time deployment."}, "questions": {"value": "1. In Eq. (2), although the constraint is defined state-wise, it remains an expectation-based formulation, effectively a soft constraint since the Q-function represents the expected cumulative cost. In contrast, FISOR formulates offline safe RL with Hamilton–Jacobi reachability constraints that treat safety violations as hard constraints. Why is the formulation in Eq. (2) better at ensuring safety than the hard-constraint formulation?\n\n2. When relabeling the reward, $V_{max}$ represents the maximum possible infinite-horizon value. Since only an offline dataset is available, $V_{max}$ is approximated as the maximum within the dataset, which can introduce bias. How does this bias affect policy learning and safety performance, and how do the authors mitigate or correct for this bias during training?\n\n3. Regarding the results on varying cost limits, can the proposed method generalize to new cost thresholds without retraining, as methods like CAPS and CCAC can? If retraining is required for each threshold, how do the authors ensure a fair comparison? According to Figure 2, while it is reasonable that the normalized reward increases as the cost budget increases, why does the normalized cost decrease?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mnPTRdHgIa", "forum": "klB3AvQZqJ", "replyto": "klB3AvQZqJ", "signatures": ["ICLR.cc/2026/Conference/Submission22357/Reviewer_tdbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22357/Reviewer_tdbs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758937563, "cdate": 1761758937563, "tmdate": 1762942183719, "mdate": 1762942183719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a minimal approach, named Constraint aware Reward (Re)Labeling (CARL), to translate offline safe RL to offline RL via reward shielding (relabeling). \n\nCARL contains the following content:\n\n1. **Iterative Policy Improvement**\n   1. offline policy evaluation $\\to$ reward relabeling $\\to$ offline policy optimization.\n2. **Batch Reward Relabeling**\n   1. Relabel the reward, which violates the constraint to a large negative value."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper has done comprehensive experiments on many benchmarks compared with latest baselines. \n\nThis paper writes a detailed related work on offline safe RL.\n\nThis paper translates the constrained optimization problem into a shielding optimization problem, where the reward relabeling can be regarded as an improved shielding method in Safe RL."}, "weaknesses": {"value": "### The key idea of this paper shares a very similar essence to CPQ [1]. However, the main difference between CPQ and CARL, and the improvement/superiority of CARL over CPQ are not explained clearly. The contribution may be limited before clarifying the following issue: \n\n1. Both CPQ and CARL share the same **Sketch of an Iterative Policy Improvement Algorithm** in Eq. (4) of this paper.\n   1. In CPQ, it follows: offline policy evaluation $\\to$ reward value relabeling $\\to$ offline policy optimization.\n   2. That is to say, CPQ already utilizes this approach in 2022.\n\n2. **The only difference is that CPQ relabels the value to 0, while CARL relabels the reward to a large negative value.**\n   1. CPQ utilizes value function relabeling:\n      1. $Q_r=1_{\\{Q_c\\leq k\\}}\\cdot Q_r'$\n      2. This is to say that CPQ relabels the unsafe condition as 0.\n   2. While CARL utilizes reward relabeling in Eq. (3) and (5).\n      1. $r_\\pi(s,a)=1_{\\{Q_c\\leq k\\}}\\cdot r(s,a)-1_{\\{Q_c> k\\}}\\cdot V_{max}$\n      2. This is to say that CARL relabels the unsafe condition as $-V_{max}$.\n   3. Consider the value function is a cumulation of rewards; there seems to be no large difference between relabeling rewards and relabeling the value function directly.\n      1. Originally, the reward value follows: $Q_r = r+\\gamma Q_r'$\n      2. In CPQ, the reward value follows: $Q_r = r+\\gamma \\cdot 1_{\\{Q_c\\leq k\\}}\\cdot Q_r'$, which is\n         1. $Q_r = r+\\gamma \\cdot 1\\cdot Q_r'$ for $Q_c\\leq k$\n         2. $Q_r = r+\\gamma \\cdot 0\\cdot  Q_r'$ for $Q_c> k$\n      3. In CARL, the reward value follows: \n         1. $Q_r = r\\cdot 1+\\gamma \\cdot Q_r'$ for $Q_c\\leq k$\n         2. $Q_r = -V_{max}+\\gamma \\cdot  Q_r'$ for $Q_c> k$\n      4. Considering that the rewards are usually positive values (Even if they are not, the rewards can be regularized to be positive without loss of generality\n         1. **Both CPQ and CARL aim to set the reward value that violates the constraint to a small value.**\n         2. Is there an essential difference between relabeling to 0 or $-V_{max}$?\n         3. Is there an essential difference between relabeling the reward signal or the reward value function?\n   4. **This difference is hard to be regarded as the central contribution of a paper if it is not explained or compared clearly.**\n3. CPQ naturally follows $K=M=1$, where the OPE and OPO are jointly optimized.\n   1. What is the necessity of discussing the content of this part if this finding is already utilized in previous work?\n   2. CPQ also suffers from the oscillation in Figure 1 even if $K=M=1$. Is there any explanation about the problem? Or are $K$ and $M$ really the main reason for this problem?\n4. CPQ shares all **Summary of CARL’s advantages** in line 309, page 6, as they share a similar essence:\n   1. CPQ can also be wrapped around existing offline RL algorithms.\n   2. CPQ also doesn’t introduce any additional hyperparameters. (The additional hyperparameters in CPQ are related to another OOD action problem, which is not considered in CARL.)\n   3. The main reason is that the only (or main) difference between CPQ and CARL is the above relabeling methods.  \n\n5. Besides, CPQ additionally addresses the OOD action problem. However, the performance of CARL is much better to CPQ.\n   1. Why does CARL not consider the OOD action problem, considering that the Bellman backup procedure is also utilized in CARL?\n\n   2. Why CARL works much better than CPQ? \n\n6. It is very important to clarify that from Section 4 to 5,\n   1. which content is already proposed in previous work (like CPQ)? \n   2. which content is completely innovative part of CARL? \n   3. why and how do the innovative parts improve CARL against CPQ?\n   4. where does the performance gain come from?\n\n\n\n### Although there is no doubt that TD3BC can be applied to CARL, there are still problems about how to apply IQL to CARL, which is glossed over in this paper. \n\n1. TD3BC has explicit policy during the training procedure. Thus, CARL can estimate $Q^\\pi_r$ and $Q^\\pi_c$ under the same policy. \n   1. The OOD problem can be mitigate by BC but can not be avoided as long as the Bellman backup procedure is updated with explicit policy.\n2. To avoid the OOD problem completely, IQL propose to update the $Q^\\pi_r$ implicitly via expectile regression, where the policy is implicitly hidden in the expectile regression to avoid explicitly sampling action during Bellman backup procedure.\n3. Considering this implicit design, it is impossible to estimate $Q^\\pi_r$ and $Q^\\pi_c$ under the same policy without extracting the policy out.\n   1. Think about why IQL regards \"value function update\" and \"policy extraction\" as two separate procedure.\n      1. To avoid the OOD problem, the value function update cannot utilize the extracted policy.\n4. This is why C2IQL [2] is proposed to to apply IQL's idea in Offline Safe RL to address this problem.\n   1. C2IQL tries to combine CPQ and IQL without breaking the implicit property to avoid OOD problem.\n   2. Since CPQ is similar to CARL, C2IQL is also similar to IQL+CARL, while C2IQL additionally points out and addresses this problem\n   3. To understand the above content, please understand the idea of IQL, IDQL, C2IQL instead of merely treating them as an algorithm.\n5. If this paper utilize the extracted policy to estimate the $Q^\\pi_c$, it has no difference between TD3BC and IQL when applied to CARL. \n   1. Extracted policy breaks IQL's key idea. When this policy is utilized to influence value function update, IQL has no difference from other actor-critic method.\n   2. Thus the backbone analysis in line 369, page 7 seems to be not very meaningful.\n6. If this paper utilize framework that implicitly keeping both functions under the same policy like C2IQL, please explain the design in detail with proper citation.\n7. Considering the similarity between C2IQL (CPQ+IQL) and CARL+IQL, please verify from section 4 and 5 again:\n   1. which content is already proposed in previous work; \n   2. which content is completely innovative part of CARL; \n   3. why and how does the innovative parts improve CARL previous work;\n   4. where does the performance gain come from;\n\n[1] Xu, H., Zhan, X., & Zhu, X. (2022, June). Constraints penalized q-learning for safe offline reinforcement learning. In *Proceedings of the AAAI Conference on Artificial Intelligence* (Vol. 36, No. 8, pp. 8753-8760).\n\n[2] Zifan, L. I. U., Li, X., & Zhang, J. C2IQL: Constraint-Conditioned Implicit Q-learning for Safe Offline Reinforcement Learning. In *Forty-second International Conference on Machine Learning*.\n\n**We highly hope that this paper can clarify these issues, as they may potentially lead to academic plagiarism in severe cases.**"}, "questions": {"value": "Please see the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kWrPcFFBdz", "forum": "klB3AvQZqJ", "replyto": "klB3AvQZqJ", "signatures": ["ICLR.cc/2026/Conference/Submission22357/Reviewer_EjRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22357/Reviewer_EjRo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816859346, "cdate": 1761816859346, "tmdate": 1762942183518, "mdate": 1762942183518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Constraint-Aware Reward Relabeling (CARL), a straightforward and hyperparameter-free approach to offline safe reinforcement learning (OSRL). CARL enforces safety constraints without using Lagrange multipliers or dual optimization by alternating between cost estimation and policy improvement. During training, rewards for actions predicted to be unsafe are replaced with strong negative penalties, effectively turning the constrained objective into an unconstrained one. The method serves as a lightweight wrapper that can be integrated with existing offline RL algorithms such as TD3-BC and IQL, enabling the learning of safe yet high-performing policies even from datasets containing many unsafe samples. Experiments on the DSRL benchmark show that CARL achieves consistently strong results, meeting cost constraints while maintaining high returns across various tasks and cost settings, highlighting its stability, generality, and practical utility for safety-critical offline RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- CARL provides a clean and practical approach to offline safe RL, removing the need for complex constrained optimization yet achieving strong safety and reward trade-offs. The method’s simplicity and consistent empirical gains make it a notable contribution. \n- Because CARL operates solely through reward relabeling at the data-processing stage, it can be seamlessly paired with diverse offline RL methods, making it a flexible and practical choice for safety-critical deployment.\n- The paper offers a clear theoretical contribution by showing that optimizing a relabeled reward function with large penalties for unsafe actions leads to policies that satisfy pointwise safety constraints, and is formally equivalent to the original constrained CMDP formulation under mild assumptions.\n- The paper is well written, with clear organization and logical flow throughout. In particular, the results are effectively visualized—figures and tables are well designed, easy to interpret, and enhance the overall clarity and impact of the presentation."}, "weaknesses": {"value": "- In the CARL framework, performance is highly dependent on the accuracy of the cost evaluation function. Inaccurate cost-to-go estimates can result in incorrect reward relabeling, which introduces significant uncertainty and may compromise both safety and reward performance. However, the paper does not include any sensitivity analysis to assess how estimation errors in the cost value impact the overall behavior of the learned policy, leaving an important aspect of robustness unaddressed.\n- Although the paper reformulates the OSRL objective as an unconstrained optimization problem, it lacks a theoretical convergence analysis of the proposed iterative procedure, especially in the context of value function approximation. Providing such an analysis, or clearly outlining conditions under which convergence is guaranteed, would significantly improve the theoretical soundness and credibility of the method.\n- This paper claims that CARL can combine with any other offline RL algorithm framework, but only two frameworks were tested — both are value-based, not generative or transformer-based (e.g., CQL, Diffuser, Decision Transformer) [1-3].\n- This paper lacks comparisons with several recent and strong baselines, which makes it difficult to fully assess the claimed performance improvements and the novelty of the proposed method relative to the current state of the art [4-6].\n- While CARL is designed as a minimalist wrapper, it introduces an additional value function training phase for cost estimation in each iteration. This added computational step—particularly when used with deep offline RL backbones—may impact training time and resource usage. However, the paper does not provide any analysis or empirical comparison of runtime efficiency, training wall-clock time, or computational complexity relative to baseline methods [7].\n- As shown in Table 1, CARL occasionally fails to satisfy the cost constraint in certain tasks, and in some settings, it does not achieve the highest reward compared to other baselines. The paper would benefit from a deeper analysis of these cases to understand the conditions under which CARL underperforms, and to clarify the trade-offs between safety and performance [8]."}, "questions": {"value": "- How sensitive is CARL’s performance to errors in the cost-to-go estimator Qc?\n- Are there theoretical guarantees that CARL will respect constraints under bounded Qc​ estimation error?\n- Could you evaluate CARL’s compatibility with non–value-based offline RL frameworks such as CQL, AWAC, or Decision Transformer?\n- Could you provide a performance comparison between CARL and additional recent baselines, including those in references [4–6]?\n\n## Reference\n\n**[1]** Janner, Michael, et al. \"Planning with diffusion for flexible behavior synthesis.\" arXiv preprint arXiv:2205.09991 (2022).\n\n**[2]** Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097.\n\n**[3]** Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in neural information processing systems 33 (2020): 1179-1191.\n\n**[4]** Wang, Ruhan, and Dongruo Zhou. \"Safe Decision Transformer with Learning-based Constraints.\" 7th Annual Learning for Dynamics\\& Control Conference. PMLR, 2025.\n\n**[5]** Guan, Jiayi, et al. \"Voce: Variational optimization with conservative estimation for offline safe reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 33758-33780.\n\n**[6]** Wei, Honghao, et al. \"Adversarially trained weighted actor-critic for safe offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 52806-52835.\n\n**[7]** Chemingui, Yassine, et al. \"Constraint-adaptive policy switching for offline safe reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 15. 2025.\n\n**[8]** Gong, Ze, Akshat Kumar, and Pradeep Varakantham. \"Offline safe reinforcement learning using trajectory classification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 16. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oJWT1SEYG6", "forum": "klB3AvQZqJ", "replyto": "klB3AvQZqJ", "signatures": ["ICLR.cc/2026/Conference/Submission22357/Reviewer_chYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22357/Reviewer_chYS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980942881, "cdate": 1761980942881, "tmdate": 1762942183300, "mdate": 1762942183300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Constraint-Aware Reward Relabeling (CARL), a straightforward and hyperparameter-free approach to offline safe reinforcement learning (OSRL). CARL enforces safety constraints without using Lagrange multipliers or dual optimization by alternating between cost estimation and policy improvement. During training, rewards for actions predicted to be unsafe are replaced with strong negative penalties, effectively turning the constrained objective into an unconstrained one. The method serves as a lightweight wrapper that can be integrated with existing offline RL algorithms such as TD3-BC and IQL, enabling the learning of safe yet high-performing policies even from datasets containing many unsafe samples. Experiments on the DSRL benchmark show that CARL achieves consistently strong results, meeting cost constraints while maintaining high returns across various tasks and cost settings, highlighting its stability, generality, and practical utility for safety-critical offline RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- CARL provides a clean and practical approach to offline safe RL, removing the need for complex constrained optimization yet achieving strong safety and reward trade-offs. The method’s simplicity and consistent empirical gains make it a notable contribution. \n- Because CARL operates solely through reward relabeling at the data-processing stage, it can be seamlessly paired with diverse offline RL methods, making it a flexible and practical choice for safety-critical deployment.\n- The paper offers a clear theoretical contribution by showing that optimizing a relabeled reward function with large penalties for unsafe actions leads to policies that satisfy pointwise safety constraints, and is formally equivalent to the original constrained CMDP formulation under mild assumptions.\n- The paper is well written, with clear organization and logical flow throughout. In particular, the results are effectively visualized—figures and tables are well designed, easy to interpret, and enhance the overall clarity and impact of the presentation."}, "weaknesses": {"value": "- In the CARL framework, performance is highly dependent on the accuracy of the cost evaluation function. Inaccurate cost-to-go estimates can result in incorrect reward relabeling, which introduces significant uncertainty and may compromise both safety and reward performance. However, the paper does not include any sensitivity analysis to assess how estimation errors in the cost value impact the overall behavior of the learned policy, leaving an important aspect of robustness unaddressed.\n- Although the paper reformulates the OSRL objective as an unconstrained optimization problem, it lacks a theoretical convergence analysis of the proposed iterative procedure, especially in the context of value function approximation. Providing such an analysis, or clearly outlining conditions under which convergence is guaranteed, would significantly improve the theoretical soundness and credibility of the method.\n- This paper claims that CARL can combine with any other offline RL algorithm framework, but only two frameworks were tested — both are value-based, not generative or transformer-based (e.g., CQL, Diffuser, Decision Transformer) [1-3].\n- This paper lacks comparisons with several recent and strong baselines, which makes it difficult to fully assess the claimed performance improvements and the novelty of the proposed method relative to the current state of the art [4-6].\n- While CARL is designed as a minimalist wrapper, it introduces an additional value function training phase for cost estimation in each iteration. This added computational step—particularly when used with deep offline RL backbones—may impact training time and resource usage. However, the paper does not provide any analysis or empirical comparison of runtime efficiency, training wall-clock time, or computational complexity relative to baseline methods [7].\n- As shown in Table 1, CARL occasionally fails to satisfy the cost constraint in certain tasks, and in some settings, it does not achieve the highest reward compared to other baselines. The paper would benefit from a deeper analysis of these cases to understand the conditions under which CARL underperforms, and to clarify the trade-offs between safety and performance [8].\n\n***Disclose the use of LLMs:*** *I used LLM for light writing assistance, including grammar refinement and phrasing suggestions. All substantive assessments, judgments, and technical evaluations in this review are my own. If the authors resolve the concerns and questions I raised, I would be willing to improve my score.*"}, "questions": {"value": "- How sensitive is CARL’s performance to errors in the cost-to-go estimator Qc?\n- Are there theoretical guarantees that CARL will respect constraints under bounded Qc​ estimation error?\n- Could you evaluate CARL’s compatibility with non–value-based offline RL frameworks such as CQL, AWAC, or Decision Transformer?\n- Could you provide a performance comparison between CARL and additional recent baselines, including those in references [4–6]?\n\n## Reference\n\n**[1]** Janner, Michael, et al. \"Planning with diffusion for flexible behavior synthesis.\" arXiv preprint arXiv:2205.09991 (2022).\n\n**[2]** Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097.\n\n**[3]** Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in neural information processing systems 33 (2020): 1179-1191.\n\n**[4]** Wang, Ruhan, and Dongruo Zhou. \"Safe Decision Transformer with Learning-based Constraints.\" 7th Annual Learning for Dynamics\\& Control Conference. PMLR, 2025.\n\n**[5]** Guan, Jiayi, et al. \"Voce: Variational optimization with conservative estimation for offline safe reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 33758-33780.\n\n**[6]** Wei, Honghao, et al. \"Adversarially trained weighted actor-critic for safe offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 52806-52835.\n\n**[7]** Chemingui, Yassine, et al. \"Constraint-adaptive policy switching for offline safe reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 15. 2025.\n\n**[8]** Gong, Ze, Akshat Kumar, and Pradeep Varakantham. \"Offline safe reinforcement learning using trajectory classification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 16. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oJWT1SEYG6", "forum": "klB3AvQZqJ", "replyto": "klB3AvQZqJ", "signatures": ["ICLR.cc/2026/Conference/Submission22357/Reviewer_chYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22357/Reviewer_chYS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980942881, "cdate": 1761980942881, "tmdate": 1763664443088, "mdate": 1763664443088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Constraint-aware Reward (Re)Labeling (CARL)** for offline safe RL. It learns a cost-to-go critic and relabels rewards in each mini-batch: if $Q_c^\\pi(s,a)>\\kappa$, assign a large negative penalty; otherwise keep the original reward. An off-the-shelf offline RL backbone (e.g., TD3-BC, IQL) is then trained on the relabeled data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Practical motivation:** avoids brittle dual updates seen in some Lagrangian methods.\n* **Intuitive method:** convert safety constraints into state-action pair penalties rather than tuning Lagrange multipliers.\n* **Simple backbone-agnostic wrapper:** CARL can be wrapped around standard offline RL backbones."}, "weaknesses": {"value": "## Positioning & Objectives\n* **Tight-budget regime but CMDP formulation:** The paper targets small cost limit $\\kappa$ and *pointwise safety*, which is closer to *hard-constraint / shielded* or *risk-sensitive* formulations. However, problem setup is based on CMDP and baselines are CMDP-style OSRL only.\n* **Theory-metric mismatch:** Theory enforces *statewise* safety; but evaluation reports *episodic normalized cost*. If pointwise safety is the goal, episodic evaluation metric may be a mismatch.\n* **Feasibility under offline coverage:** Pointwise constraints may be *infeasible* for a given $\\kappa$ with partial data coverage while the expectation constraints are *feasible*. Current submission does not discuss this scenario in detail.\n\n## Method & Claims\n* **Theorem 1 conditions (notation/assumption):** The proof sketch relies on a sufficiently punitive $-V_{\\max}$, feasibility of the pointwise-constrained problem, and bounded nonnegative (or shifted-to-nonnegative) rewards. Note that nonnegative reward is not specified in problem setup, but it is required for $V_r^{\\tilde{\\pi}*}(s) > 0$ used in the proof of Theorem 1. In addition, the main experiments use $-R_{\\max}$, not $-V_{\\max}$, weakening the applicability of Theorem 1.\n* **CARL stability vs Lagrangian:** CARL replaces dual updates with reward relabeling using Q-estimates. However, this introduces non-stationarity between the policy, fitted Q evaluation (FQE), and relabeling; and inherits FQE’s estimation noise. With the absence of convergence analysis, it’s unclear that CARL is less brittle than Lagrangian methods. Rather, it may swap dual-instability for OPE-instability. \n* **OPE dependence:** Unsafe detection hinges on FQE accuracy under distribution shift; miscalibration can over- or under-penalize. \n* **“Neighborhood suppression” claim:** With function approximation, penalizing one transition can generalize unpredictably beyond a \"local neighborhood\".\n* **\"No extra hyperparameters\" claim:** The value of *penalty magnitude* has to be determined. The paper uses ($R_{\\max}$ vs $V_{\\max}$) for experiments. However, this value is offline dataset dependent and there may not be a sufficiently good estimate at deployment time. In addition, this value varies significantly across tasks.\n\n## Evaluation\n* **Baselines:** For tight budget setting, **hard-constraint/shielded** and **risk-sensitive** baselines may be more appropriate for small $\\kappa$.\n* **Budgets & metrics:** If the aim is strict/tight safety, violation rate and/or per-state safety metrics should be reported alongside episodic cost.\n* **Statistical confidence:** Evaluation results average over 20 episodes and 3 random seeds. This seems too few to establish statistical confidence. \n\n## Reproducibility\n* The anonymous code link given in the paper is broken. No code is accessible, the link returns \"The requested file is not found.\" \n\n## Minor Typo\n* **Line 088:** “ation-value”."}, "questions": {"value": "## Theory & Guarantees\n1. The paper uses $-R_{\\max}$ as the penalty magnitude; what guarantee remains in practice, since Theorem 1 depends on $-V_{\\max}$?\n\n## Method & Stability\n2. How is the **penalty scale** chosen at deployment (task-agnostic guidance)? \n3. How much noise do you witness in the OPE relative to threshold $\\kappa$? How often are actions misclassified as unsafe/safe?\n4. Does CARL update exhibit contraction/monotone improvement? My assessment is that the paper should temper the claim that CARL alleviates Lagrangian brittleness. It is not a strictly better stability trade-off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3DHk0fBkS7", "forum": "klB3AvQZqJ", "replyto": "klB3AvQZqJ", "signatures": ["ICLR.cc/2026/Conference/Submission22357/Reviewer_LTcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22357/Reviewer_LTcd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162078409, "cdate": 1762162078409, "tmdate": 1762942182871, "mdate": 1762942182871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}