{"id": "kxVjQhkAWz", "number": 2862, "cdate": 1757292147154, "mdate": 1763552532621, "content": {"title": "Dens3R: A Foundation Model for 3D Geometry Prediction", "abstract": "Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various tasks and highlight its potential for broader applications.", "tldr": "", "keywords": ["Visual Foundation Model", "3D Geometry Prediction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/030287efd189edfdcc9524811fbe860f648cfc08.pdf", "supplementary_material": "/attachment/2baf1f2720f934c73c4c781cf7092e256e6c8d59.zip"}, "replies": [{"content": {"summary": {"value": "Dens3R is a visual foundation model for dense 3D geometry prediction from unposed images. It jointly regresses consistent pointmaps, depth, and surface normals using a novel two-stage training framework. This approach builds an intrinsic-invariant pointmap by incorporating normals, ensuring high-quality, unified geometric perception across various tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The model demonstrates SOTA performance across diverse benchmarks, including both indoor and outdoor scenes. Compelling visualizations showcase its superior accuracy and detail in depth and normal prediction, validating its effectiveness and robustness as a powerful geometry perception tool that consistently outperforms specialized methods.\n\n2. A key strength is the well-motivated approach of jointly optimizing inherently correlated geometric quantities like depths and normals. Instead of predicting them in isolation, this unified framework explicitly models their structural coupling, ensuring geometric consistency.\n\n3. The paper introduces an innovative two-stage training strategy. By leveraging surface normals—an intrinsic property—in the second stage, the model learns a representation robust to camera parameters and scale. This elegantly resolves monocular ambiguity and significantly boosts overall prediction accuracy and stability.\n\n4. The paper is backed by comprehensive experiments, including thorough ablation studies that validate each key component. A particularly impressive highlight is its transferability to downstream tasks. The excellent performance on semantic segmentation (Fig. 8c) with a frozen backbone powerfully substantiates its claim as a versatile foundation model."}, "weaknesses": {"value": "The network structure resembles DUST3R and may have sub-optimal performance when reconstructing long sequence inputs."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3KOyicCrDp", "forum": "kxVjQhkAWz", "replyto": "kxVjQhkAWz", "signatures": ["ICLR.cc/2026/Conference/Submission2862/Reviewer_b7qA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2862/Reviewer_b7qA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812083251, "cdate": 1761812083251, "tmdate": 1762916417994, "mdate": 1762916417994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their time and constructive feedbacks.\n\n Following the suggestions, we have refined several sentences throughout the paper to improve clarity with **all changes highlighted in red**, and we will continue to improve the paper. Below we summarize the main experimental additions and revisions:\n\n1. Added a quantitative comparison with DUSt3R and MASt3R on normal estimation to provide a reference against multi-view models in **Table 6**. \n2. Included a qualitative comparison between the Stage 1 and Stage 2 point map to demonstrate the effectiveness of Stage 2 in **Figure 11** and **Table 6**.\n3. Added a qualitative comparison between DUSt3R normals and our predicted normals in **Figure 14**.\n4. Provided visualizations of DUSt3R with position-interpolated RoPE in **Figure 22**.\n\nWe are more than willing to provide further clarifications or additional analyses if the reviewers have any remaining questions. \n\nWe also provide below the numerical values of the additional experiments associated with Table 6 for ease of reference:\n\n### NYUv2 (indoor)\n\n| Method       | Mean ↓ | Med ↓ | δ11.25° ↑ | δ22.5° ↑ | δ30° ↑ |\n|-------------|--------|-------|-----------|----------|--------|\n| DUSt3R      | 18.5   | 9.5   | 55.2      | 74.6     | 81.2   |\n| MASt3R      | 25.2   | 14.9  | 40.6      | 63.3     | 71.7   |\n| Ours-Stage1 | 17.8   | 11.1  | 50.6      | 75.4     | 82.8   |\n| **Ours**    | **16.1** | **7.4** | **62.5** | **78.8** | **84.0** |\n\n---\n\n### ScanNet (indoor)\n\n| Method       | Mean ↓ | Med ↓ | δ11.25° ↑ | δ22.5° ↑ | δ30° ↑ |\n|-------------|--------|-------|-----------|----------|--------|\n| DUSt3R      | 19.4   | 8.9   | 57.0      | 73.8     | 79.6   |\n| MASt3R      | 28.1   | 16.6  | 37.6      | 59.2     | 67.7   |\n| Ours-Stage1 | 18.6   | 11.4  | 49.4      | 75.1     | 81.8   |\n| **Ours**    | **16.9** | **7.1** | **64.0** | **78.1** | **82.7** |\n\n---\n\n### IBims-1 (indoor)\n\n| Method       | Mean ↓ | Med ↓ | δ11.25° ↑ | δ22.5° ↑ | δ30° ↑ |\n|-------------|--------|-------|-----------|----------|--------|\n| DUSt3R      | 21.9   | 8.2   | 57.9      | 71.7     | 76.7   |\n| MASt3R      | 29.8   | 16.6  | 39.4      | 58.9     | 66.4   |\n| Ours-Stage1 | 20.2   | 9.3   | 56.8      | 73.2     | 78.3   |\n| **Ours**    | **16.0** | **4.3** | **72.2** | **80.1** | **83.0** |\n\n---\n\n### Sintel (outdoor)\n\n| Method       | Mean ↓ | Med ↓ | δ11.25° ↑ | δ22.5° ↑ | δ30° ↑ |\n|-------------|--------|-------|-----------|----------|--------|\n| DUSt3R      | 49.7   | 42.8  | 11.6      | 26.2     | 35.9   |\n| MASt3R      | 48.9   | 40.4  | 13.0      | 29.6     | 39.1   |\n| Ours-Stage1 | 35.9   | 27.6  | 18.9      | 41.5     | 53.5   |\n| **Ours**    | **30.7** | **21.4** | **28.9** | **51.9** | **62.2** |\n\n---\n\n### DIODE-outdoor (outdoor)\n\n| Method       | Mean ↓ | Med ↓ | δ11.25° ↑ | δ22.5° ↑ | δ30° ↑ |\n|-------------|--------|-------|-----------|----------|--------|\n| DUSt3R      | 28.1   | 17.5  | 32.1      | 58.2     | 66.5   |\n| MASt3R      | 29.0   | 18.4  | 31.5      | 56.8     | 65.4   |\n| Ours-Stage1 | 23.5   | 16.7  | 33.7      | 63.2     | 72.9   |\n| **Ours**    | **20.8** | **12.8** | **43.0** | **70.7** | **77.0** |"}}, "id": "Ifuse6MiL5", "forum": "kxVjQhkAWz", "replyto": "kxVjQhkAWz", "signatures": ["ICLR.cc/2026/Conference/Submission2862/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2862/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2862/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763552538943, "cdate": 1763552538943, "tmdate": 1763552538943, "mdate": 1763552538943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Dens3R, a regression-based 3D foundation model that unifies the prediction of multiple geometric quantities including depth, surface normals, and pointmaps from unposed image inputs. The method extends prior DUSt3R/MASt3R frameworks with a shared encoder-decoder backbone, a two-stage training strategy, and position-interpolated rotary positional encoding to improve robustness and multi-task consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-engineered system that integrates multiple known effective components into a unified framework.\n\nDemonstrates consistent performance improvements across several 3D geometry benchmarks (depth, normal, matching).\n\nThe paper is technically sound and clearly written, with solid experimental validation.\n\nThe incorporation of normal prediction and staged training improves empirical robustness and output consistency."}, "weaknesses": {"value": "The core architectural and methodological ideas (multi-task learning, two-stage training, positional interpolation) are not novel and have been widely explored in prior work.\n\nThe backbone and representation design largely follow DUSt3R/MASt3R, with limited conceptual innovation.\n\nThe claim of being a “foundation model” is overstated, as the work focuses on supervised dense regression without demonstrating large-scale generalization or transfer capabilities."}, "questions": {"value": "Can the authors clarify how much of the observed improvement comes specifically from the inclusion of surface normal supervision versus other training refinements?\n\nHow does the proposed “intrinsic-invariant pointmap” differ mathematically from the scale-invariant version used in Stage 1 — is it a new representation or mainly a training objective modification?\n\nSince the positional interpolation for RoPE is borrowed from prior work, did the authors conduct ablations to quantify its contribution relative to baseline DUSt3R at higher resolutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sODXWkuZ7L", "forum": "kxVjQhkAWz", "replyto": "kxVjQhkAWz", "signatures": ["ICLR.cc/2026/Conference/Submission2862/Reviewer_V6Ty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2862/Reviewer_V6Ty"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943604680, "cdate": 1761943604680, "tmdate": 1762916416741, "mdate": 1762916416741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a geometric foundation model by designing a feed-forward framework based on the point map representation as in Dust3R, while also enabling a list of downstream tasks such as depth & normal prediction and semantic segmantation. A intrinsic invariant pointmap is proposed to enhance the geometric robustness during training by introducting a normal prediction head to recover the geometric details. The interpolatied RoPE is employed to handle multi-resolution visual input. Extensive qualitative and quantitative experimental results demonstrate the effectiveness of the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Overall, the paper is well presented and the qualitative images used for demonstrating the geometric details are impressive.\n\n(2) Different from previous baseline methods such as Dust3R and Mast3R, the proposed framework is capable of predicting high-fidelity normal maps to recover the local geometric details, which is important for downstream computer graphics related applications."}, "weaknesses": {"value": "(1) The motivation of designing a 'intrinsic invariant' pointmap is unclear to me. The pointmap is trained in a scale-invaraint manner by normalizing its geometric scale factors. Besides, the reason behind introducing the 'pointmap - normal' feature concatenatation can resolve the 'intrinsic-invaraint' ambiguity also remains unclear. Does it indicate that the normal could be further regularized by utilizing the information in the normalized pointmap?\n\n(2) Although the positional-intropolated RoPE is technically reasonable, I think the novelty here is limited thus hard to be claimed as a technical innovation. By adapting different resolution images during training and inference, it is necessary to deal with the positional embedding in ViT with the flexible sequence length. So interpolating RoPE is a natural choice instead of a novelty. \n\n(3) One highlight of this method is the high-quality normal map prediction. However, the baseline methods used for comparison are trained on single view, leading to a unfair compairson. Since the paper follows the Dust3R's framework, a more practical baseline design is to compare the normal extracted from Dust3R's point map, which also uses a pointmap representation and trained on multiple views."}, "questions": {"value": "On the normal prediction head, the authors mentioned to replace the 'one-to-many' mapping to 'one-to-one' mapping. Does this mean infer on the single image feature instead of using cross attention to aggregate the features from multiple views? \n\nOverall I think the paper proposed a technically feasible system, however some technical motivations and details look unclear to me. If the authors can address my concerns, I would consider to change my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LI17I5XqHk", "forum": "kxVjQhkAWz", "replyto": "kxVjQhkAWz", "signatures": ["ICLR.cc/2026/Conference/Submission2862/Reviewer_EqXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2862/Reviewer_EqXv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044302257, "cdate": 1762044302257, "tmdate": 1762916416075, "mdate": 1762916416075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dens3R, a model that predicts 3D geometry such as pointmaps, surface normals, depth, and image correspondences from unposed images. It uses a single transformer with shared weights and a two-stage training process: first learning scale-invariant pointmaps, then refining them into intrinsic-invariant ones using surface normal supervision. The model also adapts position-interpolated rotary positional encoding (RoPE) for high-resolution inputs and shows strong results in normal prediction and image matching."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses an important goal of predicting multiple 3D properties within one unified model.\n* The two-stage training design is well-motivated and helps reduce monocular ambiguity.\n* The position-interpolated RoPE is a simple and practical improvement for handling high-resolution data.\n* Strong empirical results support the model’s effectiveness on normal estimation and matching tasks."}, "weaknesses": {"value": "* Missing quantitative evaluation for depth prediction, which weakens the claim of a unified geometric model.\n* Lacks ablation studies to verify the contribution of Stage 2, the normal loss, and RoPE interpolation.\n* The description of the “Heads Training” process is unclear, especially regarding when and how the depth head is trained.\n* Multi-view inference and computational cost are only briefly mentioned."}, "questions": {"value": "1. Can the authors include standard depth metrics on datasets like NYUv2 or ScanNet?\n2. Are the reported results from the unified model or after separate fine-tuning for each task?\n3. How is the depth head trained, and is the matching loss still used in Stage 2?\n4. Could the paper provide ablation results showing the effects of Stage 2, the normal loss, and RoPE interpolation?\n5. What is the procedure and computational cost for multi-view inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cXtcuxDvIS", "forum": "kxVjQhkAWz", "replyto": "kxVjQhkAWz", "signatures": ["ICLR.cc/2026/Conference/Submission2862/Reviewer_4czy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2862/Reviewer_4czy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762583636056, "cdate": 1762583636056, "tmdate": 1762916415638, "mdate": 1762916415638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}