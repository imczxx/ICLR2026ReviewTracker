{"id": "x6c72680uD", "number": 5701, "cdate": 1757927731075, "mdate": 1763746093889, "content": {"title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models", "abstract": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used parameter-efficient fine-tuning (PEFT) methods for adapting large language models (LLMs) to downstream tasks. While highly effective in single-task settings, it struggles to efficiently leverage inter-task knowledge in complex multi-task learning scenarios, often requiring substantial task-specific data to achieve optimal performance. To address this limitation, we introduce MeTA-LoRA, a two-stage optimization framework that significantly improves data efficiency in multi-task adaptation. In the first stage, task-specific LoRA adapters are learned using only a few samples from each involved dataset, enabling rapid adaptation without large-scale supervision. In the second stage, the shared LoRA adapter is updated by aggregating gradients from multiple tasks to promote knowledge transfer across tasks, further reducing data usage by leveraging common patterns. In both multi-task learning and multilingual learning scenarios, our method matches or surpasses the performance of traditional full-data LoRA fine-tuning approaches, while using significantly less task-specific data.", "tldr": "We propose a data-efficient LoRA-based fine-tuning sheme for multi-task learning.", "keywords": ["data-efficient training", "multi-task learning", "parameter-efficient-training", "meta learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c57d96783ca3e6799c6a39b16e7ca128af4ad378.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework named META-LORA, designed to enhance the data efficiency of large language models during LoRA fine-tuning in multi-task scenarios through a two-stage meta-learning approach. The method first rapidly acquires task knowledge on a small number of samples through a Task-Specific Adaptation phase, followed by a Meta-Knowledge Update phase that aggregates gradients across multiple tasks to promote knowledge generalization. Experimental results demonstrate that META-LORA achieves performance comparable to or surpassing baseline methods fine-tuned on full datasets across multi-task and multilingual benchmarks, using only a minimal number of unique samples.\n\nWhile the problem addressed is highly significant and the proposed approach innovative, its core conclusions rest on experimental comparisons that warrant further scrutiny. Furthermore, the method's complexity and robustness are not sufficiently validated through ablation experiments, leaving its advantages and applicability unclear."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work addresses the “data efficiency” challenge in fine-tuning large models for multi-task scenarios. This represents a widespread and critical challenge in real-world applications, where acquiring large volumes of high-quality labeled data is often prohibitively expensive.\n\n\n2. The integration of meta-learning's “learning how to learn” philosophy with the LoRA framework constitutes a novel and intelligent approach. Decoupling the fine-tuning process into two distinct phases: task adaptation and meta-knowledge updating, is theoretically coherent and logically sound, offering fresh perspectives for addressing knowledge transfer challenges in multi-task learning.\n\n3. META-LORA ultimately produces a single, shared LoRA adapter. This enables highly efficient model deployment and inference without requiring switching or combining multiple adapters for different tasks."}, "weaknesses": {"value": "1. Parameters such as the learning rate (α) in the Task-Specific Adaptation phase , the learning rate (β) in the Meta-Knowledge Update phase , the adaptation steps (k) , the number of sampled tasks (n) , and the support/query set sizes (n_s, n_q)  are overly complex. Furthermore, the paper lacks sufficient ablation experiments  to justify these choices. Particularly concerning are the adaptation steps (k) and the number of sampled tasks (n).\n\n2. For experiments using datasets of identical size in Tables 6 and 9 , META-LORA trained for 1,000 iterations  with 16 data points per batch (support set + query set), while the baseline trained for 5 or 10 epochs. This results in a significant disparity in both the number of backpropagation passes and the total training steps. This suggests a roughly 30-fold difference in gradient updates. Wouldn't this cause the baseline to underfit? The experiments here do not support the claim that “META-LORA achieves better results than the baseline on the same small dataset.”\n\n\n3. The authors' contribution states “a novel framework designed to enhance data efficiency for multi-task LoRA adaptation ,” yet the baseline does not incorporate data-efficient strategies mentioned in the related work, such as coreset selection or data pruning."}, "questions": {"value": "1. If the final single shared adapter encounters task conflicts  in multi-task settings, could this lead to compromises between tasks? For example, if one task requires formal writing while another demands informal writing, the model might learn an average strategy. This could result in suboptimal performance on both tasks compared to methods (like HydraLoRA) that preserve specific parameters for different tasks. This conflict aspect requires discussion.\n\n\n2. The meta-knowledge update relies on the average gradient across n tasks. If the total number of tasks N is large while n remains fixed at 2, model convergence to capture knowledge across all N tasks may become extremely slow or unstable. The paper does not explore performance when N increases.\n\n3. The results of the ablation studies, shown in Fig. 2 , reveal that the task-specific adaptation (STA) stage  provides little to no improvement in model performance when the model is large (13B) or the dataset size is moderately large (100 examples), and even shows a decline in some cases, raising doubts about its effectiveness. Furthermore, even with 7B parameters and 50 examples, improvements are only observed for some datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DGNRJCwz7H", "forum": "x6c72680uD", "replyto": "x6c72680uD", "signatures": ["ICLR.cc/2026/Conference/Submission5701/Reviewer_r5pv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5701/Reviewer_r5pv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805908992, "cdate": 1761805908992, "tmdate": 1762918204730, "mdate": 1762918204730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes META-LoRA, a parameter-efficient fine-tuning method inspired by meta-learning.Through a two-stage optimization process—task-specific adaptation and meta-knowledge aggregation—it enables a single shared LoRA module to achieve efficient generalization across multi-task and multilingual scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple yet effective, cleverly introducing the meta-learning (MAML) concept into the LoRA framework.\nIt achieves exceptionally high data efficiency, requiring only a few dozen samples per task to surpass the performance of both LoRA and HydraLoRA"}, "weaknesses": {"value": "\tThe paper lacks intuitive motivation and theoretical insight — it is not sufficiently clear why the proposed two-stage optimization is designed this way or why it should work from a meta-learning perspective.\n\n\tThe proposed method mainly focuses on improving data efficiency, whereas prior works like HydraLoRA and LoRAHub innovate at the architectural level. The paper would benefit from comparisons with other data-efficient fine-tuning methods beyond the LoRA family.\n\n\tAll tasks are included in training, but there is no evaluation on unseen tasks to verify the model’s ability to rapidly adapt — which is one of the central promises of the meta-learning framework."}, "questions": {"value": "1.\tThe paper claims that META-LoRA can alleviate gradient conflicts across multiple tasks. Could the authors provide quantitative evidence or analysis to support this claim?\n\n2.\tIn the ablation study, the –STA variant appears conceptually similar to standard LoRA. Could the authors clarify whether they are effectively equivalent? Additionally, how exactly does META-LoRA’s two-stage paradigm aggregate gradients from different tasks during meta-updates?\n\n3.\tThe paper describes HydraLoRA as the state-of-the-art (SoTA) approach in multi-task learning. This statement seems not entirely accurate, and the authors may need to provide justification or rephrase it more cautiously.\n\n4.\tUnder the same amount of training data, how does META-LoRA’s two-stage iterative paradigm compare with standard fine-tuning methods (e.g., training for more epochs on the same small dataset)? A direct comparison could better highlight whether the improvement comes from the meta-learning mechanism itself or from additional training iterations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Awg06Ossw", "forum": "x6c72680uD", "replyto": "x6c72680uD", "signatures": ["ICLR.cc/2026/Conference/Submission5701/Reviewer_TBV1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5701/Reviewer_TBV1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916961335, "cdate": 1761916961335, "tmdate": 1762918204456, "mdate": 1762918204456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes META-LORA, a two-stage, data-efficient PEFT framework for multi-task (and multilingual) LLM fine-tuning. Stage I performs brief task-specific adaptation of a per-task LoRA adapter on a small support set; Stage II performs a meta-knowledge update by averaging gradients on query sets (a first-order MAML approximation) to update a single shared LoRA adapter that is used at inference. For the empirical experiments, the paper evaluates (i) a Flanv2-subset with BBH as the eval, (ii) a five-task mixture (GSM8K, QQP, CosmosQA, SiQA, PiQA/WinoGrande) with MMLU / MMLU-math / BBH, and (iii) multilingual fine-tuning on Bactrian-X (52 languages) with XCOPA, XStoryCloze, XWinograd, and EXAMS. Empirical results show that META-LORA generally surpasses LoRA and is competitive with (often better than) HydraLoRA despite using far less data (e.g., BBH wins for 13B on Flanv2; solid average gains on the five-task settings)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed algorithm is generally simple and implementable .\n2. Data-efficiency with 50–100 examples per task/language while matching or exceeding full-data LoRA/HydraLoRA in several cases.\n3. Ablations and scaling: -STA vs full META-LORA, and 50 vs 100 examples show Stage-I helps, and trends are consistent; LR sensitivity provided. \n4. Multilingual gains on XCOPA/XWinograd with tiny per-language budgets."}, "weaknesses": {"value": "1. The core comparisons concentrate on LoRA/HydraLoRA (plus LoRAHub/LoRA-MoE for Flanv2); stronger PEFT baselines (for example, MALoRA/R-LoRA, prompt/prefix/adapters with modern optimizers) are not included, limiting generality claims.\n2. No CIs / multi-seed means on BBH/MMLU/etc.; several margins are small and may fall within variance.  \n3. The runtime table (Meta-LORA 1000 iterations vs others’ epochs) lacks tokens/steps/FLOPs parity. Therefore the  wall-clock claims are difficult to interpret.  \n4. Despite the fact that XCOPA/XWinograd improve, EXAMS drops markedly (e.g., 23.84 vs BX-LLaMA’s ~29–36), which deserves analysis."}, "questions": {"value": "1. For Table 7, please report tokens processed, optimizer steps, and approximate FLOPs for each method to ensure apples-to-apples timing.\n2. What drives the drop on EXAMS despite gains elsewhere? Provide per-language breakdowns and failure analyses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sc5QBVpR7v", "forum": "x6c72680uD", "replyto": "x6c72680uD", "signatures": ["ICLR.cc/2026/Conference/Submission5701/Reviewer_NNem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5701/Reviewer_NNem"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971530753, "cdate": 1761971530753, "tmdate": 1762918204099, "mdate": 1762918204099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes META-LORA, a two-stage optimization framework for parameter-efficient multi-task fine-tuning of LLMs. The method consists of: (1) a task-specific adaptation stage that learns individual LoRA adapters using small amounts of data from each task, and (2) a meta-knowledge update stage that aggregates gradients across tasks to update a shared LoRA adapter. Experiments on multi-task and multilingual benchmarks demonstrate competitive performance with significantly reduced data requirements compared to traditional fine-tuning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear motivation: The paper identifies a real problem that existing LoRA-based multi-task methods require substantial task-specific data, which limits their practical applicability.\n2. Comprehensive experiments: The evaluation covers diverse task domains.\n3. Data efficiency: The claimed reduction in data usage (using only 100 examples per task vs. full datasets) while maintaining competitive performance is noteworthy.\n4. Well-structured presentation: The paper is generally well-written with clear methodology description and appropriate use of figures."}, "weaknesses": {"value": "1. The \"first-order approximation of MAML\" (line 161) is mentioned but not adequately justified. How does this approximation affect performance compared to full MAML?\n2. The episodic formulation with support/query sets (Eq. 2) is borrowed from meta-learning but the paper doesn't clearly explain why this is necessary for the multi-task LoRA scenario\n3. Lack of ablation on key hyperparameters: k (adaptation steps), n (tasks per iteration), support/query set sizes\n4. Missing crucial related work: The paper fails to cite and discuss \"MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\", which presents a highly similar three-stage training paradigm combining universal and domain-specific experts. Beyond MoDULA, several other relevant works on multi-task PEFT and expert mixtures are not discussed:\n(1) MoLoRA (Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning, ICLR2024)\n(2) C-Poly (Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning, ICLR2024)"}, "questions": {"value": "1. On MoDULA: How does your approach differ fundamentally from MoDULA? Please provide a detailed comparison.\n2. On meta-learning necessity: Why is the episodic meta-learning formulation necessary? Have you tried simpler alternatives like just training task-specific adapters independently and then learning to combine them?\n3. On the approximation: What is the performance gap between your first-order approximation and full second-order MAML? This ablation is critical for understanding the trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gU59Noic6R", "forum": "x6c72680uD", "replyto": "x6c72680uD", "signatures": ["ICLR.cc/2026/Conference/Submission5701/Reviewer_6gAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5701/Reviewer_6gAY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044803362, "cdate": 1762044803362, "tmdate": 1762918203771, "mdate": 1762918203771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}