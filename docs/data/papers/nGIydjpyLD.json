{"id": "nGIydjpyLD", "number": 22214, "cdate": 1758327849880, "mdate": 1759896879789, "content": {"title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval", "abstract": "Most text retrievers generate $\\textit{one}$ query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, $\\textbf{A}$utoregressive $\\textbf{M}$ulti-$\\textbf{E}$mbedding $\\textbf{R}$etriever $(\\textbf{AMER})$. Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. \\model{} presents 6 and 16\\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.", "tldr": "We investigate the limitations of single-embedding retrievers and propose a multi-embedding retriever architecture.", "keywords": ["Information Retrieval", "Retrieval", "Embedding Model", "Retrieval Diversity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/309b4b4831b30ef5a3ffbf99fa8d60716ccacc32.pdf", "supplementary_material": "/attachment/3f4f5817c98a0f90f5d95a5dcbd3e02dd6d53269.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Autoregressive Multi-Embedding Retriever (AMER) to address the limitation of standard dense retrievers, which typically generate a single query embedding and struggle to retrieve diverse sets of relevant documents for queries with multiple valid answers. AMER generates multiple query embeddings per query autoregressively, enabling better retrieval of diverse targets. Experiments on both synthetic and real-world datasets show that AMER significantly outperforms single-embedding baselines, especially when target documents are less similar to each other, demonstrating the importance and effectiveness of modeling diversity in document retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ This paper proposes a new retrieval method where the original text, processed through the proposed AMER framework, can generate multiple retrieval embeddings. These embeddings enable comprehensive retrieval of the original text from multiple dimensions, identifying multiple documents containing the correct answers and thereby improving the performance of multi-question retrieval. \n+ The effectiveness of the method is further validated through experiments."}, "weaknesses": {"value": "+ Indeed, the idea of representing queries and documents using multiple vectors for retrieval, namely multi-vector retrieval, is not new, and the authors seem to ignore such relevant literature. One notable method that the authors need to mention and compare against is \"Colbert: Efficient and effective passage search via contextualized late interaction over bert, SIGIR 20\", which is the first paper on multi-vector retrieval. The authors need to cite and compare against this paper.\n+ I understand that the authors may want to learn multiple query embeddings for retrieval, which is not explored by papers like ColBert. However, one recent paper, \"POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval, ICML 25\" has proposed a relevant idea, i.e., dynamically obtaining these query embeddings through learning to decompose queries. The authors also need to analyze the difference from this work and ideally compare the proposed method against it in experiments.\n+ Some technical details are also not clear to me, particularly, how the document embeddings are obtained. Based on my understanding, each document is also represented by multiple embeddings, which play a critical role in learning query embeddings. However, no description is provided on how these document embeddings are generated for each document."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Pw2epuHHy", "forum": "nGIydjpyLD", "replyto": "nGIydjpyLD", "signatures": ["ICLR.cc/2026/Conference/Submission22214/Reviewer_Q6bx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22214/Reviewer_Q6bx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706456626, "cdate": 1761706456626, "tmdate": 1762942117943, "mdate": 1762942117943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the task of multi-query retrieval. It introduces AMER, a retrieval model that generates multiple query embeddings autoregressively. All the predicted query vectors are then used to retrieve documents. This approach directly addresses the limitations of\nsingle-query vector retrievers and enables retrieving diverse outputs. Finally, the authors test the proposed approach on several benchmarks (one synthetic introduced in the paper and two other existing benchmarks)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important task. It shows some limitations in current methods and proposes a method that addresses them. Overall, the idea seems interesting to me, but I feel like the improvement on real world tasks is a lot lower than I expected."}, "weaknesses": {"value": "My main concern is related to the experimental part: while I agree that in principle one query can be mapped to diverse documents and the experiments show that there's a degradation in performance by choosing the most diverse set of documents, the synthetic data experiments feel tailored explicitly for this case. If the proposed method achieves 100% performance on the synthetic data, it feels like the experiment is either to simple or specifically tailored for this case. I think that overall it is true that you can in principle have a query matched to diverse documents, but there's a limit to that - if the set of documents is too diverse, then the query shouldn't retrieve all of them. So, I feel like there's no real insight gained from the synthetic data experiment. My next concern is related to the novelty which seems relatively limited since the idea of learning multiple embeddings for the same query has been explored for example in cross-modal retrieval\n\nYale Song, Mohammad Soleymani; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1979-1988\n\nweak comparison with other methods (see below)"}, "questions": {"value": "Maybe I am missing something, but why isn't the sota comparison made against methods from Fig 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yZqtfNAz4s", "forum": "nGIydjpyLD", "replyto": "nGIydjpyLD", "signatures": ["ICLR.cc/2026/Conference/Submission22214/Reviewer_XqTN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22214/Reviewer_XqTN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941355754, "cdate": 1761941355754, "tmdate": 1762942117703, "mdate": 1762942117703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets retrieval settings where a single query has multiple, diverse target documents and single-embedding retrievers fail to cover distant clusters. It proposes AMER, an autoregressive multi-embedding retriever that generates a sequence of query vectors from the query encoder while keeping the document encoder frozen. Training uses InfoNCE with Hungarian matching to align unordered positives to generated vectors and scheduled sampling to reduce exposure bias. At inference, each vector retrieves candidates that are merged via round-robin. Results show perfect coverage on synthetic data and consistent, sometimes large, gains on AmbigQA/QAMPARI, especially when targets are dissimilar."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper reframes retrieval for multi-target queries as autoregressive generation of multiple query embeddings, creatively combining sequence modeling with contrastive retrieval. It uses Hungarian matching to align unordered positives and scheduled sampling to reduce exposure bias, removing a core limitation of single-vector retrievers."}, "weaknesses": {"value": "(1) Baselines are incomplete. The paper does not compare against strong multi-vector retrievers (e.g., the ColBERT[1] family) or competitive query-rewriting/expansion approaches (such as MMLF[2]). Although architectural limitations of late interaction are mentioned, the lack of head-to-head metrics under matched budgets makes it hard to gauge relative advantage. \n\n(2) Single-answer regimes are underexplored. It remains unclear how the method behaves when a query has a single narrow intent. \n\n(3) The work does not explore instruction-tuning that ask the model to produce multiple distinct embeddings , leaving open whether instruction control could better leverage LLM generative priors for diversity.\n\n[1]. Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval, 2020.\n\n[2]. Yuan-Ching Kuo, Yi Yu, Chih-Ming Chen, and Chuan-Ju Wang. 2025. MMLF: Multi-query Multi-passage Late Fusion Retrieval. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 6587â€“6598, Albuquerque, New Mexico. Association for Computational Linguistics."}, "questions": {"value": "As above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "arBcYcXU9j", "forum": "nGIydjpyLD", "replyto": "nGIydjpyLD", "signatures": ["ICLR.cc/2026/Conference/Submission22214/Reviewer_AiVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22214/Reviewer_AiVt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163013116, "cdate": 1762163013116, "tmdate": 1762942117060, "mdate": 1762942117060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a common and practical problem: standard search retrievers are bad at handling ambiguous queries. The authors first demonstrate that existing retrievers all struggle as the distance between target document embeddings grows and then go ahead to present AMER as a solution. Instead of generating just one query vector, it auto-regressively generates multiple query vectors. To handle the fact that the multiple target documents are an unordered set, the authors also employ an elegant Hungarian matching algorithm to find the optimal pairing between generated query embeddings and target document embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's core idea is simple, intuitive, and rigorously tested. It is also well written.\n- The authors tackle a core, fundamental limitation of the dominant bi-encoder retrieval paradigm. By providing a practical architecture to move \"beyond single embeddings\", this work opens a new and important direction for retrieval model design.\n- The authors look at performance gain on synthetic benchmark that showcases it's superior performance but also show the moderate gain on real world data."}, "weaknesses": {"value": "- The authors state they \"assume a setting\" with a frozen document encoder for \"faster development\". This is a major experimental concession. While this makes testing easier, it creates a potential disconnect between the latest documents.\n- There might be a potential practical downside of higher inference cost due to *m* separate matches.\n- The true number of distinct answers (or answer clusters) varies per query, from one to many. This fixed parameter might not perform well in certain cases or might be inefficient.\n- To train this system, you need to find all the gold documents for all the different answers for each query. This can be potentially noisy and unstable."}, "questions": {"value": "- Have the authors though about any practical limitations like increased cost/latency or fixed document encoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2kPviJ96v1", "forum": "nGIydjpyLD", "replyto": "nGIydjpyLD", "signatures": ["ICLR.cc/2026/Conference/Submission22214/Reviewer_YjuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22214/Reviewer_YjuD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220632455, "cdate": 1762220632455, "tmdate": 1762942116728, "mdate": 1762942116728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}