{"id": "b6Py2zy0fK", "number": 18391, "cdate": 1758287121883, "mdate": 1759897106181, "content": {"title": "Enabling arbitrary inference in spatio-temporal dynamic systems: A physics-inspired perspective", "abstract": "Modern spatio-temporal learning techniques usually exploit sampled discrete observations to foresee the future. Actually, spatio-temporal dynamics are continuous and evolve everytime and everywhere, thus modeling  spatio-temporal dynamics in a continuous space can be long-standing challenge. Existing deep learning architectures often fail to generalize to unseen regions or graph topologies, while many physics-driven approaches are confined to Euclidean grids and scale poorly to complex graph structures. To address this gap, we propose PhySTA, a physics-inspired spatio-temporal learning framework designed for efficient and scalable arbitrary inference over graph-structured data. PhySTA integrates two key modules, i.e., (1) Continuous Operator-based Spectrum-Temporal Learning (CoSTL), which leverages a Graph-Time Fourier Neural Operator combined with Time-Gated Spectral Segmentation Perception to model continuous dynamics in operator space, (2) Adaptive Multi-scale Interaction (AMI) that constructs multi-scale subgraphs and introduces node-edge coupled convolution to capture discrete interaction patterns and refine continuous predictions. By bridging operator learning with node-edge-graph interaction, PhySTA achieves both continuity-aware dynamic modeling and hierarchical interactive refinement. Extensive experiments across large-scale benchmarks demonstrate that PhySTA attains state-of-the-art accuracy while reducing computation cost and lowering parameter overhead.", "tldr": "", "keywords": ["Neural operators", "Spatio-temporal systems", "Graph neural networks", "Data mining"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2ee6bf51459a7c0cbbee728e5f5a2d4f7e67e48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PhySTA (Physics-inspired Spatio-Temporal Learning for Arbitrary Inference), a framework that models continuous spatio-temporal dynamics on graph-structured data by integrating operator learning with multi-scale graph neural networks. The core idea is to bridge the gap between continuous real-world dynamics and discrete sensor observations, enabling reliable inference in unobserved regions.\n\nThe framework comprises two main modules:\nContinuous Operator-based Spectrum-Temporal Learning (CoSTL): Uses a Graph-Time Fourier Neural Operator (GT-FNO) and Time-Gated Spectral Segmentation Perception to model continuous dynamics in the joint spectral domain\n\nAdaptive Multi-scale Interaction (AMI): Employs a novel Node-Edge Coupled Convolution and Multi-scale Subgraph Partition (coarse-mid-fine hierarchy) to capture discrete multi-scale interactions and refine the continuous predictions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strong Theoretical and Architectural Novelty: The approach successfully extends the Fourier Neural Operator (FNO), a tool for continuous Euclidean domains, to non-Euclidean graph domains (GT-FNO) using the magnetic Laplacian and a joint graph-time spectral decomposition. This is a significant theoretical advance in spatio-temporal GNNs.\n\nRobustness to Data Sparsity: PhySTA consistently achieves top performance across all datasets under varying degrees of node masking (up to $70\\%$ sparsity). This directly validates its core claim of enabling robust \"arbitrary inference\" in unobserved regions\n\nfficiency and Low Overhead: PhySTA achieves state-of-the-art accuracy while using a lower number of parameters ($123,474$) and less GPU memory ($6,042 \\text{MB}$) compared to most deep GNN baselines like AGCRN and STG-ODE"}, "weaknesses": {"value": "1. Scalability Bottleneck of Spectral Decomposition: The paper acknowledges that the Graph Fourier Transform (GFT) step, which involves spectral decomposition (magnetic Laplacian), can be computationally demanding on very large graphs9. Since GFT's complexity is $O(N^2)$, this $N^2$ dependency limits the framework's scalability for massive real-world graphs (e.g., city-scale mobility or power grids with $N \\gg 1000$). It is crucial to discuss concrete techniques like Nyström approximation or sparse spectral methods to mitigate the $O(N^2)$ bottleneck\n\n2. Ambiguity of Multi-scale Aggregation: The AMI module's multi-scale single-layer modeling (coarse, mid, fine graphs) claims to capture complex interactions efficiently. However, the Louvain community detection used for subgraph creation is non-differentiable, potentially complicating end-to-end training. The authors should clarify how the node2center mapping and the three-level graph structure are integrated into the differentiable training pipeline."}, "questions": {"value": "Addressing GFT Scalability (The Major Bottleneck): The paper acknowledges that the $O(N^2)$ complexity of the full Graph Fourier Transform (GFT) limits scalability. Since current spatio-temporal benchmarks scale up to 716 nodes (SD), the $N^2$ barrier remains a practical constraint for city-scale graphs ($N>5000$). Could the authors propose and experimentally validate a concrete, scalable approximation technique within the GT-FNO architecture (e.g., using a localized sparse spectral filter, Nyström approximation on the magnetic Laplacian, or spectral compression) to push the scalability beyond the current constraint?\n\n\nDifferentiability and Justification of Multi-scale Graph Construction: The Adaptive Multi-scale Interaction (AMI) relies on Louvain community detection to generate coarse and fine subgraphs. Since community detection is generally non-differentiable, how is the graph hierarchy generation process integrated into the end-to-end training pipeline? Furthermore, given the complexity of AMI, can the authors provide a side-by-side comparison (e.g., in the Appendix) of the performance gains versus a simpler, fully differentiable multi-scale strategy, such as one based on standard differentiable pooling (DiffPool/TopK)?\n\n\nDetailed Analysis of Spectral Component Contributions (TGSSP): The ablation study suggests that the specialized Time-Gated Spectral Segmentation Perception (TGSSP) module contributes only minor gains compared to other components. Given the module's complexity (bandwise parameterization, time-gating, complex eigenvalues/magnetic Laplacian), can the authors provide a more detailed visualization or analysis of what TGSSP learns? For example, showing how the learned per-mode scaling factor ($\\alpha_k$) and the time-gating factor ($g(\\omega)$) prioritize or suppress specific frequency bands over time would better justify the module's role in modeling non-stationary dynamics.\n\nMore applications: could it be applied to weather and climate forecasting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "toeAcec3Sl", "forum": "b6Py2zy0fK", "replyto": "b6Py2zy0fK", "signatures": ["ICLR.cc/2026/Conference/Submission18391/Reviewer_ucx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18391/Reviewer_ucx5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524820602, "cdate": 1761524820602, "tmdate": 1762928098461, "mdate": 1762928098461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the challenge of arbitrary inference in spatiotemporal dynamic systems, where existing methods either fail to generalize to unseen regions and complex graph structures or are confined to Euclidean grids. The proposed framework PhySTA integrates two innovative core modules: Continuous Operator-based Spectrum-Temporal Learning (CoSTL), which extends neural operators to non-Euclidean domains via a Graph-Time Fourier Neural Operator (GT-FNO) and Time-Gated Spectral Segmentation Perception for continuous dynamics modeling, and Adaptive Multi-scale Interaction (AMI) that constructs multiscale subgraphs and uses node-edge coupled convolution to capture discrete interactions. Theoretically, GT-FNO is proven to have universal approximation capability for continuous spatio-temporal graph operators with controllable \\(L^2\\) error. Experimentally, on traffic and air quality benchmarks (PEMS-BAY, SD, KnowAir), PhySTA achieves state-of-the-art accuracy across various missing data ratios, reduces computation cost significantly, and has fewer parameters and lower GPU memory consumption compared to baselines, demonstrating robust generalization for arbitrary inference even in sparse sensor scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured, and its notations are generally clear.\n2. The proposed approach demonstrates performance improvements in most experimental settings compared to recent baseline methods."}, "weaknesses": {"value": "1. In the motivation section, the authors propose that the modeling of continuous changes relies on graph spectral modeling, but the correlation between the two is not clearly established.\n2. It is not specified where the truncation operation is performed.\n3. In Equation 7, the graph is divided into three layers: coarse, mid, and fine. However, these subgraphs only exist on partial nodes. Therefore, it remains unclear whether the extracted features (X_coarse, X_mid, X_fine) have missing node features respectively.\n4. Issues with notations and formatting:\n   - In Line 256, there is a missing space before \"Inspired\"; in Line 264, there is a missing space before \"The\".\n   - In Equation 8, how are the two predicted values (y_costl and y_ami) obtained? Is the result of Equation 5 equivalent to Y_AMI?\n5. How do continuous spectrum-temporal modeling and adaptive multi-scale interaction handle dynamic topology changes respectively? At present, the study seems to treat the underlying graph (i.e., adjacency matrix A) as static, without corresponding discussions on dynamic scenarios."}, "questions": {"value": "see details in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j82CyaAxWS", "forum": "b6Py2zy0fK", "replyto": "b6Py2zy0fK", "signatures": ["ICLR.cc/2026/Conference/Submission18391/Reviewer_5Bv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18391/Reviewer_5Bv2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561978824, "cdate": 1761561978824, "tmdate": 1762928098162, "mdate": 1762928098162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PhySTA, a physics-inspired framework that unifies continuous operator learning with graph-based spatio-temporal modeling. It introduces two main modules:\n(1) a Graph–Time Fourier Neural Operator (GT-FNO) equipped with Time-Gated Spectral Segmentation Perception (TGSSP) for modeling continuous spectral dynamics on graphs, and\n(2) an Adaptive Multi-Scale Interaction (AMI) mechanism that captures multi-scale node–edge relationships via coupled convolution and hierarchical graph construction.\nA Continuity–Discreteness Interaction Module (CDIM) further fuses both continuous and discrete predictions for arbitrary inference in unobserved regions.\nExperiments on large-scale traffic and air-quality datasets demonstrate strong accuracy, robustness, and efficiency compared with several state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel integration of physics-inspired operator learning and GNNs:\nThe proposed GT-FNO extends Fourier Neural Operators to non-Euclidean graphs, enabling continuous modeling over directed graphs—a clear conceptual innovation.\n\n2. Multi-scale and coupled graph design:\nThe AMI module effectively captures long-range, multi-level dependencies within a single layer, addressing over-smoothing and inefficiency issues seen in deep GNNs.\n\n3. Strong empirical performance and efficiency:\nPhySTA achieves consistent improvements across datasets with fewer parameters and memory cost (≈74% FLOP reduction), showing excellent trade-offs between accuracy and scalability.\n\n4. Comprehensive experiments and ablation analysis:\nThe inclusion of multiple datasets, mask ratios, and detailed component ablations provides good evidence of robustness and interpretability."}, "weaknesses": {"value": "* Limited comparison to recent operator-based or physics-informed baselines:\nThe paper mainly compares against classical and GNN-based methods (STGCN, DGCRN, etc.), but omits recent neural operator or PDE-based baselines such as Graph Neural Operator (Li et al., 2023) or Geo-FNO (Li et al., 2024). These would strengthen the claim of operator generalization.\n\n* Writing quality and presentation:\nThe exposition is heavy and sometimes unclear, especially in the methodology section. Some mathematical notations are inconsistent, and figures (e.g., Fig. 2, Fig. 3) are not fully self-explanatory. The authors could simplify and streamline the presentation for readability.\n\n* Ablation and interpretability could be expanded:\nAlthough the ablation table is informative, qualitative insights on how each frequency band or subgraph level contributes to the final prediction are missing. Visualizations of spectral energy distribution or temporal gating behavior would enhance interpretability.\n\n* Scalability limitation not sufficiently addressed:\nThe reliance on magnetic Laplacian spectral decomposition may hinder scalability for very large graphs. While this is briefly mentioned in the limitations, empirical evaluation on larger graphs would make the claim more convincing.\n\n* Incomplete baseline coverage:\nSome recent transformer-based and neural-operator hybrid methods (e.g., Graphormer, SpaceTimeFormer) are missing from comparison, which may weaken the “state-of-the-art” claim."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0NUzDvo5nC", "forum": "b6Py2zy0fK", "replyto": "b6Py2zy0fK", "signatures": ["ICLR.cc/2026/Conference/Submission18391/Reviewer_sBzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18391/Reviewer_sBzk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794529668, "cdate": 1761794529668, "tmdate": 1762928097841, "mdate": 1762928097841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}