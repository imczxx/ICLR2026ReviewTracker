{"id": "ruKQHG0HXp", "number": 10956, "cdate": 1758185540111, "mdate": 1759897618284, "content": {"title": "Improving Policy Optimization via Enhanced Exploration", "abstract": "Reinforcement learning has become the standard approach for aligning large language models to complex reasoning tasks. However, these methods often overlook rare valuable responses, as learning signals are dominated by high-probability, frequently sampled outputs. To address this, we propose EXploration-Enhanced Policy Optimization (EXPO), a novel approach that dynamically reweights the advantage of each response based on its generation probability. EXPO amplifies gradients from rare valuable samples, ensuring they contribute meaningfully to policy updates and guide the model toward underexplored, high-value solutions. We evaluate EXPO on multiple mathematical reasoning benchmarks. It consistently outperforms strong baselines across model scales: on Qwen2.5-Math-1.5B, EXPO surpasses DAPO by +3.0\\%; on Llama-3.2-3B-Instruct, by +3.6\\%; and on the larger Qwen2.5-Math-7B, it outperforms the DAPO by +4.6\\%, Dr.GRPO by +5.3\\% and instruction-tuned baseline by +9.1\\%,  These gains demonstrate EXPO’s effectiveness in leveraging valuable but underrepresented responses for better policy learning.", "tldr": "We propose EXPO, a reinforcement learning method that amplifies rare valuable responses in LLM training to promote exploration and improve performance.", "keywords": ["Large Language Models", "Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21080d28b5872e3c91014f3429279bc6971f1780.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a key limitation in standard RL methods used for aligning LLMs, which the authors term \"statistical short-sightedness.\" They argue that existing policy gradient algorithms are biased towards high-probability responses, causing them to overlook and suppress rare, yet valuable, reasoning paths. To address this, they propose an algorithm that modifies the advantage function by dynamically reweighting it based on the generation probability of a response. This mechanism amplifies the learning signal for low-probability, high-reward outputs and more heavily penalizes high-probability, low-reward outputs. The authors demonstrate through extensive experiments on mathematical reasoning benchmarks that EXPO consistently outperforms the baseline algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a critical problem in the RLHF of LLMs that the tendency of policy optimization to reduce entropy and converge on a narrow set of \"safe\" solutions. The work focuses on enhancing exploration to escape local optima. And it makes clear derivation from the base algorithm of DAPO. While the idea of reweighting samples is not entirely new in RL, its specific formulation is quite novel. \n\n2. This paper including a lot of experiments on different models and downstream tasks to empirically validate the efficiency of the proposed method. And this paper also includes ablation studies on $\\gamma$ and insights on training dynamics, providing a comprehensive view of the improvement of the proposed approach."}, "weaknesses": {"value": "1. The proposed method is a reweighting scheme applied directly on top of the DAPO framework. While effective, it feels more like an incremental improvement than a fundamentally new algorithm. The specific functional form for the dynamic weight, $\\alpha_{i}=clip((1-clip(\\tilde{p}\\_{i},\\delta,1))^\\gamma,0,\\alpha_{\\max})$, is presented without strong theoretical justification and feels somewhat heuristic. \n\n2. The novel approach introduces several new hyperparameters that appear crucial for its stability and performance including $\\gamma$, $\\alpha_{\\max}$, and $\\delta$. The paper includes ablation study experiments for $\\gamma$, but the sensitivity to $\\alpha_{max}$ and the design of the $\\delta$ schedule are not explored. This added complexity may make the algorithm less practical.\n\nTypos:\n\n1. Table 1: 43->43.0"}, "questions": {"value": "1. Could you explain more on Figure 5 (a) and (b)? It shows that both figures have y-label \"Pass@1 Accuracy\". But the results can not match for certain $\\gamma$.\n2. Could you provide a deeper justification for the specific mathematical form of $\\alpha_i$? Were alternative functions for reweighting based on probability explored, and if so, how did they perform?\n3. For low-reward responses ($A_i < 0$), the proposed method is designed to more heavily penalize frequent mistakes. However, it seems distinct from the primary goal of enhancing exploration of rare positive discoveries. What was the reason for this design choice, and have you tested an alternative approach where you only amplify rare positive rewards while applying a standard penalty to all negative ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jdWQf6IfKq", "forum": "ruKQHG0HXp", "replyto": "ruKQHG0HXp", "signatures": ["ICLR.cc/2026/Conference/Submission10956/Reviewer_2Hs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10956/Reviewer_2Hs1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761175059741, "cdate": 1761175059741, "tmdate": 1762922153168, "mdate": 1762922153168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an exploration-enhanced policy optimization (EXPO) algorithm which is aimed to overcoming the limitations of policy gradient algorithms, i.e. GRPO and DAPO can under-sample rare valuable responses because their gradients are expectation-weighted by the current policy. While DAPO improves gradient estimation, it does not solve the underrepresentation bias. The proposed EXPO algorithm uses a dynamic advantage weighting scheme to ensure gradients from outputs that are high-reward but low-probability under the current policy are taken into account. The authors evaluate the proposed algorithm on multiple mathematical reasoning benchmarks where it outperforms state-of-the-art baselines across model scales."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors address a fundamental weakness in policy-gradient algorithms for training LLMs.\n\n2. The experimental testbed includes a number of ablation studies to compare with DAPO."}, "weaknesses": {"value": "1. The fundamental issue raised by the authors (rare but valuable responses are undersampled) seems to me to remain unaddressed because advantage estimation is often biased or mis-signed for low-probability responses. When you oversample rare responses, you also oversample regions where the advantage estimates have the highest variance (since these responses were rarely observed or labeled)."}, "questions": {"value": "1. The proposed algorithm introduces several additional hyper parameters ($\\delta, \\gamma, \\alpha_max$). Can the authors elaborate why their testbed is a fair comparison with DAPO (with lower number of hyper-parameters) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XPY8UWeOSs", "forum": "ruKQHG0HXp", "replyto": "ruKQHG0HXp", "signatures": ["ICLR.cc/2026/Conference/Submission10956/Reviewer_anbT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10956/Reviewer_anbT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511034580, "cdate": 1761511034580, "tmdate": 1762922152449, "mdate": 1762922152449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reinforcement learning has been considered as one of the standard approaches to align LLMs to reasoning tasks. However, it often favors those high-probability responses due to the loss design. In this work, the authors propose **Exploration-Enhanced Policy Optimization (EXPO)**, an easy-to-use method that rewards the correct but low-probability responses while penalizing frequently occurring mistakes. Experimental results indicate that EXPO achieves improved overall performance compared to previous baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple and easy to use while showing improvement in performance over several baselines.\n- It covers a wide range of experiments and analyses that provide substantial support for the proposed method."}, "weaknesses": {"value": "- While the overall performance is notable, the paper does not provide a clear justification for the specific design of the training dataset. It would be more convincing if the authors conducted an experiment on a uniformly sampled subset or offered an explanation for their choice. In addition, although the authors state that the **Hard** problems converge faster, they do not provide any intuition or analysis to explain this phenomenon.\n- The paper lacks an analysis of the cases where the proposed method underperforms compared to other baselines on certain benchmarks.\n- The step size of $\\gamma$ in the left figure of Figure 5 is too large, making it difficult to determine whether the method is sensitive to hyperparameters."}, "questions": {"value": "- What is the benchmark for the Pass@1 accuracy shown in the left figure of Figure 5? Would the proposed method underperform compared to other baselines if $\\gamma$ is not properly tuned?\n- The authors mention that the performance gap between EXPO and DAPO is largest at $K=1$, which seems counterintuitive since EXPO should generate more diverse responses, while DAPO is more centralized. Therefore, the gap would be expected to increase as $K$ grows. Is there any intuition or explanation behind this phenomenon?\n- Does the distribution shift shown in Figure 1 occur only in the training dataset, or does it also appear in the testing data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tzk47Ccazu", "forum": "ruKQHG0HXp", "replyto": "ruKQHG0HXp", "signatures": ["ICLR.cc/2026/Conference/Submission10956/Reviewer_BMqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10956/Reviewer_BMqq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639716534, "cdate": 1761639716534, "tmdate": 1762922151760, "mdate": 1762922151760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reinforcement learning framework that addresses GRPO's limited exploration and training instability through three components: additional rollouts for targeted exploration on difficult prompts, online filtering to remove low-quality samples, and experience replay to amplify rare high-quality trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper surfaces a focused and important issue: standard policy gradient methods disproportionately reinforce high-frequency responses, causing diversity collapse and limiting the discovery of rare, high-value reasoning solutions. The proposed method is motivated by both empirical and theoretical analysis.\n- EXPO’s core mechanism: a dynamic, probability-based advantage reweighting, which can be seamlessly plugged into modern policy optimization frameworks, that requires minimal changes to existing RL pipelines.\n- The experimental section evaluates EXPO on five challenging math reasoning datasets using multiple model architectures and scales, comparing against contemporary RLVR methods."}, "weaknesses": {"value": "- The experimental benchmarks are restricted to recent RLVR methods tailored to LLMs, with no direct evaluation against classic exploration RL methods adapted for LLM settings.\n- While the ablation on the $\\gamma$ coefficient is conducted, other hyperparameters—such as the weight bounds, and progressive adjustment schedule—are not fully explored.\n- While the benchmarks are broad within math reasoning, all experiments are on mathematical reasoning datasets (MATH, AIME24, AMC, MinervaMath, OlympiadBench), with no evidence provided as to whether EXPO offers similar benefits on other domains where rare but high-reward trajectories exist.\n- While the paper repeatedly asserts that EXPO \"generalizes\" to various backbones and scales, the only two model families tested are Qwen2.5-Math and Llama, and there are no experiments on models larger than 7B or evaluation on other RL-fine-tuned domains."}, "questions": {"value": "1. What is the applicability of EXPO for tasks beyond mathematical reasoning and to models not trained via RLVR? Have you investigated its efficacy for domains such as open-domain QA, code generation, or reinforcement learning outside of LLMs?\n2. Can the authors clarify how hyperparameters for EXPO's dynamic weighting ($\\gamma$, $\\delta$, $\\alpha_{max}$) are tuned?\n3. Are there scenarios or data domains (outside mathematical reasoning) where EXPO fails to yield an advantage or amplifies instability?\n4. Can you provide more detailed analysis of EXPO’s stability across a wider set of hyperparameter combinations? Are there any failure modes or extreme scenarios where EXPO collapses or underperforms compared to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rq8vq1B4Wi", "forum": "ruKQHG0HXp", "replyto": "ruKQHG0HXp", "signatures": ["ICLR.cc/2026/Conference/Submission10956/Reviewer_JuBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10956/Reviewer_JuBR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931952467, "cdate": 1761931952467, "tmdate": 1763002890845, "mdate": 1763002890845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}