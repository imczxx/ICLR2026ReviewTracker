{"id": "3YKeB9R1g9", "number": 13815, "cdate": 1758223122080, "mdate": 1759897410763, "content": {"title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families", "abstract": "Effective LLM training relies on *consistency*, meaning that key quantities—such as final losses and optimal hyperparameters—scale predictably across model sizes. Qiu et al. (2025) recently showed that this consistency extends beyond scalars: whole training loss curves can *collapse* onto a universal trajectory after a simple normalization. What remains unclear is whether this phenomenon holds for LLM families trained under *practical scaling recipes*, where width, depth, learning rate, batch size, and weight decay are scaled jointly. We show that it does: loss curves collapse across scales precisely when optimization hyperparameters are set optimally for the given data budget, in accordance with recent empirical scaling laws. Collapse thus emerges as a signature of compute-efficient training. We demonstrate two applications at scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of training pathologies, and (2) the predictability of collapsed curves enables early stopping in large-scale hyperparameter tuning. Finally, we train a competitive LLM family, *Celerity*, using these insights, highlighting collapse as an effective tool for developing efficient LLMs.", "tldr": "We show that loss curves *collapse* across LLM scales when training at fixed TPP and with AdamW timescale set optimally for that TPP, making collapse a marker of compute-efficient training and a tool for tuning, diagnostics, and early stopping.", "keywords": ["Training loss curve collapse", "Compute-efficient LLM pre-training", "Tokens-per-parameter (TPP)", "AdamW EMA timescale", "Learning-rate schedules", "Scale-stable dynamics (μP)", "Early stopping for hyperparameter tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a4ae73a7f291fdfa8cf8399478edd1a690dc960.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how the training of large language models can be made more predictable and efficient. The authors observe that when optimization hyperparameters are set correctly, the normalized training loss curves for models of different sizes collapse onto a single curve. This loss collapse suggests a consistent and predictable scaling behavior across model sizes.\n\nThe paper uses this finding to propose practical tools for large-scale training, e.g., using loss collapse for early diagnostics and for tuning hyperparameters more efficiently. The authors also introduce a new model family, Celerity, to demonstrate that these principles can lead to faster and more compute-efficient training.\n\nThe results are supported by experiments across multiple model sizes, and the figures clearly show the observed alignment of loss curves."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(I am not very familiar with this specific literature. My assessment is therefore based on the content and presentation of this paper itself, rather than detailed knowledge of prior work. Judging solely by the paper, it appears to be a solid piece of work.)\n\nStrengths:\n\n- The paper addresses an important and very active research area (making large-scale model training more predictable and compute-efficient through analysis of loss-curve regularities).\n\n- The authors present convincing experimental evidence that normalized training loss curves collapse across model scales when key hyperparameters are properly aligned. The results span a range of model sizes.\n\n- The idea of using loss-curve collapse as a diagnostic tool and a guide for efficient hyperparameter tuning is valuable in large-scale training settings.\n\n- The paper is generally well-written and easy to follow. The figures are informative and communicate the main ideas well.\n\n- The introduced Celerity model family shows that the proposed ideas can work well in practice."}, "weaknesses": {"value": "- The paper's main ideas are based mostly on experiments, and the theoretical explanations are limited.\n\n- The experiments focus on GPT-like architectures trained with AdamW. It is not clear whether the same collapse behavior holds for other architectures or optimizers."}, "questions": {"value": "- Have you tested the method with more recent state-of-the-art optimizers, e.g. Muon [1]?\n\n- Does the same loss collapse happen for validation loss, or only for training loss?\n\n[1] Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9vOjMuJNys", "forum": "3YKeB9R1g9", "replyto": "3YKeB9R1g9", "signatures": ["ICLR.cc/2026/Conference/Submission13815/Reviewer_SS9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13815/Reviewer_SS9H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710347903, "cdate": 1760710347903, "tmdate": 1762924342405, "mdate": 1762924342405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors use recent work on learning curve universality in compute-optimally trained networks, combined with other observations on the timescales of weight decay to come up with a training/scaling procedure for language models that takes advantage of these effects. They show that correct selection of the weight decay timescale, via either the (coupled) weight decay parameter, learning rate, or batch size/total number of tokens can reshape the loss curve in a consistent manner. They also show that the collapse principle can be used to predict early training failures, and to develop a model that can use detailed data from small scale training runs to predict the results of large scale training runs by 10-30% of the length of the larger scale training run."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper well-integrates key recent results on scaling behavior and uses it to provide a compelling angle on how to scale large models. The experiments are generally thorough and the paper is overall well-presented. The authors provide strong evidence that the universality of learning curves, combined with properly chosen scaling procedures, can be used to train compute-optimally with less risk than with previous methods."}, "weaknesses": {"value": "One thing that is not immediately clear to me is the utility of the model presented in section 5. It seems like the best procedure would be:\n\n* Search over hyperparameters at small scale (e.g. 100M).\n* Match parameters at larger scale.\n* Carry out partial training runs at large scale.\n* Use the partial training runs to select the best run and finish it.\n\nIt seems to me that even with the model, one may have to run many training runs at the largest scale in order to obtain the normalization factor, aka the finalized loss."}, "questions": {"value": "Can you explain the details of the point raised in weaknesses? This is key to the underpinning of the model/fitting procedure proposed in Section 5; right now it is still unclear to me how the proposed model allows for optimization of the unnormalized loss at larger scale.\n\nWith regards to the learning curve predictions: can this model be used to predict the results of varied learning rate schedules, or is it mainly suited to predicting the correct overall timescales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IiLQraVlmE", "forum": "3YKeB9R1g9", "replyto": "3YKeB9R1g9", "signatures": ["ICLR.cc/2026/Conference/Submission13815/Reviewer_J2cj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13815/Reviewer_J2cj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590799815, "cdate": 1761590799815, "tmdate": 1762924341991, "mdate": 1762924341991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper demonstrates that training loss curves (what they call TLCs) of LLM training collapse onto a universal trajectory when normalized, provided that three key factors match: the AdamW timescale τ, the tokens-per-parameter (\"TPP\") ratio, and the learning rate schedule. The authors show that this collapse phenomenon emerges naturally when optimization hyperparameters are set optimally for a given data budget. They then train Celerity, a family of LLMs trained at fixed TPP with optimal τ, demonstrating the collapse across scales, collapse residuals as early diagnostics for training issues, and a predictive model enabling early stopping in hyperparameter tuning."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Quality: The biggest strength of the paper is that the experimental work is thorough and well-executed. Every major ablation you would need is provided, with lots of comparisons (e.g., varying batchsizes, LR sweeps, weight decay, ...). The appendix provides even more. \n\nOriginality: The work follows a continuous line of research about stable and predictable dynamics, and specifically extends the findings of Qiu et al. to LLM training. As such, the contribution is clear: to me, it provides sound evidence of a phenomena we previously believed to be true. The application to early stopping/hyperparameter testing is interesting and well motivated, and the case study with the 1.8B run recovery is cool.\n\nSignificance: Predictability of large scale LLM training runs is extremely important, e.g., being able to reason about optimality, avoid wasting computational resources, or restart"}, "weaknesses": {"value": "The authors are already well aware of the main limitations: the results are done with a fixed (but certainly standard) way of pretraining, i.e., single epoch, AdamW, moderate dataset sizes (\"TPP\"). The conclusions might not hold or look different for setups with much longer training, data curriculums or optimizers and schedules. Especially longer training (higher \"TPP\") would be relevant (is there a point where i breaks down?) and extrapolations beyond just training loss. That being said, the paper is already extensive enough and cannot cover too many topics at once.\n\nMinor: Celerity uses a much more refined datamix than just Slimpajama, which makes some comparison (e.g. to BTLM) somewhat hard. But that is discussed (and represents a general improvement of pretraining strategies including data+optimization).\n\nBesides, I believe the biggest weaknesses revolve around accessibility, which I would cluster into two points: readability and availability of resources. First, as mentioned above, the paper is the newest in a serious of publications around LLM training predictability and scaling, which all build upon each other. I understand the authors have an in-depth understanding of this area, but the paper is extremely dense, with lots of notation and terms that make it daunting and complex to grasp for new readers. I think this presentation can be improved. Second, and very related, the authors have not published the code or any of the models as far as I can tell. Since the exact setup is quite unique (e.g., CompleteP), releasing those artifacts would be crucial in enabling better study of the entirety of methods that are included in this line of work."}, "questions": {"value": "* Maybe I am missing something, or why are the Llama3 and Qwen3 models not included in any figure comparison? The models are in the table, yet their points miss in plots.\n\n* I would highly encourage the authors to include the list of benchmarks in the Fig. 2 caption, link to them, or make it clearer what is being measured. It is a good overview but I would initially suspicious because I could not immediately find what accuracy was actually measured.\n\n* More broadly, figure captions are not self-contained, which makes it hard to grasp details or what exact setup was used; instead I often needed to jump back and forth between text and figures.\n\n* The bias-variance tradeoff is somewhat glossed over, and without spending more extensive time studying the paper, I could not grasp this discussion exactly. Could you potentially clarify directly?\n\n* Figure 15 shows CompleteP outperform μP quite substantially in terms of not only offset but slope. You provide some explanations, but have you verified experimentally how the μP transfer is suboptimal in your LR setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FTyQuM64IQ", "forum": "3YKeB9R1g9", "replyto": "3YKeB9R1g9", "signatures": ["ICLR.cc/2026/Conference/Submission13815/Reviewer_ak8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13815/Reviewer_ak8p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839651186, "cdate": 1761839651186, "tmdate": 1762924341278, "mdate": 1762924341278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}