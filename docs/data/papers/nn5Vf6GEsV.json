{"id": "nn5Vf6GEsV", "number": 22343, "cdate": 1758329830315, "mdate": 1759896871548, "content": {"title": "Predicting Kernel Regression Learning Curves from Only Raw Data Statistics", "abstract": "We study kernel regression with common rotation-invariant kernels on real datasets including CIFAR-5m, SVHN, and ImageNet.\nWe give a theoretical framework that predicts learning curves (test risk vs. sample size) from only two measurements: the empirical data covariance matrix and an empirical polynomial decomposition of the target function $f_*$.\nThe key new idea is an analytical approximation of a kernel’s eigenvalues and eigenfunctions with respect to an anisotropic data distribution.\nThe eigenfunctions resemble Hermite polynomials of the data, so we call this approximation the \\textit{Hermite eigenstructure ansatz} (HEA).\nWe prove the HEA for Gaussian data, but we find that real image data is often ``Gaussian enough’’ for the HEA to hold well in practice, enabling us to predict learning curves by applying prior results relating kernel eigenstructure to test risk.\nExtending beyond kernel regression, we empirically find that MLPs in the feature-learning regime learn Hermite polynomials in the order predicted by the HEA.\nOur HEA framework is a proof of concept that an end-to-end theory of learning which maps dataset structure all the way to model performance is possible for nontrivial learning algorithms on real datasets.", "tldr": "We give an analytical approximation to kernel ridge regression eigenstructure that works well even on real data, and are thus able to predict KRR learning curves from raw data statistics.", "keywords": ["kernels", "kernel regression", "neural tangent kernel", "eigenstructure", "learning curves", "natural data", "MLPs"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55c4d6922b0ddd0eee1182ee811616a654662037.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical framework for predicting the learning behavior of kernel ridge regression (KRR) on high-dimensional possible complex datasets using rotation-invariant kernels.\nIt proposes the Hermite Eigenstructure Ansatz (HEA): the kernel's eigensystem (for rotation-invariant kernels) closely matches a \"Hermite eigensystem\" derived from multivariate Hermite polynomials in the data space, where the latter depends only on low-order statistics of the data distribution and has an explicit analytical form.\nFor two data models, wide Gaussian kernel on a Gaussian measure and fast-decaying dot-product kernel on a Gaussian measure, the ansatz is proven to hold asymptotically.\nNumerical simulations suggest that the ansatz also holds approximately for real datasets like MNIST and CIFAR-10.\nMoreover, MLPs in the feature learning regime are shown to learn Hermite polynomials in the order predicted by HEA (lower-degree first), suggesting broader applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-written and clearly structured, making it easy to follow the main ideas.\n* It is novel to propose the Hermite Eigenstructure Ansatz (HEA) to predict the learning curves of KRR using only low-order statistics of the data distribution. It would help to better understand the generalization behavior of kernel methods and intrinsic structure of data.\n* The theoretical analysis seem to be solid for the two specific data models.\n* Extensive numerical experiments are provided to validate the proposed theory, including synthetic data and real datasets (MNIST and CIFAR-10). The approximation seems to hold well, justifying the practical relevance of the theory."}, "weaknesses": {"value": "* Most of the theory in this paper is heuristic, relying on the proposed Hermite Eigenstructure Ansatz (HEA) without rigorous proof except for two specific data models.\nIn addition, it would be helpful to bound the difference between the prediction errors (the original model and the Hermite one) in terms of the approximation error of HEA, giving non-asymptotic guarantees.\n\n* The theoretical results are limited to rotation-invariant kernels and Gaussian data distributions, which may not generalize to more complex real-world scenarios."}, "questions": {"value": "1. What are the connections between the HEA and the Gaussian equivalence assumption (GEA) used in prior works on learning curves of kernel methods? \n2. What are the datasets that HEA fails to approximate well? Can you provide some more insights into the limitations of HEA?\n3. How will HEA be useful for practical applications, such as kernel selection or hyperparameter tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RsoPCiLJIj", "forum": "nn5Vf6GEsV", "replyto": "nn5Vf6GEsV", "signatures": ["ICLR.cc/2026/Conference/Submission22343/Reviewer_3sV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22343/Reviewer_3sV7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851135589, "cdate": 1761851135589, "tmdate": 1762942177483, "mdate": 1762942177483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"hermite eigensystem\" as an approximate formula for the eigenvalues and eigenfunctions of dot-product kernels, expressed in terms of the data covariance matrix. The authors present two limiting cases where their approximation holds exactly, then show that the learning curves obtained by applying the standard theory of kernel ridge regression under their approximation captureswell the empirical learning curves of kernel ridge regression on real data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written, and the results are clearly and logically organised.  \n- Developing an end-to-end theory of learning based directly on data statistics is a highly valuable goal.  \n- The experimental validation appears convincing and supports the proposed theoretical framework."}, "weaknesses": {"value": "1. Theoretical learning curves for kernel regression have been established in prior work (e.g., *Optimal Rates for the Regularized Least-Squares Algorithm* by Caponnetto and De Vito). It is unclear what new insights an approximate, simplified formula based on Gaussian assumptions contributes to this existing body of knowledge.  \n2. Gaussian Equivalence Principles (e.g., *The Gaussian Equivalence of Generative Models for Learning with Shallow Neural Networks* by Goldt et al.) already characterise when Gaussian approximations like the one made by the authors are valid or not. The paper does not sufficiently discuss its relation to these frameworks.  \n3. The proposed approximation appears *uncontrolled*, as the limits of its validity are neither explored nor commented upon, leaving uncertainty about its general applicability."}, "questions": {"value": "Presumably, if the data are actually assumed to come from the Gaussian distribution with covariance matrix $\\Sigma$, then the eigenfunctions should be multivariate Hermite polynomials. Have the authors tried to relate the kernel eigenvalues to the Taylor coefficients of the Kernel expansion into powers of $\\mathbf{x}\\cdot\\mathbf{y}$ exactly? Is capturing the right eigenvalues even important for the prediction of learning curves, or is it really the order of the eigenfunctions that matters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EAqv7JEBFY", "forum": "nn5Vf6GEsV", "replyto": "nn5Vf6GEsV", "signatures": ["ICLR.cc/2026/Conference/Submission22343/Reviewer_Cuso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22343/Reviewer_Cuso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880469986, "cdate": 1761880469986, "tmdate": 1762942177266, "mdate": 1762942177266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors study kernel ridge regression on anisotropic data for rotation-invariant kernels.\nThey introduce a Hermite eigenstructure ansatz (HEA), that allows to approximately compute the eigendecomposition of the kernel function.\nThe main finding is that the covariance of the data and the coefficients of the target function in the basis of this eigendecomposition are enough to predict the test error of the kernel ridge regression.\nThe result holds when the data is 'Gaussian enough', which is empirically verified for certain real datasets. \nIn particular, the width of the kernel needs to be sufficiently large, as well as the effective dimension of the data. \nThe cases when the predicted test errors are further from the real values, are thoroughly studied.\nTheoretical results prove that the predictions hold when data is Gaussian under two limiting regimes.\n\nOne of the technical issue that the authors had to deal with is how to obtain the coefficients of the expansion of the target function in the HEA of the kernel. They suggest to iteratively project the target function on the top eigenfunctions and use Gram-Schmidt orthogonalization to remove correlations between the Hermite polynomials that empirically works well even when the data is non-Gaussian, but the target is sufficiently smooth."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well-written and constitutes a timely contribution as the role of the data in learning is extremely important.\n\nHermite eigenstructure ansatz is a clean and theoretically motivated tool to understand KRR.\nThe authors present a thorough analysis of when the proposed HEA method holds, both on synthetic and real datasets. \nThey empirically study the effects of data dimension, target function, and various kernels.\nInterestingly, the authors also study the regime 'beyond kernel ridge regression' and find that their predictions hold in that scenario too, to some extent.\n\nFor the case of Gaussian data, two theorems regarding different scaling regimes (very wide kernel and very fast coefficient decay) are present. \nThis work is likely to motivate further research to understand rigorously where the limits of the use of the Gaussianity assumption are, possibly beyond kernel regimes."}, "weaknesses": {"value": "I don't find serious weaknesses in the methodology, experiments or theoretical results of the paper, although I did not check the proofs in detail.\n\nExtended related work discussion would help the reader to position current work:\n1. In (Refinetti et al., 2023), the authors claim that the neural networks, trained on 'Gaussian versions' of CIFAR-10 (with the same mean and covariance), perform worse than on the real dataset. This can be interpreted that CIFAR-10 is not 'Gaussian enough'. Perhaps whether the data is 'Gaussian enough' depends on the class of learning algorithms, with KRR not being able 'to see further' than the covariance of the data. Is such interpretation correct?\n2. There is a line of work that studies 'staircase property' of learning (e.g., Abbe et al., 2021). Is there a relation between this property and the results in Figure 4? Why both Figure 3 and Figure 4 only use monomials as a target and what would happen when polynomials are used instead?\n\nAlso, the notation of $h_i$ (eg in line 1135) can be confused with $h_{\\mathbf{\\alpha}}$.\n\nRefinetti, Maria, Alessandro Ingrosso, and Sebastian Goldt. \"Neural networks trained with sgd learn distributions of increasing complexity.\" International Conference on Machine Learning. PMLR, 2023.\n\nAbbe, Emmanuel, et al. \"The staircase property: How hierarchical structure can guide deep learning.\" Advances in Neural Information Processing Systems 34 (2021): 26989-27002."}, "questions": {"value": "See the section above. Furthermore:\n1. Throughout the text, both 'population data covariance, $\\Sigma$' (eg line 47, Figure 1) and 'empirical data covariance, $\\hat \\Sigma$' (eg line 13, Figure 3) are used. From the description in Section 4, it seems that $\\Sigma$ is the main quantity of study. Could the authors clarify to what extent they expect these results to hold when having access only to $\\hat \\Sigma$?  \n2. Line 1643 and Figure 18: what is $\\gamma$? Should it be $\\zeta$?\n3. Figure 5: in the legend, shouldn't the labels for predicted and empirical be swapped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FN35jullbq", "forum": "nn5Vf6GEsV", "replyto": "nn5Vf6GEsV", "signatures": ["ICLR.cc/2026/Conference/Submission22343/Reviewer_16Lq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22343/Reviewer_16Lq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968194483, "cdate": 1761968194483, "tmdate": 1762942176888, "mdate": 1762942176888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the hermite eigenstructure ansatz (HEA), which gives a closed form expression for the eigensystem of rotation invariant kernels. Using the eigenframework, this gives the learning curvevs for KRR on real image datasets, which are empirically shown to approximate the learning curves well using only the covariance matrix and the hermite decomposition of the target function. They also show that HEA holds for gaussian data and two limiting cases of the kernel function. Finally, the papers empirically shows that MLPs learn hermite polynomials on real datasets in the order predicted by HEA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The hermite eigenstructure ansatz solves one of the main difficulties of using the eigenframework predictions in predicting the learning curves of KRR on real datasets: estimating the eigenvalues and eigenfunctions of the kernel under the given data distribution. This a very important contribution and expands the applicability of this line of work.\n\n\nFurther, the paper provides extensive empirical evidence that the approximate closed form expression for eigenvalues and eigenfunctions can be used to very accurately predict the learning curves of KRR on a number of real image dataset and various synthetic setups, which is needed given the harndess of verfying the for real datasets HEA. \n\n\nFinally, the paper clearly demonstrates the usefulness of insights from HEA, which they use to predict how MLPs learn functions.\n\n\nOverall, I think that this is a very strong contribution that is presented clearly and concisely and should get accepted."}, "weaknesses": {"value": "1. The paper is unnecessarily and sometimes confusingly written in an underdefined and informal way. For example, the use of undefined approximate sign $\\approx$, which is crucial to the Hermite eigenstructure ansatz lines 251-260, is at time confusing. Since we are interested in predicting the learning curves, it is quite easy to define this approximate equivalence between two eigenstructures as having learning curves differing by at most something small. It is also unclear whatt does the final prediction mismatch depend on after using a number of these approximations. I think the whole framework would greatly benefit if some care was taken to quantify or at least characterize the dependence of the prediction error of using the hermite eigenstructure ansatz.\n2. As explained in lines 968-969, and if I understand correctly, the hermite ansatz framework depends on eigenlearning framework to map predictions of eigenstructure to predictions of learning curves. Since the hermite ansatz is not formal (and not quantitative, see #1), the applicability of hermite eigenstructure ansatz also depends on the applicability of the eigenframework to the case considered (so it also depends on Gaussian Universality Ansatz). This way, the error of the final prediction aggregates the error of the hermite eigenstructure prediction and the eigenframework prediction. So it’s a bit unclear whether the conditions for success (Section 4.2) are sufficient for both of these errors to be small. I feel like this “hidden” dependence on the eigenframework should be more transparently discussed in the main body. This is especially given some recent work questioning whether the eigenframework actually applies in the case of NTK.\n3. The paper only shows that HEA holds for the Gaussian data with a Gaussian kernel or other fast-decaying dot product kernels in a certain limit of the parameters of those kernels. It’s unclear what is the interpretation of this limit (e.g. taking the width of the Gaussian kernel to infinity). This section would also be much more clear if the limit of is more quantitatively defined."}, "questions": {"value": "1. Can the approximate relationship in the definition of HEA and elsewhere be formalized in terms of the error of the prediction of the learning curves? \n2. Is the HEA expected to hold whenever the Gaussian Universality Ansatz holds? How the two interact?\n3. What is the interpretation of the infinite width limit of Gaussian kernel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UqpnNNp85p", "forum": "nn5Vf6GEsV", "replyto": "nn5Vf6GEsV", "signatures": ["ICLR.cc/2026/Conference/Submission22343/Reviewer_msN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22343/Reviewer_msN8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040880282, "cdate": 1762040880282, "tmdate": 1762942176624, "mdate": 1762942176624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a principled framework for predicting learning curves of kernel ridge regression (KRR) using only empirical data statistics, specifically the data covariance matrix and the Hermite polynomial decomposition of the target function.  \nThe central theoretical idea is the Hermite Eigenstructure Ansatz (HEA), which posits that for rotation-invariant kernels on approximately Gaussian datasets, the kernel’s eigenfunctions coincide with multivariate Hermite polynomials of the data, and the corresponding eigenvalues factorize as simple monomials in the data covariance eigenvalues.\n\n\n\nThis work offers a conceptually novel and practically valuable contribution: predicting learning curves from minimal dataset statistics without explicit kernel computation.  \nIt bridges data geometry, kernel methods, and neural tangent kernel theory, showing a concrete mapping from data covariance to performance prediction.  \nThe observed similarity between MLP learning order and HEA predictions further strengthens its relevance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces the Hermite Eigenstructure Ansatz, a compact analytical surrogate for kernel eigenfunctions and eigenvalues grounded in probabilistic data geometry.  \n\n- Combines proofs, ablations, and large-scale experiments (CIFAR-5m, SVHN, ImageNet-32) with excellent agreement between theory and observation.  \n\n- The conceptual pipeline links data covariance to feature-space statistics and test error.  \n\n- Provides a data-to-performance theory that is both interpretable and predictive.  \n\n- Clearly delineates success and failure regimes (e.g., Figures 12–16)."}, "weaknesses": {"value": "- No analytical error bounds are provided for the eigenvalue approximation.  \n\n- The Gram–Schmidt coefficient estimation step can be computationally heavy for large datasets.\n\n- Minor comment: Dependence on Gaussianity: the HEA requires data to be \"Gaussian enough\"; it fails for structured or discrete datasets (MNIST, tabular). Extending it beyond this regime would strengthen the framework, but would be very difficult though (but does not undermine the results though)."}, "questions": {"value": "- Could the Gaussian assumption be relaxed to a mixture-of-Gaussians or empirical-moment approach to handle non-Gaussian data?  \n\n\n- Can the authors provide quantitative error bounds between predicted and empirical eigenvalues as a function of effective dimension or kernel width?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xvsvWlR29h", "forum": "nn5Vf6GEsV", "replyto": "nn5Vf6GEsV", "signatures": ["ICLR.cc/2026/Conference/Submission22343/Reviewer_MH8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22343/Reviewer_MH8X"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762374773497, "cdate": 1762374773497, "tmdate": 1762942176368, "mdate": 1762942176368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}