{"id": "JMweItBmbx", "number": 737, "cdate": 1756779177844, "mdate": 1763641984031, "content": {"title": "Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models", "abstract": "Effective specification-aware part retrieval within complex CAD assemblies is essential for automated design verification and downstream engineering tasks. However, directly using LLMs/VLMs to this task presents some challenges: the input sequences may exceed model token limits, and even after processing, performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires significant computational resources, and for many high-performing general-use proprietary models (e.g., GPT or Gemini), fine-tuning access is not available. In this paper, we propose a novel part retrieval framework that requires no extra training, but using Error Notebooks + RAG for refined prompt engineering to help improve the existing general model's retrieval performance. The construction of Error Notebooks consists of two steps: (1) collecting historical erroneous CoTs and their incorrect answers, and (2) connecting these CoTs through reflective corrections until the correct solutions are obtained. As a result, the Error Notebooks serve as a repository of tasks along with their corrected CoTs and final answers. RAG is then employed to retrieve specification-relevant records from the Error Notebooks and incorporate them into the inference process. Another major contribution of our work is a human-in-the-loop CAD dataset, which is used to evaluate our method. In addition, the engineering value of our novel framework lies in its ability to effectively handle 3D models with lengthy, non-natural language metadata. Experiments with proprietary models, including GPT-4o and the Gemini series, show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute accuracy improvement on the human preference dataset. Moreover, ablation studies confirm that CoT reasoning provides benefits especially in challenging cases with higher part counts (>10). Importantly, our method surpasses traditional training-free inference-time approaches (standard few-shot, self-consistency) and further demonstrates strong improvements even on open-source models (e.g., Qwen2-VL-2B-Instruct).", "tldr": "Training-free framework that uses Error Notebooks + RAG to guide VLMs for specification-aware part retrieval in CAD assemblies, yielding significant accuracy gains (e.g., +23.4% on human-preference data).", "keywords": ["Visionâ€“Language Models", "Error Notebook", "Specification-Aware Part Retrieval", "CoT Reasoning", "Human-Preference Dataset"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edef1f6a2710c6b376f59ac00d2386ab7a26e22c.pdf", "supplementary_material": "/attachment/18d30f7a879b4977bf1d82bce0f994ecfd73e8cf.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript discusses the specification aware part retrieval in complex 3D CAD assembly, which is a task that leads to VLM failure due to long context and poor reasoning ability. The author proposes a novel, non training framework that utilizes Error Notebook and RAG to enhance proprietary VLM. This notebook is constructed by collecting initial erroneous thought chains (CoT) inference from VLM, and then using VLM self reflection to generate corrected CoT trajectories. During inference, RAG retrieves these corrected examples as minority shot examples to guide VLM's inference. This article also contributes a new manual annotation CAD dataset and a two-stage pipeline to solve the problem of long context. Experiments have shown that GPT-4o has improved accuracy by 23.4% on a human preference benchmark"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors contribute a new multimodal CAD dataset that includes human preference annotations. This would be  a valuable resource for the community if it can be made public.\n\n2. The concept of leveraging corrected reasoning exemplars as reusable Error Notebooks is novel and elegant.\n\n3. This work provides a new approach for solving the problem of excessively long STEP file inputs, and the part retrieval in assemblies that it focuses on is also a broad application in CAD design."}, "weaknesses": {"value": "1. Some typos need further correction, such as missing commas after some equations and missing spaces between some words.\n\n2. The latency and resource overhead of RAG-based prompting are not quantified (e.g. the time for doing one complete inference).\n\n3. The core viewpoint is that retrieving corrected error is crucial. However, the experiment (Table 1) only compared \"w/E-Notebook\" (its method) with \"w/o-E-Notebook\" (zero sample). Missing a key baseline: a standard few shot method based on RAG that only retrieves correct examples (i.e. (query, correct_CoT, correct_answer)). Without this comparison, it is hard to know whether the performance gain comes from  error corrections or solely from known benefits provided by RAG with any relevant contextual examples."}, "questions": {"value": "1. This paper notes that CoT performance drops with 50 exemplars due to long prompt lengths. Does this suggest a fundamental limitation? Have authors explored strategies to mitigate this, such as summarizing the retrieved CoTs or dynamically selecting only the most relevant steps from each CoT?\n\n2. What is the average token length and inference time per query? This is quite important for practical applications, as it is difficult to apply if the output result is too slow due to the long time of doing CoT.\n\n3. Could authors please quantify the computational cost of building the Error Notebook? For example, how many VLM calls are required per sample to generate the corrected CoT?\n\n4. Are the geometric constraints required for part assembly essentially obtained from the retrieved STEP files, rather than being calculated by the LLM/VLM directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FAAzDsTb50", "forum": "JMweItBmbx", "replyto": "JMweItBmbx", "signatures": ["ICLR.cc/2026/Conference/Submission737/Reviewer_Hfy1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission737/Reviewer_Hfy1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810450493, "cdate": 1761810450493, "tmdate": 1762915593038, "mdate": 1762915593038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free framework for 3D CAD part retrieval using Vision-Language Models (VLMs). Its core contribution is the Error Notebook, a mechanism that uses Retrieval-Augmented Generation (RAG) to guide VLM inference with corrected reasoning exemplars, avoiding costly fine-tuning.\n\nKey contributions: i) A training-free reasoning framework (Error Notebook + RAG) that significantly improves accuracy (up to 23.4% absolute gain) without fine-tuning. ii) A new, human-annotated CAD assembly dataset with relational specifications, based on the Fusion 360 Gallery. iii) A two-stage VLM pipeline (part description generation, then retrieval) to handle long, complex STEP file inputs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The Error Notebook is a creative solution for improving proprietary, \"black-box\" VLMs that cannot be fine-tuned. It effectively adapts self-correction concepts to an inference-time strategy.\n\nThe evaluation is comprehensive, testing multiple VLMs (GPT-4o, Gemini) across various assembly complexities. The method shows robust and consistent improvements for all models, such as boosting GPT-4o (Omni) from 41.7% to 65.1% accuracy.\n\nThe two-stage pipeline is a practical engineering solution for processing lengthy, non-natural language STEP file metadata, making the approach viable for real CAD workflows.\n\nThe new human-preference dataset is a significant contribution, enabling more human-centric evaluation by filtering out ambiguous cases with multiple correct answers.\n\nThe ablations provide valuable insights, such as CoT reasoning being most beneficial for complex assemblies (>10 parts). The finding that performance is similar for 1 to 50 exemplars is also a key practical takeaway."}, "weaknesses": {"value": "The paper is empirically strong but lacks a theoretical justification for why the Error Notebook works. There is no formal analysis of its properties or potential failure conditions.\n\nThe approach requires ground-truth labels to build the Error Notebook. This reliance limits its generalizability and scalability to new domains where labels are unavailable or expensive to acquire.\n\nThe primary comparison is against the same models without the Error Notebook. The paper needs comparisons against other training-free methods (e.g., standard few-shot learning, self-consistency) to isolate the specific benefit of the Error Notebook.\n\nThe computational overhead (API costs, token usage, latency) of constructing the notebook and performing RAG-enhanced inference is not discussed."}, "questions": {"value": "- What specific function is used for RAG retrieval? How sensitive are results to this choice?\n- How can the Error Notebook be constructed or applied in settings where ground-truth labels are unavailable or expensive to obtain?\n- How does the Error Notebook compare to other training-free methods like inference-time scaling (for example self-consistency)?\n- What are the computational overheads (API calls, tokens, latency) for notebook construction and RAG-enhanced inference compared to the baseline?\n- How often do the VLMs successfully generate valid reasoning corrections given the ground truth? What happens when the correction generation itself fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uT3YOhk3DP", "forum": "JMweItBmbx", "replyto": "JMweItBmbx", "signatures": ["ICLR.cc/2026/Conference/Submission737/Reviewer_eRWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission737/Reviewer_eRWS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935899337, "cdate": 1761935899337, "tmdate": 1762915592859, "mdate": 1762915592859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a part retrieval framework that uses error notebooks + RAG for refined prompt engineering. Furthermore, they contribute a human-in-the-loop CAD dataset for evaluating the proposed approach. Evaluations are performed on closed source models and achieve upto a 23% accuracy improvement on human-preference benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- error notebooks idea is a creative solution for test-time reasoning augmentation\n- contribution of human-in-the-loop dataset"}, "weaknesses": {"value": "- comparisons are primarily on proprietary models. It would be great to see them on open-source models as well.\n- the eval metrics are somewhat under-developed. For example, there is not analysis on retrieval relevance.\n- baseline comparisons against conventional CAD part retrieval methods are missing"}, "questions": {"value": "1. why was the dataset size limited to using a single set (752 assemblies) of Fusion 360 Assembly Dataset?\n2. i might have missed this, but, how are the corrected reasoning trajectories verified?\n3. are there scenarios where the model is not able to self-correct with prompting?\n4. what kind of assemblies does the method struggle with?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sUq4D9mKod", "forum": "JMweItBmbx", "replyto": "JMweItBmbx", "signatures": ["ICLR.cc/2026/Conference/Submission737/Reviewer_f2Wn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission737/Reviewer_f2Wn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991319868, "cdate": 1761991319868, "tmdate": 1762915592742, "mdate": 1762915592742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a specific sub-problem in CAD tasks automatization, mainly part retrieval in the assembly. For this task a new approach, based on error notebooks on RAG is proposed, which can be used on top of any LLM model without fine-tuning or adaptataion. Experimental validation shows validity of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed method can be used without fine-tuning on top of any LLM model"}, "weaknesses": {"value": "1) Very limited scope of work due to selected problem\nAs far as I understand, the considered problem is retrieval of specific part in assembly by using its textual description. Each assembly in question consists of very limited amount of parts, less then 50. So the problem is like text retrieval from corpus of up to 50 texts. Yes, texts are very specific (CAD description of parts), but nonetheless the problem is small. This limits both the significance and impact of the work.\n\n2) No related works or similar methods or comparision with existing methods. \nHow such problems are solved now? Are there any approaches for part retrieval, other then LLMs? Has anyone considered the same problem before, or it is a first time?\n\n3) Unclear description and presentation. \nUp till section 2.2 i have been struggling to understand what the problem in question is. Overall the presentation is very unclear."}, "questions": {"value": "Please address the weakness #2 and comment on problem statement and significance/impact of your work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxhZDq4DcG", "forum": "JMweItBmbx", "replyto": "JMweItBmbx", "signatures": ["ICLR.cc/2026/Conference/Submission737/Reviewer_vpJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission737/Reviewer_vpJW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006566040, "cdate": 1762006566040, "tmdate": 1762915592593, "mdate": 1762915592593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nThank you very much for your valuable feedback and constructive suggestions. Although revising our work and conducting additional experiments has been challenging, your insights have genuinely motivated us and significantly strengthened the paper. \n\n**We kindly invite you to review our detailed responses for each reviewer.**\n\nIn addition, we have updated:\n\n1. **The main paper**  \n2. **The supplementary materials**, now including:  \n   - our implementation code,  \n   - the human-annotated dataset,  \n   - and a *scope illustration PDF* that clarifies the problem setting.\n\nWe sincerely hope that these improvements will help you re-evaluate our work. \n\nThank you again for your time, careful consideration, and continued support.\n\nBest regards,  \nThe Authors"}}, "id": "C9np7Uxm4C", "forum": "JMweItBmbx", "replyto": "JMweItBmbx", "signatures": ["ICLR.cc/2026/Conference/Submission737/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission737/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission737/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763639900358, "cdate": 1763639900358, "tmdate": 1763639900358, "mdate": 1763639900358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}