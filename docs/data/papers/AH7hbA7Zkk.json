{"id": "AH7hbA7Zkk", "number": 2723, "cdate": 1757223284912, "mdate": 1759898131249, "content": {"title": "Q&C: When Quantization Meets Cache in Efficient Generation", "abstract": "Quantization and cache mechanisms are typically applied individually in efficient generation tasks, each showing notable potential for acceleration. However, their joint effect on efficiency remains under-explored. Through both empirical investigation and theoretical analysis, we find that that combining quantization with caching is non-trivial, as it introduces two major challenges that severely degrade performance:\n(i) the sample efficacy of calibration datasets in post-training quantization (PTQ) is significantly eliminated by cache operation; (ii) the joint use of the two mechanisms exacerbates exposure bias in the sampling distribution, leading to amplified error accumulation during generation. In this work, we take advantage of these two acceleration mechanisms and propose a hybrid acceleration method by tackling the above challenges, aiming to further improve the efficiency of tasks while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to dynamically improve the sample selection efficacy for the calibration within PTQ for different diffusion steps. A variance compensation (VC) strategy is derived to correct the sampling distribution. It mitigates exposure bias through an adaptive correction factor generation. Extensive experiments demonstrate that our method is broadly applicable to diverse generation tasks, achieving up to 12.7x acceleration while preserving competitive generation quality.", "tldr": "", "keywords": ["diffusion model", "visual  generation models", "cache"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8485893c911dc6c4bbd6d14d9bb9b1324ce0c70d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to jointly use quantization and cache mechanisms in image generation process. The authors identify two main challenges when integrating quantization and cache and propose the TAP and VC methods to address this. Experimental results show that the proposed method accelerates the image generation process by up to 12.7× without compromising quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well-motivated. This paper aims to combine quantization and cache techniques  simultaneously to further accelerate the image generation process. In this process, the authors identified two key challenges: (1) Amplification of Exposure Bias (2) Degradation in Calibration Dataset Effectiveness. To this end, the authors specifically propose TAP and VC as solutions. The authors also provide comprehensive comparative experiments and ablation study results to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "Add a new column in Table 2 to include latency data for each configuration. I would like to see the impact of TAP and VC on latency."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "omZiaN5bL5", "forum": "AH7hbA7Zkk", "replyto": "AH7hbA7Zkk", "signatures": ["ICLR.cc/2026/Conference/Submission2723/Reviewer_itt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2723/Reviewer_itt9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761362354694, "cdate": 1761362354694, "tmdate": 1762916344989, "mdate": 1762916344989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a very practical problem in accelerating generative models, specifically diffusion transformers (like DiT). The goal is to combine two standard speedup techniques: quantization (using fewer bits for weights and activations) and caching (like the K-V cache in transformers, which saves past computations).\nThe authors' key finding is that just \"turning both on\" doesn't work—in fact, it severely degrades performance. The paper then diagnoses why this happens (they identify two major challenges) and proposes two novel methods TAP and VC, that allow these two techniques to work together effectively. The end result is a massive speedup (up to 12.7x) on ImageNet generation while maintaining the quality of the original, slow model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis is a strong, high-quality paper. The originality doesn't come from inventing a brand-new algorithm, but from investigating a subtle, negative interaction between two known techniques that everyone assumed would just work together. Identifying, analyzing, and solving this kind of interaction problem is a very valuable contribution.\n\n2.\tThe significance is crystal clear. Diffusion models are notoriously slow. A 12.7x speedup on a state-of-the-art DiT model running on ImageNet is a massive practical win. It's the kind of work that could be immediately adopted by anyone trying to deploy these models in the real world."}, "weaknesses": {"value": "1.\tA potential weakness is the focus on Post-Training Quantization (PTQ). PTQ is fast, but it's often outperformed by Quantization-Aware Training (QAT). My question is: Why not just use QAT? It's possible QAT would automatically learn to be robust to the cache interactions, making this complex PTQ-specific solution unnecessary.\n\n2.\tCould the authors include speed metric In Table 2 as well for a clear comparison?\n\nMinors:\n1.\tLine 411 ‘The results, presented in Table 6’ should be Table 2."}, "questions": {"value": "1.\tWhy the quantized model in Figure 2 does not show too much higher accumulated error than the original one? It’s inconsistent to the results in [1].\n\nReference:\n\n[1] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in Neural Information Processing Systems, 36, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "McvVcRLp9L", "forum": "AH7hbA7Zkk", "replyto": "AH7hbA7Zkk", "signatures": ["ICLR.cc/2026/Conference/Submission2723/Reviewer_2MVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2723/Reviewer_2MVF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502271452, "cdate": 1761502271452, "tmdate": 1762916344768, "mdate": 1762916344768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of enabling both caching and quantization in DiT models.  The authors first identify two key obstacles: exposure bias amplification and reduced effectiveness of calibration datasets.  They then address these issues by proposing Temporal-Aware Parallel Clustering (TAP) and a variance compensation technique. Extensive experiments and ablation studies demonstrate that the proposed approach achieves notable speed improvements with only minimal degradation in downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe paper tackles a well-motivated problem, clearly isolating the issues and addressing them with thoughtful solutions.\n\n•\tComprehensive experimental evaluation across multiple tasks, including detailed ablation studies, supports the claims of improved efficiency."}, "weaknesses": {"value": "•\tThe results lack statistical rigor. Without confidence intervals or similar measures, it is difficult to assess the significance of the reported differences.\n\n•\tCompared to techniques that employ only caching or quantization, the observed speed-up is relatively modest and, in some cases, comes at the cost of performance degradation."}, "questions": {"value": "•\tHow is the speed-up calculated?\n\n•\tWhy is the combined approach not achieving greater speed-up compared to individual techniques? For example, PTQ4DiT reports a 10x speed-up and Learn-to-Cache achieves 6.3x. Why doesn’t the combined method exceed 12.7x?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "owJVubafed", "forum": "AH7hbA7Zkk", "replyto": "AH7hbA7Zkk", "signatures": ["ICLR.cc/2026/Conference/Submission2723/Reviewer_gdd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2723/Reviewer_gdd2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950731426, "cdate": 1761950731426, "tmdate": 1762916344548, "mdate": 1762916344548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper find two key challenges that degrade performance when combining quantization and cache. To address these issues, the authors propose a temporal-aware parallel clustering and a variance compensation strategy. Higher speedup is achieved with image quality slightly hurted."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. First paper I'm aware of to combine quantization and cache for image generation.\n2. The proposed Temporal-Aware Parallel Clustering is interesting. \n3. The ablation study is detailed and validate the effectiveness of proposed approaches."}, "weaknesses": {"value": "1. Section 3.2 (Variance Compensation) lacks novelty. The approach appears almost identical to that in [1] (which the authors cite), making it difficult to count as a separate contribution.\n\n2. Why not perform quantization calibration before applying the cache? This way, the cache could better utilize the output of the quantized layers to decide what to store, and it would also avoid the calibration data issues mentioned.\n\n3. The chosen quantization method is relatively old. Combining the proposed cache mechanism with newer DiT quantization methods, such as SVDQuant[2] or ViDiT-Q[3], would strengthen the paper's impact.\n\n4. Experiments were only conducted on DiT-XL/2, showing a lack of generalizability. Experiments on more recent models like FLUX or PixArt would further enhance the paper's persuasiveness.\n\n5. The reported speedup ratios lack empirical validation. The authors didn't describe how the speedup is measured. Previous quantization works like Q-diffusion and PTQD did not provide latency results. Clearly, the actual performance often falls short of theoretical claims—for example, MixDQ[4]'s tests show that W4A8 reduces VRAM usage by 3x but only achieves a 1.45x speedup. Yet, without any explanation, the authors claim that W8A8 can deliver a 2x speedup and W4A8 a 2.5x speedup. Therefore, I have reasonable doubts about the reported speedup of the proposed approach.\n\n[1] Timestep-Aware Correction for Quantized Diffusion Models\n\n[2] Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models \n\n[3] ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation \n\n[4] MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization"}, "questions": {"value": "Please see the weaknesses above, and:\n\n1. Can the authors explicitly clarify the difference between their approach in Section 3.2 and the one presented in [1]?\n\n2. Can the authors provide results for the \"quantize-first, then-cache\" pipeline? (Perhaps also showing its performance when combined with Variance Compensation)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YfjPw4EkNi", "forum": "AH7hbA7Zkk", "replyto": "AH7hbA7Zkk", "signatures": ["ICLR.cc/2026/Conference/Submission2723/Reviewer_yPDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2723/Reviewer_yPDm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989440331, "cdate": 1761989440331, "tmdate": 1762916344376, "mdate": 1762916344376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}