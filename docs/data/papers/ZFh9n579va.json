{"id": "ZFh9n579va", "number": 23974, "cdate": 1758351248100, "mdate": 1763061077935, "content": {"title": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment", "abstract": "Single-view RGB model-based object pose estimation methods achieve strong generalization performance but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. To address these challenges, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated views and generalizes to unseen objects. The contributions of this work are threefold. First, leveraging powerful, frozen features from a foundation model, AlignPose iteratively minimizes the discrepancy between rendered and observed images across multiple viewpoints, enforcing geometric consistency without object-specific training. Second, robust handling of noisy inputs is achieved by aggregating pose candidates from an arbitrary single-view pose estimator via 3D non-maximum suppression. Third, extensive experiments on three BOP benchmarks (YCB-V, T-LESS, ITODD-MV) show AlignPose sets a new state of the art, especially on challenging industrial datasets where multiple views are readily available in practice.", "tldr": "This work presents a multi-view 6D object pose estimation method that generalizes to unseen objects through a novel multi-view optimization approach based on DINOv2 image features.", "keywords": ["6D pose estimation", "multi-view", "pose refinement", "foundation models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/05f9af927350b88b1881593ac51d47af521d4454.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AlignPose, a new method for generalizable multi-view 6D pose estimation. \nGiven pose candidates from an off-the-shelf single-view estimator, AlignPose aggregates them via 3D Non-Maximum Suppression (NMS) in a common coordinate system. \nThen the poses are refined through multi-view feature-metric alignment. \nThe refinement minimizes a robust multi-view feature-metric cost function between DINOv2 features from rendered 3D model projections and observed images across calibrated views using Levenberg-Marquardt optimization. \nAlignPose achieves state-of-the-art performance on three BOP datasets, outperforming previous multi-view methods by leveraging frozen foundation model features for zero-shot generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "++ The performance gains on YCB-V, T-LESS, and ITODD-MV demonstrate the effectiveness of AlignPose's multi-view refinement strategy over existing methods.\n\n++  This paper introduces a straightforward adaptation of FoundPose's feature-metric refinement to a multi-view setting, integrated with 3D NMS, and demonstrates its effectiveness with promising results."}, "weaknesses": {"value": "-- The work lacks an ablation study to justify the choice of the robust cost function, including a comparison with alternatives and an analysis of its hyperparameters.\n\n-- The experimental results lack data on time or speed. Analyzing the runtime is essential for understanding the practicality of this method.\n\n-- The 3D NMS method used in this paper lacks a comparison with the translation-based 3D NMS in FreeZev2 [a].\n\n-- The methodological innovation is somewhat limited. The multi-view feature-metric refinement approach presented here is essentially a straightforward extension of the single-view feature-metric refinement method in FoundPose and, therefore, lacks significant innovation.\n\n[a] Accurate and efficient zero-shot 6D pose estimation with frozen foundation models. Caraffa et al., 2025."}, "questions": {"value": "-- What is the rationale for selecting the current three BOP datasets (YCB-V, T-LESS, ITODD)? Two additional industrial datasets from the BOP-Industrial benchmark, IPD and XYZ-IBD, also provide multi-view evaluation data and appear suitable for assessing the method proposed in this paper. It would be valuable to include an evaluation on these datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmBzah7Vwa", "forum": "ZFh9n579va", "replyto": "ZFh9n579va", "signatures": ["ICLR.cc/2026/Conference/Submission23974/Reviewer_8pwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23974/Reviewer_8pwM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394007030, "cdate": 1761394007030, "tmdate": 1762942880199, "mdate": 1762942880199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We decided to withdraw this submission in order to make substantial revisions before resubmitting. We thank the reviewers for their time and constructive feedback."}}, "id": "sIBZQCqmj0", "forum": "ZFh9n579va", "replyto": "ZFh9n579va", "signatures": ["ICLR.cc/2026/Conference/Submission23974/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23974/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763061077291, "cdate": 1763061077291, "tmdate": 1763061077291, "mdate": 1763061077291, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AlignPose, a generalizable multi-view 6D object pose estimation method that does not require object-specific training. Instead of relying on single-view predictions or category-specific models, AlignPose aggregates pose hypotheses from arbitrary single-view estimators and refines them by minimizing a multi-view feature-metric alignment loss using deep learning features. The method leverages 3D non-maximum suppression to consolidate pose candidates across views and performs optimization to enforce geometric consistency across all images. Experiments on YCB-V, T-LESS, and ITODD datasets show that AlignPose achieves state-of-the-art performance in unseen object pose estimation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces AlignPose, a refinement method for unseen object pose estimation. It optimizes a consistent object pose in the world frame, jointly utilizing initial pose estimates from multiple views.\n\nAlignPose introduces a multi-view feature-metric alignment loss with non-maximum suppression, which optimizes the object pose by aligning rendered object features with real images.\n\nThe refined pose is obtained by using a Levenberg-Marquardt optimization algorithm, ensuring the robustness of the refinement.\n\nThe method achieves state-of-the-art results on multiple datasets, including YCB-V, T-LESS, and ITODD. It outperforms previous refinement approaches in unseen object pose estimation."}, "weaknesses": {"value": "The problem formulation is not clear enough. To my current understanding, the authors decompose the object pose $T_{CO}$ into two transformations, $T_{CW}$ and $T_{WO}$. This is a bit confusing since we often assume that the world frame and object frame are aligned in object pose estimation. Otherwise, it is unclear how to define the world frame beyond the object frame. A more detailed explanation would be important to improve clarity and help readers better understand the problem setup.\n\nThe comparisons appear to be unfair. The authors assume the ground-truth camera pose is available, meaning that $T_{CW}$ is known. This would significantly simplify the object pose estimation and make the comparisons with other methods unfair. Moreover, in real applications, the camera poses are often unknown or noisy. For instance, to get these poses, one needs to run some algorithms such as colmap and VGGT. The results are not always accurate.\n\nThe method relies on object meshes, which makes it inapplicable in some scenarios where the object meshes are unavailable. A discussion regarding this limitation is missing, but important. \n\nThe presented multi-view alignment method is a bit straightforward and quite similar to bundle adjustment. Given the object mesh, many alternatives could be used. For example, using the initial object pose to render an RGB image and aligning the rendered image with the query image to refine the pose."}, "questions": {"value": "How to use the initial object pose in practice?  In Eq.2, which transformation stands for this pose? I guess $T_{WO}$ in this equation is derived from the initial object pose. Is it correct?\n\nIn Eq.3, a confidence score is computed, but how to use this score in the experiments is missing. How to use this score to facilitate the pose refinement? Is it important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EVV3NufgzP", "forum": "ZFh9n579va", "replyto": "ZFh9n579va", "signatures": ["ICLR.cc/2026/Conference/Submission23974/Reviewer_B6GB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23974/Reviewer_B6GB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817730711, "cdate": 1761817730711, "tmdate": 1762942879419, "mdate": 1762942879419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to estimate the poses of objects in the scene given calibrated images from multiple view points. The method involves first getting initial coarse estimates of object poses from each input image by running an off-the-shelf object pose estimator, and then refine those estimates using a bundle adjustment technique that searches for 6DOF object pose estimates that ensures consistency between features registered to 3D from multiple viewpoints. The method achieves state-of-the-art performance on this refinement task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors perform rigorous evaluation on multiple datasets, and find that their method consistently outperforms competing approaches. \n2. The presentation of the method is easy to understand with the equations that have been written. The approach seems like a reasonable thing to try. \n3. Strong results are shown on both seen and unseen object categories, showing that the method can generalize well given initial coarse estimates of object poses are in the ballpark of the right answer."}, "weaknesses": {"value": "1. This idea of using DINO feature space metrics for bundle adjustment has already been explored in other contexts like structure from motion (see [1, 2] below). In fact, it seems like the equations in that paper are more or less equivalent to what is proposed here. I don’t think that paper is cited. \n2. The contribution seems a bit narrow here. I think this idea has been known for a while now, and is an integral part of standard bundle adjustment pipelines.  It’s just something that one would do by default for object pose refinement if they are aware of the general pose estimation literature. So I don’t think it’s adding a lot of value to write an entire paper showing that it can work well in this setting. \n\n[1] DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model\n[2] Pixel-Perfect Structure-from-Motion with Featuremetric Refinement"}, "questions": {"value": "1. My main question is that what’s really new in this paper apart from applying a well known feature space bundle adjustment technique to the multi-view 6DOF object pose refinement problem? \n2. Did the authors find that the algorithm had to be changed in a crucial way for it to work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qSku7Cmljc", "forum": "ZFh9n579va", "replyto": "ZFh9n579va", "signatures": ["ICLR.cc/2026/Conference/Submission23974/Reviewer_1kfb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23974/Reviewer_1kfb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939928053, "cdate": 1761939928053, "tmdate": 1762942878815, "mdate": 1762942878815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper deals with object pose estimation given multi-view RGB inputs. The method is based on frozen features from vision foundation models (such as DINOv2). The method performs optimization based on feature-metric refinement. Then it aggregate multiple views by 3D Non-Maximum Suppression (NMS). The method is evaluated on BOP benchmarks (YCB-V, T-LESS, ITODD-MV) and shows-  improvements over single-view methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The method shows great generalization to unseen objects since it does not need training on unseen objects.\n- The performance is much stronger than baselines such as CozyPose on BOP benchmarks.\n- The paper presentation is good."}, "weaknesses": {"value": "- It seems that the paper is only leveraging the features from existing vision foundation models (e.g. DINOv2) to do LM optimization of feature loss. The paper is not training any new models. This is OK if the method works, but it would seem that the contribution of this paper is limited.\n- It would be interesting to see how the performance would be with different vision foundation models.\n- There are some other related works that also uses LM optimization on visual features to do object pose estimation (e.g. https://arxiv.org/abs/2104.00633) and are not compared against or mentioned in this paper. Not sure how much novelty this paper contains if considering other related works."}, "questions": {"value": "- I wonder how sensitive the method is due to the error of camera calibration (intrinsic and extrinsic)?\n- Is there any failure cases? It would be great if some failure cases are shown and analyzed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dD4MGo4JJ4", "forum": "ZFh9n579va", "replyto": "ZFh9n579va", "signatures": ["ICLR.cc/2026/Conference/Submission23974/Reviewer_HURM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23974/Reviewer_HURM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066927810, "cdate": 1762066927810, "tmdate": 1762942878563, "mdate": 1762942878563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}