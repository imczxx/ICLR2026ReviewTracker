{"id": "4BjGVZ7Bxn", "number": 22008, "cdate": 1758324758547, "mdate": 1759896891210, "content": {"title": "AutoLibra: Agent Metric Induction from Open-Ended Human Feedback", "abstract": "Agents are predominantly evaluated and optimized via task success metrics, which are coarse,\nrely on manual design from experts, and fail to reward intermediate emergent behaviors.\nWe propose AutoLibra, a framework for agent evaluation, that transforms open-ended\nhuman feedback e.g. “If you find that the button is disabled, don’t click it again”, or “This\nagent has too much autonomy to decide what to do on its own” into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding\nfeedback to an agent’s behavior, clustering similar positive and negative behaviors, and\ncreating concrete metrics with clear definitions and concrete examples, which can be used for\nprompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate\nthe alignment of a set of (induced) metrics with open feedback: “coverage” and “redundancy”.\nThrough optimizing these meta-metrics, we experimentally demonstrate AutoLibra’s ability\nto induce more concrete agent evaluation metrics than the ones proposed in previous\nagent evaluation benchmarks and discover new metrics to analyze agents. We also present\ntwo applications of AutoLibra in agent improvement: First, we show that AutoLibra\nserve human prompt engineers for diagonalize agent failures and improve prompts iterative.\nMoreover, we find that AutoLibra can induce metrics for automatic optimization for agents,\nwhich makes agents improve through self-regulation. Our results suggest that AutoLibra is a\npowerful task-agnostic tool for evaluating and improving language agents.", "tldr": "We propose a new method for inducing metrics for evaluating and improving agents from open-ended human feedback.", "keywords": ["Agent", "Evaluation", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76a3022db977cc7f7b74ddd094c0dd82e29288fb.pdf", "supplementary_material": "/attachment/d76b54bca92ba2056e59305eb49160506cfbe8e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel agent evaluation framework, AutoLibra, that leverages human feedback to automatically induce evaluation metrics which can be used in LLM-as-a-Judge. AutoLibra incorporates a meta-evaluation to evaluate the quality (coverage and redundancy) of induced metric. The paper also demonstrates using AutoLibra to iteratively improve agent systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written.\n\n2. The paper studies an important problem in agent evaluation and proposes a interesting and promising solution.\n\n3. The paper propose a novel perspective of using human feedback to induce metrics for agent evaluation.\n\n3. Most design choices, such as which model to choose for each step, are well justified.\n\n4. The paper demonstrates clear effectiveness of AutoLibra in improving agent systems."}, "weaknesses": {"value": "1. AutoLibra relies on feedback from end users or experts. This creates a major concern about the practical use of AutoLibra. Real-world feedback can be noisy, ambiguous, or high-level. Please clarify the minimum quality and granularity required. Please also compare the overhead of collecting expert feedback against alternatives (e.g., expert‑designed metrics).\n\n2. The construction and evaluation details can be elaborated to strengthen the soundness of this work (see Questions)."}, "questions": {"value": "1. Line 212: What does the “similar results” mean? Which metrics are used to assess similarity? Why does the model used to rate agent trajectories have to have similar results as o3-mini high (the model used in the step of behavior clustering).\n\n2. Line 265: Why do you choose 20 different sets? What if users of AutoLibra uses more or fewer number of sets? How should users set this parameter in practice?\n\n3. Line 319-320: Why is the complexity of the evaluation of social interaction considered high?\n\n4. Section 3.3 and Table 1 show the step-wise agreement scores between AutoLibra and human annotation. How do these numbers translate to end-to-end agreement about the agent performance? Please report the end-to-end agreement scores in cases where such evaluation is necessary?\n\n5. Line 334-335: How were the failure categories proposed by authors/experts? Were they developed independently of the AutoLibra‑induced metrics?\n\n6. In general, what specifications (quality, granularity, quantity, annotator expertise) are needed to achieve the reported performance? How does AutoLibra deal with ambiguous feedback that has multiple possible ways to interpret, or high-level feedback that are hard to ground (e.g., “The agent doesn’t meet my requirements”)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CYeWehOJMI", "forum": "4BjGVZ7Bxn", "replyto": "4BjGVZ7Bxn", "signatures": ["ICLR.cc/2026/Conference/Submission22008/Reviewer_HqAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22008/Reviewer_HqAx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965587520, "cdate": 1761965587520, "tmdate": 1762942017374, "mdate": 1762942017374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoLibra, a framework which converts unstructured human feedback on agentic trajectories into interpretable eval metrics. This paper would be stronger if a more comprehensive validation of LLM grader performance was provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The loop to extract metrics and automatically measure coverage is novel (from what I have seen). \n- These metrics should help the autograders score new cases (pos and negative examples is a nice touch).\n- validated approach on 20% held out set.\n- metrics as a function of observed trajectories is a very cool idea."}, "weaknesses": {"value": "- only 118 trajectories human labelled, with each trajectory only taking 5 mins. This is quite a small sample size IMO. It would be good to see this methods applied to and validated against a larger set.\n- This method heavily relies on the LLM performance. The paper should dedicate more ablation studies and effort into current LLM proficiency at this task. \n- given the importance of LLM performance at this task I think a method such as the one detailed here https://arxiv.org/abs/2507.03772, which allows a more detailed investigation into LLM behaviours, should be explored.  \n- I think you should cite scalable oversight work as quite related e.g.works such as Constitutional AI (Bai et al., 2022), RLAIF/AI feedback (e.g., Lee et al., 2023/Anthropic) and maybe even some control work. \n- Figure 4 is hard to interpret. e.g. annotation lines and plot lines looking very similar. Is this actual data?"}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dHy8iCHMHN", "forum": "4BjGVZ7Bxn", "replyto": "4BjGVZ7Bxn", "signatures": ["ICLR.cc/2026/Conference/Submission22008/Reviewer_uF3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22008/Reviewer_uF3E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997671865, "cdate": 1761997671865, "tmdate": 1762942017134, "mdate": 1762942017134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides an evaluation framework for LLM agents that induces fine-grained behavioral metrics from open-ended human feedback, then uses LLM-as-a-Judge to score trajectories, and meta-metrics (Coverage, Redundancy) to select/optimize the metric set. \n\nPipeline is as far as I understood is: feedback grounding --> behavior clustering --> metric induction --> LLM-judging --> meta-evaluation (Coverage/Redundancy).\n\nExperiments are conducted on CoGym, Sotopia, WebArena, WebVoyager, and Baba-is-AI and they find >0.85 human agreement for grounding, judging, and meta-evaluation. Results show Coverage peaking at N=6–10 induced metrics, with Coverage reaching 88% on WebArena/WebVoyager. They also show 20% improvement on Baba-is-AI when optimizing using induced metrics rather than success rate directly."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Produces fine-grained, actionable behavioral metrics (e.g., Access Barrier Handling, Error Recovery and Adjustment, Navigation Accuracy).\n\n- Discovers failure modes that were not captured in pre-existing benchmark taxonomies (e.g., WebVoyager: Query/Search Strategy Efficiency (approx 7%), Final Output Quality ( approx18%)).\n\n- Demonstrates self-regulated improvement: optimizing induced metrics yields ~20% success gain on Baba-is-AI without directly optimizing success rate.\n\n- Step-wise human validation is consistently >0.85, which is a strong reliability signal for an LLM-based pipeline."}, "weaknesses": {"value": "In general there are not too many strong weaknesses:\n\n- LLM dependence and limited visibility in clustering:\n\nPlease report clustering stability: fix the optimal number of metrics (N), run at least three different random seeds, and quantify how similar the resulting metric sets are (for example, by matching clusters and comparing overlap, or by providing a small human-judged semantic comparison across samples).\n\n- Generalization not demonstrated across domains:\n\nA small cross-dataset test would clarify this. For example: induce metrics on WebArena, then evaluate Coverage and Redundancy on held-out feedback from WebVoyager and CoGym without re-introducing the metrics.\n\nIf any of these experiments or measurements already exist in the appendix, please point me to the exact section or figure."}, "questions": {"value": "Questions for Authors:\n\n- Can you include (or point to) a cross-dataset transfer evaluation?\n\n- Can you provide order-of-magnitude compute/token/$ estimates for one induction loop? It seems pretty costly to me. \n\n- How stable is the induced metric set across random seeds for clustering/induction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmQjOsB6EN", "forum": "4BjGVZ7Bxn", "replyto": "4BjGVZ7Bxn", "signatures": ["ICLR.cc/2026/Conference/Submission22008/Reviewer_T2SJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22008/Reviewer_T2SJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762351934872, "cdate": 1762351934872, "tmdate": 1762942016791, "mdate": 1762942016791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoLibra, a framework that converts open-ended human feedback on agent trajectories into a set of interpretable, fine-grained behavioral metrics, then uses these metrics with LLM-as-a-Judge for evaluation and for agent improvement. \n\nThe pipeline includes (i) grounding free-form feedback to concrete behaviors, (ii) clustering them into metric definitions with examples, and (iii) assessing a metric set with two meta-metrics—coverage (alignment with observed feedback) and redundancy (non-overlap/spuriousness). The authors also demonstrate applications to prompt engineering and automatic self-regulation/optimization of agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Timely problem & clear formulation. Moving beyond task-success rates to behavioral evaluation is important for LLM agents interacting with humans. The coverage/redundancy meta-metrics give a principled way to select metric sets that actually reflect what people say they want. \n\n2. Interpretability & reusability. Metric definitions with examples are human-legible and reusable across tasks, which is valuable for human–AI interaction workflows (e.g., aligning internal diagnostics with user-visible rubrics).\n\n3. Closed-loop potential. Showing that the same induced metrics can inform both evaluation and improvement (prompt iteration or self-regulation) helps bridge the usual gap between “LLM as a judge” and tangible agent gains.\n\n4. Practicality. The approach is comparatively lightweight (no bespoke simulators or dense labels) and could plug into existing logs/feedback pipelines."}, "weaknesses": {"value": "1. Judge-dependence & circularity. Every stage (grounding, clustering, judging) leans on LLMs. Without strong cross-judge and human-only checks, it risks metric drift or Goodhart effects (agents optimize to a judge’s quirks rather than human satisfaction).\n\n2. Sensitivity of the meta-metrics. Coverage and redundancy depend on how “aspects” are extracted and granularized; small parsing or clustering changes could alter the frontier. The paper would be stronger with sensitivity analyses (judge model, prompt, seed, granularity).\n\n3. Causal link to human value. It remains unclear when improving these induced metrics causally improves human satisfaction or task utility, especially in socially nuanced settings; correlations may not hold under domain shift.\n\n4. Generalization & robustness. How stable are the induced metrics across tasks, annotator populations, and time? Are they robust to verbose/hedging outputs or to adversarial behaviors that superficially satisfy rubrics?\n\n5. Cost & reproducibility details. A fuller accounting of inference cost, annotation effort, and step-by-step prompts would help others reproduce and operate this in production settings."}, "questions": {"value": "1. Missing ralted work. Your pipeline appears sensitive to the evaluation language and judge behavior. Building on findings that evaluation rubrics/languages can themselves shape agent behavior, please (a) discuss how your approach relates to and differs from Wang et al., 2025 (ICML) [1], and (b) provide robustness evidence that induced metrics remain valid under judge swap (different families/sizes), prompt/rubric rephrasings, and formatting/verbosity changes. Can you show that optimizing your metrics causally improves human satisfaction across such perturbations (anti-Goodhart tests, cross-judge A/Bs, and human-only checks)?\n\n2. Robustness to gaming and verbosity. Do agents that learn to be longer, more self-reflective, or apologetic inflate metric hits without better task outcomes? \n\n3. Noisy/contradictory feedback and metric stability. How stable are aspect extraction, clustering, coverage, and redundancy under label noise, annotator disagreement, and imbalanced feedback distributions? \n\n4. If metrics are induced on domain A and evaluated on domain B (or earlier vs. later time slices), how do coverage, redundancy, and downstream gains degrade? Do you have a principled refresh/update policy that avoids drift without overfitting to recent logs?\n\n\n[1] Wang, Z., Zhang, Z., Fang, F. &amp; Du, Y.. (2025). M$^3$HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality. Proceedings of the 42nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 267:65429-65448 Available from https://proceedings.mlr.press/v267/wang25el.html."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AWm2gkp2bJ", "forum": "4BjGVZ7Bxn", "replyto": "4BjGVZ7Bxn", "signatures": ["ICLR.cc/2026/Conference/Submission22008/Reviewer_GS7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22008/Reviewer_GS7e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762888979571, "cdate": 1762888979571, "tmdate": 1762942016553, "mdate": 1762942016553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}