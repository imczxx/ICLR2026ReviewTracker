{"id": "Mf8envGWlT", "number": 14272, "cdate": 1758231739366, "mdate": 1759897379620, "content": {"title": "Confidence is Not Competence", "abstract": "Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases: pre-generative assessment and solution execution. A simple linear probe decodes the internal \\enquote{solvability belief} of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge; although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence–competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture: a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.", "tldr": "We show that large language models separate confidence from competence through a “two-brain” architecture.", "keywords": ["Large Language Models", "Mechanistic Interpretability", "Representation Engineering", "Reasoning", "Metacognition"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/013243f3859a2568205be57840a1330b8f6b1804.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work tried to answer an intriguing research question through mech: why does an LLM's internal cognitive state (i.e. confidence level, a model's belief in its own problem-solving ability) appear decoupled from its final actions (i.e. actual competence)?  \n\nThe authors try to answer this question by first building a dataset which contains non-biased solvable/unsolvable reasoning problems (across math, logical, planning and coding domains).  \nThe confidence/solvability belief is modelled by probing the internal feature states (at the end of input prompt) to the solvability. Authors find that models' belief is actually largely linearly decodable - a linear probe can predict the internal state to its final solvability at 70% accuracy. CKA heatmaps and visualizations further confirmed the linear seperability of solvable/unsolvable states. \n\nThe paper also investigate whether this linear solvability belief direction can causally affect model's capability. They apply linear regression probe weights as a steering vector to intervene the model's internal feature states (thus to increase models' confidence level), finding that the model cannot improve its task accuracy in any domain by manipulating its solvability belief."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well written. The authors provide clear and good concept definitions and motivations. The research question is well defined, and the overall presentation makes the paper easy and enjoyable to read.\n\n2. The research problem itself is intriguing and of broad relevance. The work offers insights that can benefit not only researchers studying reasoning capabilities in large language models but also those in AI safety, mechanistic interpretability, and efficiency. The conclusions are thought-provoking and contribute meaningfully to the understanding of reasoning behavior.\n\n3. The methodology is solid and thoughtfully designed. I particularly appreciate the authors’ effort to isolate the genuine signal of solvability confidence by ruling out superficial cues such as formatting heuristics, domain-specific bias, and length bias. The causal intervention experiments for steering confidence levels are especially interesting and add depth to the analysis."}, "weaknesses": {"value": "W1. Missing Appendix.  Line 377 mentioned the reverse causal intervention study (solved -> unsovled) in Appendix C, but the Appendix is not attached to the main paper. \n\nW2. Lack of in-response confidence modelling.  When humans solve challenging problems, we often begin uncertain about a problem’s solvability. Instead, we gradually build confidence or unsolvability-awareness through iterative attempts and self-correction. I believe this dynamic evolution of confidence supports the vision of test-time scaling, where LLMs are given opportunities to explore, retry, and refine their reasoning until reaching a solution.\n\nHowever, the paper appears to model solvability-belief only as a static function of the input prompt (as shown in Table 2), rather than as a dynamic belief state that evolves during generation. This simplification undermines the validity of some core claims about modeling reasoning confidence. A valuable follow-up question would be whether steering the model’s during-generation belief state can causally influence its problem-solving capability?\n\n\nW3. The description of the “competence subspace” in Section 5.1 is vague and lacks sufficient explanation. More broadly, the Assessment Brain and Execution Brain analogy introduced in the paper is not clearly defined, which leads to potential confusion about their distinct roles and interactions. (See Q1 and Q2.) \n\n---\n*Typo*: \n\n1. Line 98: Open-R1 Math 220k citation is missing\n\n2. Line 390,  1competence -> competence\n\n3. Line 400, the single quotation mark around `unconfident`"}, "questions": {"value": "Q1. In Section 5.1, the pre-generative Belief states are clearly defined and discussed earlier in the paper. However, the definition and collection procedure for the Competence states remain unclear. Are these states extracted as averaged feature embeddings from the intermediate steps of the chain of thought? For long reasoning traces, are the embeddings averaged across all tokens, or is only the final token representation? \n\nQ2. Interpretation of belief subspace and action subspace dimensionality. Does this mean that, at the beginning of a model’s response (i.e., when processing the input question), the representation space exhibits high entropy or variance, reflecting the model’s uncertainty and multiple potential solution trajectories (or maybe only various potential ways of verbalizing an answer)? As the chain-of-thought unfolds, does the representation gradually collapse into a lower-entropy region as the model commits to a specific reasoning path or verbalization pattern?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2vbTnf6pMQ", "forum": "Mf8envGWlT", "replyto": "Mf8envGWlT", "signatures": ["ICLR.cc/2026/Conference/Submission14272/Reviewer_wCkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14272/Reviewer_wCkL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761023895876, "cdate": 1761023895876, "tmdate": 1762924723858, "mdate": 1762924723858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a geometric and dynamic framework for understanding how large language models internally represent and transition between belief and competence states.Through linear probing, manifold visualization, dimensionality analysis, and trajectory tracking, it reveals that: The model’s reasoning unfolds as a geometric journey from high-dimensional uncertainty to low-dimensional execution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The overall structure of the paper is clear and logically coherent. The authors introduce their research question in a focused way and progressively build their arguments.\n- Methodologically, the study follows the established paradigm of mechanistic interpretability, employing linear probes to decompose representations into interpretable components. By linearly separating correct and incorrect belief states with non-trivial reasoning, and further visualizing their non-overlapping distributions, the paper demonstrates that high-dimensional belief encodings can indeed be expressed in a linear representational space.\n- Furthermore, by combining both static and dynamic analysis, the authors show that, during reasoning, the model’s hidden states gradually transition from a belief-oriented mode to a competence-oriented one."}, "weaknesses": {"value": "- Figure 3 lacks sufficient clarity. \nThe first step involves constructing linear probes for each layer and each problem; however, it is unclear which layer’s hidden states were used to generate the visualization in this figure. \n\nSince four different models were tested, it would be informative to specify whether the trends were consistent across models, and which specific model’s results are displayed in Figure 3. \n\nMoreover, if the figure combines hidden states from all layers across over 800 problems, the number of scatter points appears implausibly small. Is there an undersampling or incomplete visualization?\n\n- A similar issue arises in Figure 2. \nThe models have different number of layers—for instance, Figure 1 shows that Mistral has over 36 layers, yet Figure 2 displays results for only about 28 layers. Clarification on whether layers were omitted, averaged, or truncated is recommended.\n\n-  line 110 has incomplete citation\n\n- Appendix is mentioned in main text but appendix is missing"}, "questions": {"value": "- In Figure 4 (left panel), which layer’s representations are used for comparing belief and competence states? Do all tested models exhibit the same pattern of dimensional collapse, or is this result model-specific?\n\n- Regarding the Trajectory Projection Experiment, while the figure effectively illustrates the dynamic transition from belief to inference, in Figure 4 (right panel) it seems that during the early belief phase, both the assessment and execution subspace fits rise at nearly the same rate. Does this imply that both subspaces initially accumulate information in parallel, or could it indicate a representational overlap before the divergence between belief and competence occurs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "98IlmDLAwF", "forum": "Mf8envGWlT", "replyto": "Mf8envGWlT", "signatures": ["ICLR.cc/2026/Conference/Submission14272/Reviewer_1vxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14272/Reviewer_1vxc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798073164, "cdate": 1761798073164, "tmdate": 1762924723483, "mdate": 1762924723483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the documented disconnect between LLM confidence and competence in solving tasks using mechanistic interpretability tools. Across model families (Llama, Qwen, Gemma, Mistral) and task domains (math, logic, coding, planning), the authors argue that models maintain an internal \"solvability belief\" that is linearly decodable (in particular, showing that linear probes predict decently well, but non-linear probes offer no improvement), but these belief states have little to no causal effect on the model performance. In particular, the authors illustrate that steering interventions can successfully flip the internal belief states but these interventions produce no change in the final accuracy of the models across the domains considered. The authors offer a geometric explanation for these findings, arguing that beliefs states occupy a high-dimensional manifold, whereas execution evolves on a lower-dimensional manifold."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper tackles a important question for AI safety and deployment. Understanding what drives the confidence-competence gaps has consequences for safety (models that are confidently wrong could be deployed in more dangerous ways than those that appropriately communicate their own uncertainty) and this has obvious ramifications for model trust and deployment. Most of the work (that I know of) thinking about this question focuses on model outputs, and it is valuable whether we can understand something about how model internals might produce this phenomenon.\n\n2)  The paper has an impressively exhaustive collection of experiments. The authors investigate models from four different families: Llama, Qwen, Gemma, and Mistral. By applying the mechanistic interpretability techniques across all of these models helps make the case that their findings are potentially general ---- they apply even across different model sizes, architecture details and training procedures. \nAdditionally, the authors also study multiple domains. One might worry about a paper that focused on any one of these domains alone ---  since each domain likely has different cognitive demands, it wouldn't be clear how to ex ante generalize results. The thoroughness of the authors addresses this concern.\n\n3) It is nice that the authors attempt to move beyond probing to intervening on model internals. This is nice for two reasons. First, causal intervention is the gold standard for understanding whether some model internal is functionally important. Second, in this context, the hypothesis of interest is causal: if internal belief causally drives competence, then changing the belief state should change the model's competence. A null result here (if it is valid) would be quite valuable."}, "weaknesses": {"value": "1) The paper argues that it studies the model's internal \"solvability belief.\" But the labels used to train the probes are the model's own zero-shot performance (Section 3.1). This label is misaligned ---- the probe is actually being trained to predict whether the model succeeds, not whether it believes it will succeed. As an example, the probe could therefore be learning things about the problem difficulty, heuristic markers of solvability that emerge early on the network. I do not see how the authors interpret their analysis as measuring beliefs of solvability given this choice of label. I would have found the analysis far more sensible if the model prompted the model to report its own confidence and then used those reports as the label for the probes.\n\n2) The paper's steering/intervention protocol seems circular. In particular, they train a logistic regression problem $P(Y = 1 | H) = \\sigma(w^T H + b)$. They then extract the vector $w$ and intervene by $H^\\prime = H + \\alpha w$, and claim success because the probe's predictions now change a lot across $H^\\prime$ vs. $H$. But this guaranteed by construction. In particular, $P(Y = 1 | H^\\prime) = \\sigma (w^T H + \\alpha \\| w\\|^2 + b)$. So for large alpha, you mechanically shift the logit towards a higher predicted probability. In other words, this doesn't validate to me that you've manipulated the model's beliefs -- rather it shows that you manipulated the probe's predictions.\n\n3) The tasks the authors study appear to be quite difficult for the model (e.g., Table 2) at baseline. For example, Math (Hard) only is solved correctly 8.4% of the time. Couldn't it be the case that the belief interventions don't affect performance simply because these problems are outside the reach of the models? I would have found this more convincing if the focus were on borderline problems --- in the 40-60% range. It's gives the intervention at least a plausible shot of working, making the null result more impressive.\n\n4) A key claim of the paper is that non-linear probes show no improvement over logistic regression, and therefore the belief state is fundamentally \"linear.\" At the same time, the training set only contains 846 examples. Couldn't the failure of nonlinear probes arise because of sample size? That is, non-linear models might perform poorly because they are too complex, and the logistic regression is just performing well because it is effectively regularized through the choice of simple function class. Some evidence towards this is that the authors note even the logistic regression doesn't perform all that well, and so they hedge by saying that belief is encoded in a noisy manifold. Altogether, I just don't see how the authors can rule out that this is driven by the small collection of examples they study.\n\n5) I found the paper to contain *a lot* of rhetorical flourish that made it difficult to read and made it feel like the authors are overselling. For example, phrases like: \"startling causal inertness,\" \"inescapable mechanistic reason,\" \"sharp cognitive collapse,\" \"definitive geometric explanation,\" \"the mystery is solved\" and on and on. The writing style of the paper obscures the actual results and technical steps taken by the authors."}, "questions": {"value": "See my previous discussion of the paper's weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wUmqHkwpbq", "forum": "Mf8envGWlT", "replyto": "Mf8envGWlT", "signatures": ["ICLR.cc/2026/Conference/Submission14272/Reviewer_6qjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14272/Reviewer_6qjP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932683116, "cdate": 1761932683116, "tmdate": 1762924723071, "mdate": 1762924723071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the claim that a model’s self-assessed confidence in its ability to perform a task has no causal relationship with its actual task competence. To test this hypothesis, the authors employ probing methods to manipulate the latent states of a LLM using intervention vectors. They demonstrate that while such interventions can alter the model’s expressed belief (confidence) in its success, they do not significantly affect its actual task performance.\n\nTo explain this phenomenon, the paper introduces the concept of a dimensional geometry disparity within the model’s representation space. Building on this, the authors propose the intriguing hypothesis that an LLM’s reasoning process is governed by two distinct but interacting subsystems—an assessment brain that evaluates likelihood of success, and an execution brain that carries out task reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a timely and important research topic, presenting a thorough and well-structured investigation. The proposed hypothesis provides a reasonable explanation for the observed results.\n\n- The writing has a clear narrative flow, and the authors make a commendable effort to build their central claim through a series of experiments. This storytelling approach helps readers follow the logical progression of the study and understand how each experiment contributes to the overall argument."}, "weaknesses": {"value": "- From Figure 1, I’m not fully convinced that there is a clear “climbing through” pattern across the model’s layers. The results appear rather noisy to me. Could the authors provide additional evidence or analysis to support this claim?\n\n- Many of the subsequent arguments rely on the initial claim in Section 3.2 — that linear probing accurately reflects an LLM’s internal beliefs. However, an accuracy of 70–75% seems insufficient to make this claim convincing. Is there a systematic way to validate or strengthen this connection?\n\n- While the paper attempts to draw a comprehensive conclusion by combining multiple claims, several of these claims have already been established in prior work. For instance, the idea that linear probing can capture the model’s final answer from intermediate latent states has been discussed before. This overlap somewhat weakens the novelty and contribution of the paper."}, "questions": {"value": "- On figure 1, model names should be on each subplot.\n\n- In Figure 2 and 3, How many data or different models are those figures based on? Is this observation consistent across different tasks and domain?\n\n- Is the steering vector mentioned in Line 310 identical across all questions, or is it computed separately for each question? Clarification would help in understanding the experimental setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VhB1HkgJOO", "forum": "Mf8envGWlT", "replyto": "Mf8envGWlT", "signatures": ["ICLR.cc/2026/Conference/Submission14272/Reviewer_rGGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14272/Reviewer_rGGg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096746698, "cdate": 1762096746698, "tmdate": 1762924722722, "mdate": 1762924722722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}