{"id": "wWvrC9oajI", "number": 19772, "cdate": 1758299195532, "mdate": 1759897020424, "content": {"title": "Video-Based Optimal Transport for Feedback-Efficient Offline Preference-Based Reinforcement Learning", "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL offers a promising alternative by learning reward functions from human feedback, but its scalability is hindered by the large amount of feedback required. Inspired by recent advances in Video Foundation Models (ViFMs), we present Video-based Optimal Transport Preference (VOTP), a semi-supervised preference learning framework that can learn effective reward functions from only a handful of preference labels. By leveraging optimal transport in the representation space of ViFMs for pseudolabeling, VOTP can utilize large amounts of unlabeled data for reward learning, substantially reducing the need for human supervision. Extensive experiments across locomotion and manipulation tasks show that VOTP outperforms existing PbRL methods under limited feedback. We further validate VOTP on real robotic tasks, demonstrating its ability to learn useful rewards with minimal human input.", "tldr": "A semi-supervised preference learning method that use optimal transport over embeddings of the video foundation model to perform pseudo-labeling.", "keywords": ["Preference-based Reinforcement Learning", "Offline Reinforcement Learning", "Feedback Efficiency", "Optimal Transport", "Video Foundation Models", "Semi-supervised Learning", "Robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd5d73ef565c6a3ae8545793b3aabfa1ac2ba386.pdf", "supplementary_material": "/attachment/0900ade87e89d9caa591eb55def3c30043c6aeb1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes video-based optimal transport preference (VOTP), a semi-supervised offline preference-based RL (PbRL) framework that addresses the high annotation cost of existing PbRL methods. VOTP uses video foundation models (ViFMs) to encode trajectory segments into latent representations, then applies optimal transport (OT) to compute alignments between a small set of labeled preference pairs and a large number of unlabeled pairs. Pseudo-preference labels are generated by aggregating preferences from labeled pairs weighted by their OT-based alignment strengths. These pseudo-labels, combined with the original labeled data, are used to train reward functions. The authors conducted experiments across three simulated domains (D4RL, MetaWorld, Robomimic) and two real robotic manipulation tasks to demonstrate that VOTP achieved performance comparable to policies trained with ground-truth rewards using only 5-10 labeled queries, outperforming baseline PbRL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of adapting optimal transport to compute similarities between trajectory representations for pseudo-labeling is a novel contribution to the preference-based RL field.\n2. The proposed method demonstrates sample efficiency in the low labeled data regime, outperforming the baselines in the main experiments."}, "weaknesses": {"value": "1. The claim that using a VOTP with ViFM provides robustness to nuisance variation is insufficiently validated, as the real-robot experiments are conducted in static, uncluttered environments that do not effectively test this claim. To validate the practical benefits of the ViFM encoder, experiments should have been conducted under more challenging, realistic conditions with controlled distractions, such as dynamic lighting, background clutter, or other moving objects.\n2. The baselines in the study, such as SURF (2022) and IPL (2023), are dated [1, 2]. The field has advanced, and the paper should compare against more recent methods such as Zhang et al., and Tu et al. [3, 4].\n3. The paper relies almost exclusively on scripted labels derived from ground-truth rewards. Although this is a common practice in benchmarking, it bypasses the critical challenge of handling noise, sub-optimality, and inconsistency inherent in real human preference data. Given the extremely low amount of labeled data considered, the impact of such challenges could be substantial.\n4. Ablation in Figure 3, intended to demonstrate the superiority of OT, is not convincing. The baselines (SIM-individual, SIM-mean) are naive. A more convincing comparison would involve comparing VOTP's OT-based aggregation against a stronger non-OT baseline, such as a similarity-based weighted average for the aggregation. As presented, it remains unclear whether the performance gain arises from the specific OT formulation or from any reasonable similarity-based aggregation.\n5. The method’s practicality is limited by the sensitivity of the preference threshold τ_P. As shown in Figure 5 and Table 5, this hyperparameter is highly sensitive and task dependent which harms its practical usage.\n\nReferences\n\n[1] Park, Jongjin, et al. \"SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning.\" International Conference on Learning Representations. 2022.\n\n[2] Hejna, Joey, and Dorsa Sadigh. \"Inverse preference learning: Preference-based rl without a reward function.\" Advances in Neural Information Processing Systems 36 (2023): 18806-18827.\n\n[3] Zhang, Zhilong, et al. \"Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation.\" The Twelfth International Conference on Learning Representations. 2024.\n\n[4] Tu, Songjun, et al. \"In-dataset trajectory return regularization for offline preference-based reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 20. 2025.\n\n[5] Xie, Saining, et al. \"Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[6] Madan, Neelu, et al. \"Foundation models for video understanding: A survey.\" arXiv preprint arXiv:2405.03770 (2024)."}, "questions": {"value": "1. There appears to be a terminological discrepancy regarding the trajectory encoder. The paper identifies S3D as a Video Foundation Model (ViFM), but the cited survey of Madan et al. [5, 6] does not list S3D under ViFMs. Could you clarify your definition of a ViFM and provide a justification for categorizing S3D as such in this work?\n2. To more convincingly support the proposed method, I raise the following questions connected to the weaknesses identified above.\n- Weakness #1: Could you provide experiments under more realistic, distracting conditions?\n- Weakness #2: Please provide more recent PbRL baselines for the convincing comparison.\n- Weakness #3: Could you include experiments with real human feedback to assess robustness to noise, inconsistency, or sub-optimality, which scripted labels cannot capture?\n- Weakness #4: Please add stronger non-OT baselines in the ablation (e.g., similarity-based weighted averaging) to more clearly demonstrate the specific contribution of OT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TcvZGbGQ6w", "forum": "wWvrC9oajI", "replyto": "wWvrC9oajI", "signatures": ["ICLR.cc/2026/Conference/Submission19772/Reviewer_qypy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19772/Reviewer_qypy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273792850, "cdate": 1761273792850, "tmdate": 1762931618591, "mdate": 1762931618591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VOTP, a semi-supervised offline PbRL framework that leverages optimal transport (OT) to generate pseudo-preference labels. The method utilized vision foundation models to encode video-based trajectory segments into latent representations, where OT computes the minimal mass-moving cost between labeled and unlabeled data distributions. This enables efficient preference propagation from a small number of labeled queries to a large pool of unlabeled queries. Extensive experiments across diverse domains (D4RL, Meta-World, Robomimic, and a real Sawyer robot) demonstrate strong empirical performance and highlight the method’s practical feasibility for real-world PbRL applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of integrating optimal transport with video foundation models for generating pseudo-preference labels is novel.\n2. The approach effectively reduces the amount of required human feedback.\n3. The inclusion of real-world robotic experiments is compelling and strengthens practicality of proposed method.\n4. The paper is well written and easy-to-read."}, "weaknesses": {"value": "1. Pessimism of Offline RL methods: In D4RL benchmark, most offline RL algorithms are known to achieve high performance even under completely wrong reward signals (i.e. constant or random) due to pessimism and survival instinct [1, 2, 3]. Therefore, it remains unclear whether the superior performance of the proposed method truly stems from its efficient pseudo-reward learning, or merely from the behavior cloning bias inherent in the backbone RL algorithm (i.e. IQL). The authors should include an experiment comparing performance under deliberately corrupted or constant rewards to disentangle these effects. \n\n2. Missing most recent baselines: The paper lacks comparisons with several recent and highly related offline preference learning methods, such as CPL [4], DPPO [5], LiRE [6], and APPO [7]. Including such baselines would allow for a more comprehensive evaluation and clarify where VOTP stands among the latest PbRL developments.\n\n3. Computational cost of optimal transport: Computing the optimal transport plan μ∗ can be expensive as it requires multiple iterations of the Sinkhorn algorithm for every unlabeled query. In addition, the computational complexity scales with the number of labeled queries, since the cost matrix grows proportionally to the labeled dataset size. This dependence on both labeled and unlabeled data size may limit the method’s scalability compared to typical semi-supervised approaches, whose cost is largely independent of labeled data quantity. Providing a quantitative runtime analysis would clarify the method’s practical feasibility. \n\n4. Quality of learned reward functions: As mentioned in weakness 1, it is unclear whether the learned reward estimator accurately captures the intend task objectives well. While the proposed pseudo-generated preferences are empirically shown to improve policy performance, there is no quantitative measure to assess how well the learned reward aligns with the true underlying preference structure. Incorporating a metric such as EPIC [8] or other statistical analysis would allow more thorough evaluation of reward consistency and fidelity.\n\n[1] Shin, D., Dragan, A., & Brown, D. S. Benchmarks and Algorithms for Offline Preference-Based Reward Learning. Transactions on Machine Learning Research.  \n[2] Li, A., Misra, D., Kolobov, A., & Cheng, C. A. (2023). Survival instinct in offline reinforcement learning. Advances in neural information processing systems, 36, 62062-62120.  \n[3] Choi, H., Jung, S., Ahn, H., & Moon, T. (2024, July). Listwise Reward Estimation for Offline Preference-based Reinforcement Learning. In International Conference on Machine Learning (pp. 8651-8671). PMLR.  \n[4] Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W. B., & Sadigh, D. Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning. In The Twelfth International Conference on Learning Representations.  \n[5] An, G., Lee, J., Zuo, X., Kosaka, N., Kim, K. M., & Song, H. O. (2023). Direct preference-based policy optimization without reward modeling. Advances in Neural Information Processing Systems, 36, 70247-70266.  \n[6] Choi, H., Jung, S., Ahn, H., & Moon, T. (2024, July). Listwise Reward Estimation for Offline Preference-based Reinforcement Learning. In International Conference on Machine Learning (pp. 8651-8671). PMLR.  \n[7] Kang, H., & Oh, M. H. Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning. In The Thirteenth International Conference on Learning Representations.  \n[8] Gleave, A., Dennis, M. D., Legg, S., Russell, S., & Leike, J. Quantifying Differences in Reward Functions. In International Conference on Learning Representations."}, "questions": {"value": "Please address the concerns described in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1MSiO75Wa9", "forum": "wWvrC9oajI", "replyto": "wWvrC9oajI", "signatures": ["ICLR.cc/2026/Conference/Submission19772/Reviewer_kNyD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19772/Reviewer_kNyD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906168142, "cdate": 1761906168142, "tmdate": 1762931617449, "mdate": 1762931617449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a semi-supervised preference learning framework that infers pseudo-preference labels for many unlabeled video segment pairs from a small set of labeled pairs. The method encodes segments with a video foundation model (ViFM) into latent embeddings, computes an optimal transport (OT) plan between labeled and unlabeled segment sets in the latent space under uniform marginals, aggregates labeled pairwise relations through the transport couplings with the preference score matrix to score unlabeled pairs, and then normalizes/thresholds these scores to produce pseudo-labels. A reward model trained on real + pseudo labels is used for offline RL. Experiments span D4RL, MetaWorld, Robomimic, and two real-robot tasks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has clear problem setup and writing; the proposed method and pipeline is easy to follow.\n\n2. The paper proposed a novel application of Optimal Transport to propagate preferences from limited supervision to unlabeled data to reduce the labeling costs without touching the pretrained video representation itself, inspired by prior work to use optimal transport in reward learning. \n\n3. The experiments covers a wide range of tasks with both simulation and real-robot evaluations.\n\n4. It also includes detailed ablations of design choices (video encoders, thresholds, number of preferences, etc.)."}, "weaknesses": {"value": "1. One of the very-related work to this paper is [1] which is also cited in this paper. This prior work also proposes to use optimal transport to compute the reward but it is based on a adapted visual representation learned via small number of preferences while this work is fixing the pre-trained representations and only propagating the preferences from labeled ones to unlabeled ones to optimal transport. Since one of the followup work [2] of [1] also uses learned reward function from a few preference labels to create pseudo labels, it is worth comparing to this baseline to see whether the proposed method outperforms this prior work. \n\n2. The evaluated tasks in robomimic, metaworld and real-world are relatively short horizon. The scalability of the proposed method to long-horizon, high-precision tasks (e.g., Square, Tool-Hang in robomimic) is unclear.\n\n3. One of the benefit to use large scale pretrained video encoder is their generalization capabilities across tasks. However, in this paper, it still assumes the labeled video segments and the unlabeled ones are in the same domain, which may weaken the motivation of using those pretrained video representations. \n\n\n[1] Tian, Thomas, et al. \"What Matters to You? Towards Visual Representation Alignment for Robot Learning.\" The Twelfth International Conference on Learning Representations.\n[2] Tian, Ran, et al. \"Maximizing alignment with minimal feedback: Efficiently learning rewards for visuomotor robot policy alignment.\" arXiv preprint arXiv:2412.04835 (2024)."}, "questions": {"value": "1. Assumption of the marginal. The paper has an assumption about optimal transport where the marginals are uniform. This assumption may not handle imbalanced or redundant segment distributions and am wondering if the authors have considered this situation or if they have encountered this in practice. \n\n2. More qualitative results: It would help the readers better understand how the pseudo labels are produced by providing some qualitative examples of the labeled segments and unlabeled ones. In Fig.1, the paper only has one example of preference matrix and transport plan but they are not connected with the corresponding video segments. Therefore, it would be more clear to show the corresponding segments too. For example,  you can show (i) labeled segments and their preference matrix, (ii) several unlabeled pairs, and (iii) corresponding transport plans/couplings and the computed score. \n\n3. Cost metric choice: the paper mentions Euclidean/cosine/others are potential metrics for the distance function but uses Euclidean in experiments.  If the authors can provide some explanations about why euclidean distance is chosen and include some examples of the distance with similar pairs of video segments vs different pairs, it would help readers understand more about the metrics and pretrained video representations.\n\n4. The paper is mostly focused on offline rl settings but the proposed way to learn reward function can also be used for online preference-based RL like PEBBLE[3]. I am curious to see if the inferred preference from a few labeled samples can be robust to the on-policy distribution shift in the sampled segment pairs. \n\n5. In the experiments, the paper has compared among (i) learning with task rewards (ii) learning with only small N real labels and (iii) learning with real + pseudo labels (N+M). It would also provide more information about the quality of the pseudo labels if the method is also compared against IQL learned from reward function trained with (N+M) real labels. This can be obtained from ground-truth task reward labeling and can serve as an upper bound. The gap is how far the pseudo labels are from the real labels. \n\n[3] Lee, Kimin, Laura M. Smith, and Pieter Abbeel. \"PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training.\" International Conference on Machine Learning. PMLR, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fNsv7xz3vv", "forum": "wWvrC9oajI", "replyto": "wWvrC9oajI", "signatures": ["ICLR.cc/2026/Conference/Submission19772/Reviewer_EaLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19772/Reviewer_EaLy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401789052, "cdate": 1762401789052, "tmdate": 1762931616647, "mdate": 1762931616647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}