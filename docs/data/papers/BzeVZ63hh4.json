{"id": "BzeVZ63hh4", "number": 13015, "cdate": 1758212725061, "mdate": 1759897470734, "content": {"title": "Overton Pluralistic Reinforcement Learning for Large Language Models", "abstract": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism (OP) addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single LLM to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps: 1) Similarity estimator training, which fine-tunes a Sentence Transformer for OP tasks to provide a more accurate coverage evaluation of the given responses; and 2) OP-GRPO training, which incorporates this similarity estimator into a carefully designed dual-reward system to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate that OP-GRPO achieves a \"Small models, Big perspective coverage\" effect: our trained Qwen2.5-3B-Instruct surpasses the GPT-OSS (20B) baseline with a 37.4% relative accuracy gain on the Natural Language Inference (NLI) benchmark. It also outperforms a modular-architecture baseline with a 19.1% relative improvement. Evaluations with GPT-4.1 as LLM judge for response quality assessment further confirm the robustness of our approach.", "tldr": "We propose OP-GRPO, which uses Sentence Transformer to evaluate the perspective coverage of LLM responses and fine-tunes the LLM policies. OP-GRPO achieves promising performance compared to previous baselines while providing concise responses.", "keywords": ["Pluralistic Alignment", "Overton Pluralism", "Reinforcement Learning From Human Feedback", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c938a3ff0a05755e8b70843957f6dd7e38a80455.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes OP-GRPO, a reinforcement learning framework aimed at training language models to generate pluralistic responses that reflect diverse human perspectives. The approach consists of two main components: fine-tuning a Sentence Transformer model (OP-SBERT) for similarity estimation, and training policies using Group Relative Policy Optimization (GRPO) with a dual-reward system that balances perspective coverage and uniqueness. The authors demonstrate that small models (1.5B-3B parameters) trained with their method can outperform larger baselines on NLI-based metrics and LLM-as-judge evaluations when tested on the ValuePrism-derived dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation for pluralistic AI alignment is well-articulated and represents a real challenge in the field. The technical execution shows competence in combining multiple components (SBERT fine-tuning, GRPO training, reward design) into a working system. The experimental results within the chosen evaluation framework are comprehensive, including multiple model sizes and evaluation metrics. The ablation study on uniqueness rewards provides useful insights about reward hacking through length inflation. The dual-structure output format (core perspectives + summary) is a reasonable design choice for separating reward computation from user-facing content."}, "weaknesses": {"value": "The fundamental weakness is that this work addresses dataset coverage rather than genuine pluralistic reasoning. Training a model to reproduce perspectives from a fixed dataset does not demonstrate that the model has learned to think from multiple viewpoints or that it can generalize this capability to new domains. Consider the analogy: if we train students to memorize five specific arguments about a topic, they haven't learned critical thinking - they've learned to recite answers. The paper never tests whether the trained models can generate diverse perspectives on topics outside the training distribution.\nThe choice of GRPO is inadequately justified. GRPO's group normalization provides training stability but has no obvious connection to encouraging diversity. Standard diversity-inducing techniques from the RL literature are never compared. A natural baseline would be adding an entropy bonus to encourage diverse perspective generation, or using determinantal point processes to ensure low-similarity sampling. The absence of these comparisons suggests the authors may have found that simpler approaches don't work as well for their specific task of dataset matching, but this would reveal that the problem formulation itself is misaligned with the stated goals.\nThe evaluation methodology has severe limitations. Using NLI models to measure entailment between generated and reference perspectives only validates that the model can paraphrase dataset content, not that it understands pluralism. The Accuracy@0.33 threshold appears arbitrary and chosen to make results look favorable. More importantly, there is no evaluation on cross-cultural scenarios, political diversity, religious viewpoints, or any setting that would test whether the model genuinely captures human value pluralism beyond a single dataset of moral dilemmas. The paper claims to address minority marginalization but never validates this claim with appropriate benchmarks or human studies involving diverse demographic groups.\nThe data processing reveals deeper issues. The fact that authors needed extensive filtering to remove \"redundant\" perspectives from ValuePrism, then had to augment data to reach five perspectives per prompt, suggests the underlying data distribution doesn't naturally support their task formulation. The reliance on Qwen3-14B for both data augmentation and as an evaluation baseline creates potential for subtle data leakage and circularity. The OP-triplet construction for SBERT fine-tuning uses the same LLM judge that defined what counts as \"redundant\" during data filtering, compounding the circularity."}, "questions": {"value": "How does the model perform on completely out-of-distribution scenarios like technical controversies, business ethics, or medical decision-making where the \"correct\" diverse perspectives might differ substantially from moral dilemmas? Can you provide zero-shot evaluation results on domains not covered by ValuePrism?\nWhy was GRPO chosen over simpler diversity-inducing methods? Can you provide ablation studies comparing against entropy regularization, DPP sampling, or standard policy diversity techniques? If these simpler methods work equally well or better, what is the specific contribution of using GRPO?\nThe paper claims to address marginalization of minority perspectives. How do you validate this claim? Have you conducted human evaluations with participants from diverse cultural, religious, or demographic backgrounds to assess whether their viewpoints are authentically represented? What percentage of the \"human reference perspectives\" in ValuePrism actually come from minority groups?\nThe similarity threshold of 0.70 and scale factor of 40 for OP-SBERT appear to be heavily tuned hyperparameters. How sensitive are results to these choices? Have you tested robustness across different threshold values not seen during development?\nSince the model learns to generate responses in a fixed format with explicit perspective labels, have you evaluated whether it can generate pluralistic responses in more natural conversational formats without the artificial structure? This would test whether the model learned genuine pluralistic reasoning versus pattern matching to a template."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B2dEGZubjm", "forum": "BzeVZ63hh4", "replyto": "BzeVZ63hh4", "signatures": ["ICLR.cc/2026/Conference/Submission13015/Reviewer_hPpe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13015/Reviewer_hPpe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499489828, "cdate": 1761499489828, "tmdate": 1762923756324, "mdate": 1762923756324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OP-GRPO, an RLHF framework for implicit Overton Pluralism that trains a single LLM to produce multi-perspective responses without explicit pluralism prompts or modular orchestration. The approach introduces: (i) an OP-specific similarity estimator by fine-tuning SBERT on a curated triplet dataset; (ii) a Mutual-Best Greedy Matching (MBGM) algorithm to enforce one-to-one matching for coverage evaluation; and (iii) a dual reward combining reference coverage and intra-response uniqueness, plus a small format reward. Empirically, OP-GRPO on small models (1.5B–3B) surpasses larger baselines (e.g., GPT-OSS 20B, Qwen3-8B) on OP-V2 using NLI metrics, and outperforms a modular pluralism pipeline; LLM-as-judge results further support quality improvements. Code and preprocessing details are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem framing around Overton Pluralism and a practical RLHF instantiation (GRPO) with pluralistic rewards. \n- Well-motivated engineering: OP-specific SBERT fine-tuning, MBGM to avoid many-to-one matches, and a structured output format that stabilizes reward computation. \n- Comprehensive evaluation across OP-V2 with both NLI and LLM-as-judge, plus ablations on uniqueness reward and HPO for thresholds/scales. \n- “Small models, big coverage” result is compelling; efficiency considerations (fast SBERT encoders, batch reward) are sensible. \n- Reproducibility appears strong with code, datasets, and algorithmic details (thresholds, scale factors, formats)."}, "weaknesses": {"value": "- Technical novelty is moderate; the method is a careful integration of known components (GRPO, SBERT, contrastive losses, greedy matching) rather than a fundamentally new algorithmic concept. \n- Heavy reliance on SBERT-based similarity with tuned thresholds risks reward misspecification and potential reward hacking; limited human evaluation beyond LLM-as-judge. \n- The OP-V2 pipeline depends on LLM-generated and filtered perspectives; potential bias propagation and circularity are not deeply audited (e.g., demographic coverage, minority value retention). \n- Limited analysis of failure modes: when uniqueness conflicts with coverage, or when perspectives drift outside socially acceptable windows; Overton “window” validity is assumed via references rather than verified by humans. \n- Baseline fairness and compute reporting could be more thorough (costs, latency, sensitivity to decoding choices, and robustness to domain shift). \n- NLI and LLM-as-judge choices (models, thresholds) can influence outcomes; more cross-metric triangulation or human studies would strengthen claims."}, "questions": {"value": "- How robust are results to the choice of NLI model and calibration? Have you tried multiple NLI backbones or calibration methods to ensure metric stability? \n- Can you report sensitivity of OP performance to the MBGM components individually (keyword masking, mutual best, threshold) and to the threshold/scale factors? \n- What failure cases arise when coverage and uniqueness conflict? How do you prevent reward hacking that inflates K or rephrases perspectives superficially? \n- How well does OP-GRPO generalize out-of-domain (new topics, styles) and to languages beyond English? \n- Can you provide compute/latency comparisons against the modular baseline and larger explicit-prompt systems at inference time? \n- How do you ensure generated perspectives remain within the Overton window (social acceptability) and avoid harmful content in the absence of explicit pluralism prompts? \n- To what extent do findings hold with human evaluators rather than LLM-as-judge, especially for measuring diversity and acceptability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M3SIr69cIK", "forum": "BzeVZ63hh4", "replyto": "BzeVZ63hh4", "signatures": ["ICLR.cc/2026/Conference/Submission13015/Reviewer_hHnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13015/Reviewer_hHnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708103247, "cdate": 1761708103247, "tmdate": 1762923755950, "mdate": 1762923755950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OP-GRPO, a reinforcement learning framework for Overton pluralistic alignment that enables a single LLM to produce multiple defensible viewpoints for one query without explicit prompting. The method fine-tunes a Sentence Transformer for similarity evaluation, introduces a dual reward (coverage + uniqueness + format quality), and trains with Group Relative Policy Optimization. Experiments show higher diversity and NLI/LLM-judge scores than explicit/implicit prompting and modular pluralism baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a novel implicit approach to Overton Pluralism via RLHF, avoiding modular architectures and enabling single-model pluralism.\n2. Fine-tunes SBERT on an OP-Triplet dataset and uses mutual-best greedy matching (MBGM) to improve semantic matching robustness."}, "weaknesses": {"value": "1. The methodology involves multiple interconnected stages (dataset refinement, triplet construction, SBERT fine-tuning with MBGM, and dual-reward GRPO), which may introduce complexity that could affect practical reproducibility and scalability.\n\n2. The selection of hyperparameters for weights such as $\\alpha_{cov}$ and $\\alpha_{uniq}$ could benefit from more detailed justification and ablation studies to ensure robustness and stability.\n\n3. It would be valuable to extend experiments to larger-scale models, such as those with 8B parameters or more, to further validate the approach.\n\n4. The theoretical foundation could be strengthened, particularly with a more objective and quantifiable definition of pluralism, to better distinguish genuine pluralistic alignment from subjective perceptions in the experiments.\n\n5. Ablation studies might be expanded to include end-to-end evaluations of the full pipeline, beyond isolated assessments of components like MBGM versus naive matching or the MNRL loss.\n\n6. Incorporating experimental comparisons with additional related works would help provide a clearer benchmark for the method's effectiveness.\n\n7. Given potential homology and circular biases in LLM-as-Judge evaluations (model judging model), including consistency checks with human reviews or analyses of rating variance and reliability could enhance credibility.\n\n8. The evaluation tasks could be broadened for greater comprehensiveness, such as assessing response safety; integrating frameworks like lm-eval-harness for multi-dimensional testing would offer a more thorough analysis of the method's impact."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xUuosiSO11", "forum": "BzeVZ63hh4", "replyto": "BzeVZ63hh4", "signatures": ["ICLR.cc/2026/Conference/Submission13015/Reviewer_s4CP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13015/Reviewer_s4CP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845274863, "cdate": 1761845274863, "tmdate": 1762923755552, "mdate": 1762923755552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the challenge of aligning LLMs with the pluralistic nature of human values. The authors propose Overton Pluralistic Group Relative Policy Optimization (OP-GRPO), a reinforcement learning framework that enables a single LLM to generate diverse, value-aligned responses, compared to explicit prompts or modular setups. OP-GRPO includes two key steps: (1) similarity estimator training, fine-tuning a Sentence Transformer to assess coverage of diverse perspectives, and (2) dual-reward RL training, balancing broad perspective coverage and response uniqueness. Experiments show that Qwen2.5‑3B‑Instruct trained with OP-GRPO achieves a “small model, big perspective coverage” effect—surpassing GPT‑OSS (20B) by 37.4% on NLI tasks and outperforming a modular baseline by 19.1%, with further validation from GPT‑4.1 quality assessments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the concept of Overton pluralism is interesting to take into algorithm design.\n2. the paper writing is easy to follow and understand"}, "weaknesses": {"value": "1. The algorithm itself is not particularly novel; the main contribution lies in introducing a new reward model that balances broad coverage of human perspectives with the uniqueness of each response.\n2. It remains unclear whether using RL with such a reward function provides advantages over parallel decoding or agent-based approaches, especially given the additional fine-tuning cost."}, "questions": {"value": "Please see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KhcoBNE4KG", "forum": "BzeVZ63hh4", "replyto": "BzeVZ63hh4", "signatures": ["ICLR.cc/2026/Conference/Submission13015/Reviewer_zXEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13015/Reviewer_zXEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006143824, "cdate": 1762006143824, "tmdate": 1762923755102, "mdate": 1762923755102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}