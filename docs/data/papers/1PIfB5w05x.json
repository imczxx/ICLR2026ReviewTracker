{"id": "1PIfB5w05x", "number": 23499, "cdate": 1758344650656, "mdate": 1759896811732, "content": {"title": "Price of Quality: Sufficient Conditions for Sparse Recovery using Mixed-Quality Data", "abstract": "We study sparse recovery when observations come from mixed-quality sources: a small collection of high-quality measurements with small noise variance and a larger collection of lower-quality measurements with higher variance. For this heterogeneous-noise setting, we establish sample-size conditions for information-theoretic and algorithmic recovery. On the information-theoretic side, we show that $(n_1, n_2)$ must satisfy a linear trade-off defining the _Price of Quality_: the number of low-quality samples needed to replace one high-quality sample. In the agnostic setting, where the decoder is completely agnostic to the quality of the data, it is uniformly bounded, and in particular one high-quality sample is never worth more than two low-quality samples. In the informed setting, where the decoder is informed of per-sample variances, the price of quality can grow arbitrarily large. On the algorithmic side, we analyze the LASSO in the agnostic setting and show that the recovery threshold matches the homogeneous-noise case and only depends on the average noise level, revealing a striking robustness of computational recovery to data heterogeneity. Together, these results give the first conditions for sparse recovery with mixed-quality data and expose a fundamental difference between how the information-theoretic and algorithmic thresholds adapt to changes in data quality.", "tldr": "Sufficient conditions for sparse linear regression when observations come from mixed-quality samples.", "keywords": ["High-dimensional Statistics", "Machine learning theory", "sparse regression", "Lasso", "heterogeneous noise", "LLM annotations"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/915a02c574c2c87d0a910b4699d93b2ba818bd01.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Sparse recovery has typically been discussed under the assumption that noise variance is constant (homogeneous noise). However, real data often contains a mixture of a small number of \"high-quality (low-noise)\" observations (e.g., expert measurements) and a large number of \"low-quality (high-noise)\" observations (e.g., crowdsourcing or LLM labels).\nThis paper theoretically analyzes the support recovery problem for a sparse signal $\\beta$ under mixed-quality data and defines two bounds:\ni) Information-theoretic bounds: What sample size (n₁, n₂) is theoretically necessary for recovery?\nii) Algorithmic bounds: To what extent can LASSO recover the signal?"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: This paper  newly introduced the viewpoint that the Kronecker product structure is equivalent to \"measuring a multidimensional signal at multiple stages.\" Based on this idea, it developed a signal recovery algorithm that operates with low computational complexity.\nQuality: The developed algorithm reduces the run time for the recovery by $O(10^2)--O(10^3)$. In addition, theoretical guarantees are presented mathematically. The performance is also validated numerically. In summary, this paper is of high quality. \nClarity: The writing is well structured and the results are clear. \nSignificance: The results obtained in this paper are expected to bring significant advances to the problem of compressed sensing with Kronecker structures."}, "weaknesses": {"value": "For problems where compressed sensing with a Kronecker structure is useful, the sparsity structure must be independent for each dimension. In the introduction, this paper mentions that there are examples of this in radar imaging and wireless communications, but does not provide specific examples. It is unclear how useful it is for practical applications."}, "questions": {"value": "I understand that this paper is theoretical and not intended the practical usefulness. But, I would like to ask about its practical relevance. I could not imagine example problems for which the setup assumed in this paper are realistic. Can you raise up some?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gcuZgPpRi9", "forum": "1PIfB5w05x", "replyto": "1PIfB5w05x", "signatures": ["ICLR.cc/2026/Conference/Submission23499/Reviewer_6vxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23499/Reviewer_6vxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831099723, "cdate": 1760831099723, "tmdate": 1762942683486, "mdate": 1762942683486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Sparse recovery has typically been discussed under the assumption that noise variance is constant (homogeneous noise). However, real data often contains a mixture of a small number of \"high-quality (low-noise)\" observations (e.g., expert measurements) and a large number of \"low-quality (high-noise)\" observations (e.g., crowdsourcing or LLM labels).\nThis paper theoretically analyzes the support recovery problem for a sparse signal $\\beta$ under mixed-quality data and defines two bounds:\ni) Information-theoretic bounds: What sample size (n₁, n₂) is theoretically necessary for recovery?\nii) Algorithmic bounds: To what extent can LASSO recover the signal?"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The originality of this paper lies in introducing the concept of the “price of quality” into the problem of sparse signal recovery and in rigorously analyzing the associated information-theoretic and algorithmic recovery conditions.\nQuality: This is a mathematically oriented paper. Although I could not follow every detail of the proofs, they appear to be sound and reliable.\nClarity: The manuscript is well organized, although some of the mathematical steps are rather demanding to follow.\nSignificance: The paper raises interesting theoretical questions. While it may not strongly appeal to researchers focused on practical applications, it is likely to attract considerable interest from those working on theoretical aspects of sparse signal estimation."}, "weaknesses": {"value": "Regarding Lasso estimation, only agnostic situations are analyzed."}, "questions": {"value": "I understand that this paper is theoretical and not intended the practical usefulness. But, I would like to ask about its practical relevance. I could not imagine example problems for which the setup assumed in this paper are realistic. Can you raise up some?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gcuZgPpRi9", "forum": "1PIfB5w05x", "replyto": "1PIfB5w05x", "signatures": ["ICLR.cc/2026/Conference/Submission23499/Reviewer_6vxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23499/Reviewer_6vxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831099723, "cdate": 1760831099723, "tmdate": 1763625887595, "mdate": 1763625887595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study sparse recovery from noisy linear observations of mixed quality. To be more precise, they seek to recover an $s$-sparse ground-truth $\\beta_* \\in \\mathbb R^p$ from $n$ observations\n\n$Y = X\\beta_* + Z \\in \\mathbb R^n,$\n\nwhere the measurement process is modelled by a Gaussian matrix $X$, and the entries of $Z$ model random noise and are zero-mean Gaussians. For $n_1+n_2=n$, the first $n_1$ observations $Y_1,…,Y_{n_1}$ are of high quality (modelled by noise variance $\\sigma_1^2$) whereas the remaining $n_2$ observations $Y_{n_1+1},…,Y_n$ are of low quality (modelled by noise variance $\\sigma_2^2 > \\sigma_1^2$). The authors consider the agnostic setting in which the recovery method has no information on the data quality, and the informed setting in which the recovery method knows which noise level applies to which observation.\n\nTheorem 1 provides sufficient conditions on $n_1,n_2,n$ for asymptotic support recovery in the agnostic setting. The result shows that asymptotically a high-quality observation is never worth more than two low-quality observations.\n\nTheorem 2 is the counterpart of Theorem 1 in the informed setting and shows that depending on the ratio between $\\sigma_1$ and $\\sigma_2$ a high-quality observation can be worth more than any number of low-quality observations.\n\nFinally, Theorem 3 analyzes sufficient and necessary conditions on $n$ to achieve support recovery algorithmically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Rigorous results\n+ Study well-motivated via mixed quality data in learning problems"}, "weaknesses": {"value": "- Algorithmic recovery is not analyzed in the informed setting"}, "questions": {"value": "Since the results are interesting and the derivation appears to be rigorous, I do recommend this paper to be accepted. Due to the limited reviewing time, I have only loosely screened parts of the supplementary material, so I cannot confirm correctness of the presented results. They appear to be reasonable though.\n\nWhile the paper is well-written and easy to read, the notation can be improved at certain points and there are some points that should be corrected:\n\n- X,Y,Z are all upper-case although X is a matrix and Y,Z are vectors. In contrast $\\beta$ is lower case. This is confusing.\n- l. 126: „… a linear combination of ??? has the …“\n- l. 199: Maybe I missed it, but I think Z_1,Z_2 are used without being defined\n- l. 235: Can you comment on the fact that your required size of n_* is smaller for s=p than for s=0.5p? Clearly, there is less ambiguity in the possible supports, nevertheless, this is rather counter-intuitive and should be properly discussed in the paper.\n- l. 251: „…Chernoff bound to the LHS…“ -> Shouldn’t this be RHS?\n- Eq (11): $\\sigma_4^2$ -> $\\sigma_2^4$\n- Remark 3.2, second bullet point: It would make sense to mention that the proposed estimator is motivated by the MLE discussed afterwards in Section 3.2\n- Eq (20): This should be argmin and not min\n- l. 604: $\\hat S$ is only an element of the argmin not necessarily the only solution.\n- l. 725: If I'm not mistaken, you apply Markov’s inequality here, and not a Chernoff bound. This would need to be adapted all over the document."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dvKhwqvWhj", "forum": "1PIfB5w05x", "replyto": "1PIfB5w05x", "signatures": ["ICLR.cc/2026/Conference/Submission23499/Reviewer_QshA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23499/Reviewer_QshA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718279199, "cdate": 1761718279199, "tmdate": 1762942683205, "mdate": 1762942683205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of sparse signal recovery in the mixed-quality data setting: where a subset of the data is observed with low noise, and the complement observed with different (higher, w.l.o.g) noise. They study the information theoretic thresholds of the problem, and demonstrate that the maximum likelihood estimator in the gaussian noise setting has significantly different behaviour depending on whether one is agnostic or not to the noise variances. They also analyze the LASSO and prove that it is robust under this model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original, they study a timely problem and do so with an original look at a new variable called the price of quality. I must mention that such problems were already considered in the heterogeneous statistics literature, but they were moreso concerned about inferring the heterogeneities themselves, rather than quantifying downstream implications of standard models on mixed quality data. In this regard, I find this work novel. \n\nThe work is clear, and is of high quality as the authors investigate various aspects of the problem thoroughly. \n\nThe work is significant, especially in the era of AI, where models can often be trained using a mix of real and synthetic data. In particular, the authors clarify their findings and present them as important and non-trivial rules of thumb for machine learning, such as that in the agnostic setting, where one high-quality sample is never worth more than two low-quality samples."}, "weaknesses": {"value": "I found no major weaknesses in the paper. Following were minor comments/questions: \n\n- The information-theoretic and algorithmic thresholds in (2) are mainly for Gaussian noise right? Or have these been proven for additive noise more broadly? Please clarify this, and maybe state earlier on in the work that you consider additive independent Gaussian noise.\n- Typos: First paragraph after 1.2.1, “In the first part of [our] work ([S]ection 3) …”. In the next paragraph: “, the condition requires that a linear combination of [?] has the form …”\n- A bit more clarification as to how \\sigma_1, \\sigma_2 scale w.r.t other parameters, and remind this throughout the paper.\n- Your results in Theorem 1 are mostly relevant for additive (sub)-Gaussian noise, please clarify this and that your sufficient conditions (and therefore findings) are not universal over general additive noise.\n- End of page 7, you mention “OGP” but do not explain/state the full name or what it is. It would be good to get more background on that.\n- Could you comment on how this relates to existing statistical work on heteroscedastic regression, and whether you expect improvements for algorithms which first estimate the heterogeneities, then proceed to weight samples accordingly?"}, "questions": {"value": "Questions asked in the “Weaknesses” section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1bHMIoUVcn", "forum": "1PIfB5w05x", "replyto": "1PIfB5w05x", "signatures": ["ICLR.cc/2026/Conference/Submission23499/Reviewer_zQsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23499/Reviewer_zQsn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947330209, "cdate": 1761947330209, "tmdate": 1762942682972, "mdate": 1762942682972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of support recovery from noisy observations in a linear regression setting with Gaussian design. The design is assumed to be centered and isotropic, while the noise is Gaussian with zero mean and a diagonal covariance matrix that takes two possible values: $\\sigma_1$ and $\\sigma_2$.\n\nThe paper focuses on two types of results: first, sufficient conditions for the maximum likelihood estimator (computationally inefficient) to consistently recover the support, considering both the ill-specified and well-specified noise covariance matrix cases; second, sufficient conditions ensuring that the Lasso estimator consistently identifies both the support and the sign of the regression vector."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of recovering the support of an unknown vector is a fundamental challenge in statistics, frequently regarded as a prototypical case for mathematical results on variable selection. Exploring the information-theoretic and algorithmic limitations of this problem holds, in my opinion, significant interest for the ICLR audience. \n\n - The mathematical proofs, as far as I could check, appear to be correct. \n\n - The proof of Theorem 3 overcomes some non-trivial technical challenges."}, "weaknesses": {"value": "- I suspect that **Theorem 1** is not sharp, for two main reasons:\n  - When $n_2 = 0$ (i.e., no low-quality data is present), the theorem's condition requires $n_1$ to exceed a quantity involving $\\sigma_2$. Intuitively, $\\sigma_2$ should not appear in this condition under these circumstances.\n  - As noted in lines 265–269, if $\\sigma_2 = o(s)$, the condition becomes independent of $\\sigma_1$, which seems counterintuitive.\n\n- The conclusion at the top of page 6, *\"one unit of high-quality data is worth at most 2 units of low-quality data,\"* does not seem justified by **Theorem 1**. The theorem provides only a **sufficient condition** for recovery, but it does not rule out the possibility that the maximum likelihood estimator could detect the support with a smaller value of $n_1$.\n\n- The conditions under which the results are proved, although similar to those required in many papers on this topic, are quite restrictive."}, "questions": {"value": "- Since in Thm 3 it is required that $s=o(p)$, can we replace $n_{Alg}$ by its asymptotic equivalent $2s\\log (p)$?\n\n- Can we generalize Theorem 2 to arbitrary matrices $\\Sigma$, just by replacing the LHS of (15) by $\\sum_{i=1}^n \\log(1+\\frac{\\delta s}{2\\lambda_i(\\Sigma)})$?\n\n - What would happen if, in **Theorems 1 and 2**, we replaced the condition that nonzero elements are equal to one with the condition that they are either $1$ or $-1$? Would sign recovery still hold under the same conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5mh6W4PSRu", "forum": "1PIfB5w05x", "replyto": "1PIfB5w05x", "signatures": ["ICLR.cc/2026/Conference/Submission23499/Reviewer_dyNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23499/Reviewer_dyNa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952257181, "cdate": 1761952257181, "tmdate": 1762942682776, "mdate": 1762942682776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}