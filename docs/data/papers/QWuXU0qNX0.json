{"id": "QWuXU0qNX0", "number": 4389, "cdate": 1757670639749, "mdate": 1759898035394, "content": {"title": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning", "abstract": "While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.", "tldr": "Previous  Memory-layer attempts have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2 to closes this performance gap.", "keywords": ["memory network", "moe", "pretrain", "long context"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1145cb5edc91fabdf946a902b9c53908d98501a4.pdf", "supplementary_material": "/attachment/2c0e8f1b629e8c20e1a7d578800fb19c9eb01686.zip"}, "replies": [{"content": {"summary": {"value": "UltraMemV2 refines the Memory Layer architecture as a new sparse alternative to MoE models.\nWhile the original UltraMem only reached the performance level of a 2 active expert MoE, UltraMemV2 achieves performance comparable to an 8 active expert MoE, marking a significant advancement.\nThe authors introduce several key improvements, including a memory layer in each Transformer block, a simplified implicit value expansion (IVE), a PEER-based feedforward mechanism, improved initialization for stable training, and optimized compute ratios between memory to  FFN.\nWith these enhancements, UltraMemV2 shows notable gains (+6 to +8 points) on long-context, multi-step reasoning, and memory-intensive tasks, while efficient memory access comparable to existing MoE models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Considering that MoE architectures have become a de facto standard component in LLM training, the direction of this work is highly meaningful. Improving the cost efficiency and performance of such sparse architectures is an important and timely.\n\n2. The reported performance is very promising, matching the top-k=8 MoE configuration that is commonly used in recent LLM models.\n\n3. The experiments are conducted at a large scale up to 120B parameters with 2.5B active and trained on 4.4T tokens which makes the results convincing and demonstrates the method’s scalability."}, "weaknesses": {"value": "1. It would be valuable to include more discussion or quantitative results about GPU hours, training/inference latency, and throughput, especially compared to existing MoE models.\nThese metrics would strengthen the claim that UltraMemV2 offers practical efficiency gains.\n\n2. While hyperparameter search is generally required for LLM training, the proposed model appears to be more sensitive to hyperparameter choices, such as initialization and learning-rate scheduling."}, "questions": {"value": "1. It is mentioned that UltraMemV2 cannot easily achieve higher sparsity. Did the authors analyze whether its sparsity scaling behavior differs from that of MoE models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EA1f9aQQMh", "forum": "QWuXU0qNX0", "replyto": "QWuXU0qNX0", "signatures": ["ICLR.cc/2026/Conference/Submission4389/Reviewer_rcay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4389/Reviewer_rcay"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491004737, "cdate": 1761491004737, "tmdate": 1762917332854, "mdate": 1762917332854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a sparse memory architecture by tweaking memory layer architecture to match the performance with MOEs with 8 experts. The paper does a good job of providing a Comprehensive analysis and detailed ablation studies. The paper also discusses scalability, though it’s unclear if the same scaling law of LLMs holds with trainable memory parameters. \nIt improves inference efficiency over prior work, by  simplifying value expansion with single linear projections, demonstrating that parameter efficiency of non-shared linear layers is actually not high. This paper also overcomes the limitation of number of memory layers, where previous work has shown degradation of performance if the number of memory layers are too high."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good ablation for number of layers and  overcomes the limitation of number of memory layers\n2. Matched performence with MOEs with 8 experts\n3. Uses strong benchmarks and evaluation\n4. Simplifies the  value expansion, making inference more efficient"}, "weaknesses": {"value": "Overall the contribution is light, the paper aims to bridge the gap(in performance) between MOEs and Memory layer architectures. In terms of scientific novelty, some of the approaches seem incremental and this approach seems to combine multiple incremental tweaks to achieve performance improvements over baseline. For example, “Memory Layer at Scale”(Berges, 2024) paper demonstrated that  was multiple memory layers increase performance significantly over having a single layer(In their case, performance degraded going beyond 3 layers).  Another example is adoption of simplified value expansion with a small tweak of single linear projection."}, "questions": {"value": "1. A more rigorous scaling law analysis and discussion around scaling trends are required if this is proposed as an alternative architecture to MoEs\n2. Can this approach be introduce during mid-training or post training and achieve good performance. Any discussion around the performance gap with CT would be great. \n3. There are other parametric memory work, such as memory Layers at scale. It would be good to compare the results with such alternative approaches.  \n4. It would be good to share results on how high quality RAG combines with this approach and how does this approach compares with RAG for long context tasks\n\nMinor comments:\n1. Discussion about how can this be combined with MOEs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xoXqwy9cX", "forum": "QWuXU0qNX0", "replyto": "QWuXU0qNX0", "signatures": ["ICLR.cc/2026/Conference/Submission4389/Reviewer_f3Vh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4389/Reviewer_f3Vh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936733217, "cdate": 1761936733217, "tmdate": 1762917332258, "mdate": 1762917332258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UltraMemV2, a memory-layer architecture intended to close the performance gap between memory-layer sparse approaches and Mixture-of-Experts models. The authors introduce five main changes: placing memory layers in every transformer block, simplifying implicit value expansion to a single linear projection, using FFN-based value processing inspired by PEER, a new initialization for the memory layer, and rebalancing memory and FFN computation proportions. They evaluate UltraMemV2 across proprietary and open benchmarks and report that UltraMemV2 reaches parity with 8-expert MoE under matched compute/parameters while requiring much lower memory access. They highlight particularly strong gains on memory-heavy tasks and validate scaling up 120B parameter models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Strong architectural contributions: the five design changes proposed by the authors are all justified through ablations and contribute to improved model performance\n\nStrong empirical evaluation: multiple model scales up to 120B and a diverse selection of benchmarks make the authors claims very convincing.\n\nInitialization analysis: the paper contributes a new initialization scheme to stabilize training of the memory layer, which addresses a common failure mode for large sparse modules.\n\nPractical use: UltraMemV2 is a compelling architecture for deploying models under memory bandwidth constraints because of the relatively lower memory accesses."}, "weaknesses": {"value": "The paper motivates UltraMemV2 with lower memory access and inference cost, but it would be more convincing to see latency and bandwidth comparisons vs. traditional MoE models\n\nProprietary data: this might be unavoidable but the proprietary nature of the benchmarks and data limits the reproducibility of the methods in this paper\n\nThe UltraMemV2 model has significantly worse benchmark performance on multi-hop reasoning. The paper would be improved if the authors investigated this further and demonstrated through other benchmarks whether UltraMemV2 has worse overall reasoning abilities or if it is specific to this benchmark."}, "questions": {"value": "The method lags behind early in training as mentioned by the authors. Do you have more details and possible intuitions why UltraMemV2 is slower to train initially? And what does \"early\" mean in general - how much data does it need to catch up?\n\nIs there a reason for the drop in multi hop reasoning? Is this reflective of the model's overall reasoning capabilities on downstream tasks?\n\nCould you quantify the inference cost improvements that you alluded to eg the reduction in memory access?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nyv9gpaC42", "forum": "QWuXU0qNX0", "replyto": "QWuXU0qNX0", "signatures": ["ICLR.cc/2026/Conference/Submission4389/Reviewer_hfVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4389/Reviewer_hfVp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959842567, "cdate": 1761959842567, "tmdate": 1762917331993, "mdate": 1762917331993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a memory‑layer architecture intended as an alternative to MoEs, which is an extension of UltraMem. The novelty with respect to the origianl UltraMem are, adding a memory layer to every Transformer block, simplifying the implicit value expansion (IVE) to a single linear projector, replacing value embeddings with an FFN with 1-dimensional inner layer, introducing a variance‑matching initialization, and rebalancing compute between memory and FFN. They claim parity with 8‑expert MoEs at similar active parameters/compute and advantages on long‑context tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly describes its position relative to MoE, PKM/UltraMem, and PEER.\n\nExperiments show that increasing the number of UltraMemV2 layers improves downstream accuracy even when validation loss plateaus.\n\nThe proprietary long‑context suite shows non‑trivial gains of 6.2 on multi‑round memorizing and 7.9 on in‑context learning.\n\nThe paper is explicit that UltraMemV2 underperforms early in training and benefits from continued training, and also notes dependence on per‑block placement."}, "weaknesses": {"value": "The paper asserts matching compute and parameters, but does not report KV‑cache costs, routing FLOPs, or memory traffic for both MoE and UltraMemV2.\n\nThe claim that this work is the first memory layer to match 8‑expert MoE is not accurate in light of the Memory Layers at Scale paper [https://arxiv.org/abs/2412.09764]."}, "questions": {"value": "How are KV‑cache footprint and router/TDQKR indexing costs accounted for in the iso‑compute comparisons in Tables 1–3?\n\nCould you report token/s and latency vs. batch size, plus HBM read/write estimates, for representative model sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ACIjCcUSYJ", "forum": "QWuXU0qNX0", "replyto": "QWuXU0qNX0", "signatures": ["ICLR.cc/2026/Conference/Submission4389/Reviewer_3bZy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4389/Reviewer_3bZy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178892152, "cdate": 1762178892152, "tmdate": 1762917330934, "mdate": 1762917330934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}