{"id": "hxGdAUn3sB", "number": 10786, "cdate": 1758181897463, "mdate": 1763129071927, "content": {"title": "Self-Consistency Improves the Trustworthiness of Self-Interpretable GNNs", "abstract": "Graph Neural Networks (GNNs) achieve strong predictive performance but offer limited transparency in their decision-making. Self-Interpretable GNNs (SI-GNNs) address this by generating built-in explanations, yet their training objectives are misaligned with evaluation criteria such as faithfulness. This raises two key questions: (i) can faithfulness be explicitly optimized during training, and (ii) does such optimization genuinely improve explanation quality? We show that faithfulness is intrinsically tied to explanation self-consistency and can therefore be optimized directly. Empirical analysis further reveals that self-inconsistency predominantly occurs on unimportant features, linking it to redundancy-driven explanation inconsistency observed in recent work and suggesting untapped potential for improving explanation quality. Building on these insights, we introduce a simple, model-agnostic self-consistency (SC) training strategy. Without changing architectures or pipelines, SC consistently improves explanation quality across multiple dimensions and benchmarks, offering an effective and scalable pathway to more trustworthy GNN explanations.", "tldr": "We identify the mismatch between SI-GNN training and faithfulness evaluation, show its connection to self-consistency, and propose a simple SC loss that consistently improves explanation quality without architectural changes.", "keywords": ["Self-interpretble GNNs; Trustworthy; Consistency; Faithfulness"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73a271e85a9d7c27c6b8fd0ef29b067d2ebeb31a.pdf", "supplementary_material": "/attachment/f97ffd23c13a7a3f136048a44ea76e1462177f72.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduce a simple, model-agnostic self-consistency (SC) post-training strategy for pretrained Self-Interpretable GNNs (SI-GNNs). \nRigorous analysis demonstrates that faithfulness can be explicitly optimized through the strategy, and such optimization genuinely improves explanation quality.\nExperiments show that SC consistently improves explanation quality across multiple dimensions and benchmarks, offering an effective and scalable pathway to more trustworthy GNN explanations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper provides a fresh perspective (Self-Consistency) to improves the trustworthiness of self-interpretable GNNs, , the theoretical and empirical verification of the key assumptions is sufficient, there are no obvious flaws.\n- The method demonstrates strong explanation performance across multiple tasks, improving various types of self-Interpretable GNNs.\n- The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. During the fine-tuning phase of SC (Section 3.1), this work freezes the GNN encoder and assumes that \"the encoder representation is already optimal, and only the explainer needs optimization\". However, this work fails to validate this design and could further compare it with a control group where \"the encoder is not frozen\".  \n2. Has the author conducted statistical significance tests? What percentage of the experimental results significantly outperform each baseline? This is crucial for understanding the overall performance of SC."}, "questions": {"value": "Please refer to the weaknesses for suggestions. This article is highly accomplished, and the weaknesses mentioned above will not change my positive evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03fJDbMfLW", "forum": "hxGdAUn3sB", "replyto": "hxGdAUn3sB", "signatures": ["ICLR.cc/2026/Conference/Submission10786/Reviewer_7o1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10786/Reviewer_7o1e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760697721541, "cdate": 1760697721541, "tmdate": 1762922001465, "mdate": 1762922001465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a common mismatch in self-interpretable GNNs (SI-GNNs): models are trained with classification plus conciseness regularization, but are evaluated by faithfulness (whether the highlighted subgraph alone reproduces the prediction). The authors argue that faithfulness is intrinsically linked to self-consistency (SC)—the explanation produced on a graph should be reproduced when the model is run on its own explanation. They propose a simple, model-agnostic training strategy that adds a self-consistency loss during a short fine-tuning stage where the GNN encoder is frozen and only the explainer and classifier are updated, yielding a final objective.\nThey analyze how enforcing self-consistency drives importance scores toward a few “near fixed levels,” and how this interacts with conciseness regularization (CR) such as sparsity (GISST) or MI constraints (GSAT) to suppress unimportant edges while preserving important ones.\nOn four benchmarks (BA-2MOTIFS, 3MR, BENZENE, MUTAGENICITY) and four SI-GNN families (GISST, GSAT, GAT, CAL), SC improves consistency, faithfulness, explanation accuracy, and informativeness; it also complements explanation ensembling (EE) and is substantially more efficient at inference. SC alone may hurt when CR is absent (GAT/CAL), but adding CR stabilizes and recovers gains. The method requires no architectural change and adds one low-sensitivity hyperparameter ($\\eta$) ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reframes faithfulness optimization as enforcing *self-consistency* during training and connects it to redundancy on unimportant edges; provides a model-agnostic, plug-in loss with a two-stage schedule that freezes the encoder  .\n2. Extensive experiments on four datasets and four SI-GNN families. Clear reporting that SC alone can fail without CR, plus the complementary effect with EE. Solid ablations on ($\\beta$) and ($\\eta$) sensitivity and two-stage training to rule out confounds  .\n3. The training procedure and metrics are well defined; the figures make the stability effect tangible, and the paper carefully explains when SC helps and when it needs CR support .\n4. Practical because it needs no architecture changes, adds a single hyperparameter of low sensitivity, and improves multiple explanation dimensions while being more efficient than EE at inference time ."}, "weaknesses": {"value": "1. The two-stage setup assumes that Step 1 already produces explanations covering the ground-truth rationale. Step 2 then removes redundancy. If Step 1 misses key parts of the rationale, SC may stabilize an incomplete subset rather than recover it. The paper should explicitly state this assumption and test its robustness.\n\n2. Adding more initial explanations (e.g., multiple seeds or ensembling in Step 1) could alleviate this limitation. The gains seen in Table 1 when combining with EE support this interpretation.\n\n3. All datasets are small; results on large-scale or node-level tasks would clarify generality.\n\n4. Limited theory: The fixed-level analysis is illustrative but not formal; it would be valuable to characterize conditions guaranteeing convergence toward faithful explanations.\n\n5. Incomplete metric coverage: Improvements on FID− are strong, while FID+ results remain mixed; additional discussion would improve completeness."}, "questions": {"value": "1. What happens if the Step 1 SI-GNN misses part of the ground-truth rationale? Please evaluate robustness by ablating a portion of important edges before Step 2 and reporting whether SC can recover faithfulness or simply stabilize incomplete masks.\n\n2. How many models are produced overall, and which one generates explanations? My understanding is that only a single model—the Step 2 checkpoint—is used at inference. If multiple initial explanations are generated in Step 1, are they ensembled before Step 2 or during inference?\n\n3. Consider generating multiple Step 1 explanations and comparing unions or averaged masks before Step 2. This could test whether broader initial coverage yields better results.\n\n4. Since SC alone can hurt for GAT/CAL, what is the weakest form of conciseness regularization ((L_1) or entropy penalty) that stabilizes training?\n\n5. Could encoder-side regularization, such as partial Mixup on masked graphs, reduce encoder distribution shift and improve FID+?\n\n6. Would allowing the last encoder layer to update during Step 2 better adapt to explainer changes without destabilizing learned representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YsyGVSGswg", "forum": "hxGdAUn3sB", "replyto": "hxGdAUn3sB", "signatures": ["ICLR.cc/2026/Conference/Submission10786/Reviewer_hKUr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10786/Reviewer_hKUr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664972673, "cdate": 1761664972673, "tmdate": 1762922000728, "mdate": 1762922000728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address a relevant and widespread problem in GNN (and SE-GNN in particular), which is the consistency and reliability of the explanations being extracted, and propose an approach to encourage self-consistency of explanations at training time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Trying to directly enforce consistency at training time seems like a sensible direction.\n\nExperimental results confirm improvements and provide insights into explanations quality and robustness."}, "weaknesses": {"value": "The problem of consistency of explanations was already addressed in the work by Tai et al, where they propose a simple esamble strategy (EE). The novelty of the work is thus not dramatic. The authors show that their approach improves over EE (apart from being clearly much faster), and their combination further improves results. The rationale for these results is unclear to me. Is there any substantial difference in what the approaches tackle justifying this? Is this a matter of hyperparameter choice? This is important to correctly evaluate the relevance of the contribution.\n\n\nThere are plenty of notions of faithfulness of explanations, but a key aspect is that one should measure both sufficiency (e.g. with FID-) and precision (e.g. with FID+). I thus think the authors should include FID+ (or similar metrics) to get the full picture. Given that the proposed SC component is combined with conciseness regularization (CR), I do not expect this to undermine the utility of the approach, but it would allow to get a better picture of its contribution, especially when seeing the FID+ results without CR (table 3)."}, "questions": {"value": "Can you clearly motivate the performance difference between SC and EE? aren't they basically optimizing for the same objective?\n\nCan you add FID+ results and comment them to better understand the interplay between SC and CR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0rOaP7GvJJ", "forum": "hxGdAUn3sB", "replyto": "hxGdAUn3sB", "signatures": ["ICLR.cc/2026/Conference/Submission10786/Reviewer_AnRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10786/Reviewer_AnRn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991743995, "cdate": 1761991743995, "tmdate": 1762922000042, "mdate": 1762922000042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a simple but sensible model-agnostic strategy for improving the faithfulness of self-explainable GNNs. The idea is to penalize the model for \"changing its mind\" whenever fed with a local explanation of a certain prediction during training. The technique is validated using four approaches and compared\nagainst an alternative (explanation ensembling) on four datasets, with encouraging\nresults."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: The idea behind the penalty is aligned with existing results,\n  but otherwise original -- to the best of my knowledge.\n\n- **Quality**: The proposed idea is sensible. The empirical setup is also good\n  -- the choice of datasets, metrics and competitors all look good. I appreciate\n  how the authors clearly distinguish between faithfulness (measured with Fid-)\n  and plausibility/explanation accuracy. This is surprisingly rare in the\n  literature.\n\n- **Clarity**: The text is very clear and well structured. The visualizations\n  are helpful.\n\n- **Significance**: The contribution is welcome and I think it bridges a\n  serious gap in the literature, by making theoretical insights practical.\n  The fact that the approach is model agnostic also helps.\n\nTL;DR: good paper, I like the idea and the execution."}, "weaknesses": {"value": "- **Originality**: As I mentioned, I believe the proposed technique follows\n  naturally from existing results (eg Azzolin et al, who the authors mention).\n  This is however not a major issue for me - the penalty, as I mentioned,\n  is novel.\n\n- **Clarity**: My only real complaint is that Table 1 has too many colors, making\n  it diffult to focus on what's really relevant. I'd suggest to tone it down."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0rQs2QoynS", "forum": "hxGdAUn3sB", "replyto": "hxGdAUn3sB", "signatures": ["ICLR.cc/2026/Conference/Submission10786/Reviewer_25Yx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10786/Reviewer_25Yx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111220407, "cdate": 1762111220407, "tmdate": 1762921999471, "mdate": 1762921999471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}