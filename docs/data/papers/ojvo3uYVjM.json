{"id": "ojvo3uYVjM", "number": 23141, "cdate": 1758340141956, "mdate": 1759896830657, "content": {"title": "HealMQA: A Healthcare Multimodal Question Answering Dataset for Benchmarking Large Language Models", "abstract": "Consumers increasingly rely on digital platforms to seek healthcare advice, yet existing medical question answering (QA) resources are primarily unimodal or professional-facing, limiting their relevance to real-world users. To address this gap, we introduce Consumer Healthcare Multimodal Question Answering (CHMQA), a novel task for generating clinically valid free-text answers to multimodal consumer health queries. To benchmark this task, we present HealMQA, an expert-annotated multimodal QA dataset, consisting of 1,022 real-world consumer questions paired with medically validated images and expert-written answers spanning 17 healthcare topics. We evaluate eight state-of-the-art large language models (LLMs) under zero-shot and few-shot prompting methods, complemented by automated metrics and human evaluation by licensed medical professionals. Our experiments reveal that while LLMs achieve strong performance in accuracy, clarity, and safety, they continue to struggle with effectively integrating the visual modality. These findings highlight both the promise and limitations of current systems, and position HealMQA as a foundation for advancing medically accurate and consumer-centered multimodal healthcare QA.", "tldr": "We introduce HealsMQA, an expert-annotated multimodal dataset of real consumer healthcare questions on diverse healthcare topics, and benchmark with state-of-the-art LLMs.", "keywords": ["Multimodal Question Answering", "Consumer Healthcare", "Large Language Models", "Medical AI", "Benchmarking", "NLP"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d0a6992d0bc72959294fe86ec6553f6cbd3ee7a.pdf", "supplementary_material": "/attachment/12d4059be524fb2881bd67bdfd83e828c7229249.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CHMQA, a visual question answering task that focuses on free-text answering of consumer health questions. To study this setting, the authors curate HealMQA, a benchmark dataset built from real user questions that annotators rephrase to reference an image, pair with a relevant medical image retrieved from authoritative websites, and answer in clinician-written sentences across 17 topics. The paper benchmarks eight multimodal LLMs under zero- and few-shot prompting using BERTScore, SacreBLEU, ROUGE, and a small human study by two physicians. Results suggest decent textual quality and safety but weaker image-text integration among tested systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem itself is interesting, targeting an emerging, high-impact use case: consumer health queries that often mix vague text with a clarifying photo. \n2. The dataset design is mostly sound with clear annotation steps, yielding examples that resemble what end users might ask multimodal assistants. \n3. The human evaluation, while small, adds a clinically meaningful lens that automated n-gram metrics cannot provide. \n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. I am concerned about the pairing between the image and the real scene behind the user's description in the proposed dataset. The image is not user-provided, but retrospectively retrieved to match a textual visual term. This creates **two layers of distributional shift**: (1) the real scene may contain key features that are not described by the users due to their non-professionalism. (2) the image retrieved via Google search may not reflect the user’s description. Such weak alignment makes the HealMQA dataset less suitable for benchmark curation which poses higher standard for validated data than training data curation. \n\n2. Is there an ablation study to quantify the potential image-query misalignment above? For example, start the evaluation with real user image-text pairs and then replace the paired images with ones retrieved from Google and see how the evaluation results will change.\n\n3. Inter-annotator “agreement” via BERTScore on a small pilot is an unusual proxy that conflates style and semantic overlap and can reward superficially similar wording. And I am not sure if 0.7257 should really be seen as “high level similarity and hence, agreement”. This is a potentially large gap, particularly in the medical scenario. A more feasible solution is to use clinically oriented rubric agreement (e.g., structured checklists for differential, red-flag triage, etc) and inter-rater reliability on those rubrics (κ), not just embedding similarity. The current claim of high agreement feels weakly supported.\n\n4. Is there any data leakage detection for the benchmark dataset? Note that all images and texts are directly from the Internet and they might already be consumed by existing MLLMs trained on web-crawled data, making the evaluation results less valuable.\n\n5. Evaluation under-emphasizes visual grounding. The automatic metrics are text-to-text overlap and do not verify whether an answer actually uses image evidence correctly. Although the human study notes “multimodal consistency” as the weakest dimension, again, the experiment is too small (20 items, 2 raters) to draw confident conclusions. \n\n6. I am also concerned about licensing and provenance. “Open access” or preference for .edu/.gov domains is not a license. It is unclear whether images are redistributed, and whether usage complies with original site terms and patient privacy constraints. \n\n7. Some typos. For example, in Line 200, “With HealMQA, we the creation...”. In Line 261, “addresses the user’s.”"}, "questions": {"value": "Will this benchmark dataset be open-source?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qAqFhQfWZw", "forum": "ojvo3uYVjM", "replyto": "ojvo3uYVjM", "signatures": ["ICLR.cc/2026/Conference/Submission23141/Reviewer_ws9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23141/Reviewer_ws9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228504469, "cdate": 1761228504469, "tmdate": 1762942528950, "mdate": 1762942528950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HealMQA, a clinical expert annotated multimodal question answering dataset designed for consumer healthcare queries. It consists of more than 1k real healthcare questions from online sources along with images retrieved from Google and expert written answers across several healthcare domains, primarily focusing on skin conditions. The authors define a new task, Consumer Healthcare Multimodal Question Answering - CHMQA, to evaluate how AI systems handle multimodal queries in a generalized setting. They benchmark eight LLMs, including GPT-5 and open-source models like Llama and Mistral, under zero- and few-shot settings, using automated metrics (BERTScore, ROUGE, BLEU) and human evaluation by medical professionals. Results show that LLMs produce generally accurate and clear answers but often fail to integrate visual information effectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the CHMQA task, a novel and consumer focused multimodal question answering setup that addresses the lack of datasets combining real world consumer facing health queries with visual inputs, bridging the gap between professional medical QA systems and everyday user needs.\n- It includes manual annotation and validation by licensed medical experts, providing clinically grounded answers and a structured evaluation of LLM outputs across key medical criteria, though the scale of expert involvement remains relatively limited."}, "weaknesses": {"value": "- The fundamental problem with this benchmark is that there is no relationship between the image and the user query. The quality of the benchmark would simply depend on the quality of the images retrieved by the annotators. Obtaining exact images that accurately matches the user description is quite challenging and is not a scalable approach. This would also lead to unwanted biases in the benchmark where the same image from google could be used for multiple similar user queries. \n- The evaluations are not comprehensive enough, Table 2 is missing many state of the art open and close sourced models like – Gemini 2.5 Pro [1], Gemini 2.5 Flash and medical models like Google’s MedGemma[2], Huatuogpt[3], Bimedix2[4] and LLaVA-Med.\n- Additionally models like Gemini 2.5 Pro and Gemini 2.5 Flash have muti-image support which helps them take advantage of the few-shot setting. Currently, as mentioned in A.3 models like Llama and mistral are evaluated by giving only the text input which does not incentivize them to use multimodal signals in the few shot setting.\n- When evaluating model responses in an open-ended context, the most effective current approach is to employ an LLM as a judge. Alternative methods, such as embedding-based metrics or BERTScore, even those adapted for clinical domains, fail to fully capture the nuanced intricacies of natural language.\n\n[1] *Comanici, Gheorghe, et al. \"Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\" arXiv preprint arXiv:2507.06261 (2025)*\n\n[2] *Sellergren, Andrew, et al. \"Medgemma technical report.\" arXiv preprint arXiv:2507.05201 (2025).*\n\n[3] *Chen, Junying, et al. \"Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale.\" arXiv preprint arXiv:2406.19280 (2024).*\n\n[4] *Mullappilly, Sahal Shaji, et al. \"Bimedix2: Bio-medical expert lmm for diverse medical modalities.\" arXiv preprint arXiv:2412.07769 (2024).*\n\n[5] *Li, Chunyuan, et al. \"Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\" Advances in Neural Information Processing Systems 36 (2023): 28541-28564.*"}, "questions": {"value": "Please address the above weaknesses. \n- What instructions were given to the annotators for the benchmark creation, please include detailed instructions in appendix to ensure transparency and reproducibility.\n- Line 106: “We provide a detailed human analysis of LLM” use correct article “a”.\n- Line 202: “With HealMQA, we the creation of a widely useful” please correct the grammar."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jw26wPijUg", "forum": "ojvo3uYVjM", "replyto": "ojvo3uYVjM", "signatures": ["ICLR.cc/2026/Conference/Submission23141/Reviewer_erVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23141/Reviewer_erVX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682201817, "cdate": 1761682201817, "tmdate": 1762942528532, "mdate": 1762942528532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HealMQA, a benchmark dataset for consumer-healthcare multimodal question answering (CHMQA), featuring around 1k real world user questions paired with medically validated images and expert-written answers. The dataset is positioned as bridging the gap between unimodal medical QA datasets and VQA style tasks. The authors benchmark eight large language models under zero/few‐shot settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The topic is timely and socially relevant given the growing use of foundation models for healthcare advice. \n\n- The dataset is carefully annotated by medical professionals and covers diverse consumer health topics, enhancing realism and safety awareness. \n\n- The paper is well written, provides a clear task definition and methodology, and complements automatic metrics with human evaluation."}, "weaknesses": {"value": "- The technical novelty and dataset scale are limited. HealMQA contains only about 1 k examples, which is small compared to recent multimodal medical benchmarks.\n\n- The authors do not sufficiently compare or position HealMQA relative to large and diverse multimodal datasets that already address similar challenges at greater scale and modality coverage. For example:\n-- EHRXQA (Bae et al., NeurIPS 2023): integrates textual EHR data and chest X-ray images for multi-modal question answering on structured clinical data.\n-- MedTrinity-25M (Xie et al., arXiv 2024): provides 25 M multimodal medical pairs across 10 modalities with multi-granular annotations.\n-- WorldMedQA-V (Matos et al., Findings NAACL 2025): a multilingual, multimodal benchmark for medical reasoning and LMM evaluation across diverse clinical domains.\n\n- HealMQA lacks comparison with these stronger baselines and does not clarify its unique contribution beyond the “consumer-health” focus.\n\n- The image acquisition process is insufficiently documented, raising reproducibility and bias concerns. \n\n- The evaluation focuses on generic language metrics that poorly reflect multimodal comprehension, with limited analysis of grounding, hallucination, or safety violations.\n\n- Finally, the dataset’s scale and diversity are inadequate for benchmarking large multimodal models, and the paper does not provide clear evidence of multimodal benefit (i.e., how much images improve text-only performance)."}, "questions": {"value": "Add ablations showing how multimodal input (image + text) improves over text-only baselines.\n\nAddress copyright, licensing, and bias issues in image sourcing.\n\nProvide a detailed ethical statement covering dataset release, privacy, and intended-use limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y5dNkEEgpm", "forum": "ojvo3uYVjM", "replyto": "ojvo3uYVjM", "signatures": ["ICLR.cc/2026/Conference/Submission23141/Reviewer_ngwa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23141/Reviewer_ngwa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001065181, "cdate": 1762001065181, "tmdate": 1762942528038, "mdate": 1762942528038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}