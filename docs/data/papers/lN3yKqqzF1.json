{"id": "lN3yKqqzF1", "number": 1565, "cdate": 1756892445381, "mdate": 1759898200851, "content": {"title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "abstract": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models.\nTo the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.", "tldr": "", "keywords": ["Large Language Model", "Benchmark", "Chain of Thought"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e49a02fdc19180c30b32e07ef3413dadcf6f650.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FAITHCOT-BENCH, the first benchmark for evaluating the _faithfulness_ of individual chain-of-thought (CoT) reasoning traces. It frames faithfulness detection as a binary classification task (faithful vs. unfaithful), distinguishes two major unfaithful types—Post-hoc Rationalization and Spurious Reasoning Chain—and provides the FINE-CoT dataset (1k+ instances, 300+ unfaithful, inter-annotator κ = 0.81–0.97) with eight finer behavioral signals. The paper further proposes _Faithful-Judge_, an LLM-as-Judge method achieving ~70 F1 on this dataset."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. First systematic benchmark for _instance-level_ CoT faithfulness evaluation. \n2. Task coverage (logic, math, fact, biomedical) and multi-model sources make the dataset representative.\n3. High annotation reliability and clear taxonomy of unfaithful behaviors.\n4. Faithful-Judge provides a practical and interpretable baseline with human-aligned judgments."}, "weaknesses": {"value": "1. Faithful-Judge performance (~70 F1) still leaves wide headroom; lacks ablations (e.g., self-consistency, ensemble voting).\n2. Evaluated models are mostly medium-scale; missing comparison with newer large models (e.g., Qwen3-Max, DeepSeek-R1).\n3. The notion of “faithfulness” remains behavioral—language-level proxy rather than mechanistic validation of reasoning.\n4. Dataset size (~1k) limits generalization; no clear test of cross-domain transfer.\n5. The Layout of Figure can be Improved."}, "questions": {"value": "1. Could Faithful-Judge accuracy be further improved via self-consistency sampling, multi-judge voting, or SFT/RLAIF training?\n2. How transferable is the benchmark—does a detector trained here generalize to unseen tasks or open-domain CoTs?\n3. Just curious, what training strategies (RL, consistency regularization, causal probing) could explicitly increase model faithfulness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8LhvzzIgG", "forum": "lN3yKqqzF1", "replyto": "lN3yKqqzF1", "signatures": ["ICLR.cc/2026/Conference/Submission1565/Reviewer_cHje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1565/Reviewer_cHje"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626106196, "cdate": 1761626106196, "tmdate": 1762915817423, "mdate": 1762915817423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes a timely and significant contribution to LLM interpretability by introducing FAITHCOT-BENCH, the first comprehensive benchmark for instance-level evaluation of Chain-of-Thought (CoT) faithfulness. While prior work has shown that CoT can be unfaithful at a mechanism level, FAITHCOT-BENCH formalizes unfaithfulness detection as a discriminative task. The paper also presents a systematic evaluation of eleven existing detection methods, revealing that LLM-as-judge approaches perform best, while also highlighting the increased difficulty of detection in knowledge-intensive domains and for more advanced models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1）The paper makes a foundational contribution by formalizing instance-level CoT faithfulness as a concrete discriminative task, effectively shifting the field from abstract, mechanism-level discussions to a tangible and evaluable problem.\n2） A core strength is the methodologically robust FINE-COT dataset, which stands out for its diversity across multiple domains and models, a rigorous expert annotation process yielding high inter-annotator agreement, and a principled taxonomy of unfaithfulness.\n3）The work delivers critical empirical insights through its comprehensive evaluation, revealing that faithfulness is decoupled from correctness, that detection becomes paradoxically harder for more advanced models, and that LLM-as-judge approaches are currently state-of-the-art."}, "weaknesses": {"value": "1）The paper's conceptual framework for faithfulness is fundamentally limited. It defines faithfulness as alignment with an unobservable internal process but operationalizes it using human-annotated \"observable signals.\" This pragmatic choice means the benchmark does not measure true mechanistic faithfulness but rather alignment with a human's cognitive model of what constitutes good reasoning. This risks steering the field toward optimizing for explanations that are merely plausible to humans, rather than those that genuinely reflect the model's internal computations, potentially masking non-human-like yet valid reasoning pathways.  I think it’s better to clarify the scope of the benchmark.\n2）The study's reproducibility is severely hampered by a lack of methodological detail. Key logit-based baselines, such as 'Answer Tracing' and 'Information Gain,' are described without precise mathematical formalizations, making it impossible for other researchers to replicate these results or build upon them reliably."}, "questions": {"value": "The questions refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yuuk1AYepl", "forum": "lN3yKqqzF1", "replyto": "lN3yKqqzF1", "signatures": ["ICLR.cc/2026/Conference/Submission1565/Reviewer_Sgbw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1565/Reviewer_Sgbw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809846080, "cdate": 1761809846080, "tmdate": 1762915817262, "mdate": 1762915817262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FAITHCOT-BENCH, a benchmark for evaluating the instance-level faithfulness of Chain-of-Thought (CoT) reasoning in large language models. Unlike prior aggregate analyses, this work focuses on determining whether a specific CoT genuinely reflects the model’s reasoning process.\n\nThe authors construct FINE-COT, an expert-annotated dataset of 1,000+ CoT samples across four domains (logic, factual, math, biomedical) and four LLMs (LLaMA, Qwen, GPT-4o-mini, Gemini). Each instance includes faithfulness labels, reasoning evidence, and detailed cause annotations.\n\nThey benchmark 11 detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms. Results show that LLM-as-judge methods perform best (F1 ≈ 60–70), while counterfactual and logit-based methods lag behind. The study also highlights that correctness and faithfulness are distinct and that unfaithfulness increases in knowledge-intensive or OOD settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper introduces a human-annotated benchmark of 1000 CoT traces across four tasks, annotated with both coarse (faithful/unfaithful) and fine-grained (step-level) faithfulness labels. The dataset also includes 8 distinct unfaithfulness types and achieves high inter-annotator agreement (κ = 0.81–0.97), ensuring strong reliability.\n\n2) The authors test 11 detectors for unfaithfulness, covering counterfactual, logit-based, and LLM-judge methods, and evaluate them in a clear and consistent way across all tasks.\n\n3) They present insightful analyses like CoT faithfulness does not perfectly align with task-level accuracy, task type strongly affects faithfulness, and detection is harder in knowledge-intensive domains like TruthfulQA and biomedical QA.\n\n4) The evaluation includes four LLMs, both open-source and closed-source, demonstrating statistical significance of results."}, "weaknesses": {"value": "1) FINE-COT is built from four MCQ datasets (LogicQA, TruthfulQA, AQuA, HLE-Bio) and four models, yielding ~1k trajectories in total. This excludes code generation, multi-hop QA on long contexts, tool/agent settings, and multimodal reasoning\n\n2) The multi-round process refines categories during adjudication, then rechecks for consensus. That risks confirmation bias."}, "questions": {"value": "1) Several fine-grained signals (e.g., “Step Skipping,” “Selective Explanation Bias”) may co-occur. How did annotators handle multi-label cases?\n\n2) You report κ=81–97 across domains. Is this for the faithfulness label only, or also for reason/subtype and step-level evidence?\n\n3) You show that correctness and faithfulness diverge (four-quadrant analysis). Can you quantify mutual information between correctness and faithfulness, and report correlations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IYU1E4hYVe", "forum": "lN3yKqqzF1", "replyto": "lN3yKqqzF1", "signatures": ["ICLR.cc/2026/Conference/Submission1565/Reviewer_vr5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1565/Reviewer_vr5n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997436016, "cdate": 1761997436016, "tmdate": 1762915817130, "mdate": 1762915817130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the instance-level detection of unfaithfulness in the reasoning traces of large language models (LLMs). While prior work has provided aggregate-level evidence for unfaithful reasoning, instance-level detection remains a key open challenge. The authors formulate this as a binary classification problem and construct a human-annotated benchmark for measuring reasoning faithfulness. Using this benchmark, they analyze how unfaithfulness varies across tasks and model scales. Finally, they evaluate multiple existing detection approaches, such as counterfactual-based methods, logit-based metrics, and LLM-as-judge approaches, and find that LLM-as-judge methods perform best while providing useful insights into the feasibility of unfaithfulness detection."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The work studies an important topic in AI safety, which has practical significance. It provides a novel benchmark for measuring faithfulness in reasoning, which is a complicated metric to measure, and uses that to both evaluate faithfulness across models and tasks, and evaluate existing unfaithfulness detection methods. The most significant argued contribution of the work is to make the important but complicated notion of faithfulness more measurable, which is very important given the unfaithfulness behavior in large language models. The study is comprehensive in evaluating the faithfulness of models across tasks with the benchmark, and evaluating the possibility of detecting unfaithfulness."}, "weaknesses": {"value": "The main concern about the work is that while it defines faithfulness as “a reasoning trace reflecting the internal decisions process of the model”, it does not take the internal computations into account and instead, it heavily relies on “observable traces” and “recognizable patterns” existing in unfaithful reasoning trajectories. Therefore, instead of faithfulness, the benchmark is measuring other notions, such as consistency, transparency, and causal alignment as a proxy for faithfulness. The accuracy of this proxy for measuring faithfulness as “alignment between internal computations and CoT” should be verified with experiments. For example, one could extract other more accurate but hard to obtain ground-truth labels with counterfactual experiments or mechanistic interpretability methods and check their alignment with their labeling method. Otherwise, measuring faithfulness with the proposed benchmark might be misleading. \n\nThe work also fails to cite the following related work:\n[1] “Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations”\n[2] “Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps”\n[3] “Detecting Strategic Deception Using Linear  Probes”\n[4] “Detecting Motivated Reasoning in the Internal Representations of Language Models”"}, "questions": {"value": "The work acknowledges the challenges that arise due to “Lack of ground truth.” and “Lack of effective evaluation methods” in order to answer the instance-level question: “given a specific query, prompt, and a produced CoT, does this particular reasoning trace faithfully reflect the model’s underlying reasoning”. Yet, instead of refining the definition or proposing a method to overcome this challenge, it relies on “recognizable patterns” in the unfaithful trajectories. This approach could be misleading without verification of its effectiveness. Could the authors provide evidence that the current annotations reliably track the intended notion of faithfulness by validating the benchmark against internal-signal-based approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vo62jaheKU", "forum": "lN3yKqqzF1", "replyto": "lN3yKqqzF1", "signatures": ["ICLR.cc/2026/Conference/Submission1565/Reviewer_NhQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1565/Reviewer_NhQA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127568404, "cdate": 1762127568404, "tmdate": 1762915816929, "mdate": 1762915816929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}