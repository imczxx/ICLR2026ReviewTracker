{"id": "NWXUpwHPP7", "number": 13617, "cdate": 1758219873979, "mdate": 1759897424499, "content": {"title": "FineKB: Domain-Adaptive Issue Summarization and Cluster-Aware Retrieval for Support Knowledge Bases", "abstract": "Retrieving relevant knowledge base (KB) articles for enterprise support cases is challenging, as noisy and verbose case descriptions often exhibit low similarity to KB content. We present FineKB, a domain-adaptive system that leverages large language models (LLMs), parameter-efficient fine-tuning, and cluster-aware retrieval to improve case-to-KB matching. FineKB consists of three key components: (i) a dataset builder that generates pseudo-labeled issue summaries from historical case logs for weak supervision; (ii) a domain-adaptive issue summarizer, fine-tuned with parameter-efficient methods, to produce concise and terminology-aligned summaries; and (iii) a cluster-aware retrieval engine that constructs per-KB case clusters and indexes them with multi-centroid representations to capture diverse sub-problems. Experiments on large-scale enterprise support data demonstrate that FineKB boosts retrieval performance from under 24% Recall@3 in the KB content-only baseline to over 66% Recall@3 with cluster-aware indexing. Moreover, lightweight 8B fine-tuned models match the retrieval performance of a 70B LLM, demonstrating the efficiency of domain adaptation. These results highlight that FineKB effectively bridges the gap between noisy case descriptions and structured KB articles, offering a scalable and generalizable framework for enterprise support.", "tldr": "FineKB leverages domain-adapted LLM issue summarization and clustering-based retrieval to bridge noisy case descriptions with KB articles, improving retrieval accuracy at top-3 from 24% to 66%.", "keywords": ["Knowledge base retrieval", "Large language models", "Domain adaptation", "Summarization", "Clustering", "Information retrieval", "Enterprise support", "Weak supervision", "Fine-tuning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f641dbde76fbdef491ab8546f34e10c9efec006.pdf", "supplementary_material": "/attachment/7fea5489e890c59df0d3d25116f13382dfedf9df.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes an LLM-based retrieval method for domain-specific knowledge bases. The retrieval method uses a fine-tuned LLM-based summarizer to generate summaries which are used for indexing and retrieval, and a cluster-aware indexing method is applied. The empirical evaluations examine the effeteness of the proposed retrieval method against baseline methods, as well as the performance of the fine-tuned summarizer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed retrieval method based on LLM-generated summaries is intuitive and shows better performance than the baselines.\n\n2. The methodology description and the experimental design are clear-written."}, "weaknesses": {"value": "1. The technical contribution of this work is limited, as the main contribution seems to be the introduction of the issue summaries for improving indexing effectiveness. More discussions should be provided regarding the significance of this innovation.\n\n2. The empirical experiments are not comprehensive enough, which are based on only one dataset and limited to Llama-3.1-8B and Llama-3.3-70B. More datasets should be included in order to demonstrate the generalizable improvement of the proposed method. Similarly, more LLMs from different model families should be considered.\n\n3. The characteristics of the used dataset are unclear, raising issues regarding the reproducibility. Will the dataset (\"enterprise support cases\") be released?\n\n4. Comparisons with existing retrieval methods are lacking. Such as the classic BM25 method, or dense retrievers such as NV-Embed [1].\n\nReferences\n\n[1] Lee, Chankyu, et al. \"NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.\" The Thirteenth International Conference on Learning Representations.\n[1]"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qVCfLzCizt", "forum": "NWXUpwHPP7", "replyto": "NWXUpwHPP7", "signatures": ["ICLR.cc/2026/Conference/Submission13617/Reviewer_DjZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13617/Reviewer_DjZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717651369, "cdate": 1761717651369, "tmdate": 1762924197486, "mdate": 1762924197486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a practical RAG-based enterprise KB pipeline with three components: (i) weakly supervised, one-sentence ticket summarization to normalize noisy cases; (ii) domain-adaptive summarization using SFT/QLoRA to improve faithfulness and task fit; and (iii) per-KB, cluster-aware multi-centroid indexing to capture sub-topics within each article. The authors report large retrieval gains (e.g., Recall@3 rising from under 24% with KB-only indexing to over 66% with summary-based cluster-aware indexing) and show that 8B adapter-tuned summarizers can approach the retrieval effectiveness of a much larger 70B model, suggesting a cost-efficient path to deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a clearly defined retrieval mismatch between noisy, user-authored tickets and curated KB prose, with a transparent offline/online system decomposition that practitioners can follow.\n* The paper is not limited to large language models; it systematically evaluates small language model (SLM) adapters (e.g., 8B with QLoRA) for the summarization stage, demonstrating competitive retrieval effectiveness at materially lower compute and cost, which is critical for enterprise deployment."}, "weaknesses": {"value": "* The paper does not report latency, throughput, or cost for the end-to-end pipeline, which are essential for real-world adoption and for weighing trade-offs versus simpler baselines.\n* The core idea of the method, summarize then cluster/index, follows established enterprise RAG practice. Prior work explores similar idea of “generate proxy/summary then retrieve” or multi-signal retrieval such as HyDE [1],  Aragog [2], entity-aware summaries for retrieval alignment [3], and query-context summarized signals for retrieval in sponsored search [4], The paper’s comparisons and ablations might not be sufficient to demonstrate effectiveness over these families at matched budgets settings.\n* The evaluation of the summarization component is limited to ROUGE/BERTScore, which may not capture faithfulness, usefulness for retrieval, or end-task impact. The paper should incorporate LLM-as-judge protocols [5] with explicit rubrics on faithfulness/utility, and extrinsic metrics that reflect downstream retrieval quality\n\n[1] Gao, Luyu, Xueguang Ma, Jimmy Lin, and Jamie Callan. \"Precise zero-shot dense retrieval without relevance labels.\" In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1762-1777. 2023.\n\n[2] Eibich, Matouš, Shivay Nagpal, and Alexander Fred-Ojala. \"Aragog: Advanced rag output grading.\" arXiv preprint arXiv:2404.01037 (2024).\n\n[3] Liang, Xiao, Xinyu Hu, Simiao Zuo, Jimi He, Yu Wang, Victor Ye Dong, Yeyun Gong et al. \"What You See Is What You Get: Entity-Aware Summarization for Reliable Sponsored Search.\" In Neurips Safe Generative AI Workshop 2024.\n\n[4] Mohankumar, Akash Kumar, Gagan Madan, and Amit Singh. \"Improving Retrieval in Sponsored Search by Leveraging Query Context Signals.\" arXiv preprint arXiv:2407.14346 (2024).\n\n[5] Zheng, Lianmin, Wei-Lin Chiang, Ying Sheng, et al. 2023. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” arXiv:2306.05685."}, "questions": {"value": "* Could you provide sensitivity analyses for the clustering hyperparameters (e.g., distance threshold and minimum cluster size) and show how retrieval metrics vary across a reasonable range?\n* Could you report end-to-end latency and cost for summarization, embedding, and retrieval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P8SJuYGOuc", "forum": "NWXUpwHPP7", "replyto": "NWXUpwHPP7", "signatures": ["ICLR.cc/2026/Conference/Submission13617/Reviewer_R3RM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13617/Reviewer_R3RM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812112909, "cdate": 1761812112909, "tmdate": 1762924197214, "mdate": 1762924197214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FineKB, a domain-adaptive framework for enterprise knowledge base retrieval ￼. FineKB builds pseudo-labeled issue summaries from historical support logs, fine-tunes compact LLMs with parameter-efficient methods (SFT, QLoRA), and introduces a cluster-aware retrieval index that represents each KB article with multiple centroids to capture sub-problems. On an ad-hoc dataset, FineKB is shown to be effective. Overall, the work has very limited value for larger ICLR community, due to its applied & narrow scope."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Paper is well written and motivated. \n- Experimental code is included with submission, allowing reviewers to better verify claims and understand proposed approach."}, "weaknesses": {"value": "- The proposed approach is largely an ad-hoc application without much contextualization in existing literature. For example, when it comes to generating compressed text representations to improve retrieval, there has been a lot of work since seminal PAQ work from [Lewis et al \\(2021\\)](https://arxiv.org/abs/2102.07033). \n- The method is evaluated on a proprietary, in-house dataset. As the dataset is not public, the amount of details included in the paper are too sparse to judge effectiveness of method, and no test of the proposed method on public datasets is included. \n- in the era of LLM, ROUGE/BERTscore has been shown to penalize better summarization model [(Goyal et al 2023)](https://arxiv.org/abs/2209.12356). Using techniques that measure recall in summaries, e.g. FactScore [\\(Min et al 2023\\)](https://arxiv.org/abs/2305.14251) would be more appropriate.\n- Case summary in Table 2 barely improves over using text index. Difference might not be statistically significant."}, "questions": {"value": "Minor typos: \n- use \\citep instead of \\cite -> \"Faheem et al. (2025)\" should be \"(Faheem et al., 2025)\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6126iKJGa", "forum": "NWXUpwHPP7", "replyto": "NWXUpwHPP7", "signatures": ["ICLR.cc/2026/Conference/Submission13617/Reviewer_1YKN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13617/Reviewer_1YKN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153217265, "cdate": 1762153217265, "tmdate": 1762924196858, "mdate": 1762924196858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FineKB, a system that 1) does peft fine tuning on LLMs on LLM generated summaries, and 2) uses clusterwise retrieval to improve case-to-KB matching accuracy.  Experiments on a private dataset show reasonable performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an understudied practical challenge in enterprise support, ablating some of their choices and showing that they can reach reasonable performance even with smaller models."}, "weaknesses": {"value": "My main concern is that the paper does not present meaningful novelty, uses a simple combination of techniques, does not compare to any existing methods or baselines (apart from the ablations on their method), (Most existing RAG methods can be used/modified to support the use case), and experiments are done on a proprietary dataset that is unreleased so there is no way to compare the performance of the method to any future or previous methods. I would suggest to the authors to release a dataset (that can be constructed artificially to avoid any leakage proprietary data)."}, "questions": {"value": "How does the method compare to other common RAG methods (modified for the use case)?\nCan the authors create and release a dataset that matches the attributes that the would want in a successful case to KB matching system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T075zQf3jd", "forum": "NWXUpwHPP7", "replyto": "NWXUpwHPP7", "signatures": ["ICLR.cc/2026/Conference/Submission13617/Reviewer_EVTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13617/Reviewer_EVTi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762464654964, "cdate": 1762464654964, "tmdate": 1762924196476, "mdate": 1762924196476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}