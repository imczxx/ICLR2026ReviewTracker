{"id": "9qbKOaF8YJ", "number": 15185, "cdate": 1758248710616, "mdate": 1759897322701, "content": {"title": "Parameter Release and Knowledge Reuse for Class-Incremental Semantic Segmentation", "abstract": "Class-incremental semantic segmentation aims to progressively learn new classes while preserving previously acquired knowledge. This task becomes particularly challenging when prior training samples are unavailable due to data privacy or storage restrictions, resulting in catastrophic forgetting. To address this issue, knowledge distillation is widely adopted as a constraint by maximizing the similarity of representations between the current model (learning new classes) and the previous model (retaining old ones). However, knowledge distillation inherently preserves the old-knowledge distribution with minimal modification. This constraint limits the parameters available for learning new classes when substantial information from old classes is retained. Furthermore, the acquired old knowledge is often ignored to facilitate the learning of new knowledge, resulting in a waste of previously learned procedures. The above two problems result in the risk of class confusion and deviating from the performance of joint learning. Based on such analysis, we propose Distribution-based Knowledge Distillation (DKD) via a minimization--maximization distribution strategy. On the one hand, to alleviate the parameter competition between old and new knowledge, we minimize the distribution of old knowledge after releasing low-sensitivity parameters to old classes. On the other hand, to effectively utilize the valuable knowledge previously acquired, we maximize shared-knowledge distribution between the old and new knowledge after approximating the new knowledge distribution via Laplacian-based projection estimation. The proposed method achieves an excellent balance between stability and plasticity in nine diverse settings on Pascal VOC and ADE20K. Notably, its average performance approaches that of joint learning (upper bound) while effectively reducing class confusion. The source code is provided in the supplementary material and will be made publicly available upon acceptance.", "tldr": "This paper proposes Distribution-based Knowledge Distillation (DKD), a minimization–maximization distribution strategy. The strategy enables the average performance to approach the upper bound in class-incremental semantic segmentation.", "keywords": ["Class-Incremental Semantic Segmentation; Continual learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab0cb9208e256877021d4edcf2492662dfdd0b6c.pdf", "supplementary_material": "/attachment/77407ffc647097a4da6f6a431c401ee68c8c0aea.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a distribution-based knowledge distillation method to address catastrophic forgetting in class-incremental semantic segmentation (CISS) without storing past data. The authors argue that there is a parameter competition that standard knowledge distillation methods preserve old distributions too rigidly which limits parameter space for new classes and wasting previously acquired knowledge. They propose a min–max distribution strategy that releases low-sensitivity old parameters and minimizes the old-knowledge distribution used at the current step; estimates and reuses transferable old knowledge via a Laplacian-guided position map and a projection-based confidence map; and maximizes shared knowledge using an entropy-induced objective. In the proposed method, parameter is released by per-layer L2 norms with a pruning mask, then three losses are used for training. Experiments on standard datasets show that the proposed method has good performance across multiple incremental learning settings. I don't find the ideas and motivations to be really novel in continual learning or specific to semantic segmentation. The main contribution is mainly on implementation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The loss objectives are convincingly defined from the motivations. Experiments show consistent gains on standard datasets across incremental learning settings. The training and inference overhead is low."}, "weaknesses": {"value": "The ideas are not novel in continual learning or specific to semantic segmentation. It is hard to extract new insight from this paper into continual semantic segmentation problem."}, "questions": {"value": "1. How sensitive are results to the pruning threshold, and to per-layer and per-stage thresholding?  Are masks recomputed every epoch/step?\n2. What’s the added training wall-clock and peak memory overhead of the Laplacian and projection maps at 512 x 512? Do these costs scale linearly with resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UfvgFNRZj4", "forum": "9qbKOaF8YJ", "replyto": "9qbKOaF8YJ", "signatures": ["ICLR.cc/2026/Conference/Submission15185/Reviewer_dRri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15185/Reviewer_dRri"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608461100, "cdate": 1761608461100, "tmdate": 1762925492458, "mdate": 1762925492458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the challenges of continuous semantic segmentation by proposing a method called Distribution-based Distillation. Specifically, the authors designed three different additional loss functions to aid incremental semantic segmentation and utilized laplacian-based projection to generate two maps to assist learning. Experiments on multiple datasets demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results are very good, and the proposed distribution-based distillation is relatively novel to a certain extent."}, "weaknesses": {"value": "1. The authors propose a layer-wise pruning in order to refine the previous model. How did the authors choose the pruning threshold? Plus, is there a comparison with PackNet[1] here and an explanation of the benefits?\n\n[1] PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning, CVPR 2018"}, "questions": {"value": "See the Weaknesses Above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rn21qGr6F2", "forum": "9qbKOaF8YJ", "replyto": "9qbKOaF8YJ", "signatures": ["ICLR.cc/2026/Conference/Submission15185/Reviewer_vbrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15185/Reviewer_vbrR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840459584, "cdate": 1761840459584, "tmdate": 1762925491994, "mdate": 1762925491994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an innovative approach that combines the Parameter Release (PR) and Knowledge Reuse (KR) mechanisms to address the critical challenge of catastrophic forgetting in Class-Incremental Semantic Segmentation (CISS). The paper's motivation is clear, effectively highlighting the limitations of overly strict constraints imposed by traditional Knowledge Distillation (KD), and suggesting a solution that selectively releases model capacity to allocate space for new class learning, supplemented by the KR module to consolidate both old and new knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's motivation is clear, effectively highlighting the limitations of overly strict constraints imposed by traditional Knowledge Distillation (KD), and suggesting a solution that selectively releases model capacity to allocate space for new class learning, supplemented by the KR module to consolidate both old and new knowledge."}, "weaknesses": {"value": "‌Although DKD alleviates the competition between old and new knowledge through parameter release, it fundamentally still relies on soft target alignment from the old model. When the old model itself contains errors or biases, these issues may propagate to the new model through the distillation process. Some methods related to large models need to be considered for comparison."}, "questions": {"value": "1 Eq11 contains 4 loss terms, does it require parameters to control their weights?\n2 What is the correlation and correspondence between theoretical analysis and experimental parts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kiDMdnZqGI", "forum": "9qbKOaF8YJ", "replyto": "9qbKOaF8YJ", "signatures": ["ICLR.cc/2026/Conference/Submission15185/Reviewer_koAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15185/Reviewer_koAs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904589566, "cdate": 1761904589566, "tmdate": 1762925491480, "mdate": 1762925491480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}