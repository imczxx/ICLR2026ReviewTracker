{"id": "3axAvkv3ef", "number": 7699, "cdate": 1758032700794, "mdate": 1759897838608, "content": {"title": "ARC-Encoder: learning compressed text representations for large language models", "abstract": "Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x \\in $ {4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs.", "tldr": "", "keywords": ["LLM", "Context compression", "auto-encoder", "NLP"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd2f9ddf129d1eda7bb5fa493feffc23e824065d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed a new context compressor, ARC-Encoder to reduce the size of context representations. ARC-Encoder can generalize to many LLMs and show advanced performance on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow. The motivation of this paper is strong and practical. \n2. The method formulation is clear and reasonable.\n3. Authors conduct a comprehensive evaluation on many downstream tasks to demonstrate the effectiveness and generalizability of the proposed method."}, "weaknesses": {"value": "1. Despite of the effectiveness, the idea of context compression has been provided in many previous works, some of which are not included in the referece, e.g., [1][2][3]. This impairs the novelty of the prompsed method.\n2. A big issue that resides in context compression is inevitable information loss, especially when being tested on fine-grained retrieval task, such as Needle-in-Haystack (NIAH). The authors should provide more results on these types of tasks to demonstrate their model's  capability in handling fine-grained retrieval tasks.\n\n\n\n[1] Chevalier, Alexis, et al. \"Adapting language models to compress contexts.\" arXiv preprint arXiv:2305.14788 (2023).\n\n[2] Han, Wei, et al. \"Two are better than one: Context window extension with multi-grained self-injection.\" arXiv preprint arXiv:2410.19318 (2024).\n\n[3] Zhang, Peitian, et al. \"Soaring from 4k to 400k: Extending llm’s context with activation beacon.\" arXiv preprint arXiv:2401.03462 2.3 (2024): 5."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EIaetlkJYF", "forum": "3axAvkv3ef", "replyto": "3axAvkv3ef", "signatures": ["ICLR.cc/2026/Conference/Submission7699/Reviewer_6Bmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7699/Reviewer_6Bmd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760861286419, "cdate": 1760861286419, "tmdate": 1762919757818, "mdate": 1762919757818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARC-Encoder, a method that compresses input text into compact continuous representations to reduce context length for LLMs. It achieves efficient inference without changing the model (decoder) architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ARC-Encoder offers a solution for context compression, achieving this without altering the underlying LLM architecture.\n\n2. The method demonstrates strong empirical results over different tasks."}, "weaknesses": {"value": "1. The claim that ARC-Encoder “works seamlessly with multiple LLMs” is overstated, since in practice it still requires fine-tuning separate projectors for each target model, even if the number of parameters is small.\n\n2. The encoder model is very large (e.g., ~3B parameters), which raises serious concerns about practical efficiency. The paper should provide detailed FLOPs and latency analyses to substantiate efficiency claims.\n\n3. It is unclear how the text encoder’s embeddings are initialized, given that different LLMs have distinct embedding spaces. Without careful alignment, transferring across models could lead to catastrophic forgetting or representation collapse.\n\n4. Training such a large encoder (e.g., 3B) to replace token embeddings in decoder LLMs is resource-intensive, involving multiple training stages (pretraining, finetuning, multi-decoder adaptation). The paper should clarify why this approach is preferable to soft compression methods that require LLM fine-tuning.\n\n5. It would further strengthen the paper if the authors could expand the related work discussion to include a recent study that adopts vision encoder for token compression [ref1]. Incorporating this perspective would help situate ARC-Encoder more comprehensively within the evolving landscape of encoder-based compression approaches and clarify its unique contributions.\n\n[ref1] Vision-centric Token Compression in Large Language Model (NeurIPS 2025)"}, "questions": {"value": "1. Are the fine-tuning datasets and downstream evaluation datasets strictly out-of-domain, ensuring a fair assessment of generalization?\n\n2. In the long-context experiments, what is the maximum context length tested, and how does performance scale with length?\n\n3. Does ARC-Encoder also provide benefits in short-context scenarios, or is its advantage limited to long-context settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UphFEnFOoe", "forum": "3axAvkv3ef", "replyto": "3axAvkv3ef", "signatures": ["ICLR.cc/2026/Conference/Submission7699/Reviewer_5MmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7699/Reviewer_5MmT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489383801, "cdate": 1761489383801, "tmdate": 1762919757441, "mdate": 1762919757441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARC-Encoder, which adopts an architecture consisting of an \"encoder (based on Llama3.2 3B, with the output head and causal mask removed) + a 2-layer MLP projector\". It performs average pooling on consecutive queries in the last self-attention layer (with a compression factor of 4/8) to generate continuous representations that replace token embeddings in the decoder, without modifying the decoder. The training process employs alternating pretraining tasks of \"reconstruction-continuation\" and task-specific fine-tuning, enabling a single encoder to adapt to multiple decoders (with dedicated MLP parameters accounting for < 1%). Experiments verify that ARC-Encoder achieves performance close to the open-book baseline on QA, translation, summarization, and long-context tasks. Additionally, the storage size of compressed Wikipedia representations is comparable to that of raw text. Its core contributions lie in high compatibility, multi-decoder adaptation, and long-context extension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "ARC-Encoder does not require decoder modification, enabling adaptation to existing LLMs. For multi-decoder adaptation, only a small amount of parameters are needed, resulting in low deployment costs. It covers both short- and long-context tasks, and memory analysis supports precomputation, indicating great potential for practical application."}, "weaknesses": {"value": "It has weak innovation: its framework is highly similar to ICAE, and there are no breakthrough designs in multi-decoder adaptation or long-context strategies. Furthermore, it fails to explore performance at high compression factors (16×/32×) and generalization in professional domains, nor does it provide comparisons of inference latency in real-world scenarios."}, "questions": {"value": "Have the authors compared the performance of ARC-Encoder with similar context compression architectures like ICAE under the same settings? What are the core technical differences between them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FihPfrS2YM", "forum": "3axAvkv3ef", "replyto": "3axAvkv3ef", "signatures": ["ICLR.cc/2026/Conference/Submission7699/Reviewer_kLrY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7699/Reviewer_kLrY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814295984, "cdate": 1761814295984, "tmdate": 1762919757111, "mdate": 1762919757111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARC-Encoder, a plug-and-play encoder that compresses textual context into continuous representations for frozen LLM decoders. The approach aims to reduce inference cost while maintaining downstream performance, without modifying the decoder architecture. The work is comprehensive and empirically strong, with experiments spanning multiple decoders, tasks, and compression factors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a new formulation of context compression that does not alter the decoder. Unlike prior “memory token” or “gist token” methods, ARC-Encoder performs fixed-ratio pooling within the encoder’s attention layers and connects to decoders through a lightweight MLP. This architectural separation is elegant and conceptually clean.\n2. The authors conduct a broad and fair evaluation across multiple domains. Results show consistent improvements over strong baselines, often matching or surpassing open-book settings despite heavy compression.\n3. The analytical experiments are extensive, and ablations on pretraining tasks, pooling strategies, and encoder truncation are well thought out."}, "weaknesses": {"value": "1. This paper does not provide a deeper theoretical discussion of why pooled query averaging in attention preserves semantic fidelity or why it outperforms token-level compression. A brief analytical or representational argument could strengthen the paper’s foundation.\n2. How sensitive is performance to the dimensionality of the MLP bottleneck?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MAgIT7xlN5", "forum": "3axAvkv3ef", "replyto": "3axAvkv3ef", "signatures": ["ICLR.cc/2026/Conference/Submission7699/Reviewer_darH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7699/Reviewer_darH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921039178, "cdate": 1761921039178, "tmdate": 1762919756598, "mdate": 1762919756598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}