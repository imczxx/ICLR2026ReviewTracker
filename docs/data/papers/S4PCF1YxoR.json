{"id": "S4PCF1YxoR", "number": 19142, "cdate": 1758293832813, "mdate": 1759897057165, "content": {"title": "Representation-Based Exploration for Language Models:  From Test-Time to Post-Training", "abstract": "Reinforcement learning (RL) promises to expand the capabilities of language models, but it is unclear if current RL techniques promote the discovery of novel behaviors, or simply sharpen those already present in the base model. In this paper, we investigate the value of deliberate exploration---explicitly incentivizing the model to discover novel and diverse behaviors---and aim to understand how the knowledge in pre-trained models can guide this search. Our main finding is that exploration with a simple, principled, \nrepresentation-based bonus derived from the pre-trained language model's hidden states significantly improves diversity and pass@k rates---both for post-training, and in a novel inference-time scaling setting we introduce. (1) For inference-time, exploration with representation-based diversity improves efficiency, consistently improving pass@k rates across a variety of models and reasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50\\% improvement in verifier efficiency on almost all considered tasks. (2) For post-training, we show that integrating this exploration strategy into an RL pipeline improves reasoning performance over that of the initial model and over standard RL post-training. For example, on AIME 2024, our post-trained Qwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model, demonstrating a 3x improvement in test-time sample efficiency. Overall, our findings suggest that deliberate exploration---with the right notion of diversity---is a practical path toward discovery of new behaviors beyond sharpening.", "tldr": "We find representation-based exploration for language models is helpful both at test-time and post-training time.", "keywords": ["Exploration", "language models", "reinforcement learning", "test-time scaling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/625bf9412bfa1bb53ed10d69f8f6e87b1c4078dc.pdf", "supplementary_material": "/attachment/b3d930d5620521a6bdd54c4ee6e4c6182fc4e226.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the problem of exploration in the context of LLM reasoning. Existing RL methods are known to “sharpen” the distribution of responses v/s learning novel strategies for reasoning. To overcome this limitation, the authors propose RepExp—a simple yet principled exploration technique that leverages the internal representations of LLMs. Inspired by the literature of exploration in deep RL, RepExp encourages diversity in reasoning by providing an elliptical exploration bonus based on the model’s internal representations. The authors demonstrate the effectiveness of RepExp in two distinct settings: (1) a novel inference-time selection task, and (2) RL-based post-training. Across multiple datasets and problem types, RepExp consistently outperforms existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is clearly written and highly accessible. Moreover, the exploration problem in the context of LLM reasoning is both timely and highly relevant, addressing a critical challenge within LLM reasoning.\n\n2. The paper’s introduction of the inference-time selection task—serving both as an evaluation tool for exploration and as a standalone research problem—is an interesting and valuable addition to the study.\n\n3. The proposed method RepExp is well motivated.\n\n4. The results presented in this paper are both strong and compelling, and are effectively communicated through the well-structured Research Findings (RFs)."}, "weaknesses": {"value": "1. Section 2.1: (Line 143) ““Maximally diverse and high-probability of containing a positive response”: The inference-time selection problem is defined as selecting generations based on both diversity and quality. However, from my understanding, RepExp appears to select only for diversity. Could this lead to cases where the LLM produces hallucinated or incorrect reasoning that is nonetheless selected by RepExp due to its diversity?\n\n2. In the RL post-training setting, the covariance matrix is computed using the hidden reprs of all the responses first, and then used to calculate div(x,y). From the motivation of elliptical bonuses (Lines 201-204), div(h,h<i), is the prediction error for a new “h” which is not in the training set. In this setting, isn’t “h” already in the training set? Is this approach then still principled? (prediction error is bounded by div(h|h<i))\n\n3. “this method is history aware” – this line is a bit unclear, as it could be interpreted as div(x,y) is also conditioned on past-optimisation timesteps (which I assume is not the case). I think “history” can be a bit confusing for referring to responses which are often generated in parallel (which are therefore assigned the same generation timestep). \n\n4. How do other exploration-based methods, such as unlikeness or entropy, perform on the inference-time selection tasks? As the authors note, this setting provides a nice way to isolate and assess the impact of the exploration bonus.\n\n5. (Line 403) “not optimized … hence does not yet give a wall-clock time improvement“ – doesn’t this method have a complexity of k (forward passes) * k generations * T? How can this be improved to be faster than naive autoregressive generation?"}, "questions": {"value": "1. Have you considered persisting the covariance matrix across training generations? — Can the LLM then explore novel strategies that are applicable across multiple problems?\n\n2. Does RepExp lead to “novel” reasoning patterns? Moreover, is there a way to quantitatively assess the diversity of the generated responses—for instance, analogous to how state coverage is used in reinforcement learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DPi7KExaPK", "forum": "S4PCF1YxoR", "replyto": "S4PCF1YxoR", "signatures": ["ICLR.cc/2026/Conference/Submission19142/Reviewer_iReu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19142/Reviewer_iReu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815004729, "cdate": 1761815004729, "tmdate": 1762931158886, "mdate": 1762931158886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "•\tPrincipled and simple approach: The use of elliptical bonuses derived from model representations is conceptually clean and grounded in existing exploration theory, avoiding the complexity of auxiliary networks.\n•\tBroad empirical coverage: The paper evaluates across multiple tasks, datasets, and model families, providing a comprehensive empirical picture.\n•\tClear dataset justification: The chosen benchmarks are well-motivated and represent a diverse set of reasoning and coding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tPrincipled and simple approach: The use of elliptical bonuses derived from model representations is conceptually clean and grounded in existing exploration theory, avoiding the complexity of auxiliary networks.\n•\tBroad empirical coverage: The paper evaluates across multiple tasks, datasets, and model families, providing a comprehensive empirical picture.\n•\tClear dataset justification: The chosen benchmarks are well-motivated and represent a diverse set of reasoning and coding tasks."}, "weaknesses": {"value": "•\tOverstated claims relative to results: The introduction makes strong assertions about “moving beyond sharpening” and “substantial improvements,” but empirical gains are modest and often inconsistent—particularly in RL post-training, where RepExp sometimes performs on par or worse than baselines (e.g., Figure 6).\n•\tLimited discussion of negative results: The method degrades performance for weaker models, yet this is not adequately analyzed or contextualized.\n•\tUnclear motivation and structure: The introduction is hard to follow due to numerous forward references and misplaced motivation (appearing in Section 2). The “Contributions” subsection mixes results with claims, making it difficult to distinguish novelty from outcomes.\n•\tLack of practical relevance: The benefits at inference-time (reducing verifier calls) are meaningful only when verifier cost dominates, but the paper fails to demonstrate this in a realistic application scenario.\n•\tInconclusive post-training value: For large k, RepExp sometimes underperforms Unlikeliness or GRPO; thus, it is unclear when and why this exploration actually helps.\n•\tInsufficient alignment between claims and framing: The paper promises exploration as a path toward discovering new capabilities, but the experiments focus narrowly on pass@k efficiency, not on qualitatively novel behaviors."}, "questions": {"value": "1.\tCan the authors clarify why weaker models degrade under representation-based exploration, and whether this correlates with representational quality or another factor?\n2.\tIn the RL setting, what is the computational overhead of computing bonuses compared to standard GRPO training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cYAAxQLAV6", "forum": "S4PCF1YxoR", "replyto": "S4PCF1YxoR", "signatures": ["ICLR.cc/2026/Conference/Submission19142/Reviewer_VhJg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19142/Reviewer_VhJg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855397718, "cdate": 1761855397718, "tmdate": 1762931158453, "mdate": 1762931158453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a representation-based exploration method for large language models, leveraging elliptical bonuses computed from hidden-state representations to guide both inference-time exploration and RL post-training. The authors introduce an inference-time selection setup as a controlled environment to analyze exploration, and then integrate their findings into GRPO-style post-training. While the paper presents a broad empirical study across many models and tasks, the claimed improvements are inconsistent and often marginal, especially in the post-training regime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Principled and simple approach:** The use of elliptical bonuses derived from model representations is conceptually clean and grounded in existing exploration theory, avoiding the complexity of auxiliary networks.\n- **Broad empirical coverage:** The paper evaluates across multiple tasks, datasets, and model families, providing a comprehensive empirical picture.\n- **Clear dataset justification:** The chosen benchmarks are well-motivated and represent a diverse set of reasoning and coding tasks."}, "weaknesses": {"value": "- **Overstated claims relative to results:** The introduction makes strong assertions about “moving beyond sharpening” and “substantial improvements,” but empirical gains are modest and often inconsistent—particularly in RL post-training, where RepExp sometimes performs on par or worse than baselines (e.g., Figure 6).\n- **Limited discussion of negative results:** The method degrades performance for weaker models, yet this is not adequately analyzed or contextualized.\n- **Unclear motivation and structure:** The introduction is hard to follow due to numerous forward references and misplaced motivation (appearing in Section 2). The “Contributions” subsection mixes results with claims, making it difficult to distinguish novelty from outcomes.\n- **Lack of practical relevance:** The benefits at inference-time (reducing verifier calls) are meaningful only when verifier cost dominates, but the paper fails to demonstrate this in a realistic application scenario.\n- **Inconclusive post-training value:** For large k, RepExp sometimes underperforms Unlikeliness or GRPO; thus, it is unclear when and why this exploration actually helps.\n- **Insufficient alignment between claims and framing:** The paper promises exploration as a path toward discovering new capabilities, but the experiments focus narrowly on pass@k efficiency, not on qualitatively novel behaviors."}, "questions": {"value": "1.\tCan the authors clarify why weaker models degrade under representation-based exploration, and whether this correlates with representational quality or another factor?\n2.\tIn the RL setting, what is the computational overhead of computing bonuses compared to standard GRPO training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cYAAxQLAV6", "forum": "S4PCF1YxoR", "replyto": "S4PCF1YxoR", "signatures": ["ICLR.cc/2026/Conference/Submission19142/Reviewer_VhJg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19142/Reviewer_VhJg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855397718, "cdate": 1761855397718, "tmdate": 1763111722038, "mdate": 1763111722038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies representation based exploration for language models. An timely and highly important problem. Discovery of new and diverse behaviours would be a critical component for future revolutions in many AI4 science applications for example. \n\nThe paper is exploring this question using many established based models and a decent selection of downstream tasks including AIME."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is really nicely written and structured. There was a dedicated effort to make it easy to understand and follow the structure. \n\nThe models studied include Llama, Mistral, Qwen and Phi and the tasks are representative."}, "weaknesses": {"value": "Given the importance of the question in many domains I would have loved to see a similar study for protein language models and or how far the gap is in that area. That probably will however merit its own paper and is out of scope. \n\nRight now it states that \"In addition, we have uploaded a zip file with the complete, anonymized\ncode for all our experiments and plots.\" Will the code, the data and the experimental setup be made publicly available after acceptance?"}, "questions": {"value": "See the question for code and data in weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KopLyHJMPF", "forum": "S4PCF1YxoR", "replyto": "S4PCF1YxoR", "signatures": ["ICLR.cc/2026/Conference/Submission19142/Reviewer_gWc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19142/Reviewer_gWc2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922595022, "cdate": 1761922595022, "tmdate": 1762931157932, "mdate": 1762931157932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates representation-based exploration as a means to improve reasoning and diversity in language model behavior, both at inference time and during reinforcement learning post-training. The authors propose a simple yet principled elliptic bonus derived from a model’s hidden states to encourage exploration. Across several benchmarks (MATH, GSM8K, MBPP+, Game of 24, AIME), the approach yields significant improvements in verifier efficiency and maintains performance or improves at large Pass@k. The effect holds for larger models and, specifically, harder tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Adapting elliptical bonuses to language-model representations is well motivated, interesting, conceptually elegant, and grounded in prior exploration theory.\n- Comprehensive empirical evaluation across multiple model families, scales, and sampling strategies.\n- Significant improvements in sample efficiency."}, "weaknesses": {"value": "- The proposed exploration strategy significantly degrades performance for smaller language models (e.g., Qwen-0.5B, Mistral-7B).\n- The paper’s main narrative emphasizes discovering novel behaviors beyond sharpening, yet the results primarily reflect improvements in verifier sample efficiency. For large k, performance remains comparable to the base model, suggesting that the method broadens coverage rather than uncovering qualitatively new capabilities. The work would benefit from being reframed explicitly as a study in verifier-efficiency optimization rather than behavioral discovery.\n- In the post-training experiments, the method consistently underperforms GRPO and Unlikeliness for small values of k.\n- All benchmarks employed (GSM8K, MATH, MBPP+, AIME, Game of 24) feature cheap, automatic verifiers where verification cost is negligible relative to model inference. The paper would benefit from experiments in domains where verification is more expensive"}, "questions": {"value": "- Why did you choose Qwen-2.5-7b-Instruct for the post-training experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6HJp0t7uvu", "forum": "S4PCF1YxoR", "replyto": "S4PCF1YxoR", "signatures": ["ICLR.cc/2026/Conference/Submission19142/Reviewer_ppuG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19142/Reviewer_ppuG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996087143, "cdate": 1761996087143, "tmdate": 1762931157243, "mdate": 1762931157243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}