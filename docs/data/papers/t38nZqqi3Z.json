{"id": "t38nZqqi3Z", "number": 19766, "cdate": 1758299151000, "mdate": 1759897020815, "content": {"title": "LLM Fingerprinting via Semantically Conditioned Watermarks", "abstract": "Most LLM fingerprinting methods teach the model to respond to a few fixed queries with predefined atypical responses (keys). This memorization often does not survive common deployment steps such as finetuning or quantization, and such keys can be easily detected and filtered from LLM responses, ultimately breaking the fingerprint. To overcome these limitations we introduce *LLM fingerprinting via semantically conditioned watermarks*, replacing fixed query sets with a broad semantic domain, and replacing brittle atypical keys with a statistical watermarking signal diffused throughout each response. After teaching the model to watermark its responses only to prompts from a predetermined domain e.g., French language, the model owner can use queries from that domain to reliably detect the fingerprint and verify ownership. As we confirm in our thorough experimental evaluation, our fingerprint is both stealthy and robust to all common deployment scenarios.", "tldr": "We introduce a robust LLM fingerprinting method based on semantically conditioned watermarks", "keywords": ["LLM", "Watermarks", "Fingerprinting"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4911cedcd6bca093d5f6a34dd2c2b68316b4980b.pdf", "supplementary_material": "/attachment/4376c8553e4378fb70be62207e31518ca7eae559.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new technique for fingerprinting large language models (LLMs) that intuitively embeds a semantically conditioned watermark in the model outputs. This approach effectively combines both watermarking and fingerprinting methods. The paper demonstrates that models can be fingerprinted in specific target domains, such as French, using a Red-Green watermark to embed the fingerprint. The authors thoroughly evaluate the proposed fingerprinting technique against multiple settings, including fine-tuning, the use of meta prompts, pruning, quantization, and even active attackers. The results show robust performance, at the cost of using up to 1000 queries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper evaluates the new fingerprinting technique against multiple adversarial settings.  \n2. Assumes a realistic black-box scenario, as the fingerprinting only requires querying the target model (although 1,000 queries might be too much in some cases).  \n3. Demonstrates very strong and robust performance against adversarial settings."}, "weaknesses": {"value": "While the paper presents an overall very strong and robust performance, I have the following few comments:  \n  \n1. Utility drop:  \n    - For the utility discussion, it is mentioned that there is no significant drop. However, in Table 1, the human evaluation (HE) for Llama3.2 1b drops by ~16% (0.06 drop from 0.37), which I believe is very significant. It would be helpful if this could be discussed, and why this specific setting has such a drop.  \n     - Similarly, for the utility of the picked domain, i.e., the French benchmark (FB), it can drop by ~6%, (which to be honest i expected more). It would be helpful to discuss why it drops by 6% in some cases and why it does not drop even more, since by design the fingerprint changes this domain.  \n  \n2. The fingerprint is only evaluated on instruct models. What would the results be when fingerprinting base models? Especially when later fine-tuning these base models with an instruction tuning dataset. Would the fingerprint survive changing the prompt format?  \n  \n3. While the paper evaluates against different meta prompts (which I do really appreciate), I highly suggest trying more adversarial/aggressive meta prompts. For example, only answer weather-related queries or something that significantly changes the output distribution like \"talk like a pirate\". It would be interesting to see what the limits of the current fingerprint are.  \n  \n4. Some discussion is needed on the effect of the fine-tuned models. Do they behave the same as fine-tuning clean models, or does the fingerprint affect the models? Specifically, what is the performance when fine-tuning a fingerprinted model vs. a clean model, especially in the French domain?  \n  \n5. For translation, the same model is used to translate back and forth. Maybe changing the model that translates back can make it harder for the fingerprint?"}, "questions": {"value": "Please check my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pCGbJmJvlk", "forum": "t38nZqqi3Z", "replyto": "t38nZqqi3Z", "signatures": ["ICLR.cc/2026/Conference/Submission19766/Reviewer_p73M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19766/Reviewer_p73M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826960404, "cdate": 1761826960404, "tmdate": 1762931608934, "mdate": 1762931608934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for LLM fingerprinting, designed to be robust to post-deployment modifications (like finetuning, quantization, and pruning) and stealthy against adversaries. The authors argue that existing fingerprinting methods—which rely on a model's memorization of fixed, atypical \"query-key\" pairs—are brittle and easily filtered.\n\nThe proposed solution makes two key changes:\n1.  Semantic Domain as Query: Instead of a few fixed, atypical queries, the fingerprint is triggered by any prompt from a broad, pre-determined *semantic domain*.\n2.  Statistical Signal as Key: Instead of a fixed, atypical \"key\" response, the model is trained to embed a standard statistical watermark (like KGW) into all of its responses, if and only if the prompt belongs to the trigger domain.\n\nThis conditional behavior is \"distilled\" into the model's weights via a multi-task finetuning process: one teacher teaches the model to match the watermarked output distribution and the other teacher ensures the model's behavior remains unchanged on all other topics.\n\nDetection is then performed by sending numerous (e.g., 1000) queries from the semantic domain to the suspect API, concatenating all responses, and running a standard watermark Z-test to detect the accumulated statistical signal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's core idea is a significant conceptual leap over existing \"query-key\" fingerprinting. The direct injection of watermark output pattern into the model is clever. With the power of watermarks (e.g. robustness to post-editing), the detection becomes both robust and stealthy. \n\n- The experiments are both solid and comprehensive.\n\n- The presentation is good and easy to understand."}, "weaknesses": {"value": "The paper seems good to me."}, "questions": {"value": "This is not a \"free\" or cheap fingerprint. The method requires the model owner to perform a full finetuning pass on their model (Algorithm 1). This adds significant computational cost and time to the deployment pipeline. I'm curious about the comparison on computation costs to other baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UAhZjVOaIM", "forum": "t38nZqqi3Z", "replyto": "t38nZqqi3Z", "signatures": ["ICLR.cc/2026/Conference/Submission19766/Reviewer_BJtq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19766/Reviewer_BJtq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979622647, "cdate": 1761979622647, "tmdate": 1762931608353, "mdate": 1762931608353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for fingerprinting LLMs. Compared with previous works, including memorizing specific query-key pairs, which are brittle, failing under common deployment scenarios, the proposed method replaces the fixed query set with a broad semantic domain and replaces the fixed key with a statistical watermarking based on the KGW LLM watermark. The fingerprint is embedded by distilling this watermarked behavior specifically for the target semantic domain while using a regularization loss to preserve the model's original distribution on all other domains. The detection of the fingerprint is similar to the LLM watermark, which queries the suspicious model with multiple prompts from the semantic domain, concatenates the responses, and performs a statistical Z-test. Experiments demonstrate that the fingerprint maintains a high success rate and shows significantly improved robustness compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and structured. The threat model where previous fingerprints remain less effective is well-defined, makes sense， and the motivation is clear.\n2. The paper addresses a critical and timely problem, which is protecting the copyright of the open-weight LLM. The idea of applying the LLM watermark for fingerprint is simple, straightforward, yet effective. \n3. The experimental results are strong. The proposed fingerprint method achieves a high success rate, and shows significantly improved robustness compared to baseline methods. It shows that the fingerprint is stealthy, as its queries and responses are natural language and thus harder for a deployer to detect."}, "weaknesses": {"value": "1. **The requirement for high-entropy semantic domains and the risk of watermark leakage constrain the practical applicability and stealth of the method.** While the paper demonstrates successful fingerprinting in domains like French, Math, and Medicine, Figure 6 crucially shows that the detectability of the watermark is highly dependent on the entropy of the underlying domain. This creates a significant constraint that model providers cannot arbitrarily choose any semantic domain for fingerprinting but must select those with sufficiently high entropy to ensure reliable detection. This limits flexibility, as a preferred domain for stealth (e.g., a narrow, technical domain) might be unusable due to low entropy. Furthermore, the paper does not sufficiently investigate the risk of \"watermark leakage\" across sub-domains within the same language. For instance, if the fingerprint is embedded on the \"English + Math\" domain, it is plausible that the watermark signal could diffuse into generations for \"English + Code\" prompts, as the model might not perfectly disentangle these fine-grained semantic boundaries during distillation. This leakage would undermine stealth by making the fingerprint detectable outside its intended domain and could potentially alert a vigilant adversary who tests the model across various English-language topics. The adversary could systematically probe the model with prompts from different semantic categories to identify the trigger domain, which contradicts the claimed stealth benefits of using a semantic condition.\n\n2. **The watermark introduces a degradation in the generation quality, specifically within the targeted semantic domain.** Although Table 1 shows that benchmark accuracy (e.g., FrenchBench) remains relatively stable, Table 4 shows an obvious drop in the fingerprinted domain (e.g., for Llama3.1-8B, the score on French questions drops from 6.90 to 4.82). Table 6 further corroborates this by showing a drop in GSM8K accuracy when the model is fingerprinted on the Math domain, suggesting that even performance on technical tasks can be impacted. This indicates that while the model retains its knowledge, the watermarking process distorts the output distribution in a way that makes the text less coherent. This quality trade-off is an inherent cost of the method compared with previous fingerprint methods."}, "questions": {"value": "1. Can the watermark-based fingerprint with the same language (e.g., English) but a different domain be detected?\n2. How does the watermark strength (i.e., $\\delta$ in the KGW LLM watermark) affect the detectability of the fingerprint? Will a smaller watermark strength preserve more utility performance for the same domain task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJYlZugRBt", "forum": "t38nZqqi3Z", "replyto": "t38nZqqi3Z", "signatures": ["ICLR.cc/2026/Conference/Submission19766/Reviewer_Z7Fs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19766/Reviewer_Z7Fs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996002032, "cdate": 1761996002032, "tmdate": 1762931607378, "mdate": 1762931607378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of LLM fingerprinting, identifying which model is served by querying it. Previous methods use “atypical keys” which are brittle, and instead the authors watermark a semantic domain and then train the model to emulate the red list green list watermark.\n\nTheir results show that the watermark a) doesn’t affect performance, and b) is robust to many kinds of variations, such as pruned or quantized model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. I think the application aside, the general result is that semantic domains can be used as a backdoor trigger, and it is robust across many settings, which is an interesting result.\n\nS2. Experiments are thorough, containing analysis of many of the main points of their fingerprinting method, including many common deployment variations, such as the quantized or pruned model."}, "weaknesses": {"value": "W1. I think that the setting is a bit questionable. At least in the US, whether LLMs are protected under copyright is uncertain (as some people argue model weights are just facts, and facts cannot be copyrighted), and enforcing a license attached to them is also very uncertain. Minor point though, I would just rewrite the motivation of the paper."}, "questions": {"value": "Minor point, but it seems to me that this problem of “LLM identification” can be solved in many ways, only one of which is by planting a memorized backdoor. I think there are other types of identification, such as measuring inference time, to identify the model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m5bqcPBkK3", "forum": "t38nZqqi3Z", "replyto": "t38nZqqi3Z", "signatures": ["ICLR.cc/2026/Conference/Submission19766/Reviewer_3dY3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19766/Reviewer_3dY3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013443880, "cdate": 1762013443880, "tmdate": 1762931606721, "mdate": 1762931606721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}