{"id": "seyWxIzcAn", "number": 3456, "cdate": 1757431660939, "mdate": 1763098822395, "content": {"title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "abstract": "Diffusion-based video super-resolution (VSR) methods have recently demonstrated remarkable perceptual quality; however, their reliance on future-frame information and computationally expensive iterative denoising has restricted their application in latency-sensitive contexts. We present Stream-DiffVSR, a causally conditioned diffusion VSR framework designed for efficient online inference. Our method operates strictly with past frames and integrates three key components: a four-step distilled denoiser, an auto-regressive temporal guidance (ARTG) module that injects motion-aligned temporal cues into the denoising process, and a lightweight temporal-aware decoder with temporal processor module (TPM) that enhances spatial detail and temporal consistency. Stream-DiffVSR processes 720p frames in just 0.328 seconds on an RTX 4090 GPU, significantly outperforming previous diffusion-based methods. Compared with state-of-the-art online methods such as TMP, Stream-DiffVSR achieves a substantial improvement in perceptual quality (LPIPS improved by 0.095) while reducing inference latency by more than 130X relative to previous diffusion-based VSR approaches. These results demonstrate the potential of diffusion models for practical deployment in time-sensitive rendering pipelines and real world video super-resolution systems. Notably, Stream-DiffVSR achieves the lowest latency ever reported among diffusion-based VSR methods, reducing the initial delay from over 4600 seconds to just 0.328 seconds. This makes it the first diffusion-based solution viable for real-time online deployment.", "tldr": "", "keywords": ["Video Super-Resolution", "Online Video Restoration", "Video restoration", "Diffusion models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3baf8941e340aa11fadd369294309a797bbd2be2.pdf", "supplementary_material": "/attachment/3b97ffb328b4f0aa22c5c1123e07616fd6eee1b9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a ControlNet-based video super-resolution (SR) architecture. It starts from an image SR model, i.e., StableDiffusion ×4 Upscaler, and makes the following modifications:\n\n- A rollout distillation to distill the 50-step model to a 4-step one.\n- Making the image model an autoregressive one via taking both the current noisy latent at timestep t and warped image at timestep t-1 as conditional input.\n- Making the VAE decoder a temporal one via incorporating temporal context into decoding to enhance spatial fidelity and temporal consistency.\n\nThe proposed approach presents online potential with faster speed compared with previous ControlNet SR baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow, with clear details for each component.\n- The efficiency of the proposed approach is clear compared with other baselines in the paper.\n- Extensive experiments are conducted on both synthesis and real-world datasets, though lacking some commonly used metrics for evaluation."}, "weaknesses": {"value": "- Given the fast development of existing generative models, especially for video generation models, the necessity of continuing to modify an image model for VSR does not seem reasonable. To harness an image model for VSR, a large cost lies in improving the temporal consistency, i.e., adding temporal cues, including optical flow and the temporal decoder. Such a cost can be largely avoided when turning to adopt a video generation model, such as CogVideoX[1] and Wan2.1[2], as a base prior.\n\n- The temporal consistency of the proposed approach largely relies on optical flow from the previous frame, which suffers from a limited capability for long-term information capturing. Given the advances of recent DiT-based architectures[3,4,5], the paper lacks the discussion and comparison with this new branch of VSR approaches.\n\n- The paper mostly relies on known technologies, such as ControlNet[6], 4-step distillation from SDXL Turbo[7], and temporal decoder from Upscale-A-Video[8], making the novelty kind of incremental.\n\n[1] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. ICLR 2025.\n\n[2] Wan: Open and Advanced Large-Scale Video Generative Models. ArXiv 2025.\n\n[3] DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution. NeurIPS 2025.\n\n[4] SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training. ArXiv 2025.\n\n[5] InfVSR: Breaking Length Limits of Generic Video Super-Resolution. ArXiv 2025.\n\n[6] Adding Conditional Control to Text-to-Image Diffusion Models. ICCV 2023.\n\n[7] Adversarial Diffusion Distillation. ECCV 2024.\n\n[8] Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution. CVPR 2024."}, "questions": {"value": "My main concerns are as follows:\n\n1. The weaknesses above. Note that InfVSR [1] can be seen as concurrent work, and there is no need to compare with it. It is listed because the KV-Cache technology has become a popular way for DiT-based architecture for autoregressive generation. Considering that this paper also focuses on the autoregressive manner, the authors are expected to make a theoretical discussion between the proposed approach and the KV-Cache technology, which should be a more natural way to achieve online autoregressive VSR, in my view.\n\n2. The paper lacks some important baselines for comparison, including DOVE[2] and SeedVR2[3], which both focus on one-step VSR for high efficiency.\n\n3. Synthetic datasets are just toy examples. The authors should add more quantitative and qualitative results following previous methods. Commonly used metrics such as CLIP-IQA[4], MUSIQ[5] on real-world datasets, and warping error[6] on synthetic data should be added for better comparison.\n\n\n[1] InfVSR: Breaking Length Limits of Generic Video Super-Resolution. ArXiv 2025.\n\n[2] DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution. NeurIPS 2025.\n\n[3] SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training. ArXiv 2025.\n\n[4] Exploring CLIP for Assessing the Look and Feel of Images. AAAI 2023.\n\n[5] MUSIQ: Multi-scale Image Quality Transformer. ICCV 2021\n\n[6] Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution. CVPR 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gmXucC3Kvp", "forum": "seyWxIzcAn", "replyto": "seyWxIzcAn", "signatures": ["ICLR.cc/2026/Conference/Submission3456/Reviewer_ymVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3456/Reviewer_ymVk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716206363, "cdate": 1761716206363, "tmdate": 1762916732728, "mdate": 1762916732728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Dw50CE91oe", "forum": "seyWxIzcAn", "replyto": "seyWxIzcAn", "signatures": ["ICLR.cc/2026/Conference/Submission3456/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3456/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763098821582, "cdate": 1763098821582, "tmdate": 1763098821582, "mdate": 1763098821582, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a low-latency VSR framework termed Stream-DiffVSR. The model achieves 0.3s per frame with proposed conponents of a four-step distilled denoiser, an auto-regressive temporal guidance (ARTG) module that injects motion-aligned temporal cues into the denoising process, and a lightweight temporal-aware decoder with temporal processor module (TPM) that enhances spatial detail and temporal consistency while maintaining SOTA performance with improvement of 0.095 in LPIPS index."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper proposes a streamable VSR framework with auto-regressive manner, which is straightforward and easy to follow.\n2.\tThe proposed model achieves 0.3s per frame in only one RTX 3090 GPU while maintaining SOTA performance."}, "weaknesses": {"value": "1.\tThe novelty is limited since most of the techniques are proposed in previous works. The authors should discuss in detail the contribution and novelty of the paper and enhancement that specifically developed for VSR task.\n2.\tThe model comparison is insufficient since the paper does not provide the model size of all competitors.\n3.\tThe visualization performance shows little improvements comparing to other SOTA models. And the authors should provide VSR results on more complicated textures like text or signs restoration in video.\n4.\tThe reading flow is problematic. E.g., it is better to provide exact section number of the supplement when you want to quote it in the manuscripts."}, "questions": {"value": "Refer to the weaknesses. The authors should discuss in detail the contribution and novelty of the paper and enhancement that related to VSR. More comparisons on model size is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oFrL5AZbnG", "forum": "seyWxIzcAn", "replyto": "seyWxIzcAn", "signatures": ["ICLR.cc/2026/Conference/Submission3456/Reviewer_H5c6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3456/Reviewer_H5c6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930303979, "cdate": 1761930303979, "tmdate": 1762916732104, "mdate": 1762916732104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Stream-DiffVSR proposes an autoregressive diffusion pipeline for online video super-resolution. By distilling a 50-step teacher into 4-step sampling and conditioning solely on past frames, the paper reports >100× latency reduction relative to prior diffusion VSR methods. Experimental results on REDS4, Vimeo-90K-T, Vid4 and VideoLQ show that the proposed Stream-DiffVSR achieves a better trade-off between performance and latency among diffusion-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. Extensive ablations on step count, rollout distillation, ARTG and TPM verify component contributions.\n3. The proposed method demonstrates significantly improved runtime performance compared to previous diffusion-based approaches."}, "weaknesses": {"value": "1. Core ideas (U-Net distillation, auto-regressive temporal guidance) are existing techniques; this paper presents a careful system integration rather than a fundamentally new diffusion formulation.\n2. The employed distillation strategy is similar to ADD, with no clear technical distinctions.\n3. Distillation is only compared with the original 50-step teacher; modern fast samplers are not evaluated, leaving the optimality of 4-step design unclear."}, "questions": {"value": "1. “Low-latency” is misleading. Paper repeatedly uses “real-time”, “practical deployment”, “first diffusion solution viable for real-time online VSR” – these statements are unsupported by absolute runtime numbers.\n2. Table 4 does not include comparisons with unidirectional methods.\n3. Are the results tested in Section A4 obtained under different degradation settings? If so, please provide detailed results for each degradation level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xRATNMrPnN", "forum": "seyWxIzcAn", "replyto": "seyWxIzcAn", "signatures": ["ICLR.cc/2026/Conference/Submission3456/Reviewer_F9Cr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3456/Reviewer_F9Cr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971530140, "cdate": 1761971530140, "tmdate": 1762916731828, "mdate": 1762916731828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}