{"id": "KQBjUQMmPo", "number": 8946, "cdate": 1758103510238, "mdate": 1759897752443, "content": {"title": "Beyond Weight-Only: Mixed-Precision Quantization for BERT Weights, Activations and Embeddings", "abstract": "Pre-trained language models deliver strong performance across various Natural Language Processing (NLP) tasks but remain costly to deploy due to memory and compute demands. To address this, model compression techniques such as pruning, knowledge distillation, and quantization have emerged, with quantization gaining traction due to hardware support for low precision. While uniform and extremely low-precision quantization have shown promise, mixed-precision approaches that assign variable bit-widths to weights/activations across the model offer a superior balance between compression and accuracy. In this work, we aim to evaluate the impact of mixed-precision quantization for inference on BERT language model. Unlike prior work that often neglects activation quantization, our study systematically explores both weights and activations in mixed-precision configurations. To further improve performance, we integrate knowledge distillation into the mixed-precision pipeline. We also evaluate the impact of quantization on the embedding layer, which is generally restricted solely to quantizing token weights. Evaluated on SQuAD and GLUE benchmarks, our results achieve substantial memory and computational reductions without sacrificing accuracy.", "tldr": "A quantization-aware training tool for mixed-precision inference of LLMs", "keywords": ["Quantization", "Mixed-precision", "Inference", "Embedding quantization", "Quantization-Aware Training", "NLP", "BERT"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/268e85e66fe5eaa763dde1e96f725e3e4029a922.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends mixed-precision quantization in BERT from weights-only to weights, activations, and all embedding parameters using AdaQAT, a gradient-based QAT framework that learns per-layer bit-widths via relaxed bit assignments. The method is adapted to LLMs (actually only BERT) and coupled with knowledge distillation to preserve accuracy. The proposed method achieves state-of-the-art accuracy–efficiency tradeoffs on SQuAD and GLUE, with substantial memory and compute reductions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written."}, "weaknesses": {"value": "1. Clarification on “LLM” Definition\n\nThe authors state that they “adapt AdaQAT to Large Language Models (LLMs).” However, the experiments are conducted primarily on BERT models. In most contexts, “LLMs” typically refer to large-scale causal language models (e.g., GPT, LLaMA). It would be more precise to revise the wording or clarify whether the proposed method is also applicable to such models.\n\n2. Motivation for Learnable bit-width in Activation Quantization\n\nThe paper extends AdaQAT by introducing learnable bit-widths for activations, yet the motivation behind this design is insufficiently explained. Unlike weight quantization, activation quantization is usually intended to accelerate inference by leveraging hardware-specific low-precision compute (e.g., NVIDIA GPUs support 1/4/8-bit tensor operations). The paper does not provide an analysis or empirical evidence regarding the practical benefits, e.g., latency or throughput improvements, of the proposed activation quantization scheme.\n\n3. Outdated Baselines\n\nThe baseline methods (TernaryBERT, BinaryBERT, Q-BERT, AQ-BERT, all from 2020) are relatively outdated. To ensure a fair and convincing comparison, the authors should consider including more recent quantization methods for BERT or transformer models, which would better reflect the current state of the field."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sRcSBBD2WL", "forum": "KQBjUQMmPo", "replyto": "KQBjUQMmPo", "signatures": ["ICLR.cc/2026/Conference/Submission8946/Reviewer_QKGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8946/Reviewer_QKGj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761287659008, "cdate": 1761287659008, "tmdate": 1762920686760, "mdate": 1762920686760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends mixed-precision quantization of Transformer models beyond weights to include activations and embedding layers. Building on the AdaQAT framework, the authors integrate quantization-aware training (QAT) with knowledge distillation (KD) to learn layer-wise bit-widths for both weights and activations through gradient-based optimization. The method also quantizes all embedding components (token, positional, and key embeddings), a step often skipped in prior work. Evaluations on SQuAD v1.1/v2.0 and the GLUE benchmark show that the proposed approach can maintain accuracy close to full precision while reducing memory and computational cost by up to 1.5× over 8-bit baselines and over 10× versus FP32."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides a systematic extension of mixed-precision quantization to activations and embeddings, areas that are often neglected in existing quantization studies.\n\n2. The integration of knowledge distillation within a gradient-based mixed-precision framework is executed cleanly and improves accuracy across tasks.\n\n3.  The inclusion of embedding quantization ablation is a useful practical contribution, showing that 2-bit quantization of all embeddings can retain accuracy within 2 % of the FP32 baseline.\n\n4. The writing is generally clear. Experiment details and tables are comprehensive, allowing replication of key results."}, "weaknesses": {"value": "1. The proposed work mainly extends AdaQAT to a new domain (BERT/QAT with embeddings) without introducing a new algorithmic idea or optimization principle. The core fractional bit-width learning with finite-difference gradients mechanism is identical to existing mixed-precision schemes such as FracBits [1] and BitPruning [2]. As a result, the paper reads more like an engineering case study.\n\n2. The approach remains empirically motivated. The choice of gradient approximations, oscillation thresholds, and per-layer update heuristics (e.g., MSE-based block selection in Section 3.3) lacks theoretical justification. The paper does not analyze why the method converges or why it should outperform simpler bit-width search or sensitivity-based assignments such as HAWQ [3] or Q-BERT [4].\n\n3. The experiments rely on BERTBASE with SQuAD and GLUE, models that no longer reflect the current state of quantization research or hardware trends. Modern compression benchmarks typically evaluate on larger or more diverse architectures (e.g., RoBERTa, LLaMA, Qwen). Evaluating only on BERTBASE limits generality and makes the reported gains less impactful.\n\n4. There is no analysis or ablation explaining why mixed-precision across activations and embeddings helps. No sensitivity study, gradient statistics, or visualization of learned bit-width distributions (beyond histograms) is provided to substantiate the observed improvements.\n\n5. Accuracy improvements over existing mixed-precision baselines are relatively small. The practical savings (1.5× memory vs 8-bit baselines) are incremental, especially given the heavy training cost of QAT + KD.\n\n6. The work compares primarily against Q-BERT [4], AQ-BERT [5], and Ternary/Binary BERT, missing recent optimization-based or hardware-aware methods such as SDQ [6], OMPQ [7], or orthogonal mixed-precision frameworks used in contemporary transformers.\n\nReferences\n\n[1] L. Yang & Q. Jin. FracBits: Mixed Precision Quantization via Fractional Bit-Widths. AAAI 2021.\n\n[2] M. Nikolić et al. BitPruning: Learning Bitlengths for Aggressive and Accurate Quantization. arXiv 2020.\n\n[3] Z. Dong et al. HAWQ: Hessian-Aware Quantization with Mixed Precision. ICCV 2019.\n\n[4] S. Shen et al. Q-BERT: Hessian-Based Ultra-Low-Precision Quantization of BERT. AAAI 2020.\n\n[5] C. Zhao et al. Automatic Mixed-Precision Quantization Search of BERT. arXiv 2021.\n\n[6] X. Huang et al. SDQ: Stochastic Differentiable Quantization with Mixed Precision. ICML 2022.\n\n[7] Y. Ma et al. OMPQ: Orthogonal Mixed-Precision Quantization. arXiv 2021."}, "questions": {"value": "1. What prevents AdaQAT from diverging when applied jointly to activations and embeddings? Have you observed instability in fractional bit-width updates?\n\n2. Can the authors justify the choice of the MSE criterion for layer selection? Did other metrics (e.g., Hessian-based) yield similar behavior?\n\n3. How sensitive are the results to the regularization coefficient or the oscillation threshold?\n\n4. Could the method generalize to larger or instruction-tuned transformers (e.g., LLaMA or RoBERTa), or is it tied to BERT-style architectures?\n\n5. Have you benchmarked actual inference latency on hardware, not just BitOPs estimates, to verify real speed gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sErVtOC0ov", "forum": "KQBjUQMmPo", "replyto": "KQBjUQMmPo", "signatures": ["ICLR.cc/2026/Conference/Submission8946/Reviewer_UWDy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8946/Reviewer_UWDy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970505991, "cdate": 1761970505991, "tmdate": 1762920686236, "mdate": 1762920686236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies existing AdaQAT method to quantize BERT model, covering weights, activations, and embedding layers. The authors integrate layer-wise mixed precision and knowledge distillation into the framework and evaluate on GLUE and SQuAD."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "It considers quantization for weight, activation, and embedding layers, rather than only weight quantization."}, "weaknesses": {"value": "The novelty of the method is unclear. AdaQAT has been published previously, and the extensions to per-layer optimization and adding distillation appear to be straightforward incremental improvements without new algorithmic techniques.\n\nThe baseline comparison does not fully establish superiority. In Table 2 the method compares to BinaryBERT and TernaryBERT, which have different precision targets. Though AdaQAT has better accuracy than BinaryBert, BinaryBERT has significantly lower memory footprint and BitOPs cost, making the comparison inconclusive in terms of efficiency versus quality trade-off. A more comprehensive comparison against methods targeting similar bit configurations would improve fairness and clarity.\n\nMore recent quantization approaches are missing from the evaluation, such as QuaRot and other mixed-precision or QAT/PTQ transformer quantization techniques. The method should also report results across a range of bit-width settings to better demonstrate flexibility and the quality-efficiency trade-offs.\n\nThe experimental scope is limited. The evaluation is restricted to BERT on GLUE and SQuAD, which are foundational but now considered relatively simple and saturated benchmarks. To support broader impact claims, it would be important to evaluate on more modern language models such as RoBERTa, DeBERTa, or other transformer encoders, and on more challenging benchmarks including retrieval tasks, long-context understanding, code modeling, or multilingual QA. These settings would better reflect current deployment-relevant scenarios and demonstrate robustness of the proposed method."}, "questions": {"value": "Could you analyze and demonstrate your method on more model architectures beyond BERT, for example RoBERTa, DeBERTa, or LLaMA-based encoders?\n\nCould you evaluate on modern and more challenging datasets beyond GLUE and SQuAD, such as multilingual or retrieval benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5p8xeQrdMA", "forum": "KQBjUQMmPo", "replyto": "KQBjUQMmPo", "signatures": ["ICLR.cc/2026/Conference/Submission8946/Reviewer_5MRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8946/Reviewer_5MRs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015106880, "cdate": 1762015106880, "tmdate": 1762920685837, "mdate": 1762920685837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s core idea—mixed-precision quantization for BERT (weights, activations, and embeddings)—was an active topic around 2019–2021, but has since been superseded by post-training quantization (PTQ) and hardware-aware inference optimization methods. The community’s focus has decisively shifted toward scalable low-bit inference for large language models (LLMs) and hardware-co-designed quantization (e.g., FP8, FP4, mixed-KV cache quantization), while this work remains confined to fine-tuning BERT on GLUE/SQuAD."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a systematic exploration of mixed-precision quantization for BERT, covering weights, activations, and embeddings, and reports empirical results on SQuAD and GLUE benchmarks. The overall presentation is clean, and the authors attempt to integrate knowledge distillation to mitigate quantization degradation. The topic—efficient model compression—is broadly relevant to the community."}, "weaknesses": {"value": "This work feels substantially outdated both in scope and technical contribution. The core idea of adaptive mixed-precision quantization for Transformer models was well-studied in 2019–2021 (e.g., Q-BERT, HAQ, AdaQuant, and related works). The paper does not offer any novel algorithmic insight or quantization formulation beyond existing methods; rather, it re-implements known approaches with minor variations.\n\n\nThe experimental setup is limited to BERT-base and legacy benchmarks (GLUE, SQuAD), which are already saturated and no longer representative of current research directions. No results are provided on modern architectures (e.g., decoder-only LLMs) or larger-scale models.\n\nEven for the BERT model, the final weight activation bits shown in Appendix 3 seems to tell us that uniform w2a4 is pretty much good if we train longer and search more better hyper parameters. There seems less value in the  BitOPs, saving extra 10-20MB is not too helpful given the modern hardware."}, "questions": {"value": "The learning rate is one of the most critical hyperparameters in Quantization-Aware Training (QAT). However, the authors only experiment with a single value (2e-5), which is quite limited and raises concerns about whether the reported results are robust to different training configurations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SExdn1eWTO", "forum": "KQBjUQMmPo", "replyto": "KQBjUQMmPo", "signatures": ["ICLR.cc/2026/Conference/Submission8946/Reviewer_WSap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8946/Reviewer_WSap"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762440463755, "cdate": 1762440463755, "tmdate": 1762920685299, "mdate": 1762920685299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}