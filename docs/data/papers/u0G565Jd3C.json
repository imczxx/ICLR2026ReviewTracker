{"id": "u0G565Jd3C", "number": 7789, "cdate": 1758036446847, "mdate": 1759897832303, "content": {"title": "Semantic-aware Pruning of Large Language Models via Neuron Importance Explanation", "abstract": "Large language models (LLMs) demonstrate unprecedented capabilities across diverse applications, yet their extensive parameterization creates substantial computational and memory requirements that hinder practical deployment. \nWhile structured pruning shows promise for LLM compression, existing methods use static masks that cannot adapt to different inputs, limiting performance across diverse tasks.\nIn this work, we present \\textsc{SeAP}, a novel semantic-aware structured pruning framework that adaptively identifies optimal masks based on input semantics at the pre-fill stage. Our framework features two key components: (1) an explainability-guided importance estimation that uniquely fuses local and global neuron importance to discover diverse representative mask patterns from calibration data's intrinsic characteristics, and (2) a lightweight router-based module through iterative refinement that efficiently assigns optimal masks for each input prompt. Experimental results on LLaMA-2-7B, LLaMA-2-13B and Phi-2 demonstrate that \\textsc{SeAP} outperforms state-of-the-art structured pruning methods across diverse language modeling and commonsense reasoning tasks, achieving competitive performance with reductions in memory and inference latency.", "tldr": "", "keywords": ["Model Compression; Large Language Models; Explainable ML;"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98b0f4c6e4669e66172666465c2c91eeab67716c.pdf", "supplementary_material": "/attachment/99f19654fdb1f8ed76a8a7d4297e5ee475c08d4c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SEAP, a LLM structured pruning framework. SEAP tackles the key limitation of static pruning, i.e., a single, input-agnostic mask, by learning a set of static pruning masks using calibration data and dynamically selecting the most suitable mask in inference stage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea is intuitive and effective: adopting diverse, input-specific masks is inherently superior to a \"one-size-fits-all\" approach. This principle also enables the proposed LRP metric to surpass alternatives.\n\n2. The main experiments are comprehensive, benchmarking SEAP against solid baselines (FLAP, SliceGPT, DISP-LLM) on extensive tasks (PIQA, OpenbookQA) and perplexity. The outstanding perplexity results, a low-noise metric, are particularly convincing and demonstrate SEAP's superiority.\n\n3. Furthermore, thorough ablation studies confirm the positive contribution of each component, while latency tests validate the method's practical value."}, "weaknesses": {"value": "1. The primary drawback is the increased training complexity compared to other methods. SEAP involves a complex training pipeline, limiting its practical utility. This trade-off between training cost and inference performance should be clearly positioned.\n\n2. The router is trained on five commonsense reasoning datasets and tested on two held-out tasks. While this shows some generalization, the robustness of the router to completely OOD tasks remains unexplored, like tasks involves generation. The router's effectiveness is central to the method, and its OOD performance is a key practical concern.\n\n3. Furthermore, applying the same mask to both prefilling and decoding is reasonable for simplicity, but this approach may be suboptimal for performance. Additionally, the authors do not appear to have separately evaluated the model's generation capabilities."}, "questions": {"value": "I request clarification on how both low-latency loading and memory reduction are achieved simultaneously:\n\n(a) Are all $K$ sparse sub-models (one for each mask) pre-loaded into memory, allowing the router to simply select which model to execute? This would explain the low switching cost but would seem to *increase* total memory overhead, not reduce it.\n\n(b) Or, is the full dense LLM retained in memory, with the selected mask applied dynamically via efficient indexing operations during the forward pass? This would explain the low loading time (as no new parameters are loaded), but it would not achieve the stated memory reduction, since the dense weights remain resident.\n\nThis is a crucial point for evaluating the practical viability and hardware efficiency of SEAP. I will adjust my score until I receive a satisfying response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o8gdEg4ZBG", "forum": "u0G565Jd3C", "replyto": "u0G565Jd3C", "signatures": ["ICLR.cc/2026/Conference/Submission7789/Reviewer_5DkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7789/Reviewer_5DkL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530199192, "cdate": 1761530199192, "tmdate": 1762919836366, "mdate": 1762919836366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Semantic-Aware Pruning (SAP) framework that removes redundant neurons or attention heads in pretrained LLMs by aligning pruning decisions with semantic representations derived from internal activations.\nSpecifically, SAP computes token-level semantic saliency via a lightweight projection network and prunes parameters that contribute least to contextual semantics, followed by layer-wise recalibration via a semantic consistency loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is clearly written and supported by well-designed figures that effectively illustrate the proposed framework and its workflow.\n\n2. It provides insightful qualitative visualizations that relate semantic heatmaps to the spatial distribution of pruned neurons, enhancing interpretability.\n\n3. Comprehensive layer-wise ablation studies are presented, demonstrating partial robustness of the proposed semantic scoring mechanism across network depths."}, "weaknesses": {"value": "1. The idea overlaps with activation- or gradient-based approaches such as SparseGPT [1], Wanda [2], and SlimGPT [3], which also compute importance from activations or local sensitivity. Recent works like SoBP [4] and CALDERA [5] have incorporated structured or low-rank regularization to improve efficiency.\n\n2. The cosine-similarity measure between contextual embeddings and activations lacks a theoretical link to model contribution.\n\n3. Experiments emphasize perplexity and a few GLUE-style tasks but omit instruction-following or reasoning benchmarks that test semantic retention. Baselines exclude structured compression methods such as SVD-LLM [7] and BitDistiller [8].\n\n4. The semantic projection and layer-wise recalibration require multiple fine-tuning epochs. Compared with one-shot magnitude pruning [1] or low-rank compression [7], SAP appears computationally heavier, without an analysis of total cost.\n\n5. The experimental analysis is primarily on LLaMA-2-7B and Mistral-7B, which are now outdated baselines. It would be better if the authors could benchmark against at least LLaMA-3 and Qwen-2.\n\n[1] Frantar E., Alistarh D. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. 2023.\n\n[2] Sun Y. et al. Wanda: A Simple and Effective Pruning Approach for Large Language Models. 2023.\n\n[3] Liu Z. et al. SlimGPT: Layer-wise Structured Pruning for Large Language Models. 2024.\n\n[4] Zhang H. et al. SoBP: Structured Optimal Brain Pruning for Large Language Models. 2024.\n\n[5] Li T. et al. CALDERA: Compressing LLMs Using Low Rank and Low Precision Decomposition. 2024.\n\n[6] Guo M. et al. Automatic Network Pruning via HSIC Lasso under the Information Bottleneck Principle. 2023.\n\n[7] Wang X. et al. SVD-LLM: Structured Low-Rank Compression for Large Language Models. 2025.\n\n[8] Zhao R. et al. BitDistiller: Distilling LLMs into Binary and Low-Bit Networks. 2024."}, "questions": {"value": "1. How does SAP differ algorithmically or conceptually from prior activation- or sensitivity-based pruning methods such as SparseGPT [1], Wanda [2], and SlimGPT [3], beyond the addition of the semantic projection module?\n\n2. Can the authors provide theoretical or empirical evidence demonstrating that cosine similarity between contextual embeddings and activations reliably reflects a parameterâ€™s contribution to model output?\n\n3. Have the authors considered evaluating SAP on instruction-following, reasoning, or multilingual benchmarks, and comparing against structured compression baselines such as SVD-LLM [7] and BitDistiller [8] to strengthen generality claims?\n\n4. Could the authors quantify the additional fine-tuning cost introduced by the semantic projection and layer-wise recalibration stages in terms of GPU hours or wall-clock time, relative to one-shot pruning baselines like SparseGPT [1]?\n\n5. Do the authors plan to extend their experiments to more recent model families (e.g., LLaMA-3, Qwen-2) to validate the scalability and robustness of SAP on current-generation architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xax8XozQrp", "forum": "u0G565Jd3C", "replyto": "u0G565Jd3C", "signatures": ["ICLR.cc/2026/Conference/Submission7789/Reviewer_zCcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7789/Reviewer_zCcJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958582067, "cdate": 1761958582067, "tmdate": 1762919833960, "mdate": 1762919833960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SEAP, a semantic-aware structured pruning framework for LLMs. Unlike conventional static pruning approaches that apply fixed masks to all inputs, SEAP dynamically selects pruning masks according to input semantics during the pre-fill stage. The approach combines two techniques:\n1) Explainability-guided neuron importance estimation, fusing local activations and global relevance scores via Layer-wise Relevance Propagation (LRP) to identify critical neurons.\n2)A lightweight router module, trained through iterative co-optimization with mask fine-tuning, to assign the most suitable pruning mask for each input.\nExperiments on LLaMA-2-7B, LLaMA-2-13B, and Phi-2 demonstrate consistent improvements over structured pruning baselines (e.g., DISP-LLM, SliceGPT, FLAP) on both language modeling and reasoning benchmarks (WikiText-2, PTB, ARC-E/C, HellaSwag, PIQA, BoolQ, OBQA, Winogrande)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Increasing the efficiency of LLMs is a practical and important research topic.\n2. Results are reported on multiple model scales and datasets."}, "weaknesses": {"value": "1. Dynamic pruning appears promising in terms of improving performance, but its practicality for efficient deployment, particularly on edge devices, remains questionable. When multiple masks are maintained, both the storage and memory footprints can increase significantly compared to static pruning methods. It's better to show a storage and memory comparison with existing methods. For static pruned methods, they are able to be stored in a compact format, which is lighter than the original dense model. But the dynamic method can't benefit from this. Moreover, it is unclear how the proposed method manages on-device mask switching. Whether the reported 0.047s parameter loading overhead fully accounts for the complete model-switching process (i.e., offloading weights from the previous sample and reloading weights for the current one).\n2. It remains unclear how robust the learned semantics are when domain or task distributions shift significantly."}, "questions": {"value": "1. How does SEAP behave when inference data diverges substantially from the calibration distribution?\n2. How is the model stored under the multiple-mask scenarios? What is the memory and storage cost of the proposed method, and how is it compared to static pruned methods?\n3. How does the method handle the on-device mask switching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kT4DGn3wqd", "forum": "u0G565Jd3C", "replyto": "u0G565Jd3C", "signatures": ["ICLR.cc/2026/Conference/Submission7789/Reviewer_2ze6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7789/Reviewer_2ze6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138401343, "cdate": 1762138401343, "tmdate": 1762919833485, "mdate": 1762919833485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}