{"id": "4ftPHo58O9", "number": 14032, "cdate": 1758227322424, "mdate": 1759897394816, "content": {"title": "Towards Universal Semantics with Large Language Models", "abstract": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.", "tldr": "", "keywords": ["natural semantic metalanguage", "large language models", "semantics", "universal semantics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a402a0b8e96c945bdae1d51208660b59bb4bfac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on a study that uses Large Language Models (LLMs) to address the issue of \"linguistic semantic generalization\". Simply put, it aims to enable more accurate and consistent expression and translation of meanings across different languages. The core approach is to integrate a theory called \"Natural Semantic Metalanguage (NSM)\".\n\nThe NSM theory posits that all languages share a set of \"universal semantic primitives\" (such as \"I\", \"you\", \"good\", \"bad\", \"time\"—these are the simplest words). No matter how complex a word is, it can be rephrased using these \"primitives\". For example, the word \"ill\" can be explained as \"something bad is happening inside a person’s body, and the person can feel this badness\". This type of explanation is called an \"explication\". It allows for accurate cross-linguistic translation without unnecessary complexity.\n\nFor instance, when translating the English word \"color\" into a language that has no dedicated color terms (e.g., Warlpiri), the approach first uses NSM to \"explicate\" \"color\" into simple sentences. This makes the subsequent translation much easier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It is the first study to combine Large Language Models (LLMs) with the Natural Semantic Metalanguage (NSM) framework. This breaks the limitation of traditional NSM explications, which relied on manual generation. No previous research has explored the application of LLMs in generating NSM explications. This study fills this gap and provides a new technical pathway for universal semantic representation.\n\nIt proposes an automated evaluation method for NSM explications, covering three core dimensions: validity, descriptive accuracy, and cross-linguistic translatability. Additionally, it constructs the first custom dataset for this task (containing approximately 44,000 entries of \"word-example-explanation\") and develops the DeepNSM fine-tuning model.\n\nThe DeepNSM model outperforms general-purpose LLMs such as GPT-4o and Gemini in key metrics. In cross-linguistic translatability tests for low-resource languages (e.g., Aruul, Abkhaz), it also achieves better performance in indicators like BLEU scores. This provides a feasible solution for practical needs such as low-resource language translation and cross-cultural communication."}, "weaknesses": {"value": "The example sentences and candidate explications in the dataset mainly rely on LLM generation. Although automated screening was conducted, the linguistic biases of LLMs (e.g., English-centrism) may be transferred to the dataset. This affects the accuracy of the model’s explications for words from non-English cultural backgrounds. Furthermore, manual verification only covers 149 entries in the test set, with no large-scale manual validation of the training set.\n\nThe study primarily builds the dataset and trains the model based on English vocabulary. While it mentions that the approach can be extended to other languages, it does not provide experimental validation for non-English languages (e.g., it does not test the effectiveness of NSM explication generation for languages like Chinese or Spanish). This makes it impossible to confirm the method’s generalization ability in multilingual scenarios. Additionally, the testing of low-resource languages is limited to 5 languages, with a small sample size.\n\nThe dynamic adaptability of semantic primitives is not discussed. The semantic primitives of the NSM framework are regarded as \"universal and fixed\", but the paper does not explore whether these primitives need to be adjusted according to linguistic or cultural differences. For example, some low-resource languages may have unique core semantic units. It also fails to analyze the adaptability of semantic primitives when the model processes culture-specific words (e.g., emotion-related words or words specific to a particular region)."}, "questions": {"value": "1. Since the example sentences and candidate explications in the dataset rely on LLM generation, how can the impact of LLMs’ linguistic or cultural biases on dataset quality be quantified and mitigated?\n\n2. For non-English languages (e.g., Chinese, Swahili), is it necessary to adjust the selection of semantic primitives when building the dataset? What is the specific logic behind such adjustments?\n\n3. The paper mentions that the method can be extended to other languages, but it does not provide experimental data for non-English languages. Can additional experiments on NSM explication generation for languages like Chinese or Spanish be supplemented to verify the multilingual generalization ability? For low-resource languages without clear validation of semantic primitives (e.g., some African tribal languages), how can the equivalence of semantic primitives be determined?\n\n4. The paper does not compare the effectiveness of combining LLMs with other semantic frameworks (e.g., FrameNet, WordNet semantic decomposition) versus combining LLMs with NSM. Why was NSM chosen over other frameworks, and how were its core advantages quantified in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzilkSwD8P", "forum": "4ftPHo58O9", "replyto": "4ftPHo58O9", "signatures": ["ICLR.cc/2026/Conference/Submission14032/Reviewer_dV16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14032/Reviewer_dV16"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808506540, "cdate": 1761808506540, "tmdate": 1762924521454, "mdate": 1762924521454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first study on using large language models to automate the generation of \"explications\" within the Natural Semantic Metalanguage (NSM) framework. NSM is a linguistic theory proposing that any word's meaning can be paraphrased using a small, universal set of \"semantic primes,\" offering a path to universally translatable semantic representations. The authors identify the slow, manual creation of these explications as the primary bottleneck to NSM's adoption in NLP. To address this, the paper introduces three core contributions: (1) a novel set of automatic evaluation metrics to assess explication quality, including \"legality\" (prime usage) and \"descriptive accuracy\" (a clever \"substitutability test\" using LLM log-probabilities); (2) a new, high-quality dataset of ~44,000 explications, bootstrapped by generating candidates with an LLM and filtering them using their proposed metrics; and (3) fine-tuned 1B and 8B models (\"DeepNSM\") that outperform strong baselines, including GPT-4o, on this new task. The authors demonstrate that the resulting NSM explications show significantly higher cross-translatability into low-resource languages than standard dictionary definitions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is an excellent and thoughtfully constructed paper.\n\n* Originality: The paper's originality is outstanding. It is, to my knowledge, the very first work to formally bridge the gap between modern LLMs and the well-established (in linguistics) NSM framework. This is a novel and exciting problem formulation. The authors don't just apply an LLM to a task; they propose a complete \"stack\" for a new sub-field: evaluation, data, and models.\n\n* Quality: The methodological quality is very high. The authors correctly identified that the primary challenges were the lack of evaluation metrics and data, not just the lack of a model.\n\n* The proposed \"Substitutability Score\" (Section 3.2) is the standout contribution. It is a highly creative and intelligent solution for automatically evaluating a complex semantic property. Using an LLM as a \"grader\" and decomposing the score into baseline accuracy (Δ_baseline), minimality (Δ_min), and entailment capture (Δ_ent) is a very robust approach.\n\n* The \"generate-and-filter\" pipeline for dataset creation (Section 4) is also excellent. Using their own newly-defined metrics to filter the dataset is a very smart way to bootstrap quality, and the ablation study (comparing DeepNSM to its unfiltered-data-trained variant in Table 1) clearly demonstrates the value of this filtering step.\n\n* Clarity: The paper is exceptionally well-written. It takes a concept (NSM) that is likely foreign to most of the ICLR community and explains it clearly, concisely, and with effective examples (Figure 1). The entire paper is logically structured, and the figures (especially Figure 3, illustrating the data pipeline) are highly effective.\n\n* Significance: The potential impact of this work is high. A scalable, grounded, and interpretable universal semantic representation could have major implications for low-resource NLP, machine translation, cross-lingual reasoning, and AI safety/interpretability. By open-sourcing the metrics, dataset, and models, the authors have provided all the necessary tools for the community to build upon this work. The strong cross-translatability results (Table 2) are a powerful proof of concept for the \"universal semantics\" claim."}, "weaknesses": {"value": "* **Validation of Proposed Metrics**: The entire paper's success hinges on the validity of the new automatic evaluation metrics (Legality and Substitutability). The authors briefly state that \"Metrics Align with Qualitative Judgements\" (Section 5), but this is presented as a summary (e.g., \"DeepNSM models received top rankings 46% of the time\"). This is insufficient. A formal, quantitative correlation analysis (e.g., Pearson or Spearman) between the \"Explication Score\" and blind human ratings is necessary to truly validate that this metric is a reliable proxy for human-judged quality.\n\n* **Sensitivity of the Substitutability Score**: The Substitutability Score is a complex formula combining three deltas and averaged across multiple \"grader LLMs.\" The paper does not provide an ablation or sensitivity analysis on this. For instance, how sensitive is the score to the choice of grader LLMs? What is the contribution of each component (Δ_baseline, Δ_min, Δ_ent) to the final score?"}, "questions": {"value": "* Could the authors please provide a quantitative correlation (e.GET., Pearson's r) between the proposed \"Explication Score\" and human-annotated quality scores? This would significantly strengthen the trust in the paper's central claim, which relies on this new metric.\n\n* Would it be possible to provide a breakdown of the Substitutability Score into its components (Δ_baseline, Δ_min, Δ_ent) for the models in Table 1? This would provide crucial insight into the nature of the DeepNSM models' improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XP5t2rlqlY", "forum": "4ftPHo58O9", "replyto": "4ftPHo58O9", "signatures": ["ICLR.cc/2026/Conference/Submission14032/Reviewer_JpBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14032/Reviewer_JpBr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845683294, "cdate": 1761845683294, "tmdate": 1762924521099, "mdate": 1762924521099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the framework of Natural Semantic Metalanguage (NSM) and the explications of word meanings based on NSM semantic primitives would be useful in NLP applications. They explore whether LLMs can be applied to generate good NSM explications, since usually NSM explication generation by human experts is labor intensive. The question of whether NSM explications are actually useful for NLP applications is a separate one, and the paper also touches on this via a low-resource translation experiment. I think my major issue with this paper is conceptual, having to do with buying into the premise that NSM is a useful framework, although I have several methodological concerns as well, one of the major concerns being the lack of human verification of whether the explications actually do capture the original lexical meaning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The motivation behind the work - more generalizable methods to low resource languages - is something that the field should care about more."}, "weaknesses": {"value": "My main issue with this work is that I have a difficulty convincing myself that the theoretical grounding that the work is built upon is sound and the approach of NSM primitive-based explication is actually a useful way to approach things. I confer that this may be partly due to my own background and priors, and would be open to discussions. Here are some thoughts in this regard:\n- Overall framing issues and lack of citations to work being criticized: I don't have a guess about what background of the authors are, but as a self-identified linguist, a lot of the claims and statements in this paper don't sit well with me. For instance, from the very first sentence: \"Semantics, the study of word meaning\" --- I cannot say I agree with reducing \"semantics\" to the study of word meaning; calling dictionary definitions a \"conventional semantic approach\" also seems puzzling to me. These claims are also made without references, so it's hard for the readers to evaluate where such claims are even coming from. Furthermore, even the NSM literature isn't being properly engaged with: e.g., Figure 1a) seems to be just taken from Wikipedia without citing it; the Wikipedia article cites a book as a source for it, which isn't cited in the paper.\n- The validity of NSM as a theory of meaning: I also was not sure I was convinced about the status of Natural Semantic Metalanguage as a valid theory or NSM primitives as a valid theoretical construct. From the perspective of someone who's not directly aware of the NSM theoretical enterprise but have linguistics training, I'm not sure the fundamental assumption of NSM as described in the paper, \"These primes are considered primitive because they represent fundamental semantic units that cannot be defined in simpler terms\" is entirely correct based on the list of natural semantic primes proposed. For instance, concepts like live/die could be expressed in terms of one another by negating one (\"NOT\" is a primitive, after all). The table lists \"WANT\" and \"DON'T WANT\" as primitives, and given my previous comment I think one can easily see why it's problematic for the claim that \"semantic primes are fundamental semantic units that cannot be defined in simpler terms\".\n- I also wasn't sure if the NSM explication in in Figure 1b is (1) correct and (2) actually useful. One general issue I saw (from this example and Figure 9, Appendix E) is what NSM explications in this paper are explications of. Are they explications of the standalone lexical meaning, or of the full expression that contains the lexical item? For example, in the paraphrase example given in Figure 1b supposed to be corresponding just to the meaning of the word \"sick\"? This definitely cannot be an paraphrase of the standalone lexical meaning, because it contains information about the gender of the person who is feeling sick by using the word \"she\". In general, it seemed to me that NSM explications (based on the examples given in the paper) are neither paraphrases of the lexical meaning or the full expression containing them. If the former, the current explications are incorrect because they add meanings that are not present in the lexical meaning. If the latter, the current explications miss a lot of information present in the expressions (e.g., in the \"emergency\" example in Figure 9, the paraphrase misses information about the traffic accident).\n- These observations makes me doubt the theoretical groundings and empirical validity of the framework itself. However, independently of these concerns, it is not impossible that the proposed approach of paraphrases using simpler concepts is useful for certain application scenarios. But if I were to be convinced on that level of utility, I would like to see a paraphrase of the meanings of words/phrases like \"neural networks\" or something simpler like \"cats\" or \"green\" via the primes in Figure 1a, since the claims seems to be that this IS an exhaustive list of semantic primes in English and I personally have a hard time seeing these expressed in those terms. Color is even mentioned as a domain that would benefit from NSM in Section 2.2, but I think examples would really help.\n- There is also a question of whether the NSM explications will actually be useful to humans, as this is stated as one of the goals (L150: \"Models that generate text using semantic primes could make the outputs of LLMs and other AI models accessible to speakers of all language\"). As a speaker of English, I'm really not sure whether the offered NSM explication in Figure 1b is useful or even comprehensible to me. I'm sure these are issues that have been discussed in the NSM literature, but as a person who is not knee-deep into the literature, maybe more examples and illustration of the utility would be helpful to convince a skeptic like me that this is actually something useful to do. But again, I admit that maybe my position is biased, so if others are sold on the utility of these explanations, maybe the AC can take this into consideration in making judgments.\n\nThe cross-translatability part is interesting and seems like the right way to approach things if quantitative claims about utility are to be made. However, I think BLEU and sentence embedding similarity are weak metrics. I also think without additional human experiments evaluating whether the translation is (1) more comprehensible to the native speakers if the source is NSM, and (2) whether the original lexical meaning really is preserved in the output NSM, are necessary to establish the contribution being claimed. There are some human experiments in the work which is good, but this only concerns rankings among NSMs without taking into consideration fidelity of the NSM explication to the original meaning.  \n\nMinor weakness: A lot of the critical metric definitions are pushed to the appendix (e.g., explication score, legality score, circularity...) - they should really be in the main text."}, "questions": {"value": "I described a lot of the conceptual issues I had in the Weaknesses section. These aren't couched in questions, but I'd be happy to engage in discussions about these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fJ60ePX7u6", "forum": "4ftPHo58O9", "replyto": "4ftPHo58O9", "signatures": ["ICLR.cc/2026/Conference/Submission14032/Reviewer_Mafg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14032/Reviewer_Mafg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146383011, "cdate": 1762146383011, "tmdate": 1762924520468, "mdate": 1762924520468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using LLMs to generate Natural Semantic Metalanguage (NSM) explications—paraphrases of word meanings using a universal set of ~65 semantic primes that exist across all languages. The authors introduce automatic evaluation metrics (legality, substitutability) to assess explication quality and contribute a new dataset for this task. Using this dataset, they fine-tune 1B and 8B parameter models that outperform GPT-4o on explication quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses an interesting yet underexplored research topic NSM explications, which is relevant to multiple fields including machine translation and instruction following.\n\n- The cross-translatability experiment provides appropriate validation for the effectiveness of fine-tuning.\n\n- The methodologies for the dataset collection and cross-translatability test are well-explained, and the dataset will be a valuable resource for the research community."}, "weaknesses": {"value": "- A limited number of models are applied into the experiments (small size Llama, Gemini-2.0-Flash and GPT-4o), therefore I'm a bit concerned whether the conclusions can be generalized. \n- In the substitutability test, the authors use three numbers: $\\triangle_{base}$,  $\\triangle_{min}$,  $\\triangle_{ent}$. All these three are fragile to me (see questions).\n- Some of the prompts and examples are in the appendix but not mentioned in main text, which caused difficulty understanding the paragraphs."}, "questions": {"value": "- For $\\triangle_{base}$, metric may be sensitive to sample quality. For instance, if a passage provides very clear context for the masked word, the model can predict it correctly without the explication, causing even helpful explications to show marginal improvement in $\\triangle_{base}$. Conversely, if the passage is ambiguous, even poor explications might appear helpful.\n- For $\\triangle_{min}$ and $\\triangle_{ent}$, the authors remove 2 sentences from the end of the explication or passage. However, would these metrics remain valid if the redundant information appears at the beginning (or first half) of the explication instead?\n- Figure 11 shows the prompt used for generating examples in the substitutability test. However, the prompt does not appear to include explicit controls for example ambiguity. How do the authors assess example quality and ensure appropriate difficulty levels for the substitutability test? This relates to the first concern about sample quality affecting metric reliability.\n- It would be better to see some ablation study for the experiments, for example, for the fine-tunes model, what certain ability is improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bukTzWHwv4", "forum": "4ftPHo58O9", "replyto": "4ftPHo58O9", "signatures": ["ICLR.cc/2026/Conference/Submission14032/Reviewer_bejZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14032/Reviewer_bejZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201927987, "cdate": 1762201927987, "tmdate": 1762924519746, "mdate": 1762924519746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}