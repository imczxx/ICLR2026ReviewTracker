{"id": "iU026Hr90y", "number": 7843, "cdate": 1758038674489, "mdate": 1759897827906, "content": {"title": "One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning", "abstract": "In heterogeneous multi-task decision-making, tasks not only exhibit diverse observation and action spaces but also vary substantially in their underlying complexities. While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling a broad and diverse suite of tasks, gradient conflicts and the loss of model plasticity often constrain their sample efficiency.\nIn this work, we address these challenges from two complementary perspectives: the single learning iteration and the overall learning process. First, to mitigate the gradient conflicts, we systematically investigate key architectural designs for extending UniZero. Our investigation identifies a Mixture-of-Experts (MoE) architecture as the most effective approach. We demonstrate, both theoretically and empirically, that this architecture alleviates gradient conflicts by routing task-specific representations to specialized sub-networks. This finding leads to our proposed model, \\textit{ScaleZero}. Second, to dynamically allocate model capacity throughout the learning process, we introduce an online Dynamic Parameter Scaling (DPS) strategy. This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion. Evaluations on a diverse set of standard benchmarks (Atari, DMC, Jericho) demonstrate that ScaleZero, utilizing solely online reinforcement learning with one model, performs on par with specialized single-task agents. With the DPS strategy, it remains competitive while using just 71.5\\% of the environment interactions. These findings underscore the potential of ScaleZero for effective multi-task planning.", "tldr": "We improve multi-task learning by exploring architectures, finding MoE best for gradient conflicts, and using a Dynamic Parameter Scaling (DPS) strategy for adaptive model capacity.", "keywords": ["reinforcement learning", "multi-task reinforcement learning", "world model", "MCTS", "latent space planning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b358f3d1d12521dfa1aa513bccd797c686afff8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission proposes an architectural (Soft-MoE) and a fine-tuning method (LoRA) applied to the task of heterogeneous multi-task RL & planning. The authors suggest that their are limitations of the current Resnet architecture through several axes such as (a) task conditioning, (b) encoder architecture, (c) normalization method, (d) backbone design, and (e) optimization strategy. The authors then introduce their Soft-MoE for online multi-task RL (MTRL), with adaption to new tasks being performed through low rank adaption."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Approaches an important problem & limitation within MTRL where typical MTRL problems have homogeneous tasks, this paper attempts to expand the task distribution further by introducing heterogeneous tasks into the task distribution. While this problem has been attempted before, it is still an unsolved problem."}, "weaknesses": {"value": "1. The authors of this work should review accepted methods of reporting results in RL such as [1]. Only reporting the mean/median does not give the full performance profile and clouds whether the reported results are significant or not. I cannot judge this work effectively without this information. \n\n2. The presentation of this paper could use some work. Right now it jumps around a bit\n\n(a) Method section: I found this section extremely confusing and hard to read. There were multiple missing references to figures that show experimental results (i.e. Latent Normalization paragraph -- what figure is being referenced?), and there were also multiple references that I thought were out of place here (i.e. Rakelly et al 2019 & Sodhani 2021 -- they are both solving fundamentally different problems and don't explore the difficulties of state identification). The method section also starts introducing components and experiments immediately which clouds the clarity of what the authors are intending to add to the UniZero method. \n\n(b) Background Section: The authors begin the background section with a discussion of Monte Carlo Tree Search and then a discussion of gradient conflicts, but then later on line 258 mention \"online multi-task RL.\" Thus it's a bit confusing exactly what algorithm & method are being used here. I assume that this is just a terminology issue but this needs to be explicitly clear. The background section should precisely outline the algorithm (online RL? MTRL?) that is being used to optimize the agent. Terminology from this section should then be consistent through the rest of the work. \n\n(c) Related Work: This section is missing a ton of relevant MTRL works. Line 464 explicitly is focused on model-based RL methods, while there are model free approaches, even one that uses a MoE method [4, 5, 6]. There is also recent work showing that MTRL specific architectures (like Mixture of Experts architectures) are irrelevant and that making a model bigger in MTRL actually drives performance [7]. There's also working showing that sparsity can be a driver of scaling as well [8], alongside diverse data [7]. The authors should include a discussion about these works and how they differ from their own paradigm. \n\n(d) Hyperparameters: I applaud the authors for including hyperparameters, except they should also include the common parameters used in the UniZero model, even if they are the same for the ScaleZero. Additionally, could the authors report the number of parameters used in each of the models? \n\n3. Task Curation: this strategy requires apriori knowledge of the task reward/success thresholds. This doesn't transfer well to new tasks where this prior knowledge is not readily available. Recent works have attempted to characterize the training dynamics as a method for early stopping [2] \n\n4. Missing Baselines\n\n(a) A large portion of this work is about gradient conflicts -- there are no baselines for various optimization methods that attempt to solve this issue (i.e PCGrad, CAGrad, MGDA). \n\n(b) Alternatively, previous work has found that effective normalization can perform on par with these optimization methods [3] but that strategy is not experimented with either. \n\n5. Citations\n\nLine 195: Dohare et al do not propose feature effective rank, they might use it but that's not the work that should get cited. \n\nLine 185: Dormant Neurons needs to be cited when it is defined\n\nLine 103: \"... demonstrated remarkable success in domains with known rules\" -- need to provide some evidence of this through citations\n\n\nIn general, it feels like this paper is trying to do a lot of things which clouds the overall readability of the work. Doing an empirical study of the effects of the changes in the 5 highlighted axes could be a useful submission, and doing an empirical study of the effects of adding various parameter efficient methods (such as LoRA) would also be valuable. But the way that these things are presented in the current version of the manuscript make it very difficult to understand the value of each of the methods individually. \n\n1. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare \"Deep RL at the Edge of the Statistical Precipice\", NeurIPs 2021. \n\n2. Jiashun Liu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning, ICML 2025.\n\n3. Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, M. Pawan Kumar, In Defense of the Unitary Scalarization for Deep Multi-Task Learning. NeurIPS 2022\n\n4. Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft modularization. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/32cfdce9631d8c7906e8e9d6e68b514b-Abstract.html\n\n5. Ahmed Hendawy, Jan Peters, and Carlo D’Eramo. Multi-task reinforcement learning with mixture of orthogonal experts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=aZH1dM3GOX.\n\n6. Lingfeng Sun, Haichao Zhang, Wei Xu, and Masayoshi Tomizuka. Paco: Parameter-compositional multi-task reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=LYXTPNWJLr.\n\n7. Reginald McLean, Evangelos Chatzaroulas, J K Terry, Isaac Woungang, Nariman Farsad, and Pablo Samuel Castro. \"Multi-Task Reinforcement Learning Enables Parameter Scaling.\" Reinforcement Learning Journal, vol. 6, 2025, pp. 1075–1093. \n\n8. Guozheng Ma, Lu Li, Zilin Wang, Li Shen, Pierre-Luc Bacon, Dacheng Tao, Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning. ICML 2025"}, "questions": {"value": "1. How many parameters are in the UniZero model vs the ScaleZero model?\n2. What other measures of task progress could be used instead of reward/success thresholds?\n3. Are there other parameter efficient fine-tuning methods that could be experimented with for fine-tuning a pre-trained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JmUhHBBZ9G", "forum": "iU026Hr90y", "replyto": "iU026Hr90y", "signatures": ["ICLR.cc/2026/Conference/Submission7843/Reviewer_K2YD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7843/Reviewer_K2YD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630285959, "cdate": 1760630285959, "tmdate": 1762919886378, "mdate": 1762919886378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the challenges of training a unified world model for multi-task planning. The obstacles centered around representational bottlenecks and plasticity collapse of the shared model. The authors attribute these issues to gradient conflicts as well as the diversity of the shared features. To address these issues, this work investigates and studies several modifications to the UniZero [1] model, focusing on three core axes: input representation, model architecture, and optimization strategy. Consequently, the method addresses plasticity collapse using two approaches: an internal, architectural fix called *ScaleZero* to reduce interference during a single learning step, and an external, procedural strategy called *Dynamic Parameter Scaling (DPS)* to better manage resource allocation throughout the overall learning process. ScaleZero matches the performance of specialized agents on various benchmarks (Atari, DMC, Jericho) using only a single model and standard online reinforcement learning. Furthermore, when the DPS strategy is added, it maintains competitive performance while significantly reducing environmental interactions by nearly 30% (using only 71.5% of the original). This suggests that ScaleZero is a highly promising method for effective multi-task planning.\n\n[1] Yuan Pu, Yazhe Niu, Zhenjie Yang, Jiyuan Ren, Hongsheng Li, and Yu Liu. Unizero: Generalized and efficient planning with scalable latent world models. arXiv preprint arXiv:2406.10667, 2024."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and structured.\n- Although the paper is dense, the authors present the content effectively.\n- This work combines existing techniques to provide a proper solution to challenges in multi-task planning. That constitutes novelty in my opinion.\n- I appreciate the example in Figure 1, which highlights the relationship between task performance and plasticity collapse using the Dormant Ratio and Latent Norm metrics.\n- A strength of this work is the diverse set of benchmarks addressed."}, "weaknesses": {"value": "- Although the paper is well-written and the solution is well-motivated, the justification for using a world model for multi-task planning, as opposed to model-free multi-task RL, is not entirely clear. I advise the authors to highlight the importance of world models in comparison to model-free RL, either in the text or through benchmark evaluations.\n- The claim that model plasticity collapse is related to gradient conflicts is not well presented or supported. I recommend adding a gradient conflicts metric (such as the cosine between task gradients), similar to the ones in the appendix, for the example in Figure 1. This would allow us to observe the relationship between the loss of plasticity and gradient conflicts.\n- As previously stated, there is no comparison to model-free multi-task RL methods, and the paper lacks baselines from that category. I recommend benchmarking some advanced multi-task methods [1,2,3] with similar model capacity on benchmarks like DMC.\n- Some key ablations are missing. One important ablation would be varying the number of experts, and another would be varying the number of task curation stages. I believe these ablations were not studied in this work.\n- The limitations of the proposed method are not clearly stated in the paper. In my opinion, one limitation could be that there are many moving components to achieve that performance.\n\n[1] Sun, Lingfeng, et al. \"Paco: Parameter-compositional multi-task reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 21495-21507.\n\n[2] Hendawy, Ahmed, Jan Peters, and Carlo D'Eramo. \"Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts.\" The Twelfth International Conference on Learning Representations.\n\n[3] Cho, Myungsik, et al. \"Hard tasks first: Multi-task reinforcement learning through task scheduling.\" Forty-first International Conference on Machine Learning. 2024."}, "questions": {"value": "I have some questions:\n1. What is the specific advantage of using a world model over model-free reinforcement learning (RL) in the multi-task context?\n2. Could you elaborate on the relationship between gradient conflicts and the loss of plasticity? This could be explained either qualitatively or by comparing against relevant methods from the literature.\n3. In the Dynamic Parameter Scaling (DPS) strategy, what metric is used to compute performance for task curation? Specifically, is it the training return or a dedicated online evaluation metric?\n4. In DPS, are tasks re-included if their performance drops? If not, how does the method mitigate the issue of forgetting excluded tasks? Is this addressed by the expansion and freezing mechanism of the LoRAs?\n5. Could you provide a plot tracking which condition (Progress-based Trigger or Budget-based Trigger) was activated to initiate the switch to the next expansion stage?\n6. How is the number of expansion stages chosen? Is it directly correlated with the number of tasks? Additionally, how does changing the number of expansion stages affect overall performance?\n7. What is the effect of varying the number of experts on overall performance? An ablation study investigating this factor would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lGqPMFt65R", "forum": "iU026Hr90y", "replyto": "iU026Hr90y", "signatures": ["ICLR.cc/2026/Conference/Submission7843/Reviewer_7ZaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7843/Reviewer_7ZaB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658568107, "cdate": 1761658568107, "tmdate": 1762919885869, "mdate": 1762919885869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of heterogeneous multi-task online decision-making by proposing ScaleZero, a unified world model that extends UniZero with mechanisms for mitigating gradient conflicts and preserving model plasticity. ScaleZero integrates a sparse Mixture-of-Experts (MoE) backbone to handle conflicting gradients and a Dynamic Parameter Scaling (DPS) mechanism that progressively expands model capacity through LoRA adapters based on learning progress. The approach allows a single model to achieve competitive or near-expert performance (single task agent) across multiple diverse tasks (Atari, DMC, Jericho) while using only 71.5% of the environment interactions compared to specialized models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a relevant and challenging problem: Tackles multi-task world model learning; an important step toward generalist RL agents capable of handling heterogeneous tasks with diverse observation and action spaces.\n- Principled design and justification: The paper builds on solid theoretical foundations around gradient conflict and plasticity collapse, grounding architectural and procedural design decisions with both empirical and analytical justification.\n- Well-motivated extensions to UniZero: The combination of sparse Mixture-of-Experts (MoE) layers and LoRA-based Dynamic Parameter Scaling (DPS) is technically sound and thoughtfully motivated.\n- Novel and interpretable component: Introduction of Dynamic Parameter Scaling (DPS) as an adaptive LoRA integration strategy provides a clear and interpretable mechanism for resource-efficient multitask learning.\n- Comprehensive experimentation: Evaluation across three benchmarks (26 Atari games, 18 DMC control tasks, and 4 Jericho text-based environments) convincingly demonstrates that one unified model can rival single-task specialists.\n- Strong empirical efficiency: Demonstrates competitive results while requiring only 71.5% of the environment interactions of comparable single-task agents, highlighting improved sample efficiency.\n- Thorough analysis: Empirical and theoretical analysis of MoE gradient conflict (Theorem 4.1) provides valuable insight into why conditional computation is effective for multitask planning."}, "weaknesses": {"value": "- Limited performance gain in some benchmarks: In Table 1a, the reported human-normalized mean improvement on Atari is minimal (0.39 vs. 0.38), raising questions about statistical significance of the claimed parity or superiority.\n- Lack of code availability: Reproducibility is currently unverifiable as no code release accompanies the submission, despite promising to release post-review.\n- Exploratory overreach: Section 3.2 includes several explorative design discussions (e.g., LayerNorm vs. SimNorm, ResNet vs. ViT) that provide little scientific contribution to the main narrative and could be shortened.\n- Figure legibility issues: Figure 2’s small text and visual clutter reduce readability and comprehension.\n- Marginal novelty of core techniques: The combination of MoE and LoRA, while effective, builds primarily on well-established concepts. The primary novelty lies in their integration into a unified online multitask planner rather than in the individual components.\n- Theoretical depth could be improved: The provided theorem on gradient conflict upper bounds is only informally stated and lacks empirical connection to practical convergence or sample efficiency.\n- Benchmark contextualization: The paper could better explain the specific setup for comparisons—e.g., whether “single-task UniZero” agents were tuned separately and if “competitive” results include confidence intervals."}, "questions": {"value": "- Line 122: What exact failure pattern of “plasticity collapse” is being referred to, loss of gradient signal, representational collapse, or over-regularization?\n- Line 140: How does the sparse MoE approach differ from multi-head multitask methods like MHMTSAC (Yu et al., 2019)?\n- Line 232 / Figure 2: The figure caption refers to a \"systematic exploration\" but provides little quantitative evidence; can you provide clearer metrics or summaries for each axis of exploration?\n- Line 257: If the ViT encoder’s scalability claim is central, why is this figure placed in the appendix?\n- Line 259: Given the minimal impact of encoder choice, why retain it in the main text instead of moving it to supplementary materials?\n- Line 264: What empirical evidence supports the final preference for LayerNorm over SimNorm?\n- Line 279: Why include the “Multitask Gradient Correction” subsection if it was not part of the final method?\n- Line 323 (Section 3.3): How many LoRA modules can be added in total, is there a hard limit or a soft budget threshold?\n- Figure 4: Is the 28.5% reduction in environment steps consistent across different benchmarks, or specific to DMC18?\n- Reproducibility: What are the approximate computational requirements (GPU hours, memory footprint) of training ScaleZero compared to UniZero?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iiiBQS8eMf", "forum": "iU026Hr90y", "replyto": "iU026Hr90y", "signatures": ["ICLR.cc/2026/Conference/Submission7843/Reviewer_o84j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7843/Reviewer_o84j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774991853, "cdate": 1761774991853, "tmdate": 1762919885416, "mdate": 1762919885416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ScaleZero, a unified world model for heterogeneous multi-task RL that addresses plasticity collapse and gradient interference observed in latent space planners (e.g. UniZero).\nThe proposed approach particularly combines a Mixture-of-Experts backbone into previous work UniZero, and proposes a novel Dynamic Parameter Scaling technique using LoRA's paradigm to adapt the number of parameters with respect to the learning progress. \nExperiments on several tasks included in the benchmarks of Atari, DMControl, and Jericho demonstrate improved sample efficiency and at times overall performance versus training single-task experts via world models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates and directly tackles two relevant open problems in MTRL (task interference and plasticity collapse). This is done via thorough experimenn over multiple metrics: gradient conflict, mean and median performance, and more.\n- The authors carefully study the limitations of previous SOTA world-model approach UniZero in the realm of multi-task settings, providing useful insights for further research in the direction of bridging methods towards MTRL usage.\n- The paper is clearly written and relatively easy to follow despite the large amount of content included in the Appendix. Important claims and motivations are clearly highlighted.\n- The manuscript also contains relevant theoretical analysis on the underlying aspects of how sparse MoE affect gradient conflict."}, "weaknesses": {"value": "- Absence of multi-task baselines: The authors only compare their approach as a modification to single-task experts trained individually on multiple tasks using UniZero. While the overall investigation is sound, it is currently unclear how the performance compares to state-of-the-art methods in MTRL. For example, [1] is a recent ICLR contribution that particular targets task interference via explicit task conditioning that ensures orthogonal experts. The authors' investigation in line 215 lacks any comparison with such baselines, limiting the impact and relevance of the overall analysis. As such, the current work mostly presents itself as an adaptation to an existing world-model approach (UniZero) to the MTRL paradigm; while this is a contribution in itself, insufficient evidence is provided on how these modifications compare to existing MTRL methods.\n\n\n[1] Hendawy, Ahmed, Jan Peters, and Carlo D'Eramo. \"Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts.\" The Twelfth International Conference on Learning Representations, 2024."}, "questions": {"value": "- How does ScaleZero compare to a naive multi-task RL baseline that does not learn world models nor does planning?\n- From my perspective, I would expect ScaleZero to yield improvements in terms of median performance if anything, over mean performance. This is the case in Table I.b for DMC tasks, but could you provide more insights into why the opposite happens for Table I.a? This opposite trend for Atari games could mean that ScaleZero actually fails in solving plasticity collapse, and if anything introduces even more skewness in the performance distribution over different tasks.\n- Could you please provide an explicit comparison between ScaleZero and UniZero trained on single tasks by means of compute budget (GPU hours), wall-clock training time, inference time, and size of network parameters? It's currently unclear to me whether performance is also being traded off for efficiency or additional resource usage.\n- Despite the authors claiming MoE as the most impactful modifications, why does ScaleZero NOT include MoE layers for the Jericho benchmark experiments (Line 1179)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1IUKwxZ86e", "forum": "iU026Hr90y", "replyto": "iU026Hr90y", "signatures": ["ICLR.cc/2026/Conference/Submission7843/Reviewer_QPFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7843/Reviewer_QPFt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924252008, "cdate": 1761924252008, "tmdate": 1762919884555, "mdate": 1762919884555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}