{"id": "EO6WtJ0q6G", "number": 15807, "cdate": 1758255517596, "mdate": 1759897280820, "content": {"title": "Do Large Language Models Know What They Are Capable Of?", "abstract": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task, and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparable to or worse than non-reasoning LLMs. With in-context experiences of failure, most LLMs only slightly reduce their overconfidence, though in a resource acquisition scenario several LLMs (Claude Sonnet models and GPT-4.5) improve their performance by increasing their risk aversion. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.", "tldr": "We find that LLMs are overconfident in predicting their success on tasks, but some learn from in-context experience to make more risk-averse decisions about which tasks to attempt.", "keywords": ["LLM Calibration", "Decision Making", "Overconfidence", "In-context learning", "LLM Agents", "LLM self-knowledge", "AI Safety"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b60522b205ac37d5ff02669181a0825f7c06b75e.pdf", "supplementary_material": "/attachment/1d5013807fb9c64b146f7ad1319491e31c9079f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the self-awareness of large language models (LLMs), defined as their ability to predict their own success on a given task. The authors conduct three experiments to evaluate this capability: (1) eliciting in-advance confidence estimates on single-step tasks; (2) placing LLMs in a sequential resource-acquisition scenario where they must accept or decline \"work contracts\" based on their self-assessment, learning from in-context experience; and (3) tracking how confidence estimates evolve at each intermediate step on multi-step tasks.\n\nThe core findings are that: LLMs typically do not make more accurate confidence estimates; frontier LLMs successfully learn from past successes and failures  by increasing their risk aversion; and that reasoning LLMs are typically less accurate at predicting their success."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the issue of LLM self-awareness, which has significant implications. Understanding whether models \"know what they don't know\" is fundamental in many AI areas.\n\n2. The evaluation is extensive, covering a wide range of modern LLMs from different families (Llama, GPT, Claude).\n\n3. The paper confirms interesting LLMs behaviours, although not completely unexpected. For example,  in Experiment 2 that improved profitability stems from increased risk aversion"}, "weaknesses": {"value": "1. The central conclusion that LLMs are overconfident is not new, and is extensively documented in prior work, much of which is cited by the authors (e.g., Lin et al., 2022; Tian et al., 2023; Xiong et al., 2024). While the experimental setups are novel, the main takeaway is confirmatory rather than groundbreaking.\n\n2. A significant opportunity seems to have been missed by not exploring methods to address the identified problem. The framework from Experiment 2, which measures expected profit, seems perfectly suited to use as a reward signal. Why did the authors not attempt to use this signal to improve model performance, for instance, through RL fine-tuning? A demonstration of even a simple mitigation strategy on a small-scale LLM would have substantially increased the paper's contribution."}, "questions": {"value": "- The finding in Experiment 3 that reasoning-enabled models perform no better than their non-reasoning counterparts is surprising. Do the authors have a hypothesis for this? Could it be an artifact of the prompting strategy, the RLHF training, or does it suggest that current reasoning capabilities do not extend to robust self-assessment?\n\n- The paper provides a strong foundation for future work. I would encourage the authors to build on this evaluation by exploring mitigation techniques, as this would be a highly valuable contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qmDU4x9S68", "forum": "EO6WtJ0q6G", "replyto": "EO6WtJ0q6G", "signatures": ["ICLR.cc/2026/Conference/Submission15807/Reviewer_GMRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15807/Reviewer_GMRd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555575435, "cdate": 1761555575435, "tmdate": 1762926038261, "mdate": 1762926038261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work is a quantitative analysis of the in-advance capability of a language model in determine success/failure outcome of a task. In particular, it studies whether the models can learn from recent successes/failures to make better go/no-go decisions when failure is costly, and update those predictions while working through multi-step, tool-using tasks.\n\nThe authors ran three experiments:\n* Predicting the probability of success before attempting BigCodeBench Python coding tasks\n* A modified setting where models sequentially attempt Python tasks in a â€œwork contracts\" setting that resembles a contextual one-armed bandit with an abstain option\n* Stepwise confidence on multi-step SWE-Bench Verified, with a 70-tool-call budget\n\nThe work then presents several core findings: all models overestimate their success rates, they have better than random guesses, but their guesses have no trend of increasing discriminatory power with capability. Even with in-context learning, many models remain overconfident. Some frontier models improve profit in the bandit setting mainly by abstaining more, not by sharply improving discrimination/calibration. Finally, multi-step dynamics diverge between different models, with reasoning models not necessarily more calibrated than non-reasoning variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is very well written with great flow, good schematics, and no major grammatical or formatting errors\n* The paper crystallizes an well-known phenomenon intuitively and anecdotally understood into a rigorous analysis\n* Calibration is a very important problem to study and understand"}, "weaknesses": {"value": "* Unfortunately the world of foundation models move incredibly fast and the models tested in this work is already fairly dated. For example, GPT-4.1, Sonnet 3.5, etc. are no longer available. As noted by the authors, some of the behaviors characterized are divergent, and thus likely no longer relevant to the newest class of models (e.g. GPT-5). \n* It would benefit the audience to draw connections between the in-advance setup of the work to calibration in traditional neural network architecture (e.g. [1], among others)\n\n[1] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017, July). On calibration of modern neural networks. In International conference on machine learning (pp. 1321-1330). PMLR."}, "questions": {"value": "I don't have more questions beyond what is mentioned in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bXKBokDbgi", "forum": "EO6WtJ0q6G", "replyto": "EO6WtJ0q6G", "signatures": ["ICLR.cc/2026/Conference/Submission15807/Reviewer_WUxE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15807/Reviewer_WUxE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947377901, "cdate": 1761947377901, "tmdate": 1762926037892, "mdate": 1762926037892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether LLMs can predict whether (and how likely it is that) they can complete a task before performing it. This is tested both for single response cases using the BigCodeBench benchmark and in an agentic setup using the SWE-Bench verified benchmark. The authors find that all of the investigated models (recent GPT, Claude, and LLama models) overestimate the likelihood of completing a task successfully, and while some models can be steered through in-context learning to be more cautious about predicting that they can complete a task, this behaviour generally even persists in that scenario."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* While the research question is relatively simple, the paper evaluates the question thoroughly and considers the problem from multiple angles. And to the best of my knowledge, this paper introduces some new paradigms to evaluate model confidence.\n* The paper is very well written, the experimental setup is very clear,  and it connects very well to previous work.\n* The paper adequately discusses its limitations."}, "weaknesses": {"value": "* While I think the task in Experiment 2 with fictional costs is an interesting approach, I was wondering whether this has been validated that LLMs can make such risk-based decisions when they have direct access to the underlying risks. In other words, is there evidence that LLMs can accurately compute the expected reward and base decisions on this implicit calculation? While I don't think this would change results here, since the confidence estimates still seem to be inaccurate anyways, it would be good to establish whether all LLMs can actually do this task since otherwise it may be challenging to derive anything meaningful from the model's decisions in this setup."}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V6foK0naJr", "forum": "EO6WtJ0q6G", "replyto": "EO6WtJ0q6G", "signatures": ["ICLR.cc/2026/Conference/Submission15807/Reviewer_DhWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15807/Reviewer_DhWq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762461353793, "cdate": 1762461353793, "tmdate": 1762926037411, "mdate": 1762926037411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates whether LLMs know how likely they will succeed in a given task before attempting the task. The authors conducted three experiments in which LLMs performed confidence estimation in different settings and scenarios. Through the experiments, the study find that LLMs are generally overconfident to the given tasks, but still be able to identify tasks that they are capable of better-than-random. The work also shows that for many LLMs, such 'self-awareness' effect doesn't improve with models' increasing general ability, in-context experiences, and reasoning ability. Overall, the work reveals LLMs' poor self-awareness of capability, which may potentially hinder LLMs' application in high-stakes scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Comprehensive experimental design and clear visualization of the results.\n2. Provide empirical evidence of 'self-awareness of ability' across different LLMs with different sizes."}, "weaknesses": {"value": "1. Need an explicitly defined research gap, motivation, and why the study can fill the gap. The contributions of this work have not been highlighted from previous work.\n2. How LLMs know what they are capable of is quantified by self-reported confidence in this study. Auto-regressive LLM models trained by next-token-prediction solve the confidence estimation given tasks in a fundamentally different manner from humans, making it sort of ambiguous to represent 'real confidence' in the tasks merely by reported scores. This could limit the implications of the study."}, "questions": {"value": "1. What's the difference between in-advance/answer-free confidence estimation and retrospective estimation? Why use such ways of confidence estimation? Should the formers work better in this case? \n2. How is actual success rate in Exp1 calculated? Doing each task for multiple times which produces a success rate for each individual task, or doing each task once that produces an overall success rate for the whole benchmark? \n3. In Exp1, what is the best accuracy of predicting actual success rate with the reported confidence? And worth checking whether rephrasing the confidence estimation prompts can improve the prediction of actual success rate. For example, if asking LLMs to rate in-advance difficulty of each task, rather than the confidence, will the difficulty rating predict success rate well? \n4. In Exp2, how many unique tasks are there in the pool of S and F tasks? Will the repetition of these tasks in the 9-contract sequences affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8y8qNHSlwA", "forum": "EO6WtJ0q6G", "replyto": "EO6WtJ0q6G", "signatures": ["ICLR.cc/2026/Conference/Submission15807/Reviewer_VR9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15807/Reviewer_VR9Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762473358967, "cdate": 1762473358967, "tmdate": 1762926037031, "mdate": 1762926037031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}