{"id": "OMOjEkE5rd", "number": 8424, "cdate": 1758082899350, "mdate": 1759897784633, "content": {"title": "Breaking Safety Alignment in Large Vision-Language Models via Benign-to-Harmful Optimization", "abstract": "Large vision–language models (LVLMs) achieve remarkable multimodal reasoning capabilities but remain vulnerable to jailbreaks. Recent studies show that a single jailbreak image can universally bypass safety alignment, yet most existing methods rely on Harmful-Continuation (H-Cont.) optimization. In this setting, a jailbreak image is optimized to predict the next token from harmful conditioning. Through systematic analysis, we reveal that H-Cont. has a fundamental limitation. Specifically, harmful conditioning itself biases models toward unsafe outputs, leaving limited capacity for adversarial optimization to genuinely overturn refusals. Consequently, H-Cont. is effective only in continuation-based jailbreak settings and fails to exhibit universal effectiveness across diverse user inputs. To address this limitation, we propose Benign-to-Harmful (B2H) optimization, a new jailbreak paradigm that decouples conditioning and targets (i.e., the target is not the next-token continuation of the conditioning). By explicitly forcing models to map benign conditioning to harmful targets, B2H directly breaks safety alignment rather than merely extending harmful conditioning. Extensive experiments across multiple LVLMs and safety benchmarks demonstrate that B2H achieves stronger and more universal jailbreak success, while preserving the intended jailbreak behavior. Moreover, B2H transfers well in black-box settings, integrates with text-based jailbreaks, and remains robust under common defense mechanisms. Our findings highlight fundamental weaknesses in current LVLM alignment and establish B2H as a simple yet powerful paradigm for studying multimodal jailbreak vulnerabilities.", "tldr": "We found that decoupling conditioning and targets (i.e., ensuring the target is not the next-token continuation of the conditioning) induces safety misalignment, and we propose a jailbreak method that leverages this principle.", "keywords": ["Large Vision-Language Models (LVLM)", "Safety-Alignment", "Jailbreeak"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cea8444ef43438b546bab754c4ca7af8bcb60315.pdf", "supplementary_material": "/attachment/25a060afffd75f9c51bb76e9cd846aa3ca4180fe.zip"}, "replies": [{"content": {"summary": {"value": "Multimodal large models will encounter jailbreaks. Most of the existing methods are based on Harmful Continuation, giving harmful conditions to predict the next token. This paper proposes a new optimization paradigm, Benign to Harmful, which can more effectively disrupt safe alignment without relying on harmful conditions. The experimental results show that B2H has achieved a higher success rate on multiple datasets and models, effectively maintaining the consistency of input and output."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper clearly expounds the motivation. Currently, multimodal large models will face the security alignment problem of jailbreaks, clarifies the limitations of the existing method Harmful-Continuation, and clarifies the principles of HCont and B2H.\n\n2.This paper compares the success rates of different attack methods on multiple benchmarks and models (security assessors and target models). B2H performs excellently in different types, demonstrating the powerful universal jailbreak capability and pointing out the vulnerabilities of the security alignment mechanism.\n\n3.This paper presents the corresponding Benign to Harmful Jailbreak Success Example, clearly demonstrating the effectiveness and context unity of the B2H method, which is conducive to better research on the Jailbreak problem of multimodal large models."}, "weaknesses": {"value": "1.Table 4 and Table 5 demonstrate the situation of B2H in the face of JPEG compression defense measures. In some cases, it fluctuates greatly and requires more thorough analysis. Also, how effective is it against other defense mechanisms (such as image noise addition or specialized adversarial training, etc.)?\n\n2.A more thorough analysis should be conducted on the reasons for the differences in the performance of the B2H method on different models and data, as well as the focused exploration of the reasons why its performance is inferior to that of HCont in certain cases, in order to better illustrate the consistent superiority of B2H."}, "questions": {"value": "1.Supplement the analysis of the performance and reasons of B2H when facing defense mechanisms.\n\n2.Supplement the analysis of the performance differences of B2H on different data and models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zrP42ghkTk", "forum": "OMOjEkE5rd", "replyto": "OMOjEkE5rd", "signatures": ["ICLR.cc/2026/Conference/Submission8424/Reviewer_QFX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8424/Reviewer_QFX8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636095298, "cdate": 1761636095298, "tmdate": 1762920320166, "mdate": 1762920320166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose benign-to-harmful optimization to jailbreak large vision-language models. It forcing models to map benign intention to harmful responses, optimizing an universal image to jailbreak LVLMs in both black and white box settings. B2H outperforms previous methods in both ASR and semantic consistency across various models and benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation and illustration of paper is clear and well organized\n2. The proposed B2H is easy to understand and implement\n3. Extensive experiments demonstrate the effectiveness of B2H"}, "weaknesses": {"value": "1. **Misalignment of objective and evaluation**\n\n   - Authors claim that \"... a truly effective jailbreak image should learn to overturn the model’s initial refusal to respond\" in line 217-218. However, there is no experience validate that B2H can achieve this objective.\n   - Although authors indicate that H-Cont is limited beyond continuation and B2H performs well, it is better to provide experiments that B2H can overturn the model’s initial refusal to respond under jailbreaking.\n\n2. **More experiments could strengthen the credibility of B2H**\n\n   - Can the authors evaluate the transferability when jailbreaking strong black-box models? e.g., optimizing image using Qwen2.5-VL and attack GPT-4o, Gemini2.5 pro, and Claude\n   - Whether the universal image can jailbreak the VLMs after safety fine-tuning? some reference [1][2][3]\n   - B2H appears similar to the previously mentioned B2S text trigger, which may limit its novelty. Would it yield better results if the authors optimized the image using B2S instead of B2H?\n\n3. **Limited performance on more recent model**\n   - Although B2H outperforms H-Cont across various models and benchmarks. It performance on more recent VLM (Qwen2.5-VL) is still limited compared to other multimodal Jailbreaking methods. [4][5]\n\n4. **Misleading illustration of Figure 5**\n   - The authors demonstrate that a benign prompt produces an appropriate, safe response. However, under the jailbreaking setting, if the model is compromised, it may respond to a query such as “If you see a red traffic light, what should you do?” with an answer like “Ignore the traffic light and keep going...”. It is unclear what the authors intend to convey with this figure.\n\n### Ref\n[1] Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models.\n\n[2] SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model\n\n[3] Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models\n\n[4] Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\n\n[5] Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency"}, "questions": {"value": "1. The authors note that when $\\epsilon = 255/255$, the ASR tends to drop. Why does this occur in B2H? In contrast, [6] also ablates different values of $\\epsilon$ but does not observe this phenomenon.\n\n2. I’m wondering whether the choice of the image used for optimization influences the effectiveness of the method?\n\n3. Regarding Weakness 1, if the target model is a reasoning VLM that can reflect on its previous responses, will B2H still be effective?\n\n### Ref\n[6] Visual Adversarial Examples Jailbreak Aligned Large Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dDmAW8QtjJ", "forum": "OMOjEkE5rd", "replyto": "OMOjEkE5rd", "signatures": ["ICLR.cc/2026/Conference/Submission8424/Reviewer_ZGLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8424/Reviewer_ZGLQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769446305, "cdate": 1761769446305, "tmdate": 1762920319699, "mdate": 1762920319699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that previous jailbreak methods based on harmful continuation have a limited scope and depend heavily on the harmful condition. They proposed a more general framework that performs jailbreak on benign conditioning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The setting is interesting and valid for proposing a more general jailbreak paradigm that is not dependent on the harmful condition.\n2. The proposed jailbreak paradigm achieves greater ASR on five benchmarks. Additional results of B2H+GCG are also reported for some of the benchmarks."}, "weaknesses": {"value": "1. It is mentioned that the Benign-to-Harmful pair is based on 71 benign phrases paired with 132 harmful-word targets. Is there any further analysis on how these phrases/targets are chosen, and more information on the diversity/ category balance/ variation?\n2. According to fig.3, it seems actually for InstructBLIP, query-form actually has higher ASR than that of the continuation-form (for text prompt). The statement that “Crucially, this indicates that harmful conditioning itself already biases generations toward unsafe outputs,” is directionally plausible but not fully supported; it would require controlling for image vs. text attack channels to make that claim strong."}, "questions": {"value": "See weakness. Also, how exactly does Benign-to-Harmful optimization interfere with alignment heads compared to Harmful-Continuation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GrI0sxpBHe", "forum": "OMOjEkE5rd", "replyto": "OMOjEkE5rd", "signatures": ["ICLR.cc/2026/Conference/Submission8424/Reviewer_QJ88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8424/Reviewer_QJ88"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860656668, "cdate": 1761860656668, "tmdate": 1762920319325, "mdate": 1762920319325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key weakness in the current Harmful-Continuation (H-Cont) approach for jailbreaking vision-language models—namely, that harmful prompts already bias the model toward unsafe outputs, so the optimization isn’t really breaking alignment. The authors propose Benign-to-Harmful (B2H) optimization as a clever alternative: instead of continuing harmful text, B2H explicitly maps benign prompts to harmful targets, directly overriding the model’s refusal behavior. Experiments are solid: B2H consistently outperforms H-Cont across models and benchmarks, transfers well in black-box settings, and combines effectively with text-based jailbreaks like GCG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: Introduces B2H, a novel jailbreak strategy that breaks alignment without relying on harmful prompts—conceptually cleaner than prior work.\nEmpirical Quality: Strong results across models and benchmarks, with robust transferability and compatibility with existing text-based attacks.\nClarity and Significance: Clear exposition and impactful insight into a deeper class of safety alignment failures in LVLMs."}, "weaknesses": {"value": "1. The core intuition behind B2H could be clearer. Unlike H-Cont, which relies on harmful prefixes, B2H teaches the model to produce harmful outputs from benign inputs—directly bypassing shallow refusal triggers. This exposes a deeper flaw in alignment: models often rely on surface-level prompt cues rather than understanding harmful intent. Making this point more explicit would help clarify why B2H is both novel and effective.\n\n2. The paper doesn’t probe where or how safety alignment is being bypassed within the model (e.g., attention patterns, refusal heads, logits). Including some interpretability analysis would clarify what mechanisms are being overridden during B2H optimization.\n\n3. The benign–harmful token pairs are manually constructed and relatively short (often single-token targets). It’s unclear how the method scales to longer or more naturalistic harmful outputs (e.g., multi-sentence unsafe completions).\n\n4. All benchmarks used have relatively structured prompts and known failure modes. It would be valuable to test B2H on more diverse or open-ended tasks"}, "questions": {"value": "same as weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2FoHKUjQOG", "forum": "OMOjEkE5rd", "replyto": "OMOjEkE5rd", "signatures": ["ICLR.cc/2026/Conference/Submission8424/Reviewer_SzH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8424/Reviewer_SzH4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056805490, "cdate": 1762056805490, "tmdate": 1762920318843, "mdate": 1762920318843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}