{"id": "nzwfj47tT9", "number": 23061, "cdate": 1758339024861, "mdate": 1759896833809, "content": {"title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "abstract": "Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present **DiffTester**, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of **DiffTester** is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, **DiffTester** adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that **DiffTester** delivers significant acceleration while preserving test coverage. Moreover, **DiffTester** generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://anonymous.4open.science/r/DLM4UTG .", "tldr": "Acceleration of dLLM for Unit Test Generation via Repetitive Pattern", "keywords": ["diffusion LLM", "unit test generation", "inference acceleration", "abstract syntax tree"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd541a81dc8187f97698508a3c46aeab877e76f1.pdf", "supplementary_material": "/attachment/13c4c6e1d6ac45d63b979d89398efb0c7cc5c88a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes to explore diffusion LLMs in unit test generation. In particular, the paper proposes DiffTester, an acceleration framework to speed up the decoding time of similar unit tests for the same focal methods. The experimental results show that the proposed approach could outperform existing baselines in generating higher-coverage tests within given time budgets across three different programming languages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The evaluation includes multiple programming languages. \n\nThe paper studies how diffusion LLMs could accelerate unit test code generation for the first time."}, "weaknesses": {"value": "The current evaluation cannot support the usefulness of the proposed approach in practice. Although the paper claims that the proposed approach could achieve higher coverage within the same budget (as shown in Figure-4), the conclusion is not convincing as the curves do not converge. In other words, the given time budget is not sufficient enough to show the final stable status of the proposed approach and baselines. In fact, the paper only gives 25 seconds to compare the proposed approach and baselines. However, if authors check recent LLM-driven unit test generation techniques (e.g., CoverUP[a]), people often give sufficient time (i.e., spending hours or dozens of minutes) to run test generation techniques for one  real-worlds software repository, where the coverage curves become stable within the time budgets. In addition, it also leaves another question that, when there allows more sufficient time budget, whether the proposed approach could result in higher coverage compared baselines. The current outperformance within the limited time budget cannot support the practical usage of the proposed approach. \n\n[a] CoverUp: Effective High Coverage Test Generation for Python. FSE’25.\n\n\nIn addition, although the proposed approach achieves higher coverage within a limited time budget, the coverage improvement does not come from tackling the key challenge in test generation. In fact, it is as expected that test methods for the same focal methods share the same structures (except the test inputs and oracles); but the critical challenge in test generation is how to generate valid and diverse test inputs (that could achieve the deep and hard-to-cover paths of the software), which is obviously not the focus of the work. Therefore, the significance of the proposed approach (i.e., how the work could benefit the field) needs more justification. \n\n\nThere are some unclear approach and experiment details in the current version. In particular, the complete process of extracting similar AST nodes is unclear. Is it performed in an iterative way along with the generation of more unit tests?  Moreover, would unit tests for different focal method share similar structures? In other words, can the approach only be applied to accelerate generating unit tests for the same focal methods OR for different focal methods?"}, "questions": {"value": "1.\tCould authors justify the significance of the proposed approach by showing how the technique can achieve higher coverage when given sufficient time budgets? \n\n2.\tWhat are the overheads of program analysis part in the approach (i.e., common nodes extraction)?\n\n\n3.\tWhat is the complete process of extracting similar AST nodes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aaKnm22k9T", "forum": "nzwfj47tT9", "replyto": "nzwfj47tT9", "signatures": ["ICLR.cc/2026/Conference/Submission23061/Reviewer_86Xk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23061/Reviewer_86Xk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656322721, "cdate": 1761656322721, "tmdate": 1762942498748, "mdate": 1762942498748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DiffTester, a specialized acceleration method for diffusion large language models (dLLMs) in unit test generation (UTG). DiffTester is designed based on one key assumption that unit tests for the same focal function share a very similar structure, having repetitive patterns. Based on this assumption, DiffTester identifies such patterns in the abstract syntax trees (ASTs) of the generated assertion statements, then merges these ASTs and fills in the common tokens to accelerate the diffusion process. Experimented with two dLLMs as baselines on a LeetCode-based test generation benchmark (TestEval), the authors demonstrate DiffTester reduces computational cost (tflops) and decoding time (seconds, s), increases throughput (tokens per second, tps), while maintaining the line coverage of the generated test cases. The authors also extend TestEval to Java and C++ to show the generalizability of DiffTester."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Under the same time budget, DiffTester achieves higher line coverage than the baseline models, and eventually reaches the same maximum achievable coverage.\n\n2. Comprehensive analysis: the authors not only compare to the baseline dLLMs without DiffTester acceleration, but also compare to general purpose acceleration methods for dLLMs, and demonstrate advantages in test generation."}, "weaknesses": {"value": "1. False statement/assumption. On line 45–47, the authors cite Yang et al. [1] for their motivation to use dLLMs for UTG: the next-token prediction nature of LLMs, which increases time consumption and computational cost, is the reasoning for inefficient UTG. If I read and understand their paper correctly, Yang et al.’s main conclusion is that the reason for inefficient UTG is the LLM-generated tests being invalid, and thus failing to achieve high coverage and detect real bugs on the real-world defect dataset Defects4J. However, dLLMs cannot help in such a case, or at least not demonstrated by the authors.\n\n2. The authors further mentioned that the quality of generated UT is inversely proportional to the number of tokens generated per step in a dLLM’s inference process (line 56), which motivates the authors to study an acceleration method for this specific use case. However, generating multiple tokens per step, where time consumption and computational cost are lower than LLMs, is the whole motivation that the authors argue for dLLM for UTG. This is a circular argument.\n\n3. The identified common structure is limited to simple algorithm/data structure contest problem code, but may not generalize to real-world software. Real-world software contains many more functions, and unit tests for large-scale software often require a function-call sequence, which invokes multiple functions/methods/APIs under the same class/module to set up or initialize the focal function [2,3]. In this paper, the authors oversimplify this to the same assertion structure and let the model provide inputs and outputs.\n\n4. If the authors’ assumption and argument hold, the problem of UTG falls back to fuzzing with LibFuzzer. The authors assumed that the tests have the same structure, and argued for throughput and cost as the limiting factor in UTG.\n\n   ```c\n   // fuzz_target.cc\n   extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\n     DoSomethingInterestingWithMyAPI(Data, Size);\n     return 0;  // Values other than 0 and -1 are reserved for future use.\n   }\n   ```\n\n   LibFuzzer achieves the exact purpose of DiffTester, offering a shared structure (the fuzz target), and varying the input data. It has much higher throughput and lower computation cost than any LLM or dLLM. All you need is to let the LLM generate a fuzz target.\n\n5. In the experiment on coverage vs. time (Figure 4), the authors could also include a comparison with a 7B LLM for reference.\n\n6. The authors conclude that DiffTester does not compromise the maximum achievable test coverage. However, only line coverage is reported, and the other more fine-grained metrics like branch coverage and path coverage in TestEval are missing. Considering that the benchmark is constructed from LeetCode, where the solutions are usually short in lines, line coverage, particularly the average line coverage across all submissions, is a poor metric to indicate any performance difference. I suggest the authors not justify or argue for anything based on this result. Instead, I suggest the authors evaluate the method on real-world test generation benchmarks like TestGenEval [4].\n\n\n\n[1]: Yang et al., \"On the Evaluation of Large Language Models in Unit Test\nGeneration\" ASE 2024.\n\n[2]: Pacheco et al., \"Feedback-directed Random Test Generation\" ICSE 2007.\n\n[3]: Lyu et al., \"Prompt Fuzzing for Fuzz Driver Generation\" CCS 2024.\n\n[4]: Jain et al., \"TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark\" ICLR 2025."}, "questions": {"value": "Please address concerns in **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fE2XMeFvLh", "forum": "nzwfj47tT9", "replyto": "nzwfj47tT9", "signatures": ["ICLR.cc/2026/Conference/Submission23061/Reviewer_jBu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23061/Reviewer_jBu6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711700510, "cdate": 1761711700510, "tmdate": 1762942498489, "mdate": 1762942498489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s key strength is a clever, diffusion-style formulation for unit-test generation that leverages parallel, iterative denoising to mine and lock in shared structural patterns across a batch—yielding impressive throughput and coverage gains. Empirically, results look strong and suggest real practicality for generating many tests per focal method. However, the work underexplores the reliability of generated assertions (e.g., semantic correctness, flakiness, and fault-revealing power) and lacks tighter, apples-to-apples comparisons with established LLM-based UTG baselines under the same budget. Suitable comparators with similar input/output include hybrid and prompt-engineered autoregressive approaches such as CODAMOSA, large-scale evaluations of LLMs for Java/Python JUnit/pytest generation, and recent comparative studies that analyze correctness and coverage—against which a direct head-to-head would clarify how much of the lift comes from diffusion decoding versus other factors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clever use of diffusion for UTG. \n\nIterative mask→predict→remask exposes mid-step drafts across a batch, letting you mine shared structure and lock it in—an original angle for speeding up test generation.\n\n2, Impressive empirical gains. \n\nShows higher throughput/coverage at similar budgets (parallel per-step decoding + pattern guidance), suggesting real practicality for large batches of tests."}, "weaknesses": {"value": "1. Assertion reliability under-analyzed. \n\nThe classic “oracle” issue remains: are the asserts semantically correct, flaky-free, and non-overfitted? Little on mutation/fault-revealing power, metamorphic checks, or human vetting rates.\n\n2. Limited apples-to-apples baselines. \n\nNeeds stronger head-to-head with state-of-the-art LLM UTG systems under the same time/compute budget, including hybrid SBST+LLM and prompt-engineered AR LLMs.\n\n* CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models (ICSE 23)\n\nMore baselines are available at: LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation (ASE 24) regarding the coverage and effectiveness"}, "questions": {"value": "I agree that tests are very duplicated. But as for coverage, traditional software testing tools (Codemosa for Python and EvoSuite for Java) can do well. How your approach is compared with those approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fcokzai8NJ", "forum": "nzwfj47tT9", "replyto": "nzwfj47tT9", "signatures": ["ICLR.cc/2026/Conference/Submission23061/Reviewer_GEZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23061/Reviewer_GEZX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759706009, "cdate": 1761759706009, "tmdate": 1762942498121, "mdate": 1762942498121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffTester, an acceleration framework for diffusion Large Language Models (dLLMs) in unit test generation. The key insight is that unit tests for the same focal method often share repetitive structural patterns, which can be exploited to generate multiple tokens per inference step. The authors use Abstract Syntax Tree (AST) analysis to identify common patterns across test cases and adaptively increase tokens generated per step. Experiments on TestEval benchmarks across Python, Java, and C++ show 1.5-2.8× speedups while maintaining test coverage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-Motivated Observation: The observation that unit tests exhibit repetitive structural patterns (Section 2.2, Figure 1) is well-demonstrated. The use of AST-based pattern extraction is a natural and principled approach to leverage this observation.\n- Training-Free Approach: DiffTester requires no model retraining or fine-tuning, making it immediately applicable to existing dLLMs\n- Comprehensive Multi-Language Evaluation: The extension of TestEval to C++ and Java (TestEval-C++ and TestEval-Java) strengthens the evaluation and demonstrates generalization across programming languages. The observation that C++ shows greater acceleration (2.45× vs 1.55× for Python) due to more structural repetition is valuable"}, "weaknesses": {"value": "- Limited Baseline Comparisons: The paper only compares against one general dLLM acceleration method (EB-Sampler). Comparisons with other acceleration techniques or autoregressive models with speculative decoding would strengthen the evaluation. What about comparing with standard autoregressive LLMs using techniques like speculative decoding?\n- Scalability Concerns: The evaluation is limited to relatively simple, single-function benchmarks (210 LeetCode problems). Real-world unit test generation often involves complex dependencies, class hierarchies, and repository-level context\n- Generalization Beyond UTG: While the method is specifically designed for unit test generation, could similar pattern-based acceleration apply to other code generation tasks (e.g., documentation generation, code translation)?"}, "questions": {"value": "- Figure 4 Discrepancy: Why do settings with higher time budgets sometimes exhibit worse performance than those with lower time budgets in Figure 4?\n- Method Acceleration Analysis: Could you provide a theoretical or empirical analysis outlining the conditions under which this method offers significant versus minimal acceleration?\n- Computational Cost Breakdown: What is the breakdown of computational costs between dLLM inference and AST operations?\n- Performance Comparison: How does the performance compare to autoregressive LLMs, and what is the performance gain relative to speculative decoding for autoregressive LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9MHJcjsrQw", "forum": "nzwfj47tT9", "replyto": "nzwfj47tT9", "signatures": ["ICLR.cc/2026/Conference/Submission23061/Reviewer_v4VJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23061/Reviewer_v4VJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097431381, "cdate": 1762097431381, "tmdate": 1762942497673, "mdate": 1762942497673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}