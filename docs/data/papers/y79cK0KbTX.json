{"id": "y79cK0KbTX", "number": 15680, "cdate": 1758253821260, "mdate": 1759897289415, "content": {"title": "USE: Uncertainty Structure Estimation for Robust Semi-Supervised Learning", "abstract": "In this study, a novel idea, Uncertainty Structure Estimation (USE), a lightweight, algorithm-agnostic procedure that emphasizes the often-overlooked role of unlabeled data quality is introduced for Semi-supervised learning (SSL). SSL has achieved impressive progress, but its reliability in deployment is limited by the quality of the unlabeled pool. In practice, unlabeled data are almost always contaminated by out-of-distribution (OOD) samples, where both near-OOD and far-OOD can negatively affect performance in different ways. We argue that the bottleneck does not lie in algorithmic design, but rather in the absence of principled mechanisms to assess and curate the quality of unlabeled data. The proposed USE trains a proxy model on the labeled set to compute entropy scores for unlabeled samples, and then derives a threshold, via statistical comparison against a reference distribution, that separates informative (structured) from uninformative (structureless) samples. This enables assessment as a preprocessing step, removing uninformative or harmful unlabeled data before SSL training begins. Through extensive experiments on imaging (CIFAR-100) and NLP (Yelp Review) data, it is evident that USE consistently improves accuracy and robustness under varying levels of OOD contamination. Thus, it can be concluded that the proposed approach reframes unlabeled data quality control as a structural assessment problem, and considers it as a necessary component for reliable and efficient SSL in realistic mixed-distribution environments.", "tldr": "We introduce USE, an entropy-based and algorithm-agnostic mechanism for unlabeled data quality control that enhances semi-supervised learning beyond heuristic OOD filtering", "keywords": ["Semi-supervised learning", "Unlabeled data quality", "Out-of-distribution detection", "Robust Semi-supervised learning", "entropy-based filtering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7fee79a6e66aaa72e3730a9541ac810bfcbed2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes USE (Uncertainty Structure Estimation), a simple yet effective pre-processing module designed to enhance the robustness of semi-supervised learning (SSL) when unlabeled data contains out-of-distribution (OOD) samples.  \nA proxy model trained on the labeled subset computes the entropy distribution $\\hat{p}(u)$ of unlabeled data, and an adaptive cutoff is determined by intersecting $\\hat{p}(u)$ with a uniform “structureless” reference line $1 / \\log k$.  \nSamples with entropy above $u^*$ are considered unstructured (potentially OOD) and filtered before standard SSL training.  \nExperiments on CIFAR-100 and Yelp Review datasets show consistent improvements across several SSL methods (MixMatch, UDA, FixMatch, FlexMatch), especially under high OOD ratios, without degrading clean performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation: addresses a practical and underexplored problem — OOD contamination in the unlabeled set of SSL.  \n- Simple and model-agnostic: can be easily integrated into any SSL pipeline without architecture changes.  \n- Strong empirical results: consistent improvements across baselines and contamination levels.  \n- Cross-modal validation (vision + text) demonstrates generality.  \n- Good clarity and reproducibility: equations and procedure are clearly described."}, "weaknesses": {"value": "- The uniform reference prior ($1 / \\log k$) is heuristic, with no theoretical justification or ablation on alternative priors.\n- Missing comparison with recent state-of-the-art SSL methods such as CoMatch, SimMatch, and AdaMatch, which incorporate uncertainty modeling.\n- The method depends on the proxy model’s quality, which may be unreliable when labeled data are extremely scarce.\n- No case study or visualization of filtered samples is provided, making it unclear whether USE truly removes OOD data or hard in-distribution examples."}, "questions": {"value": "- Have you analyzed which samples are filtered as “unstructured”? Are there cases where the filtered data are not truly OOD but rather hard in-distribution examples, and what proportion do they represent?\n- How sensitive is USE to the kernel density estimation (KDE) bandwidth and the accuracy or calibration quality of the proxy model?\n- Could adopting a non-uniform or learned prior improve the stability of the intersection threshold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "le4qaopVnZ", "forum": "y79cK0KbTX", "replyto": "y79cK0KbTX", "signatures": ["ICLR.cc/2026/Conference/Submission15680/Reviewer_z4CQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15680/Reviewer_z4CQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642147202, "cdate": 1761642147202, "tmdate": 1762925933443, "mdate": 1762925933443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces USE (Uncertainty Structure Estimation), a lightweight, algorithm-agnostic preprocessing method for robust semi-supervised learning (SSL). USE trains a simple proxy model on labeled data to compute entropy scores for unlabeled samples, then uses kernel density estimation (KDE) to derive a principled threshold to filter unreliable unlabeled data. Extensive experiments on CIFAR-100 and Yelp Review under controlled near/far-OOD contamination demonstrate consistent accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The writing is well-organized: smooth and easy to follow.\n\n* The motivation of the method design is intuitive and reasonable.\n\n* The experimental settings are cross-domain (CV/NLP) with detailed implementations. The results appear impressive."}, "weaknesses": {"value": "* Lacks comparison to related fields: No discussion of open-world SSL (e.g., ORCA [R1]) or Generalized Category Discovery (GCD) [R2], which assume the unlabeled pool may contain both seen-class samples and novel-class samples. These works show that novel-class samples can boost seen-class performance. This work does not consider this realistic setting, and I highly suggest the authors compare with this line of work.\n\n* Narrow empirical scope: Results are reported only on CIFAR-100 and Yelp-250. Additional results on large-scale datasets (e.g., ImageNet-1K, GLUE subsets) are required to strengthen the convincingness of the work.\n\n* Insufficient ablations and sensitivity analysis: The key components lack thorough investigation. For instance, the KDE bandwidth critically controls the smoothness of the entropy density and CDF estimates, yet no ablation is provided to demonstrate robustness across values. Similarly, while the reference curve is presented as modular, only the uniform distribution is tested; alternatives should be evaluated to validate the method's insensitivity.\n\nR1: Open-World Semi-Supervised Learning (ICLR 2022)\n\nR2: Generalized Category Discovery (CVPR 2022)"}, "questions": {"value": "* See weaknesses.\n\n* In Eq. 6, you define the USE threshold as the first intersection following the KS intuition (Sec. 3.2). Do you have any proofs or empirical justification? Why does this maximize D (Eq. 4)?\n\n* Proxy model bottleneck: Entropy scores rely on a proxy trained solely on tiny labeled sets (e.g., 200 labels), leading to underfitting and noisy entropies. The authors claim a strong proxy helps filter structureless samples in Sec 5.2. I am interested in robustness tests of the proxy. Moreover, if there are noisy labels for training the proxy, what will happen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qdh2At2Ehj", "forum": "y79cK0KbTX", "replyto": "y79cK0KbTX", "signatures": ["ICLR.cc/2026/Conference/Submission15680/Reviewer_HTgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15680/Reviewer_HTgz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984571800, "cdate": 1761984571800, "tmdate": 1762925932238, "mdate": 1762925932238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces USE (Uncertainty Structure Estimation), a lightweight method designed to make semi-supervised learning (SSL) more robust. Conducting experiments on CIFAR-100 (200 and 1000 labeled samples) and Yelp Review, demonstrating improvements in most robustness scores with fewer labels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper's primary strength is its conceptual shift in the field of semi-supervised learning (SSL). It moves the focus away from creating increasingly complex algorithms and toward the more fundamental and practical problem of unlabeled data quality, which is a critical bottleneck in real-world applications. \n2.The effectiveness is demonstrated through extensive experiments showing that it consistently improves both the accuracy and robustness of a wide range of SSL algorithms."}, "weaknesses": {"value": "1.It seems that the proposed method depends on the Proxy Model. This paper improves both the accuracy and robustness of a wide range of SSL algorithms, but it hinges on the quality of the \"proxy model\". \n2.The \"first downward crossing point\" rule is a heuristic method. Its stability is heavily affected by the choice of the KDE bandwidth. The rule lacks a theoretical basis to prove its optimality or robustness under various possible data distributions.\n3.More complex datasets are required for validation. The datasets(CIFAR-100 and Yelp Review) are standard benchmarks."}, "questions": {"value": "1.This paper improves both the accuracy and robustness of a wide range of SSL algorithms, but it hinges on the quality of the \"proxy model\". If the labeled data is insufficient or of poor quality, the proxy model will be weak, leading to inaccurate entropy calculations and potentially causing USE to discard useful data or retain harmful data. The paper itself notes that USE's gains are \"larger and more consistent\" with more labeled data (1000 vs. 200 labels), highlighting this dependency may lead to inaccurate entropy calculations and potentially causing USE to discard useful data or retain harmful data (weakness 1).\n2.The \"first downward crossing point\" rule is a heuristic method. Its stability is heavily affected by the choice of the KDE bandwidth, yet the paper does not clarify the setting method for this critical hyperparameter, which raises questions about the method's reproducibility and robustness. The rule lacks a theoretical basis to prove its optimality or robustness under various possible data distributions (weakness 2).\n3. The datasets(CIFAR-100 and Yelp Review) are standard benchmarks. However, their scale and complexity are relatively limited compared to many of today's real-world applications, such as high-resolution medical imaging or large-scale web text. Thus, the generalizability of the method in the vision and NLP domain is slightly less convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "abkbWhJNh3", "forum": "y79cK0KbTX", "replyto": "y79cK0KbTX", "signatures": ["ICLR.cc/2026/Conference/Submission15680/Reviewer_YrJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15680/Reviewer_YrJD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991721976, "cdate": 1761991721976, "tmdate": 1762925931882, "mdate": 1762925931882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In semi-supervised learning (SSL), the quality of the unlabeled data plays a significant role. This work proposes a method, called USE, to filter training samples that do not positively contribute to the prediction of the classification model. The\nUSE trains a proxy model on the labeled set to compute entropy scores for unlabeled samples, and then derives a threshold, via statistical comparison against a reference distribution, that separates informative from uninformative samples. Experiments on different datasets showed that the proposed USE improved the performance of several typical SSL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed USE method is simple and easy to implement. It functions as a plug-in stage prior to downstream SSL training, and therefore can be adopted by any SSL methods. \n2. The experiments showed that on diverse datasets across CV and NLP domains, the proposed USE method showed good results."}, "weaknesses": {"value": "May major concern is about the experiments. Since this is a data pre-processing method, to show its effectiveness, it should be compared with other pre-processing methods for SSL training. But such experiments are lacked. Therefore, the effectiveness of the method is in doubt. \n\nIn addition, the SSL methods tested in experiments are old. The latest one is FlexMatch, published in 2021. It's unclear if the proposed data pre-processing method could also improve the performance of the state-of-the-art SSL methods."}, "questions": {"value": "The proposed method is called Uncertainty Structure Estimation, but I don't find the definition of the \"structure\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PgnFtOaTzT", "forum": "y79cK0KbTX", "replyto": "y79cK0KbTX", "signatures": ["ICLR.cc/2026/Conference/Submission15680/Reviewer_3C9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15680/Reviewer_3C9p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185899002, "cdate": 1762185899002, "tmdate": 1762925931489, "mdate": 1762925931489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}