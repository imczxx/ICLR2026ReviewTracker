{"id": "Vq9vhod9fL", "number": 19900, "cdate": 1758300373359, "mdate": 1759897013086, "content": {"title": "Linear Separability in Contrastive Learning via Neural Training Dynamics", "abstract": "The SimCLR method for contrastive learning of invariant visual representations has become extensively used in supervised, semi-supervised, and unsupervised settings, due to its ability to uncover patterns and structures in image data that are not directly present in the pixel representations. However, this success is still not well understood; neither the loss function nor invariance alone explains it. In this paper, we present a mathematical analysis that clarifies how the geometry of the learned latent distribution arises from SimCLR. Despite the nonconvex SimCLR loss and the presence of many undesirable local minimizers, we show that the training dynamics driven by gradient flow tend toward favorable representations. In particular, early training induces clustering in feature space. Under a structural assumption on the neural network, our main theorem proves that the learned features become linearly separable with respect to the ground-truth labels. To support our theoretical insights, we present numerical results that align with our theoretical predictions.", "tldr": "We present a novel theoretical result showing that SimCLR training dynamics lead to clustering and linear separability, despite nonconvex loss and poor local minima.", "keywords": ["contrastive learning", "neural network", "linear separability", "training dynamics", "variational analysis", "gradient flow", "neural tangent kernel"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2a234fedee577a432ad97b2fee945258b2c1fc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies why contrastive learning (SimCLR-style) often yields well-separated clusters in the learned representation, despite the fact that the contrastive loss itself admits trivial “collapse” solutions. The authors first derive optimality conditions for the NT-Xent loss under invariant feature maps, showing (Theorem 3.1) that any symmetric (e.g. uniform) distribution on the sphere is a stationary solution. They note that, in principle, such minimizers can ignore the data’s latent structure. However, in practice SimCLR does recover semantic clusters (Fig.1). To explain this, the paper analyzes the training dynamics of a neural network under SimCLR. Using a neural-kernel (NTK) viewpoint, Proposition 4.1 lifts the gradient flow on weights to an ODE for the feature outputs $z_i=f(w,x_i)$, involving a data-dependent kernel $K_{ij}(t)=\\nabla_w f(w(t),x_i)^\\top\\nabla_w f(w(t),x_j)$. They contrast this with “vanilla” gradient flow on features alone (no network). Theorem 4.2 shows that, without a network, an invariant map stays invariant, but with a network the invariance can be broken (since $\\nabla_w f(w,x)$ may differ across data points).\n\nThe main theoretical result is Theorem 4.3, proved (for two clusters and 1D embeddings) under several assumptions such (a) initial features within each true cluster are tightly packed (small intra-cluster spread); (b) augmented views of a point remain close to the cluster mean (augmentation consistency); (c) cluster means in gradient-space are separated and intra-cluster gradient variance is small; (d) the NTK remains near its initial value (kernel stability); and (e) a numeric “parameter regime” balancing these quantities holds. Under these conditions, the gradient flow of the contrastive loss drives the two clusters’ embeddings to become linearly separable in finite time. Intuitively, once the kernel $K_{ij}$ develops a block structure correlating with the true clusters (as partly observed in experiments, Fig.5), the clusters are pulled apart in feature space.\n\nThe authors support their theory with experiments on toy and small real datasets. In synthetic examples (donuts, mixtures, MNIST/CIFAR10 in 1D), they show that SimCLR training rapidly produces a clear linear separation of cluster embeddings even from poor initializations (Fig.4). Overall, the paper claims as its main contribution a concrete dynamical mechanism (via the neural kernel) explaining why SimCLR (and related self-supervised methods) tend to recover the data’s cluster structure despite the existence of trivial minimizers. It also provides analysis of loss stationary points (showing uniform/cluster distributions are solutions) and various illustrative experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Training dynamics perspective. The paper advances understanding by analyzing how gradient descent on a neural network (not just the loss landscape) incorporates data geometry. This shows concretely that the network’s parameterization can “break” invariance in a way that fixed feature models cannot (Theorem 4.2).\n\n- Formal linear-separation result. Theorem 4.3 is a nontrivial theoretical claim: under explicit assumptions, contrastive gradient flow provably yields linear separability of clusters. To my knowledge, this is novel (or at least very recent). \n\n- Clear Synthetic experiments. The synthetic experiments effectively illustrate the theory’s claims. For instance, Fig.6 vividly shows that without a neural network the features collapse to a uniform cloud, whereas with a network the true clusters separate (confirming Theorems 4.2–4.3). The NTK evolution (Fig.5) and embeddings (Fig.4,7) provide intuition for abstract conditions. These visualizations make the paper’s arguments more concrete and accessible.\n\n- Condition on linear separability. The Fig. 3 clearly shows the condition for which SimCLR performs linear separability, which is clear and thorough.\n\n- Relates to prior theory. The paper appropriately cites and contrasts related theoretical results. For example, Wang & Isola (2020) showed that the uniform distribution is asymptotically optimal for the contrastive loss; here the authors generalize to finite regimes and characterize all symmetric stationary points (Theorem 3.1), extending Wang & Isola’s asymptotic picture."}, "weaknesses": {"value": "- Strong assumptions. The key theorem relies on several stringent conditions. For example, assumption (a) requires that all data points in a cluster be very close initially, and (c) requires that the gradient centroids for each cluster be well separated relative to within-cluster variation. In practice, random network initializations do not guarantee such structure. It is unclear how often a typical neural net satisfies (c), or how “small” the initial spread $\\varepsilon$ must be. Assumption (d) (kernel stability) essentially assumes an NTK regime, which is debatable for finite-width nets and long training. The authors verify (e) empirically in a toy setting (Fig.3), but more discussion is needed on whether these conditions hold in practice. If the assumptions fail (e.g. data is not very tightly clustered, or kernel changes significantly), Theorem 4.3 may not apply.\n\n- Limited to two clusters and 1D output. The main proof is carried out for exactly two clusters and a one-dimensional embedding, with only a brief claim that “minor modifications” handle more clusters or higher $d$. Real datasets have many classes and high-dimensional features. It would strengthen the paper to outline how the argument scales to $k>2$ clusters and $d>1$. For instance, do the linear algebra and block-structure arguments generalize cleanly? Without this, the applicability beyond toy cases is uncertain. Maybe the authors could take inspiration from the recently published paper of S. Wang: On Linear Separation Capacity of Self-Supervised\nRepresentation Learning, https://www.jmlr.org/papers/volume26/24-2032/24-2032.pdf, JMLR 2025. on how to extend to multiple latent \"clusters\" or \"manifolds\".\n\n- Clarity of dynamics. While the propositions are clear, the intuitive picture could be sharpened. Theorem 4.3’s condition (e) is especially dense: the inequality $\\Theta \\sqrt{n},\\varepsilon \\gg \\varepsilon^3/\\tau + \\gamma + \\sigma^2 + \\delta$ is not easy to parse. Some guidance (e.g. typical scales of these terms) would help. Also, assumption (b) “augmentation consistency” is stated abstractly; a reader may wonder how realistic it is (for, say, random crops on images). More explanation of these requirements and their plausibility would improve the exposition.\n\n- Partial experimental validation. The synthetic and low-D experiments are illustrative but relatively simple. It would add confidence to see at least one larger-scale or real-data experiment. For example, one could train a small SimCLR on MNIST or CIFAR-10 and show that feature separability indeed emerges (e.g. tracking kernel blocks or linear probe accuracy during training). As is, the experiments use toy data or 1D embeddings, so it remains an open question whether the same phenomenon quantitatively holds in practical settings. (The paper’s Figs.4–7 suggest it might, but they are limited in scope.)\n\n- Comparisons to non-contrastive methods. The introduction mentions VICReg, BYOL, etc., but the analysis focuses on contrastive loss. These other methods (non-contrastive or covariance-regularized) also produce separable features in practice. The paper does not discuss whether its dynamics argument extends to those losses. It would be helpful to comment on how (or whether) the key ideas carry over to VICReg or BYOL, which have no explicit negative terms.\n\n- Discussion on the role of the augmentations. The role of the data augmentations is absolutely crucial in self-supervised learning, and a quick study of it could strengthen the paper, especially by discussing the assumption b) under the lens of invariance and identifiability would be beneficial (Julius von Kügelgen et al. Self-Supervised Learning with Data Augmentations\nProvably Isolates Content from Style, https://proceedings.neurips.cc/paper_files/paper/2021/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf, NeurIPS 2021; S. Wang: On Linear Separation Capacity of Self-Supervised\nRepresentation Learning, https://www.jmlr.org/papers/volume26/24-2032/24-2032.pdf, JMLR 2025). Notably, in the Donut experiment, the linear separability is already guaranteed by results that had emerged from the identifiability literature (because invariance to the RandomRotation data augmentation in the latent space is enough to separate classes...). \n\n- The MNIST/CIFAR10 experiments are too unsurprising: the community already expects SimCLR to produce class-separable embeddings on these datasets. Perhaps a stronger test of the paper’s central claim (that training dynamics, via the neural kernel, lead gradient flow to favorable minima) would be to show SimCLR escaping a deliberately constructed ill-posed invariant local minimum (for example, embeddings initialized to a constant map or near-uniform distribution on the sphere). The authors could run controlled experiments that (a) initialize the network in such invariant / low-information states, (b) retrain using the SimCLR objective, and (c) track linear-probe accuracy, kernel blockness, invariance, and kernel drift over time. Demonstrating that SimCLR successfully recover a better local minima that linearly separates classes."}, "questions": {"value": "- Interpretation of assumptions. Can the authors provide intuition or empirical evidence on realistic setups (larger datsets and larger neural networks) for assumptions (a)–(d)? For instance, in a random deep neural network initialization, how small is the typical within-cluster spread $\\varepsilon$ relative to cluster separation? Similarly, can you measure (c)–(d) (gradient means separation and kernel drift) in a larger NN and datasets experiment training run to see if they satisfy the needed inequalities?\n\n- Extension to multiple classes. Theorem 4.3 is stated for two clusters. How does the proof extend to $K>2$ clusters? Are the cluster means assumed to form a single contrast direction (as in Thm.4.3) or multiple directions? Does one need pairwise conditions for every cluster pair, and does the kernel need a full $K\\times K$ block structure? Some clarification or a sketch for $K>2$ would strengthen the generality claim.\n\n- Comparison to VICReg/BYOL. The introduction mentions VICReg and BYOL. Is there an analogue of Theorem 4.3 for non-contrastive losses? Since VICReg and BYOL also avoid trivial collapse, it would be interesting to know if similar dynamics occur. If not directly, do the authors have insight into why contrastive vs. non-contrastive methods differ in this context?\n\n- Empirical demonstration on real data. Can the authors provide an experiment showing the same phenomenon on a modest real dataset (e.g. linear probe accuracy vs. training time on CIFAR) to validate that clusters do separate as predicted? The current figures are enlightening, but a quantitative measure of separability over training (or $\\tau$-value plots) would strengthen the empirical case.\n\n- Empirical kernel stability. Assumption (d) requires $K_{ij}(t)\\approx K_{ij}(0)$ to within $\\delta$. In many NTK studies, the kernel changes slowly only for very wide nets. Did the authors observe this stability in their experiments (e.g. Fig.5)? Could the paper include a plot of $||K(t)-K(0)||$ during training for a real network, to justify this assumption? If not stable, is there a refined analysis for the changing kernel?\n\n- The theoretical analysis focuses on low-dimensional embeddings, where compression naturally promotes separation. How does this picture change as the embedding dimension increases? Does the same dynamic separation mechanism persist for infinitely wide representations, or does it rely on a bottleneck-induced compression effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VKtysv0YTC", "forum": "Vq9vhod9fL", "replyto": "Vq9vhod9fL", "signatures": ["ICLR.cc/2026/Conference/Submission19900/Reviewer_cV97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19900/Reviewer_cV97"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649086216, "cdate": 1761649086216, "tmdate": 1762932058565, "mdate": 1762932058565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical framework for analyzing training dynamics of SimCLR loss, demonstrating (under structural assumptions on the neural network, augmentation consistency, stability of the kernel related to the dynamiscs + other) that learned features become linearly separable with respect to ground-truth labels during training. The authors claim that this is a first theoretically sound and concrete result that explains why SimCLR works even under presence of many bad local optima."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-- The paper attempts to explain the success of SimCLR via understanding how optimal latent representations emerge even though there are a number of bad local minima. \n\n-- The paper describes a structural result for the local optima of SimCLR loss, and following that, results on the dynamics of SimCLR loss are derived. \n\n-- Their main results, Theorem 4.2 and Theorem 4.3, show that under suitable initial conditions and assumptions on augmentation, kernel stability, and others, SimCLR with SGD converges to representations that are linearly separable (assuming that the data comes from a latent model with two clusters)."}, "weaknesses": {"value": "Presentation of technical detail is subpar. The main result uses a set of assumptions without justifying whether these assumptions are restrictive or reasonable. Assumptions made in proofs are not clearly laid out, with some mathematical results seemingly contradicting the language (see questions below)."}, "questions": {"value": "- There is a typo in Line 122 - It should be $T \\sim \\nu$ not $f \\sim \\nu$. \n\n- Lines 160-161 - Possibly technically inaccurate statement. How can the loss become independent of \\mu? Can the authors formally justify this claim mathematically? Similar Q arises for lines 162-165.\n\n- In definition 3: y can be equal to x? If yes, then why will this not lead to any issues?\n\n- Proposition B.2 - seems it is specific to \\psi(t) = \\log(1 + t) but the main body seems to say otherwise. \n\n - Proposition B.3 - “a” and “r” are not defined - I also couldn’t locate them easily anywhere and gave up. \n\n - Line 784-785 - seems like the authors are proving a lemma, rather than a theorem. Also it is not clear why the equality is achieved in the lower bound starting at lines 756-760. Also it seems that Theorem 3.1 holds for only a particular choice of h identified in the proof, but the theorem statement is general. What am I missing?\n\n - I don’t see the relation between the loss in Eq (6) with the loss in Eq(3). \n\n- Theorem 4.3 makes a number of technical assumptions that are critical to the proof and yet no effort is spent on discussing these assumptions and why they are not restrictive or why they are reasonable. \n\n- What about other losses? Like Info-NCE, Sigmoid, Triplet. Will they also exhibit the same structural results of the minimizers and stationary points? \n\n- I would also ask the authors to comment on Neural Collapse in Supervised CL - can their analysis reveal such a configuration arising from the training dynamics? In SCL the latent model usually is a class model and one can make use of class information to construct positive and negative pairs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pT5HkeSxJU", "forum": "Vq9vhod9fL", "replyto": "Vq9vhod9fL", "signatures": ["ICLR.cc/2026/Conference/Submission19900/Reviewer_FofN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19900/Reviewer_FofN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931435578, "cdate": 1761931435578, "tmdate": 1762932057743, "mdate": 1762932057743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the theory of SimCLR: why contrastive learning successfully finds highly structured, linearly separable features when its loss function also permits many degenerate bad solutions. The authors provide an analysis focused on the neural training dynamics driven by gradient flow. They demonstrate that the neural kernel's structure actively sculpts the feature space, amplifying weak, latent cluster signals present in the gradients. The paper's main contribution is a theorem proving that these specific dynamics, under verifiable conditions, guarantee that the learned features will converge toward a linearly separable state, thereby explaining how the training process inherently avoids bad minima and produces useful representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The theoretical analysis of the paper is very solid, deeply investigating the non-convex optimization problem in contrastive learning and providing a concrete dynamical explanation for the success of self-supervised learning.\n\n* The theory and experiments are very well combined, with the abstract mathematical theory being strongly supported by clear and intuitive numerical experiments."}, "weaknesses": {"value": "**Simplified Theoretical Assumptions**\n\nThe theoretical analysis relies on overly simplified assumptions in several key aspects, which may limit the generalizability of its conclusions to real-world SimCLR applications. For instance, the main theorem is derived based on a simplified contrastive loss function (Eq. 12, which applies data augmentation to only one side of the negative pairs) and is primarily proven in the setting of two clusters and a one-dimensional embedding. It remains unclear how these insights generalize directly to the high-dimensional, hyperspherical features under the full NT-Xent loss.\n\n**Limited Experimental Validation**\n\nThe experimental support, while intuitively compelling, is primarily limited to relatively simple or low-dimensional datasets (e.g., Donuts, MNIST) and relies heavily on visualization results (e.g., 1D trajectory plots and t-SNE). The paper lacks clearer quantitative metrics to support its core claims."}, "questions": {"value": "## 1. Regarding the Relationship to the Neural Tangent Kernel (NTK)\n\nYour analysis relies on a kernel stability assumption (in Theorem 4.3, $K(t) \\approx K(0)$), which is a common feature of infinite-width NTK analysis. However, Remark 4.1 explicitly states that your analysis does not assume an infinite-width network. Could you please clarify the connection and distinction between your kernel-based approach and the classical NTK? How is the kernel stability assumption justified in your finite-width setting, especially given that your empirical results (e.g., in Figure 5) show the kernel evolving significantly during training?\n\n## 2. Regarding Quantitative Experimental Evidence\n\nThe paper's experimental support currently relies heavily on visualizations (like 1D trajectories and  t-SNE), which are intuitively helpful. However, could the authors provide clearer quantitative metrics to support the core claim of emerging linear separability? For example, would it be possible to show a plot of downstream linear probe accuracy as it evolves over training iterations, or are there other quantitative methods you could use to demonstrate this phenomenon?\n\n## 3. Regarding the Generalization of Theoretical Simplifications\n\nThe core theorem (Theorem 4.3) is derived under several key simplifications (a one-dimensional embedding, two clusters, and a simplified loss function in Eq. 12). Could the authors elaborate on the primary technical challenges involved in extending this proof to the complete SimCLR setting—that is, to high-dimensional hyperspherical features, multiple clusters, and the full NT-Xent loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WuDZSR1slu", "forum": "Vq9vhod9fL", "replyto": "Vq9vhod9fL", "signatures": ["ICLR.cc/2026/Conference/Submission19900/Reviewer_ZPMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19900/Reviewer_ZPMv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937255397, "cdate": 1761937255397, "tmdate": 1762932057053, "mdate": 1762932057053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why contrastive learning methods produce linearly separable representations. The authors analyze a simplified \"invariance-reduced\" version of the SimCLR (NT-Xent) loss, deriving stationary conditions (Theorem 3.1) and characterizing optimal solutions. Under assumptions of a block-structured neural kernel, they analyze gradient-flow dynamics and prove a finite-time linear-separability result (Theorem 4.3). Empirical validation is provided on toy datasets and small image benchmarks.\n\nWhile the paper works on an important question in self-supervised learning, it suffers, in my understanding, from a fundamental conceptual inconsistency between its two main theoretical results, alongside several technical and presentation issues that limit its contribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Investigates a fundamental SSL question**: The paper works on why contrastive objectives yield linearly separable features which is an important and under-explained phenomenon in self-supervised learning.\n\n- **Finite-time analysis**: Provides an explicit finite-time characterization under gradient flow (Theorem 4.3), offering a dynamic perspective that goes beyond equilibrium analyses such as Wang & Isola [2].\n\n- **Kernel-based formulation**: The technical framework using neural kernels is solid and could potentially connect to recent work on neural collapse and equiangular tight frame (ETF) interpretations of representation geometry."}, "weaknesses": {"value": "### Major Issues\n\n**1. Fundamental conceptual contradiction**\n\nIn my understanding, the paper's two main theoretical results appear mutually inconsistent:\n\n- **Theorem 3.1** characterizes optimal representations as \"evenly distributed points on S^{d-1}\" (line 206). From [3] and [4], we also know that in the mini-batch scenario, evenly distributed configurations correspond to ETFs where all representations are equidistant.\n\n- **Theorem 4.3** claims that gradient flow yields **linearly separable clusters** corresponding to semantic groups.\n\nThese two geometries are fundamentally incompatible. If embeddings are equidistantly spread on the sphere (every point maximally separated from all others), there can be no compact, class-wise clusters. Linear separability of semantic clusters requires non-uniform, block-structured geometry where intra-class distances are smaller than inter-class distances. **How do we transition from evenly distributed data points to linearly separable clusters?** If data representations form an ETF, then a representation has equal distance to all other representations regardless of class membership, directly contradicting the clustered structure required for linear separability.\n\n\n### Further Weaknesses\n\n**2. Unclear novelty and overlap with prior work**\n\nThe authors claim to extend Wang & Isola [2] to the non-asymptotic case. However, [3] and [4] already analyzed mini-batch InfoNCE/KCL variants and showed that for batch size 1 < M ≤ d+1, the unique global minimizer is the ETF (regular simplex), and that for kernel contrastive losses the uniform distribution on the sphere is globally optimal even in the non-asymptotic regime. The manuscript must clarify how its NT-Xent setting differs from the finite-batch regime analyzed in [3, 4], and whether its \"symmetric discrete measures\" coincide with or generalize the ETF solution.\n\n**3. Unsubstantiated and misleading claims**\n\nThe text asserts \"Despite the presence of many undesirable local minimizers of the SimCLR loss\" without citation. In fact, Wang & Isola [2] identified one global minimizer, and [3, 4] showed that all minimizers in the mini-batch case share the same geometry, while they only differ in how they allocate different data points to positions in the ETF structure. What makes a geometry \"undesirable\" depends on whether data representations are arranged in a semantically meaningful way, which is **dataset-dependent**, not an inherent property of the optimizer.\n\n**4. Limited empirical validation**\n\n- Experiments focus on toy and small-scale datasets where linear probing is known to achieve >90% accuracy. For more complex datasets like CIFAR-100, where data points come from 100 distinct object categories, linear classifiers achieve around ~60% accuracy, indicating that learned representations are not highly linearly separable. This suggests the proposed evaluation does not generalize to realistic settings.\n- Training a ResNet-50 on CIFAR-10 using contrastive learning and learning a linear layer on frozen representations yields >90% performance, which is already well-known. The experiments merely confirm what is known rather than providing new insights.\n\n**5. Strong and unjustified assumptions**\n\n- The block-structured kernel assumption is empirically visualized but neither theoretically justified nor quantitatively verifiable. Under what conditions does this structure arise?\n\n**6. Missing citation**\n\n- The motivating question about whether the latent distribution f#μ̃ is similar to the clean distribution μ has already been answered by Zimmermann et al. [1], which is not cited.\n\n**7. Presentation and structure issues**\n\n- No dedicated Related Work section to situate the contribution within existing theoretical analyses (alignment-uniformity, spectral, neural-collapse perspectives).\n- The introduction allocates excessive space to describing general contrastive learning methods but insufficiently discusses the paper's actual results, methodology, and contributions. The contributions are blurry."}, "questions": {"value": "1. **Reconciling uniform vs. clustered solutions**: How do you reconcile the \"evenly distributed\" stationary solutions (Theorem 3.1) with the emergence of linearly separable clusters (Theorem 4.3)? This is the crucial question: we know about the optima, but why do final learned representations become linearly separable in terms of class semantics?\n\n2. **Mechanism of symmetry breaking**: What specific term or assumption in your kernel dynamics breaks the uniform symmetry to produce class-wise clusters? Is this due to augmentation consistency, initialization asymmetry, or stochasticity?\n\n3. **Relation to [3] and [4]**: How does your setting and results differ? Do your \"symmetric discrete measures\" coincide with ETF solutions when M ≤ d+1? What does the theory predict when M > d+1?\n\n\n## References\n\n[1] Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., & Brendel, W. (2021, July). Contrastive learning inverts the data generating process. In International conference on machine learning (pp. 12979-12990). PMLR.\n\n[2] Wang, T., & Isola, P. (2020, November). Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning (pp. 9929-9939). PMLR.\n\n[3] Koromilas, P., Bouritsas, G., Giannakopoulos, T., Nicolaou, M., & Panagakis, Y. (2024, July). Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses. In International Conference on Machine Learning (pp. 25276-25301). PMLR.\n\n[4] Cho, J., Sreenivasan, K., Lee, K., Mun, K., Yi, S., Lee, J. G., ... & Lee, K. (2024). Mini-Batch Optimization of Contrastive Loss. Transactions on Machine Learning Research, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0EyeQ1M23Y", "forum": "Vq9vhod9fL", "replyto": "Vq9vhod9fL", "signatures": ["ICLR.cc/2026/Conference/Submission19900/Reviewer_vGFP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19900/Reviewer_vGFP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943361979, "cdate": 1761943361979, "tmdate": 1762932056487, "mdate": 1762932056487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}