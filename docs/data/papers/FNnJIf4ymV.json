{"id": "FNnJIf4ymV", "number": 22085, "cdate": 1758325753418, "mdate": 1763525068103, "content": {"title": "AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization", "abstract": "Quality-Diversity (QD) algorithms have shown remarkable success in discovering diverse, high-performing solutions, but rely heavily on hand-crafted behavioral descriptors that constrain exploration to predefined notions of diversity. Leveraging the equivalence between policies and occupancy measures, we present a theoretically grounded approach to automatically generate behavioral descriptors by embedding the occupancy measures of policies in Markov Decision Processes. Our method, AutoQD, leverages random Fourier features to approximate the Maximum Mean Discrepancy (MMD) between policy occupancy measures, creating embeddings whose distances reflect meaningful behavioral differences. A low-dimensional projection of these embeddings that captures the most behaviorally significant dimensions can then be used as behavioral descriptors for CMA-MAE, a state of the art blackbox QD method, to discover diverse policies. We prove that our embeddings converge to true MMD distances between occupancy measures as the number of sampled trajectories and embedding dimensions increase. Through experiments in multiple continuous control tasks we demonstrate AutoQD's ability in discovering diverse policies without predefined behavioral descriptors, presenting a well-motivated alternative to prior methods in unsupervised Reinforcement Learning and QD optimization. Our approach opens new possibilities for open-ended learning and automated behavior discovery in sequential decision making settings without requiring domain-specific knowledge.", "tldr": "We propose AutoQD, a method that automatically learns behavior descriptors for Quality-Diversity optimization by embedding policy occupancy measures, enabling unsupervised discovery of diverse policies in reinforcement learning.", "keywords": ["Quality Diversity Optimization", "Policy Representation", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/340f4f6b37bcb1ea1f5172f5f44e062d69d1ef1c.pdf", "supplementary_material": "/attachment/73f7baad0a5ae79bb550b43712a84f1da5566385.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AutoQD, a new method that solves a major problem in Quality-Diversity (QD) optimization: the need for hand-crafted behavioral descriptors (BDs). Instead of simple proxies, it represents a policy's entire behavior using its occupancy measure and uses Random Fourier Features to create a high-dimensional embedding of this occupancy measure. By projecting these embeddings into a low-dimensional space, AutoQD provides meaningful BDs to an \"off-the-shelf\" QD algorithm. This enables the unsupervised discovery of diverse, high-performing policies in reinforcement learning tasks without needing any predefined, domain-specific knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It addresses a widely acknowledged and significant bottleneck in QD optimization: the reliance on hand-crafted, domain-specific BDs.\n- The combination of the occupancy measures concept with Random Fourier Features to efficiently approximate the Maximum Mean Discrepancy is creative."}, "weaknesses": {"value": "- In complex, high-dimensional state-action spaces (like a humanoid robot), a very large number of samples might be needed to get a stable and accurate embedding, potentially making the method computationally expensive or sample-inefficient.\n- A key advantage of hand-crafted BDs is their human-interpretability (e.g., \"walking speed,\" \"jump height\"). The BDs discovered by AutoQD are principal components of high-dimensional RFF embeddings, likely mathematically abstract and lacking any obvious physical meaning. This makes it difficult for researchers to understand what new behaviors are being discovered, which is often a primary goal of using QD.\n\nAuthors acknowledge these limitations."}, "questions": {"value": "- Theorem 1 is based on i.i.d. samples, whereas the practical estimator ($\\psi^{\\pi}$) uses discounted, correlated samples from trajectories. Could you elaborate on how the theoretical bounds from Theorem 1 apply to this practical estimator, especially given the non-i.i.d. nature of the data?\n- Regarding the periodic updates of the BDs, could you clarify the mechanism for managing the archive? Since updating the cwPCA projection effectively changes the coordinate system of the behavior space, how are existing policies in the archive handled? Is there a re-mapping process, and if so, how does the algorithm manage the computational cost and potential instability or 'forgetting' of previously discovered niches?\n- The paper notes that AutoQD inherits the scalability challenges of the QD optimizer, but it also seems to introduce new computational steps, like storing all high-dimensional embeddings and periodically running PCA on the full archive matrix. Could you comment on the scalability of this new step as the archive size grows, and how significant this overhead is compared to the baseline QD algorithm?\n- In the baseline comparisons, AutoQD uses a 4-dimensional BD space, while many of the RegularQD baselines use 2-dimensional descriptors. Given the paper's ablation study showing that performance scales with descriptor dimensionality, how can we disentangle the contribution of the AutoQD method itself from the contribution of simply using a higher-dimensional behavior space?\nAutoQD's Dimension: Table 5 (Line 1029 on Page 20) explicitly states that AutoQD used a \"Measures Dimension\" of 4 for the main experiments.\nBaseline's Implied Dimension: Section D.7 (Line 1070 on Page 20) describes the handcrafted descriptors for RegularQD. For robots like Walker2d, Bipedal Walker, and Hopper, it uses \"foot-contact frequencies.\" These robots have two feet, which implies a 2-dimensional descriptor (one for each foot)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PFpEA9J7VY", "forum": "FNnJIf4ymV", "replyto": "FNnJIf4ymV", "signatures": ["ICLR.cc/2026/Conference/Submission22085/Reviewer_sHcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22085/Reviewer_sHcp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327310754, "cdate": 1761327310754, "tmdate": 1762942059859, "mdate": 1762942059859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue that Quality-Diversity (QD) optimization algorithms rely on hand-crafted Behavioral Descriptors (BDs) which limit exploration diversity, and proposes the AutoQD method. AutoQD leverages the one-to-one correspondence between policies and occupancy measures in MDPs, uses Random Fourier Features to approximate the Maximum Mean Discrepancy between occupancy measures for policy embedding, reduces high-dimensional embeddings to low-dimensional BDs via Calibrated Weighted PCA (cwPCA), and combines with the QD algorithm CMA-MAE to alternate between policy optimization and BD update. Theoretically, it proves that embeddings converge to the true MMD as the number of trajectories and embedding dimensions increase. Experimentally, on six continuous control tasks from the Gym library, AutoQD outperforms five baselines in most tasks in terms of GT QD Score, diversity, and adaptability to environmental changes (friction/mass variations), enabling the automatic discovery of diverse and high-performance policies without requiring domain knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n\n2. Solid theoretical support; Eliminates reliance on hand-crafted BDs; Compatible with existing QD algorithms and verified on mainstream tasks."}, "weaknesses": {"value": "1. Low sample efficiency, needing many trajectories in stochastic environments.\n\n2. Low-dimensional BDs may miss complex behaviors (e.g., ignoring leg-lifting in Walker2d).\n\n3. Inferior maximum fitness compared to RL methods.\n\n4. Fixed kernel bandwidth; poor scalability with large policy networks.\n\n5. (Minor but Worth Discussing) A minor yet notable point worth discussing is the subjectivity of the concept of \"diversity\" itself. Diversity in QD optimization is inherently user-defined, as different users may prioritize distinct dimensions of behavioral variation for the same task. For example, in robot locomotion tasks, one user might focus on gait types (walking, hopping, sliding) as the core of diversity, while another could emphasize energy efficiency or adaptability to uneven terrain. However, AutoQD’s approach, which infers behavioral diversity from occupancy measure embeddings and MMD-based distances, essentially discovers implicit diversity definitions derived from data and mathematical metrics, rather than aligning with explicit user preferences. While this avoids the need for domain knowledge in hand-crafting BDs, it also raises the risk that the \"diverse\" policies AutoQD discovers may not fully match the specific diversity needs of practical users (e.g., a user caring about \"load-carrying capacity-related behaviors\" might find AutoQD’s focus on \"trajectory smoothness\" less relevant)."}, "questions": {"value": "1. How to avoid missing complex behaviors beyond increasing BD dimensions?\n\n2. How to control computational complexity in high-dimensional observation tasks (e.g., images in Atari)?\n\n3. What’s the path to combine with gradient-based QD for better sample efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0ys0pZUJ2", "forum": "FNnJIf4ymV", "replyto": "FNnJIf4ymV", "signatures": ["ICLR.cc/2026/Conference/Submission22085/Reviewer_cGSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22085/Reviewer_cGSg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835610736, "cdate": 1761835610736, "tmdate": 1762942059506, "mdate": 1762942059506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoQD, an algorithm for discovering diverse, high-performing policies without hand-crafted behavioral descriptors. The method combines a Quality-Diversity (QD) optimizer with an automated way to learn behavioral descriptors of policies.\n\nAutoQD represents each policy through its occupancy measure (the distribution over state-action pairs the policy visits). To compare the behavior of two different policies, one can measure the distance between their occupancy measures using the Maximum Mean Discrepancy (MMD). Computing MMD directly is costly, so the authors approximate it using random Fourier features, which transform each occupancy measure (and hence each policy) into a finite-dimensional behavioral embedding. In this embedding space, Euclidean distances approximate the true MMD distances between policies (Theorem 1). Because QD optimizers operate over low-dimensional behavior spaces, AutoQD applies Calibrated Weighted PCA to these embeddings (weighted by their performance) to project them into compact behavioral descriptors. These descriptors and their performances are then passed into a standard QD algorithm (CMA-MAE) to discover a diverse set of high-performing policies (see Figure 1 for an illustration of the entire pipeline)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "[Originality] Proposes a new theoretically motivated connection between occupancy measures and QD behavior descriptors, moving beyond handwritten heuristics.\n\n[Quality] Method is modular and seems to be somewhat compatible with different QD optimization methods (authors only test CMA-MAE QD methods, and I have a question regarding gradient-based methods)\n\n[Quality] Implementation details are well-documented in Appendix D. The code is well structured (I haven't run it, but I have read some of it and was able to find the main parts of the algorithm).\n\n[Clarity] Introduction and motivation are clear and well-written.\n\n[Clarity] Figure 1 and Algorithm 1 make the method easy to understand. I would add some \"labels\" to the different components of Figure 1 (e.g. QD archive, behavioral embeddings, etc).\n\n[Significance] Addresses a limitation of QD methods - the need for hand-crafted behavior descriptors in a principled way."}, "weaknesses": {"value": "[Method]\n\n- Computing accurate embeddings requires many trajectories, which might be very sample inefficient in long-horizon or high-variance tasks.\n\n- The kernel bandwidth and embedding dimension are fixed globally, but they could have a significant influence on performance. \n\n[Experiments]\n\n\n- Narrow experimental scope.\n\nThe experiments are mainly against QD methods that also use CMA-MAE (The experiments also include another evolutionary method (DvD-ES) and an RL method (SMERL) which are not very competitive in the tested domains). To clarify, there are no gradient-based QD baselines (e.g. DQD, PGA-MAP-Elites, PPGA). If AutoQD should be seen as an improvement to other CMA-MAE QD methods, then the paper should frame AutoQD as a CMA-MAE enhancement. Otherwise, the paper needs more broad experimental validation to support a claim of superiority.\n\n- Lack of failure analysis. (Minor)\n\nThe authors briefly acknowledge underperformance of AutoQD on Walker2d and HalfCheetah (section 4.3), but don't have any deep analysis or visual evidence. Exploring failure cases more systematically would strengthen the paper, e.g., by visualizing the learned behaviors or the structure of the descriptor space to provide intuition about what AutoQD fails to capture. Generally, it's unclear what the gained diversity looks like (see later point).\n\n- Misalignment between motivation and evaluation.\n   \nThe introduction motivates quality-diversity through robustness and open-ended learning (i.e., diverse policies should help under dynamic shifts or enable new capabilities).\n\n \"First, diverse policies provide robustness against changing conditions$\\textendash{}$when one policy fails, alternatives with different behavioral characteristics might succeed. Second, diversity is crucial for open-ended learning, where the goal extends beyond solving predefined problems to continually discovering novel capabilities and behaviors\".\n\nHowever, the experiments evaluate only internal QD metrics (QD Score, VS, and qVS), which capture diversity (mostly) and performance within the same environment. Even with experiment 4.4, the authors only examine the best policy. These policies weren't trained to be individually robust, so I'm not sure how meaningful/interesting of a comparison that is compared to looking at population-level robustness (specific suggestions in the Questions section, e.g. percentage of top K policies that achieve a certain reward under parameter shifts). The next point covers weaknesses of this experimental setup.\n\n\n- Overreliance on internal diversity metrics and lack of qualitative analysis.\n\nThe evaluation relies heavily on intrinsic metrics. VS and QD score capture internal notions of diversity but provide limited insight into behavioral / functional variety. qVS multiplies VS by the average return, yielding a crude diversity-quality product that can be misleading: a method with greater qVS may still have lower-quality policies overall. For instance, in Table 1, although AutoQD dominates the Ant environment according to these metrics, the qVS/VS ratio suggests that RegularQD achieves higher average policy performance (i.e., better quality but lower diversity). This raises the question of whether seeing only numerical gains on such metrics truly reflects meaningful improvements.\n\nMore broadly, the experiments don't convincingly demonstrate that AutoQD's learned diversity is \"useful\" (e.g., enhancing robustness or functional adaptability). The only robustness test (section 4.4) examines the best policy under environmental shifts rather than the population-level adaptability that Quality-Diversity methods are designed to provide. Visualizations of the learned descriptor space or representative behaviors would also strengthen the paper by clarifying whether the discovered diversity is interpretable and/or behaviorally meaningful.\n\n\n[Theorem 1]\n\n- Mismatch between theory and implementation.\n   \nTheorem 1 establishes a probabilistic bound under the assumption of independent samples drawn from the occupancy measure. However, the practical estimator in Eq. (6) uses correlated samples collected along trajectories, which violates this assumption. As a result, the theoretical guarantee does not strictly apply to the estimator used in AutoQD. The theorem thus serves primarily as \"theoretical motivation\" rather than a formal guarantee for the implemented method.\n\nAs far as I understand, under mild ergodicity assumptions, Eq. (4) holds in the limit, but the rate of convergence in Theorem 1 would be a lot slower. Part of the reason why this is important is that breaking the i.i.d. assumption is essential to make the method practically usable, it wasn't done just for convenience in the experiments. I would suggest making this clear in the main text of the paper.\n\n\n\n- Practical relevance of Theorem 1\n\nThere is no guidance on how many trajectories/features are needed for good approximation. The experiments primarily use D = 100, which is likely too small for the theoretical regime where the bound would hold anyway."}, "questions": {"value": "[General]\n\n- The geometry induced by MMD depends on the chosen kernel and might not reflect meaningful behavioral similarity. Do you think other more \"structrure-aware\" distances (e.g. Wasserstein/Energy) could be used instead and capture the geometry of the space better (or be more interpretable)? (I imagine that the compatibility of MMD and RFFs makes it hard to just switch).\n\n- In the introduction, the authors state: \"Crucially, there exists a one-to-one correspondence between policies and their occupancy measures.\"\nThis statement is not always true, as you need conditions like full observability and sufficient support over state-action pairs. I recommend rephrasing this to acknowledge the required assumptions (e.g., \"under standard assumptions in fully observable MDPs\") to avoid overclaiming and be precise about the problem scope. \n\n- In \"Future Work\", the authors mention integrating AutoQD with gradient-based QD methods, which implicitly suggests that such integration is non-trivial. It would be helpful to clarify why gradient-based QD methods might be harder to use with AutoQD. \nOne potential challenge is that cwPCA continually updates the affine mapping (A, b) used to compute behavioral descriptors. In a gradient-based setting, this would make the descriptor space a moving target, potentially destabilizing learning. \n\n- Scope clarification: The abstract states that the learned embeddings \"are then used as behavioral descriptors for off-the-shelf QD methods,\" which may overstate generality. The authors should check the wording both in the abstract and conclusion to specify that they focus on CMA-MAE methods and not imply that this is already compatible with the \"standard QD algorithms\".\n\n\n\n[Experiments]\n\n- Visualization of behavior space (what diversity looks like): Can you somehow visualize trajectories to show how policies differ across the learned descriptor space (to better connect the metrics of 4.3 to intuitive diveristy). Additionally, can you visualize what behavioral dimensions cwPCA captures? \n\n- Robustness and usefulness of diversity: Can the authors evaluate the diversity's benefit by testing population-level robustness? For example, measure how many policies in the archive (or the top-K) remain \"successful\" (or perform well) under dynamics changes (friction, mass). It would also help to report mean or quantile returns under these perturbations in Section 4.4, rather than only the best policy's performance.\n\n- Can you include experiments against gradient-based QD methods?\n\n\n[Theorem]\n\n- Is there any heuristic to determine when the number of trajectories or random features is sufficient for the MMD embedding to approximate occupancy-measure distances reliably? + In practice, how does QD performance vary with the number of random features or trajectories? (Ablation in Appendix E partially answers the second part of this).\n\n\nMy initial score reflects that I'm unsure whether this paper is ready for ICLR. This is a good piece of work, and I'm keen to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5dStpCBoKK", "forum": "FNnJIf4ymV", "replyto": "FNnJIf4ymV", "signatures": ["ICLR.cc/2026/Conference/Submission22085/Reviewer_aLN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22085/Reviewer_aLN2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836082800, "cdate": 1761836082800, "tmdate": 1762942059037, "mdate": 1762942059037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoQD, a method that automatically discovers behavioral descriptors for Quality-Diversity optimization. It proposes embedding a policy's unique state-action occupancy measure using Random Fourier Features, such that the L2 distance between embeddings approximates the Maximum Mean Discrepancy (MMD). A Calibrated Weighted PCA is then used to project these embeddings into a low-dimensional descriptor space, prioritizing variations among high-performing policies. This enables standard QD algorithms like CMA-MAE to discover diverse and effective behaviors without requiring hand-crafted, domain-specific features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is theoretically sound, rigorously connecting occupancy measures, MMD, and Random Fourier Features to create a principled and efficient metric for behavioral distance.\n\n\n- The proposed policy embedding method is a versatile contribution with significant potential beyond QD. This technique for representing policy behavior could be applied to other RL tasks, making it a valuable tool for the broader community.\n\n\n- The experiments are extensive and includes a diverse set of environments even thought they are all low dimensional in the state space observations and clearly show that AutoQD is able to outperform sota algorithms in terms of GT QD Score."}, "weaknesses": {"value": "- There exist a gap between the theoretical guarantees and the practical implementation of the policy embedding. Theorem 1 provides a powerful result for embeddings ($\\phi^\\pi$) constructed from i.i.d. samples drawn from the occupancy measure. However, the paper acknowledges that this sampling strategy is too inefficient for practical use. Instead, the algorithm uses a different estimator ($ \\psi^\\pi$ from Eq. 6) that averages features over all transitions in a trajectory. The paper lacks a formal analysis of how this practical estimator affects the approximation quality, leaving a gap in the theoretical justification of the implemented algorithm.\n\n- The empirical validation, while strong, is exclusively focused on environments with low-dimensional, vector-based state spaces. The scalability of the RFF-based embedding to high-dimensional observations, such as images, remains unproven. Such environments could pose significant challenges, as the curse of dimensionality might render the MMD approximation less effective or require an impractically large embedding dimension (D).\n\n- While the proposed method is interesting, it seems to completely disregard temporal information about the order of events. Two policies that produce the same state-action occupancy distribution but follow a different temporal order could lead to very different decision-making behaviors. Therefore, they should be accurately identified as distinct (or diverse) policies."}, "questions": {"value": "See the Weaknesses points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1IswWKfQ9o", "forum": "FNnJIf4ymV", "replyto": "FNnJIf4ymV", "signatures": ["ICLR.cc/2026/Conference/Submission22085/Reviewer_Xeky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22085/Reviewer_Xeky"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762205523373, "cdate": 1762205523373, "tmdate": 1762942058827, "mdate": 1762942058827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}