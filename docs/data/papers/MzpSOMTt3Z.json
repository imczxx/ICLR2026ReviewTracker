{"id": "MzpSOMTt3Z", "number": 1983, "cdate": 1756974329161, "mdate": 1759898175204, "content": {"title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "abstract": "Recent advances in large language models (LLMs) have significantly improved text-to-speech (TTS) systems, enhancing control over speech style, naturalness, and emotional expression, which brings TTS Systems closer to human-level performance.\nYet evaluation still relies largely on the Mean Opinion Score (MOS), whose subjectivity, environmental variability, and limited interpretability prevent it from faithfully capturing how human-like the synthesized audio is.\nExisting evaluation datasets also lack a multi-dimensional design, often neglecting factors such as speaking styles, context diversity, and trap utterances, which is particularly evident in Chinese TTS evaluation.\nTo address these challenges, we introduce the **A**udio **T**uring **T**est (ATT), a multi-dimensional Chinese corpus dataset ATT-Cropus paired with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on complex MOS scales or direct model comparisons, ATT asks evaluators to judge whether a voice sounds human. This simplification reduces rating bias and improves evaluation robustness.\nTo further support rapid model development, we also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for automatic evaluation. \nExperimental results show that ATT effectively differentiates models across specific capability dimensions using its multi-dimensional design. \nAuto-ATT also demonstrates strong alignment with human evaluations, confirming its value as a fast and reliable assessment tool.", "tldr": "We introduce the Audio Turing Test (ATT), an evaluation framework including a multi-dimensional Chinese corpus ATT-Corpus with an effective, Turing-Test-inspired evaluation protocol.", "keywords": ["Text-to-Speech", "Audio", "Human-likeness Evaluation", "Large Language Model", "Turing Test"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7e4de761cc28b8ad71eb25eda680d13fb297019.pdf", "supplementary_material": "/attachment/c15bc19c7a2867e13fbc7ae70ecf111844ada37f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Audio Turing Test (ATT), a novel evaluation framework for assessing the human-likeness of LLM-based Chinese Text-to-Speech (TTS) systems. ATT integrates a multi-dimensional corpus (ATT-Corpus) covering five key linguistic dimensions (e.g., Chinese-English code-switching, polyphonic characters), a Turing Test-inspired human evaluation protocol, and an automatic evaluation tool (Auto-ATT) fine-tuned on Qwen2.5-Omni-7B. Unlike traditional Mean Opinion Score (MOS) methods, ATT uses ternary judgments ([Human], [Unclear], [Machine]) to reduce bias and improve discriminative power. Experiments with 857 native Chinese listeners and 5 state-of-the-art TTS models show that ATT effectively distinguishes model performance, with top-performing Seed-TTS achieving a Human-likeness Score (HLS) of only 0.4—revealing gaps between synthetic and human speech. Auto-ATT demonstrates strong alignment with human judgments and outperforms conventional MOS predictors on trap items."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Targeted Solution to Critical Gaps: Addresses MOS’s limitations (subjectivity, low interpretability) and the lack of multi-dimensional, Chinese-specific TTS evaluation datasets, filling a key niche in LLM-driven TTS assessment.\nComprehensive Framework Design: Combines a well-constructed corpus (semi-automated generation + expert validation), rigorous human evaluation (trap items, consistency checks), and an efficient automatic tool, enabling both qualitative and quantitative analysis.\nRobust Experimental Validation: Large-scale human evaluations (857 participants) and statistical tests (GLMM) confirm ATT’s reliability, while Auto-ATT’s superior performance over UTMOSv2 and DNSMOS Pro highlights its practical value for rapid model iteration.\nActionable Insights: Identifies specific weaknesses of current TTS systems (e.g., prosodic unnaturalness, flat emotional expression) and provides fine-grained comparisons across models, voices, and linguistic dimensions."}, "weaknesses": {"value": "Language and Scenario Limitation: The framework is exclusively designed for Chinese, limiting generalizability to other languages with distinct linguistic features (e.g., tonal vs. non-tonal languages).\nNarrow Trap Item Diversity: While trap items monitor attention, the paper only mentions \"deliberately flawed synthetic clips\" and \"genuine human recordings\"—more diverse trap types (e.g., edge-case linguistic structures) could strengthen robustness.\nAuto-ATT Training Data Opacity: The paper references \"additional private evaluation data\" for Auto-ATT training without detailing its size, distribution, or how it complements public ATT-Corpus, raising questions about reproducibility.\nLack of Longitudinal or Real-World Testing: Evaluations focus on controlled audio clips; performance in real-world scenarios (e.g., background noise, dialogue context) is not explored, limiting insights into practical applicability."}, "questions": {"value": "Given ATT’s Chinese-specific design, what key adaptations would be required to extend the framework to non-tonal languages (e.g., English) or languages with unique prosodic features (e.g., Japanese)?\nHow does Auto-ATT’s performance degrade when evaluating TTS models not included in its training data (e.g., newly developed models with novel architectures), and what strategies could mitigate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tewI3X3zV0", "forum": "MzpSOMTt3Z", "replyto": "MzpSOMTt3Z", "signatures": ["ICLR.cc/2026/Conference/Submission1983/Reviewer_RQm4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1983/Reviewer_RQm4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930494146, "cdate": 1761930494146, "tmdate": 1762915984082, "mdate": 1762915984082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Audio Turing Test (ATT), a human-likeness evaluation framework for Chinese LLM-TTS that pairs (i) a multi-dimensional corpus (ATT-Corpus) spanning numerals/special characters, code-switching, paralinguistics & emotions, classical prose/poetry, and polyphonic characters, with (ii) a ternary, Turing-style human protocol that labels each clip as Human / Unclear / Machine and derives a Human-Likeness Score (HLS) (1.0, 0.5, 0.0). The authors also fine-tune Qwen2.5-Omni-7B with human judgments to create Auto-ATT, a model-as-a-judge that predicts HLS and reportedly aligns strongly with human ratings. Benchmarks across five model families show clear separation and notably low absolute human-likeness (best model ≈0.4), in contrast with high MOS reported elsewhere."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Multidimensional corpus targets common Chinese difficulty factors (polyphony, poetry syntax, code-switching).\n\n* Ternary human protocol + rationales is a simple but meaningful shift away from MOS.\n\n* The implementation of trap items as a good quality control.\n\n* Auto-ATT is a useful direction; training a speech-judge model is under-explored, and the demonstrated correlation to humans is promising.\n\n* The benchmark highlights meaningful gaps between SOTA models and human speech."}, "weaknesses": {"value": "1. (Interpretability of Human-Likeness) HLS collapses three distinct cases (Human mistaken as Machine, Machine mistaken as Human, and Unclear) into a single linear score. Without reporting how often each category is chosen, it is unclear whether high HLS reflects genuine human-likeness or annotator uncertainty. Excessive “Unclear” selections may artificially inflate scores.\n\n2. (Filtering Bias From Manual Spot Checks) The authors state that samples failing “synthesis success” or “synthesis consistency” are verified. Eliminating weak samples before evaluation can bias results toward best-case outputs. The paper does not clarify what was done with samples that failed this spot check.\n\n3. (Sampling Policy Ambiguity) It is unclear whether annotators see one sample from each system or a random subset. If some participants repeatedly select “Unclear,” this may distort HLS. Details on randomization, system coverage, and balancing are not reported.\n\n4. (Annotator and Expert Clarity) The paper inconsistently reports annotator counts (437 vs 857). Expert selection criteria are unclear, and post-hoc alignment of participant justifications to labels is subjective. The number of experts per sample and conflict resolution process are not specified.\n\n5. Missing Citations -\n* Praveen S V, Sherry Thomas, Sai Teja M S, Suvrat Bhooshan, Mitesh M. Khapra \"The State Of TTS: A Case Study with Human Fooling Rates.\" Proc. Interspeech 2025\n* Nguyen, Binh, and Thai Le. \"TURING’S ECHO: Investigating Linguistic Sensitivity of Deepfake Voice Detection via Gamification.\" Proc. Interspeech 2025\n\n6. (No Comparison With Established Listening Tests) The benchmark is not compared against MUSHRA or CMOS for ranking fidelity. While ATT may separate systems well, this has not been demonstrated relative to standard perceptual tests."}, "questions": {"value": "1. Regarding manual spot checks: “We examine synthesis success and consistency.” What is the removal policy for failed samples? How many samples were discarded per system? Could this bias the evaluation toward cherry-picked successes?\n\n2. How are samples assigned to annotators? Does each participant hear all systems so that within-subject comparison is possible?\n\n3. How often do annotators choose Human / Machine / Unclear? Does widespread “Unclear” bias the HLS scale?\n\n4. How were expert reviewers selected, and how many participated? In cases of disagreement regarding the presence or absence of an artifact, what adjudication procedure was applied? \n\n5. The annotator count is inconsistently reported as 437 and 857. Which value is correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IKils7249k", "forum": "MzpSOMTt3Z", "replyto": "MzpSOMTt3Z", "signatures": ["ICLR.cc/2026/Conference/Submission1983/Reviewer_HQSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1983/Reviewer_HQSS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980918613, "cdate": 1761980918613, "tmdate": 1762915983957, "mdate": 1762915983957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the AudioTuringTest Benchmark, a new evaluation framework for Chinese TTS models. The paper attempts to address the reproducibility and saturation issues of MOS-like metrics, the de-facto evaluation protocol for TTS models. To do this, the ATT benchmark attempts to simplify the rating criteria into a Turing Test-like metric, whether or not a speech sample is from a human. ATT is created with human judgements of synthetic and real data. The authors also create Auto-ATT a model-as-a-judge version of ATT. Results suggest that ATT is able to capture key differences between TTS systems along several axes and Auto-ATT correlates well with human judgement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- ATT attempts to address the critical limitations of MOS / pseudo-MOS by disentagling speech characteristics at the data level and simplifying the evaluation scheme \n- ATT evaluates along several axes, such as numerals, code-switching, paralinguistics, and poetry.\n- ATT can clearly distinguish the strengths and weakness of different model along each axes, allowing fine-grained insights of TTS performance \n- Auto-ATT is a novel model-as-judge that can be used to automate the application of ATT at scale"}, "weaknesses": {"value": "- The ATT corpus is developed using the TTS models the authors intend on evaluating. It is unclear how it and AutoATT generalize to unseen systems, which does not address the claimed robustness issue of pseudo-MOS.\n- ATT cannot distinguish speaker-level characteristics, which makes evaluation using speaker similiarity MOS or neural embeddings still required"}, "questions": {"value": "- Will the annotator ratings be released with the dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JL4l1DhL9t", "forum": "MzpSOMTt3Z", "replyto": "MzpSOMTt3Z", "signatures": ["ICLR.cc/2026/Conference/Submission1983/Reviewer_C953"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1983/Reviewer_C953"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125360899, "cdate": 1762125360899, "tmdate": 1762915983653, "mdate": 1762915983653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}