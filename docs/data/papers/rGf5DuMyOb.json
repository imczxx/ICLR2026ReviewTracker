{"id": "rGf5DuMyOb", "number": 20836, "cdate": 1758310735412, "mdate": 1759896956186, "content": {"title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs", "abstract": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs—primarily based on large complex transformer architectures with high computational and parameter overhead—SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.", "tldr": "Small PDE UNet Solver (SPUS) is a parameter-efficient foundation model and as accurate as medium transformers for neural PDE solvers", "keywords": ["foundation models", "neural pde solvers", "deep neural architectures"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1221a13d15c64e0aa3452b8a3f31c09b6ca3c4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SPUS, a  compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). It adopts a U-Net architecture which is underexplored as a foundation model backbone in the neural PDE solver community. It utilizes an auto-regressive strategy which predicts the entire trajectory based on the initial condition. Experiments demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall well-written and easy to follow. The problem setup and the experiment design are clear.\n- The experiment results show that the model is capable of generalizing to unseen initial conditions, unseen equations and scales well with respect to dataset size. The selected two baselines are representative.\n- The U-Net architecture is underexplored in the scientific machine learning community. This paper demonstrates the capability of U-Net, which does not have the artifact issues of ViT and ViT-like models."}, "weaknesses": {"value": "- Formatting issues. Equation (1) does not seem to be properly aligned or wrapped; Line 215 contains only a single $d$.\n- Limiting the input to a single initial condition makes the model unable to utilize temporal information. For example, the model cannot simultaneously predicts $t_{0.5}$ and $t_1$ based on $t_0$, while DPOT can change the temporal interval of the input trajectories, and POSEIDON can change the input $t$. This limits the model's capability as a foundation model.\n- From my perspective, model-efficiency is not a must for a foundation model, as therefore cannot be considered as an advantage. Moreover, apart from data scalability, model parameter scalability is also important, which could be a disadvantage of U-Net based models."}, "questions": {"value": "- What is the parameter count of the adapters?\n- Can the model generalize to different input resolutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sgeUMMObeo", "forum": "rGf5DuMyOb", "replyto": "rGf5DuMyOb", "signatures": ["ICLR.cc/2026/Conference/Submission20836/Reviewer_sNGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20836/Reviewer_sNGT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537362026, "cdate": 1761537362026, "tmdate": 1762936329189, "mdate": 1762936329189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SPUS (Small PDE U-Net Solver) — a lightweight (36 M parameter) residual U-Net–based foundation model (FM) for partial differential equations (PDEs).\nUnlike prior large transformer-based PDE FMs such as POSEIDON, DPOT, and PROSE-FD, SPUS adopts a simple convolutional encoder–decoder architecture and trains it with an autoregressive (AR) next-step prediction objective, mimicking the behavior of numerical solvers.\nThe model is pretrained on several compressible Euler PDEs from the PDE-GYM suite and fine-tuned on six unseen downstream PDEs (Euler, Navier–Stokes, and wave equations)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Demonstrates that convolutional architectures remain competitive for PDE foundation modeling, despite recent transformer dominance.\n\nProvides quantitative comparison to state-of-the-art FMs (POSEIDON, DPOT).\n\nUses challenging, publicly available PDE-GYM benchmarks and tests autoregressive training in a realistic temporal-prediction setup.\n\nPaper is clear; experimental pipeline (pretrain → finetune → rollout) is logically organized."}, "weaknesses": {"value": "- Lack of genuine novelty and conceptual contribution\n\nThe proposed approach offers no clear methodological innovation. The authors simply adopt a standard residual U-Net architecture, train it with a well-known autoregressive (AR) next-step prediction strategy, and evaluate it on existing PDE-GYM datasets. Both the architecture and the training scheme have been extensively studied in the PDE-learning literature. As a result, the paper mainly constitutes a combination of established components rather than a new idea or modeling paradigm. In addition, the overall presentation, including figures and terminology, closely mirrors the POSEIDON paper, while providing less comprehensive experimental analysis.\n\n- Loss of temporal generality compared to POSEIDON\n\nThe POSEIDON model is trained in an all-to-all fashion with continuous-in-time embeddings, enabling the model to be queried at any arbitrary time. In contrast, the AR formulation adopted by SPUS predicts only the next timestep given the current one, thus losing the ability to interpolate or query arbitrary intermediate times. This restricts the model’s flexibility and makes it unsuitable for applications that require continuous-time prediction.\n\n- Limited applicability to time-independent PDEs\n\nBecause SPUS relies entirely on an autoregressive temporal formulation, it cannot be directly evaluated on steady-state or time-independent PDEs. Supporting such tasks would require additional architectural or procedural engineering (e.g., removing temporal conditioning or introducing pseudo-time variables). The paper does not discuss how such cases would be handled, which limits the generality of the claimed “foundation model” status.\n\n- Incomplete and potentially biased downstream evaluation\n\nAlthough the authors use the PDE-GYM datasets originally introduced in POSEIDON, they report results on only 6 out of 15 downstream tasks available from that benchmark. The criteria for selecting these tasks are not discussed, and the chosen subset coincides with cases where convolutional architectures are known to perform well. This selective evaluation raises concerns that the tasks were cherry-picked to support the paper’s narrative, rather than representing a fair or comprehensive test of generalization.\n\n- Unfair comparison with POSEIDON due to inference mode\n\nThe most critical methodological flaw is the evaluation protocol for POSEIDON. In its original paper (Appendix D.6.2), POSEIDON demonstrates that autoregressive (AR) rollouts yield substantially better performance than direct one-shot predictions for long trajectories (the exact setup used in SPUS). However, in this paper, POSEIDON is evaluated only in direct mode, which is known to degrade accuracy for long-horizon rollouts. Since SPUS performs autoregressive prediction by design, this creates a fundamental evaluation mismatch that favors SPUS.\n\n- Missing comparison to smaller baseline variants\n\nThe paper claims substantial parameter efficiency, yet comparisons are made only against large versions of the baselines (POSEIDON-B = 158 M, DPOT-M = 122 M parameters). Both papers also provide smaller variants, POSEIDON-T (21 M) and DPOT-S (30 M), that achieve performance comparable to their base versions. Since SPUS (36 M) is much closer in scale to these lighter models, a fair evaluation of parameter efficiency must include them. Without this, the main empirical claim remains unsubstantiated.\n\n- Unclear evaluation protocol and dataset alignment\n\nIt is not specified at which timesteps each model is evaluated. Some downstream trajectories contain 21 timesteps, others only 15.\n\n- Inconsistent or unspecified loss functions during fine-tuning\n\nThe paper does not state which loss functions (e.g., MSE, relative L1, or hybrid losses) were used for fine-tuning DPOT, POSEIDON, and SPUS. Since the evaluation metric is MSE, using different training losses can lead to misleading cross-model comparisons. If SPUS was trained directly with MSE while the baselines used relative losses (as in their original works), the reported results may unfairly advantage SPUS.\n\n- Absence of AR evaluation for POSEIDON in Appendix results\n\nFigure A.1 in the appendix reports “error growth over time” but only for direct evaluation of POSEIDON. Since AR rollouts are known to substantially reduce long-term error accumulation, this comparison provides little insight into the true relative performance. An additional plot showing AR-based POSEIDON results would be critical for fairness. Moreover, it would be valuable to understand how POSEIDON would perform if it were fine-tuned using the same autoregressive procedure as SPUS. The paper does not clarify whether such a training regime would yield comparable or potentially improved results for POSEIDON.\n\n- No analysis of scaling behavior\n\nA key feature of any foundation model is scaling performance with model size. The paper provides no experiment or discussion on how SPUS behaves when scaled up or down. Without such evidence, it is difficult to assess whether the proposed model exhibits the characteristic scaling trends expected of an FM."}, "questions": {"value": "- How stable is the autoregressive rollout of SPUS for very long horizons (e.g., >50 time steps)? Do the authors observe systematic error accumulation or qualitative drift, and how does it compare to transformer-based models?\n\n- Given that SPUS is trained only to predict discrete next steps, how would the model behave if queried at intermediate time steps ? Would this be even possible in current settings?\n\n- Since SPUS’s design is inherently temporal, how do the authors envision adapting it to stationary or steady-state PDEs where no temporal evolution exists?\n\n- Based on their training experience, do the authors expect SPUS performance to improve with increasing model size and (pretraining) dataset size?\n\nThe authors should also review the Weaknesses section for additional (implicit) questions and points raised."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wyVKps6AT", "forum": "rGf5DuMyOb", "replyto": "rGf5DuMyOb", "signatures": ["ICLR.cc/2026/Conference/Submission20836/Reviewer_9Ka3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20836/Reviewer_9Ka3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647918889, "cdate": 1761647918889, "tmdate": 1762936328691, "mdate": 1762936328691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SPUS, a lightweight and parameter-efficient foundation model for solving a broad range of PDE systems. SPUS adopts a residual U-Net architecture with only 36M parameters. It is pretrained autoregressively to mimic numerical solvers and fine-tuned on unseen PDE systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The design is novel and simple, which demonstrates that a lightweight U-Net can serve as an PDE foundation model, challenging the transformer-dominant paradigm.\n\n2. The model achieves competitive results with only one-third the parameters of existing FMs.\n\n3. The model successfully transfers from compressible Euler to incompressible Navier–Stokes and wave equations."}, "weaknesses": {"value": "1. The paper lacks theoretical or mechanistic analysis explaining why a residual U-Net architecture generalizes across diverse PDE families, beyond the observed empirical results.\n\n2. The contribution of individual design components (e.g., residual blocks, autoregressive training, adapters) remains unclear without ablation studies.\n\n3. The experiments are limited to 2D PDEs at fixed resolution. It is not evident whether SPUS can scale to 3D domains or higher spatial resolutions.\n\n4. Including comparisons with smaller non-transformer baselines (e.g., CNNs, FNOs, or unpretrained U-Nets) would strengthen the claim of parameter efficiency and architectural simplicity."}, "questions": {"value": "1. Could the authors provide more theoretical insights into why a residual U-Net architecture can generalize effectively across PDE families with distinct dynamics?\n\n2. Have the authors conducted any ablation analyses to isolate the effect of key design components on the final performance?\n\n3. How well does SPUS scale to 3D PDEs or higher spatial resolutions? Are there architectural or computational bottlenecks that would limit such extensions?\n\n4. Would the authors consider adding smaller CNN-based or operator-based baselines to better substantiate the claim of parameter efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cxn4pck8qO", "forum": "rGf5DuMyOb", "replyto": "rGf5DuMyOb", "signatures": ["ICLR.cc/2026/Conference/Submission20836/Reviewer_iuzp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20836/Reviewer_iuzp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675589867, "cdate": 1761675589867, "tmdate": 1762936327677, "mdate": 1762936327677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SPUS, the Small PDE U-Net Solver, which aims to explore the U-Net performance in the PDE foundation model. This paper includes extensive experiments to verify its questions, such as whether SPUS generalize to unseen initial conditions or equations or not, as well as the dataset scalability. In the input-one-frame-rollout-inference setting, SPUS surpassed DPOT and Poseidon."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tIt is good to see the investigation of U-Net performance in PDE solving.\n\n-\tThe authors provide detailed and well-organized experiments."}, "weaknesses": {"value": "Despite the above strengths, this paper contains some unfair experiments that may render the experimental results meaningless.\n\n### (1) Unfair comparison.\n\nAs the authors list in section 3, there are three different forecasting settings. Especially, the correct setting in DPOT is based on several past observations to predict the future. I do not think the setting of repeating ts_0 is a correct usage of DPOT.\n\nAlso, according to the statement in “even though DPOT was pretrained on operators of both compressible and incompressible NS equations”, I think the authors directly use the pre-trained models provided by DPOT and did not align the pre-training data with SPUS. This is also quite unfair to baselines, since both pre-training and evaluation data are from PDEgym. \n\n### (2) Limited novelty.\n\nAlthough I acknowledge that the authors attempt to rethink the previous architecture of the PDE foundation model, I cannot appreciate the novelty of this paper since this is just an experiment of pre-training a U-Net with PDE data.\n\nAll the analyses are just visualizations or quantitative results. I do not think this paper elaborates on why the U-Net works and why it works in a more parameter-efficient way than Transformers.\n\n### (3) About the scalability experiments.\n\nIt is common sense that Transformers usually present log-log scalability, which involves both parameter and dataset aspects and can be rigorously tested based on extensive scaling experiments. However, from Table 2, I cannot justify the scalability of SPUS. I think the authors should follow this paper [1] for further experiments. In my opinion, I do not believe that U-Net has good scalability.\n\n[1] Scaling Laws for Neural Language Models. Tech report OpenAI 2020.\n\n[2] Training Compute-Optimal Large Language Models. Tech report DeepMind 2022.\n\n### (4) Missing relative work.\n\nActually, DPOT and Poseidon are not state-of-the-art foundation models. Please compare with the following work [3].\n\n[3] Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers, ICML 2025.\n\n### (5) Limitation in irregular geometries.\n\nThe current design is limited to regular geometries. I think the authors should discuss this in the limitations section."}, "questions": {"value": "Do the authors adopt the same pre-training data for all the compared baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "awot9K0kME", "forum": "rGf5DuMyOb", "replyto": "rGf5DuMyOb", "signatures": ["ICLR.cc/2026/Conference/Submission20836/Reviewer_XF5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20836/Reviewer_XF5w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706917001, "cdate": 1761706917001, "tmdate": 1762936327073, "mdate": 1762936327073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}