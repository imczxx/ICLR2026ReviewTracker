{"id": "wKdg1DDOrW", "number": 224, "cdate": 1756731717493, "mdate": 1759898271126, "content": {"title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback", "abstract": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves? In this paper, we propose SAIL (Self-Amplified Iterative Learning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.", "tldr": "", "keywords": ["Diffusion models;RL"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19cfa95851b1674f2831d411a4b158592575700c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper deals with the limitations of existing approaches to preference alignment for diffusion models: (1) DiffusionDPO requires large-scale human-annotated preference data; (2) Auxiliary reward modeling approaches introduce more biases, are vulnerable to reward hacking, and struggle with distributional shifts from training data. To address these issues, the authors propose their methodology with a novel argument for diffusion model.\n\nThe authors argue that diffusion models' potential hasn't been fully exploited through supervised fine-tuning on human-labeled datasets alone. Instead, they propose iterative self-improvement by cold-starting with only a small seed set of human preference data. Based on the under-exploitation assumption, they propose SAIL, an iterative self-improvement closed-loop learning process and also introduce a ranked preference mixup strategy to prevent distribution collapse.\n\nExperimental results show that SAIL achieves comparable performance to state-of-the-art methods while using only 6% of the human preference data, highlighting the sample efficiency of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of fully exploiting the base model's potential is innovative. By eliciting this potential through the proposed iterative self-improvement method SAIL, the authors achieve comparable or better preference performance using only 6% of the human preference data.\n\n2. The consistent improvement across multiple iterations further demonstrates the effectiveness of the proposed iterative self-improvement paradigm."}, "weaknesses": {"value": "1. The tables lack multiple trials and confidence intervals, which are necessary to demonstrate the statistical significance of performance improvements and validate the effectiveness of the algorithm design in ablation studies.\n\n2. It would be valuable for the authors to include results over a larger range of iterations to illustrate the performance trajectory and reveal how the improvement trend evolves as the number of iterations increases."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sLgoex1vVM", "forum": "wKdg1DDOrW", "replyto": "wKdg1DDOrW", "signatures": ["ICLR.cc/2026/Conference/Submission224/Reviewer_neyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission224/Reviewer_neyU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761089659190, "cdate": 1761089659190, "tmdate": 1762915474402, "mdate": 1762915474402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for aligning diffusion models with human preference using limited preference data. It utilizes the implicit reward function from DPO to create online preference data. To address overfitting, it mixes online preference data with the initial preference data. Experiments show that the proposed method can achieve comparable or better metrics than some previous work, using less data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Using self-rewarding to rank online data is relatively new in text-to-image generation. \n- The proposed method works well with limited data."}, "weaknesses": {"value": "- Implicit reward is adopted from previous work in LLM.\n- The mixup of online and initial preference data is straightforward.\n- Some generated images seem to have a color saturation problem.\n- There are a lot of problems in writing, e.g.\n  - Line 144-145: grammar error\n  - Line 145: some -> Some\n  - Line 344: Pic-a-Pic -> Pick-a-Pic\n  - Line 357: use -> uses\n  - Line 362: bringing challenge -> brings challenges\n  - Line 373: fix reference\n  - Line 374-375: Thus, …, so … \n  - Line 454: reveals -> reveal\n  - Line 457: suffers -> suffers from"}, "questions": {"value": "- What are the drawbacks when performing more iterations of SAIL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2dle2pQE8Z", "forum": "wKdg1DDOrW", "replyto": "wKdg1DDOrW", "signatures": ["ICLR.cc/2026/Conference/Submission224/Reviewer_SeQh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission224/Reviewer_SeQh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798343718, "cdate": 1761798343718, "tmdate": 1762915474265, "mdate": 1762915474265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SAIL, a novel framework for aligning diffusion models with human preferences without reward models or the use of large-scale human-annotated data. The idea is to bootstrap alignment through self-amplified iterative learning using a minimal set of preference data. Experiments demonstrate strong performance while using only a small part of the preference data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a self-improving framework to align diffusion models with human preferences without large-scale annotated datasets, which is novel. \n2. Thorough empirical evaluation results show that the proposed method is effective, outperforming existing alignment methods using only 6% of the annotations."}, "weaknesses": {"value": "1. Lack of Theoretical Guarantees. While the reward formulation (Eq. 8–9) is mathematically correct in the DiffusionDPO framework, the paper does not provide theoretical analysis of what distribution SAIL converges to. What is the target distribution of this method? Is it the same as DPO? If so, what explains the performance with fewer annotations? It's unclear where the observed gains come from. Whether and why the self-reward metric aligns with true human preference distributions?  A thorough analysis would benefit the paper.\n\n2. Even though the method needs fewer human annotations, it introduces additional cost to generate samples in the training loop, which is not efficient. Could the author provide some discussion on the trade-off between annotation efficiency and training efficiency? \n\n3. The central reward estimation strategy, which computes preference scores using differences in squared denoising errors between current and reference models (Eq. 6–9), closely follows the DiffusionDPO formulation and thus is not novel."}, "questions": {"value": "see Weaknesses 1. 2 \n\n1. How sensitive is the proposed method to the choice of the initial seed dataset? What happens if the seed preferences are noisy or biased?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wr8YgWTbiq", "forum": "wKdg1DDOrW", "replyto": "wKdg1DDOrW", "signatures": ["ICLR.cc/2026/Conference/Submission224/Reviewer_sMVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission224/Reviewer_sMVs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153081595, "cdate": 1762153081595, "tmdate": 1762915474055, "mdate": 1762915474055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}