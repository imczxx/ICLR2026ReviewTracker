{"id": "mzAchylAtf", "number": 2160, "cdate": 1757004169985, "mdate": 1759898165858, "content": {"title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer", "abstract": "We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720×1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation:  (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1\\% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x} speedup). In summary, SANA-Video enables low-cost, high-quality video generation. Code and model will be publicly released.", "tldr": "", "keywords": ["Video Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5e9ace86b5e4f3ec8a3e186d0e927c332b7e653.pdf", "supplementary_material": "/attachment/2b36e23eae363bd8d0c8c385782602ccf6b65563.pdf"}, "replies": [{"content": {"summary": {"value": "This paper targets efficient, high-resolution, and minute-long video generation and introduces SANA-Video, a 2 B-parameter diffusion model that replaces quadratic self-attention with linear attention and a constant-memory KV cache. Extensive experiments on benchmarks with 480p and 720p  resolution show the great efficiency and performance of SANA-Video. The manuscript is clearly structured, well illustrated."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a timely and important problem—computational bottlenecks in long, high-resolution video generation—and offers a practical solution deployable on edge GPUs, such as RTX 5090. The authors propose a novel block linear attention with constant-memory KV cache, elegantly combining linear attention and autoregressive training to unlock arbitrarily long videos without memory explosion. Experiments are good, including latency, memory, and the VBench benchmark; the 12-day 64-GPU training cost is convincingly shown to be <1 % of MovieGen while outperforming larger models. The writing is accessible, with intuitive figures, algorithm pseudocode, and some ablation studies."}, "weaknesses": {"value": "I believe the main weakness of this paper lies in the lack of in-depth ablation studies and analysis regarding the proposed technical innovations. Although Tables 2, 4, and 5 present quantitative comparisons between SANA-Video and existing SOTA models, the paper severely lacks detailed ablations that isolate and validate the contributions of the proposed methods. While Figure 3 and Figure 6 provide a simple comparison of loss convergence, this analysis is limited in two ways. First, they only reflect the change of training loss, which do not adequately capture model performance on validation or test data. Second, loss convergence is not necessarily correlated with the final generation quality of the model to my experience. Therefore, I recommend that the authors conduct comprehensive ablation experiments to validate their claimed innovations. Without such evidence, the paper appears to mainly extend the SANA T2I model to video training. Although this extension may indeed yield a faster and better-performing model for both academia and industry, it lacks sufficiently validated technical insights that could meaningfully inform future research. To better align with ICLR’s emphasis on technical novelty and rigorous validation, I encourage the authors to enrich the paper with more thorough experimental analyses focusing on their proposed innovations. \n\nIn addition, the authors only provide a description of the data filtering strategy but do not specify the detailed information about the datasets used at each stage, including their sources, size, quality, and distributions. This omission raises concerns about the reproducibility of the work. For example, if one is provided with the SANA-T2I model and the exact data preprocessing pipeline, it remains unclear how he could reproduce the reported results in Table 4 using 64 H100 GPUs within 12 days."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xEwWAJ9sh9", "forum": "mzAchylAtf", "replyto": "mzAchylAtf", "signatures": ["ICLR.cc/2026/Conference/Submission2160/Reviewer_M6uh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2160/Reviewer_M6uh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761046565626, "cdate": 1761046565626, "tmdate": 1762916059362, "mdate": 1762916059362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SANA-Video, an efficient and lightweight video diffusion model capable of generating high-quality videos up to 720p resolution and minute-level durations on consumer-grade GPUs. Its core designs include:\nLinear DiT Architecture,\nBlock Linear Attention with Constant-Memory KV Cache, \nHighly Compressive Video VAE (DCAE-V).\n\nExperiments demonstrate that SANA-Video achieves 720p video generation speeds 53× faster than Wan2.1-14B and 3.2× faster than Wan2.2-5B, while delivering comparable or superior quality to mainstream compact models (especially in semantic alignment) on VBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Engineering Innovation: The combination of Linear DiT and Block Causal KV Cache is highly practical, effectively solving memory and speed bottlenecks in long-video generation.\n\n2. Low Training Cost: Trained in only 12 days on H100 GPUs, significantly reducing costs compared to similar works (e.g., OpenSora, MovieGen), thereby lowering research entry barriers.\n\n3. Outstanding Inference Efficiency: Generates 720p videos in just 36 seconds on H100 GPUs and 29 seconds on RTX 5090 GPUs (with NVFP4 quantization), showing deployment feasibility.\n\n4. Comprehensive Experiments: Covers a wide range of tasks, including Text-to-Video (T2V), Image-to-Video (I2V), long video generation, ablation studies, VAE comparisons, and quantized deployment analysis."}, "weaknesses": {"value": "1. Generation Quality Needs Improvement: Quality scores are not high enough.\n\n2. Lack of Detailed VAE Analysis: Details on training and inference for the VAE are missing, along with its performance on high-frequency details—e.g., whether grid-like artifacts are observed.\n\n3. No Open-Source Code or Model: The claimed results and efficiency are hard to verify without access to code or model weights."}, "questions": {"value": "1. The latent channel number of DCAE-V is 32, lower than Wan2.2-VAE's 48. Have you tried training DCAE-V with higher channel number?\n\n2. Training Techniques for DCAE-V: How does DCAE-V achieve such strong reconstruction performance? Are there any special techniques used during training (e.g., dataset or algorithm tweaks)?\n\n3. 480p Inference: When generating 480p resolution videos, do you use DCAE-V or Wan VAE?\n\n4. Concerns on High Compression VAE: Does the high compression rate of DCAE-V impact the generation of fine details?\n\n5. Quantized Deployment: Does color distortion or motion stuttering occur after NVFP4 quantization?\n\n6. Improving Quality: Is it possible to further reduce the quality gap with Wan2.1-14B by scaling up Linear DiT for higher resolutions?\n\n7. Long-Video Generation Performance: How does the model perform on long-duration video generation tasks?\n\n8. The method achieves high semantic alignment scores on the VBench dataset. What could be the reasons for this? Is there a potential trade-off between semantic alignment and generation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n1qjJ3ilGQ", "forum": "mzAchylAtf", "replyto": "mzAchylAtf", "signatures": ["ICLR.cc/2026/Conference/Submission2160/Reviewer_CMDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2160/Reviewer_CMDt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653994572, "cdate": 1761653994572, "tmdate": 1762916058589, "mdate": 1762916058589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SANA-Video is a small diffusion model that swaps standard attention for linear attention and adds a constant-memory KV cache with block-wise autoregressive inference. The combo lets it generate minute-long, up-to-720x1280 videos efficiently, with low training cost and fast inference. \nCore contributions: 1. Linear DiT for video: extends SANA's linear-attention DiT to video. 2. Block Linear Attention with a constant-memory KV cache: Reformulates causal linear attention so long videos can be generated block-wise with a fixed KV state and per-token cost. 3. Low cost, competitive results, and practical deployment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Speed and efficiency at scale.\nIt replaces vanilla self-attention with linear attention (as in SANA-Image), reducing the quadratic complexity with respect to tokens to (near) linear. This significantly cuts compute for large spatiotemporal token counts and yields faster video generation without heavily inflating parameters. In practice, that means higher throughput and shorter time-to-first-frame on the same hardware.\n2. Long-video generation with bounded memory.\nDuring long-horizon synthesis, a standard KV cache grows with sequence length and quickly exhausts VRAM. This paper proposes a constant-memory KV cache together with block-wise linear attention, enabling block-autoregressive generation that keeps the memory footprint essentially flat while still carrying forward global context. The result is more stable, minute-scale generation on a single GPU.\n3. Competitive quality at lower cost.\nThe training recipe is designed for efficiency (e.g., linear attention, staged curricula), and the inference stack is optimized for speed. Reported latencies are up to 16× faster than comparable small-parameter baselines, while maintaining competitive visual quality and motion consistency. For deployment, this translates into lower operational cost and better user-perceived responsiveness.\n4. Strong text–video alignment at high resolution.\nThe model targets high-resolution outputs (e.g., 720p) and emphasizes faithful grounding to the prompt across long clips. In addition to object/style fidelity, it improves temporal adherence—keeping actions, scenes, and entities consistent over time—so the final videos look coherent rather than drifting from the original description."}, "weaknesses": {"value": "1. On minute-scale clips, the model often locks into one scenario with minimal variation in composition, lighting, or action. This can be less helpful in real-world use cases (e.g., advertising, storytelling, education), where users expect diversity, pacing changes, and clear scene evolution.\n2. Potential but unverified claim. Intuitively, this approach should help produce higher-resolution videos—even if clip duration must be shortened to keep compute in check. However, the experimental section does not report targeted evaluations or head-to-head comparisons at higher resolutions, so this benefit remains unsubstantiated. One possibility is that linear attention imposes practical limits (e.g., on spatial detail retention, kernel choice, or memory/computation trade-offs) that make rigorous high-res testing less favorable or harder to standardize.\n3. Although the results are impressive, the contribution of this work on the technical side is somewhat limited. Their two main components have been already explored in the literature, linear DiT in [1] and Constant-Sized KV Caches in [2]. So combining them is not really a significant novel idea.\n[1] Liu et. al. LinFusion: 1 GPU, 1 Minute, 16K Image. 2024\n[2] Ravi Ghadia et. al. Dialogue Without Limits: Constant-Sized KV Caches for Extended Response in LLMs. ICML 2025"}, "questions": {"value": "Please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JLfEPZwu33", "forum": "mzAchylAtf", "replyto": "mzAchylAtf", "signatures": ["ICLR.cc/2026/Conference/Submission2160/Reviewer_NYaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2160/Reviewer_NYaf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833189979, "cdate": 1761833189979, "tmdate": 1762916058409, "mdate": 1762916058409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SANA-Video, an extension of SANA for video generation. The main idea is two-fold: it adopts a Linear Diffusion Transformer similar to SANA, but extends it with Block Linear Attention and a constant-memory KV cache, enabling block-wise autoregressive generation for synthesizing long videos. The authors also propose DCAE-V, a video autoencoder that achieves a much higher spatial compression ratio (32×) than existing VAEs (e.g., Wan-VAE with 8×). In addition, they introduce several techniques to improve training efficiency and effectiveness — such as autoregressive block training with self-forcing, which combines the benefits of self-forcing and a global KV cache. As a result, the model can handle much longer temporal contexts (e.g. 1 minute) while maintaining high generation quality. They also propose a quantization techniques for better deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is generally well-written and easy to follow.\n- Sampling cost has been a really critical issue in video diffusion models, and thus, I think this paper is very well motivated.\n- Compared with existing popular video diffusion models (e.g., WAN), the efficiency improvement is decent while generating similar-quality videos."}, "weaknesses": {"value": "While I appreciate the strong improvements over existing video diffusion models, I have the following concerns:\n- Technical novelty: The main contributions seem to be extensions or combinations of existing techniques. For example, linear attention was introduced by SANA for text-to-image generation, and block-wise attention has already been employed in several recent long video generation works. I wouldn’t say this warrants rejection, but it also makes it difficult to strongly recommend the paper.\n- Scaling behavior: Given that the training cost of SANA-Video is not prohibitively high, I wonder if the authors could provide scaling-law analysis for the proposed architecture. For instance, if we train a larger model (say, 5B parameters) using the same data and setup, can we expect further improvements over the 2B variant? While efficiency is a strong point, the model quantitatively underperforms larger video models in most metrics. I’m curious whether this is due to the smaller model size or if there are architectural bottlenecks that limit performance even at larger scales.\n- Minor: Figure 1c is great, but it would be even more informative to include a version with full attention blocks for comparison."}, "questions": {"value": "See my weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ogRfOBpnHH", "forum": "mzAchylAtf", "replyto": "mzAchylAtf", "signatures": ["ICLR.cc/2026/Conference/Submission2160/Reviewer_Ai4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2160/Reviewer_Ai4Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857941370, "cdate": 1761857941370, "tmdate": 1762916058270, "mdate": 1762916058270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}