{"id": "LHwhjl2J6T", "number": 3776, "cdate": 1757517570127, "mdate": 1759898070270, "content": {"title": "TadABench-1M: A Large-Scale Wet-Lab Protein Benchmark For Rigorous OOD Evaluation", "abstract": "Existing benchmarks for biological language models (BLMs) inadequately capture the challenges of real-world applications, often lacking realistic out-of-distribution (OOD) scenarios, evolutionary depth, and consistency in measurement. To address this, we introduce TadABench-1M, a new benchmark based on a wet-lab dataset of over one million variants of the therapeutically relevant TadA enzyme, purpose-built to embody these three essential attributes. Generated across 31 rounds of wet-lab evolution, it offers unparalleled evolutionary depth and naturally presents a stringent OOD challenge. To ensure measurement consistency across this extensive campaign, we developed Seq2Graph, a scalable graph-based algorithm that systematically unifies multi-batch experimental data. Our high-fidelity benchmark highlights a critical finding: while state-of-the-art BLMs excel on a standard random split of the data (Spearman’s ρ ≈ 0.8), they fail dramatically on a realistic temporal prediction task (ρ ≈ 0.1). This stark performance gap validates the importance of our benchmark’s design principles and suggests that evolutionary depth is critical for building models with realistic utility.", "tldr": "This paper introduces TadABench-1M, a benchmark for evaluating out-of-distribution (OOD) generalization in Biological Language Models (BLMs) using protein engineering data.", "keywords": ["Benchmark", "AI for Science", "Biological Language Model", "Protein Engineering", "Gene Editing", "Protein Fitness Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3124dcf35229b2d56d3efedd2d966a4dbca730ed.pdf", "supplementary_material": "/attachment/e9ece885e720bc88522359dcf544413123288307.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TadABench-1M, a benchmark based on a wet-lab dataset of over one million variants of the therapeutically relevant TadA enzyme, aiming to focus on realistic out-of-distribution cases, evolutionary depth, and consistency in measurement. This paper also introduces Seq2Graph, a scalable graph-based algorithm that systematically unifies multi-batch experimental data. A key finding in this work is that while state-of-the-art biological language models excel on a standard random split of the data, they fail dramatically on a realistic temporal prediction task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The temporal split of the dataset in this benchmark establishes a realistic evaluation setting that mimics a real-world engineering campaign, and also reveals a significant generalization gap.\n- This work demonstrates that sequence diversity and evolutionary depth are more critical for OOD generalization than raw data volume"}, "weaknesses": {"value": "- The benchmark is designed around TadA enzyme and it’s evolution. This setting may restrict the generalization to other protein families. Also, it’s not clear whether the findings in this benchmark can be transferred to other proteins.\n- This paper mainly focuses on biological language models, such as ESM, Evo, etc. Is it possible that the findings in this work only apply to language-based methods, it would be interesting to see structure-aware methods also included in the benchmark."}, "questions": {"value": "Please refer to weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FYA9gsOyXZ", "forum": "LHwhjl2J6T", "replyto": "LHwhjl2J6T", "signatures": ["ICLR.cc/2026/Conference/Submission3776/Reviewer_ZqXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3776/Reviewer_ZqXN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541960873, "cdate": 1761541960873, "tmdate": 1762916992868, "mdate": 1762916992868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper has three main contributions:\n1) An experimental dataset on TadA protein variants\n2) Seq2Graph an algorithm to unify the directed evolution measurements \n3) Computational validations that test the generative capabilities of existing models on this dataset"}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is clearly written.\n\n2) Details of the Seq2Graph algorithm are discussed.\n\n3) Developing a benchmark for the OOD problem in protein engineering is timely and valuable for the community.\n\n4) Experimental efforts are substantial."}, "weaknesses": {"value": "1) Besides the experimentally collected data, the only algorithmic contribution is the development of the Seq2Graph; however, there is no evaluation of the algorithm itself. The validation section focuses on using the dataset as a benchmark, but the algorithm itself is not tested or evaluated. As I describe in the section below, some questionable choices are made in its development.\n\n2) The authors highlight that “bridging the OOD gap requires exploring functionally diverse regions of the sequence space, rather than simply increasing density” (line 431) and suggest that “evolutionary depth is critical for building models with realistic utility” (line 24). However, it is unclear how TadABench-1M contributes to advancing the development of said models. The paper would be strengthened by developing one of these models on the dataset and demonstrating its utility.\n\n3) The dataset is limited to one protein, which is TadA. While even a dataset on a single protein remains valuable, it is not clear if all the claims generalize to a large set of proteins, especially when compared to Proteingym and Proteinbench."}, "questions": {"value": "1) There are several choices made in the development of Seq2Graph that are not necessarily justified/empirically validated: \n\na) Why does Seq2Graph create a directed edge between two sequences only? Why not identify the k nearest neighbors? In the example of Figure 3, if the sequence D has a \\Delta rad = 975, it would seem unreasonable not to connect A and D. Picking the top-1 neighbor seems to be an arbitrary choice, not explained in the paper, as picking the top-k would not violate either the reliability or sparsity as discussed in the paper.\n\nb) The process of removing cycles and creating DAGs is reasonable from the perspective of avoiding conflicts; however, it will discard valuable information by removing edges. It seems like averaging might be another good choice.\n\nc) Most importantly, given that many different algorithm design choices for Seq2Graph could have been made, what is a fair evaluation procedure and metric for the algorithm alone? The paper fails to assess Seq2Graph as a standalone algorithm. What would happen if we made a wrong choice in Seq2Graph? What would go wrong? Is there a baseline processing algorithm to compare Seq2Graph with?\n\n2) In Figure 5, I struggle to see how diversity and round-based strategies are better than density-based strategies. For instance, the density Spearman correlation in Evo2-7B and Evo2-40B is almost always higher than the diversity and round correlations, making the claim that smaller diversity datasets perform better than large, dense datasets appear incorrect.\n\n3) It is unclear whether the results from this benchmark are generalizable across different proteins/genomes or whether this is just a phenomenon found in the TadA protein. This makes it difficult to assess the generalizability of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1jtTjJkwUv", "forum": "LHwhjl2J6T", "replyto": "LHwhjl2J6T", "signatures": ["ICLR.cc/2026/Conference/Submission3776/Reviewer_K7bE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3776/Reviewer_K7bE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744711227, "cdate": 1761744711227, "tmdate": 1762916990126, "mdate": 1762916990126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TadABench-1M, a benchmark built on 31 rounds of wet-lab data of over one million TadA enzyme variants, addressing existing BLM benchmark flaws with OOD scenarios, evolutionary depth, and the Seq2Graph algorithm for consistent fitness labels. It finds SOTA BLMs perform well on random splits but collapse on temporal splits, proving evolutionary depth is a key for OOD generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This study develops a realistic, well-validated benchmark for BLM evaluation. It builds over one million TadA enzyme variants from 31 rounds of wet-lab evolution, naturally incorporating OOD challenges (via temporal splits) and evolutionary depth (up to 25 mutations per variant). \n- The experiments are comprehensive, and the results are convincing. It contrasts random (i.i.d.) and temporal (real-world) data splits, showing SOTA BLMs perform well on random splits but fail on temporal splits."}, "weaknesses": {"value": "- The paper compares several language models and tuning strategies. It would be better to compare these baselines with diffusion language models, such as EvoDiff, DPLM, etc.\n- The results are convincing, but not very surpurising (that protein language models memorize more than generalize). These language models have similar behaviors. I think it would be better to present something different, i.e., which component may help models generalize better, and how does this benchmark helps guide building new models."}, "questions": {"value": "- How long does it take to build such a benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SqTtk6dLOh", "forum": "LHwhjl2J6T", "replyto": "LHwhjl2J6T", "signatures": ["ICLR.cc/2026/Conference/Submission3776/Reviewer_rBZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3776/Reviewer_rBZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081294763, "cdate": 1762081294763, "tmdate": 1762916988761, "mdate": 1762916988761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}