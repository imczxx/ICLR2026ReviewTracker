{"id": "8HX1Orwbit", "number": 13719, "cdate": 1758221547588, "mdate": 1759897417513, "content": {"title": "ML-Tool-Bench: Tool-Augmented Planning for ML Tasks", "abstract": "The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance.  Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.", "tldr": "The paper proposes ML-Tool-Bench to evaluate planning capabilities of tool-augmented LLMs and proposes two simple approaches to boost planning performance", "keywords": ["Large Language Models", "Tool Use", "Planning", "Monte Carlo Tree Search"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd3924dad1dbe00ac5e4fbd9c2725ef0db3218ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the tool-augmented planning capabilities of autonomous machine learning (ML) agents. Addressing the limitations of existing solutions, it proposes a benchmark framework named ML-Tool-Bench and designs improved algorithms to enhance the planning effectiveness for complex ML tasks. Finally, through experiments, the value of the framework and algorithms is verified, laying a foundation for the development of tool-augmented ML agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work effectively addresses the critical gap in existing tool-use benchmarks that overlook long-horizon planning capabilities essential for complex ML workflows, as ML-Tool-Bench, equipped with 61 task-specific tools and 15 Kaggle tabular challenges.\n2. The proposed in-memory named object management (Scratchpad) represents a key technical innovation, resolving the long-standing issue of intermediate result reuse in traditional benchmarks by allowing agents to name, store, and reference complex artifacts via key-value pairs, thus preventing global data corruption from single erroneous operations and enhancing the flexibility of multi-step trajectory execution.\n3. The study demonstrates strong problem-driven innovation by first diagnosing the inherent limitations of mainstream planning methods  and then designing targeted improvements—MCTS-Shaped with staged shaped rewards and textual feedback, and Hierarchical MCTS with subtask decomposition and tool masking—ensuring the proposed solutions directly tackle core pain points.\n4. The experimental covers two representative LLMs (GPT-4o and GPT-4.1-mini) to verify generalizability across model capabilities, compares five algorithms (3 baselines + 2 proposed methods) using dual metrics of trajectory consistency and Kaggle leaderboard percentile, and adopts dataset sampling and internal test splits to mimic real-world constraints, with quantitative results."}, "weaknesses": {"value": "1. I’m not entirely clear on the purpose of designing a new benchmark framework. Existing tool-planning frameworks already address many realistic and problem-specific scenarios, along with supporting tools and prior-knowledge categorizations. For instance, ToolBench already contains extensive relevant content for such purposes. Is the main goal here to adapt better to ML-specific contexts? Fundamentally, I believe datasets should still be more aligned with real-world tasks.\n\n2. The two improved algorithms seem mainly to modify the MCTS process flow. Compared with heuristic methods such as ToolChain*, what specific advantages do these revised versions of MCTS provide?\n\n3. If one wants to further extend the toolset or include more tools, can the dataset proposed in this paper be expanded accordingly? At present, doesn’t the variety of available tools seem somewhat limited?\n\n4. In this study, both the “structured textual feedback” in MCTS-Shaped and the “sub-task search” in Hierarchical MCTS assume that feedback is instantaneous and computational resources are sufficient. However, two key constraints present in real-world industrial settings are not considered:\n\n(A)When tool calls involve large-scale data processing, results may be delayed by minutes. Such latency could desynchronize MCTS trajectory updates and reward calculations, affecting planning efficiency, yet this scenario is not simulated in the experiments.\n\n(B)MCTS-based algorithms require multiple searches and simulations, while LLM calls (especially GPT-4o) are expensive. The experiments do not report a “call-count vs. performance” trade-off (e.g., whether Hierarchical MCTS’s gain over ReAct justifies its extra computation cost) nor explore cost-optimization strategies (such as pruning ineffective search paths)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2lXmny6DMX", "forum": "8HX1Orwbit", "replyto": "8HX1Orwbit", "signatures": ["ICLR.cc/2026/Conference/Submission13719/Reviewer_tCJh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13719/Reviewer_tCJh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484856944, "cdate": 1761484856944, "tmdate": 1762924264379, "mdate": 1762924264379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ML-Tool-Bench, a benchmark and framework designed to evaluate large language models on ML-related tool-augmented planning and execution tasks. The authors argue that prior benchmarks (e.g., MLE-Bench, MLAgentBench) mainly test end-to-end ML problem solving but overlook the crucial intermediate skill of decomposing ML tasks into tool calls and reasoning sequences. ML-Tool-Bench consists of 1,200 tool-centric problem instances covering data preprocessing, model training, and evaluation stages, each annotated with ground-truth tool APIs and reasoning trajectories. The paper further introduces a lightweight simulator that emulates realistic execution environments and proposes a hierarchical evaluation metric combining plan accuracy, execution success, and tool efficiency. Experiments across a range of models reveal consistent weaknesses in long-horizon tool coordination and parameter grounding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an underexplored aspect of LLM-for-ML research—tool reasoning—bridging the gap between abstract workflow generation and real tool invocation.\n- The dataset is carefully constructed, featuring verified tool APIs and multi-step plans that make evaluation more interpretable and diagnostic than previous black-box setups."}, "weaknesses": {"value": "- While the benchmark is well-motivated, its methodological novelty remains limited—it mainly reorganizes existing task formulations and evaluation paradigms into a cleaner schema, without introducing new modeling or learning components.\n- As a benchmark paper, the overall scale feels somewhat limited: both the dataset size and the number of baselines are modest, and the work would be more convincing if it compared tool-based reasoning with direct code-generation approaches (e.g., AutoML or ML-agent frameworks) on the same ML tasks.\n- Although the benchmark emphasizes multi-step reasoning, the actual task complexity is relatively constrained—most tasks involve fewer than five tools, with largely deterministic argument structures—restricting generalization to more realistic ML scenarios.\n- The paper would also benefit from richer qualitative case studies to illustrate representative reasoning trajectories and better contextualize quantitative results."}, "questions": {"value": "See #Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4VRKQmn8w", "forum": "8HX1Orwbit", "replyto": "8HX1Orwbit", "signatures": ["ICLR.cc/2026/Conference/Submission13719/Reviewer_f1i6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13719/Reviewer_f1i6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497010695, "cdate": 1761497010695, "tmdate": 1762924263695, "mdate": 1762924263695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ML-Tool-Bench, a tool-augmented benchmark for tabular Kaggle-style ML tasks, offering a curated suite of 61 tools and 15 competitions together with an in-memory scratchpad mechanism to persist and reuse intermediate artifacts. The authors evaluate several planning strategies (ReAct, LATS, multiple MCTS variants) and propose two improvements: MCTS-Shaped and Hierarchical MCTS. Experiments on GPT-4o and GPT-4.1-mini show that MCTS-Shaped and Hierarchical MCTS improve trajectory validity and leaderboard percentiles compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Extending tool-use evaluation to end-to-end ML workflows with long-horizon planning and artifact management.\n2. The proposed approach includes multiple complementary features.\n3. Results show consistent improvement across different models and metrics."}, "weaknesses": {"value": "1. Manual subtask or tool assignment could bias results. Hierarchical MCTS depends on hand-assigning tools to subtasks, which may imply prior knowledge and limit the generality and automation of the method.\n\n2. The scope of evaluation limited to 15 tabular data challenges. This limits the benchmark's breadth in evaluating a wider range of ML tasks. Furthermore, the benchmark’s 61 tools do not adequately demonstrate scalability to large action spaces.\n\n3. Novelty is insufficient. The method relies on  existing techniques (e.g., task decomposing) without innovations in core mechanisms.\n\n4. The paper emphasizes performance metrics such as success rate and leaderboard percentile but offers little analysis of failure cases or decision dynamics. Without visualizing search trajectories or reward evolution, it is difficult to discern whether performance gains stem primarily from shaped rewards or from hierarchical decomposition."}, "questions": {"value": "1. The description of the Hierarchical MCTS method lacks detail, particularly regarding how it decomposes complex tasks into ordered sequences of subtasks.\n\n2. Computational cost of MCTS and Hierarchical MCTS at larger tool sets or in production settings (latency, budget trade-offs) is not deeply analyzed. How much is the computational overhead and how does it compare to that of the baselines?\n\n3. How does this method apply to non-tabular ML tasks (e.g., Computer Vision, NLP), and what are the scaling strategies and bottlenecks for Hierarchical MCTS when tools far exceed 61?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MLlSM1mLEb", "forum": "8HX1Orwbit", "replyto": "8HX1Orwbit", "signatures": ["ICLR.cc/2026/Conference/Submission13719/Reviewer_N6k3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13719/Reviewer_N6k3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877337398, "cdate": 1761877337398, "tmdate": 1762924263133, "mdate": 1762924263133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ML-Tool-Bench, a tool-augmented benchmark for end-to-end tabular ML workflows drawn from 15 Kaggle challenges. Agents operate with a named-object scratchpad and a curated set of 61 tools that cover loading, cleaning, feature engineering, modeling, and prediction. The study compares multiple planning frameworks, including ReAct, LATS, MCTS variants with shaped rewards, and a hierarchical MCTS that performs subtask decomposition with tool masking. Results are reported as leaderboard percentiles and “consistency” of valid trajectories per task and model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The task is very relevant. Real ML engineering requires multi-step planning, reasoning, and execution, and the benchmark targets this long-horizon setting with artifact reuse. The scratchpad design directly addresses state corruption issues in multi-step pipelines.  \n\n2. The setup of representing ML steps as tools reduces emphasis on raw coding and focuses evaluation on the structure of the ML pipeline. The toolset is clearly scoped across the main stages of tabular ML.  \n\n3. The experiments cover several agent frameworks and explore targeted reward signals. Modeling the pipeline with decomposition and shaped rewards is shown to improve performance on many tasks."}, "weaknesses": {"value": "1. Benchmark size - With only 15 tasks the variance is high across problems, which makes aggregate comparisons unstable. The tables show very large swings in percentile across tasks, including cases with near 0 and near 100 for the same methods on different tasks. I would suggest expanding to atleast 30–50 tasks per task family to produce a more reliable signal, and claims should be scoped to tabular ML for now. \n\n2. Data interaction clarity - Many tools target code-level operations like loading dataframes, encoding, and fitting models. For the models to truly reason about the right pipeline or algorithm for the task, it would be important to review the data or run additional analysis. It is unclear if the model can do that and if so, how is it instantiated especially considering model context lengths. \n\n3. Comparability to public leaderboards - Training is capped at 10k examples per task for compute constraints, but this makes the results not comparable with the public leaderboard. The paper should add cross-validation and a small public-LB calibration to validate if this comparison holds. \n\n4. Ablations - Hierarchical MCTS bundles subtasking, tool masking, and shaped rewards. Isolating each component with a small factorial ablation would clarify where the gains come from."}, "questions": {"value": "Please include the exact prompts for each algorithmic setup and the tool-calling templates in the appendix to improve reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HjCKVrQiCt", "forum": "8HX1Orwbit", "replyto": "8HX1Orwbit", "signatures": ["ICLR.cc/2026/Conference/Submission13719/Reviewer_SNrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13719/Reviewer_SNrZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974657497, "cdate": 1761974657497, "tmdate": 1762924262759, "mdate": 1762924262759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}