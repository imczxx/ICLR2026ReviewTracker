{"id": "hm2EcwFm2Q", "number": 17513, "cdate": 1758277023178, "mdate": 1759897170040, "content": {"title": "Contrastive Representation Regularization for Vision-Language-Action Models", "abstract": "Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs).\nHowever, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. \nTo address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals.\nIn particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision.\nComplementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline.\nOur empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models;\nit pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing,\nand boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.", "tldr": "", "keywords": ["Vision-language-action models", "Robot manipulation", "Contrastive learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b52e8911cae895152b1ea71e5d331521af25647e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Robot State-aware Contrastive Loss, a contrastive regularization method designed to align VLM representations with robotic proprioceptive states in VLA models. RS-CL aims to enhance the robot’s \"control-relevant representation\" by incorporating relative distances between robot states as soft supervision, complementing the standard action prediction objective. The authors evaluate their method on simulated benchmarks (RoboCasa-Kitchen, LIBERO) and real-robot experiments, reporting consistent improvements over baselines\t1.\tThe paper tackles an important issue in VLA models—bridging the gap between visual-semantic representations and robot control signals—by introducing a conceptually simple yet effective contrastive regularization. in manipulation success rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe experiments are extensive, covering both simulation and real-world robotic tasks, demonstrating consistent performance gains and strong empirical support for the approach.\n2. The proposed RS-CL method integrates smoothly into existing VLA training pipelines, requiring minimal additional computation and no curated data."}, "weaknesses": {"value": "1.\tWriting and clarity: The paper frequently introduces new terms (e.g., VLM representations, robot control-relevant structure, robotic signals) without sufficient explanation. These concepts are not standard in robotics literature, which makes it difficult to precisely understand the intended meaning or technical novelty. The authors should define these terms more clearly and consistently.\n2.\tThe motivation for using contrastive learning remains somewhat unclear. There are prior works that explicitly incorporate object-centric or proprioception-based signals during VLA training (e.g., [1]), yet the paper does not convincingly explain why contrastive learning is particularly suited for capturing “control-relevant structure.”\n3.\tIt is also unclear whether incorporating proprioceptive information directly into the input and output of the VLA model would yield comparable results without contrastive loss. The paper should discuss why reconstructing or predicting proprioceptive states is less effective than using RS-CL.\n4.\tThe explanation of how RS-CL differs from conventional contrastive losses (such as InfoNCE) is vague. While the authors claim it is distinct, the loss formulation still appears to follow InfoNCE, differing only in weighting by state distances. Clarification on the novelty at the loss design level is necessary.\n5.\tFigure 2(b) is difficult to interpret. The visualizations do not make it obvious how “VLM embeddings are dominated by visual cues.” More quantitative or clearer visual analysis would help substantiate this claim.\n6.\tIt is unclear whether the visualized embeddings come from the frozen VLM or the fine-tuned VLA model. This distinction is critical for understanding how RS-CL affects representation learning.\n7.\tWhile the empirical results are strong, the paper could better articulate why aligning to proprioceptive states leads to improved manipulation success. The connection between representation alignment and downstream control performance could be analyzed more deeply (e.g., through probing tasks or ablations).\n\n\n[1] Yang et al., Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization, RSS 2025."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "95CsJ5Bh3d", "forum": "hm2EcwFm2Q", "replyto": "hm2EcwFm2Q", "signatures": ["ICLR.cc/2026/Conference/Submission17513/Reviewer_csLb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17513/Reviewer_csLb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623524247, "cdate": 1761623524247, "tmdate": 1762927394910, "mdate": 1762927394910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Contrastive Representation Optimization (CRO), a training paradigm for improving the alignment between visual and linguistic embeddings in multimodal large language models (MLLMs). Unlike prior contrastive pretraining methods that treat visual-text matching as a binary task, CRO performs fine-grained representation calibration during instruction tuning. The central idea is to add a representation-level contrastive loss that explicitly pushes the visual encoder’s embeddings closer to semantically corresponding text embeddings and further from mismatched samples. The authors also design a dual projection head that learns modality-specific mappings before fusion, ensuring balanced gradients and reducing the risk of representation collapse. CRO is implemented on top of several strong MLLM baselines (e.g., LLaVA, Qwen-VL, InternVL) and evaluated across multiple benchmarks, including MME, SEED-Bench, and MM-Vet. The results show consistent performance gains, particularly on tasks requiring fine-grained reasoning and grounding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical and well-motivated idea: The work addresses an increasingly recognized issue — poor cross-modal embedding calibration in current MLLMs — in a clean and effective way.\n\n- Simple yet effective method: CRO’s integration into the instruction-tuning pipeline is elegant and lightweight, requiring minimal architectural changes.\n\n- Strong empirical results: The method yields consistent improvements across diverse benchmarks, especially in localization-heavy or reasoning-intensive tasks.\n\n- Good ablations: The paper provides detailed ablation studies, showing the contribution of each component (contrastive loss, dual projections, hard-negative mining).\n\n- Clear writing and visualization: Figures explaining the alignment mechanism and representation distributions are helpful and well-presented."}, "weaknesses": {"value": "- Limited conceptual novelty: While effective, CRO is fundamentally an adaptation of well-known contrastive alignment ideas (InfoNCE, CLIP-style objectives) to MLLM fine-tuning. The innovation lies mainly in the integration strategy.\n\n- No deep theoretical insight: The paper is purely empirical; it would benefit from analysis explaining why contrastive calibration particularly helps downstream reasoning or grounding.\n\n- Dependency on quality of negatives: CRO relies on informative negative samples. The mining process is described but not extensively analyzed for failure cases.\n\n- Generalization scope: The experiments are focused on vision-language understanding; there’s no evaluation on video, audio, or embodied multimodal tasks, where alignment dynamics may differ.\n\n- Compute cost trade-off: CRO introduces additional computation due to contrastive sampling, though the paper doesn’t quantify the exact increase during large-scale fine-tuning."}, "questions": {"value": "- How does CRO perform if applied during pretraining rather than instruction tuning? Does early-stage alignment lead to better downstream generalization?\n\n- Have you examined whether CRO helps mitigate modality imbalance (e.g., text dominating vision features in fused representations)?\n\n- How are negative samples selected? Is there a risk that semantically similar images or captions are incorrectly treated as negatives?\n\n- Could CRO be extended to align other modalities (e.g., audio, 3D point clouds) using the same principle?\n\n- How stable is CRO training when scaling to larger MLLMs such as GPT-4V-like architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kzEHqutRYJ", "forum": "hm2EcwFm2Q", "replyto": "hm2EcwFm2Q", "signatures": ["ICLR.cc/2026/Conference/Submission17513/Reviewer_vra5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17513/Reviewer_vra5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939660189, "cdate": 1761939660189, "tmdate": 1762927394187, "mdate": 1762927394187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Robot State-aware Contrastive Loss (RS-CL), a regularization method for VLA models that aligns visual-language representations with robot proprioceptive states. RS-CL serves as a lightweight, plug-in auxiliary loss that operates directly on VLM embeddings, complementing the standard action prediction loss. The key idea is to assign contrastive similarity weights based on relative distances between robot states, effectively encouraging representations to capture control-relevant information. The method also introduces a representation-level augmentation called view cutoff, which masks a randomly selected camera view to improve robustness to occlusions. Experiments show consistent improvements over baselines such as GR00T N1.5 and π0."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and clearly structured.\n\n- Addresses a key bottleneck in scaling VLAs from perception to control—improving the action-awareness of representations.\n\n- Experimental validation is extensive, covering both simulation and real-world scenarios."}, "weaknesses": {"value": "- Although claimed to be lightweight, no runtime or FLOP comparison is provided. Some quantification of computational overhead (especially during training) would help support the efficiency claim.\n\n- The validation of RS-CL is limited. Evaluating RS-CL on other VLAs could help further verify the effectiveness of RS-CL.\n\n- This paper lacks some discussion with related work that also uses contrastive learning [a, b], especially [b], which also highlights the role of robot proprioception.\n\n[a] Ma et al., \"Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation\", arXiv:2406.09738\n\n[b] Jiang et al, \"Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets\", ICLR 2025\n\n- The paper compares primarily against robotics-trained VLMs. Including baselines like VICReg, SimCLR, or contrastive methods with temporal or goal-conditioning could clarify RS-CL’s unique benefits."}, "questions": {"value": "See the weakness also.\n\nThe RS-CL may miss some semantic information from robot proprioception. Does RS-CL use only joint positions or use both qpos and the end effector 6D poses? The similarity between different proprioceptions may show different semantic meanings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5SeNkWW8V0", "forum": "hm2EcwFm2Q", "replyto": "hm2EcwFm2Q", "signatures": ["ICLR.cc/2026/Conference/Submission17513/Reviewer_aA6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17513/Reviewer_aA6p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946136371, "cdate": 1761946136371, "tmdate": 1762927393734, "mdate": 1762927393734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Robot State-aware Contrastive Loss (RS-CL), a lightweight contrastive regularizer for Vision–Language–Action (VLA) models that explicitly aligns VLM-derived condition embeddings with robot proprioceptive states. Key ingredients are (1) a learnable summarization token and small projector that produces compact embeddings for contrastive training, (2) a soft-weighting scheme where pairwise contrastive weights come from Euclidean distances between proprioceptive states, and (3) a representation-level augmentation called view cutoff that masks a single view’s feature slice to cheaply produce augmented positives. RS-CL is applied both as an auxiliary loss when fine-tuning strong pre-trained VLA models (e.g., GR00T N1.5) and when training VLA models from scratch on multiple VLM backbones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "• RS-CL can be added to existing VLA pipelines with modest compute overhead (projector + adapter + view cutoff). \n\n• The paper evaluates soft-label target choices and a set of representation-level augmentations, showing that current-state distance and view-cutoff perform best."}, "weaknesses": {"value": "• The idea of aligning VLM representations with proprioceptive states is intuitive, but lacks in-depth theoretical analysis. How exactly this alignment works, and to what extent it improves the model's decision-making ability, remains unanalyzed. Experimental results (Tables 1 and 2) show that the proposed method provides limited performance improvements, making it difficult to effectively demonstrate its effectiveness and superiority.\n\n• The Franka real-robot experiments are compelling but limited in scope (a handful of tasks, 60 demonstrations per task); broader hardware trials (multiple setups, lighting/clutter variations, more repeats) would improve confidence in real-world robustness. \n\n• Authors note that contrastive path improvements vary with batch size (LIBERO smaller batch → smaller gains). Practical adoption may be sensitive to available batch sizes and compute. More analysis of tradeoffs (batch size, projector size, λ schedule) would help practitioners. \n\n• RS-CL uses proprioceptive state only; object pose, tactile, or contact signals are mentioned as future work but could be important for many manipulation tasks. The limitation is acknowledged. \n\n• While several ablations are present, it would strengthen claims to show (a) seed-level variance of gains, (b) sensitivity to β/τ/λ schedules, (c) cases where RS-CL harms performance (failure modes)."}, "questions": {"value": "1.\tHow many independent real-robot trials per task were run and under what variations (lighting, clutter, object pose perturbations)? Please report per-task trial counts and variance for the Franka experiments. If hardware trials are limited, please be explicit about failure cases observed. \n\n2.\tHow sensitive are gains to (a) λ schedule (decay to 0), (b) the soft-weight temperature β and contrastive τ, (c) projection head size (you use 2048→128), and (d) global batch size? An explicit sweep or short table would be helpful because you note batch size affects gains (LIBERO vs RoboCasa). \n\n3.\tWhat are wall-clock costs and hardware used for the fine-tuning experiments (GPU type, hours to train 60K steps) and for from-scratch training? This matters for reproducibility and adoption. \n\n4.\tAre there tasks or scene conditions where RS-CL reduces performance (e.g., when proprioception is noisy or misleading, or when visual cues are the only reliable signal)? If so, please quantify or describe mitigation. \n\n5.\tTable 3a shows current-state distance yields highest avg. Can authors provide intuition and any visualization showing how this choice affects the embedding manifold compared to next-action distances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l8vVCvjiTj", "forum": "hm2EcwFm2Q", "replyto": "hm2EcwFm2Q", "signatures": ["ICLR.cc/2026/Conference/Submission17513/Reviewer_Cz1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17513/Reviewer_Cz1M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762571781323, "cdate": 1762571781323, "tmdate": 1762927393333, "mdate": 1762927393333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}