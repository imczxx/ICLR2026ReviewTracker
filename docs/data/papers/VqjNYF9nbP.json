{"id": "VqjNYF9nbP", "number": 1913, "cdate": 1756965236950, "mdate": 1763713155592, "content": {"title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate unit test generation, yet methods for training LLMs to produce high-quality unit tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning (RL) framework that trains an LLM to generate high-quality unit test given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via RL: (1) the unit test generator is trained to maximize a discrimination reward, encouraging it to produce tests that reveal faults in the code generator’s solutions; and (2) the code generator is trained to maximize a code reward, encouraging it to produce solutions that pass the unit tests generated by the unit test generator. In our experiment, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models like GPT-4.1 and GPT-4o in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for the unit test generation.", "tldr": "", "keywords": ["Code generation", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/178c11ca84ab8e17b932a21370c7635ef76d4731.pdf", "supplementary_material": "/attachment/fd94c3d3b4b2286363ce1ee146994fadde71fe31.zip"}, "replies": [{"content": {"summary": {"value": "The authors proposed using reinforcement learning to jointly train a unit test generator and a code generator. The unit test generator is optimized for generating test cases that can discriminate gold code and model-generated code, while the code generator is optimized for pass rate. This eliminates the need for collecting expensive gold unit test and experimental results show that the proposed method has superior performance than SFT baselines and prompting frontier models to generate test cases."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of modeling code generation and unit test generation as an adversarial setup is sound.\n2. This setup removes the need for gold unit test data, which is particularly hard to collect.\n3. Qwen3-4B trained with UTRL outperforms GPT4.1 and GPT4o in terms of unit test generation, which is a strong result.\n4. The paper is well presented and is easy to follow."}, "weaknesses": {"value": "1. The experiments are mainly conducted with Qwen3-4B, lacking analysis on the effectiveness across different models such as Llama, Gemma, or more code-specific ones like Qwen Code. I also wonder if the trained unit test generator is able to improve frontier models, when they are served as the code generator.\n2. There is a hyperparameter, lambda, that balances the discrimination reward and the validity reward, which might need tuning. However, the analysis of this hyperparameter is missing.\n3. There is little discussion on the iterative training part, where the authors only mention that the discrimination reward saturates at iter 1 and keeps improving at iter 2. But how many iterations are needed, and could we expect even more improvement after iteration 2 is not mentioned."}, "questions": {"value": "1. In figure 3, the caption says “Second, the discrimination reward is defined as a ratio of sampled code solutions that do not pass at least one valid test case. In this figure, among 6 sampled code solutions, 4 code solutions (C1, C3, C4, C5) do not pass at least one valid test case, resulting in the discrimination reward of 0.667”, but the from the figure it seems like C1, C3, C4, C5 pass at least one valid test case. Do the authors mean “solutions that have at least one failure case”?\n2. I wonder what the performance would be if we train the unit test generator without the adversarial setup, i.e., only train it using RL and the gold code solution to generate valid unit tests, then use it at test time to measure how much it could improve code generation.\n3. In section 3.1, the author mentioned that the two key steps are repeated iteratively. How many iterations does it need to converge, and how is the performance changing over iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "urkEioPTiu", "forum": "VqjNYF9nbP", "replyto": "VqjNYF9nbP", "signatures": ["ICLR.cc/2026/Conference/Submission1913/Reviewer_NJwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1913/Reviewer_NJwC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337709992, "cdate": 1761337709992, "tmdate": 1762915946671, "mdate": 1762915946671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an RL method for training model to generate unit tests. Their method trains UT generator and code generator together in an adversarial loop: the UT generator is supposed to find faults, while the code generator is supposed to improve to pass more unit tests.\n\nThe method uses iterative RL training of 2 LLMs, with each model training off each other's signal, with the main contribution being a reward for UT generation, which informs the reward for code generation. The models are trained on examples from the TACO dataset.\n\nThe method is evaluated on TACO, with results showing improved UG generation as compared to base model, and as compared to an SFT model. The method is compared to multiple baselines:\n- untrained models (closed and open-source)\n- SFT model, trained on data filtered from Gemini 2.5 \n- compare to CURE UTs generated by a Qwen 2.5 7B model finetuned by CURE authors \n- Ground-truth unit tests (ceiling)\n\nMethods are evaluated according to best-of-N reranking with different models as generators and UT fidelity, which is the Spearman correlations between the ranking of several solutions according to a generated and ground truth unit tests. \nThe results indicate that UTs generated by the proposed method improve both Qwen-3-4B and Qwen-3-14B in best-of-32 and achieve higher fidelity. They also indicate that in an apples-to-apples comparison on Qwen-2.5-Instruct, the method outperforms CURE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is clearly written and well-organized, and explanation of results is clear\n- The method appears to work well as a verifier, with improvements even over proprietary models. The adversarial nature of the method is well-motivated. \n- Unit test fidelity as a metric seems to be a novel contribution. \n- Compares to recent baseline (CURE)\n- Problem importance: unit test generation is an important problem"}, "weaknesses": {"value": "- no pass@1 results. From the way that the adversarial setup is described in the intro, I would have expected to see improvements to pass@1 for the adversarial generator. It seems like the evaluation (reranking@32) is geared towards the discriminator and test-time re-ranking, but does the pass@1 of the generator also improve via training? This is shown a bit in Fig 7 but should be expanded (since code score also is not quite the same thing as pass@1). Effectively, section 4.4 could be scaled up. \n- OOD generalization: both train/test are TACO, so it's not clear that the method will generalize to other datasets. This might be something that is covered in the CodeContest experiment with Qwen-2.5 compared to CURE, but this should be made more formal/explicit. Moreover, it's not clear why the paper doesn't evaluate on the same data as CURE (which they use as a baseline) or UTGEN (which they cite).\n- missing related work: https://arxiv.org/abs/2502.14948 also iteratively improves LLMs for solving and testing using RL with DPO. \n- for CURE comparison: it would be nice to also see CURE applied to Qwen-3-4B \n- limited model size: does this still result in improvements with stronger/larger models? It would also be nice to see a larger Qwen-3 model as a baseline for UT generation in Table 1 (since we don't know what size 4o and 4.1 are.)"}, "questions": {"value": "- L156-157: under (1), your goal is to discriminate sampled code from gold code. Doesn't this assume that the sampled code is incorrect? If so, does the current method still work when the generator is a stronger coding model?\n- Qwen-3-4B: is this the instruct or base model? if it's the base model, do you still get improvements over instruct model? \n- I would suggest adding experiments showing\n\t- more on pass@1\n\t- cross-dataset transfer\n\t- comparison to larger models \n- a bit of a nit: since CURE came out in June, it seems odd to call it \"concurrent work\" (L097) and this doesn't seem necessary since the authors compare against it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLCHtnYQm2", "forum": "VqjNYF9nbP", "replyto": "VqjNYF9nbP", "signatures": ["ICLR.cc/2026/Conference/Submission1913/Reviewer_eXix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1913/Reviewer_eXix"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516209108, "cdate": 1761516209108, "tmdate": 1762915946292, "mdate": 1762915946292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UTRL, an adversarial RL loop training the language model to be code generator and a unit test generator. These components are trained in an adversarial manner with the UT generator acting as the discriminator -- distinguishing ground truth code solution from incorrect codes generated by the code generator while generating valid unit tests. The code generator on the other hand is optimized to have a higher pass rate over the generated unit tests. The paper makes use of the TACO dataset for code generation and verifies that their system leads to better quality unit tests and higher code generation performance when compared to different SFT and RL baselines (like CURE)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The adversarial setting in the method intuitive and appears to be effective compared to the baselines (although the latter can be improved, see below)\n- The paper is well-written, well-structured, and easy to follow. The problem or topic is well-motivated, and joint improvements in unit test generation and code generation could be of interest to the community."}, "weaknesses": {"value": "- Missing references to related work, I think the scope of related works discussed is very myopic, and I would broadly discuss the following line of work:\n    -  Self-play setups for LLMs: https://arxiv.org/abs/2407.19594, https://arxiv.org/abs/2401.01335, https://arxiv.org/abs/2502.14948\n    - In my opinion, it is worth emphasizing the adversarial training setup in general which this method builds on, e.g. GANs (https://arxiv.org/abs/1406.2661), https://arxiv.org/abs/1607.02533, etc.\n\n- Strength of SFT baseline: \n    - In table 1 and 2, why not present the UT + Reason baseline? No discussion why the code-generation performance of UT + Reason baseline is lesser in Fig 5 (a) despite better code fidelity.\n    - Alternate design, Prasad et al show that SFT supervision by training unit tests to be discriminative (they call attacking) and valid (with correct) outputs is effective. Sec 3.2 suggests such data is already collected during training, set of UTs which are both discriminative and valid. What happens when you train an SFT baseline with or without reasoning on this data? Would this remove the need for joint RL training?\n- Lack of OOD generalization: The paper only tests on an in-domain test set of TACO dataset. Can the author show improvements in code-generation and UT fidelity transferring to CodeContests, LeetCode, MBPP, LiveCodeBench and other mainstream code-generation datasets."}, "questions": {"value": "See above in addition to the questions below:\n1. What is a tangible takeaway from Fig 7, how does the training trend contrast with other baselines?\n2. Isn't the training online for the main results reported in Table 1, or is it iterative as well -- if so how is the number of iterations set? Can Fig 8 be extended to more iterations?\n3. The paper repeatedly makes the claim that UTRL removes the need for ground-truth UT annotations. However, the training uses the regular code-generation recipe of instruction, gold solution. When these are available, ground-truth UTs can be readily collected by sampling multiple inputs from an LLM, rejecting invalid outputs. The authors should clarify what other kinds of annotations if any they are referring to.\n4. As mentioned in Sec 3.1, you generate a set of UTs for each problem. Are the number of UTs roughly similar for various baselines?\n5. Why not show the pass@1 accuracy of the code generator before and after training? Does that not improve and the improvement only come from best of n sampling? If not, could the performance be further improved by doing best-of-N sampling over the trained code-generator model or is it not effective due to the nature of adversarial training. An ablation separating the contribution of better code generation and UT generation would strengthen this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VNsAxEkKMT", "forum": "VqjNYF9nbP", "replyto": "VqjNYF9nbP", "signatures": ["ICLR.cc/2026/Conference/Submission1913/Reviewer_qbAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1913/Reviewer_qbAT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540519000, "cdate": 1761540519000, "tmdate": 1762915943705, "mdate": 1762915943705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UTRL, an adversarial reinforcement learning framework for training a Large Language Model (LLM) to generate high-quality unit tests. The framework co-trains two models: a unit test generator ($M_{\\text{UT}}$) and a code generator ($M_{\\text{code}}$). The core idea is that $M_{\\text{UT}}$ is rewarded for creating tests that can distinguish between code generated by $M_{\\text{code}}$ and a ground-truth solution ($C^*$), while $M_{\\text{code}}$ is rewarded for generating code that passes the tests from $M_{\\text{UT}}$. The main contribution is a method to train a unit test generator without requiring ground-truth unit tests, relying only on instruction-code pairs. Experiments on the TACO dataset show that the resulting unit test generator outperforms supervised fine-tuning (SFT) and proprietary models in producing high-quality tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental setup is rigorous. The authors compare their method against a good set of baselines, including various LLMs, SFT, and the recent CURE framework. The introduction of \"best-of-N improvement\" and \"unit test fidelity\" as evaluation metrics is well-reasoned and provides a quantitative assessment of the quality of the generated tests.\n2. The paper is clearly written and well-structured. The UTRL framework is explained effectively with figures, an algorithm description, and formal reward definitions, making the methodology easy to understand.\n3.  The work addresses the important and challenging problem of automated unit test generation. Improving the quality of generated tests could provide a more reliable and scalable feedback mechanism for training and verifying code-generating LLMs."}, "weaknesses": {"value": "The central claims of the paper rest on a fundamental assumption that severely limits its practical applicability and scalability.\n\n1.  Unrealistic Requirement of Ground-Truth Code: The entire training process is critically dependent on having access to a ground-truth, executable correct solution ($C^*$) for every training instance. This assumption is unrealistic for the vast majority of programming tasks. The primary value of automated code generation lies in solving problems for which a solution is not already known. If a correct, executable `C*` is available, the need to train a new code generator is greatly diminished. This requirement makes it impossible to scale the method using large, readily available datasets of code from sources like GitHub, confining it to niche, curated datasets.\n\n2. No Improvement in Downstream Code Generation: The ultimate goal of generating better unit tests is often to improve the performance of code generators. However, the paper's own results in Figure 7 demonstrate that this goal is not achieved. The code generator trained adversarially with UTRL's generated tests performs comparably to, but slightly *worse than*, a model trained with rewards from the ground-truth unit tests. This suggests that the complex adversarial framework is, at best, a proxy for an oracle that it cannot outperform. While it successfully creates a high-quality unit test generator, it fails to show that these generated tests provide a superior training signal that pushes the code generator beyond existing capabilities. The main loop does not seem to create a net performance gain for the overall system.\n\n3.  Overstated Claims on Scalability: Given the reliance on $C^*$ and the lack of improvement for the code generator, the claim that this method can enable better scaling of LLM training beyond current SOTA methods is not well-supported. The need for verified, executable solutions for every training problem is a far more significant bottleneck than the need for unit tests, fundamentally hindering scalability.\n\n4. The proposed adversarial framework was proposed before in other similar scenario where an external interpreter is available (e.g. for Lean with STP [1])\n\n[1] Dong, Kefan, and Tengyu Ma. \"Stp: Self-play llm theorem provers with iterative conjecturing and proving.\" arXiv preprint arXiv:2502.00212 (2025)."}, "questions": {"value": "1.  The framework's dependency on a ground-truth executable solution appears to be its greatest limitation. Could the authors elaborate on the practical applicability of this method outside of curated competitive programming datasets where canonical solutions are available? How could this framework be adapted for a more realistic scenario where $C^*$ does not exist?\n\n2.  Figure 7 indicates that the UTRL-trained code generator does not surpass the performance of an agent trained with ground-truth unit tests. Does this imply that the primary contribution is the creation of a standalone, high-quality unit test generator, rather than a superior end-to-end training pipeline for code generation? What is the practical advantage of the adversarial loop if the resulting code agent's performance does not improve beyond the oracle baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4ucCbvFDt", "forum": "VqjNYF9nbP", "replyto": "VqjNYF9nbP", "signatures": ["ICLR.cc/2026/Conference/Submission1913/Reviewer_R3AP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1913/Reviewer_R3AP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947756101, "cdate": 1761947756101, "tmdate": 1762915943184, "mdate": 1762915943184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We deeply appreciate your time and effort in reviewing our manuscript. As highlighted by reviewers, UTRL is an effective adversarial reinforcement learning framework to train LLMs for unit test generation, which is a crucial and challenging problem (all reviewers). \nOur paper clearly identifies challenges in collecting high-quality unit tests to train LLMs for unit test generation (NJwC), and proposes an intuitive RL-based adversarial training method that allows LLMs to produce discriminative unit tests even without access to high-quality test suites for training LLMs (qbAT, NJwC) with clear presentations (all reviewers). Under a rigorous experimental setup (R3AP, eXix) and carefully designed evaluation metrics (R3AP, eXix), UTRL achieves strong empirical performance in unit test generation even outperforming proprietary LLMs (eXix, NJwC), and also improves code generation performance.\n\nWe appreciate the reviewer’s insightful comments on our manuscript. In response to the questions and concerns the reviewers raised, we have carefully revised and enhanced the manuscript with the following additional experiments and discussions:\n\n* Addressing the missing related works (Section 2)\n* Clarification on open-source LLM baselines (Section 4.1)\n* Comparison with Qwen3-32B (Section 4.2)\n* Additional discussion on performance of SFT baselines (Section 4.3)\n* Improving discussion and experiments regarding UTRL for code generation (Section 4.4)\n    - Comparison with additional code generation baselines (SFT, RL with unit tests generated by frontier models)\n    - Updating figure 7\n    - Clarification on evaluation metric for code generation.\n* Additional discussion on iterative training (Section 4.5)\n* Comparison to best-of-N distillation method (Appendix A.2.1)\n* Generalization experiment on out-of-distribution benchmark (Appendix A.2.2)\n* Experiment under different LLM (Appendix A.2.3)\n* Ablation study regarding hyperparameter balancing discrimination reward and validity reward (Appendix A.2.4)\n\nThese updates are temporarily highlighted in “red” for your convenience. We strongly believe that UTRL can be a useful addition to ICLR community, particularly because reviewers’ constructive comments enhanced the manuscript. Thank you very much, \nAuthors"}}, "id": "EIM3sjLQpy", "forum": "VqjNYF9nbP", "replyto": "VqjNYF9nbP", "signatures": ["ICLR.cc/2026/Conference/Submission1913/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1913/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission1913/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763713465762, "cdate": 1763713465762, "tmdate": 1763713465762, "mdate": 1763713465762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}