{"id": "0DaB4jeGaf", "number": 7858, "cdate": 1758039348094, "mdate": 1763761025101, "content": {"title": "Conquer the Quantile: Convolution-Smoothed Quantile Regression with Neural Networks and Minimax Guarantees", "abstract": "Quantile regression provides a flexible approach to modeling heterogeneous effects and tail behaviors. This paper introduces the first quantile neural network estimator built upon the \\textbf{con}volution-type smoothing \\textbf{qu}antil\\textbf{e} \\textbf{r}egression (known as \\textit{conquer}) framework, which preserves both convexity and differentiability while retaining the robustness of the quantile loss. Extending the conquer estimator beyond linear models, we develop a nonparametric deep learning framework and establish sharp statistical guarantees. Specifically, we show that our estimator attains the minimax convergence rate over Besov spaces up to a logarithmic factor, matching the fundamental limits of nonparametric quantile estimation, and further derive general upper bounds for the estimation error in more general function classes. Empirical studies demonstrate that our method consistently outperforms existing quantile networks in both estimating accuracy and computational efficiency, underscoring the benefits of incorporating conquer into deep quantile learning.", "tldr": "", "keywords": ["quantile regression", "minimax rate", "convolution", "deep learning theory", "Besov space"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63e2a339e31609c0867f52392fadb0365ea8ba10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a quantile regression framework, where an ReLU neural network is used to approximate the quantile function, and a convolution-type smooth quantile loss is used to train the network. Experimental results on synthetic data show that the proposed framework outperforms ReLU networks trained with quantile loss in terms of MSE evaluated at different quantile levels and runtime."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow\n- The paper provides strong theoretical results, where it achieves the optimal minimax convergence rate over Besov spaces up to a (log n)^3 factor. Nonasymptotic generalization bounds are also provided for nonsmooth quantile functions. This analysis has not been done for ReLU networks + convolution-type smooth quantile loss\n- While the framework itself — namely the use of a convolution-type smooth quantile loss combined with ReLU networks — is not novel, I think this is not a drawback, given the accompanying theoretical results"}, "weaknesses": {"value": "- While I understand that a key contribution is the theoretical analysis, a main concern is that the experiments are limited to synthetic data. It is unclear how the method would perform in more realistic settings\n- Only MSE loss at different quantile levels are evaluated in the experiments, while other common metrics such as MAE loss and pinball loss are missing\n- The results reported in the tables are not clearly highlighted. It appears that bold font is used when the proposed method performs the best, but it is not used when a baseline performs the best"}, "questions": {"value": "- In the tables, what does bold font represent?\n- Where does the training-time improvement of the proposed method over the baseline come from?\n- It is claimed that \"the results highlights the effectiveness of conquer to modern neural network architectures\" Can the theoretical analysis and results be extended to more commonly used modern architectures, for example, MLPs with skip connections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iD5O1BaAuX", "forum": "0DaB4jeGaf", "replyto": "0DaB4jeGaf", "signatures": ["ICLR.cc/2026/Conference/Submission7858/Reviewer_JDUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7858/Reviewer_JDUL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761352326380, "cdate": 1761352326380, "tmdate": 1762919896995, "mdate": 1762919896995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThis paper extends the convolution-smoothed quantile regression (Conquer) framework to deep neural networks.  \nIt replaces the non-differentiable pinball loss with a kernel-convolved, smooth surrogate, allowing gradient-based optimization while preserving quantile consistency.\n\nMain contributions:\n- Defines the **ConquerNN estimator**, minimizing a smoothed quantile loss over ReLU networks.\n- Provides **non-asymptotic risk bounds** and proves **minimax-rate optimality** over Besov spaces (up to logarithmic factors).\n- Derives a **generalization bound** depending on architecture parameters (depth, width, sparsity).\n- Presents **synthetic experiments** under heavy-tailed noise showing improved MSE and training stability vs. standard pinball loss networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong theoretical rigor and correct proofs.  \n2. Achieves minimax-rate optimality over Besov spaces.  \n3. Convincing argument for smooth loss improving optimization stability.  \n4. Clear synthetic benchmarks and transparent experimental design.  \n5. Full reproducibility with provided code and documentation."}, "weaknesses": {"value": "1. **Empirical limitation:** Only synthetic data tested; no real-world validation.  \n2. **Incremental novelty:** Combination of known ideas rather than new conceptual insight.  \n3. **No joint-quantile or non-crossing analysis:** Key for realistic quantile regression.  \n4. **Bandwidth selection heuristic:** No adaptive or data-driven rule proposed.  \n5. **Limited connection to ICLR topics:** The work is statistically oriented, with weak links to deep learning challenges such as distributional modeling or uncertainty quantification."}, "questions": {"value": "1. Can you propose a **data-driven rule** for selecting the bandwidth \\(h\\)?  \n2. How does the smoothing behave when training **multiple quantiles jointly** (non-crossing constraints)?  \n3. How sensitive are convergence and optimization to **misspecified \\(h\\)** or near-zero densities around the quantile?  \n4. Have you evaluated the method on **real regression datasets** or higher-dimensional tasks?  \n5. Could the smoothing principle be extended to **CRPS/Wasserstein objectives** for broader applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PYrmLbYe8T", "forum": "0DaB4jeGaf", "replyto": "0DaB4jeGaf", "signatures": ["ICLR.cc/2026/Conference/Submission7858/Reviewer_uB4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7858/Reviewer_uB4Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990865343, "cdate": 1761990865343, "tmdate": 1762919896317, "mdate": 1762919896317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the integration of the conquer estimator in quantile regression with neural networks. The theoretical results mainly show that a ReLU-activated neural network that minimizes the conquer estimator performs optimally in $L^2$ norm, up to a logarithmic factor. An error bound is also established for a general neural network. Empirical studies demonstrates the benefits of applying the conquer estimator in neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important question of using the smoothed conquer estimator for quantile regression. The theoretical result is promising in that it shows the near-optimality of the conquer loss function.\n\n* Theorem 3.2 extends to the setting of a general neural network, relaxing the shape of the network. This makes the discussion more applicable."}, "weaknesses": {"value": "* The empirical studies lack some generality. It is hard to argue why the three scenarios are representational. In addition, the empirical benefit is decoupled from the theoretical analysis in the paper: that is, the author(s) attribute the empirical advantage to the better training stability and generalization of the neural network trained with conquer, but this is not the major theoretical claims (which only shows conquer leads to a reasonably good optimum). In order to make the argument more complete, the paper would benefit from:\n\n   * Some empirical studies (maybe a numerical experiment) to directly corroborate Theorem 3.1/2.\n   * Some further stability and generalizability analysis of conquer-trained neural network.\n\n* The paper mainly analyzes the global optimum of a neural network with respect to the conquer objective, defined in (2.2). However, training a neural network hardly ever yields a global minimum. In order to see the benefit of conquer in neural network, a more practical piece of analysis would be related to the training dynamics, showing how conquer improves the training loss landscape. I understand that this is beyond the scope of the paper, but lacking it restricts the paper to a \"expressivity-type\" analysis.\n\n* The presentation of the paper and main results can be improved. While the major claims made in the theorems can be understood, some notations are hard to keep track and are not explained anywhere in the paper. Please see some questions below."}, "questions": {"value": "1. In (2.1), should we assume $A^{(1)} \\in \\mathbb{R}^{W \\times d}$ instead of $\\mathbb{R}^{W \\times W}$?\n\n2. On line 136, do we assume that the $\\infty$-norm is defined on a compact domain?\n\n3. On line 138, I cannot see how the function $f_n$ depends on $n$. Is it a typo?\n\n4. For Assumption 3, it is useful to show how $c_1$ and $c_2$ reflects in the upper bound in Theorem 3.1. A related question: it is intuitively hard to understand why the lower bound is needed: if the probability density is not supported on a subdomain, it does not appear in the expectation in the definition of $\\|\\cdot\\|_{\\ell_2}$ either, so why would that be a problem?\n\n5. In Theorem 3.1, does $L$ need to be equal to the quantity on the right-hand side, or is it more of an inequality?\n\n6. In Theorem 3.1, what does \"with probability approaching $1$\" mean exactly? Also, is it possible to rephrase the theorem into something like \"with probability no less than $1-\\delta$\" and have $\\delta$ in your upper bound?\n\n7. In section 4, have you tested residual-based neural networks? They are known to have better training stability and it is interesting to see if they can help avoid the issues in the baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KeErNi9G94", "forum": "0DaB4jeGaf", "replyto": "0DaB4jeGaf", "signatures": ["ICLR.cc/2026/Conference/Submission7858/Reviewer_f5nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7858/Reviewer_f5nf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226363625, "cdate": 1762226363625, "tmdate": 1762919895370, "mdate": 1762919895370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}