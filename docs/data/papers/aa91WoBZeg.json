{"id": "aa91WoBZeg", "number": 24391, "cdate": 1758356387460, "mdate": 1759896768555, "content": {"title": "Fair Conformal Classification via Learning Representation-Based Groups", "abstract": "Conformal prediction methods provide statistically rigorous marginal coverage guarantees for machine learning models, but such guarantees fail to account for algorithmic biases, thereby undermining fairness and trust. This paper introduces a fair conformal inference framework for classification tasks. The proposed method constructs prediction sets that guarantee conditional coverage on adaptively identified subgroups, which can be implicitly defined through nonlinear feature combinations. By balancing effectiveness and efficiency in producing compact, informative prediction sets and ensuring adaptive equalized coverage across unfairly treated subgroups, our approach paves a practical pathway toward trustworthy machine learning. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the framework.", "tldr": "", "keywords": ["Classification; Conformal Prediction; Equalized Coverage; Fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22b5b1603dcc921d0afdf1745d40e90c1f60d4c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for fair conformal inference in classification tasks. The method adaptively identifies subgroups and enforces conditional coverage guarantees on these learned groups. To evaluate subgroup fairness, the authors introduce a nonlinear variant of the Worst Slab Coverage (WSC) metric, designed to better capture coverage deficiencies in complex or non-linearly defined groups. Experiments are conducted on synthetic data and one real-world dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The approach is **novel**: instead of enumerating or greedily constructing subgroups, the authors learn subgroup structure via representation learning—allowing for more complex group definitions (for instance, XOR-type interactions) that are hard to capture with feature-based heuristics.\n- It **improves over the baseline AFCP** in terms of achieving fairer conditional coverage, with only a modest reduction in efficiency (prediction set size). The substantial runtime improvement makes it a much more practical alternative.\n- The **methodology is theoretically sound**, supported by clear formulations and proofs that establish its coverage guarantees."}, "weaknesses": {"value": "- The **empirical evaluation is limited**, with most experiments conducted on synthetic data and only a single real-world dataset (Nursery). Including larger and more commonly used datasets—such as the *Folktables* benchmarks used in prior fairness/conditional conformal prediction work ([1]–[3])—would strengthen the experimental evidence.\n- The paper does not discuss the **sensitivity of the hyperparameter β** in the final loss function, which may affect the fairness–efficiency trade-off.\n- A few parts of the **writing could be polished for clarity**, including the caption for Figure 2/Table 1 and the paragraph starting at line 405.\n\nReferences\n\n[1] O. Bastani et al: Practical adversarial multivalid conformal prediction [NeurIPS 2022]\n\n[2] C. Jung et al: Batch Multivalid Conformal Prediction [ICLR 2023]\n\n[3] AT. Vadlamani et al: A Generic Framework for Conformal Fairness [ICLR 2025]"}, "questions": {"value": "See weaknesses.\n\n- The paper focuses on **equalized coverage** as the fairness notion for conformal prediction. However, other fairness notions—such as those derived from popular ML fairness metrics—have been considered in related work ([3]). Does your method naturally extend to these alternative definitions, and if so, how?\n\nReferences\n\n[3] AT. Vadlamani et al: A Generic Framework for Conformal Fairness [ICLR 2025]"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2UwSbDxbKY", "forum": "aa91WoBZeg", "replyto": "aa91WoBZeg", "signatures": ["ICLR.cc/2026/Conference/Submission24391/Reviewer_hizj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24391/Reviewer_hizj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530064189, "cdate": 1761530064189, "tmdate": 1762943070315, "mdate": 1762943070315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FAREG, a conformal prediction method that ensures fairness by adaptively identifying “unfair” subgroups in a learned latent representation space. Unlike prior conformal predictors with equalized coverage that only consider simple or pre-defined groups (e.g. single sensitive features), FAREG learns a representation $Z = f(X)$ via a variational encoder–decoder and discovers complex subgroups (even defined by nonlinear feature combinations like XOR) associated with low coverage. The method then adjusts prediction sets to guarantee adaptive equalized coverage for these discovered subgroups. Additionally, the paper introduces WSC+, a “nonlinear” worst-case conditional coverage metric extending the worst-slab coverage (WSC) metric of Cauchois et al. (2021)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the important intersection of uncertainty quantification and algorithmic fairness. Ensuring no subgroup is underserved by predictive uncertainty is crucial for trustworthy AI. The work is timely and relevant to high-stakes domains. FAREG’s combination of representation learning with conformal prediction is novel. It significantly extends prior fair conformal methods (from single-feature groups to rich latent groups)."}, "weaknesses": {"value": "- The proposed algorithm is complex, involving a custom VAE training and Monte Carlo sampling of groups. Some steps (the PGD projection and the final aggregation of prediction sets) are not explained in depth in the main text. \n\n- FAREG focuses on one subgroup (or a mixture of one) to protect. If there are multiple distinct biased subgroups, it’s unclear if FAREG can handle them simultaneously.\n\n- some relevant baselines and related works are not mentioned. for instance  (https://arxiv.org/pdf/2505.16115) or (https://arxiv.org/abs/2305.12616)"}, "questions": {"value": "- Could you elaborate on how exactly the $T$ samples of $S$ (Algorithm 1, lines 12–16) are used to form the final prediction set $C(X_{N+1})$?\n\n\n- Under what assumptions does FAREG guarantee $P(Y \\in C(X) \\mid X \\in \\hat G) \\ge 1-\\alpha$ for the discovered group $\\hat G$? Is this guarantee exact finite-sample (with sample splitting) or only asymptotic/high-probability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JQ8d0wJAE0", "forum": "aa91WoBZeg", "replyto": "aa91WoBZeg", "signatures": ["ICLR.cc/2026/Conference/Submission24391/Reviewer_9Zja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24391/Reviewer_9Zja"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851618059, "cdate": 1761851618059, "tmdate": 1762943069704, "mdate": 1762943069704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FAREG, a fair conformal classification method that finds and fixes under-covered subgroups so that they reach the target coverage, without blowing up prediction-set size. Here are the mains steps of the method.\n1. Learn a compact representation Z of the features X.\n2. On this Z, train a small classifier that scores how likely each sample belongs to an “unfair subgroup” (i.e., tends to be under-covered).\n3. Encourage this classifier to pick the samples with the lowest conditional coverage while keeping the subgroup at least a fraction δ of the data (for statistical reliability).\n4. Build a standard conformal prediction set for everyone, then build extra subgroup-specific sets for the selected groups, and take the union. This guarantees adaptive equalized coverage for those groups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is meaningful and important. \n2. The method appears technically sound. \n3. Although the full pipeline is more involved than the summary above, the paper is mostly readable."}, "weaknesses": {"value": "First, the goal of this work is a subset of existing objectives. Prior work has focused on conditional coverage\n$$\n\\mathbb{P}\\!\\left(Y \\in C(X_{n+1}) \\mid X_{n+1}=x\\right)=1-\\alpha,\n$$\nwhich is stronger than\n$$\n\\mathbb{P}\\!\\left(Y \\in C(X_{n+1}) \\mid X_{n+1}\\in \\widehat{\\mathcal{G}}\\right)=1-\\alpha.\n$$\nIf a conformal method attains (approximate) conditional validity, the proposed method may be less useful. It would be helpful to show that results still improve when FAREG is built on top of conditionally valid conformal methods such as~[1]. Second, based on Figures 2 and 5, gains in coverage appear to come at the cost of efficiency: the proposed method achieves higher coverage but also larger prediction sets.\n\n[1] Isaac Gibbs, John J Cherian, and Emmanuel J Cand`es. Conformal prediction with conditional guarantees. Journal of the Royal Statistical Society Series B: Statistical Methodology, pp. qkaf008, 2025."}, "questions": {"value": "What is the hypothesis class $\\mathcal{H}$ in the proposed algorithm? What is the VC dimension of $\\mathcal{H}$ in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ssWaot7IIo", "forum": "aa91WoBZeg", "replyto": "aa91WoBZeg", "signatures": ["ICLR.cc/2026/Conference/Submission24391/Reviewer_LJHP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24391/Reviewer_LJHP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903431312, "cdate": 1761903431312, "tmdate": 1762943067803, "mdate": 1762943067803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce FAREG, a method to employ variational information bottleneck to achieve fair conformal prediction by identifying worst performing subgroups. They also propose an improvement over the WSC metric to capture non-linear slabs. They perform experiments on a self-constructed synthetic dataset and the Nursery dataset, comparing against standard baselines. The authors also provide code supporting the reproducibility of their method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **[S1]** The authors do a great job of motivating the paper well. Figure 1 is a great example of how the current set of conformal prediction methods struggle in certain data settings. There is a clear gap in the literature that needs to be addressed and the manuscript is an attempt at that.\n- **[S2]** The core idea of FAREG is quite novel, I haven’t come across works using variational information bottlenecks in this context and it feels like a natural fit for potentially non-linear subgroups.\n- **[S3]** The synthetic experiment in Section 4.2 is designed perfectly to highlight the proposed method's strength compared to the baselines and is a great way to show the applicability of the approach.\n- **[S4]** The code seems succinct and sufficient, supporting the neat empirical claims in the paper."}, "weaknesses": {"value": "- **[W1]** Figure 4(a) and the claims in Line 374 that the time complexity of FAREG is linear in the number of data instances seems misleading, given that this does not account for the training of the encoder-decoder network which is a non-trivial computation. ACP is a post-processing algorithm which does not involve training of any such network and therefore a fairer comparison would be to compute the wallclock times of the entire algorithms from start to end.\n- **[W2]** While the synthetic experiment is a great demonstration of the algorithm's strength, it feels like the perfect application in terms of the XNOR-like bias but the authors do not include any other synthetic constructions that could help understand the algorithm's utility in general situations. \n- **[W3]** It’s unclear to me how the following statements all hold true together: Line 87: The interpretability is enhanced; Line 483: Expressivity may sacrifice interpretability; Line 941: This result strengthens the interpretability. Intuitively, it seems that the 2nd statement in the limitations is true, so what do the other two statements mean?\n- **[W4]** The proof for Theorem 1 in Section A.4 is extremely handwavy. It uses the Theorem 1 from (Zhou & Sesia, 2024) but never establishes how it is better than it formally. There’s a strange argument about the VC Dimension and higher expressivity written in words, but no formal proof of the statistical consequences on learning the groups on the same data used for calibration."}, "questions": {"value": "- **[Q1] ** See [W1]. Can you provide a more detailed and fair comparison of the wallclock time of FAREG and AFCP, showing the entire training, processing and postprocessing steps.\n- **[Q2]** See [W2]. Would be curious to see applications of FAREG on other synthetic setups, which are slightly more complicated than XNOR for example.\n- **[Q3]** See [W3]. Could you clarify the confusion about the interpretability of the algorithm and state what the effect of the encoder-decoder architecture is?\n- **[Q4]** See [W4]. Could you provide a more formal proof for Theorem 1, clearly highlighting the superiority over AFCP and the effects of that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "op9LKbLzYs", "forum": "aa91WoBZeg", "replyto": "aa91WoBZeg", "signatures": ["ICLR.cc/2026/Conference/Submission24391/Reviewer_eNWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24391/Reviewer_eNWr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025809531, "cdate": 1762025809531, "tmdate": 1762943067311, "mdate": 1762943067311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}