{"id": "4zoMnmZzh4", "number": 17682, "cdate": 1758279185177, "mdate": 1759897160382, "content": {"title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "abstract": "Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. **VisCode-Multi-679K** is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. **VisPlotBench** is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present **VisCoder2**, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching **82.4%** overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.", "tldr": "", "keywords": ["Code Models", "Visualization", "Fine-tuning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef1f7f10f7a8df66dbf608f4d8b559b12d66b107.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the task of generating visualization code from natural language instructions. It introduces VisCode-Multi-679K, a large multi-language dataset containing executable code-image pairs and multi-turn correction dialogues; VisPlotBench, a benchmark for evaluation; and VisCoder2, a fine-tuned model based on Qwen2.5-Coder. Experiments show that VisCoder2 outperforms open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: A large multi-language dataset and a benchmark are introduced; both are useful to the community.\n- S2: The overall task setting aligns with real-world needs and is worth studying.\n- S3: The fine-tuned models VisCoder2 show good performance in terms of execution pass rate."}, "weaknesses": {"value": "- W1: The dataset is curated by combining existing datasets with filtering and cleaning, so the dataset contribution feels limited.\n- W2: The paper evaluates different coding models (mostly scaling within the same model family). Including a wider range of models (e.g., GPT-5 or reasoning-oriented models) would make the comparisons more informative; scaling size alone is not very insightful, as larger models usually perform better.\n- W3: More discussion is needed on the evaluation protocol (Section 3.4). Evaluating visual outputs semantically is inherently challenging, as it requires judging whether the generated image aligns with the textual task. Therefore, the key metrics -- Task Score  and Visual Score -- are important. However, both are judged using an LLM-as-a-judge, whose reliability is neither validated nor justified in this benchmark. Can the LLM accurately evaluate these metrics? If yes, to what extent? A more detailed analysis is needed.\n- W4: The paper’s results rely heavily on the execution pass rate, which is not highly informative. Execution success only indicates that the code runs without errors, but one could “cheat” by outputting syntactically valid yet semantically incorrect code. This makes the analysis less convincing. More discussion and results based on the Task Score and Visual Score would make the evaluation more meaningful.\n- W5: It would be valuable to analyze why other models (e.g., GPT-4.1) fail and to explain how fine-tuning helps reduce specific types of errors. Which errors does fine-tuning effectively mitigate, and which remain challenging? What insights can be drawn for practitioners to improve visualization-coding models?\n- W6: The author claims that they are building \"Coding Agents\". But there is no real autonomous loop, planning, or tool integration -- only self-debugging within a single model. So the use of \"agent\" feels somehow overclaimed.\n- W7: The paper lacks sufficient task statistics for the benchmark dataset (e.g., category distribution, code length distribution, difficulty distribution, etc.). Some statistics are provided, but more detailed and fine-grained statistics would strengthen the paper’s contribution as a benchmark paper."}, "questions": {"value": "- Line 427: \"Execution success is high across most models\" --> the execution success of which models?\n- Why does Qwen2.5-Coder-32B-Instruct perform worse than the 14B model in Table 2?\n- How are the training and testing data split? How do you guarantee the quality and independence of the training and testing sets?\n- Lines 396–398: What does \"LilyPond shows the largest gains on symbolic grammars\" mean? And what does “SVG exposes model-library sensitivity where semantic and perceptual signals diverge\" mean?\n- Regarding the fine-tuned models: how do they perform on other benchmarks? Does the model lose its capabilities on other types of tasks (e.g., MMMU, HumanEval)?\n- How does performance scale with the number of self-debugging rounds? Do more rounds lead to better performance? Is there a saturation point? More analysis would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "feqg85ZiXv", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_wBjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_wBjo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603278855, "cdate": 1761603278855, "tmdate": 1762927530959, "mdate": 1762927530959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose VisCoder2, a multi-language visualization coding agent designed to generate, execute, and iteratively correct visualization code. To support this, they introduce VisCode-Multi-679K, a large-scale dataset of executable visualization code and correction dialogues across twelve programming languages, and VisPlotBench, a diverse benchmark for systematic evaluation. Experiments show that VisCoder2 outperforms open-source baselines and matches the reliability of proprietary models like GPT-4.1, especially with self-debugging. These resources advance the development of practical, robust visualization coding agents."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* **Focus on an Important Problem:**\n\n  The paper targets automatic visualization code generation and correction, a task that is increasingly important for data analysis and reporting with LLMs but still lacks robust, multi-language solutions.\n\n* **High-Quality, Multi-Language Dataset:**\n\n  VisCode-Multi-679K covers twelve programming languages, including both popular and symbolic ones. The dataset is large-scale, contains only executable code, and includes multi-turn correction dialogues. These features make it valuable for developing and benchmarking robust visualization coding agents.\n\n* **Effective Application of Iterative Correction:**\n\n    The paper proposes an iterative correction approach that, while not highly novel (being similar to established program repair methods), is effective in practice. The results show significant improvements."}, "weaknesses": {"value": "* **Lack of Quantitative Dataset Analysis:**\n\n  The paper does not provide sufficient quantitative metrics about the dataset (such as error rates, diversity, redundancy, or semantic alignment). Without these, it is difficult to fully evaluate the dataset’s quality and practical value."}, "questions": {"value": "Please address my concern in the Quantitative Analysis. Thanks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NIk6osCnVF", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_ipL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_ipL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891370301, "cdate": 1761891370301, "tmdate": 1762927528981, "mdate": 1762927528981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisCoder2, a large-scale dataset and benchmark for multi-domain, multilingual visual code generation. It integrates code, visual plots, and textual prompts across 12 programming languages and multiple application domains (data visualization, UI generation, etc.). The benchmark aims to evaluate both code correctness and visual fidelity in multi-turn coding tasks. The dataset fills an important gap in current research, providing a broader and more diverse benchmark than existing ones like PandasPlotBench. Experiment results demonstrate the effectiveness of the proposed training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive dataset coverage: The dataset spans multiple languages and visual domains, filling a real void in existing benchmarks which are typically language- or task-limited.\n\n- Practical relevance: Targets the emerging need for models capable of generating visual outputs (plots, figures, GUIs) from natural-language instructions.\n\n- Readable and well-structured writing: The paper is easy to follow, with clear organization and accessible examples."}, "weaknesses": {"value": "- Weak multi-turn construction: The “multi-turn” setup seems artificial — simply mixing in existing conversational code data rather than building domain-specific, multimodal dialogues. This undermines the claimed contribution on multi-turn reasoning.\n\n- Incomplete evaluation coverage: Although 12 languages are used for training, only 8 are evaluated. The absence of results for the remaining languages leaves the dataset’s multilingual utility unverified.\n\n- Limited baseline comparison: The model is only evaluated on the proposed benchmark. There’s no comparison against other visual code generation benchmarks (e.g., PandasPlotBench), making it hard to assess generalization.\n\n- Unclear metrics: The paper introduces “visual” and “task” scores but fails to define how visual quality is assessed. For visual generation, perceptual and structural accuracy are as important as code execution success.\n\n- Missing analysis on debug capability: Given the inclusion of multi-turn data, it’s surprising that no explicit debugging evaluation is performed, nor any comparison to instruction-tuned models like Qwen2.5 Coder Instruct. This weakens claims of enhanced reasoning or debugging ability."}, "questions": {"value": "- How were the “visual” and “task” metrics computed? Are they human-judged, automatic, or hybrid?\n\n- What criteria guided the choice of 8 evaluation languages — were they the largest subsets, or randomly sampled?\n\n- How does the model perform on existing benchmarks like PandasPlotBench? Any transfer results?\n\n- Does the multi-turn data actually improve multimodal debugging? Or just the general debugging ability shared by regular code LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5Sb0nSswz", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_4iTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_4iTA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983484315, "cdate": 1761983484315, "tmdate": 1762927528617, "mdate": 1762927528617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisCoder2, a framework for building multi-language visualization coding agents. Specifically, the work contain three key components: VisCode-Multi-679K, a large-scale dataset of 679K executable visualization code–image pairs with multi-turn correction dialogues across 12 programming languages; VisPlotBench, a benchmark spanning 8 languages and 13 visualization categories, supporting both single-round and multi-round self-debug evaluation; VisCoder2, a family of open-source multi-language models trained on the above dataset."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The author provides a comprehensive dataset construction. They detail a well-structured pipeline including language filtering, runtime validation, and instruction synthesis. Each visualization sample is paired with rendered outputs and multi-turn feedback, ensuring executable, realistic supervision. Compared to the previous method, VisCode-Multi-679K supports 12 languages, which makes its coverage cover most languages.  \n2. The author provides extensive experiments; the proposed VisCoder surpasses open-source baselines by 10–15 points in execution pass rate and achieves parity with GPT-4.1 on several languages. The analysis of self-debug gains and error types (syntax vs. runtime vs. semantic) is particularly insightful.\n3. The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. The detail of self-debugging is missed. The paper demonstrates that iterative correction improves performance, but it does not deeply analyze how models leverage feedback logs. It is better to describe how the self-debug works. Moreover, the author shows that there are persistent failures in semantic and runtime errors. It is also better to provide an analysis of why self-debugging does not work on these cases rather than just saying it doesn't work.\n2.  The current evaluation is imbalanced. Execution pass rate dominates the quantitative results, whereas semantic accuracy and visual similarity (Task/Visual Scores) are only partially reported. I understand the space is limited, and the author provides full results in the appendix. However, compared to the execution pass rate, whether the generated results are correct is more important. If possible, it is better to manually check the results and report the correctness for every model. \n3. Both the training corpus (VisCoder) and the evaluation benchmark (VisPlotBench) are constructed by the authors and share the same generation–execution–debug protocol. The paper does not evaluate zero-shot transfer to unseen visualization libraries, natural user prompts, or noisy real-world code, leaving generalization ability uncertain, which limits the usage of the model. It is better to provide natural user test results."}, "questions": {"value": "1. The current LLMs like GPT-5 can also support image input. Is it possible to extend the current dataset? e.g., provide the image input and request code output. \n2.  It is better to provide the process of self-debug and better evaluation results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axI3jKVnz3", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_R3P7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_R3P7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055672550, "cdate": 1762055672550, "tmdate": 1762927528201, "mdate": 1762927528201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [5/5]"}, "comment": {"value": "(3) Mermaid shows notable gains across all metrics. For *GPT-4.1*: the Visual Score rises from 41 to 56, the Task Score from 57 to 77, and the proportion of high-scoring samples increases correspondingly. *VisCoder2* consistently outperforms *Qwen2.5-Coder* at the same scale, and both visual and task performance improve steadily with self-debug, reflecting that in languages with explicit graph structures and detailed runtime diagnostics, models can use feedback to refine both relational semantics and diagram layout. \n\n(4) Asymptote remains one of the most challenging languages. Although most models show some improvement in Task/Visual Score, the overall level remains significantly lower than in Python or Vega-Lite. For example, *VisCoder2-32B* increases its Task Score from 46 to 53 after self-debug, while the Visual Score only rises from 27 to 31 with almost no change in high-score proportions, indicating that models can more easily align symbolic task semantics yet struggle to reliably control geometric details and rendering behavior, leaving a substantial gap between semantic correctness and visual consistency. \n\n(5) In HTML, strong models such as *GPT-4.1* and *VisCoder2* already exhibit high initial Task/Visual Score, and self-debug leads only to moderate gains. For example, *GPT-4.1* increases its Visual Score from 48 to 51 and its Task Score from 64 to 68, while *VisCoder2-32B* increases its Visual Score from 43 to 44 and its Task Score from 61 to 62. Overall, HTML specifications are relatively “model-friendly” in terms of semantic alignment, and remaining discrepancies are more reflective of minor visual deviations introduced by layout strategies and default rendering behaviors rather than true semantic errors.\n\n**2. Effects and limitations of self-debug on Task/Visual Score.** \n\nAcross all eight languages, self-debug generally improves Task/Visual Score alongside execution reliability, with the clearest gains appearing in languages where models can leverage informative diagnostics. Python and Mermaid exemplify this trend: in Python, *VisCoder2-32B* improves from a Task Score of 56 to 68 and from a Visual Score of 49 to 58, while *GPT-4.1* in Mermaid rises from 57 to 77 (task) and 41 to 56 (visual). These cases show that when feedback exposes meaningful semantic or structural signals, models can refine both correctness and rendered appearance rather than merely repairing syntax. In contrast, LaTeX and Asymptote show limited visual improvement despite higher execution success (e.g., *VisCoder2-32B* increases visual only from 27 to 31), reflecting that symbolic or compiler-dependent failures often surface only as coarse parser messages, offering insufficient guidance for deeper refinement. SVG represents the opposite extreme: execution and Task Score increase slightly, yet Visual Score remain almost unchanged (*GPT-4.1* stays near 45; *VisCoder2-32B* increases only from 33 to 34), indicating that text-only feedback cannot reveal layout- or rendering-sensitive discrepancies. Taken together, these findings show that Task/Visual Score uncover a distinct layer of quality that execution alone cannot detect, and that the effectiveness of self-debug critically depends on how well feedback exposes semantically or visually meaningful error signals.\n\n**3. Cross-language relationship between Exec Pass and Task/Visual Score.**\n\nExamining all eight languages reveals two broad regimes. In Python, Vega-Lite, Mermaid, and HTML, execution success, task accuracy, and visual fidelity tend to rise together, indicating that most errors stem from structural or interface issues that, once resolved, directly translate into improved semantics and rendering; in these settings, execution pass rate is a reasonable proxy for overall quality. LaTeX and SVG, however, demonstrate clear decoupling: models often achieve much higher execution success without corresponding gains in semantic or visual alignment. For LaTeX, *GPT-4.1* reaches a 66.1% execution pass rate after self-debug, yet Task/Visual Score remain in the mid-fifties and twenty-to-forty ranges due to macro expansion and compile-time fragility. For SVG, execution frequently saturates while Visual Score remain around forty to fifty across models, reflecting rendering-library sensitivity that feedback cannot capture. Asymptote lies between these extremes: all three metrics remain low but tend to improve synchronously, highlighting the intrinsic difficulty of symbolic geometric drawing and that performance on this language is still far from saturated. Together, these cross-language patterns show where execution is informative of downstream quality and where Task/Visual Score provide essential complementary signals."}}, "id": "ZTykgKNzm9", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763453982221, "cdate": 1763453982221, "tmdate": 1763456071703, "mdate": 1763456071703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [4/5]"}, "comment": {"value": "3. **Characteristics of self-debug:** Across all models and languages, the first round of self-debug consistently yields the largest improvement, correcting the majority of failures caused by shallow issues such as missing syntax, mismatched parameters, or incorrect references. Starting from the second round, the rate of improvement drops sharply, and by the third round performance typically plateaus. This pattern indicates that the current feedback mechanism effectively exposes structural and interface-related errors, but provides limited signals for deeper semantic inconsistencies, complex symbolic dependencies, or issues tied to specific rendering or parsing processes. As a result, self-debug follows a stable “large first-round gains followed by diminishing returns” trajectory across the entire evaluation.\n4. **Failures remain across models:** Building upon the error categorization in **Section 4.3**, we further examined the final unsuccessful cases of *GPT-4.1* and *VisCoder2-32B* across eight visualization languages and found that the remaining failures fall into three representative patterns that are difficult for current self-debug mechanisms to resolve: (1) deep semantic inconsistencies, such as mismatched variable meanings, incorrect data relationships, or incoherent multi-step plotting logic, which rarely surface explicitly in execution logs and therefore prevent the model from identifying the true source of failure; (2) grammar- or compiler-dependent symbolic errors, including macro expansion, symbol binding, or scope-resolution failures in languages like LilyPond, Asymptote, and LaTeX, where parser messages tend to be generic and provide little actionable guidance; and (3) runtime-related behavioral errors, such as invoking rendering packages that were never loaded, calling APIs unsupported by the target language, or generating plotting logic that may trigger infinite loops or excessive resource consumption. Although these issues manifest as runtime failures, the execution feedback typically contains only coarse, non-localized error signals, making it difficult for the model to refine its output across self-debug rounds. Overall, the lack of explicit localization for semantic, symbolic, and runtime behavior-related errors constitutes the primary bottleneck behind the remaining unresolved cases.\n### 3.2 Task/Visual Score Analysis\nIn VisPlotBench, the execution pass rate and the Task/Visual Score are equally important evaluation dimensions. In **Section 4.2** of the main paper, we analyze three representative languages that highlight different behaviors and discuss the phenomena of execution–semantic mismatch, symbolic grammar gains, and rendering library sensitivity. To further address the reviewers’ questions regarding semantic correctness and visual alignment, we extend the analysis using the complete results provided in **Appendix H**, covering the remaining five languages and examining the changes in Task/Visual Score before and after self-debug as well as their overall relationship with execution pass rates.\n\n**1. Supplementary Task/Visual Score analysis for the remaining languages.**\n\n(1) For Python, we observe that from *Qwen2.5-Coder* to *VisCoder2*, and further with self-debug enabled, the mean Task/Visual Score, the proportion of high-quality samples, and the execution pass rate improve in a largely synchronized manner. For example, at the 32B scale, *VisCoder2-32B* increases its Visual Score from 49 to 58 and its Task Score from 56 to 68, with the proportion of samples scoring visual ≥ 75 rising from 42% to 46% and task ≥ 75 from 54% to 62%. This shows that execution improvements are primarily driven by joint gains in semantic alignment and visual quality rather than by shallow syntax repairs.\n\n(2) In Vega-Lite, strong models such as *GPT-4.1* and *VisCoder2* already approach saturated performance after self-debug, yet Task/Visual Score still show mild upward trends; for example, for *VisCoder2-32B* the Visual Score increases from 60 to 62 and the Task Score from 70 to 72. This suggests that in declarative languages with explicit rules and rich diagnostics, models already produce mostly correct specifications, and self-debug mainly handles long-tail issues."}}, "id": "cxowDGkZ7k", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763454026006, "cdate": 1763454026006, "tmdate": 1763456109789, "mdate": 1763456109789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [3/5]"}, "comment": {"value": "## 3. Deep Analysis of Self-Debug Behavior and Task/Visual Score\n### 3.1 Self-Debug Analysis\nIn **Appendix I**, we report the complete self-debug results of all models across the eight visualization languages. To address the reviewer’s concerns regarding how models behave during self-debug, we select four representative systems: *GPT-4.1*, *GPT-4.1-mini*, *Qwen2.5-Coder-32B-Instruct*, and our fine-tuned *VisCoder2-32B*, and provide a deeper analysis that combines language-specific characteristics with model behaviors. Tables 3, 4, and 5 present the corresponding results.\n\n### Table 3. Execution Pass Rate across Self-Debug Rounds (Python, Vega-Lite, LilyPond)\n|Model|**Python**||||**Vega-Lite**||||**LilyPond**||||\n|--------------------------|:---------:|:----:|:----:|:----:|:-------------:|:----:|:----:|:----:|:------------:|:----:|:----:|:----:|\n||Normal|R1|R2|R3|Normal|R1|R2|R3|Normal|R1|R2|R3|\n|GPT-4.1|64.3|75.0|81.6|84.2|84.5|95.4|96.1|96.1|45.5|58.2|61.8|65.5|\n|GPT-4.1-mini|64.8|73.5|79.1|80.6|84.5|95.4|96.9|96.9|22.2|37.0|50.0|57.4|\n||||||||||||||\n|Qwen2.5-Coder-32B-Ins.|50.5|70.9|78.1|79.1|83.0|87.6|89.9|89.9|30.9|40.0|43.6|43.6|\n|VisCoder2-32B|65.3|76.0|80.1|81.6|94.6|96.1|96.1|96.1|56.4|61.8|69.1|69.1|\n\n### Table 4. Execution Pass Rate across Self-Debug Rounds (Mermaid, SVG, LaTeX)\n|Model|**Mermaid**||||**SVG**||||**LaTeX**||||\n|--------------------------|:-----------:|:----:|:----:|:----:|:-------:|:----:|:----:|:----:|:---------:|:----:|:----:|:----:|\n||Normal|R1|R2|R3|Normal|R1|R2|R3|Normal|R1|R2|R3|\n|GPT-4.1|68.7|84.7|93.9|93.9|92.3|93.9|95.4|95.4|31.3|53.6|59.8|66.1|\n|GPT-4.1-mini|51.9|81.7|90.1|94.7|89.1|95.3|95.3|96.9|29.5|50.9|55.4|58.9|\n||||||||||||||\n|Qwen2.5-Coder-32B-Ins.|71.0|74.8|75.6|76.3|93.9|93.9|93.9|93.9|29.5|42.9|50.0|51.8|\n|VisCoder2-32B|87.0|89.3|90.1|90.1|81.5|84.6|86.2|86.2|42.9|55.4|59.8|61.6|\n\n### Table 5. Execution Pass Rate across Self-Debug Rounds (Asymptote, HTML)\n|Model|**Asymptote**||||**HTML**||||\n|--------------------------|:-------------:|:----:|:----:|:----:|:--------:|:----:|:----:|:----:|\n||Normal|R1|R2|R3|Normal|R1|R2|R3|\n|GPT-4.1|21.7|35.9|43.5|46.7|89.8|96.3|97.2|97.2|\n|GPT-4.1-mini|23.9|37.0|42.4|48.9|86.1|99.1|99.1|100.0|\n||||||||||||||\n|Qwen2.5-Coder-32B-Ins.|17.4|25.0|31.5|33.7|78.7|88.9|89.8|89.8|\n|VisCoder2-32B|58.7|68.5|71.7|71.7|91.7|92.6|93.5|93.5|\n\n1. **Effect of self-debug:** Self-debugging consistently improves execution reliability across all models and most languages. *GPT-4.1* and *GPT-4.1-mini* already exhibit the strongest cross-language performance at the initial generation stage, and self-debug further repairs the majority of syntax- and interface-related errors, allowing them to reach near-saturated execution rates in languages such as Python, Vega-Lite, and Mermaid. In contrast, *Qwen2.5-Coder-32B-Instruct* starts from a weaker baseline, but still benefits substantially from iterative correction. Our fine-tuned *VisCoder2-32B* achieves significantly higher initial and final execution success rates than the baseline in seven languages, with especially large gains in symbol-intensive languages such as LilyPond, Asymptote, and LaTeX, where self-debug brings it close to or even above the GPT models. Together, these results show that self-debug offers a robust, model-agnostic mechanism for correcting structural and shallow syntactic errors, and is a key driver of multi-language reliability.\n2. **Cross-language trends:** In declarative languages like Vega-Lite and HTML, clear structural rules and diagnostics allow most models to reach near-saturated execution within one or two rounds. In execution-driven languages such as Python and Mermaid, rich runtime diagnostics provide actionable signals that drive steady improvements across rounds. For symbolic or compiler-dependent languages (LilyPond, Asymptote, LaTeX), models fix shallow syntax early, but deeper semantic issues rarely surface in logs, so improvements taper off after the first round. For SVG, the rendering pipeline is sensitive to XML well-formedness but offers little semantic or layout feedback. Larger models generate more complex structures, making issues like unclosed tags or malformed attributes more common, and these are difficult to repair under weak feedback, limiting improvement."}}, "id": "jXMQt4YIgx", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763454044734, "cdate": 1763454044734, "tmdate": 1763456094991, "mdate": 1763456094991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [2/5]"}, "comment": {"value": "## 2. Additional Evaluation Results\n### 2.1 Results on PandasPlotBench\nTo address reviewers’ concerns regarding generalization beyond our proposed benchmark, we additionally evaluate *Qwen2.5-Coder*, *VisCoder*, *VisCoder2*, and proprietary models (*GPT-4.1* / *GPT-4.1-mini*) on PandasPlotBench, which covers Python-based visualization libraries including Matplotlib, Seaborn, and Plotly. Table 1 reports execution pass rate, Task Score, Visual Score, and the proportion of samples achieving a score ≥75.\n### Table 1: Results of VisCoder2 and Baselines on the PandasPlotBench Benchmark\n|Model|Matplotlib|||||Seaborn|||||Plotly|||||\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n||**ExecPass**|**Mean**||**Good(>=75)**||**ExecPass**|**Mean**||**Good(>=75)**||**ExecPass**|**Mean**||**Good(>=75)**||\n|||**visual**|**task**|**visual**|**task**||**visual**|**task**|**visual**|**task**||**visual**|**task**|**visual**|**task**|\n|GPT-4.1|94.3|75|88|69%|91%|93.7|72|86|68%|86%|76.6|61|67|58%|66%|\n|GPT-4.1+SelfDebug|**100**|**77**|**90**|70%|**94%**|98.9|**74**|**89**|**70%**|**90%**|**97.7**|**74**|**85**|**69%**|85%|\n|GPT-4.1-mini|94.3|74|86|71%|87%|92|71|83|64%|85%|70.9|55|62|51%|63%|\n|GPT-4.1-mini+SelfDebug|98.9|76|89|**73%**|91%|**100**|**74**|87|67%|**90%**|97.1|72|84|65%|**86%**|\n|||||||||||||||||\n|Qwen2.5-Coder-3B-Ins.|71.4|56|72|50%|**69%**|58.3|44|55|36%|51%|27.4|17|19|17%|18%|\n|VisCoder-3B|81.7|60|69|53%|**69%**|73.7|48|65|38%|61%|60.6|38|45|32%|44%|\n|VisCoder-3B+SelfDebug|85.1|60|70|53%|**69%**|**78.3**|48|**66**|37%|**62%**|**64.6**|40|48|34%|47%|\n|VisCoder2-3B|83.4|62|70|55%|**69%**|73.7|51|62|42%|56%|61.1|41|48|35%|45%|\n|VisCoder2-3B+SelfDebug|**86.3**|**63**|**71**|**56%**|**69%**|77.7|**53**|64|**43%**|58%|64|**43**|**52**|**37%**|**49%**|\n|||||||||||||||||\n|Qwen2.5-Coder-7B-Ins.|78.3|63|76|58%|75%|68.6|51|63|40%|62%|48|29|34|24%|31%|\n|VisCoder-7B|87.4|66|78|60%|80%|76.6|57|70|50%|68%|74.3|48|60|41%|61%|\n|VisCoder-7B+SelfDebug|91.4|67|**81**|**62%**|**83%**|90.3|62|**77**|51%|**75%**|81.7|51|65|44%|65%|\n|VisCoder2-7B|87.4|67|76|61%|78%|83.4|61|72|52%|70%|77.7|48|62|43%|63%|\n|VisCoder2-7B+SelfDebug|**92**|**69**|78|**62%**|80%|**93.7**|**64**|76|**53%**|**74%**|**87.4**|**53**|**68**|**47%**|**67%**|\n|||||||||||||||||\n|Qwen2.5-Coder-14B-Ins.|86.3|67|78|61%|78%|76.6|58|70|51%|67%|56|40|42|37%|39%|\n|VisCoder-14B|86.3|-|-|-|-|78.9|-|-|-|-|74.3|-|-|-|-|\n|VisCoder-14B+SelfDebug|93.7|-|-|-|-|92.6|-|-|-|-|93.1|-|-|-|-|\n|VisCoder2-14B|88|70|81|63%|80%|84|66|74|58%|71%|78.3|52|66|46%|65%|\n|VisCoder2-14B+SelfDebug|**94.3**|**71**|**83**|**65%**|**83**|**93.7**|**67**|**79**|**59%**|**78%**|**94.9**|**60**|**71**|**51%**|**70%**|\n\nAcross all three libraries, *VisCoder2* consistently outperforms the base *Qwen2.5-Coder* models on execution success as well as both semantic (task) and perceptual (visual) metrics. The gains further increase under the self-debug setting, where *VisCoder2-14B* achieves performance close to *GPT-4.1*. These results support the generalization capability of *VisCoder2* on unseen visualization benchmarks.\n## 2.2 Human-Eval\nWe further evaluate *Qwen2.5-Coder-Instruct* and the fine-tuned *VisCoder2* models on HumanEval and HumanEval+. Table 2 reports the Pass@1 results.\n### Table 2: Performance on HumanEval and HumanEval+\n|Model|Human-EvalPass@1|Human-Eval-PlusPass@1|\n|:---|:---:|:---:|\n|GPT-4.1|97|91.5|\n|GPT-4.1-mini|92.1|86.6|\n||||\n|Qwen2.5-Coder-3B-Ins.|84.8|79.9|\n|VisCoder2-3B|81.1|76.2|\n||||\n|Qwen2.5-Coder-7B-Ins.|91.5|84.8|\n|VisCoder2-7B|89|83.5|\n||||\n|Qwen2.5-Coder-14B-Ins.|92.1|86.6|\n|VisCoder2-14B|92.1|84.8|\n||||\n|Qwen2.5-Coder-32B-Ins.|90.9|85.4|\n|VisCoder2-32B|87.8|81.7|\n\nThe results show that *VisCoder2* exhibits only a modest 2–3 point decrease compared to the base *Qwen2.5-Coder* models on HumanEval/HumanEval+. This behavior aligns with the distributional differences between the two task families: *VisCoder2* is trained heavily on multi-language visualization code, whereas HumanEval focuses on algorithmic and data-structure problems. Such minor fluctuations are therefore expected and do not indicate systematic capability degradation. Overall, *VisCoder2* maintains stable general coding ability while achieving substantial gains on its target task of cross-language executable visualization code generation and multi-round self-debug."}}, "id": "d7m0bGO3j7", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763454067705, "cdate": 1763454067705, "tmdate": 1763454105024, "mdate": 1763454105024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [1/5]"}, "comment": {"value": "# General Response\nWe thank all reviewers for their constructive feedback. Below, we provide a consolidated General Response addressing the shared concerns across reviews, including clarifications of the VisPlotBench evaluation protocol, additional evaluations such as PandasPlotBench and HumanEval, and deeper analysis and insights from our experimental results.\n## 1. VisPlotBench Evaluation Setting\n### 1.1 Self-Debug Evaluation Protocol\nIn VisPlotBench, we adopt the same self-debug evaluation mode used in VisCoder to simulate a realistic developer-style debugging workflow. In this setting, if the model’s initial code generation fails to execute or does not produce a valid plot, the model is given up to K rounds to iteratively refine its output based on feedback from the previous attempt.\nIn each round, only the tasks that remain unsolved from the previous iteration are reconsidered. The model receives a multi-turn conversational prompt consisting of (i) the original natural-language instruction, (ii) the previously generated code that failed, and (iii) the feedback derived from the execution error. Based on this dialogue history, the model produces a revised version of the code. If the revised code executes successfully and generates a valid plot, the task is marked as solved and excluded from further rounds; otherwise, the latest failed output is recorded and carried forward to the next iteration.\n```\nLet F₀ be the set of failed tasks from the initial evaluation.\nFor i = 1 to K:\n  For each task x in Fᵢ₋₁ that is not yet fixed:\n    1. Generate a revised version of x using feedback-driven prompting.\n    2. Execute the revised code.\n    3. If execution succeeds:\n         - Mark x as fixed and record the successful output.\n       Else:\n         - Record the latest failed output for x.\nEvaluate every task using its final recorded output.\n```\nIn all experiments, we set the maximum number of rounds to K = 3. After all rounds are completed, each task is evaluated using its latest recorded output (either the successfully corrected code from an earlier round or the final failed attempt) using the same evaluation pipeline as in the initial pass.\nThis iterative mechanism mirrors the common “generate–execute–repair” workflow and provides a standardized way to evaluate how models recover from different error types across programming languages.\n### 1.2 Task and Visual Score Metrics\nIn VisPlotBench, we follow the scoring procedure introduced in PandasPlotBench, and the judge prompts are provided in **Appendix F.2**. The core idea is to use a GPT model to compare the ground-truth image and the model-rendered image within the context of the task description. For the Task Score, the judge compares the generated plot against the task instruction; for the Visual Score, the judge compares the generated plot against the ground-truth reference image. The Task and Visual Score metrics are described in detail in the PandasPlotBench paper."}}, "id": "OpFt18DmhJ", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763454129390, "cdate": 1763454129390, "tmdate": 1763454129390, "mdate": 1763454129390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}