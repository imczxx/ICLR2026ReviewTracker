{"id": "4zoMnmZzh4", "number": 17682, "cdate": 1758279185177, "mdate": 1759897160382, "content": {"title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "abstract": "Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. **VisCode-Multi-679K** is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. **VisPlotBench** is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present **VisCoder2**, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching **82.4%** overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.", "tldr": "", "keywords": ["Code Models", "Visualization", "Fine-tuning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef1f7f10f7a8df66dbf608f4d8b559b12d66b107.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the task of generating visualization code from natural language instructions. It introduces VisCode-Multi-679K, a large multi-language dataset containing executable code-image pairs and multi-turn correction dialogues; VisPlotBench, a benchmark for evaluation; and VisCoder2, a fine-tuned model based on Qwen2.5-Coder. Experiments show that VisCoder2 outperforms open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: A large multi-language dataset and a benchmark are introduced; both are useful to the community.\n- S2: The overall task setting aligns with real-world needs and is worth studying.\n- S3: The fine-tuned models VisCoder2 show good performance in terms of execution pass rate."}, "weaknesses": {"value": "- W1: The dataset is curated by combining existing datasets with filtering and cleaning, so the dataset contribution feels limited.\n- W2: The paper evaluates different coding models (mostly scaling within the same model family). Including a wider range of models (e.g., GPT-5 or reasoning-oriented models) would make the comparisons more informative; scaling size alone is not very insightful, as larger models usually perform better.\n- W3: More discussion is needed on the evaluation protocol (Section 3.4). Evaluating visual outputs semantically is inherently challenging, as it requires judging whether the generated image aligns with the textual task. Therefore, the key metrics -- Task Score  and Visual Score -- are important. However, both are judged using an LLM-as-a-judge, whose reliability is neither validated nor justified in this benchmark. Can the LLM accurately evaluate these metrics? If yes, to what extent? A more detailed analysis is needed.\n- W4: The paper’s results rely heavily on the execution pass rate, which is not highly informative. Execution success only indicates that the code runs without errors, but one could “cheat” by outputting syntactically valid yet semantically incorrect code. This makes the analysis less convincing. More discussion and results based on the Task Score and Visual Score would make the evaluation more meaningful.\n- W5: It would be valuable to analyze why other models (e.g., GPT-4.1) fail and to explain how fine-tuning helps reduce specific types of errors. Which errors does fine-tuning effectively mitigate, and which remain challenging? What insights can be drawn for practitioners to improve visualization-coding models?\n- W6: The author claims that they are building \"Coding Agents\". But there is no real autonomous loop, planning, or tool integration -- only self-debugging within a single model. So the use of \"agent\" feels somehow overclaimed.\n- W7: The paper lacks sufficient task statistics for the benchmark dataset (e.g., category distribution, code length distribution, difficulty distribution, etc.). Some statistics are provided, but more detailed and fine-grained statistics would strengthen the paper’s contribution as a benchmark paper."}, "questions": {"value": "- Line 427: \"Execution success is high across most models\" --> the execution success of which models?\n- Why does Qwen2.5-Coder-32B-Instruct perform worse than the 14B model in Table 2?\n- How are the training and testing data split? How do you guarantee the quality and independence of the training and testing sets?\n- Lines 396–398: What does \"LilyPond shows the largest gains on symbolic grammars\" mean? And what does “SVG exposes model-library sensitivity where semantic and perceptual signals diverge\" mean?\n- Regarding the fine-tuned models: how do they perform on other benchmarks? Does the model lose its capabilities on other types of tasks (e.g., MMMU, HumanEval)?\n- How does performance scale with the number of self-debugging rounds? Do more rounds lead to better performance? Is there a saturation point? More analysis would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "feqg85ZiXv", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_wBjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_wBjo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603278855, "cdate": 1761603278855, "tmdate": 1762927530959, "mdate": 1762927530959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose VisCoder2, a multi-language visualization coding agent designed to generate, execute, and iteratively correct visualization code. To support this, they introduce VisCode-Multi-679K, a large-scale dataset of executable visualization code and correction dialogues across twelve programming languages, and VisPlotBench, a diverse benchmark for systematic evaluation. Experiments show that VisCoder2 outperforms open-source baselines and matches the reliability of proprietary models like GPT-4.1, especially with self-debugging. These resources advance the development of practical, robust visualization coding agents."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* **Focus on an Important Problem:**\n\n  The paper targets automatic visualization code generation and correction, a task that is increasingly important for data analysis and reporting with LLMs but still lacks robust, multi-language solutions.\n\n* **High-Quality, Multi-Language Dataset:**\n\n  VisCode-Multi-679K covers twelve programming languages, including both popular and symbolic ones. The dataset is large-scale, contains only executable code, and includes multi-turn correction dialogues. These features make it valuable for developing and benchmarking robust visualization coding agents.\n\n* **Effective Application of Iterative Correction:**\n\n    The paper proposes an iterative correction approach that, while not highly novel (being similar to established program repair methods), is effective in practice. The results show significant improvements."}, "weaknesses": {"value": "* **Lack of Quantitative Dataset Analysis:**\n\n  The paper does not provide sufficient quantitative metrics about the dataset (such as error rates, diversity, redundancy, or semantic alignment). Without these, it is difficult to fully evaluate the dataset’s quality and practical value."}, "questions": {"value": "Please address my concern in the Quantitative Analysis. Thanks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NIk6osCnVF", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_ipL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_ipL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891370301, "cdate": 1761891370301, "tmdate": 1762927528981, "mdate": 1762927528981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisCoder2, a large-scale dataset and benchmark for multi-domain, multilingual visual code generation. It integrates code, visual plots, and textual prompts across 12 programming languages and multiple application domains (data visualization, UI generation, etc.). The benchmark aims to evaluate both code correctness and visual fidelity in multi-turn coding tasks. The dataset fills an important gap in current research, providing a broader and more diverse benchmark than existing ones like PandasPlotBench. Experiment results demonstrate the effectiveness of the proposed training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive dataset coverage: The dataset spans multiple languages and visual domains, filling a real void in existing benchmarks which are typically language- or task-limited.\n\n- Practical relevance: Targets the emerging need for models capable of generating visual outputs (plots, figures, GUIs) from natural-language instructions.\n\n- Readable and well-structured writing: The paper is easy to follow, with clear organization and accessible examples."}, "weaknesses": {"value": "- Weak multi-turn construction: The “multi-turn” setup seems artificial — simply mixing in existing conversational code data rather than building domain-specific, multimodal dialogues. This undermines the claimed contribution on multi-turn reasoning.\n\n- Incomplete evaluation coverage: Although 12 languages are used for training, only 8 are evaluated. The absence of results for the remaining languages leaves the dataset’s multilingual utility unverified.\n\n- Limited baseline comparison: The model is only evaluated on the proposed benchmark. There’s no comparison against other visual code generation benchmarks (e.g., PandasPlotBench), making it hard to assess generalization.\n\n- Unclear metrics: The paper introduces “visual” and “task” scores but fails to define how visual quality is assessed. For visual generation, perceptual and structural accuracy are as important as code execution success.\n\n- Missing analysis on debug capability: Given the inclusion of multi-turn data, it’s surprising that no explicit debugging evaluation is performed, nor any comparison to instruction-tuned models like Qwen2.5 Coder Instruct. This weakens claims of enhanced reasoning or debugging ability."}, "questions": {"value": "- How were the “visual” and “task” metrics computed? Are they human-judged, automatic, or hybrid?\n\n- What criteria guided the choice of 8 evaluation languages — were they the largest subsets, or randomly sampled?\n\n- How does the model perform on existing benchmarks like PandasPlotBench? Any transfer results?\n\n- Does the multi-turn data actually improve multimodal debugging? Or just the general debugging ability shared by regular code LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5Sb0nSswz", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_4iTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_4iTA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983484315, "cdate": 1761983484315, "tmdate": 1762927528617, "mdate": 1762927528617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisCoder2, a framework for building multi-language visualization coding agents. Specifically, the work contain three key components: VisCode-Multi-679K, a large-scale dataset of 679K executable visualization code–image pairs with multi-turn correction dialogues across 12 programming languages; VisPlotBench, a benchmark spanning 8 languages and 13 visualization categories, supporting both single-round and multi-round self-debug evaluation; VisCoder2, a family of open-source multi-language models trained on the above dataset."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The author provides a comprehensive dataset construction. They detail a well-structured pipeline including language filtering, runtime validation, and instruction synthesis. Each visualization sample is paired with rendered outputs and multi-turn feedback, ensuring executable, realistic supervision. Compared to the previous method, VisCode-Multi-679K supports 12 languages, which makes its coverage cover most languages.  \n2. The author provides extensive experiments; the proposed VisCoder surpasses open-source baselines by 10–15 points in execution pass rate and achieves parity with GPT-4.1 on several languages. The analysis of self-debug gains and error types (syntax vs. runtime vs. semantic) is particularly insightful.\n3. The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. The detail of self-debugging is missed. The paper demonstrates that iterative correction improves performance, but it does not deeply analyze how models leverage feedback logs. It is better to describe how the self-debug works. Moreover, the author shows that there are persistent failures in semantic and runtime errors. It is also better to provide an analysis of why self-debugging does not work on these cases rather than just saying it doesn't work.\n2.  The current evaluation is imbalanced. Execution pass rate dominates the quantitative results, whereas semantic accuracy and visual similarity (Task/Visual Scores) are only partially reported. I understand the space is limited, and the author provides full results in the appendix. However, compared to the execution pass rate, whether the generated results are correct is more important. If possible, it is better to manually check the results and report the correctness for every model. \n3. Both the training corpus (VisCoder) and the evaluation benchmark (VisPlotBench) are constructed by the authors and share the same generation–execution–debug protocol. The paper does not evaluate zero-shot transfer to unseen visualization libraries, natural user prompts, or noisy real-world code, leaving generalization ability uncertain, which limits the usage of the model. It is better to provide natural user test results."}, "questions": {"value": "1. The current LLMs like GPT-5 can also support image input. Is it possible to extend the current dataset? e.g., provide the image input and request code output. \n2.  It is better to provide the process of self-debug and better evaluation results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axI3jKVnz3", "forum": "4zoMnmZzh4", "replyto": "4zoMnmZzh4", "signatures": ["ICLR.cc/2026/Conference/Submission17682/Reviewer_R3P7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17682/Reviewer_R3P7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055672550, "cdate": 1762055672550, "tmdate": 1762927528201, "mdate": 1762927528201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}