{"id": "dAlewVZMIt", "number": 19229, "cdate": 1758294618631, "mdate": 1759897051223, "content": {"title": "SimPlex-GT: A Simple Node-to-Cluster Graph Transformer for synergizing homophily and heterophily in Complex Graphs", "abstract": "Graph neural networks (GNNs) have proven effective on homophilic graphs, where connected nodes share similar features. However, real-world graphs often exhibit mixed patterns including heterophily, where connected nodes differ significantly. Traditional GNNs struggle with such cases due to their inherent smoothing operations. To address this limitation, we propose SimPlex-GT, a novel Graph Transformer (GT) model  that synergizes homophilic and heterophilic patterns by integrating local GNN message passing with a novel global node-to-cluster (N2C) attention mechanism. Our approach disentangles node representations into local and global components: local features model neighborhood similarity, while global features attend to dynamic cluster prototypes learned on the fly. A learnable gating mechanism fuses these complementary views, and an orthogonality constraint encourages representational diversity.\nSimPlex-GT is trained under a self-supervised teacher–student architecture where the teacher sees the full graph and the student learns from masked inputs, with alignment enforced in a joint latent space. A dynamic masking strategy further emphasizes difficult nodes, based on prediction discrepancies. Comprehensive theoretical analysis demonstrates its strong capability, and extensive evaluations across 11 benchmark datasets show that SimPlex-GT achieves state-of-the-art performance on heterophilic graphs and remains highly competitive on homophilic graphs, all with superior computational efficiency.", "tldr": "", "keywords": ["Graph neural networks", "Homophilic and Heterophilic", "Self Supervise Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de806635c5cf9fc1e6109ab3ed2ccde43cbea79d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SimPlex-GT is Graph Transformer designed to synergize homophilic and heterophilic patterns in complex graphs. It integrates local GCN-based message passing with a sparse node-to-cluster (N2C) attention mechanism, fuses local/global features via complementary filtering or cluster smoothing, and adopts a self-supervised teacher–student framework with dynamic masking. Evaluated on 11 benchmarks, it achieves state-of-the-art performance on heterophilic graphs, remains competitive on homophilic ones, and offers superior computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Effectively handles both homophily (via GCN) and heterophily (via N2C attention) without specializing in either.\n\n2.N2C attention reduces complexity from O(N^2) to near-linear, enabling scalability for large graphs.\n\n3.Outperforms existing GNNs and GTs on heterophilic datasets (e.g., Texas, Chameleon) while matching top results on homophilic ones (e.g., Cora, PubMed)."}, "weaknesses": {"value": "1.The performance of the proposed model is slightly sensitive to the number of clusters. How should this parameter be tuned for optimal results?\n\n2.From the experimental results, most Graph Transformers perform worse than traditional GNNs. Does this suggest that designing on GTs is not meaningful? Meanwhile, traditional GNNs already have extensive work addressing both homophilic and heterophilic graphs. This gives the impression that improving GTs for handling homophily and heterophily may be unnecessary.\n\n3.Figure 2 does not seem to convey much information.\n\n4. In Equation 14, what does LN represent? Using $\\mathcal{G}$ to denote the representation here can be misleading."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ou1cjsVbJz", "forum": "dAlewVZMIt", "replyto": "dAlewVZMIt", "signatures": ["ICLR.cc/2026/Conference/Submission19229/Reviewer_msha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19229/Reviewer_msha"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807790882, "cdate": 1761807790882, "tmdate": 1762931212278, "mdate": 1762931212278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional Graph Neural Networks (GNNs) struggle to handle graph structures under mixed patterns due to their inherent smoothing operations. To address this, the paper proposes SimPlex-GT, a novel graph transformer model. This model utilizes traditional local Graph Convolutional Networks (GCNs) for message passing to tackle homogeneity issues. Simultaneously, it designs complementary filtering and clustering smoothing mechanisms, and on this basis, constructs enhanced node attention mechanisms as well as global node-to-cluster attention mechanisms to deal with heterogeneity problems. SimPlex-GT can effectively process complex structural patterns.\nIn terms of training methods, SimPlex-GT adopts a self-supervised learning framework, with masked node modeling serving as the primary proxy objective, and employs a teacher-student prediction architecture. Additionally, it introduces a node-difficulty-driven dynamic masking strategy. This strategy can adaptively adjust the masking process, enabling the model to learn more robust and information-rich representations.\nThrough comprehensive theoretical analysis and outstanding empirical performance in terms of efficiency, the authors demonstrate that SimPlex-GT maintains a high level of competitiveness on homogeneous graphs in benchmark datasets, while also improving memory and training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this article demonstrates strong innovation by proposing novel designs: complementary filtering and clustering smoothing. In complementary filtering, low-frequency information serves as the prototype, while high-frequency signals act as effective queries. By introducing graph structure awareness into the input (achieved through complementary filtering) or output (accomplished through clustering smoothing) of the node-to-cluster module, this design leverages the optimal characteristics of the two proposed models to collaboratively handle homogeneity and heterogeneity issues.\nThe local feature model focuses on neighborhood similarity, whereas the global feature model centers on dynamically learned clustering prototypes. A learnable gating mechanism integrates these complementary perspectives, and orthogonality constraints encourage diversity in representations. This design utilizes the collaborative modeling capabilities of the two branches to generate more robust node representations. It adopts masked node modeling as the primary proxy objective and employs a teacher-student prediction architecture.\nAdditionally, the paper balances task difficulty and data diversity by designing difficulty levels, maintaining a base masking rate, and preventing biased sampling."}, "weaknesses": {"value": "When designing the two models to synergize homogeneity and heterogeneity, the authors took model simplicity as the starting point and opted for single-layer Graph Convolutional Networks (GCNs). Occasionally, the authors also mentioned that stacking multiple modules could enhance the model's performance. Therefore, it seems that improving model performance was not the primary consideration in model construction. Moreover, some variables or operations in the formulas, such as those in the gating mechanism, do not appear to have detailed explanations regarding their specific solutions or implementations."}, "questions": {"value": "1）The paper employs single-layer GCNs in multiple places. For instance, 1) When designing complementary filtering, the author divides nodes into two complementary channels through a low-pass filter, which is implemented and approximated by a single-layer GCN. Can such a simple single-layer filter achieve the desired effect? 2） When using the graph structure to address homogeneity issues, a single-layer GCN branch is introduced as a residual path to capture this information and is integrated into the heterogeneous target N2C output. Can this single-layer GCN effectively capture the information? Is the model too simplistic?\n2）In lines 290 - 294, the author states, \"Note that, similar to other baselines (Rampášek et al., 2022), above designs can be viewed as a building block in our framework, and multiple blocks can be easily stacked to enhance the model’s expressive power. In our experiments, we retain a single block for simplicity.\" The author mentions that multiple blocks can be easily stacked to enhance the model's expressive power. Is it inappropriate to forgo enhancing the model's expressive power for the sake of simplicity?\n3）The author mentions in the node-to-cluster (N2C) attention model the concepts of meaningful attention joint clustering and compact attention joint clustering. How can we determine whether an attention is meaningful and compact?\n4）The teacher's output S(v;φ) and the student's output T(v;ψ) are not provided in the paper.\n5）The author refers to it as \"smart cosine similarity,\" but from the formula, it just seems to be an ordinary cosine similarity function with λorth as a balancing parameter. Where does the \"smart\" aspect come into play?\n6）The paper repeatedly mentions that orthogonality regularization promotes representational diversity. For example, an auxiliary orthogonality regularization term is introduced to encourage the two branches to learn mutually enhancing features. Specifically, which entities are subject to the orthogonality constraints?\n7）How is the gating mechanism implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nuqUrk9MkB", "forum": "dAlewVZMIt", "replyto": "dAlewVZMIt", "signatures": ["ICLR.cc/2026/Conference/Submission19229/Reviewer_jBL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19229/Reviewer_jBL4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998527578, "cdate": 1761998527578, "tmdate": 1762931211888, "mdate": 1762931211888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a Graph Transformer architecture called SimPlex-GT, which aims to provide a unified representation learning framework for complex graphs with both homophilic and heterophilic properties. The core contributions of the paper are threefold: 1) A scalable, linear complexity \"Node-to-Cluster (N2C)\" attention mechanism that approximates global attention by allowing nodes to focus on a set of dynamically learned cluster prototypes; 2) Two theoretically motivated synergy mechanisms (Complementary Filtering (CF) and Cluster Smoothing (CS)) to fuse local information from the GCN branch with global information from N2C attention; 3) A novel self-supervised learning paradigm that employs a difficulty-driven dynamic masking strategy under a teacher-student framework. The authors demonstrate the effectiveness and efficiency of this approach through theoretical analysis and extensive experiments on 11 benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Overall, the presentations are clear and easy to understand their framework and results.\n2.For each core design (N2C, CF, CS), the paper provides corresponding theoretical support (Theorems 1-4), increasing the credibility of the method and clearly explaining its underlying working principles (e.g., the variance reduction property of N2C).\n3.The experimental section is well-structured, with comprehensive benchmarking on multiple datasets. The detailed ablation studies (Tables 4, 5, 6, 7, 8) systematically validate the necessity and effectiveness of each component, making the experimental conclusions reliable."}, "weaknesses": {"value": "1.The SOTA performance achieved by the model is the result of both the novel architecture (N2C+CF/CS) and its powerful self-supervised training strategy (dynamic masking). The ablation study (Table 5) does not address the collinearity between the training strategy and N2C in terms of their impact on performance.\n2.In the methods section, the authors position the SimPlex-GT module as a general building block that can be \"easily stacked to enhance model expressiveness\" (Page 6, Lines 289-291). However, in the experimental section, they explicitly state, \"For simplicity, we only kept a single module.\" Given the well-known depth bottleneck in the Graph Transformer field (i.e., stacking multiple layers can lead to performance degradation), validating this claim in a single-layer setup weakens the experimental support for the \"stackability\" assertion.\n3.One of the motivations for introducing N2C attention is the potential use of \"hierarchical node structures\" within the graph (Page 3, Lines 160-161). This is a strong entry point, but in the subsequent theoretical and experimental analysis, the authors do not revisit this idea. The theoretical analysis primarily focuses on variance reduction rather than hierarchical representation capability (Theorems 2 and 4). As a result, the initial motivation does not fully close the loop in the final analysis."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0POrp1V9tg", "forum": "dAlewVZMIt", "replyto": "dAlewVZMIt", "signatures": ["ICLR.cc/2026/Conference/Submission19229/Reviewer_XN1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19229/Reviewer_XN1c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002427492, "cdate": 1762002427492, "tmdate": 1762931211522, "mdate": 1762931211522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SimPlex-GT, a simple yet efficient graph Transformer framework that effectively synergizes homogeneous and heterogeneous structures in graphs by combining local GNN message passing with global node-cluster attention mechanisms. Trained under a self-supervised teacher-student framework and employing a dynamic masking strategy to focus on challenging nodes, the model achieves efficient, stable, and state-of-the-art node representation learning across diverse graph datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a graph transformer model that attempts to address the challenges of heterogeneous graphs and reduce time complexity. Experimental results indicate that this approach appears to be effective."}, "weaknesses": {"value": "The paper presents numerous theorems, but many of them are problematic. \n\n1. The theoretical analysis in the paper primarily relies on an overly strong assumption: that node features can be directly aligned with labels. Introducing this assumption appears to detach the entire problem from its graph-based nature. Under this assumption, logistic regression becomes sufficient to address these issues.\n2. Certain assumptions were not included in the theorem statement but were utilized in the proof. This renders the theorem either over-claimed or incorrect. For example, in the proof of Theorem 1, the author assumes that the activation of the GNN is linear and the weights are the identity matrix (Line 761), yet this assumption is not stated in the theorem itself. By the way, this assumption will make a GNN even weaker than a logistic regression.\n3. In practice, GNNs can achieve classification performance on heterogeneous graphs that surpasses the bound proposed by the authors, thereby reducing the validity of their theorem.\n4. Theorem 1 analyzes what appears to be GCNs rather than GNNs. Many simple GNNs can achieve performance comparable to ground truth under this theoretical framework, such as GAT.\n\nMinors:\n1. This method does not appear to be applicable to graphs that cannot be clustered.\n2. No code was provided for reviewers to examine.\n \nGiven that the theoretical section constitutes a significant portion of the paper and the potential issues it may contain, I believe this paper is not ready for publication."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zKHyBTvTDV", "forum": "dAlewVZMIt", "replyto": "dAlewVZMIt", "signatures": ["ICLR.cc/2026/Conference/Submission19229/Reviewer_vMFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19229/Reviewer_vMFh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112751567, "cdate": 1762112751567, "tmdate": 1762931211069, "mdate": 1762931211069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}