{"id": "bld5GVRad0", "number": 25243, "cdate": 1758365670033, "mdate": 1759896728104, "content": {"title": "InfoBlend: Storing and Reusing KV Caches of Multimodal Information without Positional Restriction", "abstract": "The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named InfoBlend, to address both system-level and algorithm-level challenges. InfoBlend stores the KV cache on local disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated the integrated reuse and recompute mechanism within the system. The experimental results demonstrate that InfoBlend can achieve up to 54\\% reduction in response time and 2$\\times$ improvement in throughput compared to existing context caching systems, while maintaining negligible or no accuracy loss.", "tldr": "The KV cache can be reused without positional restriction, through partial recomputation.", "keywords": ["Multimodal Large Language Model", "AI System", "Position-Independent Caching"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0d06f97f9c6ad80b2f9594560c7ee8d676679a3.pdf", "supplementary_material": "/attachment/7c1cdb642495362f339e608259469e73183144d6.zip"}, "replies": [{"content": {"summary": {"value": "1.  **System Design:** Stores KV caches on disk, loads/computes them in parallel during inference, and uses static/dynamic libraries for different cache types (user uploads vs. MRAG content).\n2.  **Selective Attention Mechanism:** To mitigate accuracy loss from reusing positionally shifted KV caches, INFOBLEND recomputes all text tokens and the first 'k' tokens of each multimodal input's KV cache (INFOBLEND-k). This is based on the insight that initial tokens act as \"attention sinks\" and their KV values are most affected by positional changes. This is implemented efficiently as a single-step prefill process using a \"dummy cache\" technique."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Problem Significance:** Addresses a clear and growing bottleneck (prefill latency) in MLLM serving, particularly for complex multimodal inputs like interleaved documents or MRAG results, which hinders user experience and scalability.\n2.  **Novel Approach:** Proposes position-independent caching (INFOBLEND) for MLLMs, moving beyond the limitations of standard prefix caching. This is a necessary step for efficiently handling dynamic multimodal contexts.\n3.  **Insightful Heuristic:** The selective attention mechanism (recomputing the first k image tokens) is well-motivated by empirical analysis of attention patterns (sparsity, initial token sinks) in MLLMs. This provides a principled way to trade off computation and accuracy.\n4.  **Practical System Design:** Considers system-level challenges like large cache sizes (disk storage) and latency hiding (parallel loading/computation). The single-step prefill implementation using dummy caches enhances efficiency over two-step approaches.\n5.  **Strong Empirical Results:** Demonstrates significant improvements in TTFT (up to 54.1% reduction) and throughput (up to 2.0x increase) compared to prefix caching and CacheBlend, while maintaining relatively high generation quality."}, "weaknesses": {"value": "1.  **Accuracy Trade-off:** While INFOBLEND significantly improves quality over \"full reuse\", there is still an accuracy degradation compared to \"prefix caching\" (up to 13.6% score loss mentioned, though Figure 9 suggests the best INFOBLEND variants are closer). The paper could discuss the practical implications of this trade-off more – is this level of score drop acceptable for production systems?\n2.  **Heuristic Generalizability:** The strategy of recomputing the first 'k' tokens is based on observations in LLaVA models. While plausible, its universal applicability across different MLLM architectures (e.g., those with different vision encoders or cross-attention mechanisms) or modalities (video, audio) is not fully validated.\n3.  **GPT Score Reliability:** The evaluation relies on GPT scores for generation quality. While common, this metric can be noisy and potentially biased. Including human evaluations or other automated metrics (e.g., ROUGE, BLEU for relevant tasks, or specific VQA accuracy if applicable) could strengthen the quality assessment.\n4.  **Comparison Details:** The comparison with CacheBlend mentions CacheBlend focuses on KV deviation while INFOBLEND incorporates attention insights. A more direct comparison of *why* INFOBLEND's heuristic is better (e.g., does KV deviation correlate poorly with attention sinks?) would be insightful."}, "questions": {"value": "1.  Regarding the accuracy trade-off: Section 5.2 mentions up to 13.6% score loss for INFOBLEND-32 vs. prefix caching, but Figure 9 seems to show INFOBLEND variants achieving scores very close to prefix caching. Could you clarify the extent of the quality degradation for the optimal INFOBLEND-k variants and discuss the scenarios where this trade-off might be more or less acceptable?\n2.  How was the value of 'k' (the number of initial image tokens to recompute) determined for INFOBLEND-k? Is there a principled way to select 'k', perhaps dynamically based on the model, modality, or context length, rather than using fixed values (4, 8, 16, 32, 64)? How sensitive are TTFT and quality to the choice of 'k'?\n3.  The analysis focuses on image tokens. Does the \"attention sink\" phenomenon and the effectiveness of recomputing initial tokens apply similarly to other modalities like video frames or audio chunks when used in an interleaved manner?\n4.  Could you elaborate on why recomputing the first k tokens based on attention sink insights leads to better performance (both speed and quality) compared to recomputing tokens based on KV deviation, as done in CacheBlend?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RObFHHAPBq", "forum": "bld5GVRad0", "replyto": "bld5GVRad0", "signatures": ["ICLR.cc/2026/Conference/Submission25243/Reviewer_aAof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25243/Reviewer_aAof"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578969818, "cdate": 1761578969818, "tmdate": 1762943378653, "mdate": 1762943378653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "INFOBLEND enables position independent kv cache reuse for multimodal LLMs. While previous work like, CacheBlend recomputes tokens with the largest KV deviation, INFOBLEND also considers attention importance to selectively recompute only critical tokens, mainly at the start of image sequences. INFOBLEND uses a single-step selective attention process instead of CacheBlend’s two-step recomputation, supports parallel KV loading for multimodal data, and achieves higher throughput and lower latency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Well motivated problem about multimodal KV cache reuse. \n2. Good results on 4 model-dataset pairs, comparison with previous work.\n3. Support for multimodal by extending into disk storage and related system optimizations around them.\n4. Novel combination of attention sink and kv deviation."}, "weaknesses": {"value": "1. Comparison between this and CacheBlend not clear enough. \n2. Not sure what is new about multimodal. The delta mentioned with previous work is that there are attention sink considerations in infoblend. However, why is this multimodal specific?\n3. Different rates of Infoblend seem to be not on a tradeoff frontier curve but rather a cluster of points. There should be difference in time vs accuracy. Why isn't there difference?\n4. Can we report on bigger models or more sota methods like qwen-vl? Lack of discussion for these newer models"}, "questions": {"value": "1. How to set hyperparameters in infoblend? aka how to tune the delay-accuracy tradeoff curve?\n2. Questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xExXkyekRc", "forum": "bld5GVRad0", "replyto": "bld5GVRad0", "signatures": ["ICLR.cc/2026/Conference/Submission25243/Reviewer_a8xw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25243/Reviewer_a8xw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606809986, "cdate": 1761606809986, "tmdate": 1762943378334, "mdate": 1762943378334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper presents INFOBLEND, a position-independent KV Cache reuse system across multi-modal inputs. Evaluation across multiple MLLMs and datasets illustrate that INFOBLEND saves TTFT by 54.1% compared to prefix caching, with an accuracy loss within 13.6%"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Identifies a problem that multi-modal inputs make it harder to naively reuse KV caches due to the interleaving of text and images, making the space of combined inputs much larger.\n* Shows important illustrations of the KV trends for KV Caches of images, beyond the traditional KV Caches of text tokens."}, "weaknesses": {"value": "* I am not sure the paper adds too much technical novelty beyond applying position-independent KV Cache reuse beyond applying it to a slightly newer domain. The authors claim in their evaluation, *\"CacheBlend is also a position-independent algorithm designed for RAG system. The primary focus of CacheBlend is the KV deviation, while the INFOBLEND’s selection process involves the identification of tokens that exhibit both high attention scores and significant KV deviation\"*  It is not clear what exactly INFOBLEND is adding here, as *\"identification of tokens that exhibit high attention scores\"* leads to higher KV deviation from the average values. It seems to be the same metric viewed from two different angles.\n\n* The authors claim *\"INFOBLEND-32 reduces TTFT by up to 54.1% while maintaining a loss of score within 13.6% compared to prefix caching\",* This seems like a significant quality drop, which makes me concerned about the generalisation of this method to newer data, which could have worse degradation."}, "questions": {"value": "* How exactly is the partial reuse applied on the image tokens? I'd encourage the authors to clarify this, as it seems to be a major motivation behind the design, but I couldn't find a clear explanation.\n* There are many optimizations proposed in the paper, such as multi-tier KV storage and parallel loading. It would be useful to see a breakdown of where the gains in INFOBLEND are coming from. Is it mainly due to these optimizations, or does it come from the technical novelty of KV reuse for multi-modal data?\n* Figure 10 could have a more meaningful comparison if the value of k were similar for Cacheblend and INFOBLEND. I'd encourage the others to add this as currently the gains are not clearly visible and are hard to compare."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zIDTVrlQUb", "forum": "bld5GVRad0", "replyto": "bld5GVRad0", "signatures": ["ICLR.cc/2026/Conference/Submission25243/Reviewer_ozB4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25243/Reviewer_ozB4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756084871, "cdate": 1761756084871, "tmdate": 1762943378038, "mdate": 1762943378038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces INFOBLEND, a new system for caching and reusing key-value caches in multimodal large language model inference, especially for contexts with interleaved text and images. Unlike existing context caching methods that only reuse KV caches for exact prefix matches, INFOBLEND enables position-independent and partial reuse of KV caches, improving efficiency even when prompts differ slightly."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The MLLMs are promising.\n\n2. The inference acceleration is interesting."}, "weaknesses": {"value": "1. The writing quality is poor. Please revise the entire paper for improved logical flow. Also, it would be better to add a pipeline or an overview of the algorithms to clearly show how your model works.\n\n2. The novelty of this paper is limited. The importance of the first token has been common knowledge for at least a year. Selecting partially important tokens for re-computation has also been investigated by existing papers, such as CacheLend. The novelty of this paper regarding related works is limited; it is mainly an extension of existing text LLM acceleration approaches to MLLMs.\n\n3. The experiments are weak. Since this work focuses on LLM acceleration, it should be compared with GraphRAG and other related works that also accelerate LLM inference, as analyzed in the related work section.\n\n4. While the paper discusses multimodal data, most experiments and analysis focus on images. The effectiveness for audio, video, or other modalities is not thoroughly evaluated.\n\n5. Since this is a system-level optimization, you should compare your approach with VLLM. VLLM has been one of the most state-of-the-art baselines for such papers, even though it was published a year ago."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mRDl103yQ7", "forum": "bld5GVRad0", "replyto": "bld5GVRad0", "signatures": ["ICLR.cc/2026/Conference/Submission25243/Reviewer_3iNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25243/Reviewer_3iNp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805493029, "cdate": 1761805493029, "tmdate": 1762943377808, "mdate": 1762943377808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}