{"id": "FUiOZXchu5", "number": 16187, "cdate": 1758261236895, "mdate": 1759897255757, "content": {"title": "FuseAgent: A VLM-driven Agent for Unified In-the-Wild Image Fusion", "abstract": "Fusing multi-source images captured in the wild is often undermined by unpredictable and coupled degradations, including pixel-level misalignment, adverse weather, and dynamic artifacts. Existing solutions face notable limitations: (1) Task-specific models rely on predefined degradation priors and fail to generalize to the complex, coupled degradations present in real-world scenarios. (2) All-in-one methods, while designed for multi-fusion tasks, frequently overlook the degradation inherent in input images, leading to suboptimal performance. To address these challenges, we introduce FuseAgent, a VLM-\npowered agent system that autonomously identifies degradations in the input images and dynamically coordinates expert models to execute a tailored fusion strategy. FuseAgent undergoes a\ntwo-stage training process: an initial supervised fine-tuning (SFT) establishes basic degradation perception and tool-use skill, followed by Group Relative Policy\nOptimization for fusion (GRPO-F) augmented with multi-dimensional rewards to further enhance its decision-making and tool proficiency. Experimental results demonstrate the superior performance of FuseAgent in handling complex and coupled degradations in real-world scenes, achieving a **20\\%** average improvement across all evaluation metrics on challenging in-the-wild benchmarks.", "tldr": "", "keywords": ["Image Fusion", "Low Level", "Multimodal Large Language Model", "Multimodal Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d02beec786fcc5f257e7315cc09009f0640a1423.pdf", "supplementary_material": "/attachment/cb4e03158b0a32ea910cc315e58504e2d1ad8c7c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes FuseAgent, a VLM-driven agent that acts as a controller to dynamically plan and execute a sequence of expert models for \"in-the-wild\" image fusion. The agent is trained in two stages: first, using Supervised Fine-Tuning (SFT) on a curated dataset of expert trajectories , and second, using a reinforcement learning algorithm (GRPO-F) with a novel multi-dimensional reward signal on unlabeled data. The agentic framework itself and the SFT+RL training pipeline, including the Intrinsic and Relational Quality Rewards (IQR/RQR), are the key technical contributions.\nThe model demonstrates superior performance compared to existing task-specific and all-in-one fusion methods, and even outperforms manually designed \"Human Expert\" pipelines in quantitative evaluations. The paper also contributes a new data generation pipeline for creating complex, multi-degradation fusion scenarios and a comprehensive evaluation benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary strength of this paper is the novel conceptualization of image fusion as a dynamic, agent-based planning problem rather than a static pipeline. By employing a VLM as a controller, FuseAgent can explicitly reason about and adapt to the complex, coupled degradations found \"in-the-wild\". This is a significant paradigm shift from traditional methods, which are often brittle and fail to generalize to such varied and unpredictable inputs.\n2. The two-stage training methodology (SFT + GRPO-F) is well-conceived and effectively executed. The SFT phase provides a strong initialization by mimicking expert trajectories , while the GRPO-F stage allows the agent to generalize and discover superior strategies on unlabeled data. The design of the unsupervised, multi-dimensional reward signal (IQR and RQR) is a notable contribution, enabling the agent to learn complex policies by balancing standalone perceptual quality with the critical \"fusion compatibility\" between source images. The ablation studies strongly support the value of this hybrid training approach and the combined reward structure."}, "weaknesses": {"value": "1. The framework's performance appears to be heavily dependent on the comprehensiveness and quality of the expert models available in its library. While the authors state they used representative models, it's unclear how the agent would perform if it encounters a novel degradation for which no suitable tool is available. The paper could benefit from a discussion on the scalability of the tool suite and the agent's robustness to missing or inadequate tools.\n2. The SFT stage relies on a dataset of \"expert trajectories\". The paper states that the \"optimal action trajectory\" for this dataset is determined using an \"exhaustive search strategy\". This approach may be computationally feasible for a limited number of tools and short action sequences, but its scalability is questionable. The paper does not fully address how this data generation pipeline would cope with a significant increase in the number of tools or the complexity of the required workflows."}, "questions": {"value": "1. Regarding the tool suite: How does FuseAgent handle a scenario where it correctly identifies a degradation, but no appropriate \"expert model\" for that specific defect exists in its library? \n\n2. The SFT data generation relies on an \"exhaustive search\" to define the optimal trajectory. Could the authors elaborate on the computational complexity of this search and its scalability as the number of expert models and potential trajectory length increase?\n\n3. Could the authors comment on the inference cost of FuseAgent compared to other baselines? Further, how does the proposed method's efficiency suit real-world use cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h5Fa9RZb5C", "forum": "FUiOZXchu5", "replyto": "FUiOZXchu5", "signatures": ["ICLR.cc/2026/Conference/Submission16187/Reviewer_mtHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16187/Reviewer_mtHR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761271472277, "cdate": 1761271472277, "tmdate": 1762926349390, "mdate": 1762926349390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FuseAgent, a VLM-driven agent system designed to tackle the complex and highly variable problem of in-the-wild image fusion. The agent autonomously analyzes coupled degradations in the input images (e.g., misalignment, noise, dynamic artifacts) and dynamically plans a customized processing workflow composed of \"expert models\", and it is trained to cope with these problems by learning different expert models.\n\nThe groping method, combined with the fractal space and Gaussian-flat pattern, yields a clever double-layered training framework (SFT and GRPO-f), optimizes an unsupervised reward mechanism (IQR and RQR) and demonstrates a general automatic-tuning memory in excess of human experts. Moreover, the paper gives simulations to demonstrate its technical prowess."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel Paradigm**: The primary contribution is introducing the VLM Agent paradigm to image fusion. This dynamic \"Perceive-Plan-Execute\" framework, compared to traditional static models, represents a paradigm-level advance for handling complex, heterogeneous degradation scenarios.\n\n**Superior Decision-Making**: As shown in Table 1, FuseAgent's ability to plan the optimal processing path is impressive. Its performance surpasses not only zero-shot VLM planners and fixed, pre-defined pipelines but also the workflows manually designed by human experts, demonstrating powerful reasoning and planning capabilities.\n\n**Performance**: On the challenging \"in-the-wild\" benchmark constructed by the authors, the authors claims that FuseAgent significantly outperforms existing task-specific and all-in-one SOTA methods on several key perceptual metrics (e.g., HyperIQA, CLIPIQA).\n\n**Effective Training Framework**: The two-stage training strategy combining SFT and GRPO-F, along with the unsupervised IQR/RQR reward design, is very clever. It provides a viable and efficient blueprint for training agents to solve complex, multi-step vision tasks in the absence of expensive ground-truth labels."}, "weaknesses": {"value": "**Questionable Premise of Unification**: The paper's core premise rests on building a \"unified\" fusion framework. However, different fusion tasks (e.g., Infrared-Visible vs. Multi-Exposure) are based on fundamentally different imaging principles and physics, $ESPECIALLY$ in facing an in-the-wild situation. The paper fails to sufficiently justify why forcing these physically dissimilar tasks into a single model is necessary, or what unique advantages this unification offers over specialized, domain-specific models. This makes the \"unification\" premise seem more like an artificial setup to inflate the paper's contribution (contribution), rather than a solution driven by a practical, real-world need.\n\n**Fairness of Experimental Comparison**: The paper compares FuseAgent against existing SOTA methods (e.g., FILM, DRMF) on the authors' \"in-the-wild\" dataset. However, these SOTA methods were largely designed with the assumption of relatively clean or pre-aligned inputs; they were not built to handle such complex coupled degradations. Therefore, comparing them in a scenario they were not designed for is arguably unfair. A much more convincing baseline would be a \"Preprocessing + SOTA model\" pipeline (e.g., using the paper's own registration tool + the FILM fusion model). The current absence of this stronger, fairer baseline significantly weakens the paper's claim to SOTA performance.\n\n**Lack of Efficiency and Latency Analysis**: Authors mentioned that the paper targets real-world applications like autonomous driving, which are often latency-sensitive, yet provides absolutely no analysis of the system's inference speed, end-to-end latency, or computational cost. However, the proposed multi-step, agent-driven framework (VLM planning + sequential execution of multiple expert models) inherently introduces significant computational overhead and latency.  This is a major practical limitation that is left unaddressed.\n\n**Reliance on Off-the-Shelf Tools**: The paper's novelty lies entirely in the orchestration of expert tools, not in the tools themselves. The \"expert models\" (LoFTR, Restormer, SwinFusion, etc.) are all pre-existing, off-the-shelf methods, treated as black boxes. While this is a valid agent-based approach, it means the paper does not contribute any new, fundamental model architectures for the core tasks of preprocessing or fusion. The system's performance is therefore heavily capped by the capabilities of these external tools."}, "questions": {"value": "As introduced in Weaknesses part, this is the following questions:\n\n**Regarding Weakness #1 (Unification Premise)**: Could the authors elaborate on the practical or theoretical justification for creating a unified agent for physically dissimilar fusion tasks (like IVIF and MEF)? Beyond the goal of handling mixed degradations, what specific advantages does a single unified agent provide over using separate, specialized agents for each task?\n\n**Regarding Weakness #2 (Experimental Fairness)**: This is the key question. Have the authors conducted, or can they provide, results from experiments using a stronger, more \"apples-to-apples\" baseline, such as a \"Preprocessing + SOTA Model\" pipeline? For example, running the alignment tools from the agent's tool-suite on the inputs first, and then feeding the aligned images to a SOTA method like FILM or DRMF. How would FuseAgent's performance compare against such a baseline?\n\n**Regarding Weakness #3 (Efficiency)**: Could the authors provide any benchmark or estimation of the system's end-to-end latency (VLM planning + all model executions)? How does this computational cost compare to a single end-to-end SOTA fusion model, and how do the authors envision this being deployed in the latency-sensitive applications mentioned?\n\n**Regarding Weakness #4 (Tool Reliance)**: Given that the system's performance is bound by the quality of the expert tools, did the authors experiment with replacing or fine-tuning these tools? For instance, how much of the performance gain comes from the orchestration itself, versus simply being the first to apply a superior (but pre-existing) registration model like LoFTR within a fusion pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nD5CjTLwp8", "forum": "FUiOZXchu5", "replyto": "FUiOZXchu5", "signatures": ["ICLR.cc/2026/Conference/Submission16187/Reviewer_Ad7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16187/Reviewer_Ad7Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757789119, "cdate": 1761757789119, "tmdate": 1762926348810, "mdate": 1762926348810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose FuseAgent, a novel agentic system for multi-source image fusion designed to handle complex, real-world degradations. This system employs a Vision-Language Model as a central controller to identify image distortions and dynamically assemble a processing pipeline from a set of expert models. The agent is trained via a two-stage process, beginning with Supervised Fine-Tuning on expert trajectories, followed by a reinforcement learning phase using a custom algorithm, Group Relative Policy Optimization for Fusion. To facilitate this second stage, the authors introduce two unsupervised reward metrics—the Intrinsic Quality Reward and the Relational Quality Reward to guide the agent's policy development. The paper claims that this approach achieves significant performance improvements over existing methods on in-the-wild datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's main strength lies in its conceptualization of the image fusion problem from an agentic perspective, which is an new direction for low-level vision tasks."}, "weaknesses": {"value": "1. The authors claim that the VLM is capable of \"perceiving\" degradations. It is unclear how this \"perception\" fundamentally differs from the outcome of a standard classification model trained to identify degradation types. Could the authors elaborate on the unique advantages or novel capabilities that the VLM brings to this specific task compared to a more straightforward classification approach?\n2. The fusion pipeline detailed in Section 3.1 appears extensive, which suggests it may incur substantial computational overhead and GPU memory consumption. To properly evaluate its practical efficiency, the authors should provide a detailed comparison of FuseAgent's training time, inference time, and memory footprint against the baseline methods.\n3.  For the metrics:    \n    a) The authors used a series of IQA metrics during the training of the agent. However, these same metrics were also used for comparison in the evaluation stage. Is this comparison fair to the other methods? To my knowledge, most of the compared methods do not rely on IQA-based metrics for training, validation or testing.     \n    b) Moreover, the BRISQUE metric used during training is not even included in the comparisons in Table 3. SSIM and SF, which also appeared during the agent’s training, are again used as evaluation metrics. Therefore, all the metrics compared in this paper are those already used in the training process.    \n    c) In addition, the metrics used across different tasks in Tables 2 and 3 are inconsistent, and SSIM does not appear in Table 1. These issues require further clarification from the authors.\n4. Following the previous point, the methodology seems to tightly couple the fusion process with a curated set of evaluation metrics. What were the specific criteria for selecting these reward metrics? A clear justification is needed for why certain metrics (e.g., SF) were included while other common fusion metrics (e.g., EN) were omitted. Furthermore, on line 332, the paper claims that metrics like Qabf, Qcb, and VIF become unreliable for degraded images. Could the authors provide a rationale for why SSIM, which also has known limitations with severe degradations, is considered exempt from this issue?\n5. In the comparative experiments on degraded images, it is unclear whether the baseline fusion methods were originally designed to handle such complex source image degradations. If not, the comparison lacks fairness. If the baselines were augmented with external degradation or misalignment-handling modules for the sake of comparison, this introduces a confounding variable. In that case, how can the authors definitively attribute the reported performance gap to the fusion algorithm itself, rather than the potential inadequacy of the added pre-processing modules? The experimental setup must be clarified to ensure a conclusive and fair comparison."}, "questions": {"value": "Please see Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FVNkwZDOu1", "forum": "FUiOZXchu5", "replyto": "FUiOZXchu5", "signatures": ["ICLR.cc/2026/Conference/Submission16187/Reviewer_bKw8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16187/Reviewer_bKw8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013382025, "cdate": 1762013382025, "tmdate": 1762926348269, "mdate": 1762926348269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}