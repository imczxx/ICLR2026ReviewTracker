{"id": "MkrsbXl1GI", "number": 25096, "cdate": 1758364086251, "mdate": 1759896734391, "content": {"title": "When Language Models Lose Their Mind: The Consequences of Brain Misalignment", "abstract": "While brain-aligned large language models (LLMs) have garnered attention for their potential as cognitive models and for potential for enhanced safety and trustworthiness in AI, the role of this brain alignment for linguistic competence remains uncertain. In this work, we investigate the functional implications of brain alignment by introducing brain-misaligned models--LLMs intentionally trained to predict brain activity poorly while maintaining high language modeling performance. We evaluate these models on over 200 downstream tasks encompassing diverse linguistic domains, including semantics, syntax, discourse, reasoning, and morphology. By comparing brain-misaligned models with well-matched brain-aligned counterparts, we isolate the specific impact of brain alignment on language understanding. Our experiments reveal that brain misalignment substantially impairs downstream performance, highlighting the critical role of brain alignment in achieving robust linguistic competence. These findings underscore the importance of brain alignment in LLMs and offer novel insights into the relationship between neural representations and linguistic processing.", "tldr": "", "keywords": ["language models", "brain alignment", "brain misalignment", "linguistic competence", "neuroscience", "fMRI"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f57d609ec157b97d7b87dfa70e05257e502ee15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce a novel methodology to create \"brain-misaligned\" models by fine-tuning pretrained LLMs (BERT and GPT-2) with a dual objective: maintaining language modeling performance while actively training the model's representations to be poor predictors of human fMRI activity recorded during reading. This is achieved using an adversarial setup with a gradient reversal layer. As a control, \"brain-preserving\" models are trained using the same procedure but with permuted fMRI data, isolating the effect of genuine brain-stimulus alignment. The linguistic competence of these misaligned and preserving models is then comprehensively evaluated on over 200 downstream tasks. The central finding is that deliberately reducing brain alignment significantly impairs a model's performance on these linguistic tasks, particularly in the domains of semantics, syntax, and reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The creation of \"brain-misaligned\" models is a clever and powerful technique. Using a gradient reversal layer to explicitly penalize brain predictability, while simultaneously preserving language modeling ability, allows the authors to isolate the variable of interest in a way that correlational studies cannot. The \"brain preserving\" model serves as a well-thought-out control, accounting for potential confounding effects of the fine-tuning procedure itself."}, "weaknesses": {"value": "1. The performance drop is clear and significant for BERT models, but the effect is weaker and only marginally significant for GPT-2 on the Harry Potter dataset. Furthermore, the authors note that for GPT-2 on the Moth Radio Hour dataset, \"results are not consistent due to the weaker effect of brain removal.\" This variability undermines the universality of the core claim. The paper would be much stronger if it could offer a hypothesis or analysis explaining why this discrepancy exists.\n\n2. The authors fail to conduct experiments on the Narratives dataset, which is much larger, contains more subjects, and has been widely adopted in many previous works. Additional experiments on this dataset will provide more reliable insights.\n\n3. The authors only investigate bert and gpt2, which are out-of-date models. Many recent studies in brain-language model alignment use llama models. I think choosing llama models are especially important when you want to investigate the performance of Brain misaligned models.\n\n4. The paper would be far more transparent and impactful if it also reported absolute performance metrics across task categories instead of \"win rate\". Without this, it is difficult to judge the practical importance of the observed effect."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pliY8SDkgH", "forum": "MkrsbXl1GI", "replyto": "MkrsbXl1GI", "signatures": ["ICLR.cc/2026/Conference/Submission25096/Reviewer_Fhx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25096/Reviewer_Fhx9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600427866, "cdate": 1760600427866, "tmdate": 1762943323309, "mdate": 1762943323309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Brain Misaligned models that are adversarially fine-tuned, with a gradient-reversal head trained to anti-predict fMRI, to decrease brain predictivity while preserving standard LM loss, and Brain Preserving controls trained identically except that fMRI targets are permuted so that adversarial updates cannot meaningfully target the true mapping. The setup uses BERT-base and GPT-2-small, fMRI from Harry Potter reading and Moth Radio Hour stories, and evaluates language modeling, brain predictivity, and downstream linguistic competence via Holmes/FlashHolmes (>200 probing datasets spanning semantics, syntax, morphology, discourse, reasoning). Core findings: (i) the misaligned models reliably reduce voxel-wise brain predictivity in language ROIs without degrading LM loss; (ii) across participants and seeds, misalignment lowers win-rates on the Holmes benchmark overall, with especially robust drops in semantics (and often syntax/reasoning). The pattern replicates across model/dataset pairs, with clearest effects for BERT; GPT-2 shows similar trends with weaker significance in one setting. The paper argues this is causal evidence that brain-aligned information supports linguistic competence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is conceptually original in treating brain–LM alignment as an intervenable property rather than a purely correlational observation. Constructing “brain-misaligned” variants via adversarial fine-tuning and contrasting them with a permutation-based control is a creative way to probe causal relevance.\n\n- Methodologically, the study is careful about a manipulation check (reduced brain predictivity in language ROIs) and about holding general LM performance roughly fixed during selection, which increases confidence that downstream effects are tied to the intervention rather than to obvious capacity loss. Empirically, the evaluation spans a broad set of linguistic competencies (Holmes/FlashHolmes).\n\n- The paper is generally clear and well organized, with helpful schematics and ROI visualizations. It is a pity that most of the experiments/results are added to the appendix and are not integrated into Figures 3 and 4."}, "weaknesses": {"value": "- The framing somewhat overstates what the intervention establishes. Studying the effects of removing alignment does not automatically prove that alignment, per se, is beneficial. Removing brain–LM similarity and observing performance drops suggests that aligned information may correlate with linguistic competence, but it does not directly prove that alignment itself is necessary or causally beneficial. How do we ensure that the suggested manipulation does not act as structured noise or remove semantically relevant information? It would help to clarify whether this intervention reflects targeted “de-alignment” or a broader perturbation of representational geometry. Similarly, the construct of alignment could be made more robust: the paper currently relies on a single linear voxel-prediction metric, but conclusions might vary across similarity frameworks (e.g., RSA, CKA, or Procrustes) and layers. Reporting whether results hold across metrics, or demonstrating metric invariance, would strengthen causal claims and continue the discussion by recent comparisons ([Bo et al., 2025](https://openreview.net/forum?id=JGANEMteY8)). Finally, the naming of the Brain Preserving model is potentially misleading, since permuting fMRI targets does not intuitively preserve alignment; a label such as Permutation-Control would be clearer, and the distinction between misaligned and control models should appear earlier and be included among the stated contributions.\n\n- The experimental design would be stronger with two additional reference points: a zero-shot base model and a classic brain-tuned model (trained to predict brain activity given language stimuli or vice versa) to help disentangle generic fine-tuning effects from alignment-specific ones. Reporting and visualization also need tightening: please specify the exact number of test examples and how test samples are derived for cross-entropy estimation (especially given the ~1,211 images in Harry Potter), clarify whether multiple tokens per TR are aggregated, and whether averaging is per token or per example. \n\n- Some results are fragile and not adequately discussed. GPT-2/Harry Potter shows only a trend (p≈0.055), and Moth/GPT-2 is inconsistent due to weaker removal; discussing why adversarial removal is less effective for autoregressive models or the Moth stimuli would help.\n\nOverall, to broaden context and strengthen claims, I would recommend adding citations and, where feasible, experiments around: (a) Narratives by [Nastase et al., 2021](https://www.nature.com/articles/s41597-021-01033-3) as a widely used naturalistic fMRI resource, which would diversify stimuli and subjects; (b) [Pereira et al. (2018)](https://www.nature.com/articles/s41467-018-03068-4) as a complementary sentence-level dataset; (c) alternative representational similarity metrics (RSA/CKA/Procrustes) given active debate on metric choice; (d) concept-erasure baselines (INLP, RLACE, IGBP/single-projection) to test method-robustness of the causal effect; (e) Report exact effect sizes and confidence intervals for win-rate differences, not only p-values; (f) Clarify whether LM losses truly matched (distributions, not just means) and include masked-LM vs autoregressive comparability caveats; (g) Finally, consider a small positive-control experiment: brain-tuning the same base models (without adversarial reversal) to increase predictivity and checking for competence gains, to bracket the causal relationship in both directions (cf. recent brain-tuning gains in speech LMs). This would nicely support your core result."}, "questions": {"value": "- How layer-localized is the removal? Did you inspect per-layer brain predictivity to see whether misalignment concentrates in earlier vs later layers, and whether the competence drop tracks specific layers/heads?\n- If you compute brain-alignment with RSA or CKA (and perhaps ROI-wise CKA), do you still see the same manipulation check and downstream deficits? Any reason to expect metric dependence? A nice work comparing representation similarity frameworks is by [Williams, 2024](https://openreview.net/forum?id=zMdnnFasgC).\n- Could you repeat the intervention with INLP or RLACE on frozen representations (or hybrid: INLP on a projection bottleneck during fine-tuning) to test whether the competence drop is method-agnostic?\n- Have you tried Narratives by [Nastase et al., 2021](https://www.nature.com/articles/s41597-021-01033-3) (multiple stories, many subjects) or the sentence-level fMRI dataset by [Pereira et al. (2018)](https://www.nature.com/articles/s41467-018-03068-4) to probe genre and task sensitivity of the effect? If not, what are the expected obstacles (timing alignment, TR windows, etc.)?\n- Beyond probing, do misaligned models underperform on GLUE/SuperGLUE, coreference, and NLI when controlling for instruction-tuning? If tested, please report; if not, justify relying solely on probing.\n- Your permutation control is strong, but adversarial training can still regularize or distort representation geometry in ways unrelated to brain signals. Can you add a no-brain adversarial control (e.g., adversary trained on shuffled text features or random vectors) to bound generic adversarial side-effects?\n- Is there a specific reason competence evaluation relies on probing? Holmes is excellent for linguistic phenomena, but do you think adding task benchmarks (e.g., GLUE/SuperGLUE, coreference, QA) or behavioral psycholinguistic tests would help bridge representational competence to end-task behavior?\n\n**Comments:**\n- Lines 31–32 lack a reference to a comprehensive prior survey: [Karamolegkou et al., 2023](https://aclanthology.org/2023.findings-acl.618/).\n- Lines 076-090: The contributions paragraph reads partly like a summary of results (\"We find that the competence drop is especially pronounced…\"). Conceptually, contributions should describe what is contributed (e.g., a causal framework for testing brain alignment), not what was found. I suggest rephrasing to highlight the methodological and theoretical advances, moving empirical claims to the Results section.\n- Lines 101-103: I would recommend tightening causal language. For example, I am not sure whether you study if brain alignment *is necessary for* maintaining linguistic competence in language models, maybe I would rephrase it as *is implicated in*. \n- By Line 101, the reader still does not know that there are two model \"types\" used: brain misaligned and brain preserving. In the abstract you mention the brain preserving models as \"well-matched brain aligned counterparts\". It would be helpful to clarify earlier that both are adversarially fine-tuned variants of the base model, differing only in whether the fMRI data are permuted. This distinction is crucial for interpreting the experimental design and for understanding how the \"alignment\" being tested is manipulated. Brain-preserving models should be added to your contributions. \n- Section 3.3.2 (“Brain Preserving Model”) is confusing to me. If the fMRI responses are permuted during training, how can this model be described as “preserving” brain alignment? You mention that you \"permute the order of the fMRI images to disrupt the correspondence between stimuli and brain activity\", this to me does not suggests brain preserving. Maybe the name or the explanation needs refinement... perhaps \"Brain-Control\" or \"Permutation-Control\" would be more transparent? In addition, a zero-shot baseline and a standard brain-fine-tuned model (finetuned to predict brain signal given language stimuli, or vice versa) would provide valuable reference points, as I have noted above.\n- Lines 224–228: Please specify the number of test examples. The current phrasing, \"For each test example, it is measured the average cross entropy across the randomly masked tokens\", leaves unclear the dataset size and sampling process. Given that the Harry Potter fMRI dataset contains only around 1,211 brain images, it would be important to clarify (i) how many text–fMRI pairs were used for testing, (ii) whether multiple tokens per TR were aggregated, and (iii) how the averaging procedure interacts with the limited sample size. This information is essential for assessing the robustness and generalizability of the reported results.\n- Lines 336–337: Please correct the cross-references \"Appendix C and ??\".\n- Figures 3 and 4 would benefit from including a bar with baseline model that is neither misaligned nor brain-preserving (e.g., the original pretrained and/or a fine-tuned version). This would allow readers to disentangle whether observed differences arise from misalignment, permutation control, or general pretraining/fine-tuning effects.\n- The statistical significance markers (asterisks) in Figures 3 and 4 are difficult to interpret—please ensure consistent notation and a clear legend.\n- In Figures 3 and 11, the brain-misaligned models seem to outperform baselines on morphological phenomena. This is an intriguing and unexpected finding that deserves discussion, as it may indicate selective effects of the adversarial objective across linguistic domains.\n- Although the abstract claims experiments on two fMRI datasets and two models, the main paper only visualizes results for BERT + Harry Potter. Including GPT-2 and Moth results (even as additional bars in Figures 3 and 4) would greatly strengthen the consistency and interpretability of the findings. The divergence between BERT and GPT-2 patterns otherwise leaves conclusions somewhat underdetermined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7jTZi7HjV0", "forum": "MkrsbXl1GI", "replyto": "MkrsbXl1GI", "signatures": ["ICLR.cc/2026/Conference/Submission25096/Reviewer_7k7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25096/Reviewer_7k7B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936660674, "cdate": 1761936660674, "tmdate": 1762943322874, "mdate": 1762943322874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper trains LLMs to *poorly* predict brain activity (Wehbe et al. 2014 HP and Deniz et al. 2019 MRH fMRI datasets), in order to test the relevance of brain alignment for language task performance.\nThe main claim is that LLMs misaligned to the brain exhibit poor downstream performance, with which the authors argue that brain alignment is critical for linguistic competence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Explicitly training models for brain misalignment is novel as far as I am aware. This is a cool method for causally testing the effect of neural data regularization in both directions.\n\n2. Great control: models trained for misalignment are compared with models under the same training pipeline but with permuted stimulus-response pairs.\n\n3. Methods and results are presented clearly, making the paper easy to follow."}, "weaknesses": {"value": "### 1. Small scale of tested models\nThe models under consideration (BERT-base and GPT-small) are very small by modern standards, limiting the applicability of claims to the state of the art. In particular, smaller models might lack capacity to retain high linguistic performance despite reduced brain alignment, whereas larger models' performance might be affected by brain-misalignment less strongly or not at all.\n\n### 2. Small effect size\nThe average win rate between misaligned model and control differs by 2 percent points (0.19 vs 0.21). I further wonder what the actual difference in performance is. Even small performance differences can result in different win rates, but the effect size nonetheless can well be very small.\n\n### 3. Increased alignment of misaligned model in some regions\nThere is something weird with the training: why are the models trained to be misaligned actually more aligned for a substantial number of voxels (Fig. 2C)? It seems the authors attribute this to such regions not being language-related, but since the model is trained with whole-brain data this shouldn't happen.\n\n\n### Minor\n* L247 typo \"more tha**t** 200 datasets\"\n* L337 missing reference \"??\"\n* Figure 3A: shouldn't the win rate between two models sum up to 1?"}, "questions": {"value": "Please see Weaknesses 1-3 in particular. I am willing to increase my score with the evaluation of larger models (e.g., Qwen2.5-72B, Pythia, OLMo), and a quantification of performance deltas (rather than win rate). \n\nI like the misalignment training method! Please describe related work of training models with brain data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ghWzy0Ku0C", "forum": "MkrsbXl1GI", "replyto": "MkrsbXl1GI", "signatures": ["ICLR.cc/2026/Conference/Submission25096/Reviewer_J54j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25096/Reviewer_J54j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943724862, "cdate": 1761943724862, "tmdate": 1762943322442, "mdate": 1762943322442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a simple but important question. Does the alignment between large language models (LLMs) and brain activity matter, or is it just a coincidence? To answer this, the authors use adversarial training to erase the information in the model that can predict brain activity, without affecting the model’s language ability.\nThey use BERT-base and GPT-small models on two fMRI datasets. As a control, they train other models in which the brain data is shuffled, to rule out effects from the training itself.\nThey then evaluate these models on over 200 linguistic tasks. Models where \"brain alignment\" was erased perform worse on semantics, syntax, and reasoning tasks. The authors conclude that brain-aligned representations are important for high-level language ability in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution is proposing a new paradigm beyond simple correlation studies. The paper creates “brain de-aligned” models through adversarial training, with “brain retention” models as controls. This is a well-designed intervention experiment, providing a new tool for studying the function of representations.\n2. Using the Holmes benchmark to evaluate over 200 fine-grained linguistic tasks is a major strength. It allows the authors to see how de-alignment affects various aspects of language, not just a single task."}, "weaknesses": {"value": "1. The key findings are only based on bert-base and gpt-small. In 2025, using these models is not enough to study emergent properties like brain alignment in LLMs. Many key language abilities and representations only show up in much larger models (for example, over 7B parameters). Do these findings hold for modern models like Llama or Qwen?\n2.  On the Harry Potter dataset, the difference in GPT-2 model performance is not statistically significant (p=0.055). On the Moth Radio Hour dataset, the authors say the results are inconsistent because the “brain removal” method was weak. Appendix Figure 18 shows that the method only works for 3 out of 6 subjects. This inconsistency (good results for BERT, but not for GPT-2) weakens the main message. The paper does not explore why such architecture differences (encoder vs decoder) produce such different results.\n\nIn addition, there is a '??' at line 337, which looks unprofessional in a formal submission."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "BYarPhzW7K", "forum": "MkrsbXl1GI", "replyto": "MkrsbXl1GI", "signatures": ["ICLR.cc/2026/Conference/Submission25096/Reviewer_SLR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25096/Reviewer_SLR2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972523043, "cdate": 1761972523043, "tmdate": 1762943322062, "mdate": 1762943322062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}