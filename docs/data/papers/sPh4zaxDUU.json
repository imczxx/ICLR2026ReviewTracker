{"id": "sPh4zaxDUU", "number": 1051, "cdate": 1756830452885, "mdate": 1763482954862, "content": {"title": "Kuromi: Learning without Augmentation via Energy-based Semi-supervised Kuramoto Neurons", "abstract": "Semi-supervised learning (SSL) often relies on extensive data augmentation or complex teacher-student structures to generate reliable supervision from unlabeled data. In this work, we propose Kuromi, a novel SSL framework that discards data-space augmentation entirely and instead leverages energy-based dynamics for representation learning and pseudo-label generation. Central to Kuromi is the Artificial Kuramoto Oscillatory Neuron (AKOrN), a biologically inspired dynamic neuron model that encodes inputs as synchronized oscillatory states on a hypersphere. Through unsupervised pre-training, fine-tuning on limited labels, and energy-guided self-training, Kuromi produces low-energy, structure-aware feature representations without requiring external regularization. To better exploit labeled data and pseudo-labels, we also propose Energize, an iterative, augmentation-free latent-state smoothing method inspired by Weisfeiler-Lehman aggregation, which operates entirely in the prediction space without hyperparameters. Extensive experiments on CIFAR-10, SVHN, STL-10, and ImageNet show that Kuromi achieves state-of-the-art performance among non-transformer SSL methods and remains competitive with recent transformer-based approaches. Notably, Kuromi not only achieves state-of-the-art results on CIFAR-10 and STL-10, but also leads in efficiency with just 12M parameters, 0.18 GFLOPs, and the highest throughput across all baselines.", "tldr": "A semi-supervised learning framework without data augmentation", "keywords": ["Semi-supervised Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d8c699d3b711797068e2713ca1c35bb6556ea6c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author propose to combine AKOrN and energy model framework to tackle the few-short semi-supervised learning problem. In particular, the model different than AKOrN in a finetuning phase, where the author formulate AKOrN as an energy model, and descent the energy function to find the pseudo-label, then train the model to learn a one-shot predictor to predict the pseudo-label. The author claim the proposed model is lighted weighted yet achieve good performance, compared to many other approach on similar problems. The author also ran many experiment to verify their result. I think the model works because the iteratively nature of the AKOrN could be descending some implicit energy function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is very cool idea. The author make use of this property differently than the original AKOrN. In the original paper, the author these iterative dynamical for unsupervised objective discovery. But in this paper, the author use it to infer label. This is related to Yann Lecun's energy model idea, the AKOrN backbone implement a dynamical system, which just match this idea of energy model nicely. Even better, the author find a good use case (semi-supervised learning).\n\nThe author ran lots of experiments and ablation."}, "weaknesses": {"value": "There are some bold claim the in abstract and title, such as \"learning without augmentation.\" This is not True because the pre-training phase need augmentation. I sympathize the authors want to market the idea but please be precise.   \n\nI personally would like to see more visualization or analyze on how the model works than seeing all these benchmark. I think your model actually open a door for understanding these oscillatory dynamics in AKOrN.  When a model is able to predict the right label with \"energize\" than without \"energize\", what happened? Maybe we can visualize the progression of the dyanmical of the feature map like in the original AKOrN paper. \n\nThe author is missing some citations for sure. This idea of descending energy function for model's inference, especially to get a label, is highly related to Yann Lecun's energy model. I don't work in this area so I can't point you to the literature but I know there's lots of them, you should probably cite some."}, "questions": {"value": "This is very cool idea. Like in my summarize, I personally think the model works because the iteratively nature of the AKOrN could be descending some implicit energy function. I think an natural ablation study should be replace the AKOrN backbone with a feedforward network, in other word, do the experiment for \"feedforward SSL network + Energize.\" \nI'm just curious. Is it possible that a feedforward model combined with a \"Energize\" head, or you have to combine a dynamical/energy model with the \"Energize\" head. \nWhy do you need to train a “one fresh forward pass” and use it to predict the pseudo label. Why can't you just regress the pseudo label directly to the clean label in the semi-supervised setting?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2vxqZ3FIp", "forum": "sPh4zaxDUU", "replyto": "sPh4zaxDUU", "signatures": ["ICLR.cc/2026/Conference/Submission1051/Reviewer_GEyD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1051/Reviewer_GEyD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763006274, "cdate": 1761763006274, "tmdate": 1762915665147, "mdate": 1762915665147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Kuromi, a semi-supervised learning framework that leverages a Artificial Kuramoto Oscillatory Neurons (AKOrN) backbone which replaces conventional neurons, introducing temporal evolution and synchronization dynamics into neural computation. Kuromi first pretrains the AKOrN module in an unsupervised manner, then fine-tunes on a small, labeled subset, and finally refines pseudo-labels through a proposed energy-based denoising module called Energize. The authors demonstrate that Kuromi achieves competitive performance with non-ViT semi-supervised models while maintaining high computational efficiency, making it suitable for resource-constrained settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel and well-motivated approach for training unconventional oscillator-based models such as AKOrN. The proposed framework, Kuromi, is notable for its efficiency in terms of FLOPs and throughput. The authors conduct extensive experiments across multiple benchmarks and provide comparisons against several strong semi-supervised baselines, demonstrating that Kuromi achieves competitive performance while maintaining a significantly lower computational footprint."}, "weaknesses": {"value": "Despite the strengths of the proposed model, the overall approach is not well explained within the paper. Several concepts central to the framework, such as the concept of energy, are left largely undefined, with readers expected to consult prior work for key details, even though this paper is presented as a standalone contribution. Important parameters, such as the update rate gamma, are never explicitly described, and the external stimulus c in Equation (1) is not clearly explained or derived. In addition, the paper claims to be augmentation-free, yet the contrastive loss in Equation (3) applies two random views per input, which constitutes an augmentation process. While the method is described as unsupervised during pretraining, the presence of class-conditioned stimulus vectors effectively introduces class structure into the model. This architectural prior may lead to more explicit class separability than conventional SSL methods, which typically learn class clusters implicitly. As a result, Kuromi may benefit from a stronger effective supervision signal than competing approaches, raising questions about fairness in comparison."}, "questions": {"value": "1.\tHow many epochs are allocated to each stage of the Kuromi framework? The paper specifies total training epochs per dataset, but it is unclear how many are spent in the pretraining, fine-tuning, and Energize stages individually.\n2.\tHow is the Readout function defined in Equation (2)?\n3.\tWhat happens if the number of conditional stimuli C does not match the number of classes in the dataset? Has the model’s performance been evaluated under such a mismatch, and does it degrade gracefully or collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HKnS7nGsoS", "forum": "sPh4zaxDUU", "replyto": "sPh4zaxDUU", "signatures": ["ICLR.cc/2026/Conference/Submission1051/Reviewer_Nwhe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1051/Reviewer_Nwhe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947954797, "cdate": 1761947954797, "tmdate": 1762915664423, "mdate": 1762915664423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work demonstrates that employing the recently introduced AKOrN, an oscillatory neuron model, improves performance on semi-supervised learning tasks. Specifically, it proposes an architecture that uses AKOrN as a building block, trained through self-supervised pre-training followed by fine-tuning with pseudo-label refinement. Experimental results show that the AKOrN-based model achieves superior performance to various existing methods across multiple benchmarks, while also providing higher throughput per image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good ablation study in Section 5.2.\n- The description of the proposed method is clear and easy to follow. \n\nWhat I find the most interesting idea introduced in this work is the energy-based refinement process (Energize), which seems reasonable and leverages the energy computed in the AKOrN model. Basically, people usually measure the quality of pseudo labels by looking only at the model's output (e.g., the model's softmax confidence), while Energize shifts the focus to the model's internal variables and uses the energy value computed in the AKOrN model, which, in my understanding, represents a kind of interneuron consistency, to measure the model's confidence. The work shows that this way of measuring confidence is better for identifying reliable predictions than simply looking at softmax confidence. To me, this implies that this kind of value (if available for models other than those based on AKOrN) might be useful beyond semi-supervised learning, for example, for better understanding, interpretation, and control of deep learning models."}, "weaknesses": {"value": "The major concern is that the model used in the work is relatively small. Although achieving higher performance than larger models despite its lower throughput is certainly a significant advantage, it raises the question of how the Kuromi model would perform if trained with a larger number of parameters. At present, it still underperforms ViT-based models on the ImageNet SSL benchmark, and it would have been interesting to see how it compares when scaled to a similar throughput and model size as ViT.\nOther concerns are listed in the Questions section."}, "questions": {"value": "- Do you have any idea or rationale why the Energize gives better label refinement compared to other pseudo-label refinement methods? For example, can you find any qualitative or quantitative differences between samples with low Ek and samples with high confidence in the softmax prediction? Are they quite similar or completely different?\n\n- How do you define $J_{ij}$ in the AKOrN model? It seems that according to the appendix, the work uses self-attention for $J_{ij}$. Assuming the use of self-attention, if you use a ViT-like model (a model with self-attention) with equivalent throughput or FLOPs to the Kuromi model, how much would the performance of such ViT-like models be when applying the same pre-training and fine-tuning strategy as used for Kuromi?\n\n- At inference, does Kuromi use multiple stochastic passes to compute the prediction, as in the fine-tuning stage, or are the stochastic passes used only for pseudo-label refinement?\n\n- I could not find the mathematical definition of the energy E. How is it computed? Is it the same as in the original AKOrN paper? Also, in general, E can take negative values, but it is used in the denominator in Equations (7) and (8), which seems somewhat unusual. Could you clarify this point?\n\n- Table 6: Could you please provide a concrete description of the procedures for Random Pseudo Labels and Energy-based shown in the table? I could not find any section in the manuscript that explains each methodology in detail.\n\n- Figure3. What does it mean that w/o oscillator? Do you use the usual way to define neurons (real valued neurons without hypersphere normalization)? Also, it is not clear to me that energy-based selection means here.\n \n- L472: What is uniform averaging? Can you please describe it more concretely?\n\n\nMisc: \n- Please add parentheses (or use \\citep) to the citations when they are used as references rather than as part of the sentence.\n\n\nI will adjust the score based on the replies from the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RhjdmpMMjD", "forum": "sPh4zaxDUU", "replyto": "sPh4zaxDUU", "signatures": ["ICLR.cc/2026/Conference/Submission1051/Reviewer_bS3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1051/Reviewer_bS3J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025433856, "cdate": 1762025433856, "tmdate": 1762915663557, "mdate": 1762915663557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}