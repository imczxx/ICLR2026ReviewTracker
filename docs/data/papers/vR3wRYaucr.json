{"id": "vR3wRYaucr", "number": 18418, "cdate": 1758287553899, "mdate": 1759897104864, "content": {"title": "Generalized Linear Markov Decision Process", "abstract": "The linear Markov Decision Process (MDP) provides a principled basis for reinforcement learning (RL) but assumes that both transitions and rewards are linear in the \\textit{same} feature space. This severely limits its applicability when rewards are nonlinear or discrete. We introduce the Generalized Linear MDP (GLMDP), which retains linear transitions while modeling rewards with generalized linear models \\textbf{under potentially different feature maps}. This separation is crucial: transitions may admit rich representations learned from large unlabeled trajectories, while rewards can be modeled with limited labeled data. We show that GLMDPs are Bellman complete with respect to a new function class, enabling efficient value iteration. Based on this, we develop algorithms with provable guarantees in both \\textbf{offline} and \\textbf{online} settings. For offline RL, we design pessimistic and semi-supervised value iteration methods that achieve policy suboptimality bounds and demonstrate significant label-efficiency gains. For online RL, we propose an optimistic algorithm with a near-optimal regret bound. Together, these results broaden the scope of structured and sample-efficient RL to applications with complex reward structures, such as healthcare and e-commerce.", "tldr": "We propose a novel framework that integrates generalized linear models (GLMs) into reward modeling while preserving the structured representations of linear MDP.", "keywords": ["Structured MDPs", "Bellman Completeness", "Generalized Linear Models", "Offline Reinforcement Learning", "Sample Efficiency"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f30f6c6a33fb62c2bec7e841e2e1d8dcd658986.pdf", "supplementary_material": "/attachment/8903fd76ac6bc7b69b29419cc89d73f1a9a21477.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework called the generalized linear MDP (GLMDP), where the state transition dynamics have a linear structure while the rewards follow a generalized linear model (GLM).\nBuilding on the property that the proposed GLMDP satisfies Bellman completeness, the authors develop algorithms with provable guarantees for both offline and online settings.\nIn the offline setting, they propose a pessimism-based algorithm with a sub-optimality gap guarantee, while in the online setting, they design a UCB-based algorithm with a cumulative regret guarantee."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework extends the conventional linear MDP setting by generalizing the reward function from linear to GLM.\n\n2. The paper presents theoretically grounded algorithms for both offline and online learning under this new setup, providing provable performance guarantees."}, "weaknesses": {"value": "1. The regret bound and technical analysis for the online setting appear somewhat incremental.\nThe idea of decomposing uncertainty between reward and transition components has been studied in prior model-based RL literature [1,2]. \nIt would be helpful if the authors clarified how their decomposition differs from existing approaches.\nFurthermore, although the paper models the reward as a GLM, the transition remains linear. Hence, the resulting regret bound may still be improvable using recent techniques for minimax linear or mixture MDPs [3,4].\n\n2. The regret bound depends on the inverse of the link-function derivative ($\\kappa^{-1}$), which can be exponentially large in the worst case.\nThis issue has been actively discussed in recent GLM and logistic bandit works [5,6,7,8], where some algorithms achieve instance-dependent or even link-independent regret bounds [6, 7]. Extending such ideas to the current setting could potentially yield tighter regret bounds.\n\n3. The authors claim that showing GLMDPs are Bellman-complete is a key contribution. However, it is unclear whether Bellman completeness is essential for obtaining provable guarantees in the online setting.\nAlgorithm B.3 (GLSVI-UCB) is model-based, and the Bellman completeness property does not appear to be directly utilized in the analysis (If this understanding is incorrect, clarification would be appreciated).\nFor instance, even when the transition follows an MNL model (which is not Bellman-complete), [9] can still achieve tighter regret bounds than GLMDP-UCB. \n\n4. The experiments are restricted to the offline setting. No empirical results are provided for the online algorithms.\n\n---\n__References__\n\n[1] Uehara et al., \"Representation Learning for Online and Offline RL in Low-rank MDPs.\" ICLR 2022\n\n[2] Deng et al., \"Sample complexity characterization for linear contextual mdps.\" AISTATS 2024.\n\n[3] Zhou et al., \"Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.\" COLT 2021\n\n[4] He et al., \"Nearly minimax optimal reinforcement learning for linear markov decision processes.\" ICML 2023\n\n[5] Faury et al., \"Improved optimistic algorithms for logistic bandits.\" ICML 2020\n\n[6] Abeille et al. \"Instance-wise minimax-optimal algorithms for logistic bandits.\" AISTATS 2021\n\n[7] Lee et al., \"Improved regret bounds of (multinomial) logistic bandits via regret-to-confidence-set conversion.\" AISTATS 2024\n\n[8] Sawarni et al. \"Generalized linear bandits with limited adaptivity.\" NeurIPS 2024\n\n[9] Hwang & Oh. \"Model-based reinforcement learning with multinomial logistic function approximation.\" AAAI 2023"}, "questions": {"value": "1. Ouhamma et al. [10] studied a bilinear exponential family reward model. Could the authors elaborate on the conceptual and technical differences between GLM reward model in this paper and that of Ouhamma et al. [10]?\n\n2. The authors state that Bellman completeness, together with a positive minimum eigenvalue condition, is essential for sample-efficient offline learning.\nHowever, since the transition in GLMDP remains linear, the model is still pushforward-coverable. In that case, GLMDPs can be approximately Bellman-complete (Lemma F.1 in [11]).\nCould the authors explain how much tighter the sub-optimality gap becomes when assuming exact Bellman completeness compared to approximate completeness?\n\n3. In the proof of Theorem 3, the total regret is decomposed into terms associated with reward estimation and transition estimation. However, the theorem statement suggests that the transition term dominates the overall regret. Could the authors clarify why this occurs?\n\n\n(minor)\n1. Line 1511: The expression $\\gamma_p \\sqrt{2 d_r \\log (1 + T/d_p)}$ should be corrected to $\\gamma_p \\sqrt{2 d_p \\log (1 + T/d_p)}$.\n---\n\n[10] Ouhamma et al., \"Bilinear exponential family of mdps: frequentist regret bound with tractable exploration & planning.\" AAAI 2023\n\n[11] Amortila et al. \"Reinforcement learning under latent dynamics: Toward statistical and algorithmic modularity.\" NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yk7T5Q8aEd", "forum": "vR3wRYaucr", "replyto": "vR3wRYaucr", "signatures": ["ICLR.cc/2026/Conference/Submission18418/Reviewer_1gbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18418/Reviewer_1gbo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760888868030, "cdate": 1760888868030, "tmdate": 1762928116727, "mdate": 1762928116727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel linear MDP-based framework in which the reward and dynamics are learned using different feature maps."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical analysis carried out is rigorous and thorough. The problem setting seems interesting and promising, especially the semi-supervised nature of the problem."}, "weaknesses": {"value": "Altogether, the work that has been presented by the authors has all the ingredients needed for a compelling paper. However, the writing in the paper lacks focus and requires significant improvements. As a reader, the paper is very difficult to follow.\n\nFor instance, consider the concept of a link function, perhaps the most crucial element of the authors’ proposed framework. This concept is introduced in line 49, subsequently mentioned numerous times in the paper, yet never defined (either informally or formally). The reader is hence tasked with filling in the gaps themselves on what is perhaps the most essential concept related to the proposed framework. \n\nThis kind of omission goes on to manifest itself in numerous ways throughout the paper. For instance, variables are introduced and only properly defined several paragraphs after, if defined at all. The authors will perform some sort of theoretical analysis and only motivate it after the analysis has been carried out (the authors need to first motivate any analysis before carrying it out, otherwise it confuses the reader). Similarly, there is no proper background/preliminaries section, which makes it difficult to distinguish what is prior work vs. what is a novel contribution by the authors. \n\nMoreover, the authors’ choice on what to include in the main body vs the appendix is questionable. In particular, the vast majority of the main body is dedicated to providing increasingly repetitive theoretical results, at the cost of not including any pseudo code or empirical results (those are left in the appendix). This again adds to the confusion that a reader may experience as pseudo code, figures, and empirical results can be an effective way to explain and solidify some of the concepts in the paper. \n\nAll in all, despite a very thorough and seemingly-sound theoretical analysis, the current draft of the paper lacks a narrative ‘backbone’ that can guide the reader through the many contributions presented in the paper. I again emphasize that the authors have all the pieces needed for a compelling paper, but they have yet to find a way to put all the pieces together.\n\n**Minor Comments:**\n- The authors use $x_t \\in \\mathcal{S}$ to denote the states in their notation which is quite non-standard. Usually, one would expect $s_t \\in \\mathcal{S}$ or $x_t \\in \\mathcal{X}$."}, "questions": {"value": "What are the requirements for a link function? i.e. are there any restrictions on the type of function that could be used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uO7CVki087", "forum": "vR3wRYaucr", "replyto": "vR3wRYaucr", "signatures": ["ICLR.cc/2026/Conference/Submission18418/Reviewer_Ry1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18418/Reviewer_Ry1x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673559383, "cdate": 1761673559383, "tmdate": 1762928116408, "mdate": 1762928116408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Generalized Linear MDP (GLMDP), which retains linear transitions while modeling rewards with generalized linear models under potentially different feature maps. This separation is crucial: transitions may admit rich representations learned from large unlabeled trajectories, while rewards can be modeled with limited labeled data. The authors show that GLMDPs are Bellman complete with respect to a new function class, enabling efficient value iteration. Based on this, the authors develop algorithms with provable guarantees in both offline and online settings. For offline RL, the authors design pessimistic and semi-supervised value iteration methods that achieve policy suboptimality bounds and demonstrate significant label-efficiency gains. For online RL, the authors propose an optimistic algorithm with a near-optimal regret bound. Together, these results broaden the scope of structured and sample-efficient RL to applications with complex reward structures, such as healthcare and e-commerce."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. This paper is well executed. It proposes a Generalized Linear MDP framework, which retains linear transitions while modeling rewards with generalized linear models under potentially different feature maps. The authors show that GLMDPs are Bellman complete with respect to a new function class. The authors design algorithms with provable guarantees for GLMDPs in both offline and online settings."}, "weaknesses": {"value": "1. MDPs with general function approximation are widely studied in the RL theory literature, e.g., RKHS MDPs and MDPs with Bellman Eluder dimension. The authors should elaborate more on the advantages and motivation of the proposed Generalized Linear MDP framework compared to existing frameworks for MDPs with general function approximation.\n2. The algorithm design and theoretical analysis in this paper seem to be a combination of existing techniques in linear MDPs (i.e., least squares value iteration) and offline RL (i.e., the pessimism idea), or a straightforward extension of these techniques to the generalized linear model, i.e., incorporating the $g(\\cdot)$ function."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xv7i2E6GCJ", "forum": "vR3wRYaucr", "replyto": "vR3wRYaucr", "signatures": ["ICLR.cc/2026/Conference/Submission18418/Reviewer_sCNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18418/Reviewer_sCNJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748326703, "cdate": 1761748326703, "tmdate": 1762928115938, "mdate": 1762928115938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the classical linear MDP framework in reinforcement learning by allowing nonlinear or discrete rewards through generalized linear models (GLMs) while keeping linear transitions. This decoupling enables learning rich transition structures from large unlabeled data while modeling rewards using limited labeled samples. The authors prove that GLMDPs remain Bellman complete, ensuring tractable value iteration, and propose algorithms for both offline and online settings. In the offline case, they introduce Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant SS-GPEVI that improves label efficiency by leveraging unlabeled trajectories. For online learning, they design GLSVI-UCB, an optimistic algorithm achieving near-optimal regret bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A new framework: it extends the linear MDP framework to generalized linear models (GLMDP), enabling nonlinear or discrete reward modeling while preserving tractable Bellman updates.\n- Theoretical completeness: Proves Bellman completeness and sample-efficient guarantees for both offline and online algorithms under the generalized setting.\n- Algorithmic soundness: Designs both pessimistic (GPEVI) and optimistic (GLSVI-UCB) algorithms with rigorous finite-sample and regret bounds.\n- Empirical validation: Provides experiments showing improved performance and label efficiency compared to standard linear or purely supervised baselines."}, "weaknesses": {"value": "The regret analysis assumes globally bounded derivatives of the link function (Assumption 3), which may exclude common choices like the logistic function. Could the authors explain more about this?"}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DzyV97cFdg", "forum": "vR3wRYaucr", "replyto": "vR3wRYaucr", "signatures": ["ICLR.cc/2026/Conference/Submission18418/Reviewer_JHtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18418/Reviewer_JHtV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996184989, "cdate": 1761996184989, "tmdate": 1762928115070, "mdate": 1762928115070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}