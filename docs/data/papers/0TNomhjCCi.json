{"id": "0TNomhjCCi", "number": 23596, "cdate": 1758346075835, "mdate": 1763701320766, "content": {"title": "How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity", "abstract": "Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. \nWe address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a “Complexity Paradox” emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.", "tldr": "Agent safety is brittle, eroding with intent concealment and masked by capability limits (Complexity Paradox), demanding multi-dimensional assessment for robust alignment.", "keywords": ["Agent Safety", "Holistic Harms", "Safety Benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09bf96295d2bc9296a71719378bdad44d70abdab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the \"brittleness\" of LLM agent safety. It argues that current tests are too simple and focus on direct, obvious harms. The authors propose a new benchmark called OASIS, which evaluates agents along two axes: \"intent concealment\" (how hidden the bad goal is) and \"task complexity\" (how many steps are in the task) . They test several models and find that agent safety gets much worse when the intent is hidden. They also find a \"Complexity Paradox,\" where agents seem safer on very hard tasks, but only because they fail the task before they can do the harmful part."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I agree that we would need a more sophisticated benchmark to evaluate the AI agent safety issue. I think the idea of testing safety on these two dimensions of concealment and complexity is a good direction. The paper also has some interesting findings, like the \"Complexity Paradox\" and the fact that some models are \"static\" in their safety checks while others are \"dynamic\"."}, "weaknesses": {"value": "First, the paper says agents seem safer on complex tasks, but it could be due to their \"planning capabilities\" failing them, and they can't complete the task. Therefore,  how would the authors distinguish the results from to be a test of capability and a new insight about safety alignment?\n\nSecond, the way the benchmark (OASIS) was created seems a bit circular. The paper says the tasks were \"synthesized using Gemini 2.5 Pro\" and then validated by humans. Could the authors provide more information about who the annotators are? And what does the annotation process look like? Without those details, it's hard to argue whether the benchmark reflects \"real-human\" threats.\n\nThird, I think the paper could benefit from a more thorough literature review. The idea of \"intent concealment\" in a multi-step task is explored in previous works (e.g., https://openreview.net/forum?id=KI1WQ6rLiy)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ipOqo9sfqf", "forum": "0TNomhjCCi", "replyto": "0TNomhjCCi", "signatures": ["ICLR.cc/2026/Conference/Submission23596/Reviewer_m3j4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23596/Reviewer_m3j4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854235529, "cdate": 1761854235529, "tmdate": 1762942729177, "mdate": 1762942729177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that current safety evaluations for LLM agents, which focus on \"atomic harms,\" are insufficient. It proposes that agent safety must be evaluated along two orthogonal dimensions: \"Intent Concealment\" (obscuring malicious goals within benign narratives) and \"Task Complexity\" (diluting harmful steps within long, multi-step workflows).\n\nTo enable this, the authors introduce OASIS, a new hierarchical benchmark and stateful simulation sandbox. Through experiments on a suite of SOTA models, the paper presents several key findings:\n\nSafety alignment degrades sharply and predictably as intent concealment increases. A \"Complexity Paradox\" emerges, where agents appear safer on more complex tasks; the authors demonstrate this is an illusion caused by capability limitations (i.e., the agent fails the task) rather than improved safety reasoning. The paper also identifies heterogeneous safety mechanisms, finding that most models rely on \"static, pre-execution\" checks (which are brittle), whereas the GPT-5 family, for example, exhibits a more robust \"dynamic, in-workflow\" monitoring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's primary strength is its novel problem formulation. By shifting the focus from \"atomic harms\" to the more realistic, orthogonal dimensions of \"intent concealment\" and \"task complexity,\" it reveals a critical and overlooked gap in safety research.\n- The paper goes beyond reporting simple refusal rates. The discovery and classification of \"static, pre-execution\" vs. \"dynamic, in-workflow\" safety mechanisms is a key insight into *how* safety systems fail."}, "weaknesses": {"value": "- While acknowledged by the authors, the curated set of 53 general-purpose tools is a limitation. Real-world agents will need to interact with thousands of dynamic, heterogeneous, and evolving third-party APIs. It is unclear how these findings (especially the \"Complexity Paradox\") will scale when the complexity of tool use itself.\n- While the sandbox is described as \"high-fidelity,\" the tasks are ultimately synthetic, and the tool outputs are pre-synthesized. This means the agent cannot elicit new harmful information from a \"live\" tool."}, "questions": {"value": "The Harm Progression Score (HPS) measures the proportion of harmful steps executed, which implies all harmful steps are weighted equally. In a multi-step plan, the severity of harm seems non-linear (e.g., \"emailing to purchase\" a harmful item seems more severe than \"searching\" for it). Could the authors comment on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tRPyJqS7pW", "forum": "0TNomhjCCi", "replyto": "0TNomhjCCi", "signatures": ["ICLR.cc/2026/Conference/Submission23596/Reviewer_zWPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23596/Reviewer_zWPb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982029168, "cdate": 1761982029168, "tmdate": 1762942728915, "mdate": 1762942728915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OASIS, a benchmark and simulation framework for evaluating LLM-agent safety under varying levels of intent concealment and task complexity. The authors show that safety alignment degrades sharply when malicious intent is hidden and uncover a “Complexity Paradox,” where agents appear safer on harder tasks due to capability limits. They also distinguish performance between pre-execution and in-workflow safety mechanisms."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors show how intent concealment and task complexity interact to influence the safety performance of language-model agents. They highlight the “complexity paradox” where the observation that agents may appear *safer* in more complex scenarios simply because they fail to act, reflecting capability limitations rather than genuine safety awareness.\n- The paper introduces a diagnostic framework that evaluates both process and outcome through diverse metrics such as the *Hierarchical Refusal Rate* and *Harm Progression Score*. They also show the discrepancy between pre-execution and post-execution safety judgments, providing valuable insight into how agents handle dynamic risk."}, "weaknesses": {"value": "The most critical weakness of this paper lies in its lack of clear explanations, definitions, and transparency. Many descriptions of the experimental setup are vague or informal—closer in tone to a blog post than an academic paper. As a result, the work falls short of reproducibility standards: it is difficult to fully understand the authors’ design decisions or replicate their experiments. In particular, no concrete examples of datasets, task instances, or tool usage are provided, which further limits interpretability. Please read my questions below."}, "questions": {"value": "- **Line 212:** What exactly constitutes the “ground-truth plan”? Is this plan provided as an input to the model, or is it generated by the authors as a reference?\n- **GPT-5-mini’s FPR:** Why is the false-positive rate notably high only in the *Idealized* scenario? Intuitively, less complex conditions should yield better performance according to the paper’s claims.\n- **Qwen3 reasoning traces:** What do these traces actually show? Do models acknowledge safety risks but proceed anyway, or do they omit mention of them entirely? The lack of detailed qualitative analysis limits interpretability.\n- **Tool selection:** How were the *53 tools* chosen? What are they specifically? For example, why were utilities such as `port_scanner` and `get_crypto_price` included? Please clarify the selection criteria and execution process, ideally in an appendix.\n- **Code availability:** Why is the submitted code not accessible? Transparency here is crucial for validation.\n- **Concealment levels:** Could the authors provide one example per *concealment* level? The annotation procedure for both concealment and complexity levels remains underexplained.\n- **Post-execution refusals:** How are these judged? What are the precise inputs and outputs for evaluation?\n- **Terminology consistency:** The paper uses *static* and *pre-execution* interchangeably, as well as *dynamic* and *in-workflow*. Since all evaluations appear to involve static text inputs, maintaining consistent terminology (e.g., *pre-execution* vs. *in-workflow*) would greatly improve clarity.\n- **Related work:** The following appear to be missing and should be discussed for completeness:\n    - https://arxiv.org/abs/2412.15701\n    - https://arxiv.org/abs/2409.16427"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YCMASiTu09", "forum": "0TNomhjCCi", "replyto": "0TNomhjCCi", "signatures": ["ICLR.cc/2026/Conference/Submission23596/Reviewer_ajqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23596/Reviewer_ajqH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992872793, "cdate": 1761992872793, "tmdate": 1762942728657, "mdate": 1762942728657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OASIS benchmark to evaluate LLM agent safety along two dimensions simulateneously, intent concealment (how well malicious intent is hidden) and task complexity (e.g. length of tool chains). It investigates how these two orthogonal factors affect agent’s safety alignment and execution or refusal. Authors test several SOTA LLM in their defined realistic and idealized scenarios and identify that safety alignment degrade proportionally with intent concealment and a complexity paradox, where agents appear safer on complex tasks, but maybe due to capability limitation, rather than safety reason. Authors also study different LLMs show static and dynamic refusal."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a new two-dimensional benchmark with per-step harm labels. The involvement of domain-experts and double verification by authors in benchmark preparation is good.\n\n2.\tThe paper shows how concealed intent and task complexity jointly affect safety at different levels of either one, which seems logical for making comprehensive decision about agent’s safety capability than unidirectional measurement.\n\n3.\tThe benchmark is evaluated on 8 different LLMs and identified several interesting phenomenon such as Complexity-Safety Tradeoff, static-dynamic refusal decision etc."}, "weaknesses": {"value": "1.\tThe paper states that “all tasks were synthesized using Gemini 2.5 Pro,” but does not clarify how Gemini generated these tasks, or what prompting or control strategy was used. Overall, how they were generated. Without proper justification, it’s difficult to assess whether the resulting tasks are realistic or represent an actual real-world scenario.\n\n2.\tAlthough the evaluations are well-organized, with the small benchmark, it’s difficult to say if the findings are actually statistically generalizable.\n\n3.\tThe paper primarily combines existing concepts into a new dataset and sandbox. While evaluations are interesting in their scope, some findings are somewhat intuitive, except maybe dynamic vs static refusal, but it’s narrow in scope."}, "questions": {"value": "Please answer the generalization and benchmark question in the weakness section,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VWjSoUuBsK", "forum": "0TNomhjCCi", "replyto": "0TNomhjCCi", "signatures": ["ICLR.cc/2026/Conference/Submission23596/Reviewer_dxu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23596/Reviewer_dxu3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016954561, "cdate": 1762016954561, "tmdate": 1762942728280, "mdate": 1762942728280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to all Reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank you for your time and the constructive feedback provided on our submission. We value your insights, which have been instrumental in refining the clarity, rigor, and depth of our work.\n\nWe have carefully revised the manuscript to address the concerns raised by all reviewers. To facilitate your review, we have formatted the revisions as follows:\n\n1. **In the Main Text:** We have incorporated specific clarifications and additional details (e.g., definitions in Section 4.1, experimental setups). These local updates are marked in blue text.\n    \n2. **In the Appendix:** We have added some new content, including the more detailed Data Synthesis Protocol (some subsections in Appendix A) and qualitative Case Studies (Appendix B & C). The titles of these new sections are marked in blue to indicate these are newly added modules.\n    \n\n**Summary of Major Updates:**\n\n- **Enhanced Transparency on Dataset Creation:** Detailed disclosure of our Human-in-the-Loop protocol (Appendix A).\n    \n- **Qualitative Analysis:** New case studies illustrating \"Safety Failure\" and \"Dynamic Refusal\" behaviors to complement our statistical findings (Appendix B & C).\n    \n- **Clarification of Core Concepts:** Refined definitions of \"ground-truth plan\" and the distinction between \"static\" vs. \"dynamic\" refusals (Section 4.1).\n    \n\nWe have provided detailed, point-by-point responses to each reviewer below, citing the specific line numbers in the revised manuscript where these changes can be found.\n\nBest regards,\n\nThe Authors"}}, "id": "io1TLS0RP0", "forum": "0TNomhjCCi", "replyto": "0TNomhjCCi", "signatures": ["ICLR.cc/2026/Conference/Submission23596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23596/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission23596/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763702377539, "cdate": 1763702377539, "tmdate": 1763702377539, "mdate": 1763702377539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}