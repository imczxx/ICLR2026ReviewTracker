{"id": "gHEm8A4zsJ", "number": 22076, "cdate": 1758325660092, "mdate": 1759896887777, "content": {"title": "GAT++: Adaptive Relation-Aware Graph Attention Networks", "abstract": "Leveraging high-order structural semantics in knowledge graphs (KGs) is critical for modeling complex user preferences in recommendations. However, during multi-hop propagation, semantic noise arising from heterogeneous relation distributions obscures meaningful preferences, making it challenging to learn robust user-item representations. To address this challenge, we propose GAT++, a novel graph convolutional network that integrates relation-aware attention mechanisms with contrastive denoising regularization to learn robust and expressive user-item representations. At its core, GAT++ introduces an adaptive attention module that captures multiple semantic relation spaces by projecting entities into relation-specific subspaces and learning distinct relation weight distributions. To further suppress noise from high-order message passing, we introduce a contrastive regularizer that leverages multi-relation subgraph variants to enforce consistency across augmented views. Moreover, we develop a personalized denoising encoder that dynamically refines user-item representations end-to-end, removing the need for external data generation modules. We evaluate GAT++ on extensive real-world datasets across music, literature, and food domains. GAT++ achieves up to 34.81% improvement in Recall@N over strong baselines, demonstrating its effectiveness and generalizability across diverse recommendation scenarios.", "tldr": "", "keywords": ["Novel View Synthesis", "Dynamic Scene", "Gaussian Splatting"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59b79fdb113b5c79c00f6f32f522c5ad8b30fdbc.pdf", "supplementary_material": "/attachment/4c0e845d5b723d4503c95996f7189768a608cbb2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper propose a graph attention framework designed to improve knowledge-graph-based recommender systems by addressing high-order semantic noise. The method combines:\n\t1. Relation-aware multi-space attention, projecting entities into relation-specific subspaces,\n\t2. Contrastive denoising regularization, aligning subgraph variants,\n\t3. A personalized denoising encoder, a Transformer-style module that refines user-item representations.\nExperiments on three public datasets (Last.FM, Book-Crossing, Dianping-Food) show large gains over baselines such as KGAT, CKAN, and DR4SR+."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an important problem, semantic noise and heterogeneity in multi-relational KGs for recommendation.\n2. The overall architecture is coherent and modular, combining relational attention, contrastive regularization, and sequence-level denoising.\n3. The empirical results are strong and consistent across multiple datasets and metrics (AUC, F1, Recall@K, NDCG).\n4. The ablation studies are well-structured and demonstrate that each proposed component contributes positively."}, "weaknesses": {"value": "1. **Limited Novelty.**  \n  The main contributions appear to be incremental combinations of existing approaches.  \n  Relation-specific projections or similar concepts have been already explored in *CompGCN*, *KGIN*, and *KGAT*.\n  Contrastive denoising follows the paradigm of *SGL (SIGIR 2021)* and *KGCL (SIGIR 2022)*.\n  and the personalized denoising encoder resembles *BERT4Rec* or *DIEN*.  \n  The claim of being the “first to introduce relation attention weight distributions” is not substantiated.\n\n2. **Methodological Vagueness.**  \n  Core equations (1)–(4) lack precision.  \n  It is unclear how projection matrices \\(M_r\\) interact with entity embeddings (additive or multiplicative).  \n  The \"adaptive saliency mechanism\" is not formally defined, and the sampling of \"subgraph variants from salient relations\"\n  lacks reproducible specification.  \n  The role and placement of the denoising encoder within the GAT layers remain ambiguous.\n\n3. **Implausible Empirical Gains.**  \n  Reported improvements (e.g., +424% Recall@N over GAT) are unusually large for this domain.  \n  No variance, seed averaging, or significance testing details are provided despite repeated \"p < 0.01\" claims.  \n  Runtime and scalability analyses are missing even though the model adds computationally heavy components.\n\n4. **Incomplete Baselines and Fairness.**  \n  The paper omits strong recent baselines such as *LightGCN*, *SimGCL*, *NCL*, and *KGCL*.  \n  *DR4SR+* is not a directly comparable baseline as it targets sequential recommendation rather than KG reasoning.  \n  Hyperparameter tuning fairness is not discussed.\n\n5. **Clarity and Presentation Issues.**  \n  The text overuses vague terms such as \"fine-grained semantics\" and \"semantic robustness\" without clear definitions.  \n  Notation inconsistencies (e.g., \\(L_{Noise}\\) vs. \\(L_{Noize}\\), \\(e_u^{R0}\\) vs. \\(e_u^{r_n}\\)) hinder readability.  \n  Figures are schematic but fail to illustrate the precise data flow or architectural layering.\n\n6. **Insufficient Analysis and Interpretation.**  \n  The paper lacks visualization or interpretation of learned attention weights, qualitative case studies, or  \n  error analyses that could clarify why the model performs better.\n\n7. **No Theoretical Justification.**  \n  Claims such as \"contrastive regularization maximizes mutual information\" are not derived or proven.  \n  The discussion of \"robustness to high-order noise\" remains purely empirical and lacks formal analysis."}, "questions": {"value": "1. How are the \"relation-specific subspaces\" and \"salient relations\" concretely defined and implemented?  \n2. How many contrastive views per node are sampled, and what is the computational cost of this procedure?\n3. Were improvements verified over multiple random seeds?  Please include standard deviations in all tables.\n4. How do you prevent potential data leakage between KG triples and user–item interactions?\n5. Have you compared with modern contrastive recommendation baselines such as *SimGCL*, *NCL*, or *KGCL*?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ro5SN7Le9W", "forum": "gHEm8A4zsJ", "replyto": "gHEm8A4zsJ", "signatures": ["ICLR.cc/2026/Conference/Submission22076/Reviewer_FaRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22076/Reviewer_FaRs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919027997, "cdate": 1761919027997, "tmdate": 1762942055503, "mdate": 1762942055503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GAT++, a relation-aware graph attention framework for knowledge-enhanced recommendation. It projects entities into relation-specific subspaces with adaptive attention, adds a contrastive denoising regularizer built from salient relation-subgraph variants, and introduces a personalized denoising encoder trained end-to-end. Experiments on Last.FM, Book-Crossing, and Dianping-Food report consistent gains, including a cold-start setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The encoder is described with a Transformer formulation, eliminating external generators and aligning with task objectives."}, "weaknesses": {"value": "Mathematical/notation consistency issues: Mixed “Lnoize” (Eq. (5)) vs “LNoise” (Fig. 2), and ambiguous indexing in Eq. (4); \n\nOutdated baseline coverage (2019–2020 only): As reported gains are only established over pre-2021 baselines, the experimental section currently provides limited evidence for contemporary competitiveness; stronger conclusions would require including recent SOTA baselines or justifying their omission."}, "questions": {"value": "Could you precisely specify how the “relation-subgraph views” are constructed and sampled? In particular, (a) how are “relation variants” defined per layer and what saliency score selects the “top two” variants; (b) what is the negative-sample distribution (in-batch vs. memory queue) and the value/sensitivity range of the temperature τ"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k7vcsRdmcu", "forum": "gHEm8A4zsJ", "replyto": "gHEm8A4zsJ", "signatures": ["ICLR.cc/2026/Conference/Submission22076/Reviewer_EJpo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22076/Reviewer_EJpo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959420611, "cdate": 1761959420611, "tmdate": 1762942055018, "mdate": 1762942055018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GAT++, a graph attention network for knowledge graph-enhanced recommendation systems that addresses semantic noise during multi-hop propagation. The method features three components: (1) adaptive relation-aware attention with relation-specific projections, (2) contrastive denoising regularization using multi-relation subgraphs, and (3) a personalized denoising encoder. Experiments on three datasets show up to 34.81% improvement in Recall@N over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a novel relation-aware attention mechanism that explicitly models multiple semantic relation spaces through learnable projection matrices, enabling fine-grained discrimination among heterogeneous relational dependencies in knowledge graphs.\n\n2. Extensive experiments have been conducted across three diverse datasets from different domains (music, literature, and food), with comprehensive comparisons against multiple state-of-the-art baseline methods and thorough ablation studies demonstrating statistical significance.\n\n3. The effectiveness of individual components has been well validated through systematic ablation studies, showing that each proposed module (adaptive attention, contrastive denoising, and personalized encoder) contributes meaningfully to the overall performance improvements."}, "weaknesses": {"value": "1. The paper's title, abstract, and introduction fail to clearly specify that this is recommendation system research, creating significant confusion for readers. While the methodology section and experiments clearly focus on user-item recommendation tasks, the early sections present the work as general graph neural network research, which is misleading given the task-specific nature of the proposed solutions.\n\n2. The paper proposes modifications to GAT architecture by introducing relation-specific projections and multi-space attention mechanisms. However, GAT has been extensively studied for many years with numerous architectural variants proposed. It is unclear how this paper makes a significant contribution to the already rich literature of GAT architecture designs, particularly given that the core innovation appears to be relatively incremental adaptations for recommendation scenarios."}, "questions": {"value": "1. Why doesn't the paper clearly identify itself as recommendation system research in the title and early sections?\n\n2. What are the fundamental technical contributions beyond existing GAT architectural variants that justify publication in a top-tier venue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RgH0ApIHOX", "forum": "gHEm8A4zsJ", "replyto": "gHEm8A4zsJ", "signatures": ["ICLR.cc/2026/Conference/Submission22076/Reviewer_kmrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22076/Reviewer_kmrg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168763881, "cdate": 1762168763881, "tmdate": 1762942054577, "mdate": 1762942054577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}