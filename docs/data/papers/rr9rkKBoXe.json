{"id": "rr9rkKBoXe", "number": 11903, "cdate": 1758204554740, "mdate": 1759897547660, "content": {"title": "Adaptive TD-Lambda for Cooperative Multi-agent Reinforcement Learning", "abstract": "Recent advancements in multi-agent reinforcement learning (MARL) have prominently leveraged Temporal Difference Lambda, TD($\\lambda$), as a catalyst for expediting the temporal difference learning process in value functions. TD($\\lambda$) in value-based MARL algorithms or the Temporal Difference critic learning in Actor-Critic-based (AC-based) algorithms synergistically integrate elements from Monte-Carlo simulation and Q function bootstrapping via dynamic programming, which effectively addresses the inherent bias-variance trade-off in value estimation. Based on that, some recent works link the adaptive $\\lambda$ value to the policy distribution in the single-agent reinforcement learning area.However, because of the large joint action space, the large observation space, and the limited transition data in Multi-agent Reinforcement Learning, the computation of policy distribution is infeasible to be calculated statistically.\nTo solve the policy distribution calculation problem in MARL settings, we employ a parametric likelihood-free density ratio estimator with two replay buffers instead of calculating statistically. The two replay buffers of different sizes store the historical trajectories that represent the data distribution of the past and current policies correspondingly. Based on the estimator, we assign Adaptive TD($\\lambda$), \\textbf{ATD($\\lambda$)}, values to state-action pairs based on their likelihood under the stationary distribution of the current policy. \nWe apply the proposed method on two competitive baseline methods, QMIX for value-based algorithms, and MAPPO for AC-based algorithms, over SMAC benchmarks and Gfootball academy scenarios, and demonstrate consistently competitive or superior performance compared to other baseline approaches with static $\\lambda$ values.", "tldr": "", "keywords": ["Multi-agent Reinforcement Learning", "Adaptive TD-Lambda", "Likelihood-Free Density Ratio", "Parametric Importance Sampling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/776ef03e3387d1898e915ce17f053163819aa4cd.pdf", "supplementary_material": "/attachment/348c87659c082143e9883c8c4d6273d3ff3482aa.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed ATD($\\lambda$), a novel method to automatically set $\\lambda$ for TD learning. To do so an estimation network is used, which is trained to compute the $f$-divergence between an off-policy bigger dataset and an on-policy smaller one. The proposed method can be plugged in every method doing TD learning, both value-based algorithms and for critics in actor-critic methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "TD($\\lambda$) is a very popular algorithm for TD learning, and usually the setting of $\\lambda$ is both a delicate and sensible choice. Thus, being able to automatically determine such value is a valid contribution to the field. Also, I like the idea of using two different datasets to determine such value, to express the degree of off-policy-ness of a transition. Experimental results seem quite good, and provide a valid support to the claimed benefits of the method."}, "weaknesses": {"value": "The paper itself does not look great: the explanations are often quite convolved and difficult to follow, mixing mathematical details with implementation ones. The space given to the exposition of your own proposed method is very short. Also, in terms of experimental results, quite often you are mentioning results that are not included in the main paper, and this is not a good practice."}, "questions": {"value": "- The paragraph explaining the $\\lambda$-return extension in the Background is quite difficult to actually understand. I would suggest to rephrase it to be a lot clearer and more detailed.\n\n- Most of Section 4.1 does not seem very useful: QMIX has already been introduced in the Background, and not being a contribution of yours, I do not see why you employ an entire page to get into so many details (including implementation ones, like config files, which are not extremely useful in a ML paper at this stage) for something that is not really your work...\n\n- It is not very clear how Equation (8) follows from (7) when we get $f$ to be the Jensen-Shannon divergence. Perhaps, given that this is one of the focal point of the paper, it would deserve a more detailed explanation.\n\n- You mention actor-critic methods quite a lot in the introduction of the paper, and claim that you are combining your ATD($\\lambda$) with MAPPO. However, I do not see any explanation on how this integration is done. For example, you say that your method requires minimal changes (i.e., only to add the on-policy small buffer), but this holds true for value-based methods (which are off-policy), but not for actor-critic algorothms like MAPPO. For those for example you need to add both buffers, resulting in quite some additional resources usage.\n\n- Figure 4 is partly covering the caption of Figure 3. You probably modified some margins to get them to fit, but this should not be done...\n\n- $Q$-learning based methods like QMIX are off-policy, but they do not require importance sampling. Why did you decide to integrate it as one of the baselines? How is it used in there?\n\n- In Figure 5 caption, you say that MMM2 is a super-hard map, but before you said it is only hard. Which is the correct one?\n\n- Actually, I do not see any result for MAPPO in the paper... Then why do you claim you have extended your ATD($\\lambda$) to this as well?\n\n- The same goes for results on GRF: there is none in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V8wFR0dSXb", "forum": "rr9rkKBoXe", "replyto": "rr9rkKBoXe", "signatures": ["ICLR.cc/2026/Conference/Submission11903/Reviewer_MhPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11903/Reviewer_MhPt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760626775327, "cdate": 1760626775327, "tmdate": 1762922915124, "mdate": 1762922915124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ATD($\\lambda$): a classifier-based, likelihood-free scheme that learns a per-transition weight $\\omega$ from a pair of replay buffers \n(recent on-policy vs older off-policy), maps $\\omega$ via a sigmoid to a per-transition $\\lambda$, and plugs this adaptive trace into TD($\\lambda$) targets for centralized critics in MARL (QMIX, MAPPO). \nExperiments on SMAC and GRF show empirical improvements over several fixed-$\\lambda$ baselines and some off-policy correction methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and practical motivation: tuning $\\lambda$ in large, non-stationary multi-agent settings is hard; an automatic per-transition scheme is appealing.\n\n- The method is modular and easy to insert into existing CTDE pipelines (QMIX, MAPPO), which increases engineering utility.\n\n- Empirical evaluation covers multiple MARL benchmarks and includes several ablations and baselines.\n\n- The authors include derivations and attempt to provide theoretical justification for contraction and stability under certain assumptions."}, "weaknesses": {"value": "1. The core mechanism — a classifier to distinguish recent vs older replay samples, using that output as a per-transition weight and mapping it to $\\lambda$ — is not inherently multi-agent. \nIt can be applied identically in single-agent RL. \nThe paper’s contributions are largely (a) the idea of learned per-transition $\\lambda$ via a two-buffer classifier and (b) engineering how to integrate it into CTDE critics. \nThere are no algorithmic changes that exploit MARL-specific structure.\nThis may lead readers to suspect that MARL was chosen primarily because its baselines are easier to improve rather than because the problem requires a multi-agent treatment.\n\n2. The paper treats $\\omega$ (sigmoid of classifier output) as the adaptive $\\lambda$ and argues intuitively that on-policy samples should get larger $\\lambda$ and off-policy smaller $\\lambda$. \nHowever, there is no formal definition of an optimal $\\lambda$ (e.g., minimizing TD error MSE, minimizing update variance, maximizing sample efficiency) and no proof that the classifier-derived $\\lambda$ approximates such an optimum. \nThe classifier learns a surrogate “on-policyness” score, not the $\\lambda$ that would minimize any well specified loss.\n\n3. The paper emphasizes that the method relates to off-policy correction (importance sampling), but the equivalence or relationship between the proposed objective and classical importance weighting is not clearly established.\nIt remains unclear whether the resulting target is unbiased or systematically biased by the sigmoid mapping and classifier errors.\n\n4. Classifier-based density ratio estimation is known to be high-variance or biased in regimes with high overlap or low support.\nThe paper does not provide implementation details (update frequency, batch balancing, network capacity, regularization, etc.) nor quantitative diagnostics showing how classifier quality affects critic performance.\nThe observation that $\\lambda$ collapses to 0 in highly stochastic settings (e.g., SMACv2) is mentioned but not analyzed in depth."}, "questions": {"value": "1. Can the authors theoretically or empirically demonstrate that the learned λ leads to a bias–variance tradeoff closer to an optimal value (e.g., minimizing TD error MSE)?\n\n2. Please include a clear pseudocode or algorithm box describing the full training loop, including how the two replay buffers are maintained, how the classifier is trained, and how λ is cached or updated.\n\n3. A simple λ-annealing baseline (e.g., linearly or exponentially decaying λ as in [1] could serve as a stronger and more computation-efficient comparison.\n\n[1] Revisiting Cooperative Off-Policy Multi-Agent Reinforcement Learning. ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wh0eV9knet", "forum": "rr9rkKBoXe", "replyto": "rr9rkKBoXe", "signatures": ["ICLR.cc/2026/Conference/Submission11903/Reviewer_nB5g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11903/Reviewer_nB5g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015349513, "cdate": 1761015349513, "tmdate": 1762922914712, "mdate": 1762922914712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ATD (Adaptive TD Lambda) which uses a smaller on-policy buffer to compute the off-policyness of the sampled trajectories. ATD learns a separate network $\\omega_\\phi$ which estimates the density ratio with a lower bound objective. Experimental results are provided for SMACv1 and GRF."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The approach is relatively simple, and adjusting $\\lambda$ automatically with the density ratio computed from the two replay buffers is a nice idea. However, as I mention below in Weaknesses, the authors need to clarify the line between ideas/contributions from previous work and ATD."}, "weaknesses": {"value": "1. The contributions of this work are not clear. There are some phrases like “inspired by” Sinsha et, al. 2022, Grover et, al. 2019), or Hu et, al. 2021. but it is not clear from the text what the main contribution is or what the challenge is of applying these techniques to MARL. \n1. Related to the first point, if the off-policyness can be computed over states and joint actions, then it is a simple application of previous techniques to MARL. There is nothing specific to MARL which makes it hard to apply these techniques, such as factorized policies or the exponential complexity of the joint state/action space.\n1. The experiments are limited to saturated benchmarks such as SMAC and GRF. Although the authors acknowledge that it cannot be applied well to SMACv2, that is a significant limitation as it is recently a more standard benchmark than SMACv1. The reason for this limitation is not provided in Appendix C (as they mention in the Conclusion). I suggest the authors go deeper into why this limitation persists, as it might provide hints for some challenges in MARL, and later some additional modifications to the current approaches which will be more novel in the MARL context.\n1. In the background section, the “true state of the environment” in line 132 is not related to centralized training. The Dec-POMDP is a formulation, and centralized training is a choice on how to train policies. Similar comment holds for line 137 where partial observability is a characteristic of the formulation/setup of the environment, and it is not related to decentralized execution. Please separate these  in the text as it will confuse readers.\n1. The observations $z$ for line 136 should have agent Id conditioning. \n1. The notation $T$ is used twice to indicate the observation-action history and the target value in Eq.1."}, "questions": {"value": "1. What is the connection between off-policyness and $\\lambda$? What is the justification behind Eq. 9-10? My understanding is that if the density ratio $\\omega$ is high, we have higher off-policyness and thus we would want to go more towards TD(0), but it seems like the reverse is happening in Eq. 9-10. \n1. Please address my comments in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lIuJVORZxn", "forum": "rr9rkKBoXe", "replyto": "rr9rkKBoXe", "signatures": ["ICLR.cc/2026/Conference/Submission11903/Reviewer_R6oj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11903/Reviewer_R6oj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467683727, "cdate": 1761467683727, "tmdate": 1762922914236, "mdate": 1762922914236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called ATD(λ) for adaptively determining the λ value in TD(λ) for multi-agent reinforcement learning (MARL). In traditional methods, λ is a hyperparameter that needs to be preset, whereas this paper's method dynamically assigns appropriate λ values for each transition by calculating the likelihood of state-action pairs under the current policy. The authors use a likelihood-free density ratio estimator and two replay buffers of different sizes (one for off-policy data and one for approximating on-policy data) to estimate the \"on-policy degree\" of sampled transitions. The method was evaluated on SMAC (StarCraft Multi-Agent Challenge) and Google Football tasks, showing improved performance compared to fixed λ values."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important hyperparameter tuning issue in MARL by making the λ value adaptive rather than fixed, reducing the burden of manual parameter tuning.\n\n2. The proposed method applies to different types of MARL algorithms, including value-based algorithms (like QMIX) and actor-critic-based algorithms (like MAPPO), demonstrating good adaptability.\n\n3. The authors provide a detailed theoretical analysis (Section 4.2 and Appendix A), connecting their method with f-divergence and importance sampling, providing solid theoretical foundations for the proposed algorithm."}, "weaknesses": {"value": "1. In Figure 3, the authors show winning rate curves but do not sufficiently explain why adaptive λ values lead to performance improvements. There's a lack of intuitive explanation of how λ values affect learning dynamics.\n\n2. The text in the picture is too small; even when enlarged, it’s still unreadable. In Figure 4, the curves are difficult to distinguish, especially in the super-hard task scenarios, making it challenging to interpret performance comparisons.\n\n3. Figure 6 shows that the ratio between on-policy and off-policy buffer sizes is a critical hyperparameter with a significant impact. The authors chose 50x as the default ratio based on empirical observation, but did not provide theoretical guidance or sensitivity analysis to support this choice.\n\n4. The results in Figure 10b show inconsistent effects of applying importance weights across different scenarios - helpful in 10m_vs_11m but detrimental in 6h_vs_8z. This suggests the method may not be robust, but the authors don't provide clear guidance on when to apply importance weights."}, "questions": {"value": "1. How does the method scale to environments with more agents? The largest test case appears to be 27 agents vs. 30, but how would it perform with hundreds of agents?\n\n2. How much does the network architecture of the λ predictor influence performance? Would simpler or more complex networks affect performance?\n\n3. Can this adaptive λ method extend beyond cooperative MARL to other RL paradigms, such as competitive or mixed cooperative-competitive settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ud4tVubHaO", "forum": "rr9rkKBoXe", "replyto": "rr9rkKBoXe", "signatures": ["ICLR.cc/2026/Conference/Submission11903/Reviewer_7bbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11903/Reviewer_7bbf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887178500, "cdate": 1761887178500, "tmdate": 1762922913801, "mdate": 1762922913801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}