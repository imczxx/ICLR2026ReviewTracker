{"id": "g9q9uzedDd", "number": 7977, "cdate": 1758047824166, "mdate": 1762926206692, "content": {"title": "Towards Principled Design for Graph Neural Networks Through Governing Law of Dynamic Learning Behavior", "abstract": "Graph Neural Networks (GNNs) have been extensively evaluated in the machine learning regime. In contrast to most studies that primarily focus on the empirical assessment of model performance across different graph datasets, we ratchet the gear of GNN benchmarking another notch forward to understand the graph learning mechanism that shapes characteristic learning behaviors of each GNN model.\nSpecifically, we introduce a comprehensive benchmark framework `PDEGNN-Bench` to evaluate GNNs derived from six representative governing equations, i.e., partial differential equations (PDEs), for graph heat isotropic/anisotropic diffusion, non-local diffusion, reaction–diffusion, Hamiltonian system, wave transport, and oscillatory synchronization. By linking each GNN model instance to its corresponding governing equation, we establish new insights into the design principle for new GNNs by understanding the relationship between mechanistic interpretations and descriptive learning performance. \nTo that end, we seek to explore two fundamental questions: (1) How well does each governing equation respond to the challenge of over-smoothing in GNNs? (2) How does the homogeneity degree of graph topology influence model performance across PDE families? \nTaken together, our benchmark provides a systematic evaluation of leading GNN models through the lens of underlying physical mechanisms. \nThrough well-designed experiments, we demonstrate that each family of governing equations exhibits distinct model generalization and interpretability characteristics, offering guidance for designing suitable GNNs for the new graph data.", "tldr": "", "keywords": ["Graph learning", "PDE-informed GNNs", "Oversmoothing", "Graph Homophily"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/47eeb30626fd68f3d20e8f8f1fbb71f86118445e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a summary of over-smoothing performance for several types of PDE-based GNN models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "A summary of some of the major PDE-based GNN models under two over-smoothing metrics is presented in a single paper. This can serve as a reference for future researchers."}, "weaknesses": {"value": "1. The benchmarking should ideally focus on **mechanisms** (not classes of PDEs) that are specifically designed to mitigate over-smoothing, with insights or explanations of why they work or do not work, or under what situations they work best or collapse. This is not done in this work. For example, over-smoothing mitigation mechanisms like those in DREW, FROND, $\\omega$GNN, or other recent models that do try to address over-smoothing should be analyzed and discussed. The paper claims to evaluate leading GNN models (I assume in the over-smoothing mitigation domain) but falls short of this objective.\n\n1. Although the paper summarizes the over-smoothing mitigation performance of several classes of PDE-informed GNNs, it has not provided good insights and understanding of the effectiveness of over-smoothing mechanisms. Over-smoothing mitigation mechanisms must be designed to specifically target this phenomenon, which is inherent in all message-passing aggregation type of GNNs. Some of the GNN models presented are **not** suitable to mitigate over-smoothing (based on our current understanding of GNNs, not on what is claimed at the time the works are published, a hint is to look for theoretical guarantees): these models are more helpful for other aspects, e.g., some mitigates the heterophily issue or different geometries. To present these models in this over-smoothing benchmarking context does not provide the reader any significant insights into what works for mitigating over-smoothing and may in fact mislead new researchers into thinking that these models are not effective, when in fact they are good for targeting a different issue. There is a possibility to integrate the over-smoothing mechanisms in W1 into a model to obtain a model that mitigates over-smoothing **AND** other issues. It would be interesting to see what works and what not.\n\n1. The novelty of this work is lacking as it is benchmark proposal. Furthermore, the metrics used are not new and taken from existing literature."}, "questions": {"value": "1. It is unclear if the benchmarking has been done with the best hyperparameter and other options like neural network architecture choices in each of the PDE neural GNN models. The results shown are typically worse than those reported in the other papers, except for BRICK.\n\n1. The work \"Let Brain Rhythm Shape Machine Intelligence for Connecting Dots on Graphs\" (the BRICK model) has not been published as at the time of this review (15 Oct 2025), nor is it available on any public repository like arXiv on and before 15 Oct 2025. This is a serious authorship anonymity leakage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sgdHHR921T", "forum": "g9q9uzedDd", "replyto": "g9q9uzedDd", "signatures": ["ICLR.cc/2026/Conference/Submission7977/Reviewer_82R6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7977/Reviewer_82R6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760506527709, "cdate": 1760506527709, "tmdate": 1762919987093, "mdate": 1762919987093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "9xaV7jKkgG", "forum": "g9q9uzedDd", "replyto": "g9q9uzedDd", "signatures": ["ICLR.cc/2026/Conference/Submission7977/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7977/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762926205515, "cdate": 1762926205515, "tmdate": 1762926205515, "mdate": 1762926205515, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDEGNN-BENCH, a comprehensive benchmark designed to study GNNs inspired by Partial Differential Equations. Unlike prior empirical benchmarks, it grounds GNN evaluation in dynamical systems theory by linking each architecture to a corresponding governing equation—covering six canonical forms: isotropic/anisotropic diffusion, non-local diffusion, reaction–diffusion, Hamiltonian, wave, and oscillatory synchronization dynamics. Using seven datasets spanning a homophily range of 0.11–0.81, the framework investigates two key behaviors: oversmoothing (depth-induced representation collapse) and homophily sensitivity (topology-induced performance variance). New diagnostics—effective rank and class-mix score —quantify these effects. The study yields a model-agnostic oversmoothing alarm and regression-based homophily sensitivity index. The benchmark reveals distinct scaling and robustness patterns across PDE families and provides a theoretical lens for principled GNN design"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and straightforward writing. The article's argument is well-defined.\n2. Six governing PDE classes offer a structured, interpretable organization of existing PDE-GNNs.\n3. Provides a dataset-level prior requiring no label tuning, aligning with the paper’s “principled design” goal.\n4. Provides a theoretical bridge between continuous dynamics and discrete message-passing mechanisms, supporting principled model design."}, "weaknesses": {"value": "1. All benchmarks rely on small citation/social graphs, lacking evaluation on larger or dynamic graphs.\n2. No direct evidence found for runtime or memory profiling (Appx A.2 Figure 4 mentions inference time but lacks analysis).\n3. While the envelope heuristic (Fig. 3a) is visually convincing, there is no metric quantifying alarm accuracy or depth-stop saving\n4. No direct evidence found for performance improvement when the alarm is applied during training.\n5. PDE-governed models (especially non-local and Hamiltonian types) may entail higher complexity, yet wall-clock or GPU-memory cost is not reported (No direct evidence found).\n6. No code has been provided for readers to reproduce.\n7. Error in line 267: E_W -> E_b."}, "questions": {"value": "Reference Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lTf2Lr3M4R", "forum": "g9q9uzedDd", "replyto": "g9q9uzedDd", "signatures": ["ICLR.cc/2026/Conference/Submission7977/Reviewer_otPJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7977/Reviewer_otPJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760587467253, "cdate": 1760587467253, "tmdate": 1762919986742, "mdate": 1762919986742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDEGNN-BENCH, a benchmark framework for systematically evaluating Graph Neural Networks governed by different partial differential equations (PDEs). The authors compare six PDE families (diffusion, non-local diffusion, reaction-diffusion, Hamiltonian, wave transport, oscillatory synchronization) across seven datasets with varying homophily levels (h ∈ [0.11, 0.81]). The paper proposes two main contributions: (1) a dual-metric oversmoothing detection system using effective rank and class-mix score, and (2) a model-agnostic oversmoothing alarm based on dataset-specific envelopes derived from validation optima. The authors claim to reveal distinct oversmoothing behaviors and homophily sensitivities across PDE families, providing guidance for principled GNN design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel systematic comparison framework across PDE families fills an important gap in the literature.\n\n2. Comprehensive experimental design covers representative models from each PDE family with unified evaluation protocols.\n\n3. Generally well-written with clear problem motivation and systematic presentation of results.\n\n4. The systematic comparison could provide valuable guidance for practitioners, and the oversmoothing detection tools may have practical utility despite methodological limitations."}, "weaknesses": {"value": "1. Experimental Scope Limitations:\n  - Only 7 datasets, heavily biased toward citation networks\n  - Missing key application domains (molecular graphs, social networks) where PDE interpretations have physical meaning\n  - No synthetic graphs with controlled properties to isolate specific effects\n  - Insufficient diversity to support broad claims about \"principled design\"\n\n2. Methodological Issues:\n  - Oversmoothing alarm relies on circular definition using existing model optima\n  - No validation that envelope method actually predicts oversmoothing vs. mere difference\n  - Homophily analysis conflates multiple dataset properties (size, domain, density)\n  - Integration time ≠ discrete layers equivalence unvalidated\n\n3. Statistical Rigor:\n  - No significance testing for performance differences\n  - Linear regression with only 7 data points lacks power\n  - Missing confidence intervals and effect size analysis\n  - No correction for multiple comparisons across models\n\n4. Technical Implementation:\n  - Limited hyperparameter search (only 3 learning rates)\n  - Missing crucial implementation details for reproducibility\n\n5. Theoretical Depth:\n  - Lacks rigorous theoretical analysis connecting PDE properties to observed behaviors\n  - Explanations remain largely post-hoc empirical observations\n  - No predictive framework for selecting appropriate PDE families"}, "questions": {"value": "1. Oversmoothing Alarm Validation:\n  - How does the envelope method perform on architectures not included in the envelope construction?\n  - Can you provide validation against ground-truth oversmoothing scenarios?\n  - What is the false positive rate when the alarm stops beneficial training?\n\n2. Depth Equivalence:\n  - Have you empirically validated that integration time T corresponds to T discrete layers?\n  - How do different ODE solvers affect your conclusions?\n  - Why fix the step size at 1.0 rather than optimize it?\n\n3. Homophily Analysis:\n  - How do you disentangle homophily effects from confounding variables (size, domain, density)?\n  - Why assume linear relationships rather than testing for non-linear effects?\n  - Can you replicate findings on synthetic graphs with controlled homophily?\n\n4. Statistical Significance:\n  - Are the observed performance differences statistically significant?\n  - What are the confidence intervals for the β₁ slopes in Figure 3b?\n  - How does multiple testing correction affect your conclusions?\n\n5. Generalization:\n  - How confident are you that conclusions from citation networks generalize to other domains?\n  - Would results hold on larger graphs (>10K nodes)?\n  - Can you predict optimal PDE choice given graph properties?\n\n6. Implementation Details:\n  - How were \"default hyperparameters\" chosen for each model?\n  - What are the computational cost differences across PDE families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gbEvEJmRNN", "forum": "g9q9uzedDd", "replyto": "g9q9uzedDd", "signatures": ["ICLR.cc/2026/Conference/Submission7977/Reviewer_RY3x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7977/Reviewer_RY3x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813220791, "cdate": 1761813220791, "tmdate": 1762919986390, "mdate": 1762919986390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDEGNN-BENCH, a benchmark for evaluating GNNs governed by differential equations. It links physical dynamics (e.g., diffusion, wave, Hamiltonian) with learning behaviors like oversmoothing and homophily sensitivity. Six PDE-based GNN families are analyzed using two interpretable metrics—Effective Rank and Class-Mix Score—to measure expressiveness and separability. A model-agnostic oversmoothing alarm based on their joint statistical envelope is proposed. Experiments on seven datasets reveal how different governing equations influence depth scalability, stability, and structural bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A systematic, cross-equation comparison of PDE-based GNNs under unified conditions.\n\n2. A novel, model-agnostic \"oversmoothing alarm\" that can warn during training when a model is becoming too deep and losing expressive power.\n\n3. A quantitative assessment of how sensitive each PDE-based GNN is to the homophily level of the graph data."}, "weaknesses": {"value": "1. Despite rich analysis, the evaluation relies mainly on small datasets; assessment on much larger and more diverse graphs is needed to substantiate the conclusions.\n\n2. The set of ODE models is limited; additional baselines (e.g., [1], [2]) should be included.\n\n3. How sensitive is oversmoothing to the choice of numerical solver in PDE-GNNs (e.g., Euler, RK4, adaptive, Symplectic Euler)? Do solver-induced numerical dissipation/dispersion effects meaningfully alter (r_eff, S)?\n\n4. The Hamiltonian GNN in HamGNN does not strictly follow canonical Hamiltonian equations due to multiple layers. Would it be more appropriate to evaluate HANG from [2]?\n\n[1] Q. Kang et al., “Unleashing the potential of fractional calculus in graph neural networks with FROND,” ICLR 2024.\n\n[2] Zhao, K., Kang, Q., Song, Y., She, R., Wang, S., & Tay, W. P. (2023). Adversarial robustness in graph neural networks: A Hamiltonian approach. NeurIPS 36, 3338–3361."}, "questions": {"value": "See the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yOEaYis8Te", "forum": "g9q9uzedDd", "replyto": "g9q9uzedDd", "signatures": ["ICLR.cc/2026/Conference/Submission7977/Reviewer_PV8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7977/Reviewer_PV8N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922399692, "cdate": 1761922399692, "tmdate": 1762919986042, "mdate": 1762919986042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}