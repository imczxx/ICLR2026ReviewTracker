{"id": "lqJT6xmuH3", "number": 6608, "cdate": 1757990479260, "mdate": 1759897905363, "content": {"title": "Equilibrium Language Models", "abstract": "Large Language Models (LLMs) excel across diverse applications but remain impractical for edge deployment due to severe memory bottlenecks at the edge devices. We propose Equilibrium Language Models (ELMs), a novel compression framework that replaces groups of Transformer layers with a lightweight fixed-point network, reinterpreting deep computation as solving for an equilibrium state. To achieve ELMs, We introduce *Group Pruning Policy Optimization*, which automatically learns optimal pruning intervals. Moreover, we propose *One-Step KV-Cache*, which drastically reduces memory overhead by storing only the final iteration cache without compromising the accuracy, to enable effective deployment at the edge devices. Across different tasks such as common sense reasoning, mathematical problem solving, and code generation, ELMs prune 28\\% of parameters while retaining 99\\% of the accuracy of dense fine-tuned LLMs, establishing a new direction for memory-efficient edge deployment of large models.", "tldr": "", "keywords": ["Large Language Models", "Model Compression", "On-Device Inference", "Fixed-Point Network"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f31869a4a9695179b7f0019577eb52613c4e0bae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission proposes Equilibrium Language Models (ELMs), a novel compression framework for Transformer-based large language models targeting edge deployment. The core idea is to replace groups of consecutive Transformer layers with a lightweight fixed-point network, reinterpreting deep computation as solving for the equilibrium state of a fixed-point system. To enable ELMs, the authors introduce two key components: (1) Group Pruning Policy Optimization (GPPO), a reinforcement learning-based method to automatically identify optimal layer pruning intervals; and (2) One-Step KVCache, which reduces memory overhead by storing only the final iteration’s cache without accuracy loss. Experiments on Qwen2.5 and Llama3.2 models show that ELMs prune 28% of parameters while retaining 99% of the accuracy of dense fine-tuned LLMs across commonsense reasoning, mathematical problem-solving, and code generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work breaks from traditional LLM pruning (e.g., weight-level, head-level, or heuristic layer pruning) by leveraging fixed-point network theory (from Deep Equilibrium Models, DEQs) to replace layer groups. \n\n2. GPPO avoids the limitations of heuristic pruning metrics (cosine similarity, perplexity) by framing layer interval selection as a policy optimization problem. \n\n3. One-Step KVCache is a critical optimization for edge deployment"}, "weaknesses": {"value": "1. All experiments use LLMs with exactly 28 layers (Qwen2.5-1.5B/7B, Llama3.2-3B). It remains unclear how ELMs perform on models with different depths.\n\n2. The paper highlight the significance for edge deployment, however, the paper shows no data on latency results for edge devices. \n\n3. While One-Step KVCache reduces memory, the submission only reports KV cache size (Table 3) but not total inference memory (including model parameters, activations). Edge devices often have strict RAM limits (e.g., 4GB on low-end phones), so total memory data is essential."}, "questions": {"value": "1. What happens if the inherited Transformer layer is replaced with a simple FFN? Or if W_h is not initialized as an identity matrix? Such ablations would clarify which components drive performance retention.\n\n2. How do you choose the convergence threshold for One-Step KVCache? \n\n3. The paper mentions using Stochastic Jacobian-Free Backpropagation (SJFB) to reduce training cost , How does ELMs’ training time/GPU memory compare to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uwR0duvge", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_wdiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_wdiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857933678, "cdate": 1761857933678, "tmdate": 1762918929317, "mdate": 1762918929317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Equilibrium Language Models (ELMs), a new family of Transformer-based language models designed to reduce inference-time memory usage and improve computational efficiency without sacrificing performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-written and clearly structured; the mathematical formulations are concise and supported by clear intuition.\n\n- Novel modeling approach: Reformulating decoding as a fixed-point equilibrium problem is original and elegant.\n\n- Parameter efficiency: Demonstrates substantial parameter pruning without requiring fine-tuning."}, "weaknesses": {"value": "- While the paper emphasizes parameter pruning (~28%), it does not report key deployment metrics such as inference latency, memory consumption, or speedup over dense baselines. This makes it difficult to verify the real-world efficiency gains, especially since equilibrium solvers may incur higher per-step computation even if they reduce parameter count.\n\n- Although the paper presents an elegant formulation of Equilibrium Language Models (ELMs) and demonstrates strong pruning performance, it does not discuss the cost, stability, or convergence of obtaining these models. In particular, the Group Pruning Policy Optimization process likely introduces significant computational overhead, as it involves iterative search and training steps that may require multiple retraining cycles.\n\n- In Table 2, ELMs are not consistently better than all baselines under the same compression level. For instance, LLM-Streamline (Layer) outperforms ELMs on MMLU at M=8, and Sheared LLM performs better on HumanEval. This suggests that ELMs may not be universally superior. A more systematic comparison across different  M values and tasks is necessary to understand when ELMs are advantageous or potentially weaker than existing methods."}, "questions": {"value": "- What happens when the pruned layers are not contiguous? For example, instead of pruning 12 consecutive layers, what if we prune two separate groups of six layers distributed across the model? \n\n\nI am open to discussing this further during the rebuttal and will be happy to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ULLK3v0fH4", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_Vx6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_Vx6z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951808665, "cdate": 1761951808665, "tmdate": 1762918929028, "mdate": 1762918929028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELMs, a model compression framework for LLMs. Instead of pruning individual weights or layers, ELMs replace groups of Transformer layers with a fixed-point network. This approach reinterpretes deep sequential computation as solving for an equilibrium state. The paper introduces: 1. Group Pruning Policy Optimization (GPPO): a reinforcement learning–based policy that learns optimal layer intervals for conversion to fixed-point modules; 2. One-Step KV Cache: a technique that stores only the final iteration’s cache to reduce memory cost during inference without sacrificing accuracy; 3. Adaptive Solvers (Broyden/Anderson acceleration): used to speed up convergence of fixed-point iterations.\n\nExperiments on Qwen2.5-1.5B, Qwen2.5-7B, and Llama3.2-3B across commonsense reasoning, math, and code benchmarks show that ELMs prune ~28% of parameters while maintaining ≈99% of the dense model’s accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reformulates deep computation as solving for an equilibrium state, which is quite novel.\n2. The approach consistently outperforms all baselines, including recent strong pruning methods. The method still reaches good performance on hard reasoning tasks (e.g., GSM8K, MATH, HumanEval), where most compression methods fail."}, "weaknesses": {"value": "1. The GRPO approach seems to only search for the start of the layers to be pruned under a fixed number of pruned layers. In this case, the search space is not very huge. How is the computation cost comparing to going through all of the possible start layers? Also, GPPO adds a non-trivial training loop with policy optimization, which might not be required for simpler pruning methods. \n2. Evaluations are limited to post-finetuned LLMs. It remains unclear how ELMs generalize to instruction-tuned multi-task setups."}, "questions": {"value": "1. How does the choice of fixed-point iteration solver (simple, Broyden, Anderson) affect latency in real-time inference on GPU/edge devices?\n2. Can GPPO be applied jointly to multiple layer groups (non-contiguous intervals), or is it limited to a single interval per model?\n3. What is the training cost of GPPO, and how is it comparing to enumerating method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ffzWbTW79M", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_6rpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_6rpi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978364961, "cdate": 1761978364961, "tmdate": 1762918928604, "mdate": 1762918928604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a compression method for LLMs called \"Equilibrium Language Models\" (ELMs). The approach replaces a continuous block of layers in the model with a single lightweight equilibrium layer, which computes its output through iterative solving, thereby achieving parameter compression. To support this framework, the paper introduces three auxiliary components: 1) a reinforcement learning-based \"Group Pruning Policy Optimization\" (GPPO) method for automatically selecting the layer blocks to be replaced; 2) a \"single-step KV cache\" mechanism designed to reduce memory overhead during iterative inference; and 3) the application of existing numerical solvers to accelerate convergence. Experiments demonstrate that this method maintains performance close to the original model across multiple benchmarks after pruning approximately 28% of the parameters, outperforming several baseline approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "A New Perspective on Compression\nThe paper rethinks model pruning as an implicit depth problem. This offers a new way to approach this field, showing that we can use iterative computation to replace a large number of parameters.\n\nStrong Empirical Results \nThe paper shows excellent results in balancing parameter reduction and accuracy, outperforming strong existing baselines. This demonstrates the great potential of the ELM framework.\n\nEfficient Memory Optimization\nThe \"One-Step KV Cache\" is a simple but effective innovation. It directly solves the huge memory overhead that the iterative method could cause during LLM inference. This is especially valuable for long sequence generation."}, "weaknesses": {"value": "Lack of Key Inference Latency Evaluation\nThe authors claim the method is friendly for edge deployment, but they don't provide any real-world speed data (like tokens/sec) to support this. The iterative process, the solver, and the convergence checks all add extra overhead. The final latency might even be worse than the original model. Without this data, the practical value of the paper is questionable.\n\nMissing Analysis of GPPO's Practicality and Cost\nAutomatic search is nice, but its cost is critical to determine if it's feasible. The authors need to quantify the search cost of GPPO (e.g., how many GPU-days it takes) and discuss its cost relative to the final model training. \n\nLimited Pruning Strategy\nThe method is limited to pruning a single, contiguous block of layers. This is a strong assumption and may not be the best approach. The authors should discuss why they made this choice (e.g., technical limitations or to simplify the problem) and explore the possibility of extending it to more flexible pruning patterns, like multiple, non-contiguous blocks."}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3BaS1k4Mc8", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_4aYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_4aYQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999434821, "cdate": 1761999434821, "tmdate": 1762918928279, "mdate": 1762918928279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}