{"id": "lqJT6xmuH3", "number": 6608, "cdate": 1757990479260, "mdate": 1759897905363, "content": {"title": "Equilibrium Language Models", "abstract": "Large Language Models (LLMs) excel across diverse applications but remain impractical for edge deployment due to severe memory bottlenecks at the edge devices. We propose Equilibrium Language Models (ELMs), a novel compression framework that replaces groups of Transformer layers with a lightweight fixed-point network, reinterpreting deep computation as solving for an equilibrium state. To achieve ELMs, We introduce *Group Pruning Policy Optimization*, which automatically learns optimal pruning intervals. Moreover, we propose *One-Step KV-Cache*, which drastically reduces memory overhead by storing only the final iteration cache without compromising the accuracy, to enable effective deployment at the edge devices. Across different tasks such as common sense reasoning, mathematical problem solving, and code generation, ELMs prune 28\\% of parameters while retaining 99\\% of the accuracy of dense fine-tuned LLMs, establishing a new direction for memory-efficient edge deployment of large models.", "tldr": "", "keywords": ["Large Language Models", "Model Compression", "On-Device Inference", "Fixed-Point Network"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f31869a4a9695179b7f0019577eb52613c4e0bae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission proposes Equilibrium Language Models (ELMs), a novel compression framework for Transformer-based large language models targeting edge deployment. The core idea is to replace groups of consecutive Transformer layers with a lightweight fixed-point network, reinterpreting deep computation as solving for the equilibrium state of a fixed-point system. To enable ELMs, the authors introduce two key components: (1) Group Pruning Policy Optimization (GPPO), a reinforcement learning-based method to automatically identify optimal layer pruning intervals; and (2) One-Step KVCache, which reduces memory overhead by storing only the final iteration’s cache without accuracy loss. Experiments on Qwen2.5 and Llama3.2 models show that ELMs prune 28% of parameters while retaining 99% of the accuracy of dense fine-tuned LLMs across commonsense reasoning, mathematical problem-solving, and code generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work breaks from traditional LLM pruning (e.g., weight-level, head-level, or heuristic layer pruning) by leveraging fixed-point network theory (from Deep Equilibrium Models, DEQs) to replace layer groups. \n\n2. GPPO avoids the limitations of heuristic pruning metrics (cosine similarity, perplexity) by framing layer interval selection as a policy optimization problem. \n\n3. One-Step KVCache is a critical optimization for edge deployment"}, "weaknesses": {"value": "1. All experiments use LLMs with exactly 28 layers (Qwen2.5-1.5B/7B, Llama3.2-3B). It remains unclear how ELMs perform on models with different depths.\n\n2. The paper highlight the significance for edge deployment, however, the paper shows no data on latency results for edge devices. \n\n3. While One-Step KVCache reduces memory, the submission only reports KV cache size (Table 3) but not total inference memory (including model parameters, activations). Edge devices often have strict RAM limits (e.g., 4GB on low-end phones), so total memory data is essential."}, "questions": {"value": "1. What happens if the inherited Transformer layer is replaced with a simple FFN? Or if W_h is not initialized as an identity matrix? Such ablations would clarify which components drive performance retention.\n\n2. How do you choose the convergence threshold for One-Step KVCache? \n\n3. The paper mentions using Stochastic Jacobian-Free Backpropagation (SJFB) to reduce training cost , How does ELMs’ training time/GPU memory compare to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uwR0duvge", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_wdiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_wdiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857933678, "cdate": 1761857933678, "tmdate": 1762918929317, "mdate": 1762918929317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response Part.1 (#1 Inference Efficiency; #2 GPPO costs)"}, "comment": {"value": "We sincerely thank the reviewers for their valuable feedback. We have carefully studied all comments, and these insightful suggestions are highly helpful for strengthening our work. Below, we summarize several common concerns raised across multiple reviews and provide global responses. All additional experiments will be further completed within the discussion phase and clarifications presented here will be integrated into the revised manuscript.\n\n**#1 Inference Efficiency of ELM**\n\nWe appreciate the reviewers’ constructive suggestions regarding the practical efficiency of ELM on GPUs/edge devices. In response, we conducted further evaluations focusing on both memory footprint and runtime speed in realistic deployment. Experiments are performed on Qwen2.5-1.5B-Instruct with $M=8$ layers pruned in the benchmark GSM8K. Inference was executed on a single L20 GPU with BFloat16 precision.\n\n**#1.1 Memory Efficiency**\n\nDuring inference, the memory consumption of LLMs is primarily determined by model parameters and KV-cache size. The baseline dense model occupies 3.09 GB, while our ELM model requires only 2.44 GB, yielding a 21\\% reduction in model memory. Using One-Step KV-cache, ELM further reduces KV-cache memory to 86.0 MB at 4K context length, achieving 25\\% savings compared to Dense.\n\n|| model(GB) | KV-cache(MB) |\n| - | - | - |\n| Dense | 3.09 | 114.7 |\n| ELM(w/o One-Step KV) | 2.44 | 114.7 |\n| ELM | 2.44 | 86.0|\n\n**#1.2 Computational Efficiency**\n\nReviewers expressed concerns regarding whether the fixed-point iterations in ELM yield actual decoding speedups. We emphasize two key points:\n- Iteration count can be greatly reduced via acceleration methods(Broyden, Anderson) and computations in solvers introduce minimal additional latency.\n- SJFB training enables ELM to maintain high generation quality even with a relaxed convergence threshold $\\delta$.\n\nWe evaluate ELM with various solvers and thresholds. Results show that with an average of only 1.1 iterations, ELM achieves 95\\% of the original accuracy while decoding at 98.3 tokens/s, corresponding to a 28.7\\% speedup over the Dense baseline. Acceleration solvers introduce negligible additional overhead, e.g., Anderson’s method still achieves a 14.9\\% speedup with $\\delta=4$, similar with simple solver.\n\n| | threshold | decode speed(token/s) | iters | GSM8K Acc. |\n| - | - | - | - | - |\n| Dense| -- | 76.4 | 8 | 72.4 |\n| ELM(simple) | 0.0 | 74.9 | 8 | 71.8 |\n| ELM(broyden) | 4.0 | 82.8 | 4.2 | 70.1 |\n| ELM(anderson) | 4.0 | 87.8 | 3.4 | 71.1 |\n| ELM(simple) | 4.0 | 87.2 | 3.6 | 71.6 |\n| ELM(simple) | 10.0 | 93.6 | 2.4 | 70.1 |\n| ELM(simple) | 100.0 | 98.3 | 1.1 | 68.8 |\n\nThese results collectively demonstrate that ELM provides substantial improvements in both memory consumption and decoding efficiency. We will incorporate these analyses in the revision. In future work, we plan to further investigate deployment on more constrained edge platforms such as mobile devices.\n\n**#2 Costs of GPPO**\n\nWe thank the reviewers for raising the question regarding the costs of GPPO. While GPPO does introduce an additional policy optimization stage, its overhead is modest and well justified by the performance benefits it provides.\n\nConcretely, for coding task with 10k calibration dataset, training a single ELM on Qwen2.5-1.5B-Instruct with $M=8$ layers pruned (starting from a fixed layer) at 4 epochs requires 0.44 hours on 8×H20 GPUs (0.15 GPU·days) with 17.5 GB memory usage. Enumerating all possible start layers would therefore require 0.15 × (28 − 8 + 1) = 3.15 GPU·days, which is clearly expensive. In contrast, GPPO employs a lightweight policy network and LoRA-based updates. With 2 epochs of training, GPPO can converge to the optimal pruning strategy and the training takes 0.2 hours (0.07 GPU·days) with 21.5 GB memory, introducing minimal extra overheads.\n\nWe also note that heuristic metrics (PPL, Cosine Similarity) require comparable computational time but fail to identify optimal pruning configurations, resulting in degraded downstream performance. GPPO offers an automatic, efficient, and stable solution for selecting the optimal start layer in ELM construction. A comparison is summarized below and will be included in the revision.\n\n| | Time(GPU·days) | Memory(GB) | Start layer | Performance(MBPP) |\n| - | - | - | - | - |\n| ELM training | 0.15| 17.5 | -- | -- |\n| Enumeration | 3.15| 17.5 | 2| 70.6 |\n| GPPO| 0.07| 21.5 | 2| 70.6 |\n| PPL | 0.99| 10.1 | 8| 66.6 |\n| Cos | 0.03| 8.7 | 9| 65.4 |"}}, "id": "OOWZh49mvd", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763625720511, "cdate": 1763625720511, "tmdate": 1763625720511, "mdate": 1763625720511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Equilibrium Language Models (ELMs), a new family of Transformer-based language models designed to reduce inference-time memory usage and improve computational efficiency without sacrificing performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-written and clearly structured; the mathematical formulations are concise and supported by clear intuition.\n\n- Novel modeling approach: Reformulating decoding as a fixed-point equilibrium problem is original and elegant.\n\n- Parameter efficiency: Demonstrates substantial parameter pruning without requiring fine-tuning."}, "weaknesses": {"value": "- While the paper emphasizes parameter pruning (~28%), it does not report key deployment metrics such as inference latency, memory consumption, or speedup over dense baselines. This makes it difficult to verify the real-world efficiency gains, especially since equilibrium solvers may incur higher per-step computation even if they reduce parameter count.\n\n- Although the paper presents an elegant formulation of Equilibrium Language Models (ELMs) and demonstrates strong pruning performance, it does not discuss the cost, stability, or convergence of obtaining these models. In particular, the Group Pruning Policy Optimization process likely introduces significant computational overhead, as it involves iterative search and training steps that may require multiple retraining cycles.\n\n- In Table 2, ELMs are not consistently better than all baselines under the same compression level. For instance, LLM-Streamline (Layer) outperforms ELMs on MMLU at M=8, and Sheared LLM performs better on HumanEval. This suggests that ELMs may not be universally superior. A more systematic comparison across different  M values and tasks is necessary to understand when ELMs are advantageous or potentially weaker than existing methods."}, "questions": {"value": "- What happens when the pruned layers are not contiguous? For example, instead of pruning 12 consecutive layers, what if we prune two separate groups of six layers distributed across the model? \n\n\nI am open to discussing this further during the rebuttal and will be happy to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ULLK3v0fH4", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_Vx6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_Vx6z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951808665, "cdate": 1761951808665, "tmdate": 1762918929028, "mdate": 1762918929028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELMs, a model compression framework for LLMs. Instead of pruning individual weights or layers, ELMs replace groups of Transformer layers with a fixed-point network. This approach reinterpretes deep sequential computation as solving for an equilibrium state. The paper introduces: 1. Group Pruning Policy Optimization (GPPO): a reinforcement learning–based policy that learns optimal layer intervals for conversion to fixed-point modules; 2. One-Step KV Cache: a technique that stores only the final iteration’s cache to reduce memory cost during inference without sacrificing accuracy; 3. Adaptive Solvers (Broyden/Anderson acceleration): used to speed up convergence of fixed-point iterations.\n\nExperiments on Qwen2.5-1.5B, Qwen2.5-7B, and Llama3.2-3B across commonsense reasoning, math, and code benchmarks show that ELMs prune ~28% of parameters while maintaining ≈99% of the dense model’s accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reformulates deep computation as solving for an equilibrium state, which is quite novel.\n2. The approach consistently outperforms all baselines, including recent strong pruning methods. The method still reaches good performance on hard reasoning tasks (e.g., GSM8K, MATH, HumanEval), where most compression methods fail."}, "weaknesses": {"value": "1. The GRPO approach seems to only search for the start of the layers to be pruned under a fixed number of pruned layers. In this case, the search space is not very huge. How is the computation cost comparing to going through all of the possible start layers? Also, GPPO adds a non-trivial training loop with policy optimization, which might not be required for simpler pruning methods. \n2. Evaluations are limited to post-finetuned LLMs. It remains unclear how ELMs generalize to instruction-tuned multi-task setups."}, "questions": {"value": "1. How does the choice of fixed-point iteration solver (simple, Broyden, Anderson) affect latency in real-time inference on GPU/edge devices?\n2. Can GPPO be applied jointly to multiple layer groups (non-contiguous intervals), or is it limited to a single interval per model?\n3. What is the training cost of GPPO, and how is it comparing to enumerating method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ffzWbTW79M", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_6rpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_6rpi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978364961, "cdate": 1761978364961, "tmdate": 1762918928604, "mdate": 1762918928604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a compression method for LLMs called \"Equilibrium Language Models\" (ELMs). The approach replaces a continuous block of layers in the model with a single lightweight equilibrium layer, which computes its output through iterative solving, thereby achieving parameter compression. To support this framework, the paper introduces three auxiliary components: 1) a reinforcement learning-based \"Group Pruning Policy Optimization\" (GPPO) method for automatically selecting the layer blocks to be replaced; 2) a \"single-step KV cache\" mechanism designed to reduce memory overhead during iterative inference; and 3) the application of existing numerical solvers to accelerate convergence. Experiments demonstrate that this method maintains performance close to the original model across multiple benchmarks after pruning approximately 28% of the parameters, outperforming several baseline approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "A New Perspective on Compression\nThe paper rethinks model pruning as an implicit depth problem. This offers a new way to approach this field, showing that we can use iterative computation to replace a large number of parameters.\n\nStrong Empirical Results \nThe paper shows excellent results in balancing parameter reduction and accuracy, outperforming strong existing baselines. This demonstrates the great potential of the ELM framework.\n\nEfficient Memory Optimization\nThe \"One-Step KV Cache\" is a simple but effective innovation. It directly solves the huge memory overhead that the iterative method could cause during LLM inference. This is especially valuable for long sequence generation."}, "weaknesses": {"value": "Lack of Key Inference Latency Evaluation\nThe authors claim the method is friendly for edge deployment, but they don't provide any real-world speed data (like tokens/sec) to support this. The iterative process, the solver, and the convergence checks all add extra overhead. The final latency might even be worse than the original model. Without this data, the practical value of the paper is questionable.\n\nMissing Analysis of GPPO's Practicality and Cost\nAutomatic search is nice, but its cost is critical to determine if it's feasible. The authors need to quantify the search cost of GPPO (e.g., how many GPU-days it takes) and discuss its cost relative to the final model training. \n\nLimited Pruning Strategy\nThe method is limited to pruning a single, contiguous block of layers. This is a strong assumption and may not be the best approach. The authors should discuss why they made this choice (e.g., technical limitations or to simplify the problem) and explore the possibility of extending it to more flexible pruning patterns, like multiple, non-contiguous blocks."}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3BaS1k4Mc8", "forum": "lqJT6xmuH3", "replyto": "lqJT6xmuH3", "signatures": ["ICLR.cc/2026/Conference/Submission6608/Reviewer_4aYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6608/Reviewer_4aYQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999434821, "cdate": 1761999434821, "tmdate": 1762918928279, "mdate": 1762918928279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}