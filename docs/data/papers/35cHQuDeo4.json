{"id": "35cHQuDeo4", "number": 13380, "cdate": 1758217158654, "mdate": 1762964550009, "content": {"title": "Sound in Sights: Deriving Visual Insights from Audio for Comprehensive Video Understanding with Large Multimodal Models", "abstract": "Video understanding is inherently multimodal, requiring both visual and auditory cues to form a complete representation of dynamic scenes. However, most existing video understanding models rely solely on visual content, overlooking informative audio cues, such as spoken instructions or environmental sounds, for scene understanding and event comprehension. Progress in audio-visual reasoning has been hindered by the lack of high-quality supervised fine-tuning (SFT) data that jointly considers video and audio. To address this gap, we introduce SoundInSight, a large-scale audio-visual question answering dataset comprising over 80k question–answer pairs from online videos, created via a multimodal large language model (MLLM)-assisted annotation pipeline. SoundInSight provides rich supervision for audio-visual reasoning, enabling MLLMs to be fine-tuned for audio understanding. We find that current video MLLMs heavily rely on visual information, hindering effective multimodal learning. To mitigate this, we propose an audio-only pretraining stage, which significantly improves audio-visual reasoning performance. Additionally, to evaluate audio-visual comprehension, we construct a high-quality, manually curated test set of 1,000 samples requiring joint audio-visual understanding, exceeding standard benchmarks in complexity. Models fine-tuned on SoundInSight with the proposed training strategy achieve substantial performance gains on this new benchmark. Moreover, on the challenging VideoMME evaluation, our approach significantly improves performance in Information Synopsis subcategory, demonstrating the efficacy of incorporating audio. The SoundInSight dataset and code will be publicly released to facilitate further research.", "tldr": "We propose SoundInSights, a large-scale audio-visual question answering dataset, and a baseline method for video understanding.", "keywords": ["Multimodal Large Language Model", "Video Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/168149a2e8d36afa69885436313ac54b223703a3.pdf", "supplementary_material": "/attachment/629c166414f0c0f48e1bb5b4a67ad2e1aa869ff1.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces SoundInSights, a large-scale (approx. 80k QA pairs) audio-visual corpus created with hierarchical audio annotations to encourage genuine joint AV reasoning. An additional curated benchmark with 1,000 QA pairs is also proposed to assess tasks requiring audio-visual integration. The authors enhance a vision-centric MLLM (LLaVA-OneVision) by adding an audio encoder (Qwen2-Audio encoder) and propose a two-stage training method: audio-only caption pre-training followed by joint AV fine-tuning. Experiments show clear improvements on their benchmark and VideoMME evaluation, confirming the value of their approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors clarified the problem motivation, many events are “visible in sound” but not in frames. Figure 1 illustrates the \"turn signal\" case where audio modality prevents hallucinations from pure visual input. A simple and reproducible training recipe is also proposed, the two-stage tuning scheme is straightforward and effective, with implementation details (e.g., learning rate, batch, computational resource usage) are given. Apart from these, well-designed supervision signals are also presented, a hierarchical audio annotating mechanism is proposed to encourage the modeling of temporal structure and both fine- and coarse-grained audio semantics. \n\nResults suggest that models over rely on visual modality, with both quantitative improvements on audio related tasks (Tab. 2) and qualitative cases support the motivation (Figure 5)."}, "weaknesses": {"value": "1. Insufficient Baseline Comparisons\n\nThe paper primarily compares against a baseline model (LLaVA-OV-7B) that lacks audio processing capabilities. This restricts the conclusions that can be drawn regarding the effectiveness of the proposed method. To convincingly demonstrate broader applicability, comparisons against current state-of-the-art audio-visual models such as video-SALMONN, VideoLLaMA2, AVicuna, or Macaw-LLM are necessary (Even suggesting that the proposed approach narrows the gap is also meaningful).\n \n2. Unexplained Performance Drop in Specific Tasks\n\nOn the VideoMME benchmark, the Temporal Reasoning subtask performance notably decreases from 43.5 to 40.1. The authors should investigate and explain this unexpected decline, potentially analyzing alignment strategies, handling of temporal dependencies, or error patterns to provide clarity.\n\n3. Potential Issues with Data Overlap and Leakage\n\nSince the dataset is derived from videos included in LLaVA-Video-178K, there is a possibility of overlap or unintended leakage between the training and test data. The paper currently does not detail explicit procedures for data deduplication at the video, audio, or text level, raising concerns regarding the integrity of the evaluation setup.\n\n4. Limited Evaluation Breadth\n\nThe evaluation is restricted to the newly proposed benchmark and VideoMME. This lack of testing on established audio-visual benchmarks such as AVQA, AVSD, MUSIC-AVQA, AV-Odyssey, or AV-TrustBench limits the generalizability and external validity of the findings. Further experimentation on these additional tasks would strengthen confidence in the proposed approach.\n\nMinor issues\n\n1. There are discrepancies in reporting the dataset sizes used in Stage 2 training (e.g., inconsistently described as \"80k AV-QA plus 80k LLaVA-Video QAs\" in Sec 3.4 versus \"80k AV-QA plus 100k open-ended plus 100k multi-choice\" in Sec. 4.1). \n2. Minor typos such as \"gardient\" and \"SoundingSights\"."}, "questions": {"value": "My questions primarily lie in the following perspectives:\n\n## Dataset construction\n1. Data leakage prevention\nHow did you ensure no overlap between the SoundInSights 80k training set and the 1k benchmark, and between your training data and VideoMME clips? Especially since sources are from YouTube via LLaVA‑Video‑178K.\n\n2. Validation of \"both modalities are required\"\nFor the proposed 1k multiple‑choice benchmark, please report the A‑only / V‑only / A+V accuracies to demonstrate that questions truly require cross‑modal reasoning?\n\n3. Quality control\nThe hierarchical audio descriptions use GPT‑4o and the 1k benchmark was curated from the approximately 4k ChatGPT filtered candidates. Was there any quality control process performed?\n\n4. Benchmark Composition and Difficulty\nPlease share the topic/source distribution, typical temporal spans, and answer distribution of the 1k benchmark. This would further confirm coverage beyond a few easy tropes and calibrate multiple choice chance levels.\n\n## Method \n1. Inconsistent Stage‑2 mixture sizes\nSec. 3.4 says Stage‑2 mixes 80k AV‑QA + 80k LLaVA‑Video QAs, while Sec. 4.1 reports 80k AV‑QA + 100k open‑ended plus 100k multi‑choice. Which one is correct? \n\n2. Stage‑1 supervision granularity\nAs stated in the manuscript, Stage‑1 trains on 80k Level‑3 audio captions. Did you evaluated the performance of using Level‑1/Level‑2 as well, or curriculum from fine‑ to coarse‑grained?\n\n3. Feature fusion\nThe architecture concatenates audio and vision embeddings into the LLM. Did you evaluate cross‑attention or gated fusion to mitigate modality interference?\n\n4. Sampling rate \nWhat are the frame sampling rate, clip length, audio sampling rate, and token counts per modality during inference?\n\n## Evaluation protocol and baseline fairness\n1. Baseline models\nPlease also include comparisons with video-SALMONN, VideoLLaMA2, AVicuna, Macaw‑LLM... under the same evaluation protocol.\n\n2. A‑only / V‑only / A+V ablations on external benchmarks\nOn VideoMME, please also report performance when feeding only video, only audio, and both to your model. \n\n3. Broader benchmark coverage\nPlease also report results on datasets such as AVQA, AVSD, MUSIC‑AVQA, AV‑Odyssey, AV‑TrustBench (maybe not all of them). This would demonstrate generalization beyond your own benchmark and one general purpose set.\n\n## Analysis of robustness and failure modes\n1. Why does Temporal Reasoning decrease (from 43.5 to 40.1)? Is the audio branch over‑attended or mis‑aligned with the frame timing?\n\n2. What happens if you time‑shift the audio, swap with other clips, or mute segments? This would demonstrate whether the model uses audio rather than the priors.\n\n## Minor issues\n\nPlease also fix the \"SoundingSights\" and typos like “gardient”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hkqL1h7SIY", "forum": "35cHQuDeo4", "replyto": "35cHQuDeo4", "signatures": ["ICLR.cc/2026/Conference/Submission13380/Reviewer_BCn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13380/Reviewer_BCn2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760855695107, "cdate": 1760855695107, "tmdate": 1762924022249, "mdate": 1762924022249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2QODo9VsPB", "forum": "35cHQuDeo4", "replyto": "35cHQuDeo4", "signatures": ["ICLR.cc/2026/Conference/Submission13380/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13380/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964548650, "cdate": 1762964548650, "tmdate": 1762964548650, "mdate": 1762964548650, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoundInSights, an audio-visual question answering dataset based on LLaVA-Video-178k."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This work extends the annotations of LLaVA-Video-178k by adding annotations for audio information."}, "weaknesses": {"value": "- There are already well-annotated video datasets that consider audio, such as Finevideo and Cinepile. If annotation based on MLLM is intended, models with SOTA performance on audio-visual data can also be used for annotation, such as Gemini-2.5, video-SALMONN 2, and Qwen2.5-Omni. The annotation method used in this paper separates audio descriptions from visual descriptions, ignoring the synchronization of audio and video.\n- The annotations of SoundInSights are completely generated by LLMs, lacking manual verification.\n- SoundInSights is entirely based on LLaVA-Video-178k and only serves as an extension of it, resulting in very limited novelty.\n- In Table 2, the improvement on VideoMME is actually very limited, and it might even be caused by errors. Moreover, the tested benchmarks are too few. Only VideoMME is tested among public benchmarks. All these lead me to question the effectiveness of the SoundInSights dataset."}, "questions": {"value": "- See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DBj3HXYYh9", "forum": "35cHQuDeo4", "replyto": "35cHQuDeo4", "signatures": ["ICLR.cc/2026/Conference/Submission13380/Reviewer_a33s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13380/Reviewer_a33s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625923417, "cdate": 1761625923417, "tmdate": 1762924021805, "mdate": 1762924021805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the problem wherein most existing Multimodal Large Language Models (MLLMs) are restricted to video understanding and cannot receive audio input. A scalable MLLM-assisted annotation pipeline for generating hierarchical audio descriptions and context-aware AV QA pairs is proposed. Based on this, the SOUNDINSIGHTS training set, containing 80k samples, and a human-verified test set are introduced. Using LLaVA-One-Vision-7B as the base, the authors adopted a 2-stage training method, confirming the importance of audio information for MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Exploring end-to-end audio understanding for MLLMs is a promising research direction.\n- The manuscript is clearly written and easy to follow.\n- The promise to open-source the data and code is a valuable contribution to the paper's reproducibility."}, "weaknesses": {"value": "Honestly, I find that this paper, in its current form, does not yet meet the standards for publication.\n- The paper's novelty is insufficient. The fine-grained use of MLLMs to assist in constructing training data has already been proposed in prior work such as [1]. Furthermore, the model's architectural design is outdated.\n- The experimental validation is inadequate. Firstly, the method is not compared against a sufficient number of strong baselines, particularly recent SOTA models like video-SALMONN-2+ and Qwen2.5/3-Omni. Secondly, the evaluation is conducted on a limited selection of benchmarks. To substantiate the paper's claims, the analysis must be broadened to include other established benchmarks like WorldSense, AVUT, and Video-Holmes, in addition to VideoMME.\n- There is a clear gap between the model's performance and the state-of-the-art. Many 7B models, for instance, achieve scores over 60 on VideoMME. The reported performance is substantially lower, which makes it difficult to be convinced of the paper's contributions.\n\n\n[1] Sun et al., \"video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model\", ICML 2025."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3ZiFNDFrqw", "forum": "35cHQuDeo4", "replyto": "35cHQuDeo4", "signatures": ["ICLR.cc/2026/Conference/Submission13380/Reviewer_KKKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13380/Reviewer_KKKJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811249323, "cdate": 1761811249323, "tmdate": 1762924021335, "mdate": 1762924021335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a high-quality audio-visual question answering dataset, a Benchmark, along with its accompanying construction workflow. When applied to the fine-tuning phase of AVLLMs, this dataset and workflow can enhance general video understanding Benchmarks, such as VideoMME."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a high-quality dataset and Benchmark for joint audio-visual understanding, which contributes to the further development of the field.\n2. By using the provided dataset to train audio-visual understanding models and achieving performance improvements on VideoMME, the authors have verified that the dataset is beneficial for audio-visual understanding."}, "weaknesses": {"value": "1. Although the dataset and Benchmark provided by the authors hold certain value for the field, they fail to elaborate on the essential differences between the proposed dataset and Benchmark. The work can only be regarded as a supplement to previous studies, thus making a relatively small contribution to advancing the field of audio-visual understanding.\n2. When constructing high-quality audio-visual data, the key lies in creating QAs (Question-Answers) or Captions that require simultaneous use of audio and visual information for responses. While the authors have attempted to propose some solutions, these solutions have not been properly validated or explained.\n3. Multi-granularity synthetic data has long been proposed in LLaVA-Video [1] and widely adopted in many subsequent works, so it cannot be considered an original contribution. Furthermore, from the perspectives of model design and training strategies, the authors have not put forward any key technical contributions either.\n\n[1] LLaVA-Video: Video Instruction Tuning With Synthetic Data"}, "questions": {"value": "1. What is the essential difference between SoundInSights and previous Benchmarks and datasets, especially more advanced ones such as AV-Odysee [1]?\n2. Why can the authors' data construction scheme synthesize content that mandates reasoning based on both audio and visual information? The authors need to verify and elaborate on this question.\n\n[1] AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YtbnIumd9N", "forum": "35cHQuDeo4", "replyto": "35cHQuDeo4", "signatures": ["ICLR.cc/2026/Conference/Submission13380/Reviewer_Uv3i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13380/Reviewer_Uv3i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842268949, "cdate": 1761842268949, "tmdate": 1762924020796, "mdate": 1762924020796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}