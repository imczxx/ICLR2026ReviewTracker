{"id": "373rsDQsq4", "number": 11306, "cdate": 1758195882201, "mdate": 1759897594784, "content": {"title": "Three Forward, One Backward: Memory-Efficient Full-Rank Fine-Tuning of Large Models via Extra Forward Passes", "abstract": "Fine-tuning large language models (LLMs) has achieved significant success in downstream tasks.\nHowever, as the model size continues to grow, traditional fine-tuning methods have become increasingly impractical due to their high computational and memory costs.\nThis has motivated researchers to explore parameter-efficient and memory-friendly fine-tuning strategies to enable scalable approaches, with Low-Rank Adaptation (LoRA) standing out as a representative work.\nHowever, the LoRA update is restricted to a low-rank subspace, which results in suboptimal performance compared to the full-parameter update.\nRecent research has also explored memory-efficient fine-tuning LLMs using just forward passes while suffer from high variance in gradient estimation and low convergence speed.\nTo address the issues above, we propose a new alternating optimization framework called LMAO (Low-rank and Memory-efficient Zeroth-Order Alternating Optimization), which combines the advantages of LoRA and MeZO.\nThis method alternately updates the low-rank components and zeroth-order directions during training.\nBy performing three forward propagations and one backward propagation, each update is full-rank, thereby reducing feature loss and enabling efficient fine-tuning under strict memory constraints.\nWe provide theoretical guarantees on the convergence and convergence rate of this method.\nEmpirical results demonstrate that, in experiments on multiple models (e.g., OPT, RoBERTa-large), LMAO achieves performance comparable to first-order methods.\nThis presents a practical and scalable solution for fine-tuning large-scale models.\nOur source code is available at https://anonymous.4open.science/r/mlao-C2EC/.", "tldr": "", "keywords": ["\"LLM tuning\"", "\"LoRA\"", "\"Zeroth order\""], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1c5d18bbe5630238fdf00a3d7a397e4cb07c1dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Large language models have shown remarkable performance but fine-tuning them is computationally and memory-intensive due to growing model sizes. The motivation is to develop scalable fine-tuning methods that reduce resource demands while maintaining high performance on downstream tasks. Challenges include LoRA's suboptimal performance from low-rank constraints and MeZO's high variance and slow convergence from zeroth-order gradient estimates. The solution, LMAO, alternates low-rank matrix updates using backpropagation with zeroth-order updates on base parameters via forward passes, enabling full-rank fine-tuning with three forwards and one backward per iteration under memory constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LMAO integrates the low-rank efficiency of LoRA with the memory-saving forward-only updates of MeZO. This hybrid approach achieves full-rank parameter updates without excessive memory use. It effectively reduces feature loss common in low-rank methods while controlling variance better than pure zeroth-order techniques.\n\n2. Experimental results on RoBERTa-large demonstrate that LMAO outperforms baselines in few-shot and many-shot settings. It achieves accuracies close to or surpassing full fine-tuning on tasks like SST-2 and SNLI. This highlights its ability to optimize medium-sized models effectively across sentiment and inference tasks.\n\n3. On larger OPT models up to 6.7B parameters, LMAO consistently ranks highest on most SuperGLUE tasks. It improves over LoRA on classification and multiple-choice problems like BoolQ and COPA. Such performance validates its scalability and generalization to autoregressive architectures.\n\n4. The theoretical framework provides convergence guarantees under Lipschitz and expected smoothness assumptions. Descent lemmas and rate analyses ensure reliable optimization. This mathematical rigor strengthens the method's foundation beyond empirical evidence."}, "weaknesses": {"value": "1. LMAO requires more forward passes than standard LoRA, increasing computational overhead. This can extend training time significantly on large datasets. Efficiency gains in memory do not fully offset the added flops.\n\n2. Zeroth-order components introduce inherent noise despite alternation. This variance may hinder convergence on noisy or complex tasks. Smoother gradients from full backpropagation are absent in parts of the update.\n\n3. Evaluations are limited to models up to 6.7B parameters. And the utilized evaluation datasets are outdated. Scaling issues could emerge with extreme sizes.\n\n4. Comparisons omit advanced PEFT variants like sparse adapters. Only basic LoRA and MeZO are baselines. Broader benchmarking would clarify relative advantages.\n\n5. Inference implications are undiscussed after training. Merging adapters into base weights could add latency. Deployment considerations are overlooked."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1KSR6kBvdv", "forum": "373rsDQsq4", "replyto": "373rsDQsq4", "signatures": ["ICLR.cc/2026/Conference/Submission11306/Reviewer_9oXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11306/Reviewer_9oXR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803102447, "cdate": 1760803102447, "tmdate": 1762922448146, "mdate": 1762922448146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel PEFT method for large language models. The proposed approach integrates the advantages of LoRA and MeZO by alternating between low-rank gradient updates and zeroth-order updates. Specifically, LMAO performs three forward passes and one backward pass per iteration, enabling full-rank parameter updates under stringent memory constraints. Overall, the paper presents a reasonably innovative idea; however, it would benefit from a broader set of experiments to better demonstrate its generalization capability across different model architectures. Moreover, I would like to see its performance on MMMU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[1]. The authors provide open-source code and describe the experimental setup thoroughly, which enhances the transparency and reproducibility of the work.\n\n[2]. The proposed method, LMAO, demonstrates strong empirical performance, outperforming or matching full fine-tuning on multiple NLP benchmarks, showing both effectiveness and efficiency.\n\n[3]. The paper is clearly written, well-organized, and easy to follow. The motivation, methodology, and results are presented in a logical and coherent manner."}, "weaknesses": {"value": "[1]. The method reduces memory usage during the zeroth-order phase, but the backward pass for low-rank updates still incurs considerable memory cost. This may limit scalability when applied to very large models or long-sequence tasks.\n\n[2]. Although the paper claims good scalability, experiments are only conducted on models up to OPT-2.7B. Evaluating larger models such as LLaMA 3 (13B or 70B) or Qwen2.5 would better support the scalability claim.\n\n[3]. The experiments focus only on text classification and inference tasks. It remains unclear whether the method generalizes well to generative or multimodal settings such as MMMU."}, "questions": {"value": "[1]. The performance on multimodality settings?\n\n[2]. The performance on DyLoRA or AdaLoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fCM2XTITuK", "forum": "373rsDQsq4", "replyto": "373rsDQsq4", "signatures": ["ICLR.cc/2026/Conference/Submission11306/Reviewer_N9p4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11306/Reviewer_N9p4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296639486, "cdate": 1761296639486, "tmdate": 1762922447744, "mdate": 1762922447744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LMAO (Low-rank and Memory-efficient Zeroth-Order Alternating Optimization), a novel fine-tuning method for large language models that combines the strengths of LoRA and MeZO in an alternating optimization framework. LMAO performs gradient-based updates on low-rank LoRA matrices using one forward and one backward pass, followed by memory-efficient zeroth-order updates on full-rank base model weights using two additional forward passes. This \"three forward, one backward\" approach ensures base model parameters receive full-rank updates, overcoming LoRA's primary limitation while maintaining memory efficiency. The paper provides theoretical convergence analysis for this alternating scheme and demonstrates through extensive experiments on RoBERTa-large and OPT models up to 6.7B parameters that LMAO consistently outperforms LoRA and MeZO, achieving performance competitive with and sometimes superior to full fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The main strength is the creative combination of LoRA and MeZO. It uses the precise gradients from backpropagation for the low-rank part while leveraging the memory efficiency of ZO for the full-rank part, achieving the best of both worlds.\n* The method's effectiveness is demonstrated on models of increasing scale, from RoBERTa-large (350M) up to OPT-6.7B. This suggests the approach is a scalable solution for future, even larger models.\n* The authors provide a formal convergence analysis for their algorithm. This adds a layer of theoretical rigor that is often missing in purely empirical papers and builds confidence in the method's stability."}, "weaknesses": {"value": "* The most significant weakness is the computational cost. The \"three forward, one backward\" approach inherently requires more computation per iteration than LoRA (one forward, one backward) or MeZO (two forward). The paper acknowledges this as \"training inefficiency\" in the conclusion but fails to provide a quantitative analysis of the wall-clock training time. This is a crucial piece of information for practitioners to assess the practical trade-offs.\n*  While the method is motivated by memory efficiency, the paper does not present direct measurements of peak GPU memory usage during training. A table or plot comparing the memory consumption of LMAO against FT, LoRA, and MeZO would make the \"memory-efficient\" claim much more concrete and impactful.\n*  By combining two different optimizers, LMAO introduces more hyperparameters that require tuning (e.g., learning rate for LoRA $\\eta_{BA}$, learning rate for ZO $\\eta_W$, perturbation scale $\\epsilon$, LoRA rank $r$ and alpha). While a sensitivity analysis for $r$ and $\\alpha$ is included, a more in-depth discussion on the tuning strategy and the sensitivity to the relative learning rates would be beneficial for the method's practical adoption."}, "questions": {"value": "1.  Could the authors provide a quantitative comparison of the wall-clock training time for LMAO against the baselines (especially LoRA and FT) for a representative task? For example, reporting the time to complete 1K training steps for OPT-1.3B on one of the SuperGLUE tasks would be very informative for understanding the practical computational trade-offs.\n2.  To further strengthen the claims, could the authors provide a table or figure detailing the peak GPU memory consumption (e.g., in GB) of LMAO compared to FT, LoRA, and MeZO under an identical experimental setup (e.g., same model, batch size, and task)?\n3.  LMAO uses separate learning rates for the ZO update ($\\eta_W$) and the LoRA update ($\\eta_{BA}$). Could the authors comment on the sensitivity of the model's performance to the ratio of these two learning rates? Are there any heuristics or best practices you discovered for setting them?\n4. In Algorithm 1 Line 9, the step computes a projection and then updates each parameter by that scalar times its corresponding perturbation entry z. That is a rank-1 direction in the vectorized parameter space (one random direction), not a full-rank update. The paper repeatedly asserts “ensuring full-rank updates,” which is incorrect per-iteration; at best, many different random directions across iterations can span the full space in expectation. Please fix the claim and its implications (e.g., “reduces feature loss”) or use multiple independent directions per step."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VONLIu5uYg", "forum": "373rsDQsq4", "replyto": "373rsDQsq4", "signatures": ["ICLR.cc/2026/Conference/Submission11306/Reviewer_6rvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11306/Reviewer_6rvB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545793577, "cdate": 1761545793577, "tmdate": 1762922447260, "mdate": 1762922447260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LMAO introduces a novel alternating optimization framework for fine-tuning large language models, combining LoRA's low-rank efficiency with MeZO's memory-friendly zeroth-order updates. By performing three forward passes and one backward pass per iteration, it enables full-rank adaptations while minimizing memory use and feature loss, achieving convergence guarantees and superior performance over baselines like LoRA and MeZO on tasks such as SuperGLUE with models like OPT and RoBERTa."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper features solid writing with a clear, logical structure and concise language, allowing quick understanding of innovations and results. The theory is rigorous, supported by convergence proofs and rate analyses under assumptions like Lipschitz smoothness for reliability. Experiments are thorough, covering models like RoBERTa and OPT series across tasks like SuperGLUE, including few-shot setups, ablations, sensitivity analyses, and comparisons to baselines such as LoRA and MeZO."}, "weaknesses": {"value": "While the paper is innovative, it has some drawbacks. Though it targets resource-constrained fine-tuning, LMAO doesn't match MeZO's ultra-low, inference-level memory efficiency due to the required backward pass, which adds noticeable memory overhead. It emphasizes full-rank updates for improved expressiveness, but the performance gains over LoRA are often marginal(see Ablation Study), making the benefits feel underwhelming. Plus, the extra forward passes introduce significant computational costs, potentially making it inefficient for long sequences or large datasets, and rendering it somewhat niche or even a bit of a half-measure compared to simpler options like LoRA."}, "questions": {"value": "1) the MeZO baseline results in Table 2 on certain datasets seem unusually low—for instance, on SST-2 and CB—which raises questions about their reliability and might suggest inconsistencies in implementation or evaluation setup[1][2][3].\n\n2) The experiments fall short by missing key baselines; they should include comparisons with advanced zeroth-order optimization algorithms designed to mitigate MeZO's high variance issues, such as SubZero[2] and LoZO[3]. Additionally, contrasts with LoRA variants that emulate full-rank updates—like DoRA[4]—would be valuable. Overall, the evaluations need a more comprehensive analysis that balances fine-tuning performance against memory usage and computational time overhead.\n\n[1] Zhang, Yihua, et al. \"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark.\" International Conference on Machine Learning. 2024.\n\n[2] Yu, Ziming, et al. \"Zeroth-order fine-tuning of llms in random subspaces.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025. \n\n[3] Chen, Yiming, et al. \"Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures.\" The Thirteenth International Conference on Learning Representations.\n\n[4] Liu, Shih-Yang, et al. \"Dora: Weight-decomposed low-rank adaptation.\" Forty-first International Conference on Machine Learning. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fdFz5rrR4p", "forum": "373rsDQsq4", "replyto": "373rsDQsq4", "signatures": ["ICLR.cc/2026/Conference/Submission11306/Reviewer_sMQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11306/Reviewer_sMQJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667833936, "cdate": 1761667833936, "tmdate": 1762922446719, "mdate": 1762922446719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}