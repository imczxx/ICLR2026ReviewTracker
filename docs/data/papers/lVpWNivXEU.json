{"id": "lVpWNivXEU", "number": 13668, "cdate": 1758220636771, "mdate": 1759897421178, "content": {"title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer", "abstract": "Recent advances in multimodal foundation models capable of both image understanding and generation have opened exciting avenues for building unified systems that seamlessly handle diverse vision-language tasks. Despite the progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with up to 48× faster inference speed compared to existing unified multimodal models.", "tldr": "We propose a novel layerwise timestep-expert flow-based architecture for efficient image generation", "keywords": ["Diffusion", "flow-matching", "generative models", "efficiency", "timestep experts"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/867e35065ff071454889a571b0c729920a8284d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes LaTtE-Flow, a novel architecture for extending image understanding models to perform image generation. Its core innovations are the Layerwise Timestep Experts (LTE), which partition transformer layers into groups specialized for different flow-matching timesteps to reduce inference computation, and Timestep-Conditioned Residual Attention (TCRA), a mechanism for reusing attention maps across layers. The method significantly reduces inference computational cost by using different activation layers on different time steps, while maintaining and even improving on both image generation and image understanding.\n\nThe proposed method is novel and seems to do lower computational cost at inference without sacrificing the performance. I suggest a weak accept."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1: The idea of LaTtE-flow is simple but effective. Based on its extension nature, it may be applied on any VLMs and trains them to also perform image generation with low computational cost. \n\n2: LaTtE-flow yields better image generation and image understanding compared with advanced models of similar scale at much fewer activation parameters and average inference time.  \n\n3: The paper delivers a thorough ablation study."}, "weaknesses": {"value": "1: The authors do not include the performance of the base model, Qwen2-VL-2B in Table 2, making it hard to compare how much gain LaTtE-Flow gives to the image understanding task. \n\n2: The description of Table 2 does not match the content. It says \"we report the number of activated parameters\", however it doesn't. It also does not perform the computation cost(or time cost) for different models like in Table 1. \n\n3: As an extension method, the authors only experiment LaTtE-Flow on one base model, Qwen2-VL-2B and does not generalise the method to other VLMs."}, "questions": {"value": "1: Could you perform a more complete Table 2, with baseline performance(base model Qwen2-VL-2B) and computational cost( FLops, or other metrics)?\n2: Is LaTtE-Flow able to perform on other base models besides Qwen2-VL-2B? How does it perform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GKXSKfmn3t", "forum": "lVpWNivXEU", "replyto": "lVpWNivXEU", "signatures": ["ICLR.cc/2026/Conference/Submission13668/Reviewer_BahB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13668/Reviewer_BahB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761024686049, "cdate": 1761024686049, "tmdate": 1762924236054, "mdate": 1762924236054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LaTtE-Flow, a unified MLLM which incorporates flow-matching-based image generation with VLM. First, they propose Layerwise Timestep Expert, distributing transformer layers into different timestep-specific experts, to improve the inference efficiency of unified MLLM. Second, they propose a gate attention approach to reuse the attention map in previous layers. Experiment results show that LaTtE-Flow outperform previous unified MLLM while being more efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **The idea is interesting.** The idea of decouple multiple flow matching steps into multiple transformer blocks is interesting and results in good performance.\n\n2. **Efficient.** LaTtE-Flow is very efficient, 6 times faster than Janus Pro. The author provides real running time to verify their claim.\n\n3. **Effective.** LaTtE-Flow achieves low latency while keeping strong image understanding and generation performance."}, "weaknesses": {"value": "1. **Compare and discuss with concurrent works.** Although using different data and model size, I suggest the author compare and discuss with newer Unified MLLM, including LMFusion, Blip3o and Bagel. \n\n2. **Unification of generation and understanding.** LaTtE-Flow use different visual encoders and different sets of parameters for image understanding and generation. If the model first generates an image and then performs VQA based on the generated image, it requires two forward passes. I hope the authors can discuss this scenario.\n\n3. **Lack of scale-up experiments.** This paper reports experiments only on 2B models. This is understandable, possibly due to computational constraints. It would be better if the authors could also run an 8B-scale model to demonstrate scalability."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1KorCG3tzx", "forum": "lVpWNivXEU", "replyto": "lVpWNivXEU", "signatures": ["ICLR.cc/2026/Conference/Submission13668/Reviewer_6gpc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13668/Reviewer_6gpc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656644842, "cdate": 1761656644842, "tmdate": 1762924235771, "mdate": 1762924235771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LaTtE-Flow, a unified and efficient framework for multimodal large language models that jointly handle visual understanding and image generation. The key idea is to enhance generation efficiency without sacrificing understanding capability. To achieve this, LaTtE-Flow introduces a Layerwise Timestep-Expert (LTE) design, where different Transformer layer groups specialize in specific timesteps of the flow-based generation process—thus reducing redundant computation during sampling. Additionally, a Timestep-Conditioned Residual Attention mechanism enables effective information reuse across layers and timesteps, improving coherence and stability in generation. With these designs, LaTtE-Flow substantially accelerates the flow-based generation process while maintaining high-quality visual outputs and strong understanding performance, outperforming prior unified models in both accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear and well-motivated problem statement, effectively highlighting the efficiency–quality trade-off in unified multimodal generation and offering a logically coherent solution through a flow-based Transformer design.\n\n2. The proposed Layerwise Timestep-Expert mechanism is both elegant and practical, significantly improving inference efficiency by activating only relevant Transformer layers at each timestep.\n\n3. The integration of Timestep-Conditioned Residual Attention is innovative, allowing effective feature reuse across layers and timesteps, which enhances both generation quality and training stability."}, "weaknesses": {"value": "1. The paper does not provide a direct comparison between the proposed LaTtE-Flow and the original VLM backbone on multimodal understanding tasks, leaving unclear how much the unified training or flow-based adaptation affects understanding performance.\n\n2. The work lacks quantitative results on standard text-to-image generation benchmarks, which limits the evaluation of LaTtE-Flow’s true generative capability and generalization to open-ended visual synthesis.\n\n3. Although the architecture introduces several novel components, the ablation studies are relatively insufficient — many key design choices, such as the number of timestep experts or the specific contribution of residual attention, are not systematically analyzed."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hfmPDlij2j", "forum": "lVpWNivXEU", "replyto": "lVpWNivXEU", "signatures": ["ICLR.cc/2026/Conference/Submission13668/Reviewer_B17S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13668/Reviewer_B17S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980032313, "cdate": 1761980032313, "tmdate": 1762924235394, "mdate": 1762924235394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core idea of this paper is to distribute the timestep modelling of flow matching models to different transformer layers, improving inference speed by activating only a small subset of layers at each sampling timestep. Moreover, a Timestep-Conditioned Residual\nAttention mechanism is proposed to incorporate attention results across timesteps groups. The proposed method is instantiated with a VLM (i.e., Qwen2.5VL-2B), in the context of unified multimodal models. Following LMFusion, the LLM part is frozen to preserve the understanding ability of the original VLM. For image generation, the generation layers are trained on the ImageNet dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow. The sampling efficiency of visual generation is a significant research question.\n\n2. The solution of distributing timestep modelling across transformer layers is intuitive. And the proposed time-conditioned residual attention effectively incorporates cross-layer information, boosting convergence and overall performance.\n\n3. Comprehensive studies on the design choices, such as expert groups and the effects of residual attention, are conducted in the experiments."}, "weaknesses": {"value": "1. *Unclear Motivation:* As stated in the abstract, the paper studies unified multimodal models that struggle to achieve the same level\nof performance compared to specialist models. However, the paper only addresses the problem of sampling efficiency, which seems to have digressed from the core issue of unified models.\n\n2. *Experiments Are Incomprehensive:* Although the paper is for unified multimodal models that include both text and image, the image generation of the model is only trained and evaluated on the class-conditial generation dataset---ImageNet."}, "questions": {"value": "The idea of Layerwise Timestep-Expert seems a universal solution to all diffusion/flow-matching models, would it be applicalble to a wider range of DiTs for visual generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CSldGsunIR", "forum": "lVpWNivXEU", "replyto": "lVpWNivXEU", "signatures": ["ICLR.cc/2026/Conference/Submission13668/Reviewer_wLVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13668/Reviewer_wLVs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762271406128, "cdate": 1762271406128, "tmdate": 1762924235133, "mdate": 1762924235133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}