{"id": "jGXTx64gal", "number": 18832, "cdate": 1758291236905, "mdate": 1759897078882, "content": {"title": "FERD: Fairness-Enhanced Data-Free Adversarial Robustness Distillation", "abstract": "Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from the teacher to the student without accessing the training data. While existing methods focus on overall robustness, they overlook the robust fairness issues, leading to severe disparity of robustness across different categories. In this paper, we find two key problems: (1) student model distilled with equal class proportion data behaves significantly different across distinct categories; and (2) the robustness of student model is not stable across different attacks target. To bridge these gaps, we present the first Fairness Enhanced data-free Robustness Distillation (FERD) framework to adjust the proportion and distribution of adversarial examples. For the proportion, FERD adopts a robustness guided class reweighting strategy to synthesize more samples for the less robust categories, thereby improving robustness of them. For the distribution, FERD generates complementary data samples for advanced robustness distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a uniformity constraint on feature-level predictions, which suppress the dominance of class-specific non-robust features, providing a more balanced representation across all categories. Then, FERD constructs Uniform-Target Adversarial Examples (UTAEs) from FAEs by applying a uniform target class constraint to avoid biased attack directions, which distribute the attack targets across all categories and prevents overfitting to specific vulnerable categories. Extensive experiments on three public datasets show that FERD achieves state-of-the-art worst-class robustness under all adversarial attack (e.g., the worst-class robustness under FGSM and AutoAttack are improved by 15.1% and 6.4% using MobileNetV2 on CIFAR-10), demonstrating superior performance in both robustness and fairness aspects. Our code is available at: [https://anonymous.4open.science/r/FERD-2A48/](https://anonymous.4open.science/r/FERD-2A48/).", "tldr": "", "keywords": ["Data-Free Robustness Distillation; Robust Fairness"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a87078b28b36d5fa687f32b8d894772c67846e1.pdf", "supplementary_material": "/attachment/9e126095bcee30679a8f224349cc829e604c89cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework, FERD, to address the robustness and fairness issues in data-free adversarial robustness distillation. The authors find that, even with uniformly sampled synthetic data, the robustness of the student model in traditional DFRD methods still varies significantly across categories, and the success rate of adversarial attacks also varies depending on the target class. FERD addresses this issue through two strategies: Robustness-guided class reweighting: This increases the generation of samples for weakly robust classes; Fairness-aware sample and uniform target adversarial sample generation: This improves the class coverage of adversarial samples by uniformly constraining feature levels and uniformizing attack targets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is easy to follow.\n\n2. The proposed method is novel. A class reweighting strategy is proposed to enhance robustness against weak classes. FAEs and UTAEs are designed to improve fairness at both the feature and attack target levels.\n\n3. The experimental result demonstrates the proposed method can effectively alleviate fairness issues in data-free ARD.\n\n4. This paper also provides an explanation from a theoretical level."}, "weaknesses": {"value": "1. This paper only selected one teacher model for one data set. I am curious whether the method is effective when different teacher models are selected.\n\n2. The experimental result in newer architectures, such as VIT, can further verify the effectiveness."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TfoRq9c4cs", "forum": "jGXTx64gal", "replyto": "jGXTx64gal", "signatures": ["ICLR.cc/2026/Conference/Submission18832/Reviewer_VYHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18832/Reviewer_VYHq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761028828423, "cdate": 1761028828423, "tmdate": 1762930802881, "mdate": 1762930802881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that data-free robust distillation (DFRD) methods create models with unfair robustness, meaning they are robust for some class but non-robust for others. The authors find this is because DFRD trains on an equal number of samples for all classes and uses attacks that are not diverse. They propose FERD, which fixes this by generating more synthetic data for the non-robust classes and creating special adversarial attacks that target all classes uniformly. Experiments show this method improves the robustness of the weakest classes, leading to better overall fairness in the context of DFRD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to investigate and address the problem of robust fairness specifically within the context of Data-Free Robustness Distillation (DFRD), an area where fairness considerations have been largely overlooked.\n2. The proposed class reweighting strategy introduces a mechanism to generate intentionally imbalanced synthetic data tailored for fairness improvement, adapting generation proportions based on class vulnerability within the data-free constraint."}, "weaknesses": {"value": "1. The paper claims state-of-the-art robust fairness in DFRD but fails to compare against a crucial baseline: combining a standard DFRD data generation method with established fairness-aware adversarial distillation techniques like Fair-ARD or ABSLD. It is plausible that simply applying existing fairness distillation losses to student training, using data from a generic DFRD generator, could yield comparable fairness improvements. Without this comparison, the paper does not demonstrate that FERD's specific components offer benefits beyond a straightforward integration of known DFRD generation and fairness AD methods.\n\n2. Robust fairness methods (e.g., ABSLD) demonstrate their effectiveness by showing significant improvements in worst-case robustness or NSD while maintaining comparable or slightly improved average robustness relative to strong baselines. This isolates the contribution specifically to fairness. However, FERD's results show a large increase in average robustness alongside the worst-case improvement (e.g., Table 1, RN-18/CIFAR-10: Avg. AA +3% vs. Worst AA +1% compared to DFHL). This large average gain makes it difficult to determine if the worst-case improvement is a direct outcome of the fairness mechanisms or simply a byproduct of the model becoming significantly stronger overall. While NSD improves, the paper does not isolate the fairness contribution by comparing against a baseline with similar average robustness, thus obscuring whether FERD primarily enhances fairness or just general robustness, which happens to also lift the worst-case performance.\n\n3. As a pioneering work in DFRD fairness, the paper's evaluation could be strengthened by providing additional context. Including teacher's own fairness profile and student performance under data-available adversarial distillation—using both standard (e.g., RSLAD) and fairness-aware (e.g., ABSLD) methods—would offer valuable reference optimal points. Without these comparisons, and alongside the significant average robustness gains (Weakness 2), it remains somewhat challenging to fully gauge the significance of FERD's fairness improvements specifically within the constraints of the data-free setting."}, "questions": {"value": "-\tWhile Section 2.1 discusses several DFRD methods (DFARD, DERD, DFHL), your experiments only include DFHL as a direct DFRD baseline, primarily comparing against adapted DFKD methods. Could you explain the rationale for omitting DFARD and DERD from the empirical comparison and for this specific choice of baselines?\n-\tFor the adapted DFKD baselines, why did you use standard PGD for attack generation instead of the inner maximization objective proposed in the original RSLAD paper? Could this choice underestimate the adapted baselines' potential robustness?\n\nThis paper tackles the important problem of DFRD fairness with strong results. However, concerns about baselines and context limit the current evaluation. I am open to revising my score based on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BBrJWmtLPq", "forum": "jGXTx64gal", "replyto": "jGXTx64gal", "signatures": ["ICLR.cc/2026/Conference/Submission18832/Reviewer_jJ8d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18832/Reviewer_jJ8d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186157518, "cdate": 1761186157518, "tmdate": 1762930802434, "mdate": 1762930802434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work offers a valuable perspective on robust fairness under data-free settings and provides convincing experimental evidence, though the methodological and theoretical originality are limited."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses the intersection of data-free robustness distillation and robust fairness, which is an emerging and practically important direction for fairness-aware model compression and deployment on edge devices.\n\nThe experiments are extensive and demonstrate consistent improvements in worst-class robustness and normalized standard deviation (NSD) across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.\n\nAblation studies, hyperparameter analyses, and visualization of synthetic samples (Fig. 6, Table 3) enhance the reproducibility and credibility of results."}, "weaknesses": {"value": "Although the paper includes proofs of conjectures (Appendix A.1–A.2), they are mostly intuitive restatements of empirical observations and lack rigorous formalism. \n\nThe fairness claim relies mainly on NSD and worst-class robustness. Introducing additional fairness metrics would make the evaluation more convincing.\n\nThe manuscript is lengthy and includes numerous large figures and dense equations that affect readability."}, "questions": {"value": "Although the paper includes proofs of conjectures (Appendix A.1–A.2), they are mostly intuitive restatements of empirical observations and lack rigorous formalism. \n\nThe fairness claim relies mainly on NSD and worst-class robustness. Introducing additional fairness metrics would make the evaluation more convincing.\n\nThe manuscript is lengthy and includes numerous large figures and dense equations that affect readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eGlOFHmLCV", "forum": "jGXTx64gal", "replyto": "jGXTx64gal", "signatures": ["ICLR.cc/2026/Conference/Submission18832/Reviewer_WX8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18832/Reviewer_WX8r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664501306, "cdate": 1761664501306, "tmdate": 1762930801869, "mdate": 1762930801869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies **robust fairness in Data-Free Robustness Distillation (DFRD)** — transferring robustness from a teacher to a student model without original data.\n\nThe authors identify two fairness issues:\n\n(1) class-wise robustness disparity under uniformly sampled synthetic data, and\n\n(2) target-dependent vulnerability under adversarial attacks.\n\nThey propose **FERD**, which enhances fairness at both the *proportion* and *distribution* levels:\n\n- **Robustness-guided Class Reweighting** generates more synthetic data for weakly robust classes.\n- **Fairness-Aware / Uniform-Target Adversarial Examples (FAEs / UTAEs)** apply uniformity constraints on feature and target spaces to prevent biased robustness transfer.\n\nExperiments on **CIFAR-10, CIFAR-100, and Tiny-ImageNet** show that FERD achieves higher worst-class robustness (+15.1% under FGSM) and lower NSD than prior DFRD methods, suggesting improved fairness and stability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and well-defined problem setup: fairness within DFRD.\n- Dual-level solution addressing both sample imbalance and attack-target bias.\n- Consistent empirical results across datasets and attack variants.\n- Ablation studies verify component contributions."}, "weaknesses": {"value": "- **Limited theoretical rigor:** The design is intuitive but lacks formal analysis — no explicit robustness bounds, convergence arguments, or assumption statements are provided.\n- **Restricted evaluation scope:** Experiments are confined to small datasets (CIFAR-10/100, Tiny-ImageNet) and a single perturbation budget (ε = 8/255), leaving its generality under stronger or diverse attacks unexplored.\n- **Hyperparameter opacity:** Key parameters (τ, γ, λ) are fixed without justification or sensitivity analysis, creating uncertainty about robustness across configurations."}, "questions": {"value": "- Could you include **ε-sweep results (ε = {4,6,8,10}/255)** to test whether fairness improvements persist under stronger perturbations?\n- Have you explored the **stability of results** under different τ, γ, and λ settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j8JYU3Tqdo", "forum": "jGXTx64gal", "replyto": "jGXTx64gal", "signatures": ["ICLR.cc/2026/Conference/Submission18832/Reviewer_wkEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18832/Reviewer_wkEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089408003, "cdate": 1762089408003, "tmdate": 1762930801343, "mdate": 1762930801343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}