{"id": "EkzVuZj60c", "number": 1719, "cdate": 1756911870329, "mdate": 1762956284771, "content": {"title": "Compositional Text-to-Image Generation Via Region-aware Bimodal Direct Preference Optimization", "abstract": "Despite the rapid progress of text-to-image (T2I) models, generating images that accurately reflect complex compositional prompts (covering attribute bindings, object relationships, counting) still remains challenging. To address this, we propose \\ourmethod, a framework to enhance T2I model's capability of compositional text-to-image generation. We begin by introducing an carefully designed pipeline to construct a large-scale preference dataset, \\ourdataset, with strictly quality control. Then, we extend Diffusion DPO to jointly optimize image and text preferences, which is shown to greatly effective in improving the models to follow complex text prompt in generation. To further enhance the models for fine-grained alignment, we employ a region-level guidance method to focus on regions relevant to compositional concepts. Experimental results demonstrate that our \\ourmethod substantially improves compositional fidelity, consistently outperforming prior methods across multiple benchmarks. Our approach highlights the potential of preference-based fine-tuning for complex text-to-image tasks, offering a flexible and scalable alternative to existing techniques.", "tldr": "", "keywords": ["Compositional Text-To-Image Generation", "Direct Preference Optimization", "Diffusion Models", "Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6dd09afb0bb81cb6a83478560ee5a8be7cb32560.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents BIDPO, a framework that extends Diffusion DPO to bimodal preference optimization for improving compositional text-to-image generation. The main contributions are jointly optimizing both image and text preferences, introducing region-level guidance to focus on relevant image regions, and developing an automated pipeline to construct a large-scale preference dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive data pipeline: The automated data construction process is systematic and well-designed, incorporating multiple quality control stages and featuring refined design specifically tailored for compositional text tasks.\n\n2. Experiments show significant improvements over SDXL, with performance gains across multiple benchmarks."}, "weaknesses": {"value": "1. Why does SDXL-SFT show performance degradation? Does this indicate that the quality of the constructed dataset is insufficient? Typically, diffusion-DPO also performs SFT before DPO, and SFT usually leads to performance improvements. Can you explain why SFT results in performance decline in this paper?\n\n2. Although the metrics show significant improvements on benchmarks, visible distortions can be observed in the generated images in Figure 5. Can you design methods to enhance compositional text capabilities while maintaining perceptually acceptable image quality?\n\n3. Can you explain the reasons for the collapse observed in TextDPO?"}, "questions": {"value": "Address the issues mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t9VuwQMSxs", "forum": "EkzVuZj60c", "replyto": "EkzVuZj60c", "signatures": ["ICLR.cc/2026/Conference/Submission1719/Reviewer_SZEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1719/Reviewer_SZEN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636831631, "cdate": 1761636831631, "tmdate": 1762915867972, "mdate": 1762915867972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "e9IsGMapJw", "forum": "EkzVuZj60c", "replyto": "EkzVuZj60c", "signatures": ["ICLR.cc/2026/Conference/Submission1719/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1719/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762956283810, "cdate": 1762956283810, "tmdate": 1762956283810, "mdate": 1762956283810, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his work proposes BIDPO, a novel framework that improves model alignment by performing fine-grained preference optimization on both text and image modalities. In the standard DPO, the optimization process learns to prefer winning images over losing images for a given prompt. In this updated formulation, the BIDPO, proposes to also align the preference on correct prompt for a given image, where the incorrect prompt is obtained by slight perturbation and obtained a rigorous image editing/VLM verification framework. Additionally, it proposes a region-level guidance mechanism that guides the model’s focus towards regions of interest in the preference optimization objective. This mechanism is shown to substantially enhance the capability for fine-grained text-to-image alignment. Finally, the paper proposes BICOMP, a new preference dataset that consists of 57,474 original images and 94,502 edited images, covering six dimensions: color, shape, texture, spatial relationship, non-spatial relationship and numeracy. Preference optimization experiments on SDXL architecture with the proposed BIDPO methods show the efficacy of the proposed scheme."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- BICOMP dataset construction scheme looks interesting and would be helpful in other directions (including joint signal in image and text alignment) since there exists instructions on few edits and corresponding edited images.\n- BIDPO seems novel in the addition of winning and losing text pairs in the DPO formulation. Although the text component in the BIDPO has not been flushed out and verified thoroughly on other models."}, "weaknesses": {"value": "- There seems to be no comparison against existing popular DPO methods such as MAPO, SPO, RankDPO, Flow-GRPO, etc. \n- The proposed method has been only applied on a single diffusion model SDXL. It is unclear if the scheme would translate to any other diffusion model, specially models which jointly processing image and text modalities. \n- The region-level guidance seems to only provide marginal improvements compared to other components and does not provide any significant contribution to the paper. \n- Many relevant works (see missing refs) have been omitted in the manuscript"}, "questions": {"value": "- For data creation, how do you ensure that the your generation pipeline rules out anatomically bad or incorrect / blurry generations from the Image generation models?\n- It’s unclear how the proposed method compares against popular DPO methods such as Flow-GRPO, RankDPO, SPO, LPO, MAPO, etc.\n- How does the method scale to models which jointly process image and text modalities such as SD3.5, Flux, Qwen-Image, etc.? Since at the center of this work is the fact that underlying diffusion model needs to understand the difference between correct and incorrect captions, it would be better to apply such an approach to a model which jointly processes image and text information. \n- There seems to be some discrepancy between Tab 4 and Tab 5. The DPG-Bench score for SDXL (baseline) do not align. Tab 4 reports 73.38 and Tab 5 reports 79.86. \n- SDXL-BIDPO with and without region-level guidance does not seem to be helping much on DPG-Bench and this is the only evidence of the region-level guidance approach.\n- How does one extend the proposed method to video modality?\n\n\nMissing References:\n- Margin-aware Preference Optimization for aligning diffusion models without reference (MAPO): https://arxiv.org/abs/2406.06424 \n- SPO : Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step https://arxiv.org/pdf/2406.04314v1 \n- Flow-GRPO— Training Flow Matching Models via Online RL : https://arxiv.org/pdf/2505.05470 \n- RankDPO — Scalable Ranked Preference Optimization for Text-to-Image Generation:  https://arxiv.org/abs/2410.18013  \n- Latent Preference Optimization (LPO) — Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization   : https://arxiv.org/pdf/2502.01051\n- DSPO — Direct Score Preference Optimization for Diffusion Model Alignment https://openreview.net/forum?id=xyfb9HHvMe \n- Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization https://arxiv.org/abs/2410.05255"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OKPYtxGphK", "forum": "EkzVuZj60c", "replyto": "EkzVuZj60c", "signatures": ["ICLR.cc/2026/Conference/Submission1719/Reviewer_LDgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1719/Reviewer_LDgq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786748034, "cdate": 1761786748034, "tmdate": 1762915867827, "mdate": 1762915867827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiDPO, a novel framework to enhance compositional text-to-image generation by combining bimodal direct preference optimization (DPO) with region-aware guidance. The authors construct a large-scale, high-quality preference dataset (BiComp) using a carefully designed automated pipeline and extend Diffusion DPO to jointly optimize image and text preferences. Experimental results on T2I-CompBench, GenEval, and DPG-Bench demonstrate improvements over baseline in attribute binding and object relationships."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces bimodal DPO, a novel framework to improve compositional T2I without the need for additional inputs or reliance on external tools.\n2. It proposes a BiDPO formulation that jointly optimizes image and text preferences, which is a well-motivated extension of DPO methods.\n3. The BiComp dataset is large and carefully constructed with strict quality control.\n4. The method achieves consistent improvements across multiple benchmarks over its baseline."}, "weaknesses": {"value": "1. The data construction pipeline may be potentially redundant. It uses multiple external tools (GroundingDINO, SAM etc.), which is computationally expensive and complex. As far as I know, Qwen2.5-VL alone may be capable of both detection and captioning, \nWhat is the difference of using Qwen2.5-VL to write the region information and template-based new caption and your pipeline? Qwen-Image-Edit also allows image editing using text prompts, without the need for object masks. \n2. Although the proposed method requires no additional inputs or integration of external LLM, DPO relies on a large, high-quality preference dataset which is expensive to create and may be sensitive to the quality and distribution of the data. \n2. The paper lacks comparisons with alternative methods, such as Attend-and-Excite[A], RPG-DiffusionMaster[B], implemented on the same base model, making it difficult to assess relative performance gains.\n4. Figure 4 is not explained, especially how TextDPO and ImageDPO are combined in practice.\n5. The choice of state-of-the-art models for comparison could be more up-to-date  (e.g., Nano-banana[C], GPT-Image-1[D]) .\n\n[A] Chefer, Hila, et al. \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\" ACM transactions on Graphics (TOG) 42.4 (2023): 1-10.\n\n[B] Yang, Ling, et al. \"Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms.\" Forty-first International Conference on Machine Learning. 2024.\n\n[C] Google. Introducing gemini 2.5 flash image, our state-of-the-art image model. 2025.\n\n[D] OpenAI. Gpt-image-1. 2025."}, "questions": {"value": "1. Can you provide a more detailed explanation of how the bimodal DPO loss combines image and text preferences (e.g., in Figure 4)?\n2. Why was LoRA chosen over full fine-tuning? Were any experiments conducted to compare their effectiveness?\n3. Can you compare BiDPO with other compositional-generation methods (e.g., Attend-and-Excite) under the same base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oLxYO9mCRO", "forum": "EkzVuZj60c", "replyto": "EkzVuZj60c", "signatures": ["ICLR.cc/2026/Conference/Submission1719/Reviewer_PsCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1719/Reviewer_PsCF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959949016, "cdate": 1761959949016, "tmdate": 1762915867603, "mdate": 1762915867603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an improved formulation for Direct Preference Optimization in diffusion modles. Speciifcally, the paper shows how images from altered prompts can be used to construct improved preference pairs (i.e. pairing the right caption for the image vs mismatched captions). In addition ot this, there's also a mask applied at the pixel/latent level to ensure that important regions in the sample are highlighted in the optimization. Results on prompt following benchmarks such as T2I-Compbench, GenEval, and DPG-Bench indicate that the method is able to bring notable improvements to the base SDXL model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a very interesting and compelling idea of a) preparing preference pairs in a targeted manner with precise modifications between the positive negatives and b) an improved loss fomrulation where mismatched captions can be used for additional supervision.\n\nThe paper also achieves promising results on SDXL and is reasonably well-presented."}, "weaknesses": {"value": "[Major]\n\nFollowing Diffusion-DPO, there's been a bunch of work incorportaitng additional structure both in terms of the data construction and the loss into the formulation [a,b,c]. DenseDPO for instance applies the loss at finer temporal granularity for video models, which is quite close to the idea of having a spatial mask in images. Similarly, CaPO and RankDPO (among many other works in this direction) also demonstrate good improvements over the original Diffusion-DPO formulation, including on these same benchmarks. It would be a good idea for the paper to acknowledge these. \n\nAdditionally, an important caveat is regarding the source of data/prompts. A lot of previous DPO works (Diffusion-DPO/KTO, CaPO) rely on either Pick-a-Pic or DiffusionDB which have images that are quite different from T2I-Compbench/GenEval. Here, the data directly includes prompts from T2I-Compbench training set among other sources. While this is clearly not overfitting, it also might explain why the performance improvements on T2I-Compbench are so outsized compared to the gains on other benchmarks (which are still good, but not as dramatic). \n\n\n\n[Minor]\n\nI'd also be curious to see how the BiDPO formulation performs beyond the SDXL LoRA fine-tuning setting with newer models (e.g. with SD3/Flux etc.)\n\nThe effect of the region-guidance seems quite minimal, I would be curious if this is really necessary, or is there some scenario where it can be particularly useful? \n\n\n[a] Lee et al. \"Calibrated Multi-Preference Optimization for Aligning Diffusion Models\", CVPR 2025\n\n[b] Karthik et al. \"Scalable Ranked Preference Optimization for Text-to-Image Generation\", ICCV 2025\n\n[c] Wu et al. \"DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models\", NeurIPS 2025"}, "questions": {"value": "I think broadly the paper introduces an interesting concept for improving text-to-image models, and I'm inclined towards accepting it. I'd mostly want clarity from the authors on a) the effect of the data source and b) contextualizing the paper better in the contex to existing works on diffusion model alignment (e.g. there are 25 papers here on DPO https://github.com/xie-lab-ml/awesome-alignment-of-diffusion-models). While it's impossible to exhaustively cover all the works, there must atleast be some effort to summarize the existing work in the field."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4DtrZHZQv4", "forum": "EkzVuZj60c", "replyto": "EkzVuZj60c", "signatures": ["ICLR.cc/2026/Conference/Submission1719/Reviewer_qSVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1719/Reviewer_qSVV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977439869, "cdate": 1761977439869, "tmdate": 1762915867415, "mdate": 1762915867415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}