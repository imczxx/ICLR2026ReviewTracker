{"id": "fyQANoXid7", "number": 12312, "cdate": 1758207021465, "mdate": 1759897518142, "content": {"title": "Towards Robust Multimodal Learning via Adaptive Model Assembly", "abstract": "Adversarial fine-tuning is a widely used strategy to enhance the robustness of vision-language pre-trained models (VLPs) such as CLIP, ALBEF, and TCL. Traditional methods, however, typically fine-tune a single static model to defend against a specific attack type, limiting their ability to generalize to diverse or unseen adversarial threats. To address this, we propose Multimodal Adaptive Adversarial Fine-tuning (MAAF), a novel framework that achieves robust multimodal learning by adaptively assembling input-conditioned model parameters at inference time. MAAF starts from a shared base model and learns multiple defense vectors, which are dynamically fused through a lightweight, input-aware generation network to produce robust, sample-specific model parameters. This adaptive assembly allows the model to resist a wide range of adversarial attacks without retraining. Extensive experiments on standard vision–language benchmarks show that MAAF substantially enhances adversarial robustness while preserving clean accuracy, consistently outperforming existing fine-tuning methods. The results also provide insights into the distribution of defense vectors, the importance of adaptive fusion, and the optimal number of vectors for achieving a balance between robustness and stability. Code is available at \\href{https://anonymous.4open.science/r/MAAF-63FC}{https://anonymous.4open.science/r/MAAF-63FC}.", "tldr": "", "keywords": ["Adversarial fine-tuning", "adversarial robustness", "adaptive model generation", "multimodal attack", "multimodal learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/689fa35f72ea27696abc7971de98b682ebf53e74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed multimodal adaptive adversarial fine-tuning to enhance the adversarial robustness of vision-language pretraining models against unimodal and multimodal attack strategies. \nTraditional adversaial fine-tuning methods is designed for unimodal adversarial attacks and cannot be generalized to unseen attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Compared with previous adversarial training methods for vision-language models that primarily focus on visual attacks, MAAF explores adversarial robustness by learning a set of defense vectors and dynamically fusing them to construct input-specific defense models, thereby achieving strong robustness against diverse adversarial perturbations."}, "weaknesses": {"value": "1. Insufficient experiments. Adversarial fine-tuning for vision-language pretrained models should consider not only unimodal but also multimodal attack scenarios. The authors claim that the proposed MAAF enhances robustness against both types of attacks. However, the experimental evaluation is incomplete — it omits key benchmarks such as AutoAttack (including APGD with cross-entropy loss and APGD with DLR loss, each with 100 iterations). In addition, the perturbation budget for BERT-Attack is fixed at 1, with no further investigation of different perturbation magnitudes.\n\n2. To ensure the integrity of the experiments, the authors should evaluate MAAF using different vision encoder variants, such as ViT-L/14 and ViT-B/32. In particular, for the transfer-based attack evaluation, it would be important to examine whether MAAF can provide stronger adversarial defenses when applied to multimodal large language models (e.g., LLaVA).\n\n\n3. The robustness improvement of MAAF may partly result from its use of randomized multimodal attacks during fine-tuning, giving it a stronger and more diverse training setup than the fixed-attack baselines, thus raising fairness concerns in the comparison.\n\n4. The ablation studies are incomplete and do not isolate the key sources of improvement. In particular, the paper lacks experiments that remove the adaptive fusion network, fix the attack distribution, or compare multiple defense vectors without adaptive weighting. As a result, it remains unclear whether the performance gains stem from the proposed adaptive fusion mechanism, the diversity of defense vectors, or the randomized attack training.\n\n5. The paper admit that \"MAAF introduces additional inference overhead due to dynamic model synthesis” but lacks quantitative analysis of its computational or inference cost, making it unclear how efficient or scalable MAAF is in practice."}, "questions": {"value": "The questions are detailed in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lylznHRseP", "forum": "fyQANoXid7", "replyto": "fyQANoXid7", "signatures": ["ICLR.cc/2026/Conference/Submission12312/Reviewer_MzE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12312/Reviewer_MzE1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535602428, "cdate": 1761535602428, "tmdate": 1762923239839, "mdate": 1762923239839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MAAF, a new framework to defend VLPs against adversarial attacks. Instead of training one static model, MAAF learns a base model and a set of \"defense vectors\". At inference time, a small fusion network predicts how to best combine these vectors based on the specific input sample. This creates a custom, input-specific model to defend against attacks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of creating an input-specific model assembly is interesting. The loss function (Eq. 8) is well-motivated, especially the inclusion of the embedding alignment term ($\\mathcal{L}_{vl}$) and the cosine regularization term ($\\mathcal{L}_{cos}$) to ensure semantic consistency and defense diversity.\n\nThe authors validate their method across multiple VLP architectures (dual-encoder CLIP and fusion-based ALBEF/TCL) and different tasks (Image-Text Retrieval and Visual Entailment)"}, "weaknesses": {"value": "The most importantly, the paper completely fails to cite or discuss a highly relevant and recent category of methods: test-time adversarial prompt tuning [1,2]. It seems that the authors overlook the set of works for adversarial prompt tuning on VLMs.\n\n[1] TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models, CVPR 2025\n[2] Clip is strong enough to fight back: Test-time counterattacks towards zero-shot adversarial robustness of clip. CVPR 2025\n\nThis omission is the most severe problem. MAAF's core mechanism, adapting parameters at inference time to fight an attack, is conceptually very similar to test-time prompt tuning. In MAAF, a fusion network $\\mathcal{F}_{\\phi}$ generates weights $\\lambda_k$ to create new parameters $\\theta$. In TAPT, parameters (the prompts) are also optimized or generated at test time for the same goal. By ignoring this entire field, the paper's claim to novelty is significantly undermined. The method must be compared against these TAPT baselines.\n\nThe method introduces significant overhead at inference time. For every sample, the model must first run the fusion network $\\mathcal{F}_{\\phi}$ and then perform parameter assembly ($\\theta_0 + \\sum \\lambda_k \\Delta\\theta_k$) before it can even begin the standard forward pass. The authors admit this overhead in the conclusion 16 but provide zero quantitative analysis (e.g., latency in milliseconds, or extra FLOPS). Without this data, the practical utility of the method is unknown.\n\n\nMinors:\n\n- Image Quality: All figures are low-resolution bitmaps and are blurry. These should be high-quality vector graphics (e.g., PDF/SVG).\n\n- Appendix: The appendix sectioning appears to be incorrect (e.g., starting with .1, .2).\n\n- Missing LLM Declaration: The paper is missing the mandatory ICLR policy statement on the use of Large Language Models (LLMs) for writing or polishing the text, if the authors use it."}, "questions": {"value": "- How does the method fundamentally differ from test-time adversarial prompt tuning (TAPT) methods [1, 2]?Please provide a detailed comparison.\n- What is the exact inference overhead of MAAF? Please provide the wall-clock latency (in ms) and/or FLOPS required for the fusion network and parameter assembly, and compare it to the baseline model.\n- Why does performance degrade when $K > 3$? Does this mean MAAF can only effectively handle 2-3 \"types\" of attacks?\n- Please add the mandatory ICLR LLM usage declaration if the authors use it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a3bnSbHUgF", "forum": "fyQANoXid7", "replyto": "fyQANoXid7", "signatures": ["ICLR.cc/2026/Conference/Submission12312/Reviewer_B4hu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12312/Reviewer_B4hu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566717415, "cdate": 1761566717415, "tmdate": 1762923238637, "mdate": 1762923238637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed MAAF (Multimodal Adaptive Adversarial Fine-tuning) which is an adversarially trained defense mechanism that is meant to adapt to various attacks during inference. Specifically, during the inner loop of adversarial training, MAAF allows attacks to be sampled from a rather large search space (including ) and it learns a collection of defense vectors (the same size of the entire model), whose linear combination is believed to generate defenses against different types of attacks. MAAF was evaluated on 3 VLP models on image-text retrieval and visual entailment against TeCoA and FARE under attacks of various types. The results show that MAAF consistently achieves better robustness while maintaining clean  accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The choice of VLM architectures and tasks for evaluation is pretty comprehensive.\n+ MAAF shows strong and consistent empirical improvement over the baselines.\n+ Input-adaptive defense is an interesting topic.\n+ Good insights from PCA visualization of defense vectors and fusion weight distributions."}, "weaknesses": {"value": "+ The size of each defense vector is the same as the model to protect, which requires the resources to train a model that is multiple times larger than that model, which doesn't sound like a scalable approach when the VLPs are no longer as small as less than 100 million parameters. In a sense MAAF sounds like a MoE but uses linear combination instead of routing. \n+ The idea to defend against more attacks by using a more diverse adversarial search space is pretty natural and people have started to do similar staff years ago like in [1] and making defense mechanism to adapt to inputs is also not entirely new as we have test-time adaption or meta-learning based adversarial training like [2].\n+ The paper does not include an ablation study to see what if we use a larger adversarial search space but keep the vanilla adversarial training what would be the result, or if we have models trained for individual attack types and then ensemble them what would be the result. The current comparisons with baselines don't in fact validate the merit of MAAF because it already uses more data and more parameters.\n\n1. Tramer, Florian, and Dan Boneh. \"Adversarial training and robustness for multiple perturbations.\" Advances in neural information processing systems 32 (2019).\n2. Bamdad, Amirmohammad, Ali Owfi, and Fatemeh Afghah. \"Adaptive Meta-learning-based Adversarial Training for Robust Automatic Modulation Classification.\" arXiv preprint arXiv:2501.01620 (2025)."}, "questions": {"value": "Please refer to the weaknesses for my concerns about his paper. The following questions are only listed for easier understanding of what I am concerning.\n+ What is the cost of training and using MAAF vs. the unprotected model and the baselines?\n+ How would MAAF compare with meta learning based adaptive adversarial training?\n+ Can you apply defense vectors only to some select layers of the model?\n+ Do attacks of the same type triggers similar defenses?\n+ How does MAAF compare to an ensemble of models trained individually on their own search space? Or a model that doesn't uses the defense vectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UtWkitLXXZ", "forum": "fyQANoXid7", "replyto": "fyQANoXid7", "signatures": ["ICLR.cc/2026/Conference/Submission12312/Reviewer_qfa1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12312/Reviewer_qfa1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953797228, "cdate": 1761953797228, "tmdate": 1762923238299, "mdate": 1762923238299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAAF (Multimodal Adaptive Adversarial Fine-tuning), a framework for improving the robustness of vision–language pre-trained models (VLPs) such as CLIP, ALBEF, and TCL. Unlike traditional fine-tuning approaches that produce a single static model, MAAF learns multiple defense vectors and adaptively assembles them at inference time using an input-aware generation network. This design allows for sample-specific parameter adaptation without retraining. Experiments on standard benchmarks show consistent robustness improvements over existing fine-tuning methods, with additional analyses on the behavior and diversity of defense vectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea is conceptually interesting. The adaptive assembly of model parameters for each input provides a novel and flexible approach to robustness.\n\n2. The experimental design includes results demonstrating the improvements on multiple VLPs brought by MAAF.\n\n3. The paper includes clear writing and presentation. The paper is well-organized, and the analysis of defense vector distribution adds interpretability and depth.\n\n4. The proposed method shows balanced performance. The work maintains strong clean accuracy while improving adversarial robustness, which is practically valuable."}, "weaknesses": {"value": "1. The attackcoverage is limited. The paper does not evaluate against representative VLM-based attack frameworks such as VLAttack, nor against black-box text adversarial attacks like TextHoaxer (AAAI 2021), LeapAttack (KDD 2022), or PAT (KDD 2023), which limits the generality of the claimed robustness.\n\n2. There is a lack of computational analysis. The cost of adaptive model assembly, including parameter generation and adaptive fusion, is not quantified, making it unclear how scalable the approach is for real-time or large-scale applications."}, "questions": {"value": "1. Could the authors include evaluations under VLAttack and text-based adversarial settings to demonstrate robustness across modalities?\n\n2. How significant is the computational overhead introduced by the adaptive assembly mechanism during inference?\n\n3. Can the proposed method generalize to cross-modal or multimodal black-box attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1kFhCYK8RS", "forum": "fyQANoXid7", "replyto": "fyQANoXid7", "signatures": ["ICLR.cc/2026/Conference/Submission12312/Reviewer_3kNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12312/Reviewer_3kNM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967182546, "cdate": 1761967182546, "tmdate": 1762923237985, "mdate": 1762923237985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}