{"id": "q8jq1RPfB0", "number": 14801, "cdate": 1758244009519, "mdate": 1759897348545, "content": {"title": "GCSGNN: Towards Global Counterfactual-Based Self-Explainable Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) exhibit superior performance in various graph-based tasks, ranging from scene graph generation to drug discovery. However, they operate as black-box models due to the lack of access to their rationale for a specific prediction. To enhance the transparency of GNNs, graph counterfactual explanation (GCE) identifies the minimal modifications to the input graph that cause the GNN to change its prediction to a different class. Current GCE methods face two major challenges: (1) they adopt a post-hoc explanation paradigm by separately training an explainer model for a trained GNN. This sequential optimization\nprocess yields suboptimal explanations since the GNN training process is not exposed to the explainer. (2) Current methods are primarily local-level approaches, which means that they generate explanations for each input sample individually. As a result, they cannot capture the shared prediction rationales that generalize across the entire input data distribution. \nTo address these two challenges, we propose a novel Global Counterfactual-based Self-explainable GNN (GCSGNN) framework. GCSGNN can simultaneously act as a GNN, providing predictions on input samples, and an explainer, generating explanations for its predictions. Furthermore, GCSGNN is trained to identify common patterns in the GNN embeddings across input samples, enabling it to learn global (i.e., model-level) explanations. Extensive qualitative and quantitative analysis across various datasets demonstrates that our GCSGNN achieves outstanding performance against the baseline methods. Our code can be found at https://anonymous.4open.science/r/gcsgnn.", "tldr": "", "keywords": ["Graph Neural Networks", "Self-explainability", "Graph Counterfactual Explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1299412acaec9d126b61fb6f2bb9330b74343d71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors of this work start by pointing out critical issues with current graph counterfactual explanations. Namely, since current existing GCE methods are post hoc, there is an external explanation model that is detached from the GNN they are seeking to train. They point out that this can cause the generation of inconsistent explanations. Another issue is the limited scope of GCE methods; most methods take an individual input graph and generate an explanation per instance. They point out that this methodology fails to diagnose global patterns consistent throughout the distribution of graphs. Their method tries to find global patterns that are more consistent and stable by exploring the embedding space. Specifically, they find the channels (dimensions) in the embedding space that influence the graph prediction the most. They find counterfactual embeddings that are vectors that have the same dimension as the number of important channels. They then convert the counterfactual sub-embedding into a subgraph that acts as the counterfactual explanation. They describe their counterfactual explanation in the form of a tuple called counterfactual graph embedding edits (CGEE) which consist of the position of the important channels/dimension of the embedding space and the counterfactual sub-embedding vector. They introduce the notion of coverage where given a GNN the coverage of a set of CGEE is the portion of input graphs such that when applying one tuple in the set of CGEE produces a valid counterfactual (change of label w.r.t. the GNN). The authors also prove that their method achieves better explanations (by maximizing the MI between label and prediction for both the original GNN and counterfactual explanation) than any post-hoc methods. This proof is straightforward observation that they employ an unconstrained optimization in comparison to a constrained optimization over the parameter space that post-hoc methods use. Their framework takes learnable matrices that learn important channels in the embedding space and counterfactual sub-embeddings, and minimizes the global-level GCE loss function. They also train an encoder and decoder according to a reconstruction loss. They conduct experiments on several baselines on graph counterfactual interpretability. They assess w.r.t. their metric of coverage and proximity (graph edit distance to the CFE) the other baselines to see how methods fare. Finally, they assess other factors such as parameter analysis, a case study, and runtime analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well written. From a scan of the literature it seems they are filling a gap in the literature utilizing several existing frameworks/methodologies to provide an improvement to existing graph interpretability works, particularly in counterfactual explanations.\n\n\nS2. Methodology is reasonable and initial implementation decisions make sense. \n\n\nS3. The results in some experiments suggest that the method is more effective than existing post-hoc methods while also being more effective than other global CFE methods."}, "weaknesses": {"value": "W1. The idea of exploring the embedding space and exploring critical channels is a somewhat novel angle. The one issue with this is the reliability/stability of going from embedding space to graph space. Ideally, the mapping from important sub-embeddings to critical subgraphs that act as the CFE should be exact, however in practice this is not usually the case. The authors do not give any guarantees or even assurances on the reliability of sub-embeddings being mapped to important subgraphs. The authors should explore this weakness or at the very least justify this point.\nW2. The authors conduct experiments on several baselines. However, they have missed a critical baseline on counterfactual graph explanations [1]. If this work is not relevant to their work they should still justify their choice of excluding it since it is a global graph counterfactual method. \n[1] Bajaj, Mohit, et al. \"Robust counterfactual explanations on graph neural networks.\" Advances in neural information processing systems 34 (2021): 5644-5655."}, "questions": {"value": "1.) Can you explain why the reconstruction loss ensures that sub-embeddings can be mapped reliably to important subgraphs for CFE?\n2.) As per W2 [1] was left out and if it intentional can you justify the reasoning of excluding it from this work, otherwise I do believe this is a relevant baseline and should at least be mentioned in the work as it is quite relevant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B3FoP4ogf1", "forum": "q8jq1RPfB0", "replyto": "q8jq1RPfB0", "signatures": ["ICLR.cc/2026/Conference/Submission14801/Reviewer_8dsz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14801/Reviewer_8dsz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680406983, "cdate": 1761680406983, "tmdate": 1762925152489, "mdate": 1762925152489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenges of post-hoc explainer misalignment and the inefficiency of \nlocal-level-only explanations in GNNs . The authors propose GCSGNN, a self-explainable GNN \nthat jointly learns to predict and explain its own predictions . Its main contribution is a framework\nfor learning global counterfactual explanations by identifying and applying shared edit rules \n(CGEEs) directly in the graph embedding space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper combines the GNN and its explainer into a single model that trains together. Unlike post-hoc methods, it helps capture the model's dynamics better and provides predictions and explanations simultaneously.\n- The model is designed to find global explanations that apply to multiple graphs in the dataset, one graph at a time, like the local methods.\n- The experiments demonstrate that the model achieves significant time and performance improvements over the baselines"}, "weaknesses": {"value": "- Limitations of Binary Classification: The framework is specifically designed for binary \nclassification. The counterfactual loss only maximizes the probability of a single, fixed class 1. This design limits the method's applicability, and the paper makes no attempt to discuss its use for multi-class classification scenarios.\n- Lack of proximity optimization: The paper's methodology is disconnected from the main goal of GCE, which is to find \"minimal \nmodifications\". The final objective function contains no loss term that explicitly minimizes the \ngraph edit distance or penalizes large counterfactuals. The model is only trained to find a valid \nlabel change, not a minimal one, which ignores a core principle of GCE.\n- Ambiguous formulation: The paper doesn’t provide a precise mathematical definition for \"proximity”. It is vaguely \ndescribed as a \"squared sum\" on one-hot vectors 2, but this omits the full details of a proper \nGED calculation, such as costs for insertions and deletions. This affects the reproducibility of the method.\n- Baseline Comparisons: The experimental results in Table 1 show that most baselines perform at 0.00 coverage, time \nout (TOO), or are n/a in proximity. This is highly suspicious and suggests an unfair experimental\nsetup. The baselines were significantly modified from their original (e.g., factual) purpose, and \nall post-hoc methods are unfairly evaluated on explaining the GCSGNN model itself. This \ncomparison makes the claimed superiority of GCSGNN unreliable.\n- Unclear ablation study (Figure 4): The ablation study in Figure 4 presents large improvement percentages (e.g., +2785.0%). This \nis because the \"ablation\" is not a meaningful removal of a component, but rather a comparison \nagainst a broken model where components, such as the encoder or generator, are fixed at their \nrandom initialization. This only proves that a trained component is better than a random one, \nwhich is a uninformative and trivial claim.\n- Unclear theoretical analysis: The entire proof relies on a critical assumption: that their \"proxy method\" of editing the \nembedding space is a valid approximation for the true, complex problem of editing the graph's \nstructure. This strong assumption isn’t supported, which affects the whole analysis considerably."}, "questions": {"value": "- Although the authors provide the source code, the details of the experimental setups \nare not clear enough. For example, what is the base model that the post-hoc methods \noptimize? Does it constitute the GCSGNN model’s encoder and predictor? In that case, \nthe comparisons are unfair. How do authors set hyperparameters for the baselines?\n- The paper claims that global methods are better than local explainer, as they provide \nexplanations for multiple graphs of a dataset and offer a general insight. However, these \ntwo approaches address different problems, as local methods can provide fine-grained \nand sample-specific explanations, which are more useful in many cases. Are there any \nother points to consider when acknowledging global methods over local ones? \nAdditionally, the paper claims that counterfactual methods are superior to factual \nmethods, but they do not provide sufficient evidence to support this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N3ryhLci69", "forum": "q8jq1RPfB0", "replyto": "q8jq1RPfB0", "signatures": ["ICLR.cc/2026/Conference/Submission14801/Reviewer_PcXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14801/Reviewer_PcXW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947337302, "cdate": 1761947337302, "tmdate": 1762925151983, "mdate": 1762925151983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GCSGNN, a self-explainable graph neural network that jointly learns prediction and counterfactual explanation. Instead of finding instance-specific structural edits, GCSGNN learns a small set of global counterfactual graph embedding edits (CGEEs)—shared latent transformations that can flip the prediction for many graphs simultaneously. The model consists of four jointly trained modules (encoder, counterfactual generator, decoder, and predictor) and optimizes classification, counterfactual, and reconstruction losses. Experiments on molecular and image-graph datasets show that GCSGNN achieves higher counterfactual coverage and lower generation time than post-hoc counterfactual baselines such as CF-GNNExplainer and GCFExplainer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and formulation. The paper clearly articulates the limitations of existing post-hoc counterfactual GNN explainers and motivates the need for global, self-explainable counterfactual reasoning.\n\n- Novel conceptual idea. Learning shared counterfactual edit templates in latent embedding space is an original and elegant approach to discover global reasoning patterns.\n\n- End-to-end design. The unified architecture jointly trains the explainer and predictor, avoiding costly post-hoc optimization and enabling fast inference.\n\n- Strong empirical results. The method consistently outperforms prior counterfactual explainers in terms of coverage and efficiency on multiple datasets."}, "weaknesses": {"value": "- Limited connection to global explanation literature. The paper primarily compares to post-hoc counterfactual explainers but does not discuss related model-level explanation methods such as XGNN and GNNInterpreter, which also aim to extract global reasoning patterns. Positioning GCSGNN relative to these works would clarify its contribution to the broader explainability landscape.\n\n- Interpretability of latent edits. Counterfactuals are generated by manipulating embedding channels, yet the semantic meaning of these edits in terms of node or edge structure remains unclear. More examples or visualizations are needed to demonstrate that CGEEs correspond to meaningful graph modifications.\n\n- Lack of discussion on decision-boundary understanding. Although counterfactual generation implicitly explores the decision boundary, the paper does not analyze or visualize how embedding edits relate to the classifier’s boundary. A clearer discussion of boundary behavior, potentially referencing works like GNNBoundary, would strengthen the conceptual grounding.\n\n- Limited metric diversity. Evaluation is largely restricted to coverage and proximity; including metrics such as fidelity, sparsity, or diversity would provide a more comprehensive assessment of interpretability."}, "questions": {"value": "- Can the authors provide more concrete examples or visualizations to show how specific CGEEs correspond to meaningful node- or edge-level changes in the graph?\n- Since counterfactual generation inherently explores boundary regions, can the authors analyze how the learned edits interact with or traverse the decision boundary of the classifier?\n- Would additional interpretability metrics, such as fidelity, sparsity, or diversity of CGEEs, yield more nuanced insights into the model’s explanations?\n- How sensitive are the results to the number and dimension of CGEEs (k, dₛ)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1qHkeZHIz2", "forum": "q8jq1RPfB0", "replyto": "q8jq1RPfB0", "signatures": ["ICLR.cc/2026/Conference/Submission14801/Reviewer_prPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14801/Reviewer_prPM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989929655, "cdate": 1761989929655, "tmdate": 1762925151553, "mdate": 1762925151553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a method for the generation of global graph counterfactuals (GCSGNN) based on as opposed to the most often occurring instance-level/local counterfactual explanations. A major selling point of the paper is, that the the method is self-explainable, that is, one does not need to carry out post-hoc analysis to obtain model explanations. The method is evaluated on a collection of common bench-mark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The modelling problem of finding global counterfactual explanations is very interesting, and an area still under development. The general idea of the paper is well-conveyed, and the code is made publicly available already at this time. The proposed method shows good performance compared to baselines on the results reported by the authors."}, "weaknesses": {"value": "- I am slightly confused about whether the scope of the paper is only to generate global or also local/instance level counterfactual explanations? The ability to generate global counterfactual explanations is highlighted as a main contribution, but at the same time, the ability of the model to generate local GVE's is highlighted (e.g. line 133-134).\n- The theoretical analysis in section 3 amounts to pointing out, that the GCSGNN can achieve a superior mutual information between the label and the predicted factual/counterfactual since optimization occurs over a larger parameter space. This is frankly not surprising. The analysis carried out is centered around a post-hoc counterfactual generation interpretation of GSCGNN, and does not give a clear argument in favor of self-explainable methods. Besides, the authors do not consider whether the proposed joint optimization procedure can have a negative impact on the classification performance. Lastly the analysis I recommend that the authors reconsider the role of this section, and in particular tighten the mathematical rigor. \n- The authors do not at all consider the fact that the same graph can have different representations (i.e. be the same up to isomorphism). \n- Experiments and evaluation: As I understand it counterfactuals are the decoded graphs of the counterfactual embeddings; I find this slightly misleading as an encoding of the generated counterfactual is not necessarily going to the same as the counterfactual embedding. This can have potentially large consequences for the evaluation of the model in terms of coverage depending on whether the validity of a counterfactual is computed with respect to the counterfactual embedding or the encoding of the decoded counterfactual."}, "questions": {"value": "- \"Explainer Misalignment\" is considered a major challenge for GNNs. But is this really the case? One could argue that training a post hoc explainer is preferable, as one would then be able to generate explanations in cases where the training procedure of the model we wish to explain is not known or not controlled.\n- Line 110: Is the encoder permutation invariant?\n- Line 125-127: Which training objective? Please be explicit if possible.\n- The lines 87-92 and lines 127-131 are almost exactly the same. I suggest that the authors make this more concise to minimize redundancy.\n- Line 131: How are the global counterfactuals obtained from the counterfactual subembeddings? In my understanding the subembeddings are not used to obtain a global GCEs, but are rather combined with the embeddings of a specific input graph to produce an instance level, local GCE.\n- Typo in equation one: \"$\\textbraceleft t \\ldots \\textbraceright \\text{ for some s} \\in  \\mathcal{S}$\" should be \"$\\textbraceleft \\ldots \\text{ for some s } \\in  \\mathcal{S}\\textbraceright$\".\n- On line 110-111 the method is said to consist of the models $f_p$ and $f_e$, however, at line 171-172 the method additionally has the element $f_c$ denoting the counterfactual explainer. Later in section 4.1 \"Model overview\" a decoder $f_d$ is introduced. I urge the authors to be consistent in the model setup.\n- Line 179-180: Is this assumption reasonable? And how does it impact the analysis if it is not?\n- Line 182-183: It is stated that \"GCSGNN aims to find the set of parameters that maximizes both the mutual information between the label and f\". Please elaborate on this connection between the mutual information and the training objective as it is not evident from the text.\n- Table 1: Many of the methods reported perform extremely poorly (e.g. 0.00 coverage on the Aids dataset). Why do they perform so poorly?\n- Can you provide samples and illustrations of the global counterfactuals produced, and the graphs which are sampled from the models? I do not see any samples reported in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gMpoH6ExwL", "forum": "q8jq1RPfB0", "replyto": "q8jq1RPfB0", "signatures": ["ICLR.cc/2026/Conference/Submission14801/Reviewer_UeZp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14801/Reviewer_UeZp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173978660, "cdate": 1762173978660, "tmdate": 1762925151127, "mdate": 1762925151127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}