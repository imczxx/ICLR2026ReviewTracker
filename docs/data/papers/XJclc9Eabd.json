{"id": "XJclc9Eabd", "number": 15019, "cdate": 1758246929152, "mdate": 1759897335072, "content": {"title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations", "abstract": "Recent progress in large-scale robotic datasets and vision-language models (VLMs) has advanced research on vision-language-action (VLA) models.  However, existing VLA models still face two fundamental challenges: (\\textit{i}) producing precise low-level actions from high-dimensional observations, (\\textit{ii}) bridging domain gaps across heterogeneous data sources, including diverse robot embodiments and human demonstrations. Existing methods often encode latent variables from either visual dynamics or robotic actions to guide policy learning, but they fail to fully exploit the complementary multi-modal knowledge present in large-scale, heterogeneous datasets. In this work, we present \\textbf{XR-1}, a novel framework for versatile and scalable VLA learning across diverse robots, tasks, and environments.\nAt its core, XR-1 introduces the \\emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and robotic motion.  UVMC addresses these challenges by (\\textit{i}) serving as an intermediate representation between the observations and actions, and  (\\textit{ii}) aligning multimodal dynamic information from heterogeneous data sources to capture complementary knowledge. To effectively exploit UVMC, we propose a \\emph{three-stage training paradigm}: (\\textit{i}) self-supervised UVMC learning, (\\textit{ii}) UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and (\\textit{iii}) task-specific post-training.  We validate XR-1 through extensive real-world experiments with more than 12,000 rollouts on six different robot embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently outperforms state-of-the-art baselines such as $\\pi_0$ and GR00T-N1.5 while demonstrating strong generalization to novel objects, background variations, distractors, and illumination changes. Our project is at \\href{https://xr-1-vla.github.io/}{https://xr-1-vla.github.io/}.", "tldr": "XR-1 introduces Unified Vision-Motion Codes to train VLA models that outperform SOTA across 120 tasks on six distinct robot embodiments.", "keywords": ["Vision-Language-Action Model", "Robot Learning", "Imitation Learning", "Latent Representation", "Large-Scale Pretraining"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2699bcf8fd14fd3edb27d14030621c66c1ebda41.pdf", "supplementary_material": "/attachment/40179db0e65493c68c8952e8dfd529759dc64104.zip"}, "replies": [{"content": {"summary": {"value": "To bridge gaps across heterogeneous data sources, this paper proposes a novel framework, XR-1, for versatile and scalable VLA learning. The core of XR-1 is the Unified Vision-Motion Codes (UVMC) which jointly encodes visual dynamics and robotic motion via a dual-branch VQ-VAE. Through UVMC, the vision dynamic information and motion action are aligned and can be serving as an intermediate representation. Based on UVMC, a three-stage training paradigm is proposed: (1) self-supervised UVMC learning, (2) UVMC-based pretaining on large-scale cross-embodiment robotic datasets, and (3) task-specific post-training. Extensive experiments verify the superior performance over SoTAs such pi-0. Ablation study is also well designed to show the effectiveness of the proposed training paradigm."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper is well-presented and easy to follow. The motivations are clear and insightful, and the methodology is straightforward to understand.\n\n2. The idea is both novel and interesting. Achieving generalization across different robots and learning from heterogeneous data is an important challenge. The unified representation method that aligns visual dynamics with action execution based on UVMC is an inspiring approach to address this issue. While similar training paradigms can be found in other computer vision tasks, such as research on text-to-human motion, this represents the first application of such a paradigm in robot action learning, with adaptations made to fit the robotics domain.\n\n3. Comprehensive experiments effectively demonstrate the superior performance of the proposed method, and the ablation studies validate the effectiveness of the designed components and the training paradigm."}, "weaknesses": {"value": "1. Generally, self-supervised training on VQ-VAE is a crucial stage, and the learning of the codebook significantly impacts performance. However, this paper appears to lack discussion on this aspect, and certain details, such as the size of the codebook, remain unspecified, which may affect reproducibility.\n\n2. In the proposed method, human demonstration videos (Ego4D) are utilized during the pretraining stage, showcasing its capability to leverage heterogeneous data sources and cross-embodiment. This is a significant strength. However, the impact of using human source data has not been analyzed.\n\n3. Typos, for instance,  $z_{mo}^e$ after \"visual dynamics codes\" should be \"$z_{vis}^e$\" in line 229."}, "questions": {"value": "1. In stage 2 and 3, how are actions decoded? Using the pretrained VQ-VAE decoder or a new action head?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xpcavX4ljk", "forum": "XJclc9Eabd", "replyto": "XJclc9Eabd", "signatures": ["ICLR.cc/2026/Conference/Submission15019/Reviewer_gGq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15019/Reviewer_gGq7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727896552, "cdate": 1761727896552, "tmdate": 1762925348741, "mdate": 1762925348741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces XR-1, a framework for building generalist Vision-Language-Action (VLA) robots. Its core contribution is the Unified Vision-Motion Codes (UVMC), a discrete latent representation that jointly encodes visual dynamics and robotic actions into a shared space via a dual-branch VQ-VAE. Through a three-stage training paradigm, XR-1 leverages this unified representation to enable precise low-level control and strong cross-embodiment generalization, demonstrably outperforming state-of-the-art models across 120+ tasks on six different robot embodiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The technical quality and empirical rigor are exceptional. The paper is grounded by an immense evaluation campaign. \n2. The consistent and significant outperformance of strong, recent baselines (e.g., $\\pi_{0}$, GR00T-N1.5) across diverse scenarios (bimanual, dexterous, long-horizon) provides compelling evidence for the framework's effectiveness."}, "weaknesses": {"value": "1. For a fairer assessment of UVMC's value, the authors should include baselines that use alternative representation learning paradigms. For instance, a comparison with methods that use only visual dynamics latents or only action latents would directly isolate the benefit of the unified vision-motion approach. \n2. The paper treats the UVMC as a \"black box.\" While the results demonstrate its effectiveness, it lacks a qualitative or quantitative analysis of what the model learns in this unified space. The authors may consider the following: (1) Perform a nearest-neighbor analysis in the codebook space. For example, show that the motion code for \"grasp\" is close to the visual code for a hand closing around an object, even across different embodiments; (2) Visualize the latent space with t-SNE/UMAP plots, coloring points by task semantics (e.g., \"picking,\" \"placing\") versus embodiment-specific details, to empirically demonstrate the embodiment-agnostic and semantically structured nature of UVMC.\n3. The contribution of the large-scale human video data (Ego4D) is unclear and not quantitatively ablated. The paper states human data uses only $L_{vis}$ , but it's ambiguous how these *actionless* videos ultimately improve low-level robot control.\n4. The paper extensively reports success rates but provides little discussion of *when and why XR-1 fails*. Understanding the limitations is crucial for future research."}, "questions": {"value": "All my questions are highly related to the weaknesses.\n\n1.  To isolate UVMC's value, could you include baselines that use only visual or only action latents, demonstrating the specific benefit of their unification?\n\n2.  Could you provide an analysis (e.g., nearest-neighbor retrieval or latent space visualization) to show that UVMC learns semantically meaningful, embodiment-agnostic clusters?\n\n3.  What is the quantitative impact of the human video data (Ego4D) on final robot performance? An ablation study removing it from Stage-1 would clarify its contribution.\n\n4.  Could you discuss the primary failure modes of XR-1? In which task types does it consistently struggle, and what is the typical root cause (perception, UVMC prediction, or control)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kx88d7tMjq", "forum": "XJclc9Eabd", "replyto": "XJclc9Eabd", "signatures": ["ICLR.cc/2026/Conference/Submission15019/Reviewer_vfX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15019/Reviewer_vfX7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905783497, "cdate": 1761905783497, "tmdate": 1762925348358, "mdate": 1762925348358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents XR-1, a unified vision-language-action framework that introduces the Unified Vision-Motion Codes (UVMC) to jointly quantize and align visual dynamics with motor actions. The authors conduct extensive real-world experiments with over 12,000 rollouts across six different robot embodiments, demonstrating strong cross-task and cross-embodiment generalization."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents extensive real-world experiments, validating XR-1 with over 12,000 rollouts across six different robot embodiments.\n- This paper presents XR-1, a unified vision-language-action framework that introduces the Unified Vision-Motion Codes (UVMC) to jointly quantize and align visual dynamics with motor actions.\nThe authors conduct extensive real-world experiments with over 12,000 rollouts across six different robot embodiments, demonstrating strong cross-task and cross-embodiment generalization.The proposed method is novel, introducing the Unified Vision-Motion Codes (UVMC) that jointly quantize and encode both actions and observations."}, "weaknesses": {"value": "- The paper does not appear to include simulation experiments. Although the real-world evaluation is rich and convincing, an evaluation on a standard public benchmark would provide a more comprehensive assessment of performance and facilitate reproducibility for the research community. *I strongly suggest author take this into consideration*.\n- Analytical justification of UVMC: The core innovation UVMC lacks deeper justification beyond ablation studies. The observed improvements could potentially be influenced by hyperparameter choices or implementation factors. I recommend adding more analytical or theoretical explanations to strengthen the evidence for UVMCâ€™s effectiveness.\n- Interpretability of UVMC: The interpretability of the UVMC alignment mechanism could also be improved. Although the KL alignment encourages visual and motion features to share the same codebook, the paper does not analyze how these codes are organized or clustered in semantic space. For example, which codes may correspond to grasping, rotation, or translation. As a result, the semantic consistency of UVMC remains an assumption rather than an explicitly demonstrated property."}, "questions": {"value": "- I am somewhat unclear about the UVMC design. During reconstruction, both observation and action encoders take additional inputs such as c_t and l, o. Could the authors provide more explanation on the rationale behind this design?\n- The latent variables are defined as $z_{\\text{vis}} = E_{\\text{vis}}(c_t, c_{t+h})$ and $z_{\\text{mo}} = E_{\\text{mo}}(a_{t:t+h}, m_{t:t+h}).$\nThis paper propose to let the latent representation be as close as possible. However, therotically, the same $(a_{t:t+h}, m_{t:t+h})$ can correspond to very different $(c_t, c_{t+h})$, since the action sequence and motor states only describe the robot itself, while the visual inputs capture the entire scene, including background and objects, which can be changed arbitrarily across episodes. Directly aligning these two spaces might only be reasonable under specific dataset distributions. Could the authors clarify this assumption or provide further justification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OmtqFyoQwn", "forum": "XJclc9Eabd", "replyto": "XJclc9Eabd", "signatures": ["ICLR.cc/2026/Conference/Submission15019/Reviewer_PrUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15019/Reviewer_PrUm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905863026, "cdate": 1761905863026, "tmdate": 1762925347738, "mdate": 1762925347738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes XR-1, a three-stage training approach for vision-language-action models (VLAs) that leverages heterogeneous data sources from both Internet-scale human videos and a mixture of different robot datasets to produce a new VLA that outperforms prior VLAs, such as $\\pi_0$, RDT, UniVLA, and GR00T-N1.5. The authors also introduce Unified Vision-Motion Codes (UVMC), a novel dual-branch latent representation that jointly encodes visual information and motion information into discrete codes in a shared VQ-VAE cookbook. The three training stages consist of self-supervised learning of UVMC across both human videos from the Internet and diverse robot datasets, cross-embodiment UVMC pretraining that injects the learned representations into the policy's VLM backbone through learnable input tokens, and platform-specific post-training for downstream tasks. Extensive experimental evaluations across 6 robot embodiments, 120 tasks, and 120K rollouts demonstrate XR-1's superior performance over prior VLAs such as $\\pi_0$, UniVLA, RDT, and GR00T-N1.5."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is well-written and easy to follow.\n* The experiments assess policies over an extensive set of embodiments, tasks, and trials in the real world (6 robot embodiments, 120 tasks, 120K trials). The proposed XR-1 policy outperforms the next best prior method by a significant margin (e.g. +30% in absolute success rate over $\\pi_0$ in both the Dual-Arm UR-5e and Tien Kung 2.0 task suites).\n* The authors propose Unified Vision-Motion Codes (UVMC), a technically novel VQ-VAE-based latent representation that encodes both visual information and robot motion information in a joint latent space.\n* XR-1's strong performance holds across various tasks and generalization settings, validating its efficacy and supporting the paper's claims.\n* The paper presents extensive details on implementation, training data, additional experiments, and evaluations, with plenty of details in the appendix. The authors have clearly put significant effort into providing details that may be helpful for understanding the paper better and reproducing the method and experiments."}, "weaknesses": {"value": "* A major weakness motivating my current recommendation for this paper is that comparing XR-1 to prior methods such as $\\pi_0$, RDT, UniVLA, and GR00T-N1.5 in Sections 4.2 and 4.3 does not seem to be a fair comparison given that the former benefits from pretraining on a superset of tasks in the XR-D dataset that are similar to the embodiment-specific tasks that all the methods are post-trained and evaluated on (or provides support for the robot embodiments that are evaluated on after post-training, such as Dual-Arm UR-5e, which appears in both XR-D pretraining and in post-training). Perhaps a more fair direct comparison with the baseline methods would be a variant of XR-1 that is fine-tuned on task-specific datasets without XR-D pretraining. This would enable a more fair analysis of whether the XR-1 training recipe and UVMC representation are beneficial, since there is potentially a confounding factor in the performance improvement that comes from additional supervision from pretraining data collected on embodiments similar to the ones used for post-training evaluations. Currently, it is not clear whether the proposed training recipe or the XR-D dataset pretraining is contributing more to the performance. Further discussion on this or clarifications on the differences between train and test would be critical for adjusting my recommendation for this submission (I am willing to adjust my score since I believe there are several positive aspects to this paper otherwise).\n* In the abstract, the authors write, \"Existing methods often encode latent variables from either visual dynamics or robotic actions to guide policy learning, but they fail to fully exploit the complementary multi-modal knowledge present in large-scale, heterogeneous datasets.\" Similarly, in the related works, the authors write, \"However, since the pre-training involves lagre-scale heterogeneous robotics and human data, existing VLA models remain limited ability to achieve effective unification across the heterogeneous modalities.\" These are fairly broad and abstract claims, and it is not clear what they mean and whether there is empirical validation for such claims. Clarification on the meaning behind these statements and why they are true (through supporting evidence) would strengthen the paper. For example, $\\pi_0$ and OpenVLA have shown multi-task multi-embodiment control ability as well as adaptability to new robot embodiments through post-training, and it is not clear why XR-1 would be unique in these respects.\n* In Section 4.2, the authors write, \"Several baselines even collapse to 0% performance on harder tasks, which we attribute to insufficient auxiliary supervision and gradient conflicts during multi-task optimization.\" More details on this finding would strengthen the paper. Are prior methods struggling to fit the multi-task training dataset, or is there some other issue causing relatively poor performance compared to XR-1? Details on how baseline methods are trained and qualitative descriptions of how they fail would be helpful to aid understanding.\n* There is no discussion or experiments on having two separate cookbooks for visual representations and motion representations. An ablation experiment showing that a unified cookbook is important would strengthen the paper."}, "questions": {"value": "* In the \"Lightweight Models\" discussion in Section 4.4, in Table 4 rows (1) and (2), the difference between these two experiments is not clear. What is \"FT.\" here and why does this correspond to \"incorporating UVMC into Stage-3 training\" as described in the main text? Perhaps some additional description in the table caption or main text can be helpful to the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "whmNag1BBb", "forum": "XJclc9Eabd", "replyto": "XJclc9Eabd", "signatures": ["ICLR.cc/2026/Conference/Submission15019/Reviewer_H8iB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15019/Reviewer_H8iB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981940212, "cdate": 1761981940212, "tmdate": 1762925347309, "mdate": 1762925347309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}