{"id": "7FZFkJKD9f", "number": 4588, "cdate": 1757716167251, "mdate": 1759898024466, "content": {"title": "Stochastic Interpolants via Conditional Dependent Coupling", "abstract": "Existing image generation models face critical challenges regarding the trade-off between computation and fidelity. Specifically, models relying on a pretrained Variational Autoencoder (VAE) suffer from information loss, limited detail, and the inability to support end-to-end training. In contrast, models operating directly in the pixel space incur prohibitive computational cost. Although cascade models can mitigate computational cost, stage-wise separation prevents effective end-to-end optimization, hampers knowledge sharing, and often results in inaccurate distribution learning within each stage. To address these challenges, we introduce a unified multistage generative framework based on our proposed \\textbf{Conditional Dependent Coupling} strategy. It decomposes the generative process into interpolant trajectories at multiple stages, ensuring accurate distribution learning while enabling end-to-end optimization. Importantly, the entire process is modeled as a single unified Diffusion Transformer, eliminating the need for disjoint modules and also enabling knowledge sharing. Extensive experiments demonstrate that our method achieves both high fidelity and efficiency across multiple resolutions.", "tldr": "", "keywords": ["Generative Model", "Couplings", "Few step generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf859d593ebf8d76e187f37bd7a0805b92f49ac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Flow Matching with Conditional Dependent Coupling, a framework for image generation that operates directly in the pixel space. Instead of relying on a latent representation, the method models low-resolution images as being drawn from a Gaussian noise source and learns the transport process from low-resolution to high-resolution images. All stages are unified within a single DiT backbone and a resolution embedding as the condition for different stages. Compared to conventional pixel-space generation, Conditional Dependent Coupling significantly reduces transport cost and inference time. Experiments on ImageNet-1K at 256×256 and 512×512 resolutions demonstrate competitive FID and IS scores compared with state-of-the-art approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors unify low-resolution generation and high-resolution upsampling within a single DiT backbone. Compared to previous multi-stage approaches, this design simplifies the overall framework and reduces engineering overhead.\n2. The authors provide strong theoretical support for the effectiveness of conditional dependent coupling. In particular, they formally demonstrate that the proposed method reduces transport cost and inference computation cost, as established in Theorem 2 and Theorem 3.\n3. The experimental results are competitive with other state-of-the-art methods operating directly in pixel space."}, "weaknesses": {"value": "1. The paper does not provide sufficient information regarding the training cost, including hardware specifications, total training epochs, and computational resources. Although Table 3 in the Appendix includes some configuration details, it appears mislabeled as PixelFlow, suggesting potential inconsistencies. Moreover, the reported training duration is only 10 epochs, which is unusually short for this line of work, where diffusion-based methods typically require 400–1600 epochs on ImageNet. If accurate, this represents an approximately 40× to 160x reduction in training cost, which would be a remarkable advantage and thus requires clearer justification and verification.\n\n2. The paper does not include ablation studies that quantify the contribution of key design components. While the model comprises K flow-matching stages, the authors only report inference cost per stage, without analyzing how each stage affects the final image quality. Additionally, there are no visualizations of intermediate outputs across stages, which would provide valuable insights into the effectiveness of the multi-stage transport process. A direct comparison between single-stage (direct pixel modeling) and multi-stage settings is also missing—this experiment would serve as the most direct empirical evidence supporting the core claim that the proposed coupling design, rather than the training recipe or backbone choice, drives the improvement.\n\n3. Although inference speed is presented as one of the main contributions, the paper lacks a clear description of how “Speed” in Table 1 is defined or measured. Furthermore, comparisons with stronger recent baselines such as REPA (FID = 1.42) [1] and LightingDiT (FID = 1.35) [2] are missing, even though these methods achieve superior performance with comparable model sizes. Including such baselines would provide a more convincing demonstration of the proposed method’s practical advantage.\n\n[1] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think, ICLR 2025.\n[2] Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models, CVPR 2025."}, "questions": {"value": "1. Could you clarify the actual training cost of the proposed methods including hardware specifications, total wall-clock time and number of training epochs for reproducibility?\n2. Could you provide more ablation studies including the proposed improvement over previous one-stage pixel-wise methods or latent space-based methods?\n3. How exactly is the 'Speed' metric in Table 1 defined and measured (Is it seconds per image)? Is the Speed in Table 1 evaluated under the same configuration (batch size, GPU type)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q0QHxkvpk0", "forum": "7FZFkJKD9f", "replyto": "7FZFkJKD9f", "signatures": ["ICLR.cc/2026/Conference/Submission4588/Reviewer_Kt39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4588/Reviewer_Kt39"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538043152, "cdate": 1761538043152, "tmdate": 1762917457560, "mdate": 1762917457560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel multi-stage generative framework based on stochastic interpolants, using a conditional dependent coupling strategy to address the computational efficiency-high-fidelity generation trade-off by structuring the process as a coarse-to-fine sequence of stages. Each stage uses an interpolant trajectory that transforms a structured prior, which is derived from the upsampled output of the preceding stage augmented with noise, into the target distribution. The framework ensures end-to-end optimization through a unified DiT model, enabling cross-resolution knowledge sharing while minimizing transport costs and inference time. Extensive experiments demonstrate that this approach achieves comparable or superior performance to existing generative models across multiple metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed solution, a unified multi-stage framework based on conditional dependent coupling, and coupled with a single DiT model, offers a solution that enables address the limitations of disjointed multi-stage approaches, while facilitating knowledge sharing across resolution stages and end-to-end optimization.\n\n2. Extensive experiments to compare various methods are provided, demonstrating highly competitive generation performance."}, "weaknesses": {"value": "1. The relationship and distinction between the proposed \"conditional dependent coupling\" and \"data-dependent coupling\" in the stochastic interpolant literature needs further clarification. A clearer discussion on how the work extends prior theory would help in precisely situating its theoretical contribution. Furthermore, the current theoretical presentation is highly technical, which may cause too much understanding burdens for a broader audience.\n\n2. The depth of the experimental discussion is limited, maybe due to limited space, which has been taken by theoretical presentation. Since the experiments involve comparisons with numerous baselines across several metrics, the results and their implications are not discussed with sufficient granularity."}, "questions": {"value": "My questions have been stated in the detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IujZyXAYn7", "forum": "7FZFkJKD9f", "replyto": "7FZFkJKD9f", "signatures": ["ICLR.cc/2026/Conference/Submission4588/Reviewer_Wnzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4588/Reviewer_Wnzi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904086225, "cdate": 1761904086225, "tmdate": 1762917457267, "mdate": 1762917457267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the trade-off between computation and fidelity in existing image generation models (e.g., VAE-based latent space models suffer from information loss, pixel-space models are computationally expensive, and cascade models lack end-to-end optimization). It proposes **a unified multi-stage generative framework based on stochastic interpolants and a novel Conditional Dependent Coupling (CDC) strategy**. The framework decomposes high-resolution image generation into coarse-to-fine interpolant trajectories across K stages, all parameterized by a single Diffusion Transformer (DiT) to enable end-to-end training and knowledge sharing. Formal proofs (Theorem 2, Theorem 3) demonstrate that the method reduces transport cost and inference time compared to single-stage models. Extensive experiments on ImageNet-1k (256×256 and 512×512 resolutions) show state-of-the-art (SOTA) performance across metrics like FID (1.55–1.97 on $256\\times 256$, 2.65 on $512\\times512$), Inception Score (IS, up to 307.4), and inference efficiency (linear scaling with image size, Figure 2a). The source code is promised to be public upon acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Conditional Dependent Coupling for Accurate Distribution Learning**\n    1. The CDC strategy ensures each stage k learns the accurate target distribution, unlike baseline multi-stage methods that target noisy, semantically vague distributions. \n    2. The joint distribution formulation establishes conditional independence across stages, enabling a Markov chain inference process that avoids the computational complexity of autoregressive long-history conditions.\n    3. CDC reduces transport cost by leveraging structured priors (upsampled low-resolution images with noise) instead of unstructured Gaussian noise.\n2. **Unified DiT Design Enabling Efficiency and Knowledge Sharing**\n    1. A single DiT parameterizes all stages, eliminating disjoint modules and enabling end-to-end training.\n    2. Resolution embeddings (sinusoidal encoding of feature map resolution) enable the DiT to adapt to different stages without retraining.\n    3. The unified design simplifies CFG integration, as a single CFG strength $S_\\text{cfg}$ works across all stages.\n3. **Comprehensive Empirical Validation Across Metrics and Resolutions**\n    1. CDC-FM shows competitive performance of 30+ baselines on ImageNet-1k/256, with FID=1.81 (comparable to SiD2’s 1.72) and faster inference (Speed=1.31 vs. MAR-H’s 1.23). This demonstrates competitive fidelity and efficiency.\n    3. Robust performance at 512×512 resolution, showing scalability to high resolutions.\n4. **Theoretical Rigor with Formal Proofs**"}, "weaknesses": {"value": "1. **Incomplete Theoretical Analysis of Key Mechanisms**\n   1. The paper claims the unified DiT enables \"knowledge sharing\" across stages, but provides no formal analysis of how resolution embeddings $e_k$ facilitate this (e.g., gradient flow across stages, parameter reuse statistics). No direct evidence found in the manuscript. I can only find qualitative claims in Sec. 3.2. This limits understanding of why the unified design outperforms stage-specific models.\n   2. Ambiguities in coupling definition and Markov structure. The joint distribution in Eq. (14) conditions all stages on $x^{(k)}$, yet inference uses a Markov chain over stage outputs (Eq. 16), which appears inconsistent unless justified by conditional independence (Eq. 15). However, the transition from full conditioning to Markovian generation lacks explicit derivation. In Eq. (17), $\\rho^{(k)}_0(\\hat{x}^{(k)} | \\hat{x}^{(k-1)}_1)$ is defined using $U_k(\\hat{x}^{(k-1)}_1))$, but $\\hat{x}^{(k-1)}_1$ is ambiguous. Should it be $\\hat{x}^{(k-1)}_1$ (the output of stage $k−1$)? This notation mismatch could confuse implementation. Also, the deterministic mapping $m_k (x^k_1) = U_k(D_k(x^k_1))$ (Eq. 9) assumes invertibility of downsampling, but neighborhood averaging ($D_k$) is not invertible; the impact of this approximation on coupling fidelity is not discussed.\n   3. The diminish factor $\\gamma$ (Eq. 20) is critical for balancing diversity and efficiency, but the paper does not explain why $\\gamma$=2.0 is optimal (Figure 2b shows FID dips at $\\gamma$=2.0 then rises) or how $\\gamma$ interacts with other hyperparameters (e.g., noise scale $\\sigma$). This impacts practical guidance for users.\n2. **Limited Experimental Ablations**\n    1. The paper lacks ablations of the unified DiT vs. stage-specific DiTs. Without this comparison, it is unclear whether the unified design truly improves knowledge sharing or if performance gains come from CDC alone.\n    2. The choice of linear interpolant ($\\alpha_t=1−\\tau, \\beta_t=\\tau$) is assumed without comparison to alternative interpolants (e.g., cosine, linear with warmup) that may affect trajectory smoothness (Eq. 12; Sec. 3.2; p.4). This limits understanding of sensitivity to path design.\n    3. The diminish factor $\\gamma$ controls noise decay ($\\sigma_k = \\gamma^{-(k-1)\\sigma}$; Eq. 20), but only $\\sigma \\in [1.0, 5.0]$ is tested (Fig. 2b); no analysis of its interaction with stage count K or resolution hierarchy is provided.\n    4. Resolution embedding via sinusoidal encoding (Sec. 3.2) is described briefly; its necessity versus simpler alternatives (e.g., learned embeddings, no embedding) is not verified.\n    5. Qualitative results (Figure 3, Appendix B) show class-conditional generation but lack side-by-side comparisons with baselines (e.g., DiT-XL/2, PaGoDA, FractalMAR) to visually validate \"high fidelity\" claims.\n3. **Reproducibility and Practical Guidance Gaps**\n    1. While Appendix G provides implementation details (Table 3), it does not report memory footprints or training wall-clock time per model/depth. This limits practical adoption and reproductivity of this paper.\n    2. The CFG integration (Sec. 3.4) uses a single $S_\\text{cfg}$ across stages, but the paper does not test how $S_\\text{cfg}$ impacts different stages (e.g., whether early stages benefit from higher $S_\\text{cfg}$ for diversity). This limits understanding of CFG’s role in the multi-stage framework.\n    3. No analysis of failure cases (e.g., mode collapse, structural artifacts in high-frequency regions) is included."}, "questions": {"value": "1. **What evidence confirms that resolution embeddings enable knowledge sharing across stages?** The paper claims the unified DiT shares knowledge via resolution embeddings \n$e_k$, but no empirical (e.g., parameter activation heatmaps, gradient similarity across stages) or theoretical (e.g., bound on parameter reuse) evidence is provided. Could you add an ablation comparing the unified DiT to stage-specific DiTs (same total parameters) on ImageNet-1k/256, reporting FID, training time, and memory usage? Additionally, could you clarify how $e_k$ is fused with timestep embedding (e.g., cross-attention layers, concatenation) with a small diagram or equation?\n2. **How does the Markov chain structure (Eq. 16) reduce error accumulation compared to non-Markov multi-stage models?** The paper states the Markov property (stage k depends only on k-1) avoids compounding errors, but no data supports this. Could you report FID per stage (e.g., FID at $k=1, k=2, ..., k=K$) for CDC-FM and a baseline non-Markov multi-stage model (e.g., PixelFlow)? Additionally, could you measure the dependence between stage $k$ and $k-2$ to confirm the Markov property holds in practice?\n3. **What principles guide the choice of diminish factor $\\gamma$ and stage-specific noise scales $\\sigma_k$?** Figure 2b shows $\\gamma$=2.0 optimizes FID, but the paper does not explain why. Could you add an analysis of how $\\gamma$ impacts noise diversity across stages and how $sigma_k$ interacts with $\\gamma$?\n4. **Why CDC-FM only uses 10 epochs for IN-1K-256 while other models use 400-1400 epochs?** Table 3 (Appendix G) shows CDC-FM trains for only 10 epochs on IN-1K-256, while baselines in IN-1K image generation task train for 400-1400 epochs. Could you clarify why *significant* fewer epochs suffice (only 2.5\\% training)? Is it due to faster convergence from CDC or other factors? Also, you state that the per-gpu batch size is 4. Hence, could you report the number of your total batch size and how much memory it consumes during training/inference? \n5. In addition, the name of Table 3 must be a mistake because it is titled 'Complete Parameter Configuration for *PixelFlow*' and Figure 2 is missing because you do not put a main caption of your Figure 2 but only sub-captions (a)-(d).\n\nOverall, the paper makes contributions to multi-stage image generation via CDC and a unified DiT. However, it falls short of a higher score mainly due to sufficient experiments/ablations and mathematical ambiguities in Markov Chain, which makes me feel less confidence about some claims in the paper. Addressing the questions above would strengthen the contribution and soundness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DgaTN01PYA", "forum": "7FZFkJKD9f", "replyto": "7FZFkJKD9f", "signatures": ["ICLR.cc/2026/Conference/Submission4588/Reviewer_L5yh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4588/Reviewer_L5yh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978728370, "cdate": 1761978728370, "tmdate": 1762917456888, "mdate": 1762917456888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}