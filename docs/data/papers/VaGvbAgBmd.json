{"id": "VaGvbAgBmd", "number": 13941, "cdate": 1758225555615, "mdate": 1759897401634, "content": {"title": "Do We Need All the Synthetic Data? Targeted Image Augmentation via Diffusion Models", "abstract": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase the\nsize of the data by up to 10-30x to improve the in-distribution performance. In this\nwork, we show that synthetically augmenting part of the data that is not learned\nearly in training with faithful images—containing same features but different\nnoise—outperforms augmenting the entire dataset. By analyzing a two-layer CNN,\nwe prove that this strategy improves generalization by promoting homogeneity in\nfeature learning speed without amplifying noise. Our extensive experiments show\nthat by augmenting only 30%-40% of the data, our method boosts generalization\nby up to 2.8% in a variety of scenarios, including training ResNet, ViT, ConvNeXt,\nand Swin Transformer on CIFAR-10/100, and TinyImageNet, with various\noptimizers including SGD and SAM. Notably, our method applied with SGD\noutperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet.", "tldr": "We propose a novel targeted image augmentation method using synthetic images from diffusion models", "keywords": ["Synthetic data generation", "Image augmentation", "Diffusion models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc1411702f9ebfa07d5671b756861e335213ff13.pdf", "supplementary_material": "/attachment/0560f8cc7a95b746ec04db2fa55dc9221f7a727e.zip"}, "replies": [{"content": {"summary": {"value": "This author studies diffusion-based synthetic data augmentation for image classification. Unlike prior works that augment the entire dataset (often by generating 10×–30× more images), the authors propose targeted augmentation: identify the subset of training samples whose features are “slow-learnable” and generate synthetic variants only for these."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Focusing diffusion-based augmentation on slow-learnable examples is a fresh take on synthetic augmentation.\n2)  The authors analyze a simplified two-layer CNN to compare SAM vs. SGD (showing SAM learns “noise” more slowly) and prove that generating faithful images accelerates learning of slow features without amplifying noise (Theorems 4.1–4.3).\n3) The experiments are extensive and well-controlled. On CIFAR-10/100 and TinyImageNet, across multiple architectures, the proposed method consistently outperforms baselines (random subset, full augmentation, simple upsampling) and yields up to 2.8% test accuracy\n4) By augmenting only ~30–40% of data, the method substantially reduces synthetic data generation time (e.g. 3.6h vs 12h on CIFAR-10) compared to full-data diffusion."}, "weaknesses": {"value": "1) All experiments are on relatively small benchmarks (CIFAR-10/100, TinyImageNet). It is unclear if the approach scales to large datasets (e.g. full ImageNet) or real-world settings.\n2) Identifying slow-learnable examples via early clustering of model outputs (or high loss) is somewhat heuristic.\n3) The method relies on a text-conditional diffusion model (GLIDE) with class prompts and uses the real image as guidance. It is not fully clear how much the performance depends on prompt engineering or the specific diffusion backbone.\n4) The 2-layer CNN analysis, while insightful, assumes a very stylized data distribution (two patches, Gaussian noise, cubic activations) and early-training approximations. It is not guaranteed these results carry over to deep architectures and natural images.\n\nMissing relevant references:\n\n1) GenMix: Effective Data Augmentation with Generative Diffusion Model Image Editing\n\n2) Context-guided Responsible Data Augmentation with Diffusion Models"}, "questions": {"value": "1) How sensitive are the results to the specifics of the clustering step? For instance, does choosing a different number of clusters or a different layer for features change which examples are selected as “slow”?\n2) Did you compare clustering versus simply picking top-θ% highest-loss examples (or uncertain examples)? Table 7 suggests clustering works better than high-loss, but can you elaborate on why?\n3) How was the denoising step (e.g. 50 steps) chosen? Would an adaptive schedule (tailored per image) improve results?\n4) Have you considered whether targeted augmentation helps other tasks (e.g. detection) or robustness measures beyond accuracy?\n5) How does performance compare to simply oversampling the slow examples (as in weighted sampling) without generating new images? This would isolate the benefit of synthetic variety."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hu2sFwTyAS", "forum": "VaGvbAgBmd", "replyto": "VaGvbAgBmd", "signatures": ["ICLR.cc/2026/Conference/Submission13941/Reviewer_prBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13941/Reviewer_prBk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760657941802, "cdate": 1760657941802, "tmdate": 1762924444365, "mdate": 1762924444365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a targeted augmentation approach using diffusion models, focusing on \"slow-learnable\" images (identified via early training clustering) by adding noise and denoising to create faithful variations with different noise. A theoretical analysis with a two-layer CNN suggests this promotes uniform feature learning and reduces minibatch variance compared to upsampling. Experiments on CIFAR-10/100 and TinyImageNet show accuracy improvements (up to 2.8%) with ResNet, ViT, and other models, and the method complements optimizers like SGD and SAM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Efficiency**: Augmenting only 30–40% of data outperforms full-dataset augmentation, offering a practical, resource-aware solution.\n\n**Empirical Support**: Ablation studies on augmentation factors and initialization provide useful insights.\n\n**Compatibility**: Works well with existing methods (e.g., SAM), boosting performance further."}, "weaknesses": {"value": "**Missing Prior**: The method overlaps with \"Boomerang\" [1], which uses similar noise-add-and-denoise techniques for data augmentation for classification, but it’s not cited or compared. Notably, they use all of the dataset for synthetic data generation, and they see gains in accuracy, which contradict experiments in this paper. \n\n**Theory-Practice Gap**: The claim of mimicking SAM’s feature learning (e.g., sections 4.1–4.2 suggest SAM-like noise suppression and uniform learning) doesn’t fully align with empirical results, where gains add to SAM’s effects (abstract notes up to 2.8% improvement with SAM). This suggests the method might address different aspects of training dynamics than intended, and further analysis could clarify this discrepancy.\n\n**Convergence claim**: The assertion of faster SGD convergence (Theorem 4.3, Corollary 4.4) relies on synthetic noise variance being lower than upsampling variance, but the link to the \"small noise\" assumption (section 4.4) isn’t fully derived or supported with training curves, leaving uncertainty about its practical impact.\n\n**Idealized and unrealistic theory setting**: The model assumes simplified conditions (e.g., P=2 patches, orthogonal features in section 3), which may not capture the complexity of real image data, potentially limiting the theory’s applicability to broader settings. \n\n**Scope limitation**: Experiments are confined to small datasets, and the claim of effectiveness across diffusion models (abstract) lacks support from multiple generators, which could restrict the method’s generalizability and leave its robustness untested.\n\n[1] Luzi L, Mayer PM, Casco-Rodriguez J, Siahkoohi A, Baraniuk R. Boomerang: Local sampling on image manifolds using diffusion models. Transactions on Machine Learning Research."}, "questions": {"value": "- Could you explain why the atypical activation function $\\sigma(z) = z^3$ was chosen over ReLU, and does the theory hold if ReLU is used instead?\n\n- Please cite, and compare/discuss results in the 'boomerang' paper, as it seems to contradict results in this paper.\n\n-  Can authors please add training-loss curves to support the convergence claim and explore why stacking with SAM works?\n\n- It would be great (but no necessary) to include a simple metric (e.g., feature similarity) to verify \"faithfulness\" of synthetic images.\n\n- Could authors test on a larger dataset and with another diffusion model to broaden applicability?\n\n## Overall \n\nThis is a helpful and promising approach for efficient augmentation, with solid small-scale results. Addressing the prior comparison, clarifying theory-practice links, and expanding experiments could make it even more impactful!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "naeXvCUfYh", "forum": "VaGvbAgBmd", "replyto": "VaGvbAgBmd", "signatures": ["ICLR.cc/2026/Conference/Submission13941/Reviewer_9nHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13941/Reviewer_9nHM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536117532, "cdate": 1761536117532, "tmdate": 1762924443912, "mdate": 1762924443912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel data augmentation strategy designed to improve the generalization of image classification models. The core contribution is a method that selectively applies augmentation only to a subset of the training data identified as slow-learning samples. The authors demonstrate that this targeted approach improves classification performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The central idea of targeting slow-learning samples for augmentation is novel and intuitive. The rationale that focusing augmentation efforts on more challenging examples seems a logical approach to improving model robustness and generalization.\n- The paper provides extensive empirical validation across three different datasets, showing credibility to the proposed method's effectiveness. The observation regarding the characteristics of slow-learned samples is particularly interesting and further discussion on this would make paper more interesting."}, "weaknesses": {"value": "- The theoretical analysis relies on a simplified two-layer CNN assumption. This raises questions about the direct applicability and relevance of the derived theorems to the deeper, more complex architectures commonly used in practice. The paper would be strengthened by a discussion bridging this theoretical gap.\n- I have concerns regarding the significant computational overhead of the proposed method. Utilizing a diffusion model for data generation, even for a subset of the data, is inherently more expensive than traditional augmentation techniques. Also, the multi-step pipeline may limit its practical adoption.\n- The experiments are confined to relatively small-scale datasets. It is unclear how the method would perform on larger, more complex datasets such as ImageNet. An explanation for the choice of datasets and a discussion on the method's potential scalability would be beneficial."}, "questions": {"value": "- What are the fundamental, identifiable differences between the samples classified as \"slow-learning\" versus \"fast-learning\"? If distinct features or patterns characterize these slow-learning samples, could a model be developed to identify them a priori? Such an approach could simplify the overall pipeline by removing the need for an initial training phase solely to identify these samples.\n- Could the proposed augmentation strategy, which focuses on difficult examples, be adapted to benefit tasks outside of classification, such as improving sample quality or diversity in image generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wFyhUdpQzw", "forum": "VaGvbAgBmd", "replyto": "VaGvbAgBmd", "signatures": ["ICLR.cc/2026/Conference/Submission13941/Reviewer_s4Wo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13941/Reviewer_s4Wo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890259184, "cdate": 1761890259184, "tmdate": 1762924443495, "mdate": 1762924443495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a training strategy that can smartly combine real data and synthetic data for training an improved classifier. To verify the effectiveness, this paper evaluated the method on image data augmentation for classification across backbones and datasets. This paper also provided some analysis on simple MLP layers. Besides, the method is a plug-and-play module and also evaluated plugged into the DiffuseMix."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dominant strength is that the current data augmentation paper only focuses on how to generate data with high fidelity and diversity for a more robust decision boundary. However, a very small paper focuses on how to balance the real set and the synthetic set during the training process. This paper fills the blank for current generative-based data augmentation research."}, "weaknesses": {"value": "This method is general, but the evaluations are limited.\n\n1/ The evaluated backbones are too weak, and whether better-pretrained backbones can overlay the benefit of your method.\n\n2/ Since this method is a plug-and-play module, why not evaluate it based on more state-of-the-art methods like [1,2,3,4]? Meanwhile, you should at least discuss them in the related work.\n\n3/ Lack of evaluations on fine-grained datasets.\n\n4/ This method seems like can be applied not only for image classification datasets, how for the augmentations in detection, segmentation even in other modalities like text and videos.\n\n\nReferences\n\n[1] Effective Data Augmentation With Diffusion Models\n\n[2] Enhance image classification via inter-class image mixup with diffusion model\n\n[3] Inversion Circle Interpolation: Diffusion-based Image Augmentation for Data-scarce Classification\n\n[4] Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation"}, "questions": {"value": "If you can solve my concerns, the method can be very general, and then I can raise my score to 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YcaqTYGATa", "forum": "VaGvbAgBmd", "replyto": "VaGvbAgBmd", "signatures": ["ICLR.cc/2026/Conference/Submission13941/Reviewer_pN7D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13941/Reviewer_pN7D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906241976, "cdate": 1761906241976, "tmdate": 1762924443045, "mdate": 1762924443045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}