{"id": "rv0j1CyRQK", "number": 1205, "cdate": 1756864301449, "mdate": 1759898221372, "content": {"title": "Cross-View Yaw Estimation in Location Uncertainty with Line-Aligning Yaw Scoring", "abstract": "Accurate rotation estimation is crucial in autonomous navigation and AR/MR (Augmented/Mixed Reality) applications. Small angular errors can lead to significant misalignment or navigation failures. Among the three rotation angles—pitch, roll, and yaw—yaw is the most challenging to estimate, as it lacks direct geometric cues, such as gravity-aligned structures. Yaw estimation given a BEV (Bird’s Eye View) image is treated as an inseparable cross-view localization problem that accompanies location and inevitably hypothesizes the height and distance of the ground pixel. We introduce LAYS, a line-alinging yaw scoring approach that enables precise yaw estimation by reducing the problem into a separated 1-DoF estimation, without the assumption of ground height and distance despite location uncertainty. In our method, BEV pixels are matched with a ground view column based on feature similarity. Using the relative yaw of the ground column, match scores are assigned to a yaw bin for each 2D pose pixel. To address location uncertainty, our method identifies line correspondence between the ground and BEV, and formulates the problem such that one such correspondence is sufficient to determine yaw. LAYS achieves state-of-the-art sub-degree yaw accuracy, improving from 6.55\\% to 34.81\\% on the Mapillary Geo-Localization dataset, 41.36\\% to 67.05\\% on the Ford Multi-AV dataset, and  12.39\\% to 23.67\\% on the VIGOR dataset, setting a new benchmark for precise localization in real-world scenarios.", "tldr": "", "keywords": ["cross-view localization", "rotation estimation", "aerial view"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/540f00341f848a058fcd5e8e018b8b1334ee6c87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenging problem of accurate yaw estimation, which is critical for autonomous navigation and AR/MR applications but difficult due to the lack of direct geometric cues. The authors propose LAYS, a line-aligning yaw scoring method that reformulates yaw estimation as a one-degree-of-freedom problem independent of ground height or distance assumptions. Experimental results on several benchmarks, including Mapillary Geo-Localization, Ford Multi-AV, and VIGOR, demonstrate substantial improvements in sub-degree yaw accuracy over prior approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces LAYS, a line-aligning yaw scoring method that reformulates yaw estimation as a 1-DoF problem, which is both conceptually elegant and practically effective.\n\n2. The idea of leveraging line correspondences between BEV and ground views provides strong geometric grounding and interpretability. The approach does not rely on assumptions about ground height or distance, improving robustness in real-world scenes.\n\n3. The authors show that isolating yaw estimation as a 1-DoF (degree of freedom) problem can yield benefits in downstream full pose (e.g., 3-DoF) localization tasks, illustrating the practical value of decoupling yaw from translation estimation.\nLAYS achieves significant improvements in sub-degree yaw accuracy across multiple benchmarks (Mapillary, Ford Multi-AV, VIGOR), clearly outperforming prior methods."}, "weaknesses": {"value": "The analysis of failure cases appears limited: for instance, what happens when there are few dominant linear structures (roads, lanes) visible, or when the matching line correspondence is ambiguous?"}, "questions": {"value": "Could the authors clarify how the method handles cases where the dominant linear structure (e.g., road lane line) is absent, ambiguous, or heavily occluded (e.g., dense vegetation, parking lots)? How robust is LAYS in such scenarios?\n\nCould the authors can utilize tools like GeoCalib [1] to handle inputs without gravity-alignment?\n\nThe method treats yaw estimation independently of translation (x,y) estimation, have the authors experimented with feedback loops where yaw estimation is used to refine translation?\n\n[1]GeoCalib: Single-image Calibration with Geometric Optimization, ECCV 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dmFQZAV3CI", "forum": "rv0j1CyRQK", "replyto": "rv0j1CyRQK", "signatures": ["ICLR.cc/2026/Conference/Submission1205/Reviewer_6GEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1205/Reviewer_6GEP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400971381, "cdate": 1761400971381, "tmdate": 1762915706887, "mdate": 1762915706887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response & Critical Clarification on Positional Uncertainty"}, "comment": {"value": "We thank all reviewers for their time and feedback, and are encouraged that N7WJ and 6GEP found our line-aligning formulation \"novel,\" “intuitive,” and \"practically effective.\".\n\nHowever, we must first clarify a ***key incorrect assumption*** (raised by 7Qns and snYM) that appears to underline critical concerns about our contribution, novelty, and experimental setup.\n\nWe clarify that: ***Our method does NOT assume a known position***; it explicitly operates under ***large location uncertainty*** (e.g., +-20m) during both training and evaluation. Concerns from 7Qns (“assuming x and y are accurate”) and snYM (“assuming ground-truth position is given”) are based on this incorrect premise. \n\nOur method's novelty lies in estimating yaw without requiring an accurate (x, y), enabled by our  line-alignment formulation. ***The system is explicitly designed to handle location uncertainty, not to ignore it.***\n\nWe hope this clarification ***helps reassess our contributions*** under the correct problem setting. Our title and abstract explicitly state the ***location-uncertain setup***, and the introduction explains that “an accurate position is not commonly available,” motivating our formulation. The method and evaluation also detail the process of estimating yaw with unknown location, robustly accounting for the possibility of all locations within the noise range.\n\nTo summarize the key intuition (Fig. 1, Sec. 3.1, 3.4):\n\n* A single column-to-line match defines a radial line of possible positions, not a point, that agrees on the correct yaw.\n* Based on each pixel-wise match, for each 2D pose candidate (x, y) we compute a hypothesized yaw θ_est; incorrect poses produce inconsistent θ_est across pixels.\n* Correct yaw yields consistent votes from linear-structure pixels over many (x, y), concentrating in the correct yaw bin of the 3D score volume S(θ, x, y).\n\nAgain, we do not assume (x, y) is known. We evaluate all candidate (x, y) positions through the exhaustive voting process, and the correct yaw is the one that yields consistent line-alignment across these uncertain locations, i.e., the yaw associated with the highest-scoring bin in the 3D score volume.\n\nThis noisy-location evaluation setup is explicitly described in the paper. As stated in L284: \"we add perturbations to the ground truth poses in both position and orientation...\", and the Table captions clearly indicate that location is noisy during evaluation.\n\nThis clarification is important because the incorrect assumption about a known location appears to cause several concerns, including the perception that our method resembles prior work.\n\nWe will update the Abstract and Introduction, and add clarifying remarks to make this central contribution more explicit. Detailed responses to individual comments will follow."}}, "id": "E2dHWkF0Dw", "forum": "rv0j1CyRQK", "replyto": "rv0j1CyRQK", "signatures": ["ICLR.cc/2026/Conference/Submission1205/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1205/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1205/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724418053, "cdate": 1763724418053, "tmdate": 1763724549062, "mdate": 1763724549062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LAYS, a novel approach for cross-view yaw estimation that decouples orientation estimation from location uncertainty. The key innovation lies in formulating yaw estimation as a line alignment problem between ground-level and bird's-eye view (BEV) images, rather than treating it as a byproduct of joint pose estimation. The method extracts column-wise features from ground images, matches them with BEV pixels, and employs a pairwise voting mechanism to estimate yaw without relying on ground height assumptions. Extensive experiments on three benchmarks demonstrate state-of-the-art performance, particularly under challenging noise conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes several valuable contributions. The core idea of treating yaw estimation as an independent 1-DoF problem represents a significant shift from prevailing approaches that couple orientation with location estimation. The line alignment formulation is both novel and intuitive, effectively leveraging structural correspondences between perspectives.\n\nThe technical execution is thorough, with careful attention to feature extraction, matching, and voting mechanisms. The column-wise feature aggregation with relative yaw encoding is particularly clever, as it naturally handles viewpoint differences without explicit projection models.\n\nExperimental validation is comprehensive, spanning multiple datasets with varying characteristics. The substantial improvements over current methods are convincing, especially the performance gains under ±180° yaw noise where existing approaches struggle. The ablation studies provide solid evidence for design choices, and the demonstration that LAYS can enhance existing 3-DoF methods by reducing their search space is practically significant."}, "weaknesses": {"value": "While the method excels at yaw estimation, its standalone capability for precise location estimation appears limited. The formulation naturally distributes scores along radial lines, which is optimal for orientation but suboptimal for pinpointing exact positions. The paper briefly mentions this limitation but could more explicitly discuss the implications for practical deployment.\n\nThe computational requirements of the multi-resolution scoring and exhaustive matching aren't thoroughly analyzed. In applications like autonomous navigation or mobile AR, inference efficiency matters, and some discussion of computational trade-offs would strengthen the practical contributions.\n\nThe evaluation, while comprehensive across datasets, remains within relatively structured environments. The method's performance in highly unstructured scenes (e.g., natural environments without clear linear features) remains an open question, though this is perhaps beyond the paper's current scope."}, "questions": {"value": "1. Given the method's strength in yaw estimation but limitations in precise localization, have you considered hybrid approaches that combine LAYS with complementary position estimation techniques? What would be the architectural implications?\n\n2. Could you provide more insight into computational requirements and potential optimizations? For real-time applications, are there strategies to reduce the matching or voting complexity?\n\n3. The method assumes gravity-aligned images. How sensitive is performance to residual pitch/roll errors that might occur in practical IMU-assisted systems?\n\n4. The line alignment concept is powerful. Might this principle extend to estimating other parameters, such as camera height, by analyzing multiple line correspondences?\n\n5. Some failure cases or challenging scenarios would be informative. Are there particular scene types or conditions where the line alignment assumption breaks down?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgGziaWQ8v", "forum": "rv0j1CyRQK", "replyto": "rv0j1CyRQK", "signatures": ["ICLR.cc/2026/Conference/Submission1205/Reviewer_N7WJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1205/Reviewer_N7WJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833112021, "cdate": 1761833112021, "tmdate": 1762915706686, "mdate": 1762915706686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper estimates the Yaw rotation in cross-view localization problem. The core idea is to match BEV image pixels to ground-view image columns based on feature similarity. The paper presents results on certain datasets with sota performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1 The paper introduces a specific formulation for yaw estimation, framing it as a line alignment problem rather than a direct pixel-level correspondence task."}, "weaknesses": {"value": "1 The work focuses solely on yaw estimation, assuming ground-truth position is given. This severely limits its practical utility for real-world applications like autonomous driving or VR/AR.\n\n2 The methodological pipeline (Column Feature Extraction, Ground-BEV Matching, Pair-wise Yaw Voting) bears a strong resemblance to parts of frameworks like OrienterNet[1], which jointly solve for location and orientation. The paper fails to clearly articulate the fundamental novelty of its components beyond this existing work.\n\n3 The experimental setup is limited. The absence of evaluations on standard benchmarks like KITTI raises concerns about the generalizability of the reported performance.\n\n4 As shown in Table 3, the experimental performance is not good.\n\n[1] OrienterNet: Visual Localization in 2D Public Maps with Neural Matching. CVPR 2023"}, "questions": {"value": "1 The experiments appear to be conducted under the assumption of a known, noise-free ground-truth position. Could you clarify this? In real-world scenarios, positional information (e.g., from GPS) is always noisy. How would your method perform with inaccurate position inputs, and what is the expected degradation in yaw estimation accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6CEdYsje8j", "forum": "rv0j1CyRQK", "replyto": "rv0j1CyRQK", "signatures": ["ICLR.cc/2026/Conference/Submission1205/Reviewer_snYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1205/Reviewer_snYM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001615371, "cdate": 1762001615371, "tmdate": 1762915706474, "mdate": 1762915706474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LAYS (Line-Aligning Yaw Scoring), a novel 1-DoF yaw estimation framework that matches BEV pixels to ground-view columns via feature similarity and uses line correspondences for pairwise yaw voting, effectively decoupling yaw estimation from location uncertainty. It addresses the challenge of accurate yaw estimation in cross-view localization, where conventional methods struggle due to the lack of direct geometric cues and dependence on ground height assumptions. The proposed work achieves sub-degree yaw precision and substantial gains across multiple datasets (Mapillary, Ford Multi-AV, VIGOR), establishing yaw as an independent, solvable subproblem and improving global pose localization accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a method that transforms a challenging 3-DoF rotation problem into a decoupled 1-DoF line-alignment task, enabling efficient and accurate yaw estimation without assuming ground height or distance.\n2. Experiment section demonstrates state-of-the-art sub-degree accuracy and consistent improvements (up to ~30% absolute gains) across multiple major cross-view localization datasets, showing robustness and generalization."}, "weaknesses": {"value": "1. In [CVPR 2020 - Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching], Shi et al. proposed cross-view image retrieval based on similarity between features encoded from ground panorama and polar-transformed aerial images, which is very similar to proposed method in terms of yaw alignment; proposed work claims disentanglement of yaw with the other 2 DoF, but limited experiment is performed and presented in supporting the claim, assuming $x$ and $y$ are accurate estimation is a strong assumption; hence would summarize for limited novelty and validation for proposed work.\n2. Column-wise feature matching and yaw-bin scoring may incur significant computational overhead compared to end-to-end regression models, hence limited feasibility in real-world deployment.\n3. in Eq.2, the $\\|\\|$ notation is not clearly indicated in terms of which kind of normalization."}, "questions": {"value": "1. could the authors present more thorough experiment results on validating disentanglement of yaw improves 3 DoF pose accuracy with ablations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DHXGsSMezT", "forum": "rv0j1CyRQK", "replyto": "rv0j1CyRQK", "signatures": ["ICLR.cc/2026/Conference/Submission1205/Reviewer_7Qns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1205/Reviewer_7Qns"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762780431916, "cdate": 1762780431916, "tmdate": 1762915706385, "mdate": 1762915706385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}