{"id": "w5k4FfyNnP", "number": 6377, "cdate": 1757975432354, "mdate": 1759897918646, "content": {"title": "FALSA: Fairness-Aware Latent Space Alignment in Vision-Language Models for Medical Image Segmentation", "abstract": "Vision-language models (VLMs) achieve impressive generalization in medical image segmentation but often propagate demographic biases from large-scale pretraining data. These biases manifest not only across single attributes (e.g., gender, race) but also through intersectional bias, where overlapping identities amplify disparities, undermining equitable clinical reliability. To address these problems, we introduce FALSA, a Fairness-Aware Latent Space Alignment framework that mitigates representational and predictive disparities without sacrificing accuracy. FALSA integrates three components: (1) Demographic-Invariant Contrastive Learning (DICL) to align multimodal embeddings while suppressing group-specific leakage, (2) Adaptive Fairness Calibration (AFC) to adversarially remove demographic cues from latent features, and (3) Unified Cross-Attribute Fairness Loss (UniFairLoss) to jointly reduce intra- and inter-attribute disparities, including intersectional ones. Applied to SAM and SAMed on the Harvard-FairSeg benchmark, FALSA achieves state-of-the-art equity-scaled Dice and IoU scores, reducing disparity indices and performance gaps by up to 70-80\\% while improving overall segmentation accuracy. Unlike prior models that compromise segmentation accuracy, FALSA advances both fairness and performance, providing a scalable framework for equitable multimodal healthcare AI. Code and Data will be available at https://github.com/.../FALSA.", "tldr": "We introduce a fairness-aware latent space alignment (FALSA) framework for vision-language models that reduces demographic and intersectional biases in medical image segmentation while improving accuracy and equity across populations.", "keywords": ["Fairness", "Vision Language Model", "Medical Image Segmentation", "Latent Space Alignment", "Bias Mitigation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35149c5f727c39ee5db9832bd95ac425b13177c0.pdf", "supplementary_material": "/attachment/e98073c764a193707efd4d2575fa238f4f99537c.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces a method to improve the fairness of vision-language models for medical image segmentation, while improving the performance. It consists of three components, namely (1) Demographic-Invariant Contrastive Learning (DICL), (2) Adaptive Fairness Calibration (AFC), (3) Unified Cross-Attribute Fairness Loss (UniFairLoss). The evaluation is performed on SAM and SAMed on the HarvardFairSeg benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe method improves both fairness and performance, something which has not been very common in literature and is valuable.\n\n•\tThe method is rather efficient, the increases in number of parameters and FLOPs are only very minor.\n\n•\tA number of relevant analyses are included, e.g. ablation showing the impact of each component, impact on intersectional demographic bias, etc.\n\n•\tExtensive experimental results are reported, although sometimes one wonders if the results are too granular and aggregate metrics could be reported instead.\n\n•\tThe paper is clearly written overall."}, "weaknesses": {"value": "•\tThe method consists of three components, each of which seems to have a small marginal improvement. It could be interesting to see if these components can also be combined with the other techniques that were reported to boost the performance further.\n\n•\tThe evaluation is performed across two models only, but maybe there are no other suitable candidates for this specialized task. \n\n•\tOne benchmark is utilized (Harvard-FairSeg Dataset). It is fairly large-scale, but I wonder if there are other benchmarks that are commonly used for the task or if only this one is used. In general using more than one benchmark can help show the general usefulness of the solution.\n\nMinor: tables 1 and 2 are on the page before the corresponding section, would be better to have them after the start of the section if possible."}, "questions": {"value": "•\tAre there other vision-language models (other than SAM and SAMed) that are commonly used for the specific task?\n\n•\tAre there other popular datasets that people investigate for the selected task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pgRsWK17d7", "forum": "w5k4FfyNnP", "replyto": "w5k4FfyNnP", "signatures": ["ICLR.cc/2026/Conference/Submission6377/Reviewer_PUAA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6377/Reviewer_PUAA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760723908739, "cdate": 1760723908739, "tmdate": 1762918664759, "mdate": 1762918664759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces FALSA (Fairness-Aware Latent Space Alignment), a novel framework designed to mitigate demographic and intersectional biases in vision-language models (VLMs) for medical image segmentation without sacrificing accuracy. FALSA integrates three key components: (1) Demographic-Invariant Contrastive Learning (DICL) to align multimodal embeddings across demographic groups while preserving semantic content; (2) Adaptive Fairness Calibration (AFC), an adversarial method that dynamically removes demographic cues from both visual and textual features using group-adaptive gradient reversal; and (3) Unified Cross-Attribute Fairness Loss (UniFairLoss), which jointly minimizes intra- and inter-attribute performance disparities, including those arising from intersecting identities (e.g., race × gender × language). Evaluated on the Harvard-FairSeg benchmark using SAM and SAMed backbones, FALSA achieves state-of-the-art results—reducing disparity indices and performance gaps by up to 70–80% while simultaneously improving segmentation accuracy (Dice and IoU). The framework demonstrates strong generalization across prompt types (clinician- vs. ChatGPT-generated), intersectional subgroups, and even out-of-distribution datasets, offering a scalable and efficient solution for equitable multimodal healthcare AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FALSA improves both segmentation accuracy (Dice/IoU) and fairness metrics, achieving state-of-the-art results on the Harvard-FairSeg benchmark.\n\n2. FALSA explicitly addresses intersectional disparities, not just single-attribute bias, making it more aligned with real-world demographic complexity.\n\n3. FALSA reduces disparity indices (DI) and relative performance gaps (RPG) significantly, while also boosting overall segmentation performance across both SAM and SAMed backbones."}, "weaknesses": {"value": "1. The methodological novelty of FALSA is somewhat incremental. While it introduces a combination of Demographic-Invariant Contrastive Learning (DICL), Adaptive Fairness Calibration (AFC), and a Unified Cross-Attribute Fairness Loss (UniFairLoss), these components are largely extensions of existing paradigms—contrastive regularization, adversarial de-biasing, and fairness-aware loss weighting. The combination is well-engineered but not deeply novel in theoretical insight. The framework primarily aggregates known fairness strategies rather than proposing fundamentally new mathematical or causal fairness principles\n\n2. One potential limitation lies in the scope of the evaluation, which is confined to a single anatomical task—optic cup and rim segmentation in retinal fundus images—using the Harvard-FairSeg dataset. While this benchmark is well-suited for studying fairness, the generalizability of FALSA to other medical imaging domains (e.g., chest X-rays, brain MRI, or histopathology) with different visual characteristics, pathologies, and bias patterns remains unverified. The paper hints at potential extension to other tasks, but without empirical validation on diverse modalities or diseases, it is unclear whether FALSA’s fairness gains are robust across broader clinical contexts.\n\n3. The interpretability and causal reasoning of fairness mechanisms are limited. The paper does not establish whether the fairness improvements result from genuine mitigation of demographic signal leakage or from a smoothing effect that homogenizes representations across all groups. Without explicit causal or attributional analyses, it is difficult to conclude whether FALSA meaningfully enhances fairness or simply regularizes feature space variance. This concern is amplified by the lack of counterfactual fairness analysis or disentanglement experiments. \n\n4. There is a potential bias–utility trade-off underexplored in the results. Although the authors claim “no compromise in segmentation accuracy,” the tables suggest modest fluctuations in Dice and IoU across some subgroups. The paper would benefit from detailed statistical significance tests (e.g., Wilcoxon signed-rank or paired t-tests) to confirm that improvements in fairness metrics are not offset by minor but systematic losses in clinical accuracy for certain populations"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8KVEvh2rst", "forum": "w5k4FfyNnP", "replyto": "w5k4FfyNnP", "signatures": ["ICLR.cc/2026/Conference/Submission6377/Reviewer_LgN6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6377/Reviewer_LgN6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897587722, "cdate": 1761897587722, "tmdate": 1762918664268, "mdate": 1762918664268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the problem of mitigating demographic biases in VL models for medical image segmentation. They propose FALSA, which consists of three components: a contrastive loss to align across demographics, adversarial debiasing, and a regularizer to minimize performance gaps. The authors conduct experiments on the Harvard-FairSeg dataset, finding that FALSA improves overall Dice and IoU while also improving on several fairness metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors tackle an important problem, addressing fairness in segmentation.\n- The proposed method exhibits good empirical performance.\n- The authors conduct ablations on each of the components."}, "weaknesses": {"value": "1. The novelty of the proposed method is rather limited. Similar ideas for fair contrastive learning by pair selection have been proposed in prior work [e.g. 1, 2]. Group adversarial learning also has been explored extensively in prior work as the authors cite, and though the authors propose a new per-group weighting strategy, it seems quite ad-hoc and is not theoretically justified. Finally, there is no theoretical justification or intuition for why we need all three losses at once. \n\n2. The motivation of the paper does not clearly define the fairness objective. Is the goal to have embeddings that are demographic-invariant (i.e. same distribution of z), or to achieve the same performance across all groups? The two objectives differ since the Bayes error across groups may be different.\n\n3. The authors emphasize intersectionality in their motivation, but the proposed method does not address intersectionality in a satisfactory way in my view. The primary challenge of intersectionality is that per-group sample sizes may be too small to accurately estimate loss or performance. None of the three losses address this, unlike other fairness concepts like subgroup fairness [3].\n\n4. The authors only evaluate on a single dataset. MIMIC-CXR might be another good candidate that contains image, text, demographics, and segmentation masks.\n\n5. It is unclear why the proposed method is specific to segmentation. As the method is debiasing the shared embedding space, it seems like it can be used for other VLM tasks like zero-shot classification or generation. It is also unclear why the method is specific to the healthcare setting.\n\n\n[1] Shen, Aili, et al. \"Contrastive learning for fair representations.\" arXiv preprint arXiv:2109.10645 (2021).\n\n[2] Ma, Martin Q., et al. \"Conditional contrastive learning for improving fairness in self-supervised learning.\" arXiv preprint arXiv:2106.02866 (2021).\n\n[3] Kearns, Michael, et al. \"Preventing fairness gerrymandering: Auditing and learning for subgroup fairness.\" International conference on machine learning. PMLR, 2018."}, "questions": {"value": "1. For the UniFairLoss, are per-group task performance metrics calculated within only the samples in a mini-batch? If so, this seems like it would result in high variance.\n\n2. Can the authors provide some intuition on why FALSA improves overall performance over the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fECn5X9qry", "forum": "w5k4FfyNnP", "replyto": "w5k4FfyNnP", "signatures": ["ICLR.cc/2026/Conference/Submission6377/Reviewer_HNvw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6377/Reviewer_HNvw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152955086, "cdate": 1762152955086, "tmdate": 1762918663713, "mdate": 1762918663713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}