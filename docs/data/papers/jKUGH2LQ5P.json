{"id": "jKUGH2LQ5P", "number": 25213, "cdate": 1758365374881, "mdate": 1758806396223, "content": {"title": "Breaking the Invisible Leash: Support Expansion in RLVR via Off-Policy Transport", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably raises pass@1, yet standard on-policy updates often amplify a base modelâ€™s existing modes, curbing exploration and missing correct traces with negligible prior mass. We introduce an off-policy transport objective that evaluates guided rollouts at the plain prompt via a token-wise transport ratio. Under mild conditions, we prove a strictly positive drift in the plain-prompt log-likelihood of correct traces, implying support expansion in expectation. We achieve this objective through a GRPO-compatible pipeline that draws guidance from either an external policy or the model itself. The analysis explains why stepping beyond purely on-policy RLVR expands empirical support, unifying recent observations across off-policy guidance, replay-based optimization, and stepwise hint scaffolding under a single transport mechanism.", "tldr": "", "keywords": ["Reinforcement Learning", "RLVR", "LLM Reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}