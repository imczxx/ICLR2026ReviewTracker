{"id": "BcjZCertEk", "number": 6909, "cdate": 1758001190691, "mdate": 1759897884589, "content": {"title": "Learning-Time Encoding Shapes Unlearning in LLMs", "abstract": "As large language models (LLMs) are increasingly deployed in the real world, the ability to ``unlearn'', or remove specific pieces of knowledge post hoc, has become essential for a variety of reasons ranging from privacy regulations to correcting outdated or harmful content. Prior work has proposed unlearning benchmarks and algorithms, and has typically assumed that the training process and the target model are fixed. In this work, we empirically investigate how learning-time encoding in knowledge encoding impact the effectiveness of unlearning factual knowledge. \nWe conduct two studies: (i) examining how paraphrased descriptions influence unlearning performance, and (ii) analyzing unlearning when multiple facts are embedded within the same training text chunk. \nOur empirical study reveals two important implications: a new perspective for interpreting unlearning performance and practical strategies for improving LLM unlearning.", "tldr": "", "keywords": ["Large language model", "unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2ae9f9016efbea3b0baa100801fa387d2fa3c57.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of knowledge unlearning in large language models (LLMs). While prior work has primarily focused on developing new methodological approaches to improve unlearning performance on public benchmarks, this paper instead investigates how the way target knowledge is encoded in the training data affects the effectiveness of unlearning. To this end, the authors consider two main experimental settings: (1) examining the effect of text paraphrasing on unlearning, and (2) examining the effect of text chunk composition on unlearning. Through a series of controlled experiments, the paper observes that models trained with paraphrased data exhibit more effective unlearning, and that isolating forget knowledge within text chunks leads to more efficient forgetting."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper approaches the unlearning problem from a perspective, analyzing how the learning-time encoding (the way target knowledge is represented and learned during training) affects the effectiveness of unlearning. The framing that \"how and what is learned determines how well it can be forgotten\" clearly distinguishes this work from prior studies focused solely on algorithmic improvements. This is a valid and valuable perspective for deepening our understanding of knowledge unlearning in LLMs.\n* The paper systematically separates and investigates three factors, text paraphrasing, chunk entanglement, and sentence isolation, to examine how each influences the difficulty of unlearning. This stepwise, controlled design provides a meaningful experimental setup for exploring the structural causes of unlearning difficulty."}, "weaknesses": {"value": "* In each experimental setting (e.g., FT-Single vs FT-Unlearn-Mul), the initial learning strength of the forget and retain knowledge differs. Models trained with paraphrased data tend to encode the same facts more strongly, resulting in higher initial scores and making unlearning appear more difficult. The paper attempts to account for this difference using the Norm-AUC metric, but this measure has a structural limitation: models with higher initial scores may be disadvantaged in relative evaluation, since the same absolute decrease in score yields a smaller relative change. Consequently, Norm-AUC does not fully normalize for differences in learning intensity. It would be more appropriate to analyze unlearning difficulty using models that are fine-tuned to have comparable initial forget/retain scores.\n* The results for Problem 3 and Problem 4 appear relatively self-evident given the experimental setup. When the forget and retain sets coexist within the same chunk, unlearning naturally fails, and when they are fully separated by chunk boundaries, performance improves as gradient interference is removed. These experiments therefore confirm rather than extend what is already understood about representation entanglement in unlearning.\n* The improvement observed in Problem 5 also lacks a sufficiently clear explanation. The only difference in this setting is that facts are arranged as independent sentences rather than connected text, yet the paper provides no theoretical or quantitative justification for why this should make unlearning more effective. While the empirical trend is interesting, the causal interpretation of this result remains underdeveloped.\n* The paper evaluates only two unlearning algorithms, Gradient Ascent (GA) and Task Vector (TV). Although these represent the optimization-based and representation-editing paradigms, respectively, this scope is too narrow to support general conclusions about unlearning mechanisms. It would be beneficial to include retain-aware or preference-based algorithms (e.g., Gradient Difference, Direct Preference Optimization, etc.) that explicitly leverage retain data during optimization, as such methods may exhibit different behavior with respect to the proposed encoding effects.\n* Minor typos\n  * Line 125: paraphased -> paraphrased\n  * Line 290: only three combinations are listed"}, "questions": {"value": "* In Section 2.3, the authors justify focusing on the fine-tuning stage for studying unlearning. However, fine-tuning typically involves a much smaller and more carefully curated dataset, often with significant manual filtering. In contrast, the pretraining stage usually carries a much higher risk of including sensitive or private user data. Could the authors elaborate on why fine-tuning is considered a more realistic or representative setting for privacy-sensitive unlearning?\n* As a learning-time strategy for improving the post-hoc efficiency of unlearning, the paper proposes separating knowledge during training. However, in the Problem 5 setting, where each fact is trained as an independent sentence, might this strategy introduce side effects such as degraded fluency, loss of contextual coherence, or weaker entity-level reasoning in generated text? Have the authors observed or considered such trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WCtHBkVlJ7", "forum": "BcjZCertEk", "replyto": "BcjZCertEk", "signatures": ["ICLR.cc/2026/Conference/Submission6909/Reviewer_PLQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6909/Reviewer_PLQ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565968323, "cdate": 1761565968323, "tmdate": 1762919149983, "mdate": 1762919149983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how the way knowledge is encoded during fine-tuning impacts the effectiveness of post-hoc knowledge unlearning in LLMs. The authors argue that how knowledge is represented (shaped) in the training corpus significantly influences how difficult it is to later remove.\nThe authors conduct a rigorous, controlled empirical study by fine-tuning two LLMs (Llama2-7B and Gemma2-2B) on two extended benchmarks (Eval-DU+ and TOFU+) which use synthetic, fictitious knowledge to avoid pre-training contamination. They systematically test two main encoding factors:\n- Paraphrasing: They compare models trained on single vs. multiple paraphrased descriptions of facts.\n- Text Entanglement: They analyze unlearning when facts-to-be-forgotten (\"forget set\") are embedded in the same text chunk (e.g., a paragraph) as facts-to-be-kept (\"retain set\").\n\nTheir results consistently show that unlearning is harder when the forget set facts were paraphrased, but easier when the retain set facts were paraphrased.  Critically, the study also finds that unlearning individual facts is exceptionally difficult when forget and retain facts are entangled in the same training text chunk."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores a novel aspect on data shapes for unlearning in LLMs. Most unlearning research focuses on post-hoc algorithms. This work provides a new and important perspective by showing that data curation strategies during fine-tuning are a critical and overlooked factor.\n- The findings are clear and consistent across two different model families (Llama2, Gemma2) , two different datasets (Eval-DU+ and TOFU+), and two representative unlearning algorithms (Gradient Ascent and Task Vectors)."}, "weaknesses": {"value": "- Paraphrasing vs. Frequency: The effect of using multiple paraphrases (e.g., in FT-Unlearn-Mul) is closely related to simply increasing the frequency of the fact in the training data. The paper argues this encourages \"structured\" representations and distinguishes itself from related work on frequency, but it doesn't empirically disentangle the effect of the paraphrasing from the frequency.\n- No real-world dataset: The use of synthetic data is a key strength for experimental control, but also a potential weakness. Real-world corpora are far messy, and the entanglement of facts is likely more complex than the binary \"entangled chunk\" (FT-Mul-Chunk) or \"isolated sentences\" (FT-Mul-Chunk-Iso) settings explored.\n- Missing quality checking for the data augmentation (e.g., paraphrasing, separating) via GPT4o.\n- In the paper, the author mentions four combinations of models and datasets. However, only three of them are shown, Gemma2 with TOFU+ is missing."}, "questions": {"value": "- While you acknowledge the limitation of focusing on fine-tuning, do you have any hypotheses on how these findings might translate to the pre-training regime? Knowledge is both heavily paraphrased and heavily entangled during pre-training. Does this imply that unlearning pre-trained knowledge will always be as difficult as your FT-Mul-Chunk scenario, unless the fact is extremely rare?\n- Can you explicitly explain the baselines represents a random-chance unlearning in Figure 4,5,6?\n- Is FT-Mul-Chunk-Iso part of FT-Mul-Chunk?\n- Could you please provide new results based on LLama3 and Gemma3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PRejgnM6iR", "forum": "BcjZCertEk", "replyto": "BcjZCertEk", "signatures": ["ICLR.cc/2026/Conference/Submission6909/Reviewer_bmBj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6909/Reviewer_bmBj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864974024, "cdate": 1761864974024, "tmdate": 1762919149573, "mdate": 1762919149573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of shaping of factual knowledge during fine-tuning phase on the effectiveness of knowledge unlearning in LLMs. The analysis reveals.that the knowledge demonstrated in various paraphrased ways are harder to unlearn, while unlearning is more efficient when knowledge is presented with various paraphrased formats overall. Moreover, the paper shows a novel insight that knowledge that is presented inside a single chunk of bath data are harder to unlearn while retaining the other knowledge inside the same chunk."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(S1) This work is well motivated, and the core question is important and novel, while not trivial.\n\n(S2) I commend the authors for the clear organization of research questions, appropriate experimental designs, and well-organized writing."}, "weaknesses": {"value": "(W1) **Limited mechanistic understanding**: While the intuition that presenting factual knowledge in multiple paraphrased format leads to more structured representation is compelling and aligns with experimental results, the analysis relies on the observation of knowledge unlearning success, and the understanding on the mechanism governing this behavior is limited. For example:\n- Is there any difference in the distribution of the update vector or knowledge circuits [1], that is computed for unlearning single knowledge and paraphrased knowledge, respectively?\n- The improved effectiveness of unlearning upon paraphrasing both knowledge types (regarding RQ2) might be attributed to the increased training steps rather than the structures of representation induced by paraphrase, as the size of the fine-tuning dataset is increased. How can we remove this possibility?\n\n(W2) **Scope of the experiment**: While the authors have well justified the experimental setup of unlearning factual knowledge that is first encountered during fine-tuning phase, previous work ([2,3]) have demonstrated that the knowledge obtained during pretraining and fine-tuning phase may be encoded in a different way. This somewhat limits the scope of this workâ€™s contribution. Could you share your thoughts on the applicability of the insights provided in this work to the unlearning of the knowledge acquired during pretraining?\n\n\n\n[1] https://arxiv.org/abs/2405.17969\n[2] https://arxiv.org/abs/2405.05904\n[3] https://arxiv.org/abs/2503.21676"}, "questions": {"value": "Please see the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ukYT1Zr24L", "forum": "BcjZCertEk", "replyto": "BcjZCertEk", "signatures": ["ICLR.cc/2026/Conference/Submission6909/Reviewer_Av7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6909/Reviewer_Av7s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914143981, "cdate": 1761914143981, "tmdate": 1762919148938, "mdate": 1762919148938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}