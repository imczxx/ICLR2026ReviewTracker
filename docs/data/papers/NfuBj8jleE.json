{"id": "NfuBj8jleE", "number": 1714, "cdate": 1756910477943, "mdate": 1763737165592, "content": {"title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling", "abstract": "The reasoning process of  Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks.\nA promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems.\nTo tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs.\nTo incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. \nTo enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts.\nExtensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.", "tldr": "", "keywords": ["Knowledge Graph", "Large Language Models", "Knowledge-enhanced reasoning", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c392b2179b2ca855094df5bd9e62d13a19a71162.pdf", "supplementary_material": "/attachment/fa58472005ab615e562d5a081d3b43685d5af162.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of poor generalization in existing Large Language Model (LLM) methods for Knowledge Graph Question Answering (KGQA), which often fail when faced with reasoning patterns not seen during training. The authors propose \"Explore-on-Graph\" (EoG), a novel framework designed to incentivize LLMs to autonomously explore diverse reasoning paths on a knowledge graph (KG)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core contribution—incentivizing autonomous exploration on KGs via reinforcement learning—is a novel and important step beyond prevalent imitation-based methods. While RL has been used for KG reasoning in the past, its application to modern LLMs with the proposed two-phase reward structure (outcome + path-refinement) is original. The \"path-refined reward\" is an intuitive and clever mechanism to make the exploration process more efficient and meaningful, rather than just rewarding the final outcome. This directly addresses the critical problem of out-of-distribution generalization in KG reasoning."}, "weaknesses": {"value": "The path-refined reward, Rpath, is a key component of the method. Its calculation (Equation 4) requires a \"ground-truth reasoning path\" rg. However, the paper fails to explain how these ground-truth paths are obtained for the training data across all five datasets. While some datasets (e.g., 2WikiMultihop) may provide such paths, it is not standard for others like CWQ, WebQSP, or GrailQA. This is a critical missing methodological detail. \n\nThe proposed SFT + RL pipeline, particularly using GRPO with multiple rollouts per prompt, appears to be computationally very expensive. The paper mentions using 8xH100 GPUs but does not provide a broader discussion on the training time, total computational budget, or the trade-offs between performance gains and the significant training overhead. A comparison of the computational cost against simpler fine-tuning methods would be valuable for assessing the practical applicability of EoG."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GHn8srabdS", "forum": "NfuBj8jleE", "replyto": "NfuBj8jleE", "signatures": ["ICLR.cc/2026/Conference/Submission1714/Reviewer_Fb3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1714/Reviewer_Fb3S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622216333, "cdate": 1761622216333, "tmdate": 1762915864863, "mdate": 1762915864863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Explore-on-Graph (EoG) framework that incentivizes LLMs to autonomously explore diverse reasoning paths on Knowledge Graphs for Question Answering. The methods consist two stages: (i) SFT using long CoT by Gemini 2.5 Flash, (ii) reinforcement learning with Group Relative Policy Optimization using both outcome reward and path refined reward. The authors evaluate EoG on five KGQA benchmarks and demonstrate state-of-the-art performance, though I must say the results for the baseline are mostly missing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This method outperforms powerful closed-source models such as GPT-5 and Gemini-2.5 Pro, which is impressive.\n2. The paper is well written, with clear motivation and problem formulation, and provides a good example (Figure 1) illustrating the limitation of existing approaches and the EoG methods.\n3. Smaller open-source LLM trained by EoG can compete with larger closed-source ones, which means EoG addresses some of the current compute resource limitations.\n4. Well-organized experiment, ablation study effectively demonstrates the importance of each component."}, "weaknesses": {"value": "1. Limited technical novelty: The core contribution combines existing techniques (SFT, GRPO, simple reward design) without significant algorithmic innovation. The path reward is particularly simplistic, using only substring matching. In the area of KG reasoning, using reinforcement learning to explore path is a general and common practice. Check MINERVA [1] and DeepPath [2].\n\n2. The reliance on Gemini 2.5 Flash for dataset generation creates a dependency that may limit reproducibility. The paper doesn't discuss alternatives or provide the generated datasets.\n\n3. More than half of the baseline experimental results in Table 1 are missing, and the authors do not provide any explanation in the paper. I do not think that the effectiveness of EoG can be proven based on Table 1 alone, as the baseline results are extremely limited.\n\n4. The paper lacks a discussion of training costs, convergence time, and computational requirements for the RL stage.  As the paper says it trains EoG on 8*H100 GPUs, which is very costly. This may be a strong limitation of the proposed method. \n\n5. The substring-based matching in Equation 4 is brittle and may not capture semantic equivalence, paraphrases, or partial correctness in reasoning paths.\n\n\n[1] Das, Rajarshi, et al. \"Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning.\" International Conference on Learning Representations. 2018.\n[2] Xiong, Wenhan, Thien Hoang, and William Yang Wang. \"DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.\" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017."}, "questions": {"value": "1. What are the computational costs of the RL training stage compared to standard supervised fine-tuning? How many GPU hours are required for convergence?\n2. Have you considered more sophisticated path reward designs that account for semantic similarity rather than exact substring matching? For example, using graph edit distance or learned similarity metrics?\n3. Can you provide the generated CoT datasets or detailed statistics about them to enable reproducibility without access to Gemini 2.5 Flash?\n4. How sensitive is the method to the quality of the initial SFT stage? What happens if a weaker teacher model is used for data generation?\n5. Have you analyzed failure cases where exploration leads to incorrect paths? How does the model handle ambiguous questions where multiple valid reasoning paths exist?\n6. Could you provide theoretical analysis or empirical evidence about the exploration efficiency and coverage of the reasoning space? For example, to improve the content of case study?\n7. Why are so many experimental results missing in Table 1? (I don’t think “the original paper did not report the experimental results of this dataset” is a good answer)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WucPD7qADU", "forum": "NfuBj8jleE", "replyto": "NfuBj8jleE", "signatures": ["ICLR.cc/2026/Conference/Submission1714/Reviewer_dVt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1714/Reviewer_dVt9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634588727, "cdate": 1761634588727, "tmdate": 1762915864716, "mdate": 1762915864716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Explore on Graph (EoG), where not only the correct answer but also the reasoning path is rewarded via reinforcement learning, enabling the model to explore novel reasoning paths that fall outside the distribution of pre-defined rules or supervised fine-tuning data.\n\nThe experiments are comprehensive and show strong results on five KGQA datasets, outperforming not only open-source but also even closed-source LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is well grounded. Not only the answer but also the reasoning path can server as good reward signals.\n2. The experiments show strong results. For example, table 1 show EoG outperforms not only open-source but also even closed-source LLMs, and table 4 show strong results on OOD settings. The improvement is significant."}, "weaknesses": {"value": "1. Some implementation details are not clear. Are phase 1: Outcome Reward Modeling and phase 2: Path-refined Reward Modeling implemented sequentially or simultaneously (as in Equation 5)?\n2. Reproducibility: The code is currently unavailable, which hinders verification, reproduction, and improvement efforts. Open-source code is crucial for these processes."}, "questions": {"value": "1. Same as Weakness 1: Are phase 1: Outcome Reward Modeling and phase 2: Path-refined Reward Modeling implemented sequentially or simultaneously (as in Equation 5)?\n2. For the OOD experiment (Figure 5), what do the x and y axes represent? Is the model trained on the dataset on the x-axis and then evaluated on the dataset on the y-axis, or vice versa?\n3. For the OOD experiment (Figure 5), what is the performance of other models in the OOD settings? It would be useful to compare EoG not only with EoG-SFT but also with other SOTA models (as in Table 1).\n4. Some things can be improved for clarity and readability. For example, in Figure 3, the meanings of the x and y axes are only available in the main body text. The reading experience would be greatly enhanced if the x and y axes were directly labeled in the figure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ka7kNAtHc1", "forum": "NfuBj8jleE", "replyto": "NfuBj8jleE", "signatures": ["ICLR.cc/2026/Conference/Submission1714/Reviewer_Gg3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1714/Reviewer_Gg3c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717871297, "cdate": 1761717871297, "tmdate": 1762915864574, "mdate": 1762915864574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Explore-on-Graph (EoG)**, a KG-augmented LLM reasoning framework that couples (i) supervised fine-tuning on long chain-of-thought traces and (ii) reinforcement learning with a **path-refined reward**. The core idea is to *incentivize autonomous exploration* of novel multi-hop paths on knowledge graphs, improving generalization beyond rule/imitation patterns. Concretely, the RL stage optimizes a GRPO-style objective using a final-answer **outcome reward** (entity-level F1 extracted from the `<answer>` tag) and an auxiliary **path reward** that measures how many ground-truth triples appear in the model’s `<think>` text; a weighted sum defines the joint reward. Experiments on **WebQSP, CWQ, GrailQA, QALD-10-en, and 2WikiMultihop** show consistent gains over recent KG-enhanced systems, and ablations indicate the path-refined reward materially contributes beyond SFT alone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear motivation & problem framing.** The paper articulates why rule/imitation approaches struggle on OOD patterns and positions exploration as the missing capability. Figure 1 illustrates this vividly. \n* **Method is simple, modular, and reproducible in principle.** The rewards (answer F1; path triple-match ratio) are transparent and plug into a standard GRPO objective with KL control.  \n* **Strong empirical results across diverse KGQA datasets** with consistent gains vs. strong baselines; ablations show each component’s contribution and explore the α trade-off between outcome and path rewards. \n* **Ablations that are decision-useful.** Removing SFT, outcome, or path rewards degrades performance in expected ways, supporting the design choices. (Table 2 & ratio analyses in the text.)"}, "weaknesses": {"value": "1. **Potential reward gaming / verification gap.** The **path reward** credits substring co-occurrence of `(subject, relation, object)` tokens in `<think>` text rather than **verified KG traversals**. This leaves room for *verbalization without execution* (i.e., asserting triples to earn reward). The paper should either (a) execute the predicted path against the KG to produce a structural match reward, or (b) at least audit hallucinated triples vs. KG edges. \n2. **LLM-judge reliance for qualitative criteria.** The analysis of comprehensiveness/relevance/exploration uses **GPT-4o-mini** as judge; such measures can be noisy and model-biased. Human evaluation or KG-grounded automatic proxies would strengthen claims. \n3. **Comparisons to closed-source LLMs** (Gemini 2.5, “GPT-5”) are intriguing but ambiguous: API prompting details, temperature, and decoding budgets can shift outcomes; moreover, the “GPT-5” reference seems tenuous. I recommend focusing the SOTA claim on open-source or adding stricter evaluation parity. \n4. **Statistical reporting.** Tables omit **confidence intervals/standard errors** and **significance testing**, especially important across multi-seed RL runs. This weakens the strength-of-evidence for SOTA claims. (No CIs in Table 1.) \n5. **Scope of robustness/OOD probes.** While the paper argues improved OOD generalization, it would help to (i) include **systematic stress tests** (entity aliasing, edge deletions, spurious edges), and (ii) evaluate **path length sensitivity** with explicit adversarial splits beyond the included subsets."}, "questions": {"value": "1. **Path reward verification.** Can the authors compute the path reward by **parsing the predicted path** into a sequence of relations and **executing it** on the KG to verify edge existence (vs. substring matching)? If not feasible, can they report a *hallucination rate* of triples mentioned in `<think>` but absent in the KG? \n2. **Robustness to reward hacking.** Did the authors observe behaviors where the model *lists many unrelated triples* to improve match probability? Any safeguards (length penalty, entropy regularization, path-structure constraints)?\n3. **Ablation on α and stability.** Figure discussing α suggests a sweet spot. Please include **training curves** and variance over **≥3 seeds** for each α to assess RL stability. (Also report GRPO hyperparameters.) \n4. **Closed-source comparison protocol.** For Gemini/GPT, please provide **identical decoding parameters**, prompt templates, and **budget parity** (n-samples, context length), or move these to an appendix with full reproducibility details. \n5. **KG execution failure modes.** In cases where KG lacks labels/edges (Figure 6-style), how often does EoG succeed via exploration vs. spurious textual correlations? Any breakdown by hop length and relation sparsity?\n6. **Significance & compute.** Please add **CIs** for Table 1 and **report training compute** (SFT tokens, RL steps, batch, GPU hours) to contextualize the gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tkou6L2QZx", "forum": "NfuBj8jleE", "replyto": "NfuBj8jleE", "signatures": ["ICLR.cc/2026/Conference/Submission1714/Reviewer_Fvcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1714/Reviewer_Fvcs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186519327, "cdate": 1762186519327, "tmdate": 1762915864300, "mdate": 1762915864300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}