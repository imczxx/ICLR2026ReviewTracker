{"id": "CcYCgF491G", "number": 21561, "cdate": 1758319028412, "mdate": 1763669942718, "content": {"title": "Do AI models perform human-like abstract reasoning across modalities?", "abstract": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? In this work, we investigate the abstraction abilities of AI models using the ConceptARC benchmark. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation allows us to assess whether models solve tasks using the abstractions that ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based task representations match human output accuracy, the best models' rules are frequently based on surface-level ``shortcuts'', and capture intended abstractions substantially less often than do humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone.  In the visual modality, AI models' output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules.  In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities.  We believe that our evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence.", "tldr": "Pairing accuracy with rule evaluation shows models’ correct outputs frequently mask shortcut-based or shallow abstractive reasoning.", "keywords": ["ARC", "Multimodal Reasoning", "Abstraction", "Vision Language Models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/822266d08e9517873660deecbc1095a9bad881ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines whether high performance on ConceptARC tasks truly reflects human-like abstract reasoning. The authors evaluate several multimodal reasoning models across textual and visual input settings, vary reasoning depth and tool access, and analyze not only output accuracy but also the natural-language rules generated by the models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The question addressed is timely and relevant to the ongoing discussion regarding whether LLMs genuinely perform abstraction-based reasoning.\n2. The analysis goes beyond output accuracy and attempts to assess conceptual generalization by examining natural-language rule descriptions.\n3. The experimental matrix is comprehensive (modalities × reasoning depth × tool access × multiple models)."}, "weaknesses": {"value": "1. Although the paper provides a thorough and well-organized empirical analysis, it does not introduce a new method, evaluation metric, or conceptual framework. The contribution is therefore primarily diagnostic and observational, which limits the originality and impact.\n2. The conclusions drawn are largely descriptive summaries of observed performance patterns, rather than explanatory insights that deepen our understanding of reasoning mechanisms. The paper stops at “what happens” rather than addressing “why it happens” or “how this informs the design of future reasoning systems”.\n3. In visual reasoning, performance may be affected by perceptual encoding (object recognition, segmentation, spatial grouping) as well as abstraction and rule application. Unlike textual inputs, where the perceptual layer is absent, the visual results cannot be directly interpreted as reasoning failures. However, the paper does not experimentally disentangle these two sources of error, making the interpretation of results less reliable.\n4. Recent work on reasoning benchmarks has also studied compositional and spatial abstractions. The manuscript lacks a clear articulation of how its findings extend, differ from, or challenge existing conclusions in this line of work. This weakens the contribution framing."}, "questions": {"value": "Referencing the weaknesses, my main concern is about the contribution and novelty claim. The paper provides a thorough in-depth analysis, but it does not appear to introduce a novel method or evaluation approach, and the conclusions read more as descriptive characterizations rather than forming systematic guidance for model or benchmark design.\n\nIn addition, for the visual reasoning results, it may be helpful to more explicitly situate the work in the broader visual reasoning literature,  e.g., RAVEN (Zhang et al. 2019), CVR (Zerroug et al. 2022), Bongard problems, where abstract reasoning has been a central theme for many years.\n\nIf I have misunderstood the intended contribution or positioning, I would appreciate clarification from the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iOdEsLGRo5", "forum": "CcYCgF491G", "replyto": "CcYCgF491G", "signatures": ["ICLR.cc/2026/Conference/Submission21561/Reviewer_RFre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21561/Reviewer_RFre"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375733088, "cdate": 1761375733088, "tmdate": 1762941836658, "mdate": 1762941836658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether current multimodal reasoning models demonstrate genuine human-like abstraction on the ConceptARC benchmark. The authors evaluate several leading models, including OpenAI’s o3 and o4-mini, Gemini 2.5 Pro, and Claude Sonnet 4, across both textual and visual modalities, varying reasoning effort and the use of external Python tools. Their method introduces a dual-level evaluation framework that measures not only output-grid accuracy but also the alignment of model-generated natural-language rules with the intended abstract concepts of each task. The study finds that while models can match or surpass human accuracy in textual settings, many correct outputs are based on unintended, surface-level patterns. In contrast, visual performance remains much lower, though models sometimes identify correct abstract rules but fail to execute them properly. These findings suggest that accuracy-based benchmarks overestimate abstract reasoning in text and underestimate it in vision, calling for more nuanced evaluation of abstraction-centered reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a dual-level evaluation framework that jointly assesses output accuracy and abstraction quality, offering a more reliable measure of reasoning ability.\n2. The experiments are comprehensive and well-controlled, providing clear insights into how modality, reasoning effort, and tool use affect abstract reasoning in current multimodal models."}, "weaknesses": {"value": "Although the paper presents a thorough empirical investigation, its contribution remains largely descriptive and methodologically incremental. The analysis primarily focuses on the issue of shortcut reasoning, showing that models often rely on superficial cues such as color indices, pixel-level correlations, or numeric encodings rather than forming generalizable abstract concepts—an issue that has already been well documented in prior research [1]. The authors also mention related phenomena—such as the faithfulness gap between generated natural-language rules and internal reasoning processes, and the execution gap where models correctly identify abstract rules but fail to apply them, however, these observations are discussed qualitatively, without being developed into new analytical frameworks, theoretical explanations, or algorithmic solutions.\n\nThe experimental setup is fully based on the existing ConceptARC benchmark, using standard tasks, prompts, and accuracy-based evaluation metrics. The paper does not propose new datasets, task variations, or evaluation protocols that could better isolate or measure abstraction capabilities. As such, it is difficult to view the work as offering a fundamentally new evaluation framework. \n\nOverall, while the experiments are extensive and the findings informative, the paper provides limited methodological novelty and offers relatively few new insights into how to advance abstraction or reasoning in multimodal models.\n\n[1] Samuele Bortolotti et al., A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts. NeurIPS 2024."}, "questions": {"value": "How do the authors expect their findings to generalize to other reasoning benchmarks or domains (besides ConceptARC) that involve different types of abstraction (e.g., temporal, numerical reasoning, or others)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IJSkGr40CB", "forum": "CcYCgF491G", "replyto": "CcYCgF491G", "signatures": ["ICLR.cc/2026/Conference/Submission21561/Reviewer_TPSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21561/Reviewer_TPSZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558036988, "cdate": 1761558036988, "tmdate": 1762941836385, "mdate": 1762941836385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper  investigates the abstraction abilities of large SOTA reasoning AI models (o3, o4-mini, Google’s Gemini 2.5 Pro,  Anthropic’s Claude Sonnet 4) using the ConceptARC benchmark. They show that the  (1) using accuracy alone to evaluate human-like abstract reasoning is insufficient, due to the frequent use of shortcuts by the modes, and (2) analysis of the models' linguistic explanations of their solutions reveal that models often fall short of using human-like abstractions intended by ConceptARC design, even if the final output is correct."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and well grounded in cognitive theory.\n\nTheoretically sound experiment design. \n\nThis work is a broadly interesting, novel, and theoretically valuable contribution."}, "weaknesses": {"value": "The authors acknowledge that the study still relies on the natural language rule being a faithful description of the internal procedure."}, "questions": {"value": "1. It would be informative to understand failure modes, as they were being classified by the authors. For instance the authors note that (1) \"o3’s rules often focused on colours and individual pixels rather than objects\" and relying on integer number for colours, such as (2) \"value for green, 3, is greater than the value for red, 2\".  I'm not convinced that  (2) should be treated as a shortcut, because a human given the same puzzle, in text form only, could conceivably reason about number is the same way. \n\n2. Have the authors considered analyzing the models' reasoning in forms other than the text explanations, but perhaps including also the Python code that was used by the models, or asking the models to generate programmatic implementations of their solutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "300SkPgVLx", "forum": "CcYCgF491G", "replyto": "CcYCgF491G", "signatures": ["ICLR.cc/2026/Conference/Submission21561/Reviewer_sHsM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21561/Reviewer_sHsM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648494035, "cdate": 1761648494035, "tmdate": 1762941836084, "mdate": 1762941836084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper audits LLMs/VLMs on ConceptARC, asking whether model-generated “rules” reflect intended abstractions or shortcut heuristics. It compares text vs. vision settings, with/without external tools, and uses human labels to categorize rules (intended vs. “correct-unintended”). Main takeaways: text looks relatively strong but often rides shortcuts; vision lags largely due to execution/Per-pixel perception; tools and reflective feedback help somewhat."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Full human evaluation on model-produced solutions; appreciate the effort.\n\n2. Overall, easy to read."}, "weaknesses": {"value": "1. **Missing literature review / comparison to baselines**, which makes it hard to position the work—especially given the ARC-AGI motivation vs. ConceptARC-only experiments (ConceptARC is generally easier: smaller search space, limited OOD).\n\n\n2. **Novelty is limited.** While this paper is fully empirical, the main insights are already well-known in the community: LLMs struggle on complex multi-hop tasks (e.g., ARC-AGI2), models can be poor executors even with a correct NL instruction, VLM per-pixel recognition is a bottleneck, and feedback from an external executor (“Reflection”) helps.\n\n\n3. I disagree with the “correct-unintended” labeling example (lines 188–190). If a rule solves all demos, it should be treated as an alternative intended rule unless the demos explicitly disambiguate competing hypotheses. Priors are mentioned but not given for either ARC-AGI or ConceptARC about how to craft solutions; moreover, “per-pixel objectization” can be a valid abstraction in this paradigm. Please provide a sharper operational definition: e.g., classify as correct-unintended only when a rule fits all train demos but fails the held-out test specifically because the demos did not disambiguate (As in Figure 4 Middle)—under this criterion, the lines 188–190 example would not qualify since it also solves the test.\n\n\nAs written, the work lacks publishable research contribution or scholarly form: related work and baselines are missing, and the novelty is limited."}, "questions": {"value": "1. Given the identical setup, why not test on ARC-AGI or ARC-AGI2?\n\nMinor:\n1. Figure 2 legend box has dots overlaying text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p1Jlz0WGYI", "forum": "CcYCgF491G", "replyto": "CcYCgF491G", "signatures": ["ICLR.cc/2026/Conference/Submission21561/Reviewer_CGz5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21561/Reviewer_CGz5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853132198, "cdate": 1761853132198, "tmdate": 1762941835782, "mdate": 1762941835782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rigorous and thoughtful empirical investigation into how state-of-the-art multimodal reasoning models perform on the **ConceptARC** benchmark. Unlike previous evaluations that focused solely on grid-level accuracy in ARC-like problems, the authors analyze **whether models capture intended abstractions**, by jointly assessing (i) grid accuracy and (ii) natural-language rules generated by the models to explain their reasoning.\n\nThe study spans **textual and visual modalities**, **varying reasoning budgets**, and **tool-use settings** (with or without Python execution). The dual evaluation provides new insight into how much of models’ apparent success arises from genuine abstraction versus “shortcut” pattern matching. Key findings include:\n\n* Models such as o3 match or surpass human accuracy in textual settings, but perform far worse visually.\n* Even when textual outputs are correct, nearly 30% of o3’s reasoning rules reflect unintended or spurious abstractions.\n* In the visual modality, models often produce correct-intended rules but fail to apply them accurately—indicating procedural rather than conceptual limitations.\n* Allowing Python tool use substantially improves performance in visual settings, likely by leveraging external computer vision functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Timely and meaningful contribution.** The work directly addresses the gap between performance-based and concept-based measures of intelligence, an issue at the heart of ARC/ARC-AGI.\n* **Methodological novelty.** The dual-layer evaluation of grid accuracy and rule abstraction is methodologically elegant and exposes nuanced differences between human and model cognition.\n* **Comprehensive experimental design.** The study systematically controls for modality, reasoning effort, and tool access across four reasoning models and three baselines.\n* **Rich interpretability and cognitive framing.** The authors’ use of “intended vs. unintended rules” operationalizes abstraction fidelity in a manner that is both cognitively interpretable and empirically testable.\n* **Human-model comparison adds depth.** By aligning human rule data with model-generated rules, the paper quantifies abstraction alignment rather than relying solely on raw success rates."}, "weaknesses": {"value": "* **Limited interpretability of rule faithfulness.** While natural-language rules correlate with generated outputs, there is no quantitative measure of *rule–behavior alignment*. This weakens the claim that models “used” those abstractions.\n* **Subjectivity in annotation.** Rule classification (correct-intended / unintended / incorrect) relies on manual consensus; inter-rater reliability or annotation protocol statistics are missing.\n* **Incomplete exploration of reasoning-effort scaling.** High-effort o3 and extended token budgets were omitted due to cost, leaving open whether abstraction quality scales further.\n* **Under-analyzed abstraction bottleneck.** The paper identifies “shortcut” reasoning but offers little mechanistic or representational explanation (e.g., which components cause pixel- or color-based shortcuts)."}, "questions": {"value": "Please see the weakness above.\n1. Do you encounter issues with annotation consistency or ambiguity in classifying rules, and how was inter-rater agreement verified?\n2. How sensitive are the results to prompt design, especially in the *visual* modality where slight wording changes might affect spatial parsing?\n3. Have you examined whether certain concept groups (e.g., *CleanUp*, *ExtendToBoundary*) systematically elicit shortcut reasoning, suggesting task-type bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gd5916OheZ", "forum": "CcYCgF491G", "replyto": "CcYCgF491G", "signatures": ["ICLR.cc/2026/Conference/Submission21561/Reviewer_abxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21561/Reviewer_abxG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955467117, "cdate": 1761955467117, "tmdate": 1762941835580, "mdate": 1762941835580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}