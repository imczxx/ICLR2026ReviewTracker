{"id": "BsZeTuB5fD", "number": 8682, "cdate": 1758094941424, "mdate": 1759897769825, "content": {"title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models", "abstract": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect tokens: once a token is generated, it typically remains fixed. The key challenge is to identify potential errors in the inputs. In this paper, we propose Remasking-enabled Diffusion Language Model (RemeDi), a mask-based DLM that introduces remasking as another fundamental mechanism, enabling more flexible text refinement in diffusion-based text generation. To achieve this, RemeDi jointly predicts token distributions and per-token confidence scores at each step. The confidence scores determine which tokens to be unmasked after the current step, allowing the model to identify tokens with low quality and remask them. These remasked tokens can be resampled with richer context in subsequent steps. We design a remask-aware pipeline to train this ability, including supervised fine-tuning which teaches the model to detect and remask incorrect tokens in addition to predict mask tokens, and reinforcement learning which optimizes full generation trajectories toward higher rewards. Experiments show that RemeDi achieves the state-of-the-art results among open-source DLMs on multiple datasets.", "tldr": "We propose RemeDi, a new diffusion-based text generation model that introduces remasking allowing model to detect and resample low-confidence tokens during generation.", "keywords": ["diffusion language model", "discrete diffusion", "masked diffusion model", "language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b5eb11772c0f45a9cda34a11cd402f7372cf2f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **RemeDi (Remasking-enabled Diffusion Language Model)**, which adds a self-reflective remasking mechanism to diffusion language models (DLMs). Traditional DLMs fix tokens once unmasked, making them unable to revise early mistakes. RemeDi introduces a dual-stream transformer: a **Token Prediction Stream (TPS)** predicts token distributions, while an **Unmasking Policy Stream (UPS)** predicts per-token confidence. Low-confidence tokens are re-masked and regenerated in later steps, allowing iterative self-correction. Training proceeds in two stages: **Remask SFT** (supervised fine-tuning) teaches the model to identify and remask incorrect tokens using mixed masking and random replacement; **Remask RL** fine-tunes full trajectories using task-specific rewards. Experiments on code, math, and reasoning benchmarks show that RemeDi surpasses prior DLMs (Dream, LLaDA, LLaDOU) and even matches or exceeds autoregressive baselines of similar size.\n\nHowever, despite the promising empirical results, the paper exhibits several methodological and reporting weaknesses. Many experimental details are under-specified: the reward design for Remask RL is not clearly defined, and critical hyperparameters or normalization schemes are absent, even in the appendix. The paper does not report any compute or training-time comparisons, leaving unclear whether the observed gains stem from algorithmic innovation or simply greater computational cost. Similarly, convergence curves and efficiency analyses are missing, despite claims of faster training. The method section, while conceptually sound, omits low-level implementation details—such as how the UPS interacts with TPS during training or how remask thresholds are selected—making reproduction difficult. While the writing is clear at a high level, the technical exposition lacks sufficient depth and cross-references to equations and figures, resulting in incomplete methodological transparency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addresses a concrete limitation of DLMs** — RemeDi directly tackles the inability to revise early decoding errors through a principled, learnable remasking mechanism.\n2. **Clear and interpretable design** — The dual-stream architecture (TPS + UPS) separates token prediction from confidence estimation, enabling transparent remasking decisions.\n3. **Comprehensive experiments** — Evaluations cover math (GSM8K, MATH), code (HumanEval, MBPP), and reasoning tasks (ARC-C, AlpacaEval), consistently improving over Dream and LLaDA.\n4. **Empirical evidence for self-correction** — Qualitative examples and remask frequency analysis show that RemeDi learns to remask more frequently on harder or more structured tasks.\n5. **Training stability and convergence** — Compared to LLaDOU RL, Remask RL converges faster and achieves higher accuracy under identical settings."}, "weaknesses": {"value": "1. **Limited quantitative comparison to other edit-based diffusion models** (e.g., ReMDM, Seed Diffusion). The paper cites prior edit-based diffusion models (e.g., ReMDM, Seed Diffusion) but omits quantitative comparisons, leaving unclear how RemeDi’s improvements scale relative to these baselines.\n2. **Under-specified reward formulation in Remask RL** — lacks details on normalization, weighting, or how rewards interact with confidence-based remasking.\n3. **Compute and efficiency concerns** — The two-stage SFT → RL training likely increases overall cost, but compute parity with baselines is unreported, making it unclear whether improvements stem from algorithmic design or added compute.\n4. **Figure placement and clarity** — Key visualizations (like the dynamic remasking behavior) appear in the appendix instead of the main paper.\n5. **Missing convergence curves** — The claim that Remask RL converges as efficiently as other DLMs lacks quantitative support.\n6. **Necessity of two-stage training unclear** — It is not shown whether a stronger SFT baseline or a joint training objective could achieve similar effects.\n7. **Stability and variance during RL** — The paper lacks reporting on reward variance, remask rate dynamics, or regularization methods.\n8. **Ablation under matched compute not provided** — It is unclear whether RL brings improvement beyond additional training time."}, "questions": {"value": "1. **Comparison scope:** Why were models like *ReMDM* and *Seed Diffusion* not included in quantitative comparisons? Could a smaller-scale reimplementation or proxy be feasible?\n2. **Reward design:** Please provide explicit formulas or examples of the reward functions used in the RL phase and clarify normalization or scaling strategies.\n3. **Effectiveness of RL:** How does the RL-trained model differ behaviorally from the SFT-only version (e.g., fewer unnecessary remasks, faster correction)?\n4. **Visualization and convergence:** Could you move the dynamic remasking visualization (currently in appendix) to the main paper and add training loss curves to verify efficiency claims?\n5. **Compute accounting:** Please provide compute cost (GPU hours, wall time, token count) for both stages. How does SFT-only compare to SFT→RL at equal compute?\n6. **Necessity of two-stage design:** Did you test a single-stage joint objective or stronger SFT baseline under matched compute?\n7. **Stability:** What regularization methods (e.g., KL penalty, reward whitening, gradient clipping) were applied to stabilize RL?\n8. **Matched compute ablation:** Could you run an SFT-only baseline using the same compute as SFT→RL to confirm that RL brings genuine benefit?\n9. **Generalization to other DLMs:** Since the model is based on LLaDA-8B, could this remasking mechanism generalize to other diffusion LMs such as Dream? If so, what architectural adjustments are needed?\n10. **Random alternative tokens:** In SFT, are random alternatives drawn uniformly from the vocabulary or sampled from the model’s top-k predictions?\n11. **Inference efficiency:** The paper states RemeDi achieves faster convergence; could you include **inference latency comparisons** (e.g., tokens per second vs. LLaDA), given that TPS + UPS may add computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jvJ5g39hks", "forum": "BsZeTuB5fD", "replyto": "BsZeTuB5fD", "signatures": ["ICLR.cc/2026/Conference/Submission8682/Reviewer_AeNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8682/Reviewer_AeNa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792965822, "cdate": 1761792965822, "tmdate": 1762920496459, "mdate": 1762920496459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates remasking as a way to improve masked diffusion language models. Remasking works by replacing some already-unmasked tokens with mask tokens, which subsequently get unmasked again. This way, uncertain or incorrect tokens can be revised and potentially fixed, thereby improving accuracy with minimal additional cost.\n\nThe proposed method, called RemeDi, introduces two new policies to identify the tokens to be remasked, one based on supervised fine-tuning (SFT) and one based on reinforcement learning (RL). This is done via a dual-stream transformer architecture: The token prediction stream (TPS) predicts unmasked tokens while the unmasking policy stream (UPS) predicts per-token confidence scores (for both masked and unmasked tokens). The resulting model achieves significant gains on a variety of benchmarks compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "### S1. Strong benchmark performance\nThe proposed model achieves strong benchmark performance, beating many diffusion and autoregressive baselines of similar size.\n\n### S2. Flexible post-training method\nThe proposed method is flexible enough to be generally applicable to many existing masked diffusion models, and can be combined with other optimizations such as block diffusion.\n\n### S3. Analysis of remasking behavior\nThe analysis of the remasking behavior of the trained policies provides valuable insights. It sheds light into task difficulty as well as which part of the sampling process may be most prone to errors."}, "weaknesses": {"value": "(ordered by decreasing severity)\n\n### W1. Source of performance improvements is unclear\nIt is somewhat unclear which improvements stem from the continued (multi-task) pretraining and which come from the remasking during sampling. This question is partially addressed by comparing Remask SFT with vanilla SFT and Remask RL with LLaDOU RL, but some confounders remain. The multitask objective employed by Remask SFT may be beneficial even in a vanilla (or adaptive) inference setting where the model confidence is obtained heuristically. Similarly, the Remask RL approach is very similar to the one proposed by LLaDOU (Huang et al., 2025), so it’s unclear where exactly the improvements come from.\nThe former can easily be addressed by reporting the performance of Remask SFT using vanilla and adaptive masked diffusion sampling (Kim et al., 2025). The latter can be addressed by providing a detailed analysis of the differences between Remask RL and LLaDOU RL and, if necessary, ablating atomic changes to measure their individual impact (e.g. training vs. sampling improvements).\n\n### W2. Reproducibility\nAs far as I can tell, there will be no model weights or codebase accompanying the paper, which is a major concern for reproducibility, especially given the clarity concerns regarding the source of the performance gains (W1).\n\n### W3. Motivation and effect of dual-stream architecture\nThe motivation for using a dual-stream architecture over conceptually simpler approaches (e.g. a dedicated unmasking policy head) is unclear, and no ablations are performed on this. Similarly, the computational overhead of the proposed architecture (more parameters, slower forward pass) may provide an unfair advantage compared to baselines. The former can be addressed by providing appropriate ablations, whereas the latter calls for reporting inference speed of the proposed method compared to baselines in addition to benchmark accuracies.\n\n### Conclusion\nAs presented, reasons to reject the paper outweigh reasons to accept: While the benchmark performance of the proposed model is impressive and beats state-of-the-art methods (S1), the lack of clarity regarding the source of those gains (W1) together with concerns about reproducibility (W2) and inference speed (W3) amount to significant concerns regarding soundness. I will be happy to increase my final score if these weaknesses can be addressed."}, "questions": {"value": "- Q1. What is the effect of the multi-task objective (Eq. 5) together with incorrect token augmentation (Eq. 3)? More specifically, what is the performance of Remask SFT when using standard vanilla/adaptive sampling (Kim et al., 2025) compared to full Remask SFT? (also see W1)\n- Q2. What is the difference between Remask RL and LLaDOU RL (Huang et al., 2025)? Specifically, is the sampling policy in Eq. 8 not identical to the one from Eq. 9 in LLaDOU? If so, where do the improvements of Remask RL over LLaDOU come from? (also see W1)\n- Q3. Will the model weights and/or training code be open-sourced? (also see W2)\n- Q4. How does the proposed dual-stream architecture compare to more naive baselines, e.g. a single unmasking policy head or a single additional Transformer block (as employed by LLaDOU)? (also see W3)\n- Q5. What is the performance overhead associated with RemeDi compared to baselines? (also see W3)\n\nNits (not considered for final score):\n- L31: Austin et al. (2022) should be cited for discrete diffusion.\n- L34: Sahoo et al. (2024) and Shi et al. (2024) should be cited for masked diffusion.\n- L103: typo: missing space after “Recent studies”\n- L160: typo: “paradigm offers” -> “paradigms offer”\n- L203: typo: extra space after “(Guo et al., 2025)”\n\n---\n\n### References\n- Austin et al. (2022): https://arxiv.org/abs/2107.03006\n- Huang et al. (2025): https://arxiv.org/abs/2505.10446\n- Kim et al. (2025): https://arxiv.org/abs/2502.06768\n- Sahoo et al. (2024): https://arxiv.org/abs/2406.07524\n- Shi et al. (2024): https://arxiv.org/abs/2406.04329"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZElyofPZEl", "forum": "BsZeTuB5fD", "replyto": "BsZeTuB5fD", "signatures": ["ICLR.cc/2026/Conference/Submission8682/Reviewer_Z76m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8682/Reviewer_Z76m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827516776, "cdate": 1761827516776, "tmdate": 1762920495800, "mdate": 1762920495800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RemeDi augments diffusion LMs with a learnable remasking policy: a per-token confidence head (UPS) decides which tokens to unmask or re-mask, trained via remask-SFT then trajectory-level RL, yielding consistent gains on math/code/general tasks under block-wise variable-length decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Turns remasking from an inference hack into a learned policy; per-token confidence (UPS) guides when to unmask or re-mask.\n2. Two-stage training (remask-SFT + RL) aligns token-level corrections with sequence-level rewards; compatible with monotonic denoising.\n3. Works atop block-wise variable-length decoding (LLaDA-style); integrates without changing the base noise family.\n4. Confidence head provides interpretable signals and a knob to trade exploration vs. commitment.\n5. Orthogonal to many decoding/acceleration tricks; likely to stack with other DLM advances."}, "weaknesses": {"value": "1. No clear accounting of UPS params/FLOPs/memory or train/infer throughput; lacks equal-quality step/latency comparisons to AR/other DLMs.\n2. Missing UPS structure/attachment ablations (bi-residuals, zero-init bridge, layer choice) and sensitivity to ρ_(t,\"incorrect\" )  and ratio r.\n3. RL uses preference and verifiable rewards; robustness and transfer to new domains remain uncertain.\n4. Sparse head-to-head with edit-flow/seed-diffusion/predictor-corrector under identical settings.\n5. Potential remask oscillation (low-confidence flip-flops); termination/annealing not systematically reported.\n6. Higher engineering complexity (dual streams + two-stage training) raises adoption barriers."}, "questions": {"value": "1. Report UPS overhead (params/FLOPs/memory) and equal-quality latency/tokens-per-second vs. AR/strong DLM baselines, with quality–latency Pareto curves on fixed hardware.\n2. Provide ablations for UPS attachments and components (bi-residuals, zero-init bridge, layer choice) plus sensitivity sweeps of (\\rho_{t,\\text{incorrect}}) and ratio (r).\n3. Decompose reward contributions, test cross-domain transfer without retuning, and include robustness/calibration (e.g., ECE) under distribution shift.\n4. Add unified head-to-head comparisons with edit-flow, seed-diffusion, and predictor-corrector using identical checkpoints, datasets, and decoding budgets.\n5. Quantify remask oscillation and compare termination/annealing (fixed/decayed thresholds, hysteresis) to show stability–quality trade-offs.\n6. Summarize engineering cost (LoC, stages, time, hardware) and release a minimal plug-and-play UPS recipe with an inference-only lightweight variant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N9KtOzedfD", "forum": "BsZeTuB5fD", "replyto": "BsZeTuB5fD", "signatures": ["ICLR.cc/2026/Conference/Submission8682/Reviewer_arzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8682/Reviewer_arzr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891208928, "cdate": 1761891208928, "tmdate": 1762920495343, "mdate": 1762920495343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper diagnoses a structural limitation of mask-based diffusion language models (DLMs)—namely that once a token is unmasked it becomes effectively immutable, which prevents the model from performing self-reflection and post-hoc correction of its outputs. To mitigate this, the authors propose a self-reflective remasking mechanism (RemeDi) that augments the diffusion process with learnable per-token confidence scores and an explicit remasking operation that can reintroduce previously unmasked tokens for further refinement. They train the mechanism with a two-stage procedure—supervised Remask SFT followed by policy-style Remask RL—so the model learns when and how to re-mask and revise. Crucially, RemeDi preserves the diffusion model’s noise-monotonicity (the monotone decrease of corruption during denoising) while enabling iterative edit-and-refine behavior, and the paper reports substantial empirical improvements on mathematical reasoning, code generation, and general language tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. RemeDi enables self-reflective iterative refinement by jointly predicting per-token confidence scores and remasking low-confidence tokens so that previously revealed tokens can be selectively re-sampled without breaking the diffusion noise schedule.\n\n2. The paper introduces a principled two-stage training pipeline, with supervised Remask SFT to teach detection and remasking and outcome-based Remask RL to optimize whole-generation trajectories, which yields stronger performance.\n\n3. The remasking mechanism supports a wide range of edit behaviors such as replacement, insertion, deletion, merging and splitting, and produces empirical gains on mathematics, code generation, and general instruction benchmarks compared to prior open-source diffusion language models."}, "weaknesses": {"value": "1. No reproduction code is provided; please supply an anonymous repository link or include the code in the supplementary materials.\n2. The model diagram in the appendix is unclear — please clarify how $p$ and $h$ are predicted simultaneously and whether any new modules were introduced.\n3. Please add a baseline that performs RL directly on LLaDA using the same datasets you used."}, "questions": {"value": "1. I suggest moving some experimental results to the appendix and placing the main architecture figure in the main text.\n2. Why did you run experiments with LLaDA rather than LLaDA 1.5?\n3. Is inference latency substantially slower compared to the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L2Q2DFJkqS", "forum": "BsZeTuB5fD", "replyto": "BsZeTuB5fD", "signatures": ["ICLR.cc/2026/Conference/Submission8682/Reviewer_4oJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8682/Reviewer_4oJS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147915284, "cdate": 1762147915284, "tmdate": 1762920494965, "mdate": 1762920494965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}