{"id": "Z2w69IZqSW", "number": 16399, "cdate": 1758264225832, "mdate": 1763460359387, "content": {"title": "Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts", "abstract": "Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2â€“3$\\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.", "tldr": "", "keywords": ["Mixture-of-Experts", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/153748eba15b6c13f8502c83bd7d3242561c384f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper postulates that elastic allocation of compute in MoE layers (e.g., activating more experts per token than in the training setup) can provide additional flexibility for inference. However, as the authors show, the naive approach, where we just increase the top-k in the router, does not result in model quality improvement. The authors hypothesize that the reason is lack of collaboration between experts - the model is not able to leverage new combinations of experts not seen during training. The proposed solution is to \"simulate\" training with larger top-k by sampling smaller groups of experts from larger, \"ideal\" groups, that will be seen during inference. The paper shows that when this method is in use, increasing router top-k to ks larger than seen during training is significantly more effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The motivation is clear. The paper tackles a natural problem, provides convincing reasoning on its nature, and proposes a solution.\n3. The methodology is consistent and the results are positive.\n4. Evaluation is based on three open-source models and two training scenarios (LoRA and MoE FFN)."}, "weaknesses": {"value": "1. Examination of the proposed approach is limited to training on the small instruction-tuning dataset.\n2. The instruction fine-tuning is extremely lightweight (only 50K samples). Let's say we have an MoE model trained with topk=2. If we want to infer it with topk=4, we could just directly fine-tune on top-k=4, instead of using co-activation sampling, where we sample 2 experts from groups of size 4. The argument of the reduced training cost with stochastic co-sampling is not relevant since the entire fine-tuning phase has negligible cost compared to the model pretraining. This weakness is connected to weakness 1. mentioned above.\n3. The introduction of hierarchical router loss feels a bit orthogonal to the main body of the paper, especially given results in the Section 5.3, showing that the main ingredient contributing to the benefit of EMoE is co-activation sampling."}, "questions": {"value": "1. Did authors consider evaluating the proposed technique in the pretraining or continued pretraining setup?\n2. In Table 1, there is a comparison of models trained with k experts when using k'>k during inference. Could the authors add to the comparison the model fine-tuned on the target k'? So for example, in the second group of Table 1, I would love to see a comparison between EMoE(k'=16) and OLMoE-1B-7B-0924 directly fine-tuned on topk=16. This model directly fine-tuned with topk=16 would technically have a larger cost, but as mentioned before, any cost of fine-tuning on the small dataset can be considered negligible, so ultimately the best approach is the one giving the best performance after tuning on this dataset. \n3. The authors may also comment or provide additional experiments on the points mentioned in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6qi5uEcGcH", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Reviewer_rgdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Reviewer_rgdX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909084554, "cdate": 1761909084554, "tmdate": 1762926521562, "mdate": 1762926521562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the topic of improving the inference-time scalability of Mixture-of-Experts (MoE) models. The authors observe that standard MoE architectures, when trained with a fixed number of active experts (k), tend to perform poorly if a larger number (kâ€²>k) is used at inference. They identify the cause as insufficient co-training of expert combinations and propose Elastic Mixture-of-Experts (EMoE) to mitigate this issue. EMoE combines (1) stochastic co-activation sampling, which encourages expert collaboration across diverse subsets without additional training cost, and (2) a hierarchical router loss that produces more stable expert rankings. The experiments show that EMoE models trained with small k exhibit improved performance when evaluated with higher kâ€², outperforming standard Top-k and dynamic routing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The topic is very timely and important, as efficient scaling and inference-time flexibility in MoE models are of central interest to current LLM research.\n* The paper is clearly written and presents the motivation and results in a structured way.\n* The empirical evaluation is thorough and includes ablations isolating the effects of the proposed components.\n* The approach introduces almost no additional training cost and could be easily adopted in existing MoE frameworks."}, "weaknesses": {"value": "The main conceptual limitation is the lack of comparison with a model trained with a larger number of experts. Figure 2 in the paper itself shows that when a model is trained with higher k_train, it already generalizes better to larger kâ€² at inference. This raises an important baseline question: if we simply train the model for larger k, would that not achieve the same or better performance than EMoE, possibly without additional algorithmic complexity? \n\nSince EMoE aims to emulate large-k behavior while training with small k, the natural control experiment would be: train standard MoE with k_train = 6 or 8, evaluate it at comparable inference budgets, and compare both training cost and resulting performance. Without this, it is difficult to assess whether EMoEâ€™s advantage stems from genuine elasticity or from the baselineâ€™s artificially small k. In practice, the training-cost versus performance trade-off determines real-world usefulness, and that dimension is underexplored."}, "questions": {"value": "1. How does EMoE compare to a model trained directly with a higher k_train, both in performance and in compute cost?\n2. Would the claimed benefits persist if training compute were scaled accordingly (i.e., fair total-FLOP comparison)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gv4Je08Er5", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Reviewer_kgGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Reviewer_kgGr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933975549, "cdate": 1761933975549, "tmdate": 1762926521215, "mdate": 1762926521215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the topic of improving the inference-time scalability of Mixture-of-Experts (MoE) models. The authors observe that standard MoE architectures, when trained with a fixed number of active experts (k), tend to perform poorly if a larger number (kâ€²>k) is used at inference. They identify the cause as insufficient co-training of expert combinations and propose Elastic Mixture-of-Experts (EMoE) to mitigate this issue. EMoE combines (1) stochastic co-activation sampling, which encourages expert collaboration across diverse subsets without additional training cost, and (2) a hierarchical router loss that produces more stable expert rankings. The experiments show that EMoE models trained with small k exhibit improved performance when evaluated with higher kâ€², outperforming standard Top-k and dynamic routing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The topic is very timely and important, as efficient scaling and inference-time flexibility in MoE models are of central interest to current LLM research.\n* The paper is clearly written and presents the motivation and results in a structured way.\n* The empirical evaluation is thorough and includes ablations isolating the effects of the proposed components.\n* The approach introduces almost no additional training cost and could be easily adopted in existing MoE frameworks."}, "weaknesses": {"value": "The main conceptual limitation is the lack of comparison with a model trained with a larger number of experts. Figure 2 in the paper itself shows that when a model is trained with higher k_train, it already generalizes better to larger kâ€² at inference. This raises an important baseline question: if we simply train the model for larger k, would that not achieve the same or better performance than EMoE, possibly without additional algorithmic complexity? \n\nSince EMoE aims to emulate large-k behavior while training with small k, the natural control experiment would be: train standard MoE with k_train = 6 or 8, evaluate it at comparable inference budgets, and compare both training cost and resulting performance. Without this, it is difficult to assess whether EMoEâ€™s advantage stems from genuine elasticity or from the baselineâ€™s artificially small k. In practice, the training-cost versus performance trade-off determines real-world usefulness, and that dimension is underexplored."}, "questions": {"value": "1. How does EMoE compare to a model trained directly with a higher k_train, both in performance and in compute cost?\n2. Would the claimed benefits persist if training compute were scaled accordingly (i.e., fair total-FLOP comparison)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gv4Je08Er5", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Reviewer_kgGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Reviewer_kgGr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933975549, "cdate": 1761933975549, "tmdate": 1763591481639, "mdate": 1763591481639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Elastic Mixture-of-Experts (EMoE), a training strategy that enables post-hoc inference scalability in sparse MoE architectures. By introducing stochastic co-activation sampling and a hierarchical router loss, the authors aim to alleviate the mismatch between the training-time expert co-occurrence distribution and the inference-time routing patterns when increasing the number of activated experts. Experiments on several MoE backbones (LoRA-MoE, OLMoE, DeepSeek-V2-Lite) demonstrate smoother performance scaling and moderate improvements under larger inference budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work systematically investigates the phenomenon that scaling up the number of activated experts at inference time causes performance degradation, which is empirically well presented.\n\n- The proposed stochastic sampling mechanism and hierarchical router regularization are simple to implement and compatible with existing MoE frameworks.\n\n- The method yields clear empirical gains with nearly no additional training cost, offering some practical relevance for deployment scenarios with dynamic compute budgets."}, "weaknesses": {"value": "- The paper does not provide sufficient comparison against directly fine-tuning or training a model with a larger number of active experts ð¾ under the same data and compute budget. Empirically, the reported improvements are modest (mostly 1â€“2 points) and statistically insignificant relative to directly increasing ð¾. Since the training cost difference is marginal, simply fine-tuning with a larger  ð¾ remains a simpler and equally effective alternative, which undermines the necessity and originality of EMoE.\n\n- The proposed stochastic co-activation sampling and hierarchical router loss offer only minor methodological variations over existing ideas such as DropMoE, expert dropout, or router regularization. \n\n- The elasticity of EMoE is restricted: the method performs reasonably only within roughly 2â€“3Ã— of the training (K_{\\text{train}}); performance deteriorates beyond this range. The method is also sensitive to the hyperparameter (K_{\\text{ideal}}), requiring task-specific tuning. Moreover, EMoE does not address fundamental MoE bottlenecks such as communication overhead or expert load imbalance, raising doubts about its scalability and practicality for large-scale deployment."}, "questions": {"value": "- How does EMoE fundamentally differ from directly fine-tuning or training a model with a larger number of active experts?\n\n- Can the authors provide quantitative evidence that stochastic co-activation sampling improves expert collaboration, rather than simply introducing noise? For example, can you measure diversity or mutual information across experts before and after applying EMoE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LvZkBewxHk", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Reviewer_UsPv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Reviewer_UsPv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223731363, "cdate": 1762223731363, "tmdate": 1762926520865, "mdate": 1762926520865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Typically, MoE models use a fixed number of experts (k) during both training and inference. The authors identify that if one tries to activate more experts at inference (kâ€² > k), performance unexpectedly drops after a slight increase, rather than improving. They trace this to a lack of learned collaboration among experts beyond the combinations seen during training. The paper proposes (1) Stochastic co-activation sampling, which during training randomly samples diverse combinations of experts (from a larger candidate pool) to co-activate, thereby teaching experts to work together in various groupings. (2) Hierarchical router loss, a regularization term (based on KL divergence) that encourages the routerâ€™s output distribution to be sharp (non-uniform), producing a clear expert ranking for each token. EMoE is shown to extend the inference expert count to about 2â€“3Ã— the training count effectively."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Well-jusified problem to develop a method that offers large-k flexibility at inference, while avoiding its training cost. A good motivation example.\n\nThe authors convincingly diagnose this as a result of experts never having learned to work together in those larger groupings.\n\nDuring training, instead of always using the top-k experts, EMoE occasionally samples a small subset from a larger top-$k_{ideal}$ pool. It is a simple but innovative solution.\n\nThe paper is well-written, clearly explained.\n\nThe experiments are extensive (Moe using FFN and Lora, nine diverse tasks). EMoE-trained models exhibit monotonic improvement as the number of inference experts increases, eliminating the drop seen in standard MoEs."}, "weaknesses": {"value": "The motivation of the hierarchical router loss in view of the entire paper that rather focuses on dealing with the problem of lack of collaboaration is a little weak.\n\nOne missing comparison is to a model trained with a higher k from the start. For instance, if we train a model with k_train = 4 (using 4 experts per token) and use 4 at inference, how does that compare to an EMoE model trained with k_train = 2 but using 4 at inference?"}, "questions": {"value": "How does the method behave beyong the scenario with more than 3x experts at inference?\n\nAre there any disadvanatges of having a too sharp expert distribution? given the theme of the paper, could making the router too sharp hurt diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VaI8bW4lyO", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Reviewer_hP7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Reviewer_hP7x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762278650759, "cdate": 1762278650759, "tmdate": 1762926520253, "mdate": 1762926520253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for the detailed, constructive, and highly valuable feedback. Your comments were exceptionally thorough and have significantly improved the quality of this work.\n\nTo address the key concerns, particularly the request for direct comparisons with larger k_train baselines, clearer motivation for the hierarchical routing loss, empirical evidence at larger data scales, and broader validation, we have made the following revisions:\n\n## Experimental Additions\n\n- **Added larger k_train baselines in Table 5 (Page 18).**\n    \n    We now include a direct comparison between the standard Top-k baseline (k_train=6) and EMoE (k_train=2).\n    \n    The results show that while the k_train=6 model performs comparably to EMoE at its native budget (k' = 6), it suffers a sharp degradation at lower budgets (k' = 2), and also requires substantially higher training FLOPs and peak GPU memory.\n    \n- **Added an analysis of the Hierarchical Routing Loss ($\\mathcal{L}\\_\\text{HR}$) in Figure 7 (Page 10).**\n    \n    We visualize its effect on expert co-activation matrices and quantify the entropy changes on GSM8K and HellaSwag.\n    \n    These results demonstrate that $\\mathcal{L}\\_\\text{HR}$ significantly improves routing consistency under low-budget inference.\n    \n- **Added EMoE (k_train) scaling results in Figure 8 (Page 10).**\n    \n    Using higher training budgets (k_train=3) and (k_train=4), we show that both peak performance and elasticity continually improve.\n    \n- **Added experiments on a 21B-parameter MoE model (ERNIE-4.5-21B-A3B) in Table 1 (Page 7).**\n    \n    The results, included in Table 1, confirm that EMoE remains effective at much larger parameter scales, demonstrating strong scaling with model size.\n    \n- **Added experiments using larger instruction-tuning datasets (100K and 200K) in Table 6 (Page 18).**\n    \n    We confirm that the elasticity window remains stable even when the data scale is increased by 4Ã—, indicating that EMoE is applicable across different dataset sizes, and, notably, elasticity can be achieved with as few as 50K samples, as shown in the main experiments.\n    \n- **Added mutual-information analyses across mathematical, commonsense, and coding tasks (GSM8K / HellaSwag / HumanEval) in Table 7 (Page 19).**\n    \n    These results show that both EMoE sub-methods consistently improve expert specialization across domains.\n    \n\n## Textual Revisions\n\n- Revised **Figure 1 and its explanation** to more clearly highlight the performance collapse of large k_train baselines under low inference budgets (Introduction, Sec 3.2).\n- Clarified the **motivation for the hierarchical routing loss**, emphasizing that it complements stochastic co-activation sampling and addresses inaccurate routing under low-budget inference.\n- Revised **Figures 5 and 10** to more clearly illustrate the flexibility of EMoE across a wide range of hyperparameter configurations.\n\nThank you once again for your insightful comments."}}, "id": "qVc1gpIGYf", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763462832779, "cdate": 1763462832779, "tmdate": 1763468801770, "mdate": 1763468801770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for the detailed, constructive, and highly valuable feedback. Your comments were exceptionally thorough and have significantly improved the quality of this work.\n\nTo address the key concerns, particularly the request for direct comparisons with larger k_train baselines, clearer motivation for the hierarchical routing loss, empirical evidence at larger data scales, and broader validation, we have made the following revisions:\n\n## Experimental Additions\n\n- **Added larger k_train baselines in Table 5 (Page 18).**\n    \n    We now include a direct comparison between the standard Top-k baseline (k_train=6) and EMoE (k_train=2).\n    \n    The results show that while the k_train=6 model performs comparably to EMoE at its native budget (k' = 6), it suffers a sharp degradation at lower budgets (k' = 2), and also requires substantially higher training FLOPs and peak GPU memory.\n\n- **Added EMoE (k_train) scaling results in Figure 8 (Page 10).**\n    \n    Using higher training budgets (k_train=3) and (k_train=4), we show that both peak performance and elasticity continually improve.\n    \n- **Added an analysis of the Hierarchical Routing Loss ($\\mathcal{L}\\_\\text{HR}$) in Figure 7 (Page 10).**\n    \n    We visualize its effect on expert co-activation matrices and quantify the entropy changes on GSM8K and HellaSwag.\n    \n    These results demonstrate that $\\mathcal{L}\\_\\text{HR}$ significantly improves routing consistency under low-budget inference.\n    \n- **Added experiments on a 21B-parameter MoE model (ERNIE-4.5-21B-A3B) in Table 1 (Page 7).**\n    \n    The results, included in Table 1, confirm that EMoE remains effective at much larger parameter scales, demonstrating strong scaling with model size.\n    \n- **Added experiments using larger instruction-tuning datasets (100K and 200K) in Table 6 (Page 18).**\n    \n    We confirm that the elasticity window remains stable even when the data scale is increased by 4Ã—, indicating that EMoE is applicable across different dataset sizes, and, notably, elasticity can be achieved with as few as 50K samples, as shown in the main experiments.\n    \n- **Added mutual-information analyses across mathematical, commonsense, and coding tasks (GSM8K / HellaSwag / HumanEval) in Table 7 (Page 19).**\n    \n    These results show that both EMoE sub-methods consistently improve expert specialization across domains.\n    \n\n## Textual Revisions\n\n- Revised **Figure 1 and its explanation** to more clearly highlight the performance collapse of large k_train baselines under low inference budgets (Introduction, Sec 3.2).\n- Clarified the **motivation for the hierarchical routing loss**, emphasizing that it complements stochastic co-activation sampling and addresses inaccurate routing under low-budget inference.\n- Revised **Figures 5 and 10** to more clearly illustrate the flexibility of EMoE across a wide range of hyperparameter configurations.\n\nThank you once again for your insightful comments."}}, "id": "qVc1gpIGYf", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763462832779, "cdate": 1763462832779, "tmdate": 1763521157272, "mdate": 1763521157272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for the detailed, constructive, and highly valuable feedback. Your comments were exceptionally thorough and have significantly improved the quality of this work.\n\nTo address the key concerns, particularly the request for direct comparisons with larger k_train baselines, clearer motivation for the hierarchical routing loss, empirical evidence at larger data scales, and broader validation, we have made the following revisions:\n\n## Experimental Additions\n\n- **Added larger k_train baselines in Table 5 (Page 18).**\n    \n    We now include a direct comparison between the standard Top-k baseline (k_train=6) and EMoE (k_train=2).\n    \n    The results show that while the k_train=6 model performs comparably to EMoE at its native budget (k' = 6), it suffers a sharp degradation at lower budgets (k' = 2), and also requires substantially higher training FLOPs and peak GPU memory.\n\n- **Added EMoE (k_train) scaling results in Figure 8 (Page 10).**\n    \n    Using higher training budgets (k_train=3) and (k_train=4), we show that both peak performance and elasticity continually improve.\n    \n- **Added an analysis of the Hierarchical Routing Loss ($\\mathcal{L}\\_\\text{HR}$) in Figure 7 (Page 10).**\n    \n    We visualize its effect on expert co-activation matrices and quantify the entropy changes on GSM8K and HellaSwag.\n    \n    These results demonstrate that $\\mathcal{L}\\_\\text{HR}$ significantly improves routing consistency under low-budget inference.\n    \n- **Added experiments on a 21B-parameter MoE model (ERNIE-4.5-21B-A3B) in Table 1 (Page 7).**\n    \n    The results, included in Table 1, confirm that EMoE remains effective at much larger parameter scales, demonstrating strong scaling with model size.\n    \n- **Added experiments using larger instruction-tuning datasets (100K and 200K) in Table 6 (Page 18).**\n    \n    We confirm that the elasticity window remains stable even when the data scale is increased by 4Ã—, indicating that EMoE is applicable across different dataset sizes, and, notably, elasticity can be achieved with 50K samples, demonstrating the plug-and-play nature of the method and its low overhead.\n    \n- **Added mutual-information analyses across mathematical, commonsense, and coding tasks (GSM8K / HellaSwag / HumanEval) in Table 7 (Page 19).**\n    \n    These results show that both EMoE sub-methods consistently improve expert specialization across domains.\n    \n\n## Textual Revisions\n\n- Revised **Figure 1 and its explanation** to more clearly highlight the performance collapse of large k_train baselines under low inference budgets (Introduction, Sec 3.2).\n- Clarified the **motivation for the hierarchical routing loss**, emphasizing that it complements stochastic co-activation sampling and addresses inaccurate routing under low-budget inference.\n- Revised **Figures 5 and 10** to more clearly illustrate the flexibility of EMoE across a wide range of hyperparameter configurations.\n\nThank you once again for your insightful comments."}}, "id": "qVc1gpIGYf", "forum": "Z2w69IZqSW", "replyto": "Z2w69IZqSW", "signatures": ["ICLR.cc/2026/Conference/Submission16399/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16399/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission16399/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763462832779, "cdate": 1763462832779, "tmdate": 1763607324970, "mdate": 1763607324970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}