{"id": "4w9HzBBLRk", "number": 25295, "cdate": 1758366294242, "mdate": 1763068412050, "content": {"title": "Towards Multimodal Understanding, Reasoning, and Tool Usage across Vision, Speech, and Audio in Long Videos", "abstract": "Long-form, multimodal video understanding requires models to integrate vision, speech, and ambient audio while reasoning coherently over extended contexts. However, existing benchmarks often emphasize either long temporal contexts or rich multimodal content, but rarely both. Moreover, they are typically restricted to multiple-choice evaluations and a single accuracy metric, offering limited insight into where models succeed or fail. To address these gaps, we introduce **STARBench**, a diagnostic benchmark designed for long-form, multimodal video understanding. STARBench features open-ended, intent-driven questions that reflect how humans naturally engage with video content. It supports single- and multi-turn dialogues, encompassing multimodal reasoning and agentic tool-use tasks across rich video, audio, and speech contexts. Each question includes a reference answer and a rubric with graded criteria, enabling interpretable and traceable evaluation. Importantly, STARBench is generated via a scalable, human-validated pipeline, ensuring reproducibility and coverage. Complementing the benchmark, we propose **STARAgent**, an agentic system for analyzing long videos using pre-processing, search, and refinement tools. Evaluating state-of-the-art closed- and open-source MLLMs on STARBench reveals substantial limitations: the top-performing Gemini-2.5-Flash reaches only 52.95\\%, while open-source models remain below 25\\%. STARAgent, leveraging structured reasoning over long videos, achieves 44.66\\%, highlighting the challenge of complex, real-world video understanding. By combining breadth, interpretability, and reproducibility, STARBench provides a practical foundation for benchmarking and improving MLLMs on long-form, multimodal video tasks. All code, including the agentic pipeline, and datasets will be released publicly.", "tldr": "STARBench is a human-validated benchmark for long-form multimodal video understanding, and STARAgent is an agentic pipeline for multimodal long video understanding, together exposing current state-of-the-art MLLMs’ limits", "keywords": ["multimodal", "long-form video understanding", "benchmark", "agentic pipeline", "question answering", "scenario-driven QA"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e9af7c3f35795c5d0d036af54a6e7031e5c42642.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents STARBench, a benchmark designed to evaluate the multimodal understanding capabilities of large language models (MLLMs) in long-form videos, integrating vision, speech, and audio signals. It also introduces STARAgent, an agentic system for analyzing long videos using a combination of preprocessing, search, and refinement tools. The authors highlight the limitations of current state-of-the-art models and demonstrate how STARBench can be used to systematically evaluate and improve multimodal reasoning over extended temporal contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ STARBench offers a novel and comprehensive framework for evaluating multimodal understanding in long-form videos. Unlike prior benchmarks, it integrates audio, speech, and visual signals, and focuses on tasks involving reasoning, tool usage, and cross-modal integration.\n+ The benchmark uses a human-validated pipeline for creating and scoring the questions, which adds credibility to the evaluation process and ensures that the tasks are relevant and grounded in real-world video data."}, "weaknesses": {"value": "- I acknowledge the contribution of this paper in benchmark construction; however, the evaluation design appears inadequate in several aspects: (1) Regarding closed-source multimodal large models, only Gemini was included for comparison, while other models such as GPT-4o were omitted. These models could help reveal distinctive characteristics of the proposed dataset. (2) There is a lack of comparison with video-agent-related methods. Among all the compared approaches, only those based on multimodal large models were considered, while only the method proposed in this study incorporates multiple tools, which clearly constitutes an unfair comparison.\n- Some experimental findings warrant further in-depth analysis: (1) Why does the performance of the proposed method remain similar across videos of varying durations? (2) In Table 5, why does the 'S' modality alone achieve the best performance (even the combination of S+V performs worse than S alone, and further incorporating the A modality leads to additional degradation)? Does this suggest that the visual modality contributes minimally in this dataset? (3) Why are the experimental results for single-turn and multi-turn questions so similar? Theoretically, multi-turn questioning represents a more challenging setting. Does this indicate potential biases in the annotation of the benchmark?\n- Both the data annotation process and the agent model constructed in this study utilize the QWen model, which appears methodologically questionable."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZZpA0wHdqT", "forum": "4w9HzBBLRk", "replyto": "4w9HzBBLRk", "signatures": ["ICLR.cc/2026/Conference/Submission25295/Reviewer_cjgM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25295/Reviewer_cjgM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910247702, "cdate": 1761910247702, "tmdate": 1762943389636, "mdate": 1762943389636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QVNTAR2vuj", "forum": "4w9HzBBLRk", "replyto": "4w9HzBBLRk", "signatures": ["ICLR.cc/2026/Conference/Submission25295/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25295/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763068411173, "cdate": 1763068411173, "tmdate": 1763068411173, "mdate": 1763068411173, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STARBench, a benchmark for long-form, multimodal video understanding, addressing gaps in existing benchmarks. STARBench uses open-ended, intent-driven questions that support dialogues and tool use across video, audio, and speech contexts. And the proposed STARAgent achieves 44.66% accuracy through structured reasoning on complex videos."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark spans multiple dimensions, including reasoning tasks and multimodal tasks, demonstrating its wide applicability and rigorous challenges."}, "weaknesses": {"value": "- The paper exclusively uses the Qwen3-30B-A3B model to generate answers, which may introduce biases in the generated outputs.  \n\n- The evaluation employs only the Qwen3-14B model as the benchmark, which is also likely to lead to biased evaluation results due to its limitations.  \n- The range of models tested in this study is insufficiently broad. The paper should consider testing more advanced models, such as Gemini-2.5-pro and GPT-5, and enabling the thinking mode of these models to fully evaluate their potential.\n\n- The experiments are limited to relatively small models with around 7B size. The lack of larger and more powerful models in the evaluation limits the comprehensiveness of the results."}, "questions": {"value": "- Why did the Qwen-Omni fail to show advantages in the audio-visual alignment task?  \n- How does StarAgent perform on the DailyOmni, WorldSense, and VideoHolmes benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pXd4JRJQ3o", "forum": "4w9HzBBLRk", "replyto": "4w9HzBBLRk", "signatures": ["ICLR.cc/2026/Conference/Submission25295/Reviewer_WiQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25295/Reviewer_WiQH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989468484, "cdate": 1761989468484, "tmdate": 1762943389298, "mdate": 1762943389298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **STARBench**, a benchmark for **long-form multimodal video understanding** that integrates vision, speech, and ambient audio. It emphasizes **open-ended, intent-driven questions** and **graded evaluation rubrics**, enabling interpretable assessment beyond multiple-choice formats. The authors also introduce **STARAgent**, an agentic system combining preprocessing, search, and reasoning tools for long-video analysis. Evaluations show the task’s difficulty—**Gemini-2.5-Flash** achieves **52.95%**, open-source models <25%, and **STARAgent** 44.66%. The benchmark and pipeline are **scalable, human-validated, and reproducible**, providing a foundation for future research."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated benchmark: Addresses a key gap by combining long-term reasoning and multimodal understanding.  \n- Interpretable evaluation: Uses open-ended questions and graded rubrics for finer diagnostic insights.  \n- Agentic complement: STARAgent demonstrates structured reasoning** for long-video comprehension.  \n- Scalable and reproducible: The human-validated pipeline ensures quality and broad applicability.  \n- Public release: Code, dataset, and pipeline enhance research transparency and community impact."}, "weaknesses": {"value": "- **Lack of comparison with domain benchmarks:**  \n  While STARBench is well-motivated, it **does not include comparisons with influential benchmarks** in *audio-visual understanding* [1–2] or *video reasoning* [3–5]. Such comparisons would help position STARBench in terms of **coverage, difficulty, and evaluation philosophy** relative to established efforts.  \n\n- **Limited experimental comparisons and insufficient baseline coverage:**  \n  1. **Absence of multi-agent baselines:** STARAgent is not compared with related *multi-agent* systems such as **Daily-Omni** [2], limiting insight into its improvements over existing frameworks.  \n  2. **Need for cross-benchmark evaluation:** Evaluating STARAgent on additional long-form multimodal benchmarks could better demonstrate the **generality and scalability** of the multi-agent framework.  \n  3. **Insufficient model and method baselines:** As a benchmark paper, STARBench would benefit from **more diverse baselines** (both closed- and open-source) to establish **stronger diagnostic references**.  \n\n- **Incomplete experimental details:**  \n  The paper lacks crucial implementation details—such as **number of frames or clips per video**, **handling of videos exceeding 30s**, and **treatment of truncated or dropped audio** for models with limited context capacity. These details are vital to **validate experimental soundness and reproducibility**.\n---\nReference\n\n[1] WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs,https://arxiv.org/abs/2502.04326\n\n[2] Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities,https://arxiv.org/abs/2505.17862\n\n[3] RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video, https://arxiv.org/abs/2505.02064\n\n[4] Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos\n\n[5] Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning? https://arxiv.org/abs/2505.21374"}, "questions": {"value": "The key concerns are outlined in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bs89kkCIhr", "forum": "4w9HzBBLRk", "replyto": "4w9HzBBLRk", "signatures": ["ICLR.cc/2026/Conference/Submission25295/Reviewer_orZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25295/Reviewer_orZe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994577617, "cdate": 1761994577617, "tmdate": 1762943388937, "mdate": 1762943388937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes STARBench, a diagnostic benchmark designed for long-form, multimodal video understanding. STARBench features open-ended, intent-driven questions that reflect how humans naturally engage with video content. It supports single- and multi-turn dialogues, encompassing multimodal reasoning and agentic tool-use tasks across rich video, audio, and speech contexts."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed benchmark supports single- and multi-turn dialogues, encompassing multimodal reasoning and agentic tool-use\ntasks across rich video, audio, and speech contexts.\n2. The proposed STARAgent outperforms the selected MLLMs."}, "weaknesses": {"value": "1. SVBench should be included in the dataset comparison since it contains multi-turn Q&As.\n2. The comparison of video duration and numbers with existing benchmarks is not provided.\n3. Important details about the evaluation are unknown, including video sampling, devices, and maximum context length, which can significantly impact the evaluation results.\n4. The authors only evaluate MLLMs of the 7B/8B size, while the parameter scaling law on this benchmark is unclear.\n5. The generalized performance of the proposed STARAgent on other video benchmarks is unknown.\n6. Only one closed-source MLLM is evaluated, which weakens the results of this paper.\n7. The rationality of the agentic tasks is insufficiently analyzed. More analysis about the tool design, task requirement of tools, and protocol of tool calling should be done.\n8. As the proposed STARAgent utilizes multiple large models and its total parameters are far more than those of baselines, the performance comparison is unfair.\n\nMinor: There are so many benchmarks named StarBench. Please consider renaming your benchmark.\n\n\nReferences:\n[1] Yang, Z., Hu, Y., Du, Z., Xue, D., Qian, S., Wu, J., ... & Xu, C. (2025). Svbench: A benchmark with temporal multi-turn dialogues for streaming video understanding. ICLR."}, "questions": {"value": "Please reply to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VLoZTY39Fl", "forum": "4w9HzBBLRk", "replyto": "4w9HzBBLRk", "signatures": ["ICLR.cc/2026/Conference/Submission25295/Reviewer_bBJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25295/Reviewer_bBJM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343885010, "cdate": 1762343885010, "tmdate": 1762943388599, "mdate": 1762943388599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}