{"id": "7cDfYiqe4X", "number": 16660, "cdate": 1758267372138, "mdate": 1759897226484, "content": {"title": "Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications", "abstract": "Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. \nAdditionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce \\textbf{Protap}, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. \nSpecifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. \nFor each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. \nOur empirical studies imply that: \n(i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. \n(ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora.\n(iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. \nCode is publicly available at https://anonymous.4open.science/r/protap-1CC5.", "tldr": "We introduce Protap, a benchmark for evaluating protein modeling across diverse realistic tasks.", "keywords": ["protein representation learning", "protein downstream application", "protein benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fd527f54ef77357cac5e37f5deeabb4a26653b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Here, the authors assemble a benchmark suite to evaluate pretrained protein models. They conduct an extensive grid evaluation of different pretraining strategies across different architectures, pretraining models on a subset of proteins from UniRef that overlap with protein structures from the AlphaFold Protein Database."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the high-level sketch of this work is interesting and impactful: effectively, the authors are seeking to do a comprehensive evaluation of different pretraining strategies and architectures, across different kinds of downstream tasks. What differentiates this from previous similar benchmarks would be: \n\n1) The inclusion of three (very) differentiated pretraining tasks. Most benchmarks only include tasks based upon language modeling (e.g. masked language or autoregressive models), but previous evidence has noted that these tasks may scale sub-optimally for some kinds of downstream applications. The inclusion of tasks that use different kinds of signal than recovering missing amino acids in sequences is interesting especially in light of calls from these prior works to explore diversified pretraining tasks.\n\n2) Similarly, a much more diversified range of model architectures than previous works, including models that use structural priors compared to models that operate on sequence-only.\n\n3) The comparison of pre-trained protein models to domain-specific models. This sets a higher standard for benchmarking and is relevant given the observation that pre-trained protein models fail to outperform randomly initialized baselines or regression on one-hot encoded sequences."}, "weaknesses": {"value": "However, there are numerous issues in the actual execution of this work, that lead to an overall poor evaluation from me. \n\n1) There is likely substantial leakage between one of the pre-training strategies and one of the downstream tasks. In the protein family prediction pretraining task, models are pre-trained to predict family labels from Uniprot. Critically, we can expect families to be either subsets of, or in some cases, effectively entirely overlap gene ontology function classes in the protein function annotation prediction task. This is caused by the fact that the Gene Ontology Consortium uses phylogenetic inference and sequence homology to transfer functional annotations, which overlap with the same strategies that Uniprot use to assign protein families. \n\n2) Standardizing pretraining data across pretraining tasks occludes the way that these tasks have different generality and scalability. Pretraining tasks that rely upon structure or annotation are necessarily limited to smaller pretraining datasets than what is possible for sequence-only pretraining tasks. Furthermore, sequence-only models have been previously documented to rely upon implicit learning of evolutionary homology for their effectiveness, which is only possible if the models have been exposed to many homologs during pretraining - so training them on filtered-down datasets where only sequences where structure is available may hurt their effectiveness a lot more than if they were trained on larger datasets relative to models that explicitly incorporate priors. In other words, it is difficult to fully disentangle pretraining tasks from data because pretraining tasks are designed to scale and learn from data differently - and critically, this work is intended to make a general statement about pretraining strategies in general (i.e. which strategies are more effective), as opposed to e.g. pretraining strategies in low-data regimes. \n\n3) Many critical details are omitted from the manuscript, and it is impossible to evaluate whether the benchmarking has been done properly without these details. There are many cases of this, but the ones that stood out to me were:\n- There is no information on how long models were trained for, or what stopping criterion was used. This is not reported in Supplement D, which only documents hyperparameter settings. Currently, all of the pre-trained models perform fairly poorly compared to randomly initialized equivalents on most tasks (with the exception of the protein functional annotation task for the protein family prediction pretrained models, but I noted that this can be explained by leakage), and it is not clear if the authors have trained these models to sufficient convergence to draw the conclusion that pretraining is often ineffective.\n- The description of the pretraining tasks is incomplete and confusing, and details are omitted from even the supplementary. For example, for the multi-view contrastive learning pretraining task, I cannot tell if this is on subsequences, substructures, or subsequences plus substructure. The section starts by saying the task \"preserves the representational similarity between the correlated substructures of proteins\", which suggests learning on structure, but then subsequently says \"we generate two views... from its amino acid sequence as positive samples\", which suggest learning on sequence. But the views are denoted differently, and in Figure 1a, Gseq and Gspace are represented differently (with arrows connecting Gspace) suggesting Gseq is sequence and Gspace is structure. There is no formal definition of the pretraining task even in the supplementary, so I cannot figure out what this task actually is.\n- There is limited explanation of the pretraining data curation, and this is especially critical for the protein family prediction task. The authors explain that they take structures from APDB and cross-reference these against Uniprot to collect sequence and family labels. But what level of evidence in family labels do they consider? What databases do they collect identifiers from? What does the distribution of family labels look like (e.g. if they're considering all databases/identifiers this will likely be sparse and very high-dimensional - again, exactly what is the pretraining task, is it a cross-entropy classification problem over all of these)? Again, there is no detail in supplementary about this.\n\n4) The comparisons of Protap to previous benchmark suites are overstated:\n- In particular, Table 1 aims to frame Protap as the benchmark that covers the most applications, heavily suggesting that their benchmark is comprehensive compared to others. But this table is curated to be specific to features present in Protap, but does not include any features present in other benchmark suites but not present in Protap - as the authors themselves note, other benchmarks cover aspects like generative design, protein engineering, protein-protein interactions, or simply aim for depth of coverage over a specific topic over breadth. In other words - I think it is inaccurate to frame Protap as comprehensive compared to previous efforts, because it is clear these efforts are complementary.\n- Second, at least three of the five datasets are from prior machine learning efforts where substantial benchmarking work has already been done for the individual dataset in the original paper. This is in contrast to many previous benchmarks, which curate datasets from large-scale bioinformatics databases, existing experimental data, or even collect experimental data by themselves. I raise this because in contrast to these works, much less translational effort has been put into the curation of this benchmark e.g. instead of needing to format and clean messy data for machine learning purposes or make decisions on training/test splits, this benchmarking effort primarily recycles this existing work done by other authors. I'm uncomfortable with the assertion that these applications have not been thoroughly studied in \"existing benchmarks\", because even as this work is comprehensive, these applications have been benchmarked by the original authors that curated the dataset."}, "questions": {"value": "Overall, the issues I raise in weaknesses are substantial enough that I think it would be challenging to comprehensively address them all within a short rebuttal period - especially because of the lack of complete information in the original manuscript, it is possible that clarifications of these questions will raise further issues. But to begin with:\n\n1. Provide full details on pretraining datasets, pretraining losses, and exactly how models were trained.\n2. Provide a stratification/split of the function classification task such that there is no overlap and homology with pretraining data.\n3. Train the sequence and function pretrained models on larger datasets more reflective of the scale of data these pretraining tasks would typically interact with, as opposed to structure-scale datasets.\n4. Reduce the claim that Protap is comprehensive compared to previous benchmark suites, and more accurately portray it complementary, and be more explicit about how this benchmark recycles existing datasets that have already been thoroughly evaluated with baselines (although not the specific strategies/baselines in this work)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ncTC9wpWii", "forum": "7cDfYiqe4X", "replyto": "7cDfYiqe4X", "signatures": ["ICLR.cc/2026/Conference/Submission16660/Reviewer_Jhsw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16660/Reviewer_Jhsw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760739128110, "cdate": 1760739128110, "tmdate": 1762926721200, "mdate": 1762926721200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Protap, a new benchmark for protein modeling that systematically evaluates a wide range of backbone architectures, pretraining strategies, and domain-specific models on a suite of five realistic downstream protein applications. Protap encompasses both widely studied general tasks and two specialized, industrially relevant tasks (enzyme-catalyzed protein cleavage site prediction and targeted protein degradation via PROTACs) that are largely absent from prior benchmarks. The paper offers a holistic comparison across architectures (including sequence-based, structure-based, and hybrid models), pretraining regimes (MLM, multi-view CL, protein family prediction), and domain-specific models, reporting extensive experiments and analysis on standardized datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Protap covers a broad swath of both general and specialized protein tasks, filling gaps unaddressed by leading prior benchmarks.\n2. The experimental design is comprehensive, testing a wide array of architectures and several pretraining objectives. The study further incorporates and details strong domain-specific models for specialized tasks.\n3. The paper presents performance tables comparing scores of all baseline and domain-specific models using standard metrics, and provides setup for all experiments, including details on hyperparameters and data preprocessing. Results are reported clearly with mean and standard deviation over multiple seeds, and ablations are provided.\n4. All code, data, and pipeline details are made available, with careful documentation."}, "weaknesses": {"value": "1. The experimental setup is extensive, but direct benchmarking versus leading results from highly cited works on individual tasks (especially broader benchmarks like ProtTrans, MSA Transformer) is missing. For example, for mutation effect and function prediction, existing strong baselines from recent works are absent as explicit baselines in the main results tables. This makes it harder to quantitatively situate the new results relative to the current state of the art in every aspect.\n2. While the experimental results are detailed, the investigation into why certain architectures excel in specific contexts is somewhat superficial, at times reduced to high-level hypotheses about inductive bias. The potential confounding effects of hyperparameter choices, data overlap, or domain-specific tuning are not exhaustively ruled out.\n3. The modeling of the non-protein components in multimodal interaction tasks is oversimplified. In tasks involving interactions between proteins and other molecules (such as Protein-Ligand Interaction (PLI) and PROTACs), the final predictive performance depends on the ability to represent all participating components, not just the protein."}, "questions": {"value": "1. For MVCL and PFP pretraining, can the authors specify the exact loss functions, sampling (especially for negatives in MVCL), and any rebalancing strategies used? Please clarify if temperature scaling is applied and how the family-imbalance in PFP is handled.\n2. The observation in Table 4b is very interesting: on the PROTACs degradation prediction task, the general-purpose EGNN model outperforms two domain-specific models (DeepPROTACS, ET-PROTACS). This is a counter-intuitive result. Could you provide some potential hypotheses to explain this phenomenon?\n3. In tasks involving small molecules, such as PLI and PROTACs, the general-purpose protein models were uniformly configured with a GVP encoder to handle the small molecules. What was the rationale for this design choice? Could this have systematically lowered the performance ceiling of the general-purpose models on these tasks, thereby affecting the fairness of the comparison against domain-specific models, which may have more optimized molecular representation modules? Did you consider or try using a standard, pre-trained molecular encoder as a stronger baseline?\n4. The main text of the paper primarily presents and discusses the results from frozen-encoder fine-tuning, while the results for full-parameter fine-tuning, a common paradigm, are placed in the appendix. Could you explain the reasoning behind this organizational choice? Given that the full-parameter fine-tuning results in the appendix (Table 6) show a more complex phenomenon (with mixed performance, sometimes even showing a decrease), would this alter or supplement the main conclusions you draw in the main text regarding the value of the pre-trained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YX99OUY8tY", "forum": "7cDfYiqe4X", "replyto": "7cDfYiqe4X", "signatures": ["ICLR.cc/2026/Conference/Submission16660/Reviewer_wErr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16660/Reviewer_wErr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015456662, "cdate": 1761015456662, "tmdate": 1762926720554, "mdate": 1762926720554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Protap, a benchmark that evaluates protein modeling approaches across five realistic applications: cleavage site prediction, PROTAC-mediated degradation, protein–ligand interaction, function annotation, and mutation effect prediction. Protap provides a unified framework to assess trade-offs between general foundation models and task-specific designs. By comparing pretrained protein language models, structure-based and hybrid architectures, and domain-specific models, the study reports that training from scratch can sometimes outperform frozen large encoders, structural information often boosts downstream performance, and domain-specific models dominate in specialized applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark scope is extensive along three axes tasks, architectures, and pre-training strategies, and it includes two biologically meaningful specialized applications that are under represented in prior benchmarks. The protocol describes pre training data sources, downstream splits, and multi seed reporting, which improves reproducibility."}, "weaknesses": {"value": "- Lack of cross-benchmark synthesis: While Protap positions itself against previous benchmarks, it does not compare its findings with prior benchmarks that consistently reported strong benefits of pretraining on downstream tasks. In contrast, Protap observes that training from scratch often outperforms pretrained models. The paper does not explain what differences in setup (e.g., task selection, optimization regime, data scale) might account for this contradiction.\n- Domain-specific model advantage not fully contextualized: While domain models often outperform pretrained PLMs in specialized applications, the paper does not clearly provide deeper insights into the underlying reasons.\n- Possible under-optimization of pretrained encoders or fine-tuning process: The weaker performance of pretrained encoders may be due to insufficient optimization of pre-training or limited exploration of fine-tuning strategies. The paper does not present ablations to rule out these possibilities.\n- Ambiguity in molecular encoders: Section 3.1 states that molecular components are represented by randomly initialized GVP encoders, but it is unclear whether these encoders are trained downstream. This could significantly impact performance."}, "questions": {"value": "- Previous benchmarks reported benefits of pretraining on downstream tasks, whereas Protap often finds the opposite. What differences in setup explain this contradiction?\n- In cases where domain-specific models outperform general pretrained models, can the authors provide deeper insights into the underlying reasons for this advantage?\n- Did the authors conduct ablations to ensure that pre-trained encoders or fine-tuning process are not under-optimized?\n- For the molecular encoders, are the randomly initialized GVP encoders trained during downstream experiments, and if so, under what settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NanIjXU6OA", "forum": "7cDfYiqe4X", "replyto": "7cDfYiqe4X", "signatures": ["ICLR.cc/2026/Conference/Submission16660/Reviewer_R4Qj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16660/Reviewer_R4Qj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591558835, "cdate": 1761591558835, "tmdate": 1762926719587, "mdate": 1762926719587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Protap introduces a benchmarking suite for evaluating general‑purpose and domain‑specific protein models on five realistic downstream applications: two specialized tasks (enzyme‑catalyzed protein cleavage site prediction and PROTAC‑based targeted protein degradation) and three general tasks (protein‑ligand interaction prediction, protein function annotation, and mutation effect prediction). The benchmark systematically compares a wide spectrum of architectures (sequence‑based Transformers, geometric graph neural networks, sequence-structure hybrid models), pre‑training strategies (masked‑language modeling, multi‑view contrastive learning and protein family prediction), and domain‑specific models across these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Prior benchmarks such as PEER and TAPE focused on general sequence or structural tasks; none considered enzyme cleavage or PROTAC‑mediated degradation. \n\n- The paper clearly outlines four concrete observations drawn from its benchmark evaluation: (1) pretraining improves downstream performance but gains vary significantly across tasks; (2) task-specific models can outperform general-purpose PLMs when domain adaptation is well-aligned with task distribution; (3) there is low cross-task correlation, indicating that high performance in one application (e.g., protein fitness prediction) does not generalize to others (e.g., PROTAC efficacy); and (4) current models struggle particularly with enzyme cleavage site prediction, suggesting this task remains an open challenge. These observations are well-motivated, empirically supported, and help ground the benchmark’s value beyond raw leaderboard comparisons.\n\n- The benchmark evaluates a broad range of architectures, from sequence language models (ESM‑2, ProtBERT) to geometric GNNs (EGNN, SE(3) Transformer, GVP) and hybrid models (SaProt, D‑Transformer). It also includes domain‑specific models tailored to each task (e.g., UniZyme for cleavage, DeepProtacs and ET‑PROTACs for PROTACs, KDBNet and MONN for ligand interactions). This breadth helps researchers understand how architectural choices and inductive biases affect performance across tasks.\n\n- The results demonstrate that models incorporating 3D structural information (EGNN, SE(3) Transformer, GVP, D‑Transformer) generally outperform pure sequence models. Even when pre‑trained, geometric models achieve stronger performance on protein–ligand interaction and PROTAC tasks. The authors argue that structures provide essential inductive biases absent in sequence‑only architectures.\n\n- Table 4 (page 9) compares general models to eight domain‑specific models. General architectures such as EGNN and D‑Transformer match or exceed specialized models for PROTACs, while specialized models like KDBNet outperform general models on protein–ligand interaction. For protein function annotation, DeepFRI and DPFunc (models that incorporate structural and functional information) significantly outperform general models. The benchmark, therefore, highlights the contexts where specialized inductive biases are necessary.\n\n- The authors release the code and data, including hyper‑parameter settings and pre‑training details, enabling the community to reproduce and extend the experiments."}, "weaknesses": {"value": "- While the benchmark adds enzyme cleavage and PROTAC tasks, it still omits other important applications such as protein folding, stability prediction, de novo protein design, antigen–antibody binding and protein–protein interactions beyond binary classification. Benchmarks like ProteinBench and ProteinGym include tasks for protein design and conformational dynamics, whereas Protap focuses mainly on classification/regression problems. Extending the benchmark to cover additional tasks would significantly increase its impact.\n\n- The specialized tasks use relatively small datasets: the enzyme cleavage task has 375 training proteins and 92 test proteins, and the PROTAC task has 343 training pairs and 202 test pairs. Small sample sizes may lead to high variance and limit the generalizability of conclusions. In addition, the annotation quality of enzyme cleavage sites and PROTAC ternary complexes varies across sources; the benchmark does not discuss potential noise or biases in these datasets.\n\n- Performance is measured by metrics such as AUC, AUROC and MSE. For tasks like cleavage site prediction and PROTAC degradation, false negatives can have very different practical implications than false positives (e.g., missing a cleavage site may be more problematic than incorrectly predicting one). Moreover, the reported metrics are cost-agnostic and insensitive to imbalance in ways that can mask real-world risk. In cleavage-site prediction (and similarly in PROTAC degradation screens), a false negative (missing a true site/degrader) is often far more damaging than a false positive, yet AUROC/AUC weight FP and FN equally. So two models with the same AUROC can have radically different miss rates at the FP budgets labs can afford.\n\n- The benchmark compares models of varying sizes (10M to 650M parameters) but does not investigate scaling laws. Even though the authors observe that smaller models can outperform larger ones (e.g., EGNN vs. ESM‑2), they do not explore whether performance improves with moderate scale (ESM-2 family) or how parameter counts relate to data efficiency. ProteinBench emphasises scaling laws for protein representation models; Protap could benefit from a similar analysis."}, "questions": {"value": "- Your RQ1 suggests that encoders trained from scratch often outperform or match pre-trained PLMs (under frozen or linear-head settings). However, your conclusion may not be directly comparable: the examples you highlight often compare a pre-trained sequence model with a supervised structure-based model, rather than comparing models within the same modality. Within-modality trends may actually agree, e.g., ProteinWorkshop (ICLR’24) shows pre-training does benefit structure-based models. Could your “scratch > pre-train” finding simply reflect cross-modality gaps or domain-task mismatch, rather than a failure of pre-training itself? Would you expect the conclusions to change if you compared pre-trained vs scratch within each modality (e.g., a pre-trained structure GNN vs supervised one), or if you fully fine-tuned the sequence models under the same protocol?\n\n- You evaluate MLM-style and related pre-training but appear not to include next-token (autoregressive) pre-training; even though AR protein LMs (e.g., ProGen-style families) are used beyond generation (zero-shot mutation effect via Δlog-likelihood/perplexity, and embeddings from hidden states with simple pooling). Could you clarify why AR objectives were excluded?\n\n\n**Suggestions for improvement**\n- Expand task coverage: Incorporate additional downstream tasks such as protein–protein interaction prediction, antigen/antibody binding, protein folding stability and generative design. Including these tasks would align Protap more closely with benchmarks like ProteinGym and ProteinBench.\n\n- Increase dataset size and diversity: Augment the enzyme cleavage and PROTAC datasets by integrating additional experimental sources and negative examples to reduce sampling bias. Where feasible, leverage high‑throughput assays or simulated data to enlarge the training sets.\n\n- Investigate scaling laws and parameter efficiency: Systematically vary model size within each architecture (e.g., small, medium, large EGNN/SE(3)/Transformer) to understand how performance scales with parameters and data. This could reveal whether moderate scaling yields better trade‑offs than extremely large language models.\n\n- Explore multi‑task and cross‑task learning: Evaluate whether jointly training a model on multiple Protap tasks improves generalization, following the findings from PEER. Additionally, investigate whether knowledge learned from specialized tasks (e.g., cleavage prediction) transfers to general tasks (e.g., function annotation).\n\n\nIf the authors address the weaknesses outlined above, or provide a right justification for the points raised in my questions, or incorporate any of the suggested future directions or clarify points I may have misunderstood, I would be willing to raise my score to an 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bz8YD3oTNk", "forum": "7cDfYiqe4X", "replyto": "7cDfYiqe4X", "signatures": ["ICLR.cc/2026/Conference/Submission16660/Reviewer_i3yn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16660/Reviewer_i3yn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768357262, "cdate": 1761768357262, "tmdate": 1762926718417, "mdate": 1762926718417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}