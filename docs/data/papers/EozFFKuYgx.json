{"id": "EozFFKuYgx", "number": 9980, "cdate": 1758154160910, "mdate": 1759897682228, "content": {"title": "Scalable Pretraining of Retrieval Models", "abstract": "Modern retrieval models are typically pretrained on masked language modeling (MLM) or causal language modeling (CLM) tasks, where retrieval capabilities emerge only incidentally. Achieving state-of-the-art performance requires finetuning these models on supervised query-document pairs, which are expensive to curate and sparse for specialized domains. We propose a scalable pretraining approach that directly targets retrieval tasks using a fully self-supervised pipeline. By leveraging web-scale text corpora, we construct retrieval pairs without manual annotations. Our method employs a novel contrastive objective that aligns prefix embeddings generated by a causal transformer with suffix embeddings from an anti-causal transformer. This enables the model to learn fine-grained associations between queries and their completions, analogous to the next-token prediction paradigm in generative models. Our results demonstrate the viability of this method, suggesting that it can bridge the data scarcity gap in information retrieval and imbue retrieval models with the zero-shot reasoning capabilities typically reserved for generative language models.", "tldr": "", "keywords": ["retrieval", "embeddings", "large language models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/442184400c14129c846443c5b09970a8f3172780.pdf", "supplementary_material": "/attachment/fbdf08c4ed7122ace4428349f70123e70ea77bea.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Prefix–Suffix Language Modeling (PSLM) as a scalable pretraining framework for retrieval models. It divides each text sequence into a prefix (treated as a “query”) and a suffix (treated as a “document” or “answer”), then uses causal and anti-causal masking to obtain two representations. A contrastive InfoNCE loss brings representations from the same sequence closer while pushing apart those from different sequences. The authors further introduce a three-stage training process: self-supervised pretraining, weakly supervised positive pairs, and optional hard-negative finetuning, and claim improved retrieval performance over MLM/CLM baselines with better scalability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important problem in retrieval pretraining, aiming to align the pretraining objective more closely with real-world query–document matching tasks.\n2. The prefix–suffix formulation provides an intuitive way to model “query–answer” correspondence within the same sequence, keeping the architecture compact and training objective straightforward.\n3. Experiments are conducted across multiple retrieval benchmarks, showing stable and consistent gains over MLM and CLM baselines, with additional ablation and qualitative analyses."}, "weaknesses": {"value": "1. The paper’s main limitation lies in its limited conceptual novelty. The prefix–suffix contrastive pretraining objective closely resembles prior methods such as RetroMAE, CoCondenser, and Contriever. These works already explored similar contextual or sentence-level contrastive schemes, and the paper does not clearly articulate what PSLM adds beyond re-framing or scaling.\n2. The contribution is mainly engineering-oriented rather than conceptual. Most of the performance gain seems to come from large-scale training, distributed optimization, and more negatives, rather than a fundamentally different modeling principle.\n3. Several recent and strong retrieval or embedding models such as E5, GTR, and modern BGE variants are not included in comparison, which weakens the empirical strength of the SOTA claim.\n4. The claim of scalability is insufficiently supported. The paper lacks iso-compute or iso-data comparisons, GPU-hour cost curves, or throughput analysis. Without quantitative cost–performance trade-offs, the “scalable” claim remains mostly qualitative."}, "questions": {"value": "1. Can you provide iso-compute or iso-data comparisons with MLM/CLM and other recent retrieval pretraining methods?\n2. How exactly does your scalable implementation improve training efficiency? Please include GPU-hour and throughput metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rTT28G3qoq", "forum": "EozFFKuYgx", "replyto": "EozFFKuYgx", "signatures": ["ICLR.cc/2026/Conference/Submission9980/Reviewer_cSe9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9980/Reviewer_cSe9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538115312, "cdate": 1761538115312, "tmdate": 1762921415243, "mdate": 1762921415243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Prefix-Suffix Language Modeling (PSLM), a self-supervised contrastive pretraining approach for retrieval models that aims to parallel next-token prediction in generative LMs. Instead of MLM/CLM objectives, the model learns to align prefix embeddings from a causal transformer with suffix embeddings from an anti-causal transformer at every token position. The authors introduce several training “bells and whistles” (e.g., distributed contrastive loss, multi-positive labels, diagonal masking) and demonstrate that PSLM scales to web-scale corpora and improves zero-shot retrieval and instruction-following ability on benchmarks such as MTEB and IFEval. They further show significant improvements when combined with supervised phases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. An important topic is studied, scalable pre-training for dense retrieval. \n2. The analogy of the proposed PSLM to next-token prediction is interesting. \n3. The distributed contrastive loss is useful for large-scale retrieval training. \n4. The MTEB and IFEval benchmarks are used for evaluation, each including a wide range of datasets."}, "weaknesses": {"value": "1. The novelty of PSLM is limited and the relationship to prior work is under-positioned. There is a large body of works on pre-training for information retrieval, where the pre-training objectives are not limited to masked language modeling and next token prediction. PSLM is very close to contrastive pretraining using prefix-complement spans, as used in Contriever and Contrastive Predictive Coding-style objectives (Oord et al., 2018). The conceptual novelty appears incremental, though framed as foundational. Citation discussion acknowledges Contriever yet undersells methodological similarity. \n\nOther pre-training objectives, such as \"RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder\", although also based on MLM, are also very powerful. They are not discussed or compared in the paper. \n\n2. The method descriptions have discrepancy in Figure 1 and Figure 4. Figure 1 shows that the first token (suffix) is paired with the last token (prefix), while in Figure 4, the first token (suffix) is paired with the first token (prefix). This is a bit confusing. If Figure 1 describes the method accurately, matching the first token and the last token does not really make sense to me. \n\n3. The performance of stronger models on MTEB is not reported, such as RetroMAE. Baselines are not sufficient. Nomic-Embed also has better performance (59.9) than reported in Table 1. \n\n4. Related work based on large language models are not mentioned: \nLlm2vec: Large language models are secretly powerful text encoders\nFinetuning llama for multi-stage text retrieval. \nLlama2Vec:Unsupervised Adaptation of Large Language Models for Dense Retrieval\nUnleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling\n\n5. The citation of dataset IFEval (Sun et al., 2024) is wrong. Sun et al., 2024 introduce \"MAIR: A Massive Benchmark for Evaluating Instructed Retrieval\" for retrieval tasks, but IFEval is from paper \"Instruction-Following Evaluation for Large Language Models\" and constructed for LLM evaluation. Compared to IFEval, MAIR is better for evaluation.  \n\n6. There is potential data leakage. Some datasets used in validation are likely included in the retrieval portion of WIR data. The paper should explicitly quantify data contamination risk."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HuZ1uWC1ow", "forum": "EozFFKuYgx", "replyto": "EozFFKuYgx", "signatures": ["ICLR.cc/2026/Conference/Submission9980/Reviewer_rubc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9980/Reviewer_rubc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750275186, "cdate": 1761750275186, "tmdate": 1762921414905, "mdate": 1762921414905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a novel approach for self-supervised training of a retriever. The core idea involves selecting the correct suffix (i+1, L) from a candidate set using the representation of a prefix (0, i), where the prefix and suffix representations are obtained through left-to-right and right-to-left Transformers, respectively. This method enables large-scale unsupervised training of retrieval models and achieves improved performance on the MTEB benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach of predicting the suffix using the prefix demonstrates a certain degree of novelty.\n2. Experimental results indicate a positive improvement, although it almost levels off after supervised fine-tuning (SFT)."}, "weaknesses": {"value": "1. There is a lack of sufficient discussion and insight into why this method performs better than next token prediction during pre-training. Both methods essentially involve predicting future text based on a prefix—one predicts the very next token, while the other predicts the corresponding suffix. However, suffix prediction inherently requires roughly double the computational cost. A deeper exploration of why pre-training via suffix prediction yields superior results would be highly valuable.\n2. Table 1 shows that PSLM-160M exhibits almost no improvement over CLM-160M after Phase 3 fine-tuning. This observation further reinforces the concerns raised in my first point.\n3. In Table 1, the performance of PSLM-1.4B is worse than that of PSLM-160M. This raises concerns about whether the performance gain of this method scales reliably with increasing model parameters.\n4. The discussion of related work could be strengthened. Several other works have also explored scalable training for retrieval, fully within an autoregressive loss formulation, such as https://arxiv.org/abs/2306.13421 and https://arxiv.org/abs/2410.01651."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d9FyfZfyB7", "forum": "EozFFKuYgx", "replyto": "EozFFKuYgx", "signatures": ["ICLR.cc/2026/Conference/Submission9980/Reviewer_8aH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9980/Reviewer_8aH2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884214210, "cdate": 1761884214210, "tmdate": 1762921414432, "mdate": 1762921414432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes prefix-suffix language modeling (PSLM), a novel approach pertaining to retrieval models. Learned lessons from pretraining generative models, PSLM aligns the prefix and the suffix in the embedding space to improve the retrieval performance. This paper also addresses scalability issues when using contrastive learning in distributed scenarios and introduces several pre-training tricks. This paper pretrains the retrieval models in a multi-phase training scheme. Experiments show that with PSLM, the pretrained models can produce state-of-the-art performance compared to traditional pre-training schemes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper identified the pretraining issue of retrieval models and addressed it using a proper method."}, "weaknesses": {"value": "1. It would be better to provide additional results across a wider range of model sizes, such as 320M, 640M, 3B, and 7B, to show the scalability of the proposed methods.\n2. The paper writing is quite poor. For example, the paper lacks an explanation of the design philosophy and only introduces the specific steps of their methods."}, "questions": {"value": "1. Can we merge phase 2 with phase 3 into one phase?\n2. How about the performance of the proposed method in the long-context scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m0mgbnGoyu", "forum": "EozFFKuYgx", "replyto": "EozFFKuYgx", "signatures": ["ICLR.cc/2026/Conference/Submission9980/Reviewer_PCeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9980/Reviewer_PCeL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936738315, "cdate": 1761936738315, "tmdate": 1762921414070, "mdate": 1762921414070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}