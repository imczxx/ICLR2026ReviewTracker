{"id": "MwsUeQqj22", "number": 11577, "cdate": 1758202025704, "mdate": 1759897566719, "content": {"title": "Off-Policy Learning in Large Action Spaces: Optimization Matters More Than Estimation", "abstract": "Off-policy evaluation (OPE) and off-policy learning (OPL) are foundational for decision-making in offline contextual bandits. Recent advances in OPL primarily optimize OPE estimators with improved statistical properties, assuming that better estimators inherently yield superior policies. Although theoretically justified, this estimator-centric approach neglects a critical practical obstacle: challenging optimization landscapes. In this paper, we provide theoretical insights and empirical evidence showing that current OPL methods encounter severe optimization issues, particularly as the action space grows. We show that estimator-aware policy parametrization can mitigate, but not fully resolve, optimization challenges. Building on this, we explore simpler weighted log-likelihood objectives and demonstrate that they enjoy substantially better optimization properties and still recover competitive, often superior, learned policies. Our findings emphasize the necessity of explicitly addressing optimization considerations in the development of OPL algorithms for large action spaces.", "tldr": "", "keywords": ["off-policy evaluation", "off-policy learning", "large action spaces", "offline RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8793ac9c03843f3fd63ebe40a30b54e7ad235324.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that in offline contextual bandits with very large action spaces, the main bottleneck isn’t statistical estimator quality but the optimization landscape induced by common OPE-driven objectives (IPS/DR and large-action variants). It shows these objectives become highly non-concave and plateau-prone as the effective action size grows, and that “estimator-aware” policy parameterizations (e.g., POTEC’s cluster-level policy) help but don’t fix the root issue. In response, the authors propose PWLL objectives that trade value-estimation fidelity for concave (often strongly concave) optimization under linear-softmax policies, leading to more robust learning and competitive, or superior, policies in large-K settings. Empirical results on large recommender datasets support this optimization-first view."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Reframes OPL progress from “better estimators => better policies” to “better optimization => better policies” an interesting perspective shift.\n\n\nConnects asymptotic solutions of OPE estimators to policy-class design (objective-aware parametrization), giving clear recipes (e.g., restrict to $S_0(x)$, cluster-level optimization).\n\n\nAdvocates and analyzes a PWLL family that prioritizes concave landscapes, unifies/contrasts several known surrogates\n\nComprehensive experiments across three large-K recommenders, stress-tests with batch-size and LR-schedule sweeps strengthen the optimization narrative.\n\nClear comparison showing POTEC’s gains stem from optimization structure rather than estimator improvements."}, "weaknesses": {"value": "The concavity guarantee for PWLL holds for linear softmax policies, but many real systems use deeper, non-linear policies or two-tower networks. It’s unclear how much of PWLL’s optimization advantage persists beyond this setting or whether local strong concavity emerges in practice.\n\n\nWhile the paper argues estimation fidelity is overrated for learning, some readers will want generalization or regret bounds for PWLL (even if biased as an estimator). e.g., performance guarantees vs. the best policy in a class under support deficiencies or misspecified reward noise.\n\n“Reward” is reported but the construction of rewards in the static datasets (implicit feedback, scaling, propensity correction, negative sampling) is not deeply detailed in the main text. Since PWLL weights use $R_i$ (and sometimes $\\exp(R_i/\\beta)$), reward scale/noise critically affects training stability, so more robustness checks (e.g., rescaling/noise injection/label smoothing) would help."}, "questions": {"value": "Since LPI and especially RegKL use $R_i$ inside $g(\\cdot)$ or $\\exp(R/\\beta)$, how sensitive are results to rescaling or label noise? A controlled study varying reward distributions and the temperature $\\beta$ would help operationalize the recipes.\n\nPWLL resembles advantage-weighted/behavior-regularized policy updates in offline RL (e.g., CRR/AWAC). A brief comparison (objectives, weighting, guarantees) would position your contribution within a broader literature and might suggest cross-pollination."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RhL8Zqh8m7", "forum": "MwsUeQqj22", "replyto": "MwsUeQqj22", "signatures": ["ICLR.cc/2026/Conference/Submission11577/Reviewer_u2kL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11577/Reviewer_u2kL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760697644852, "cdate": 1760697644852, "tmdate": 1762922664506, "mdate": 1762922664506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that in off-policy learning (OPL) for large action spaces, the main challenge lies in optimization, not in improving the statistical accuracy of off-policy evaluation (OPE) estimators. Through theoretical analysis and large-scale experiments, the authors show that common OPE-based objectives (e.g., IPS, DR, MIPS, OffCEM) have highly non-concave landscapes that hinder learning, even when the estimators are accurate.\n\nThey propose two remedies: (1) objective-aware policy parametrization, aligning the policy’s structure with the estimator’s bias to ease optimization, and (2) policy-weighted log-likelihood (PWLL) objectives, which are concave and easy to optimize but still yield strong policies.\n\nExperiments on large recommendation datasets demonstrate that these simpler, optimization-focused methods outperform complex OPE-based approaches. The key takeaway is that optimization-friendly objectives lead to better policies than statistically precise but hard-to-train estimators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper challenges the dominant estimator-centric paradigm in off-policy learning (OPL), emphasizing that optimization bottlenecks—not estimation errors—are the main limiting factor in large action spaces. This conceptual reframing is original and relevant to both theory and practice.\n\n- Strong theoretical foundation: The authors provide rigorous asymptotic analysis showing how common off-policy evaluation (OPE) objectives induce difficult non-concave landscapes and multiple local optima, particularly as action space grows. The derivation of objective-aware parametrization and analysis of concavity in PWLL objectives is theoretically sound.\n\n- Clear algorithmic proposals: The introduction of Policy-Weighted Log-Likelihood (PWLL) objectives is elegant and well-motivated. These objectives are shown to yield strongly concave optimization problems under linear softmax policies, leading to simple yet effective learning procedures.\n\n- Comprehensive empirical study: Experiments on large-scale datasets (MovieLens, Twitch, GoodReads) convincingly demonstrate that PWLL-based methods (e.g., cLPI, RegKL) outperform complex OPE-based baselines (e.g., OffCEM, POTEC) in both stability and final reward, validating the paper’s core claim.\n\n- Practical insights and implications: The work provides guidelines for large-scale OPL design—notably the benefits of objective-aware and lightweight parametrizations—offering direct takeaways for practitioners."}, "weaknesses": {"value": "- Limited novelty in algorithmic form: While the conceptual shift toward optimization-centric design is insightful, the specific PWLL objectives (LPI, cLPI, RegKL) closely resemble known reward-weighted or behavior-cloning-style objectives. The paper’s novelty lies more in perspective than in methodology.\n\n- Limited theoretical depth on optimization dynamics: Although the paper states concavity results, the theoretical section does not deeply analyze gradient dynamics or convergence rates under stochastic optimization—key for understanding large-scale practicality.\n\n- Scope of baselines for theoretical analysis and experiment: The study omits recent pessimistic or PAC-based OPL methods (e.g., conservative or uncertainty-aware learning) that might also improve optimization robustness. Including them could strengthen the empirical claims.\n\n- Clarity and positioning: The framing could better articulate how PWLL fits within or generalizes existing frameworks like reward-weighted regression or entropy-regularized policy gradients, to avoid appearing incremental."}, "questions": {"value": "- What does $\\hat{r}$ mean in the asymptotic regime in Eq (10)? Doesn't $\\hat{r}$ become $r$?\n- Trade-off analysis: How does the loss in value estimation accuracy quantitatively relate to the gains in optimization stability? Is there a measurable regime where OPE objectives outperform PWLL?\n\n- Generalization to reinforcement learning: Can the optimization-centric insights transfer beyond contextual bandits to full RL settings where multi-step credit assignment exists?\n\n- Robustness: How does PWLL perform under misspecified reward signals or biased logged data (e.g., when π₀ has deficient support)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GtVBmGdOze", "forum": "MwsUeQqj22", "replyto": "MwsUeQqj22", "signatures": ["ICLR.cc/2026/Conference/Submission11577/Reviewer_Vgm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11577/Reviewer_Vgm9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761313713102, "cdate": 1761313713102, "tmdate": 1762922664021, "mdate": 1762922664021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that in offline contextual bandits with very large discrete action spaces, the dominant “estimator-centric” approach to off-policy learning ignores a crucial practical issue: the optimization landscape is hard to optimize. The authors make 4 main claims:\n1.\tMany existing OPE objectives induce highly non-concave objectives when paired with common parametric policies; they show that this pathology worsens as the size of action space grows.\n2.\t When the underlying policy is parameterized as a full softmax over all actions, gradient computation and optimization remains costly and poorly conditioned (except POTEC, because it explicitly reduces the effective action space by fixing intra-cluster behavior).\n3.\tPolicy-Weighted Log-Likelihoods (PWLLs) are easy to optimize because of their concavity under common parameterizations. \n4.\tExperimental results show PWLL methods are more robust than estimator-centric baselines across hyperparameters, and the gap between PWLL and previous work widens as the action space grows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper shows clearly how estimator-centric OPE objectives break down in very large action spaces and how it can be beneficial to analyze the OPE problem from an optimization perspective. Theoretically, they derive how different objectives behave in the infinite-data regime. Then switching to log-likelihood (PWLL) objectives, combined with linear softmax parameterizations, yields favorable curvature (concavity) properties and improved optimization behavior in practice. Empirically, they validate these insights with extensive large-scale experiments on real recommendation datasets, and the performance gap widens as the action space becomes larger.\nTaken together, these theoretical and empirical contributions make a convincing case for an optimization-centric approach; the work appears to be among the first to systematically demonstrate, both analytically and at scale, that this reframing improves training stability and policy quality."}, "weaknesses": {"value": "- Missing baselines, e.g.  Contextual Bandits with Large Action Spaces: Made Practical (Zhu et, al. ICML 2022)\n- Reproducibility & ablations: while the datasets are large and convincing, the paper would benefit from more ablations: (i) seed variability and statistical significance, (ii) whether PWLL remains robust to different hyperparameters (e.g. $\\beta, \\tau$) (iii) how PWLL performs in comparison to POTEC with different cluster size. \n- Lack of qualitative results e.g. the optimization landscape in toy examples"}, "questions": {"value": "1. Clustering sensitivity: POTEC and MIPS rely on embeddings/clustering. Could the authors include sensitivity analysis to embedding quality or cluster size? How often does poor clustering make POTEC fail?\n2. Under what conditions does the bias introduced by PWLL become severe, and how should practitioners mitigate it?\n3. Would the argument (favor optimization over estimation) still hold for non-trivial (multilayer) softmax policies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o0Ntv0ndeR", "forum": "MwsUeQqj22", "replyto": "MwsUeQqj22", "signatures": ["ICLR.cc/2026/Conference/Submission11577/Reviewer_mJqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11577/Reviewer_mJqf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892577270, "cdate": 1761892577270, "tmdate": 1762922663537, "mdate": 1762922663537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a policy-weighted log-likelihood (PWLL) objective for off-policy learning. They present various formulations of off-policy learning objectives and show that their proposed equation generalizes previous objective functions while the policy probability model is replaced by its logarithmic form. When applying the LPI, cLPI, and RegKL specializations, they demonstrated that their objective function is easily optimized, although the resulting solution can be potentially biased."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors proposed a generalized formulation that integrates and extends previous off-policy learning objectives whose optimization is easily performed. The method is novel."}, "weaknesses": {"value": "The authors provided empirical evidence showing that the proposed method outperforms existing approaches. Nonetheless, it is difficult to recommend the paper for acceptance due to the following deficiencies. The motivation for introducing the logarithmic form is unclear, and the authors fail to adequately justify the modification either mathematically or intuitively."}, "questions": {"value": "It remains unclear whether the introduction of logarithmic term is theoretically or intuitively well-motivated and whether it avoids pathological cases while achieving an optimal solution comparable to that of previous methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fBCpml9Vsj", "forum": "MwsUeQqj22", "replyto": "MwsUeQqj22", "signatures": ["ICLR.cc/2026/Conference/Submission11577/Reviewer_hky7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11577/Reviewer_hky7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206041953, "cdate": 1762206041953, "tmdate": 1762922662979, "mdate": 1762922662979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}