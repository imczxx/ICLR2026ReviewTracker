{"id": "JT6L583Iae", "number": 14209, "cdate": 1758230332025, "mdate": 1759897383576, "content": {"title": "Data Shifts Hurt CoT: A Theoretical Study", "abstract": "Chain of Thought (CoT) has been applied to various large language models (LLMs) and proven to be effective in improving the quality of outputs. In recent studies, transformers are proven to have absolute upper bounds in terms of expressive power, and consequently, they cannot solve many computationally difficult problems. However, empowered by CoT, transformers are proven to be able to solve some difficult problems effectively, such as the $k$-parity problem. Nevertheless, those works rely on two imperative assumptions: (1) identical training and testing distribution, and (2) corruption-free training data with correct reasoning steps. However, in the real world, these assumptions do not always hold. Although the risks of data shifts have caught attention, our work is the first to rigorously study the exact harm caused by such shifts to the best of our knowledge. Focusing on the $k$-parity problem, in this work we investigate the joint impact of two types of data shifts: the distribution shifts and data poisoning, on the quality of trained models obtained by a well-established CoT decomposition. In addition to revealing a surprising phenomenon that CoT leads to worse performance on learning parity than directly generating the prediction, our technical results also give a rigorous and comprehensive explanation of the mechanistic reasons of such impact.", "tldr": "We investigate the impact of distribution shift and data noise for a CoT decomposition for the k-parity problem.", "keywords": ["transformers", "llm", "chain of thought", "parity", "robustness"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7843f1cce8cc9e7ff6b218b7b03d6c567f51dbdf.pdf", "supplementary_material": "/attachment/ea3782e229ecb9d18726726a8e4d288309a703fc.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors theoretically study the training of transformers under the CoT setting, focusing on using the transformers to solve a k-parity problem. They prove that the transformers with specific parameterization can solve this problem by one-step gradient descent, under the 'imbalanced data' assumptions. In addition, they study the impact of poisoned data when considering such a CoT setting, and provide a condition where the injected poisoned data can hurt the CoT training and hence the prediction."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It is of great importance to study the hurts of data shift toward CoT training, as a small perturbation might have a significant impact after a long chain."}, "weaknesses": {"value": "1. The presentation and writing of this paper are quite confusing and, in many places, misleading. Several mathematical concepts are left poorly defined, and some definitions themselves appear questionable. I have to combine these two issues here, since it is not clear whether the confusion arises from inherently problematic definitions or from vague exposition. In particular, the definitions of data and the Transformer architecture are highly unclear. If we denote by $\\mathbf x\\in \\mathbb{R}^d$ a single input instance for a $k$-parity problem, with its corresponding label $y=\\prod_{i\\in P}\\mathbf x_i$, then $\\mathbf x$ should naturally serve as the sequence input to the Transformer, where each token has dimension 1. However, the authors further introduce another notation \n$\\mathbf x$ whose dimensionality seems to be $n$—the number of data samples—which is highly confusing. It is not clear why the data sample size $n$ should appear as part of the Transformer’s input dimension. The approach of concatenating the same coordinate across i.i.d. data points as one sequence token is both non-standard and conceptually meaningless. For example, in image classification, stacking the first pixel from all images together provides no meaningful information for predicting image labels. This design choice also raises another critical issue: if the Transformer’s structure $n$, then the trained model cannot handle datasets with different sizes, even if they belong to the same problem domain. In that case, how could a Transformer trained on $n$ samples make predictions on a single test sample in Theorem 4.1? Moreover, several notations are not properly specified. For instance, the definition of the position set $P$ is ambiguous—does \n$P$ remain fixed across all data points, or is it part of each data point? While the notation $\\mathcal{D}^P$ possibly suggests that $P$ is fixed, such an important definition must be explicitly stated. The definition of “uniform” is also vague: are the coordinates independent, or is independence irrelevant to the argument? This ambiguity makes it difficult to follow the theoretical assumptions of the paper.\n\n2. In fact, prior works [1, 2] have already studied how Transformers can perform similar forms of token selection using *orthogonal positional embeddings*, covering both cases where $P$ is fixed and where $P$ is treated as part of the data. Given these prior results, I find the assumptions in Theorem 4.1 highly questionable. I cannot understand why, under the condition that all data are uniformly distributed, then Transformer would fail to select the corresponding tokens. Since the authors do not clearly specify how $P$ is constructed or how the training is performed in Theorem 4.1, I assume that in this paper’s setting, $P$ is fixed and the training loss corresponds to the *CoT loss* defined in equation (3). \nUnder this setup, although the original $k$-parity problem could have multiple solutions (because the same $y$ can be generated from different $\\mathbf{x}$, as only the product matters), the CoT loss should make the solution *identifiable*, since it requires every intermediate product in the reasoning chain to match the ground truth. \nThis identifiability ensures that it is fully possible to construct a specific Transformer that successfully performs token selection.\nFor instance, in Figure 1, token 17 only needs to \"remember'' the positions of token 2 and token 3, and such a structure clearly exists under the orthogonal positional embedding setting. \nIn other words, the $k$-parity problem should be \"solvable\" for Transformers, even when the data are \"balanced\". I know there might exist some gaps, but intuitively speaking, each step in the CoT process essentially requires conducting a $\\\\{-1, 1\\\\}$ binary sparse classification, which has already been studied in [1] without requiring any distinction between relevant and irrelevant tokens. \nAlthough I am not certain whether the mathematical proof in this paper is formally correct or the distinctions of mathematical derivations between this paper and  [1], I am highly skeptical that the special reparameterization structure, forward function, and loss function used in this work do not artificially restrict the expressive power of the Transformer model, which also appears in their Theorem 4.2.\n\n3. The authors seem to overlook an essential fact: as I mentioned earlier, each step of the CoT process essentially performs a binary classification task. For a binary classification problem, the worst-case scenario corresponds to random guessing---not to a deterministic error. In some sense, a model that always classifies +1 as -1 is still performing the correct mapping, since the output can simply be flipped afterwards. Therefore, the authors’ analysis of poisoned data is conceptually flawed and mainly reflects the limitations of their model design rather than any inherent poisoning effect. In their formulation, fixing the sign of $V$ completely restricts the model’s expressive power: it prevents the network from performing any \"sign flip\".  I strongly disagree with the conclusion that an odd number of flips is necessarily harmful. For example, if we flip only the final output of the CoT process but keep $V$ trainable---so that the model can learn its own output sign---then such \"poisoned'' data would be entirely harmless for training. From my perspective, the so-called poisoning effect arises only because of an unnecessary and artificial constraint on the model’s parameterization, not because of any fundamental limitation of the Transformer architecture or CoT. Even from the motivation of the design, I also do not find further implications or insights of such 'flipping'. I believe it would be more interesting to investigate the noise perturbation in other settings, like using CoT to conduct GD in [3].\n\n4. Besides these major technical issues, the assumptions and settings of this paper are also somewhat limited and simplified, like one-step GD with a large learning rate, and an over-simplified reparameterization. I would like to point out that [4] does not use such simplification: all the blocks of $V$ and $K^\\top Q$ are trainable. In fact, [5] considers similar transformer structures, i.e. a \"position-only\" Transformer (while they do not consider fixing $V$).\n\n[1]. Zhang et al. Transformer learns optimal variable selection in group-sparse classification.\n\n[2]. Wang et al. Transformers provably learn sparse token selection while fully-connected nets cannot.\n\n[3]. Huang et al. Transformers learn to implement multi-step gradient descent with chain of thought. \n\n[4]. Zhang et al. Trained transformers learn linear models in-context.\n\n[5]. Jelassi et al. Vision transformers provably learn spatial structure."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2TgzQrtQ6c", "forum": "JT6L583Iae", "replyto": "JT6L583Iae", "signatures": ["ICLR.cc/2026/Conference/Submission14209/Reviewer_reZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14209/Reviewer_reZi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312054635, "cdate": 1761312054635, "tmdate": 1762924667827, "mdate": 1762924667827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical analysis of Chain-of-Thought (CoT) under data shifts and data poisoning. Prior works showed that CoT can empower transformers to solve parity efficiently under ideal i.i.d. and clean data conditions. This paper studies how distribution shift and corruption in intermediate CoT steps jointly affect learnability. The authors derive a necessary and sufficient condition (Theorem 4.2) for successful CoT training and show that even small deviations can cause failure."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper first rigorously analyze CoT robustness under distribution shift and label poisoning.\n2. Theorems 4.1 and 4.2 give clear analytic characterizations of success/failure thresholds, including asymptotic rates.\n3. Some insights provided by this work's theoretical study is valuable, such that data shift always leads to worse training performance."}, "weaknesses": {"value": "1. The choice of linear layer function is too artificial and may not generalize well.\n2. The paper does not clearly contrast its theoretical assumptions with prior CoT expressivity works (e.g., Merrill & Sabharwal 2024; Kim & Suzuki 2025) beyond citing them."}, "questions": {"value": "Can authors discuss what a data shift means in reality for example, in a real math dataset. Including this discussion would make the presentation more informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FjImXfKSdA", "forum": "JT6L583Iae", "replyto": "JT6L583Iae", "signatures": ["ICLR.cc/2026/Conference/Submission14209/Reviewer_ppKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14209/Reviewer_ppKD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984116413, "cdate": 1761984116413, "tmdate": 1762924667195, "mdate": 1762924667195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how transformers solve the k-parity problem, focusing on a \"generalized\" k-parity problem where, with probability ρ, the k relevant bits have a 50/50 chance of being all 1 or all -1. The authors theoretically analyze the model's training behavior on Chain of Thought (CoT) data, considering the impact of ρ and data poisoning. Their findings provide insights for real-world LLM training, highlighting vulnerabilities in CoT under data shifts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates that a one-layer transformer can effectively learn to solve the k-parity problem without CoT, as shown in Theorem 4.1, indicating the problem's tractability under imbalanced distributions.\n\n2. It reveals that models trained with CoT data are highly sensitive to data shifts, such as poisoning and distribution changes (regulated by ρ), with Theorem 4.2 providing a rigorous necessary and sufficient condition for training success. This is both novel and insightful for understanding CoT's limitations.\n\n3. Additional experimental results validate the theoretical analysis, offering empirical support for the derived conditions."}, "weaknesses": {"value": "1. The setting is relatively simple: the model only needs to learn specific positions using one-hot positional encoding, and the analysis focuses on one-step gradient updates, which may oversimplify real-world scenarios.\n\n2. The apparent contradiction between Theorems 4.1 and 4.2/4.3 is initially confusing: if the model can effectively solve the problem in one step without CoT, why does it struggle in the CoT setting? After all, the CoT decomposition breaks the k-parity problem into multiple simpler 2-parity steps, so one might expect that applying the method from Theorem 4.1 at each step would lead to effective learning. Moreover, even if the model learns an incorrect CoT, could the final prediction still be correct?\n\n\n3. The experiments, while supportive, are limited in scale and may not fully capture the theoretical complexities, such as the high-dimensional requirements for asymptotic results, and a more realistic dataset, making the practical insight for this paper be very limited."}, "questions": {"value": "Please refer to the weakness part. If there are any misunderstandings on my part, please point them out, and I will reconsider my evaluation of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MiKZV0N3SX", "forum": "JT6L583Iae", "replyto": "JT6L583Iae", "signatures": ["ICLR.cc/2026/Conference/Submission14209/Reviewer_7ooa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14209/Reviewer_7ooa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988294786, "cdate": 1761988294786, "tmdate": 1762924666452, "mdate": 1762924666452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of chain of thought reasoning under data shifts, focusing on the k-parity problem. The authors investigate two types of data shifts: (1) distribution shifts characterized by parameter ρ (measuring deviation from uniform distribution), and (2) data poisoning in CoT reasoning steps. The paper provides necessary and sufficient conditions for successful CoT training under these shifts. Key findings include: CoT has low poisoning tolerance,  i.e, distribution shifts always hurt performance (even when they leak information about target bits), and surprisingly, CoT can perform worse than direct prediction for imbalanced distributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "very interesting question and approach! \n- Novel investigation: Rigorous characterization of the three-way relationship between distribution shift, poisoning, and performance\n- Surprising finding that distribution shift always hurts, even when it leaks information\n- Honest discussion of limitations"}, "weaknesses": {"value": "- Results limited to k-parity with specific CoT decomposition\n- The imbalanced k-parity problem (Theorem 4.1) was already known to be solvable without CoT, so showing CoT performs worse is not surprising. \n- Theoretical guarantees require impractically large d; experiments at realistic scale show different behavior\n- Limited practical impact due to restrictive assumptions (uniform/near-uniform distributions, binary inputs, specific problem structure)"}, "questions": {"value": "- Have you considered other CoT decompositions for k-parity (e.g., different tree structures)? Would they exhibit similar vulnerability?\n- Can any insights transfer to other problems beyond parity? What properties of k-parity are essential to your results?\n-  why does the case with 40% poisoning in first two steps perform comparably to no poisoning when ρ=1?\n- Could you provide finite-sample bounds with explicit dependence on d rather than asymptotic results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yZVQc6F3pA", "forum": "JT6L583Iae", "replyto": "JT6L583Iae", "signatures": ["ICLR.cc/2026/Conference/Submission14209/Reviewer_mTtR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14209/Reviewer_mTtR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762368755910, "cdate": 1762368755910, "tmdate": 1762924665573, "mdate": 1762924665573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}