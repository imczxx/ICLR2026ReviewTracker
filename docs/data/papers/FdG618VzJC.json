{"id": "FdG618VzJC", "number": 12968, "cdate": 1758212264550, "mdate": 1763320551769, "content": {"title": "On the Anisotropy of Score-Based Generative Models", "abstract": "We investigate the role of network architecture in shaping the inductive biases of modern score-based generative models. To this end, we introduce the Score Anisotropy Directions (SADs), architecture-dependent directions that reveal how different networks preferentially capture data structure. Our analysis shows that SADs form adaptive bases aligned with the architecture's output geometry, providing a principled way to predict generalization ability in score models prior to training. Through both synthetic data and standard image benchmarks, we demonstrate that SADs reliably capture fine-grained model behavior and correlate with downstream performance, as measured by Wasserstein metrics. Our work offers a new lens for explaining and predicting directional biases of generative models. Code to reproduce our experiments is included in the supplementary material.", "tldr": "We investigate the role of network architecture in shaping the directional inductive biases of modern score-based generative models.", "keywords": ["Optimization", "Deep Learning", "Diffusion Models", "Inductive Biases"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c1df4b80e810a28b85ef53838a206e23577669a.pdf", "supplementary_material": "/attachment/ff2ab16c293fd8b8e564c477441a5232a6aa4a49.zip"}, "replies": [{"content": {"summary": {"value": "The authors study the inductive biases of neural networks trained on denoising score matching and hypothesize that these inductive biases can be captured by fixed linear bases. In an idealized setting, where the inner weights of a linear network $\\Omega = \\Phi \\Theta$ are trained on denoising score matching for a rank one Gaussian $N(0,vv^T)$, the authors demonstrate that the inductive bias of the family of linear networks is described by the eigenvectors of $\\Phi$. More generally, the authors define the average geometry of a family of score networks in terms of the averaged outer product, where the averaging is done over the parameter distribution and a ‘probing distribution’ on the inputs of the score network. The authors claim that the eigenvectors of this matrix capture the inductive biases of the family of score networks and provide experiments which evidence for this claim."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Understanding the inductive biases of diffusion models is an integral problem in deep learning, and the authors provide some progress towards this endeavor.\n- The ideas of the paper are generally architecture-independent.\n- The paper is generally well-written and it is easy to understand their central claims."}, "weaknesses": {"value": "- The novelty of the paper is limited, as the paper borrows significantly from previous attempts to formalize inductive biases in the discriminative setting [1], [2]. While the paper makes some improvements over previous work [3] in understanding inductive biases for diffusion models (e.g., moving beyond CNNs), some similar ideas are borrowed.\n- I don’t find the central claim of the paper (i.e., that the average geometry describes the inductive bias of score networks) to be very well supported. Theoretically, it is essentially unprincipled except for some results in idealized settings, and the scope of the experiments is pretty narrow. \n\nReferences:\n[1] Ortiz-Jiménez, Guillermo, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. \"Neural anisotropy directions.\" Advances in Neural Information Processing Systems 33 (2020): 17896-17906.\n[2] Radhakrishnan, Adityanarayanan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. \"Mechanism for feature learning in neural networks and backpropagation-free machine learning models.\" Science 383, no. 6690 (2024): 1461-1467.\n[3] Kadkhodaie, Zahra, Florentin Guth, Eero P. Simoncelli, and Stéphane Mallat. \"Generalization in diffusion models arises from geometry-adaptive harmonic representations.\" arXiv preprint arXiv:2310.02557 (2023)."}, "questions": {"value": "I am confused by the statement of Theorem 1, specifically the line ‘the mean error to the optimal solution $v = u_i$ decays as…”. In what sense is $v = u_i$ an ‘optimal solution’, if the optimization is being done over $\\Theta$? It seems like $v = u_i$ is an assumption of the theorem, and indeed the proof uses this fact in Equation (17). Also, I believe Equation (19) has an error, since the vectors $p$ and $q$ are not independent (although the proof claims that they are). In reality, I think you should have something like $$\\mathbb{E}[\\langle p, q \\rangle] =  \\mathbb{E}_{N(0,vv^T)}[x^T (\\Omega^{\\ast})^T \\Phi x] + \\sigma^2 \\mathbb{E}[\\epsilon^T (\\Omega^{\\ast})^T \\Phi \\epsilon] + \\mathbb{E}[\\epsilon^T \\Phi \\epsilon] > 0.$$\n\nThus, it is not clear to me whether the final sentence of Theorem 1 is true. Please let me know if I have misunderstood something here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WTTsLqmGTA", "forum": "FdG618VzJC", "replyto": "FdG618VzJC", "signatures": ["ICLR.cc/2026/Conference/Submission12968/Reviewer_UNQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12968/Reviewer_UNQs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760987420609, "cdate": 1760987420609, "tmdate": 1762923724542, "mdate": 1762923724542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new theoretical framework to analyze the directional inductive biases of score-based generative models (diffusion models). It defines Score Anisotropy Directions (SADs) — architecture-dependent directions in the model output space — which are said to describe how different neural network architectures (e.g., U-Nets, DiTs) preferentially model certain data structures. The authors propose the concept of an average geometry matrix whose eigendecomposition predicts which data directions are more easily learned. They provide both analytical derivations (for simplified settings) and empirical evidence on synthetic and image datasets to support the claim that alignment between data and architectural geometry determines generalization quality. The experiments mainly focus on Wasserstein distances as a measure of generative performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel conceptual framing:**\nThe paper offers an original perspective by proposing SADs as a way to quantify architectural inductive biases in score-based generative models.\n2. **Synthesis of architecture and anisotropy analysis:**\nIt connects the anisotropic conditioning of the optimization landscape to generative modeling performance, extending concepts like Neural Anisotropy Directions (NADs) from discriminative to generative contexts.\n3. **Theoretical–empirical bridge:**\nThe authors combine analytical derivations for simplified cases with experimental validation on U-Net and DiT architectures, showing distinct anisotropy profiles across architectures.\n4. **Insightful empirical angle:**\nThe work provides technical examples illustrating how geometric alignment affects generalization and how this connects to data augmentation effects in diffusion training.\n5. **Potential interpretability contribution:**\nThe proposed framework could, in principle, help explain why certain architectures generalize better or exhibit specific generative artifacts."}, "weaknesses": {"value": "1. **Conceptual overstatement and lack of rigor:**\nMost results are heuristic and not rigorously proven. The central claims (Conjecture 1 and its supporting theorems) are largely empirical conjectures, not formal results. The link between anisotropy and generalization remains speculative.\n2. **Theoretical fragility:**\nThe formal parts (e.g., Theorem 1) rely on linearized and oversimplified assumptions that are far removed from the nonlinear reality of diffusion networks. The claimed generalization to complex architectures is not mathematically justified.\n3. **Limited experimental depth:**\nThe experiments use small-scale datasets (MNIST, CIFAR-10, CelebA-HQ) and low-dimensional toy problems. The evidence is insufficient to validate claims about “predicting generalization before training.”\n4. **Ambiguous interpretation of metrics:**\nThe use of Wasserstein distances as the sole quantitative indicator of generative quality is weak — no perceptual or likelihood-based metrics are provided. The connection between these distances and “generalization” is asserted but not justified.\n5. **Unclear practical relevance:**\nWhile the average geometry can be computed at initialization, it is not demonstrated that this analysis provides actionable insights for model design or training."}, "questions": {"value": "- The matrix $G_\\mathcal{F} = \\mathbb{E}[ F_\\theta (x_\\sigma, \\sigma)F_\\theta(x_\\sigma, \\sigma)^\\top] $ depends on both $P$ and the initialization scheme. How sensitive are your results to these choices?\n- In Theorem 1, the model is assumed linear. How do you justify extrapolating these linear results to complex, nonlinear diffusion architectures?\n- The conjecture linking SADs to eigenvectors of  $G_\\mathcal{F}$ is intuitive, but is there any formal result supporting it for nonlinear or trained networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NROiDxqzEL", "forum": "FdG618VzJC", "replyto": "FdG618VzJC", "signatures": ["ICLR.cc/2026/Conference/Submission12968/Reviewer_NrCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12968/Reviewer_NrCH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677767863, "cdate": 1761677767863, "tmdate": 1762923724002, "mdate": 1762923724002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies directional inductive biases of score-based generative models. The authors introduce Score Anisotropy Directions (SADs), which is an set of orthonormal directions in the output space, obtained as the eigenvectors (smallest to largest) of an average geometry matrix. In a linear DSM setting, the authors show that SGD training leads to anisotropy and converging fastest along small eigenvalue directions. For the general nonlinear models, they conjecture the same ordering and support it with: (i) synthetic rank‑one datasets aligned to different bases; (ii) experiments where performance follows the eigenvector order for U‑Net/iDDPM and DiT. On real datasets, they vary an alignment measure between the data and the model's geometry via orthogonal transforms and find that lower alignment improves the performance, while high alignment underperforms or offers no gain over the baseline. The paper also analyzes average geometry matrix for common architectures and connects SADs to Geometry‑Adaptive Harmonic Bases."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think this is a good empirical paper that proposes an interesting phenomenon for score-based generative models. I especially appreciate the clarity around SADs, how to quantify the \"preferred directions\" in generative models, and the demonstration that lower data–geometry alignment leads to better generation quality. The result is simple to grasp through the manifold hypothesis of data distribution, yet seems to broadly applicable across architectures and optimization algorithms."}, "weaknesses": {"value": "The only weakness I can think of is that, while extensive experiments are conducted to verify the main conjecture, it is rigorously proved only for a linear DSM toy model. The paper would be stronger with an intuitive argument in a wide‑network/NTK or mean‑field limit showing why the SAD eigen‑ordering should persist and clarifying conditions under which their conjecture holds true."}, "questions": {"value": "Could the authors provide intuition for why Conjecture 1 should hold for non‑linear networks, and clarify how your average geometry matrix relates to NTK in the wide‑network/linearization limit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Rrw6veglJD", "forum": "FdG618VzJC", "replyto": "FdG618VzJC", "signatures": ["ICLR.cc/2026/Conference/Submission12968/Reviewer_z2i3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12968/Reviewer_z2i3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972994293, "cdate": 1761972994293, "tmdate": 1762923723332, "mdate": 1762923723332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the framework of *Score Anisotropy Directions* (SADs) as a tool for a better understanding of how score-based generative models perform, by providing a rough prediction on how well the model will perform, according to the geometry of the model output and that of the given dataset. This framework identifies the intrinsic geometric bias which the model in use has, and tells us how well this aligns with the dataset geometry is an indicator of the performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work presents a new lens for studying the performance of score-based generative models. It even offers some novel insights, such as a training recipe that applies appropriate rigid motions to align the data with the SADs, and the bold claim that subspaces with small eigenvalues are easier for the models to learn. \nThe central topic itself is a timely one, as the impact of generative models is rapidly growing, while we still have little understanding on how they work. \nI can see this work serving as a great starting point for the geometric/inductive bias approach to the theoretical understanding of score-based generative models."}, "weaknesses": {"value": "As much as I like the idea and the approach, I also have some questions and concerns about the current state of the paper. \n\nThe writing, especially in the introduction, has room for improvement. For example, while the introduction claims to provide a precise notion of \"geometry\" (around line numbers 46--48), the very first definition of SADs (Definition 1) is somewhat vague. How is a \"preference\" measured, and what does it mean to \"generate data *along* a direction\"?\n\nI would also suggest moving the related works section to the front instead of keeping it at the end with the conclusions, as this would give readers stronger motivation to delve further into the paper.\n\nMeanwhile, I find it quite unclear what the figures are meant to convey, even after reading the entire paper. I don't see what the 3D projection of the $\\mathbb{R}^{256}$ space in Figure 1 is intended to depict, especially with the point clouds obscuring the geometry. It is also not clear how I should interpret the 2D plots in Figure 2 and 3. It would be helpful if the authors could further elaborate on these visualizations.\n\nFor further questions, please see below."}, "questions": {"value": "1. In Theorem 1, is having $\\lambda_{D-1} \\neq \\lambda_D$ an (implicit) assumption? Also, it would be helpful if the statements are written more precisely and rigorously, such as the \"*mean error* to the optimal solution\" and \"SGD steps with respect to $\\Theta$ *for $\\boldsymbol{v} = \\boldsymbol{u}_i$* have *covariance* $\\propto \\lambda_i$; what is the *mean error*, and how does a SGD step be \"with respect to $\\Theta$\" and \"for $\\boldsymbol{v} = \\boldsymbol{u}_i$\" simultaneously, and what could it having a *covariance* mean? I personally think a theorem statement should be clearly understandable without referring to its proof. \n\n2. I am not sure if Markov's inequality (4) is the right way to back up the claim of \"small eigenvalue directions are easier to learn\". After all, Markov's inequality is just an upper bound for the tail probability; it can say something about small eigenvalue directions as the right hand side will be small, but nothing can be inferred from it about the behavior along large eigenvalue directions. \n\n3. The intuitive explanations in Section 3.2 seem reasonable, but only when considered locally around a point *$\\boldsymbol{x}_\\sigma$* that happens to have a large log-density. In defining the *average geometry*, we average out the \"geometry\" with respect to the distribution of *$\\boldsymbol{x}_\\sigma$,* which will smooth (or even worse, possibly cancel) out directions that are only meaningful in a local sense. Thus, although it seems like there is an explanation about this right after Definition 2, it is still unclear to me how the local intuition applies to the notion of average geometry. For example, in an extreme toy case where the data distribution is bimodal in $\\mathbb{R}^2$, with each mode having its own \"meaningful\" directions that are orthogonal to those of the other mode, the resulting average geometry would be meaningless. Is there any intuition as to why we should expect the *average geometry* to possess a global semantic meaning?\n\n4. It seems like Figure 7 is arguing that the traditional training recipe (which amounts to setting $\\boldsymbol{W} = \\boldsymbol{I}$) should not perform well, which is contrary to what we are observing in reality. What is the punchline of this figure, and what am I missing?\n\n5. Why is $\\mathrm{SW}_2$ chosen as the performance measure? Is there a particular reason why more “standard” metrics such as FID or the Inception Score were not used?\n\n6. As a follow-up question of the previous one, how exactly was $\\mathrm{SW}_2$ calculated?\nIt is well known that reliably computing $\\mathrm{SW}_2$ in high-dimensional spaces is challenging, since two random vectors are almost always nearly orthogonal; as a result, a randomly chosen direction is unlikely to serve as a meaningful proxy for $\\boldsymbol{\\theta}$ in equation (3), with very high probability. How is this issue addressed?\n\n7. I hope the authors will consider adding visual results for the experiments on CelebA and CIFAR-10 in the paper. It would also be helpful if they could include a brief verbal description of these results in the rebuttal, since presenting figures may be difficult at that stage.\n\n8. For the notion of average geometry to provide meaningful insights into training diffusion models in practice, it seems like it should ideally capture information that evolves as the model parameters (or their distribution) change during training. However, the current definition appears to depend only on the distribution used at initialization, offering no indication of how the geometry of the outputs evolves as the parameters are updated, nor how it would interact with the data distribution during the training. How does the concept of average geometry account for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eIZleOk1NO", "forum": "FdG618VzJC", "replyto": "FdG618VzJC", "signatures": ["ICLR.cc/2026/Conference/Submission12968/Reviewer_hXcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12968/Reviewer_hXcw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990336109, "cdate": 1761990336109, "tmdate": 1762923722833, "mdate": 1762923722833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the directional preferences of network architecture is theoretically identified and analyzed from the harmonic analysis point of view. The geometry in generative model architecture is clarified and the output space induced by the directional preference of network architecture is decomposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clarity**: This work is clearly presented with informative visualization of preliminary results on iDDPM which enhances the motivations. The logic is generally linear and the whole paper is easy to follow. Theories are well articulated and key takeaways are summarized with clarity at the end of each block of analysis and discoveries.\n\n2. **Quality**: The proposed theories are proved comprehensively and coherently on the importance of directional preference in network architectures and the impact of it. The claimed importance of anisotropic conditions is also in general supported by the phenomena in numerical experiments."}, "weaknesses": {"value": "1. Although the claimed discoveries are supported by experiments, datasets used in experiments are in low resolutions where the highest is only $56\\times 56$. Experiments on higher dimensionality may indicate results on the contrary to the theoretical discoveries. It would be more comprehensive to conduct experiments on datasets such as ImageNet with the resolution of $256\\times 256$.\n\n2. Before this work, there is already existing research discussing and modifying generative models on directional sensitivity, such as Kuramoto Orientation Diffusion Models [1] and Artificial Kuramoto Oscillatory Neurons [2] where [1] proposes a new diffusion model. It would improve the novelty and significance to discuss [1] and [2] and highlight the novelty accordingly.\n \n[1] Yue Song, T. Anderson Keller, Sevan Brodjian, Takeru Miyato, Yisong Yue, Pietro Perona, and Max Welling, Kuramoto Orientation Diffusion Models, 39th Conference on Neural Information Processing Systems, 2025.\n\n[2] Takeru Miyato, Sindy Löwe, Andreas Geiger, Max Welling, Artificial Kuramoto Oscillatory Neurons, ICLR 2025."}, "questions": {"value": "1. Could the authors elaborate on \"directional preferences\" stated? Are directional preferences equivalent to importance of features in training and inference processes? I am wondering how the directional preferences in network architectures will enhance or reduce the features in data such as images.\n\n2. In terms of the motivation, why is it necessary to study the directional preferences in network architectures for generative models, except its absence but prominence in studies of discriminative models? How does it influence the generation quality of generative models?\n\n3. Could the authors clarify the meaning of $\\boldsymbol{v}$? What is the relation between $\\boldsymbol{v}$ and $\\sigma$?\n\n4. Why is  the best performance achieved when the data is not aligned with the “geometry” that is induced by the score network, but instead lives in the subspace defined by its smallest eigenvalues? Could the authors provide an intuitive explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nwTiewYzrf", "forum": "FdG618VzJC", "replyto": "FdG618VzJC", "signatures": ["ICLR.cc/2026/Conference/Submission12968/Reviewer_rSjW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12968/Reviewer_rSjW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996076186, "cdate": 1761996076186, "tmdate": 1762923722357, "mdate": 1762923722357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}