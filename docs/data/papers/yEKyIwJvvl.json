{"id": "yEKyIwJvvl", "number": 12518, "cdate": 1758208367682, "mdate": 1759897504381, "content": {"title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba", "abstract": "We introduce $\\textbf{MAVE}$ ($\\textbf{M}$amba with Cross-$\\textbf{A}$ttention for $\\textbf{V}$oice $\\textbf{E}$diting and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2\\% of listeners rated MAVE-edited speech as perceptually equal to the original, while 24.8\\% prefered the original and 18.0\\% MAVE- demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires $\\sim6\\times$ less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.", "tldr": "MAVE is a cross-attentive Mamba model for voice editing and zero-shot text-to-speech that produces more natural, high-fidelity speech while being faster and more memory-efficient than prior methods.", "keywords": ["Mamba", "cross-attention", "voice editing", "zero-shot TTS", "autoregressive model", "structured state-space model", "speech synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8c081b3d8aeb597edeb63bf7efb90df2bbd6909.pdf", "supplementary_material": "/attachment/4d48284223f8b139bb3b786a23dbae41a4b9efce.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes **MAVE**, a hybrid architecture leveraging a Mamba-based state-space sequence model enhanced with cross-attention for efficient, high-fidelity, and context-aware speech editing and zero-shot text-to-speech (TTS) generation. MAVE models long-range dependencies in audio tokens via Mamba, while dynamically aligning text and audio through cross-attention on phoneme embeddings, enabling bidirectional and precise speech modifications. The empirical evaluation on standard benchmarks demonstrates performance gains in naturalness, intelligibility, and speaker consistency compared to contemporary baselines such as VoiceCraft and FluentSpeech."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The integration of Mamba SSMs with a cross-attention mechanism tailored for audio-text alignment addresses the quadratic inefficiency of transformer-based decoders in long speech generation."}, "weaknesses": {"value": "1. Lack of novelty. This work is essentially a copy of VoiceCraft in every aspect — design, writing, and overall structure — with the only difference being that the model architecture is changed from Transformer to Mamba.\n\n2. A severe lack of baselines, such as F5-TTS and MaskGCT, which both support TTS and editing."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VrGZHpi1PU", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Reviewer_zYYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Reviewer_zYYp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539480901, "cdate": 1761539480901, "tmdate": 1762923385952, "mdate": 1762923385952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes the first structured-state-space model (Mamba) successfully adapted for text-conditional speech generation (Voice Editing and Zero-Shot TTS) by fusing it with cross-attention layers for linguistic alignment. The proposed approach outperforms autoregressive and diffusion-based approaches in fidelity, efficiency, and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is novel and will intrigue the community since it is the first paper that uses MAMBA for these speech tasks according to the authors. I do remember that some people used it in the past but for different tasks (Mamba in Speech: Towards an Alternative to Self-Attention).\n- The paper is very well presented with very good experiments and explanations. Good presentation of both objective and subjective metrics. I do like the very well set up subjective evaluation and the details provided in the Appendix."}, "weaknesses": {"value": "- The only weakness that I see is that the authors did not do a comparison with more Speech Editing or Zero-shot methods. They compared only with VoiceCraft. I would suggest to compare with other models too in order to have a more complete analysis."}, "questions": {"value": "- Interesting masking with a mask token. I haven't seen that before. When I was doing research in Speech Editing we used to use masks of zeros on the mel-spectrogram but now since you use tokens it has to change. Very interesting detail.\n\n- Last paragraph of 3.2.1, can you elaborate a bit more of why MAMBA performs that good? Is there an ablation study for that? This paper will create a lot of discussion in the speech field since the audience is divided on the attention vs SSM, so it would be nice to give explanations on why this works better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cGcLdRirBz", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Reviewer_niSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Reviewer_niSb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956649332, "cdate": 1761956649332, "tmdate": 1762923385591, "mdate": 1762923385591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned speech generation. The core innovation is a hybrid design combining a Mamba (SSM) backbone for efficient, long-sequence audio modeling with a cross-attention mechanism for robust text conditioning. This architecture is designed to effectively manage the significant length mismatch between text and audio modalities.\n\nThe authors evaluate the model on two primary tasks. For speech editing, MAVE achieves performance on the RealEdit benchmark that is **comparable** to the state-of-the-art VoiceCraft model, with results largely within the confidence intervals of the baseline. The model's strengths are more clearly demonstrated in zero-shot text-to-speech (TTS), where MAVE achieves **statistically significant improvements** over VoiceCraft in both MOS naturalness and intelligibility.\n\nA key contribution of this work is its computational efficiency; MAVE requires approximately **6x less inference memory** than the Transformer-based VoiceCraft. The strong zero-shot TTS performance is presented as a direct application of the model's ability to learn speaker characteristics from audio context, a mechanism that is learned during its training on the speech editing (in-filling) task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   **Originality:** The paper's primary original contribution is the MAVE architecture. It proposes a hybrid design that combines a Mamba backbone, chosen for its linear-time efficiency in modeling long audio sequences, with a separate cross-attention mechanism for text conditioning. This architecture is a well-motivated solution for applying SSMs to a cross-modal task, specifically addressing the significant length mismatch between text and audio sequences. This integration of a Mamba decoder with a flexible, length-agnostic cross-attention module is a novel approach in this domain.\n*   **Clarity:** The paper is clearly written. The core architectural idea is presented logically and is supported by Figure 1, which effectively visualizes the text/audio data flow and the detailed decoder block. The authors explain the token rearrangement strategy that unifies the speech editing and TTS tasks under a single autoregressive framework. The inclusion of a theoretical complexity analysis in the appendix (A.4) also adds to the clarity of the model's proposed benefits.\n*   **Quality:** The paper's claims are supported by a thorough evaluation. On the primary speech editing task (Table 1), the MOS scores for naturalness and intelligibility are comparable to the VoiceCraft baseline, with results largely falling within the reported confidence intervals. While this demonstrates competitive performance, the model's quantitative strengths are more clearly shown in the zero-shot TTS evaluation (Table 2). Here, the model achieves a statistically significant improvement over the baseline in both naturalness (3.48 vs. 3.22) and intelligibility (4.20 vs. 4.01). Furthermore, the ablation study (Table 5) is of high quality and provides a strong justification for the Mamba + Cross-Attention design, showing it outperforms both a \"Mamba-only\" (concat) approach and a \"Transformer + Cross-Attention\" model.\n*   **Significance:** The significance of this work is twofold. First, from a practical standpoint, the paper presents a model that achieves competitive-to-superior generative quality while being significantly more efficient. The reported \\~6x reduction in inference memory (Table 4) is a significant practical contribution, potentially making SOTA-level speech generation more accessible. Second, from a scientific standpoint, this work provides a viable blueprint for replacing the dominant Transformer backbone in complex, text-conditioned autoregressive audio models. It demonstrates that SSM-based hybrids can offer a favorable trade-off between performance and efficiency, which may encourage further research into similar architectures."}, "weaknesses": {"value": "The claim in the abstract that the model is \"not explicitly trained on the \\[zero-shot TTS] task\" is potentially misleading. Section 3.2.3 clarifies that the editing task uses surrounding audio for speaker context, while the TTS task uses a prepended prompt. The core mechanism—conditioning on audio tokens for speaker identity—appears to be a fundamental part of the training objective, not a purely emergent capability. A more precise framing would be that zero-shot TTS is a direct and successful *application* of the speaker context mechanism learned during in-filling. The authors are encouraged to clarify this framing in the final version.\n\nThe paper's claims about state-of-the-art *speech editing* performance are not strongly supported by the data in Table 1. The MOS scores for MAVE versus VoiceCraft on the RealEdit benchmark are very close, with overlapping confidence intervals, suggesting performance is, at best, on par. This SOTA claim is further weakened, as the authors note, by the evaluation being on an incomplete version of the RealEdit benchmark. In contrast, the model's superiority is much clearer in the zero-shot TTS task (Table 2) and its efficiency (Table 4). The paper would be stronger if it re-framed its primary contribution around these more significant and clearly demonstrated achievements: namely, achieving *comparable* editing quality and *superior* TTS quality with a *dramatically* more efficient architecture.\n\nA significant practical limitation of the \"speech editing\" framework is its reliance on manual segmentation. The model requires the user to explicitly define the \"before\" and \"after\" audio spans for an edit. It cannot, for example, take a full audio file and a corrected transcript and \"automatically find and fix\" the errors. This makes it a powerful *component* for an editing tool, but not a fully automatic \"corrector,\" which limits its immediate practical utility. This limitation should be discussed, and a constructive path for future work would be to investigate integrating this model with an automatic text-audio aligner to create a true, end-to-end \"find-and-fix\" system.\n\nThe zero-shot TTS evaluation, while showing strong results on LibriTTS, could be made more robust. First, the evaluation is on clean read-aloud speech, which does not directly test the model's main strength: its training on \"in-the-wild\" Gigaspeech audio. Second, Table 3 shows a clear trend of the performance gap to ground truth widening as the generated text gets longer. To strengthen the paper's claims, the authors could (1) add a TTS evaluation on an \"in-the-wild\" test set (e.g., held-out Gigaspeech samples) to validate its robustness, and (2) provide a brief analysis of *why* long-form quality degrades (e.g., is it speaker similarity or text alignment?) to better guide future work on Mamba's long-context state."}, "questions": {"value": "*   Could the authors please clarify the \"not explicitly trained for zero-shot TTS\" claim? Section 3.2.3 implies that speaker context is learned from surrounding audio tokens during editing. How does this mechanism fundamentally differ from prepending a reference prompt for zero-shot TTS, which seems like a direct application of the same learned capability?\n*   The practical utility of the editing feature relies on manual segmentation of the \"before\" and \"after\" audio. Have the authors investigated a path to a fully automated system, for instance, by combining MAVE with a text-audio aligner that could automatically identify and propose mismatched spans for correction?\n*   Given the paper's focus on Mamba's efficiency, did the authors experiment with replacing the Transformer-based text encoder with a Mamba-based one? This could create a more architecturally homogenous model and potentially yield further efficiency gains.\n*   Section 3.2.3 mentions \"cross-speaker editing\" as a possibility. Was this capability evaluated? For example, how well can the model edit a phrase into a target speaker's voice using a reference prompt from a *different* speaker?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The primary ethical concern is the dual-use nature of this technology, which falls under `potential harmful insights, method & application`. The MAVE model is a highly effective tool for generating audio deepfakes. Its zero-shot capability significantly lowers the barrier for misuse, requiring only a few seconds of a target's voice to generate new, arbitrary speech. The paper's own results show this synthetic speech is often indistinguishable from the original, making it a potent tool for misinformation, such as faking statements from public figures, or for fraud.\n\nThese risks directly impact `privacy, security, and safety`. The method enables severe violations of an individual's voice privacy by allowing non-consensual cloning. This, in turn, poses a security risk, as the technology could be used to bypass voice-based authentication systems. Such applications threaten personal and financial safety through impersonation scams, extortion, or targeted harassment.\n\nWhile the authors acknowledge these risks in their ethics statement, the mitigations they propose are societal suggestions rather than technical safeguards. Given the model's high performance, robustness, and potential for harm, this paper warrants a specialized review by the ethics committee to fully assess its societal impact."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nH0vt5N42y", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Reviewer_zZAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Reviewer_zZAj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975461183, "cdate": 1761975461183, "tmdate": 1762923385267, "mdate": 1762923385267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part - A)"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback and constructive comments. Few key issues were highlighted by multiple reviewers. In particular, questions were raised about:\n\n(a) Limited comparison with recent models apart from VoiceCraft and FluentSpeech \n\n(b) Validation of the theoretical linear complexity of our proposed MAVE\n\nWe provide a unified response to these points here, before addressing individual comments in subsequent sections.\n\n$\\textbf{Limited comparison with recent models (F5-TTS, MaskGCT):}$ \n\nWe fully agree that a comparison with more recent speech editing models is essential. Below we provide results for F5-TTS and MaskGCT and we refer to them in detail in our response below:\n\n**Table R1.** Comparison of speech editing models on RealEdit benchmark\n\n| Model                 | WER (medium) ↓ | WER (large) ↓ | UTMOS ↑ |\n|--------------------|--------------------|---------------|---------|\n| F5-TTS               | 9.5                      | 11.1             | 3.11    |\n| MaskGCT           | 8.6                      | 10.5             | 3.11    |\n| VoiceCraft           | 6.9                      | 8.4              | 3.45    |\n| **MAVE (ours)**   | **5.9**                | **7.5**          | **3.74**|\n| Ground Truth      | 5.2                      | 6.8               | 3.92    |\n\n\nWe would like to clarify that the reason these methods were not included in our initial comparisons is that they are primarily presented as zero-shot text-to-speech (TTS) systems, and neither provides quantitative results nor dedicated benchmarks for speech editing in their original publications. That said, MaskGCT briefly notes on its paper that it can support speech editing and includes two illustrative examples on their webpage. On the other hand, the authors of F5-TTS do not mention speech editing capabilities in their paper, but their GitHub repository contains code demonstrating how to adapt the model for this task. So, we adjusted the official codebases of F5-TTS and MaskGCT for the speech editing and evaluated on RealEdit benchmark. To ensure a fair comparison, all methods utilized the Montreal-Forced-Aligner (MFA) [A] for precise text-audio alignment. The results (see Table R1) demonstrate that MAVE maintains a significant performance advantage over these baselines, achieving the lowest Word Error Rate (WER) and the highest perceptual quality (UTMOS) [B]. \n\nFurthermore, we have evaluated MaskGCT and F5-TTS for the TTS task on LibriTTS (dev.clean) with more clean audios (see Table R2). Also, in direct response to reviewer feedback on robustness, we have conducted an additional evaluation on the noisier dev.other split (see Table R3). Our method yields the best scores in speaker similarity (SIM) and perceptual quality (UTMOS) for both noisy and clean subsets.\n\nIt is important to underscore that our paper's contribution is a state-of-the-art speech editing model. Its ability to yield $\\textbf{``very competitive\"}$ zero-shot TTS results (as mentioned in the main paper) is presented as a strong secondary demonstration of its general in-context learning capabilities, not as a new SOTA for the TTS task itself.\n\n**Table R2.** Comparison of models on LibriTTS **clean** (dev.clean) subset\n| Model                | WER (medium) ↓ | WER (large) ↓ | SIM ↑ | UTMOS ↑ |\n|:------------------:|:--------------:|:-------------:|:-----:|:-------:|\n| F5-TTS              | 5.8                | 6.9               | 0.56  | 3.71    |\n| MaskGCT          | 7.7                | 9.1               | 0.57  | 3.75    |\n| VoiceCraft          | 7.5               | 9.3                | 0.55  | 3.93    |\n| **MAVE (ours)**  | **6.6**          | **7.4**           | **0.57** | **4.03** |\n\n**Table R3.**  Comparison of models on LibriTTS **noisy** (dev.other) subset. Ground Truth (GT) is shown separately for reference.\n| Model                 | WER (medium) ↓ | WER (large) ↓ | SIM ↑ | UTMOS ↑ |\n|:------------------:|:------------------:|:-------------:|:-----:|:-------:|\n| F5-TTS               | **6.6**        | **8.6**       | 0.52      | 3.50      |\n| MaskGCT           | 8.6             | 10.1          | 0.49      | 3.37      |\n| VoiceCraft          | 8.1             | 9.8             | 0.54      | 3.54      |\n| **MAVE (ours)**  | 6.8             | 8.9             | **0.56** | **3.68** |\n| GT                      | 3.8             | 4.9             | 0.64      | 3.82      |"}}, "id": "5D00SkjcGJ", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729259180, "cdate": 1763729259180, "tmdate": 1763729259180, "mdate": 1763729259180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part - A)"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback and constructive comments. Few key issues were highlighted by multiple reviewers. In particular, questions were raised about:\n\n(a) Limited comparison with recent models apart from VoiceCraft and FluentSpeech \n\n(b) Validation of the theoretical linear complexity of our proposed MAVE\n\nWe provide a unified response to these points here, before addressing individual comments in subsequent sections.\n\n$\\textbf{Limited comparison with recent models (F5-TTS, MaskGCT):}$ \n\nWe fully agree that a comparison with more recent speech editing models is essential. Below we provide results for F5-TTS and MaskGCT and we refer to them in detail in our response below:\n\n**Table R1.** Comparison of speech editing models on RealEdit benchmark\n\n| Model                 | WER (medium) ↓ | WER (large) ↓ | UTMOS ↑ |\n|--------------------|--------------------|---------------|---------|\n| F5-TTS               | 9.5                      | 11.1             | 3.11    |\n| MaskGCT           | 8.6                      | 10.5             | 3.11    |\n| VoiceCraft           | 6.9                      | 8.4              | 3.45    |\n| **MAVE (ours)**   | **5.9**                | **7.5**          | **3.74**|\n| Ground Truth      | 5.2                      | 6.8               | 3.92    |\n\n\nWe would like to clarify that the reason these methods were not included in our initial comparisons is that they are primarily presented as zero-shot text-to-speech (TTS) systems, and neither provides quantitative results nor dedicated benchmarks for speech editing in their original publications. That said, MaskGCT briefly notes on its paper that it can support speech editing and includes two illustrative examples on their webpage. On the other hand, the authors of F5-TTS do not mention speech editing capabilities in their paper, but their GitHub repository contains code demonstrating how to adapt the model for this task. So, we adjusted the official codebases of F5-TTS and MaskGCT for the speech editing and evaluated on RealEdit benchmark. To ensure a fair comparison, all methods utilized the Montreal-Forced-Aligner (MFA) [A] for precise text-audio alignment. The results (see Table R1) demonstrate that MAVE maintains a significant performance advantage over these baselines, achieving the lowest Word Error Rate (WER) and the highest perceptual quality (UTMOS) [B]. \n\nFurthermore, we have evaluated MaskGCT and F5-TTS for the TTS task on LibriTTS (dev.clean) with more clean audios (see Table R2). Also, in direct response to reviewer feedback on robustness, we have conducted an additional evaluation on the noisier dev.other split (see Table R3). Our method yields the best scores in speaker similarity (SIM) and perceptual quality (UTMOS) for both noisy and clean subsets.\n\nIt is important to underscore that our paper's contribution is a state-of-the-art speech editing model. Its ability to yield $\\textbf{``very competitive\"}$ zero-shot TTS results (as mentioned in the main paper) is presented as a strong secondary demonstration of its general in-context learning capabilities, not as a new SOTA for the TTS task itself.\n\n**Table R2.** Comparison of models on LibriTTS **clean** (dev.clean) subset\n| Model                | WER (medium) ↓ | WER (large) ↓ | SIM ↑ | UTMOS ↑ |\n|:------------------:|:--------------:|:-------------:|:-----:|:-------:|\n| F5-TTS              | **5.8**               | **6.9**               | 0.56  | 3.71    |\n| MaskGCT          | 7.7                | 9.1               | 0.57  | 3.75    |\n| VoiceCraft          | 7.5               | 9.3                | 0.55  | 3.93    |\n| **MAVE (ours)**  | 6.6          | 7.4           | **0.57** | **4.03** |\n\n**Table R3.**  Comparison of models on LibriTTS **noisy** (dev.other) subset. Ground Truth (GT) is shown separately for reference.\n| Model                 | WER (medium) ↓ | WER (large) ↓ | SIM ↑ | UTMOS ↑ |\n|:------------------:|:------------------:|:-------------:|:-----:|:-------:|\n| F5-TTS               | **6.6**        | **8.6**       | 0.52      | 3.50      |\n| MaskGCT           | 8.6             | 10.1          | 0.49      | 3.37      |\n| VoiceCraft          | 8.1             | 9.8             | 0.54      | 3.54      |\n| **MAVE (ours)**  | 6.8             | 8.9             | **0.56** | **3.68** |\n| GT                      | 3.8             | 4.9             | 0.64      | 3.82      |"}}, "id": "5D00SkjcGJ", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729259180, "cdate": 1763729259180, "tmdate": 1763739087544, "mdate": 1763739087544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAVE, an autoregressive TTS model that integrates a Mamba backbone (for efficiency) with cross-attention (for text-speech alignment). The inspiration comes from the fact that existing methods are often either high-fidelity but expensive, like Transformers, or efficient but struggle with coherence, like diffusion models . The paper demonstrates that MAVE achieves state-of-the-art performance in speech editing on the RealEdit benchmark as compared to VoiceCraft with 6x less inference memory. It also outperforms VoiceCraft in speaker similarity and naturalness for zero-shot TTS."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution of this paper is integrating Mamba with cross-attention to allow conditioning the model on text without explicit alignments. While I am not entirely familiar with related work in this area, this seems to be one of the first papers to do this and is a strong contribution. The ablations in Table 5 support the usefulness of MAVE having both Mamba and cross-attention; Mamba-only and Transformer-only underperform MAVE.\n2. The human evaluations show that the model is essentially perceptually equal or better than the ground-truth speech, which is encouraging."}, "weaknesses": {"value": "1. MAVE is only compared to Voicecraft (over all test examples) and FluentSpeech (over a 14-example subset). There are lots of new open-source TTS models, many over a year old; F5-TTS, MaskGCT, VoiceStar. The paper lacks comparisons to a lot of these models and without these, the paper’s claim of state-of-the-art results ‘outperforming leading autoregressive and diffusion models’ is misleading.\n2. The authors emphasize one of MAVE’s main advantages is its linear-time complexity. However, the results in Table 4 show that MAVE is actually slower than VoiceCraft on the RealEdit benchmark. The claim of superior speed for longer sequences is purely theoretical (discussed in Appendix A.4)  and is not validated with an experiment. I’d recommend the authors experiment with longer text generations and show that the model is much faster than baselines experimentally."}, "questions": {"value": "1. Can you provide a plot (e.g., sequence length vs. inference time) that shows the crossover point where MAVE actually results in faster wall-clock speed than theTransformer's quadratic scaling?\n2. The model’s naturalness reduces as the length of the generation increases (Table 3), as expected. You attribute this problem to the training dataset, which has examples of moderate length. However, it is also possible that the model architecture cannot maintain long-range coherence (although theoretically, given that it is based on Mamba plus cross-attention, it should). Can you design and run an experiment that disentangles these two possible causes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKjIDqp3Sa", "forum": "yEKyIwJvvl", "replyto": "yEKyIwJvvl", "signatures": ["ICLR.cc/2026/Conference/Submission12518/Reviewer_CPTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12518/Reviewer_CPTT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762586336635, "cdate": 1762586336635, "tmdate": 1762923384981, "mdate": 1762923384981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}