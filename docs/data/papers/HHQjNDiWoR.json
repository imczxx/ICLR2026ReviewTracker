{"id": "HHQjNDiWoR", "number": 24888, "cdate": 1758361591454, "mdate": 1759896743744, "content": {"title": "Can LLMs Serve as Causal Inference Agents? A Study on Post-Training Methods", "abstract": "Despite the potential of Large Language Models (LLMs) to democratize causal inference, they currently struggle with quantitative reasoning. This paper investigates whether post-training can transform an LLM into a practical and accessible causal inference agent for non-professionals. To facilitate this, we first introduce the DeepCausal dataset, a novel collection of seven computational causal inference tasks designed for both training and evaluation. We then propose DeepCausal, an LLM-based agent that enables users to perform complex causal analysis using natural language. Our core methodology involves a comprehensive comparison of online and offline post-training techniques. We find that while offline training equips LLMs with fundamental causal concepts, online post-training is crucial for teaching them how to apply these rules to solve problems, resulting in a significantly more effective, robust, and generalizable model. Our extensive experiments demonstrate that DeepCausal effectively performs causal effect estimation, providing clear, interpretable explanations in natural language. By lowering the technical barrier, our work makes complex causal analysis accessible to a broader audience and establishes the viability of using post-trained LLMs for sophisticated causal reasoning.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Causal Inference", "Post-training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b5c34303357050b135c7a1437d103a132577ff7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author introduced the DeepCausa dataset (with variants) for training large language models in causal reasoning and evaluated the effectiveness of popular post-training methods in enabling models to perform this task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work explores the suitability of using LLMs for certain causal tasks, representing a non-trivial effort that provides valuable insights and lays the groundwork for further research.\n2. The experiments are comprehensive and well-documented, offering detailed analyses across multiple aspects, including generalization, internalization, and robustness of the models."}, "weaknesses": {"value": "1. The causal tasks and datasets used in this paper assume that the required statistics (e.g., probabilities for do-operators) are explicitly provided in the questions, meaning the LLM’s role is largely retrieving numbers and performing simple arithmetic. This assumption limits the applicability of the framework to more complex causal reasoning tasks.\n2. In the motivation example in Figure 1, only two probabilities are provided: P(B=1|A=0) and P(B=1|A=1). Probabilities like P(B=0|A=1) or P(B=0|A=0) are omitted, which seems sufficient for computation. If all necessary information is provided in this way, the problem could be solved heuristically without requiring an LLM, raising questions about the necessity of the model.\n3. One of the main contributions of this work is the training dataset. However, without a clearer understanding of its quality (such as question diversity and entity variety), its usefulness for further research is limited. Providing supplementary material with detailed dataset statistics would strengthen the contribution."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ND48xg4xjK", "forum": "HHQjNDiWoR", "replyto": "HHQjNDiWoR", "signatures": ["ICLR.cc/2026/Conference/Submission24888/Reviewer_srnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24888/Reviewer_srnV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449324197, "cdate": 1761449324197, "tmdate": 1762943234468, "mdate": 1762943234468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the influence of post-training techniques on the LLMs' causal inference abilities. Authors introduce the DeepCausa benchmark, a comprehensive dataset that contains seven core training causal tasks and five test tasks. In the experiments, the authors evaluate five post-training methods, including SFT, DPO, KTO, PPO, and GRPO. Finally, authors conclude that post-training approaches can enhance LLMs' causal inference abilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is important and practical. Both post-training methods and causal reasoning are essential for LLMs.\n\n2. The results analyses are comprehensive, and the authors analyze diverse aspects of agents, including the generalization, internalization, and robustness."}, "weaknesses": {"value": "1. The proposed benchmark and source codes are not open-sourced.\n\n2. I think the motivation may be a little contradictory. Specifically, in the introduction, the authors claim that \"we can develop a causal\ninference agent that explains its assumptions and reasoning in plain language.\" However, in Figure 1, it seems that LLMs are still mainly doing the numerical calculation rather than a detailed language explanation. Besides, the term \"backdoor adjustment set\" may still be hard to understand for non-experts in causal inference. \n\n3. Since there already exist other formal causal reasoning benchmarks (e.g., the CLADDER [1]), I would suggest that authors add an individual section on the differences between their newly proposed DeepCausa and existing benchmarks. Why can‘t other benchmarks test the abilities of post-training methods?\n\n4. The related work section is too short. I think there should be at least two separate parts: post-training methods for LLMs, and LLMs' causal inference abilities. Authors should consider revising the related work.\n\n5. I think Table 1 is unclear and could be misleading. Authors should consider listing other models, post-training methods, and base model (DeepSeek-R1-Distill-Qwen-14B) with separate lines. Currently, it's hard for readers to tell which one is the baseline simply from the table.\n\n6. I believe only one base model (DeepSeek-R1-Distill-Qwen-14B) is not enough. Authors should consider including more base models to verify the generality of their findings.\n\n> [1] Jin Z, Chen Y, Leeb F, et al. Cladder: Assessing causal reasoning in language models[J]. Advances in Neural Information Processing Systems, 2023, 36: 31038-31065."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OoeaCigi4S", "forum": "HHQjNDiWoR", "replyto": "HHQjNDiWoR", "signatures": ["ICLR.cc/2026/Conference/Submission24888/Reviewer_cKjQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24888/Reviewer_cKjQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451473395, "cdate": 1761451473395, "tmdate": 1762943233803, "mdate": 1762943233803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **DeepCausa**, a new benchmark and dataset for evaluating and training LLMs on causal inference tasks. It formulates seven core causal estimation problems (ATE, CDE, ETT, NDE, NIE, PN, PS) as natural-language reasoning questions, and enables reinforcement learning (especially GRPO) by providing programmatically computable rewards. Experiments show that a 14B model trained with GRPO reaches ~93% accuracy on the benchmark, surpassing larger models, while maintaining performance on math reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces the first causal dataset that supports RL-based training with automatic rewards.\n2. Provides quantitative evidence that GRPO can improve causal reasoning accuracy without degrading general reasoning (math)."}, "weaknesses": {"value": "1. The “agent” claim is overstated; the model remains a passive CoT generator without environment interaction or intervention ability.\n2. Evaluation is confined to the same synthetic distribution used for training; no results on external causal benchmarks (CLadder, CLEAR, CaLM) are reported.\n3. No ablation on reward shaping or robustness to real data."}, "questions": {"value": "1. How well would the trained model transfer to unseen causal benchmarks such as CaLM or CLadder?\n2. Could the benchmark be extended to allow environment-level interaction (e.g., tool-based causal discovery)?\n3. Does the model’s performance degrade when SCM variables have realistic semantics rather than random symbols?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vXTdOJMTuC", "forum": "HHQjNDiWoR", "replyto": "HHQjNDiWoR", "signatures": ["ICLR.cc/2026/Conference/Submission24888/Reviewer_3eeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24888/Reviewer_3eeJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625545543, "cdate": 1761625545543, "tmdate": 1762943233534, "mdate": 1762943233534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether post-training methods can enhance the causal inference capabilities of large language models (LLMs). The authors introduce DeepCausa, a comprehensive benchmark comprising seven causal tasks for training and five test sets for evaluation. They systematically compare five post-training methods—SFT, DPO, KTO, PPO, and GRPO—and demonstrate that online RL methods, particularly GRPO, significantly improve the causal reasoning abilities of smaller LLMs, enabling them to outperform larger baseline models. The study also evaluates generalization, internalization, and robustness under distribution shift and noisy data conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental design is thorough, covering multiple causal tasks, training methods, and evaluation dimensions.\n\n2. The introduction of the DeepCausa benchmark is a valuable contribution, providing a structured dataset for training and evaluating causal inference agents.\n\n3. The analysis is comprehensive, with clear comparisons across methods and detailed ablation studies."}, "weaknesses": {"value": "1. The abstract could be more concise and formal. For instance, the phrase “To this end, this paper investigates whether post-training can turn LLMs into effective causal inference agents” could be rephrased to better align with academic tone.\n\n2. The introduction repeatedly uses “we” and could be structured more objectively. For example, “We then systematically evaluate…” could be replaced with a more formal passive or impersonal construction.\n\n3. The paper lacks a discussion on the practical utility of using LLMs for formal causal reasoning, especially given the existence of specialized causal inference tools (e.g., DoWhy, CausalML). The authors should clarify the real-world scenarios where an LLM-based agent would be preferable.\n\n4. While the DeepCausa dataset is introduced, its naming and branding could be more distinctive (e.g., “CausalAgent-Bench” or similar) to enhance recognition and reuse.\n\n5. There is no direct comparison with existing causal reasoning benchmarks (e.g., CLADDER, CLEAR). A dedicated section explaining how DeepCausa differs and why it is better suited for evaluating post-training methods would strengthen the contribution.\n\n6. The figures (e.g., Figure 1) use light colors and small text, making them difficult to read. The samples are overly detailed; a more abstract and summarized visualization would improve clarity and impact."}, "questions": {"value": "1. How does DeepCausa compare to existing causal reasoning benchmarks in terms of task coverage and difficulty?\n\n2. Can the authors provide more insight into why online RL methods (especially GRPO) outperform offline methods so significantly?\n\n3. What are the limitations of using synthetic SCMs for training, and how might this affect real-world applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lrid65ewkv", "forum": "HHQjNDiWoR", "replyto": "HHQjNDiWoR", "signatures": ["ICLR.cc/2026/Conference/Submission24888/Reviewer_dTGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24888/Reviewer_dTGZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255000438, "cdate": 1762255000438, "tmdate": 1762943233109, "mdate": 1762943233109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}