{"id": "W5BPGXR9jf", "number": 16222, "cdate": 1758261906788, "mdate": 1759897253604, "content": {"title": "NerVE: Nonlinear Eigenspectrum Dynamics in LLM Feed-Forward Networks", "abstract": "We introduce NerVE, a unified eigenspectral framework for understanding how feed-forward networks (FFNs) in large language models (LLMs) organize and regulate information flow in high-dimensional latent space. Despite FFNs dominating the parameter budget, their high-dimensional dynamics remain poorly understood. NerVE addresses this gap through lightweight, memory-efficient tracking of eigenspectrum dynamics via four complementary metrics: Spectral Entropy (dispersion), Participation Ratio (effective dimensionality), Eigenvalue Early Enrichment (top-heaviness), and Jensen-Shannon divergence (distributional shifts). Our {\\em key insight} is that FFN nonlinearities reinject and reshape variance across eigenmodes, fundamentally governing latent dimension utilization. We validate NerVE across model scales and diverse architectural configurations that each uniquely shape FFN dynamics: normalization strategies (PreLN, PostLN, MixLN, Norm-Free) controlling variance flow; FFN weight geometries constraining latent space; positional encoding and activation functions modulating information propagation. Across these settings, NerVE consistently recovers stable spectral signatures that correlate with model's generalization ability and respond predictably to design choices, providing actionable insights for architectural optimization beyond trial-and-error.", "tldr": "We introduce NerVE, a lightweight eigenspectral framework revealing how FFN nonlinearities reinject and reshape variance across eigenmodes.", "keywords": ["Eigenspectral analysis", "feed-forward networks", "Nonlinearity", "latent space geometry", "Large Language Models (LLMs)"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63dd5fc24ec5d6c2d789a341d55624838388f35f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This papers proposed to study the MLP/FFN sub-layers in LLM transformers using a spectral analysis. The authors argue that studying the activations immediately before and after the non-linearity (GeLU or ReLU or SwiGLU) allows one to capture the non-linear dynamics of the FFN. The authors proposed 4 metrics on the eigenspectrum of the covariance matrix that capture different effects (how top heavy the spectrum is, how much it has changed after the FFN). Studying these metrics in decode-only transformers shows several effects, most notably the FFN leads to more uniform spectrum/less top heavy eigenspectrum (e.g. fig 1). The authors also study several architectural design choices and how they affect these 4 metrics in some cases.\n\nI am giving a 6 but would probably have given a 5 if there was the option."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic of studying the effect of FFNs in LLMs is interesting and the idea of using the eigenspectrum dynamics to do so is well motivated.\n- Several results have interesting insights such as in section 3.1, the FFN nonlinearity redistributes the variance across various eigenvalues.\n- Studying several architectural choices such as layernorms and their positioning or spectral norm is interesting (although also see \"weaknesses\" about how thorough this is)."}, "weaknesses": {"value": "1. Overall, I think the paper identifies an interesting area but feels somewhat underdeveloped. Some particular areas where this comes across:\n\na. There doesn't seem to be a consistent message across section 3.2-3.5. Each of the subsections looks at a different architectural choice but the conclusions don't seem to tie together, e.g. how does the finding that rope prevents mid-to-deep spectral connect to the positioning of LN? For me the most interesting part of these sections was in section 3.4 that normalised participation ratio can be used as a diagnostic for healthy scaling, which I would maybe try to develop more into a consistent narrative across the section. For example, perhaps bad hyperparameter scaling choices (say the variance of the weight init or LR not scaling with width/depth) will become apparent via the normalised participation ratio not being consistent across scales, whereas with healthy scaling choices the PR is constant across scales (as seems to be with Pre-LN in figure 6)?\nb. It feels like there could be some theoretical analysis here that could strengthen the foundations of the paper. E.g. what should one expect the dynamics of the Nerve metrics to be before/after a non-linearity under \"healthy\" behaviour and \"unhealthy\" behaviour. Should it be scale invariant?\nc. I would study mixture of experts first before choosing to study e.g. RoPE or LN positioning, when discussing the effect of FFN architectural choices. This feels like an oversight. If it is too expensive to train and MoE then it should be possible to just take an open-source pretrained one and study the final checkpoint (or multiple checkpoints if available)?\nd. Some of the claims are quite strong for being backed up by one experiment. E.g. in the discussion around hyperspherical normalisation the authors write \"EEE post values remain high across depth, indicating the persistence of dominant directions despite the extended capacity\", which doesn't seem obvious to me looking at Figure 5 (the early layers seem higher in weight and spectral normalisation).\ne. I think the choices of Pre-FFN and Post-FFN are fine (at the top of page 3), but would also be interested to know what would happen if one studied x itself or FFN(x). Maybe more interesting for the latter is to study x + FFN(x) as maybe it's most important to understand how the FFN affects the residual stream?\n\n2. It could be argued that spectral entropy, participation ratio, and early eigenvalue enrichment all look at a similar quantity (how dominant leading eigenvalues are). Indeed, in the plots (say figures 1 or 3) the first three columns look correlated. I'd ask what the benefit of these three separate metrics is, especially given that they add a lot of acronyms to the paper which the reader has to juggle around in his/her head, which can be confusing."}, "questions": {"value": "1. What is the effect of RMSNorm not LayerNorm, as 3.2 is motivated by the centering of LN (which does not exist in RMSNorm).\n2. Likewise, what if one uses a different optimiser than Adam?\n3. \"The EE score is... average vertical distance\" is this not just the area between?\n\ntypos:\n- \"quantifies\" not quantify line 201\n- \"yields\" not yield line 222\n- \"t' in line 69"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MSMFa3REzY", "forum": "W5BPGXR9jf", "replyto": "W5BPGXR9jf", "signatures": ["ICLR.cc/2026/Conference/Submission16222/Reviewer_jJuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16222/Reviewer_jJuL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760896472881, "cdate": 1760896472881, "tmdate": 1762926381787, "mdate": 1762926381787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"NerVE,\" a framework to analyze LLM feed forward networks by tracking the eigenspectrum of their pre- and post-activation covariance matrices. Using four metrics (spectral entropy, participation ratio, eigenvalue early enrichment, and JS divergence), it argues that the FFN's primary role is to \"reinject variance\" by taking the top-heavy output of attention and \"flattening\" its spectral distribution. This \"spectral reshaping\" increases the effective dimensionality for the next layer. The authors use this framework to provide geometric explanations for the effectiveness of architectural choices like Pre-LayerNorm and RoPE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core claim—that FFNs function as spectral reshapers to re-awaken inactive dimensions is a compelling and intuitive explanation for their role. It provides a strong conceptual model that moves beyond viewing FFNs as simple key-value memories.\n2. The chosen suite of four metrics is a strength. While SE and PR are related, the addition of EEE (to distinguish between different types of flat spectra) and, crucially, JS Divergence (to quantify the nonlinearity's effect) provides a more complete picture than any single metric.\n3. The experiments effectively isolate variables. The norm-free analysis (Section 3.2), for instance, is a good way to demonstrate the FFN's compensatory role, showing that ReLU's piece-wise linear nature provides a regularization effect that the smoother GELU lacks."}, "weaknesses": {"value": "1. The paper makes claims about \"LLMs\" but bases its findings on very small models (70M-130M). These spectral dynamics are not guaranteed to hold at the 1B+ parameter scales where architectural optimization is most critical. The findings need to be validated on larger models.\n2. The paper repeatedly shows that \"healthy\" spectra (high PR, low EEE) correlate with low validation loss but fails to prove causation. It's just as likely that a well-optimized model produces these spectra as a byproduct of good performance. A direct intervention study (e.g., a spectral regularizer) is needed to make a causal claim."}, "questions": {"value": "1. Seems the four metrics (entropy, participation ratio, eigenvalues, and JS divergence) are frequently used in related interpretability works. Can the author differentiate itself from other research that apply these similar metrics? Can the author provide a more detailed related works including what metrics are used for study what kind of phenomenon?\n2. As mentioned in weakness 1, have the authors validated your key findings—particularly the efficiency of Pre-LN and the anti-collapse function of RoPE—on any models larger than 1B parameters?\n3. As mentioned in weakness 2, have the authors considered an intervention study (e.g., adding a spectral regularizer) to demonstrate that enforcing a \"healthy\" spectral signature causes better model generalization, rather than just correlating with it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VW0zjVBMHh", "forum": "W5BPGXR9jf", "replyto": "W5BPGXR9jf", "signatures": ["ICLR.cc/2026/Conference/Submission16222/Reviewer_KsxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16222/Reviewer_KsxW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816668516, "cdate": 1761816668516, "tmdate": 1762926381293, "mdate": 1762926381293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to All the Reviewers"}, "comment": {"value": "Dear Reviewer **yZ9g, KsxW, jJuL,**\n\n\nWe are currently analyzing results from the new experiments, and a few runs are still in progress. We will update the draft with the new results, highlighting all the additions, and post it alongside the rebuttal no later than **Nov 22 (Saturday) AoE**. Thank you for your understanding.\n\nBest, \n\nAuthors."}}, "id": "6anFT8Wt0l", "forum": "W5BPGXR9jf", "replyto": "W5BPGXR9jf", "signatures": ["ICLR.cc/2026/Conference/Submission16222/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16222/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16222/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763698549333, "cdate": 1763698549333, "tmdate": 1763698566694, "mdate": 1763698566694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper introduces NerVE, a unified eigenspectral framework for analyzing feed-forward network (FFN) dynamics in LLMs.\n- It tracks eigenspectrum dynamics of representations using Spectral Entropy, Participation Ratio, Eigenvalue Early Enrichment, and Jensen-Shannon Divergence to quantify variance dispersion, dimensionality, and nonlinear redistribution.\n- The main finding is that FFN nonlinearities reinject and redistribute variance, activating underused dimensions and flattening eigenspectra to enhance latent space utilization, while demonstrating a correlation with validation loss.\n- Experiments were run with GPT-2 and Llama-style architectures trained from scratch, showing that NerVE provides an interpretable, data-efficient tool for understanding FFN dynamics beyond empirical tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a systematic, spectral lens to study FFN dynamics, an often-overlooked but important component of transformer models.\n- The four complementary eigenspectrum metrics are theoretically interpretable, and capture distinct aspects of the representations\n- The paper covers normalization variants, activation types, etc, making the analysis framework more broadly applicable."}, "weaknesses": {"value": "- If I understand correctly, the paper treats activations from different sequences as interchangeable. In that case, what aspects of the analysis are specific to LLMs or transformer architectures? From this perspective, it may be valuable to also examine other types of models with FFNs and compare their behavior to that of LLMs. Alternatively, extending the analysis to explicitly account for the sequential structure of tokens could yield further insights.\n- The models analyzed in the paper are relatively small by LLM standards (up to around 130M parameters). While it may not be feasible to train larger models from scratch, it could be informative to leverage open-weight models with available training checkpoints (e.g. Pythia) to study how the observed behaviors scale with model size.\n- It would also be helpful to include some analysis on downstream tasks, such as computing the proposed metrics on datasets or domains unseen during training, to test whether the observed patterns generalize.\n- Expanding the conclusion by summarizing the main findings and contributions would help consolidate the paper’s message."}, "questions": {"value": "- When computing correlations with validation loss, are the spectral metrics measured on training data or separate held-out sets?\n- Could a similar analysis be done on the other components of the transformer blocks? This may help link the inherent sequential structure of the tokens to your analysis\n- Could NerVE’s metrics generalize to other architectures (eg. non-transformers such as ResNets)? This could test whether the observations are a general FFN property.\n- Could you discuss some practical implications based on the proposed framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NiCoSyuxZa", "forum": "W5BPGXR9jf", "replyto": "W5BPGXR9jf", "signatures": ["ICLR.cc/2026/Conference/Submission16222/Reviewer_yZ9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16222/Reviewer_yZ9g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976907711, "cdate": 1761976907711, "tmdate": 1762926380941, "mdate": 1762926380941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}