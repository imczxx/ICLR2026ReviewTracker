{"id": "Y9b5UuGi9O", "number": 66, "cdate": 1756728376976, "mdate": 1759898276885, "content": {"title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "abstract": "Although Large Vision-Language Models (LVLMs) have demonstrated remarkable performance on downstream tasks, they frequently produce contents that deviate from visual information, leading to object hallucination. To tackle this, recent works mostly depend on expensive manual annotations and training cost, or decoding strategies which significantly increase inference time. In this work, we observe that LVLMs' attention to visual information is significantly enhanced when answering caption queries compared to non-caption queries. Inspired by this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a training-free, plug-and-play hallucination mitigation method that leverages the attention activation pattern corresponding to caption queries to enhance LVLMs' visual perception capability. Specifically, we use probing techniques to identify attention heads that are highly sensitive to caption queries and accurately estimate optimized intervention directions for their outputs. This intervention strengthens LVLM's fine-grained visual perception capabilities, thereby effectively mitigating object hallucination. CAI reduced object hallucination by an average of 6.03% across five widely used LVLMs and five benchmarks including both discriminative and generative tasks, demonstrating state-of-the-art (SOTA) performance while incurring little additional inference cost and preserving other foundational capabilities.", "tldr": "We propose Caption-sensitive Attention Intervention (CAI), a training-free method, that refines caption-sensitive attention heads outputs during inference to enhance the fine-grained visual perception capability and mitigate object hallucination.", "keywords": ["Larger Vision-Language Model", "Hallucination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1c8340e562f9d274c2e634e4f49374ce76b0d78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CAI (Caption-Sensitive Attention Intervention), a training-free, plug-and-play method to reduce object hallucination in Large Vision-Language Models (LVLMs). CAI identifies caption-sensitive attention heads—those with stronger visual activation under caption prompts—and applies inference-time attention shift interventions to enhance visual grounding. Experiments on five LVLMs and benchmarks show up to 6.03% hallucination reduction, surpassing prior training-free methods like VCD, OPERA, and VTI with little inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The discovery that caption prompts systematically enhance visual attention, and exploiting this through attention-head steering, is novel and insightful.\n- Experiments are robust, covering 5 models × 5 benchmarks, with detailed ablations, latency analysis, and query optimization strategies.\n- The method is described step-by-step and is easy to follow.\n- Offers an effective alternative to decoding-based approaches, with broad implications for multimodal interpretability and reliability research."}, "weaknesses": {"value": "The intervention directly adds attention-based perturbations to hidden states (Eq. 9). Without formal constraints, it remains unclear how these modified representations remain within the model’s valid latent manifold. Could you elaborate on how to guarantee that the modified attention remains within the LVLM’s representational manifold? Why does this intervention not introduce unintended artifacts?\n\nCould you clarify how the intervention is applied to the network (does it apply to all layers or only specific ones)? Have the authors analyzed inter-layer dependencies—for example, whether interventions at lower layers amplify or dampen representations at higher layers? From such analysis, is it possible to determine the layers where interventions would be most effective?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rNWkYMGNZb", "forum": "Y9b5UuGi9O", "replyto": "Y9b5UuGi9O", "signatures": ["ICLR.cc/2026/Conference/Submission66/Reviewer_bUqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission66/Reviewer_bUqS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission66/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556148268, "cdate": 1761556148268, "tmdate": 1762915445468, "mdate": 1762915445468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free approach to reduce object hallucinations in LVLMs, especially in non-caption tasks. The authors claim that during non-caption VQAs, there is a lack of visual attention. To address this, they precompute a shift score in the attention heads using a set of images and caption queries, which is then used at inference time to adjust the attention scores for a given input image-question pair. This idea is implemented on three LVLMs and evaluated on five hallucination benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well-written and easy to follow.\n* This paper addresses object hallucination, which remains a critical problem in LVLMs.\n* The idea of adjusting visual attention for queries where the models originally fail to provide strong visual focus seems interesting.\n* Addressing hallucination through intervention at inference is an efficient choice, compared to techniques that require heavy training."}, "weaknesses": {"value": "* In Fig. 1: \"Are there both a helmet and a motorcycle in the image?\" The reason LLaVA 1.5 answered \"Yes\" is because (most likely) the model heavily suffered from a \"yes\" bias. This issue has been highlighted in prior works, e.g., *Mitigating Object Hallucination in LVLMs via Data-augmented Phrase-level Alignment, ICLR 2025*; you can do a quick study, check the confusion matrix with and without CAI -- you will most likely see that the improvements mainly stem from YES to NO.\n\n* The experimental setup is very weak; the proposed method is applied on old models—LLaVA 1.5 (released in 10/2023), Qwen VL Chat (released in 08/2023), and LLaVA Next (released in 01/2024). Current models are much better than the 2023 models. If the proposed method can improve the performance of current LVLMs (e.g., QwenVL2.5/3, InternVL2.5/3, LLaVA-One-Vision1/1.5), it would be of interest to the community.\n\n* The baselines used in comparisons are old; both OPERA and VCD were released in 2023. You should compare against more recent decoding methods. You should also include training-based methods in the comparisons, such as HADPO and HALVA, among other offline RL approaches.\n\n* The benchmarks utilized in this work, such as POPE and CHAIR, are old and do not accurately evaluate the hallucination of LVLMs. POPE evaluates object existence based on 500 images and does not cover other forms of object hallucination, such as object attributes and relations. Similarly, CHAIR also evaluates only 500 images from MSCOCO using limited ground-truth information. Additionally, MMHal-Bench has only 96 images to test on. While these benchmarks have been commonly used in the past, they lack diversity and rigor, and there is a need for more critical evaluation. The authors can explore better benchmarks like HallusionBench, AMBER, M-HalDetect, and GAVIE.\n\n* l.270: Are you using a precomputed shift vector for each model, or are you computing this shift vector on the fly? If it is computed on the fly, I would be interested in understanding the relationship between the extra computation required at inference time and the improvement achieved.\n\n* LLaVA-Next is not included when evaluated on CHAIR, MMHal-Bench, and MHumanEval!\n\n* In l.324, the authors claimed to achieve SOTA performance without comprehensive comparisons.\n\n* The authors mentioned about using 5 LVLMs in the abstract but they mention about 3 in the paper!"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CM6x87OhxR", "forum": "Y9b5UuGi9O", "replyto": "Y9b5UuGi9O", "signatures": ["ICLR.cc/2026/Conference/Submission66/Reviewer_mtt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission66/Reviewer_mtt9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission66/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606739059, "cdate": 1761606739059, "tmdate": 1762915445240, "mdate": 1762915445240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for mitigating the visual hallucinations in LVLMs, whose main idea is to modify the attention heads responsive to visual tokens during inference time. These attention heads are selected from a subset of heads which are most activated when fed caption queries. \n\nMore specifically, the method consists of two steps:\n- [offline] Selection of attention heads to modify - the authors propose to identify these by training binary classifiers on last token’s modified attention scores when answering a caption query and a non-caption query for a VQA task. Among the $l \\times h$ classifiers for each layer and each head, the top $K$ are selected for modification.\n- [offline] Estimation of the correction (shift) vector - calculated as the difference between the original output attention scores of a caption and a non-caption query for a VQA task.\n\nAt inference time, the attention scores of the identified $K$ heads are modified with the calculated shift vector."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well written method section.\n- Improvements across models and datasets."}, "weaknesses": {"value": "My concern regarding the presented method is its practicality - the method works well for object recognition but might degenerate some other tasks than require textual understanding (like text translation in Figure 4). I suppose this is due to the design of the process for selecting the intervention heads, which masks the attention towards textual tokens when calculating the modified attention output. This implies that the binary classifiers are trained on synthetic representations and selecting best heads based on their accuracy might lead to poor results. For example, if a head achieves high accuracy on synthetic representations but is also critical for a textual task (like translation), CAI will hurt the performance of the LVLM."}, "questions": {"value": "- Judging from tables 10 and 11, it seems that CAI improves OCR but degenerates translation capabilities which rely on text. How does one need to modify your approach to fix this? Would the translation performance improve solely by tuning the hyperparameters or does one need to invoke the correct heads - eg, by training the classifiers on specific data?\n- I don’t understand how the optimal query was determined. Which metrics did you look at for the selection? It is also not clear which query setting was used in the experiments - the optimal query or the ensemble query?\n- What is the role of the data used to train the classifiers? Are these models robust to the choices and amount of the data samples?\n- I do not see which experiment shows that it is necessary to keep both $\\alpha$ and $K$ as hyperparameters. What kind of results would you get when only tuning $K$ for a constant $\\alpha = 1$?\n\n\nNits:\n- Figure 5 and 6 are lacking details. For FIgure 6 - What is K on the left, what is alpha on the right? On which dataset and model were these numbers obtained? What is Percentage on y-axis? Do you mean accuracy? For Figure 5 - from which experiment were these results taken fro?\n- In line 197, should it be $b$-th rather than  $b^′$-th?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eUQaecN3RK", "forum": "Y9b5UuGi9O", "replyto": "Y9b5UuGi9O", "signatures": ["ICLR.cc/2026/Conference/Submission66/Reviewer_bzdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission66/Reviewer_bzdp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission66/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901970061, "cdate": 1761901970061, "tmdate": 1762915445058, "mdate": 1762915445058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce CAI, a training-free method to mitigate a training-free method to mitigate object hallucination in Large Vision-Language Models. The authors first identify that caption queries uniquely amplify visual attention in particular attention heads compared to non-caption queries, endowing the model with fine-grained visual perception capability. Leveraging this phenomenon, CAI employs a three-step inference-time intervention: (1) probing these caption-sensitive heads via SVM classifiers, (2) computing perception-refined vectors as shift directions between caption and non-caption responses, and (3) steering attention outputs using intensity parameters.  Evaluations across five benchmarks show CAI reduces hallucination by 6.03% on average, achieving SOTA performance while preserving foundational capabilities and minimizing inference latency. This work contributes novel insights into attention optimization, a practical intervention framework, and strong generalizability across models and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is the first to explicitly reveal that caption queries uniquely enhance visual attention in specific LVLM attention heads. This discovery is substantiated by quantitative evidence and layer-wise analysis, demonstrating that the attention amplification correlates with reduced object hallucination and provides valuable insights into fine-grained visual perception mechanisms.\n- The proposed method demonstrates state-of-the-art performance across different benchmarks while maintaining strong generalizability across LVLM architectures. It achieves superior hallucination mitigation with significantly lower inference latency, ensuring practical efficiency without performance compromises."}, "weaknesses": {"value": "- The study does not adequately address how variations in non-caption queries affect the proposed method. The probing methodology relies on a limited set of non-caption queries, potentially leading to inaccurate perception-refined vectors when handling diverse real-world queries. This limitation may compromise the method's robustness in practical applications."}, "questions": {"value": "- This paper uses SVM for probing caption-sensitive attention heads. Have the authors considered or experimented with alternative classification methods ? \n- The analysis of hyperparameters is currently conducted only on the POPE benchmark. I believe additional validation across diverse benchmarks is necessary, as different task types may exhibit distinct parameter sensitivities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZvoiIAH2tA", "forum": "Y9b5UuGi9O", "replyto": "Y9b5UuGi9O", "signatures": ["ICLR.cc/2026/Conference/Submission66/Reviewer_M7q8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission66/Reviewer_M7q8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission66/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137101555, "cdate": 1762137101555, "tmdate": 1762915444902, "mdate": 1762915444902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}