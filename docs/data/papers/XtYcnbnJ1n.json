{"id": "XtYcnbnJ1n", "number": 7578, "cdate": 1758028415386, "mdate": 1759897844896, "content": {"title": "Continuous-time Analysis of Adam in Min-Max Games: Local Convergence and Implicit Gradient Regularization", "abstract": "Although Adam has been widely used in solving min-max games, its dynamical behavior in this setting has not been studied as extensively as in standard minimization. In min-max games, each player aims to minimize their own loss function, which may lead to the intuitive assumption that Adam’s properties in minimization naturally extend to min-max games with minor adjustments.  In this paper, we demonstrate that this belief is incorrect. Specifically, we investigate the local convergence and implicit gradient regularization aspects of Adam in min-max games, focusing on how its first- and second-order momentum parameters influence the dynamics. We show that, in both aspects, the momentum parameters affect Adam’s behavior in min-max games in precisely the opposite manner to their effect in minimization. Our methodology is based on the continuous-time analysis techniques commonly used for studying learning algorithms. We further provide numerical experiments to support our theoretical findings.", "tldr": "We proposed continuous-time models for Adam in min-max games and studied their dynamical behaviors.", "keywords": ["Continuous-time Analysis", "Min-Max Games", "Adam"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcad2236babc1abc24e08f8ecf17f5d2ba9a493e.pdf", "supplementary_material": "/attachment/f25b7027fde7243583d3cd2e60c1a3a57957687a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a continuous time counterpart of Adam for minmax optimization in the deterministic setting. First the paper shows that this continuous-time system approximates well Adam for minmax optimization under sufficient regularity on the objective function (Theorem 3.1).  Using this dynamical system viewpoint, the paper derives local convergence results for both continuous and discrete time systems by studying the Jacobian of the vector field at some local Nash equilibrium (Proposition 4.1) and discusses the influence of the parameters of the algorithm on the convergence behavior (Theorem 4.3-4.4) when local convergence can be shown in a suitable regime of step sizes. It is shown that the hyperparameters affect Adam in the opposite manner as for standard optimization. Simulations illustrate some of the claims of the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-organized. \n2. Adaptive gradient methods are relatively less explored in minmax optimization compared to their counterpart in minimization. Moreover it is already challenging to analyze Adam in mere minimization and still an active research topic given the popularity of the algorithm. The minmax setting is clearly more challenging due to both the complications due to the game setting (cycling behavior …) and the more involved Adam algorithm.  \n3. The continuous time approach is an interesting approach in general to get some insights about an algorithm. While this is not new, it is interesting to see that the Jacobian for Adam can be related to that of GDA under some assumptions (Proposition 4.1)."}, "weaknesses": {"value": "1. **Difference between optimization and min-max optimization** ‘Adam diverges even for bilinear objectives, regardless of parameter choices (Corollary 4.7). These findings highlight fundamental differences in Adam’s dynamics between min-max games and minimization.’ I think it should be clearly noted that even GDA diverges for min-max games when using the same step sizes. The paper highlights a lot the contrast between minimization and minmax for Adam. It is well known that the behavior of GD for minimization and GDA for min-max are different. Unlike what the paper emphasizes, it is hence not surprising in my opinion that this is also the case for Adam between optimization and minmax. One of the main differences is the use of momentum which might give some hope. Well, it has been shown that negative momentum can help (e.g. Gidel et al. 2019) which this paper seems to confirm. The second momentum (which appears additionally in Adam) is shown to be not so relevant in the deterministic setting for local convergence. \n\n2. The paper seems like an immediate follow-up to the paper of Feng et al. 2025 about continuous-time heavy ball momentum for minmax games, which adopts a similar approach and has similar local convergence results. The difference with Adam is the use of adaptive step sizes but some of the assumptions (bounded gradients for instance, see below) imply that these have somehow little effect. There are some new technical treatments, the main one being probably Lemma B.10. But I think that technical novelty is limited overall. Further discussion and more detailed comparison with that paper would be relevant. \n\n3. **Missing related work**: the discussion is missing a number of relevant works analyzing the local convergence of GDA for minmax and even some results for adaptive methods (similar to Adam) for minmax: \n\n- Several works in the literature have conducted local stability analysis for variants of GDA for minmax optimization with refined local convergence results and even characterization of strict local minmax equilibria: \n \nChi Jin, Praneeth Netrapalli, and Michael I Jordan. What is local optimality in nonconvex-\nnonconcave minimax optimization? ICML 2020. \n\nFiez, Tanner, et al. \"Global convergence to local minmax equilibrium in classes of nonconvex zero-sum games.”NeurIPS 2021.\n\nFiez, Tanner, and Lillian J. Ratliff. Local convergence analysis of gradient descent ascent with finite timescale separation. ICLR 2021.\n\nHsieh, Ya-Ping, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. ICML 2021.\n\n- Even adaptive algorithms for minmax optimization (similar to Adam) have been proposed and analyzed with non-asymptotic guarantees, these are based on adaptively learning a time scale separation (i.e. different step sizes for both variables) which has been shown to lead to convergence (see works above for results for GDA without adaptive steps): \n\nX. Li, J. Yang, and N. He. ‘Tiada: A time-scale adaptive algorithm for nonconvex minimax optimization.’ ICLR 2023. \n\nJ. Yang, X. Li, and N. He. Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. NeurIPS 2022. \n\n- The continuous time analysis of adaptive gradient methods (including Adam) for (stochastic) optimization has been initiated in a number of prior works in the literature, preceding even Ma et al. 2022 (which actually acknowledges that in their paper as they reuse the previously proposed dynamical systems): \n\n  Da Silva, A. B., & Gazeau, M. (2020). A general system of differential equations to model first-order adaptive algorithms. Journal of Machine Learning Research, 21(129), 1-42.\n\n  Barakat, A., & Bianchi, P. (2021). Convergence and dynamical behavior of the ADAM algorithm for nonconvex stochastic optimization. SIAM Journal on Optimization, 31(1), 244-274.  \n\nBarakat, A., Bianchi, P., Hachem, W., & Schechtman, S. (2021). Stochastic optimization with momentum: convergence, fluctuations, and traps avoidance. Electronic Journal of Statistics, 15(2), 3892-3947.  \n\nGadat, S., & Gavra, I. (2022). Asymptotic study of stochastic adaptive algorithms in non-convex landscape. Journal of Machine Learning Research, 23(228), 1-54.  \n\nNot also that the continuous time analysis has a much older tradition in the literature, including in the stochastic setting (see e.g.  Borkar 2008 ‘Stochastic Approximation: A Dynamical Systems Viewpoint’ and the references therein, see also e.g. Benaim 1996 ‘A dynamical system approach to stochastic approximations’, SIAM Journal on Control and Optimization, to name a few references among others). \n\n4. **Assumption 4.2:** While this assumption is used in some prior works cited by the paper, it does not seem to be used in the local convergence analysis of GDA (see all references above). One of the conclusions of the paper regarding the influence of the momentum parameter $\\beta$ is also intimately related to this assumption. Indeed, if the antisymmetric part (which is responsible for oscillations) is dominant, it is not so surprising that (positive) momentum will then hurt as its effect will amplify past information influence, and negative momentum might help.  \n\n5. **Using prior work for more refined results?** Since the Jacobian of Adam and of GDA are related (Proposition 4.1) and since the Jacobian of GDA has been thoroughly studied (e.g. in the works above), I expect that much more refined results can be established under weaker assumptions (even necessary and sufficient conditions of stability, using time-scale separation and characterization of strict local minmax equilibria) than the ones made in this work (e.g. relaxing Assumption 4.2. which seems quite restrictive).  \n\n6. **Local Nash:** It is known that even for some simple functions, local or global Nash equilibria may not exist in nonconvex-nonconcave games (see e.g. Jin et al. 2020, Proposition 6). As $(x^\\star, y^star)$ is defined as a local Nash and Theorems 4.3-4.4 show local convergence to that NE, it seems that the results implicitly assume existence of local NE. It would be worth giving precisions on what are the assumptions to guarantee their existence or give concrete examples of functions for which the statement is not vacuous.\n\n7. **Restriction to deterministic setting:** The paper is focused on the deterministic setting which is restrictive in practice given the common use of stochastic gradients. While I understand that the analysis of the deterministic can be a first step, I think that similar conclusions will hardly follow in the stochastic setting. For instance, corollary 4.5 argues that the second-order momentum hyperparameter might not have a role ‘in the presence or absence of local convergence’ for Adam, I think its role will be important in the stochastic setting to adapt the step size and to control noise, perhaps also for variance reduction. \n\n8. **Experiments:** Theoretical results (Theorems 4.3-4.4) are about local convergence to local Nash equilibria and are hence concerned with function values around local Nash equilibria, but the experiments are about average gradient norms. It is not clear how experiments validate or support the theoretical claims, especially in nonconvex-nonconcave settings such as in the experiments. \n\n9. **Theorem 3.1:** The assumption of bounded derivatives excludes even simple quadratic objectives. In addition, under this assumption, the $v_n$ sequences are bounded which means that the adaptive (effective) step sizes are upper and lower bounded, and the behavior is closer to a simple momentum algorithm. \n\n**Typos/Minor comments:**\n\n- Proposition 2.1: This is a classical textbook result in dynamical systems, I do not think it is appropriate to cite some 2021 paper for this. \n- Lemma B.3 is an elementary result in linear algebra, again I do not think a citation is required. \n- In the proofs, Lemma B.8  is a well-known and standard result in the literature (e.g. in control) but not presented as so, see discrete time Lyapunov equation for Schur stable matrices. I do not think a 2 page proof is needed here, or at least it should be clearly said that this is a well-known result and that a proof is provided for completeness.\n- l. 116-117: ‘a local Nash equilibrium. This concept is the standard solution concept in min-max games and plays the same role as a local minimum in minimization problems.’ I do not think it is that standard, see the reference above Jin et al. 2020 precisely about the discussion of which relevant solution concept should be the focus and the limitations of local Nash. \n\n- l. 214: ‘In section 4.1 describes’ \n- l. 260: ‘We are now present our’  \n- l. 122, l. 127: should be $m_n$s rather than $v_n$s\n- l. 137: spectrum and real part notation used before their definitions.\n- l. 348: affect?"}, "questions": {"value": "1. **Assumption 4.2:** What’s the exact mathematical definition of $\\mathcal{A} >> \\mathcal{S}$? Can you provide concrete examples where this assumption is satisfied? Is it satisfied for bilinear games? It seems that it is not needed for GDA in prior work about local convergence, why is it needed here? Does it simplify the analysis? Which part of the algorithm or its continuous time analysis pushes to consider this assumption? \n\n2.  **Critical points of Adam DA:** A first important step in analyzing the dynamical system ‘continuous Adam DA’ is to study its equilibrium (or critical) points. Are there other points than the points for which both gradients (w.r.t. x, y) are zero (i.e. equilibrium points of continuous GDA)? In that case it means that the results only guarantee local convergence to points which are equilibrium points of the GDA dynamics.   \n\n3. **Proof of Theorem 4.4:** Why don’t you end up having a similar Jacobian to analyze as for the continuous setting (Theorem 4.3) so that you do not have to deal with all the derivations in Lemma B. 10 in particular? While the discrete version induces some discretization errors (plus non-stationarity terms to control), the analysis should be similar for the main leading terms intuitively.\n\n4. **Local Nash vs vanishing gradients:** Some Hessian assumptions are needed and made in the appendix (Lemma B. 5) to recover local Nash from vanishing gradients. Are these stated in the main part in the statement of the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WC7scSezCv", "forum": "XtYcnbnJ1n", "replyto": "XtYcnbnJ1n", "signatures": ["ICLR.cc/2026/Conference/Submission7578/Reviewer_3qbU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7578/Reviewer_3qbU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765948737, "cdate": 1761765948737, "tmdate": 1762919672582, "mdate": 1762919672582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the suitability of Adam to find a saddle point of the from $min_x max_y f(x,y)$ instead of a minimum $min_{x,y} f(x,y)$. This is by no means a sure thing. With a suitable initialization, a gradient ascent/descent procedure can do this when initialized close enough to a regular enough saddle point.  Another popular approach consists in using wildly different step sizes for the minimization and maximization components, at considerable computational cost.  In order to discuss the performance of Adam in that context, the authors define a continuous-time version of Adam (Chizat-style), argue that the continuous-time and discrete-time algorithms are not that far away from each other, and finally show that a substantial momentum does not help the convergence speed of the continuous-time version. They also argue that (contrary to what has been discussed in the case of minimization), momentum does not implicitly regularizes towards low-curvature regions of the cost function (not surprising as saddle points in games often have dramatic shapes). Empirical studies on GANs substantiate the findings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- a thorough analysis with satisfying theory\n- a good reminder that min-max problems are much trickier than optimization problems"}, "weaknesses": {"value": "- not too surprising to those who know that min-max problems are tricker than optimization."}, "questions": {"value": "- Proposition 2.1. -- Please confirm the regularity assumptions required for this theorem. Min-max problems in games often lead to cost function with rather irregular saddle-points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kjJqz2BfUx", "forum": "XtYcnbnJ1n", "replyto": "XtYcnbnJ1n", "signatures": ["ICLR.cc/2026/Conference/Submission7578/Reviewer_5mRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7578/Reviewer_5mRK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846470067, "cdate": 1761846470067, "tmdate": 1762919672210, "mdate": 1762919672210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of Adam in min-max games, revealing that its behavior is precisely the opposite of its behavior in standard minimization. Using continuous-time analysis, the authors derive a high-fidelity model for Adam Descent-Ascent (Adam-DA). The analysis shows that in min-max games, a smaller first-order momentum allows for convergence over a wider range of step-sizes, the reverse of what is optimal for minimization. Furthermore, Adam-DA provably diverges on bilinear games, unlike in minimization. The study finds that a smaller first-order momentum and a larger second-order momentum guide the optimization trajectory towards flatter regions of the loss landscape, which again is the opposite of the effect in minimization. Extensive experiments on GAN training validate these theoretical findings, showing that the identified parameter settings lead to lower gradient norms and improved performance. This work crucially corrects the misplaced intuition of transferring Adam's properties directly from minimization to adversarial settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows that Adam's momentum parameters have precisely the opposite effect in min-max games compared to minimization. It directly challenges and corrects a common assumption in the field, providing a crucial new theoretical lens for understanding and applying Adam in adversarial settings like GANs.\n\n2. The paper provides a more accurate $O(h^3)$ approximation of Adam's discrete dynamics than prior results.  This theoretical result is complemented by extensive experiments on multiple GAN architectures and datasets, which show a clear correlation between flatter minima and improved performance.\n\n3. Despite the complex theory, the paper presents its findings with simple and actionable guidelines (e.g., \"use smaller $\\beta$ and larger $rho$\").  This bridges a critical gap between theory and practics."}, "weaknesses": {"value": "1. The experimental validation is confined primarily to GANs. The paper's claims about \"min-max games\" are broad, but it does not demonstrate its findings on other critical applications like adversarial training, leaving the generalizability across the full problem domain uncertain.\n\n2. The empirical comparisons are largely against the standard Adam baseline or its own ablated versions. It would be better to include more results of state-of-the-art algorithms specifically designed for min-max games, to show whether the discovered parameter settings make Adam superior to these specialized methods.\n\n3. The theoretical analysis is presented in a deterministic setting.  However, Adam is almost exclusively used in stochastic optimization.  The paper does not address how stochastic noise, a fundamental aspect of Adam's typical use case, interacts with its conclusions on local convergence and implicit regularization, which is a significant gap."}, "questions": {"value": "1. The theoretical analysis is in the deterministic setting.  How do you expect the presence of stochastic gradient noise, which is intrinsic to the practical use of Adam, to interact with or alter your core findings regarding the opposite roles of $\\beta$ and $\\rho$ in local convergence and implicit regularization?\n\n2. To enable a more comprehensive evaluation and firmly establish the robustness of the proposed method, it is essential to provide additional experimental results that include more state-of-the-art baselines and other \"min-max games\" expect for GAN."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kxlvaGDDZt", "forum": "XtYcnbnJ1n", "replyto": "XtYcnbnJ1n", "signatures": ["ICLR.cc/2026/Conference/Submission7578/Reviewer_z4QD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7578/Reviewer_z4QD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923064152, "cdate": 1761923064152, "tmdate": 1762919671698, "mdate": 1762919671698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of Adam in min-max games compared to its role in standard minimization problems. The authors find that these two roles differ, challenging previous assumptions. Specifically, momentum parameters influence Adam’s behavior in min-max games in the opposite way to their effect in minimization. Numerical experiments are provided to support these claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The study raises an interesting question about the differing roles of Adam in min-max games and standard minimization problems.\n2. The empirical evidence helps illustrate the observed behavioral differences."}, "weaknesses": {"value": "1. The claim that Adam behaves differently in min-max games compared to standard minimization seems somewhat artificial and potentially misleading, as the authors compare Adam-DA with Adam rather than directly analyzing the original Adam across both settings.\n2. The mathematical techniques employed are fairly standard, offering little in terms of novel methodological contribution.\n3. The analysis focuses on local convergence and implicit gradient regularization, but the rationale for emphasizing these two perspectives is not clearly justified."}, "questions": {"value": "Can the conclusions be generalized to other optimization algorithms, such as SGD or AdamW?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GhP9BRILaP", "forum": "XtYcnbnJ1n", "replyto": "XtYcnbnJ1n", "signatures": ["ICLR.cc/2026/Conference/Submission7578/Reviewer_NpU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7578/Reviewer_NpU6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924866896, "cdate": 1761924866896, "tmdate": 1762919670905, "mdate": 1762919670905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}