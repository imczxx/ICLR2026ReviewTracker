{"id": "YwwGqR3BTa", "number": 13333, "cdate": 1758216662877, "mdate": 1759897444375, "content": {"title": "GraphFLEx: Structure Learning $\\underline{\\text{F}}$ramework for $\\underline{\\text{L}}$arge $\\underline{\\text{Ex}}$panding $\\underline{\\text{Graph}}$s", "abstract": "Graph structure learning is a core problem in graph-based machine learning, essential for uncovering latent relationships and ensuring model interpretability. However, most existing approaches are ill-suited for large-scale and dynamically evolving graphs, as they often require complete re-learning of the structure upon the arrival of new nodes and incur substantial computational and memory costs. In this work, we propose GraphFLEx—a unified and scalable framework for Graph Structure Learning in Large and Expanding Graphs. GraphFLEx mitigates the scalability bottlenecks by restricting edge formation to structurally relevant subsets of nodes identified through a combination of clustering and coarsening techniques. This dramatically reduces the search space and enables efficient, incremental graph updates. The framework supports 48 flexible configurations by integrating diverse choices of learning paradigms, coarsening strategies, and clustering methods, making it adaptable to a wide range of graph settings and learning objectives. Extensive experiments across 26 diverse datasets and graph neural network architectures demonstrate that GraphFLEx achieves state-of-the-art performance with significantly improved scalability.", "tldr": "GraphFLEx is a unsupervised graph structure learning framework designed to handle large and expanding graphs effectively. It confines the number of relevant nodes for potential connections by leveraging clustering and coarsening techniques.", "keywords": ["Graph Neural Networks", "Unsupervised Graph Structure Learning", "Graph Coarsening", "Graph Clustering"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b77bf026f7eeba30febd460ba9b463b9fcec4b90.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GraphFLEx, a framework for Graph Structure Learning (GSL) on large expanding graphs. It integrates three core modules—clustering, coarsening, and learning—to restrict edge formation to structure-relevant node subsets, enabling incremental updates and avoiding full retraining. Supporting 48 configurations, GraphFLEx is validated on 26 datasets (including the 2.4M-node Ogbn-products) for computational efficiency, scalability, and performance on downstream tasks (node classification, link prediction). Theoretical support includes a neighborhood preservation theorem and complexity analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the key pain point of \"full retraining\" in large expanding graph GSL by unifying static and dynamic graphs into an expanding graph sequence, with an incremental update design aligned with industrial needs.\n2. Modular architecture balances flexibility and interpretability, supporting combinations of classic methods and enabling component-wise role identification via module decomposition.\n3. Rigorous theoretical and experimental backing: the neighborhood preservation theorem ensures structural accuracy, complexity analysis clarifies scalability boundaries, and experiments cover diverse datasets to verify multi-dimensional performance."}, "weaknesses": {"value": "1. The main limitation of this work is that the three components in the framework—clustering, coarsening, and structure learning—are not organically integrated, and the authors fail to clearly explain the necessity of their approach. Furthermore, each module is based on a substantial amount of existing work, making the approach somewhat disjointed and lacking a clear core innovation. The engineering contributions alone may not be sufficient to meet the standards required for a research-level publication.\n2. The comparison with baseline methods is limited, as it primarily includes older models, without considering newer techniques like Graph Neural Networks (GNNs), which are highly relevant in modern graph learning tasks.\n3. While GraphFLEx supports incremental learning for expanding graphs, it lacks detailed discussion on how updates to existing nodes might affect the overall graph structure, especially as graph size increases.\n4. The effectiveness of graph coarsening and clustering may be limited when dealing with heterogeneous graphs, where node connections are more complex and harder to represent accurately."}, "questions": {"value": "1. How does GraphFLEx handle heterogeneous graphs, where node connections are more complex and varied? Does the performance of graph coarsening and clustering degrade in such scenarios, and if so, how can this be addressed?\n2. What is the impact of hyperparameter settings (such as for graph clustering, coarsening, and structure learning) on the model’s overall performance? How sensitive is the framework to these parameters, and is there an optimal strategy for tuning them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QZUfnFNJHe", "forum": "YwwGqR3BTa", "replyto": "YwwGqR3BTa", "signatures": ["ICLR.cc/2026/Conference/Submission13333/Reviewer_byRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13333/Reviewer_byRg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760875212237, "cdate": 1760875212237, "tmdate": 1762923992094, "mdate": 1762923992094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel graph structure learning framework named GRAPHFLEX. It was designed to address structural learning challenges on large-scale and scalable graph data. GRAPHFLEX begins by partitioning large-scale graph data into communities, compressing the size of individual communities, and constructing them into a coarse-grained hypergraph. It then learns the graph structure between hypergraphs through graph structure learning methods. Experimental results demonstrate the effectiveness of GRAPHFLEX."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The authors conducted evaluations on 26 datasets, thus significantly enhancing the persuasiveness of the experimental findings.\n\nS2. The study addresses a clearly defined and innovative research problem, i.e., large-scale scalable graph structure learning.\n\nS3. The paper is logically structured, with coherent exposition of the research problem and detailed descriptions of GRAPHFLEX's modular components."}, "weaknesses": {"value": "W1. The technical descriptions lack clarity. In Section 3.4, the authors model connections between hypernodes and input nodes. This approach appears questionable due to the granularity mismatch: hypernodes represent communities while input nodes correspond to individual original nodes.\n\nW2. The experimental scale doesn't adequately reflect real-world application scenarios. Figure 4(a-c) illustrates structural evolution with 30 new nodes added across three timesteps. However, in real-world large-scale graphs (e.g., online social networks), single-timestep expansions typically involve substantially more than 10 nodes. It is unclear if the authors have validated model efficacy when handling larger-scale node increments.\n\nW3. There is a lack of technical innovation. The three core modules comprising GRAPHFLEX appear to predominantly rely on existing technical components, such as established graph clustering algorithms and conventional hypergraph generation methods. This constitutes a critical limitation in terms of methodological innovation."}, "questions": {"value": "Q1. What precisely is meant by \"incoming nodes\" mentioned in Section 3.3? Does the graph clustering performed in Section 3.2 apply solely to the initial graph, or does it encompass the union of both the initial and expanded graphs?\n\nQ2. What substantive modifications or architectural innovations were introduced to adapt the baseline algorithms for large-scale scalable graph learning scenarios?\n\nQ3. According to the results reported in Table 4, GRAPHFLEX achieves optimal performance in both computational efficiency and accuracy. While this is an encouraging experimental outcome, there typically exists a trade-off between computational cost and precision in conventional settings. Could the authors explain the technical rationale behind GRAPHFLEX's simultaneous optimization of these two typically competing objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dq4lQWSUm7", "forum": "YwwGqR3BTa", "replyto": "YwwGqR3BTa", "signatures": ["ICLR.cc/2026/Conference/Submission13333/Reviewer_qEQZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13333/Reviewer_qEQZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913152243, "cdate": 1761913152243, "tmdate": 1762923991515, "mdate": 1762923991515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this manuscript, the authors aim to address the challenge of graph structure learning on large or expanding graphs. They propose GraphFLEx, which consists of three key modules: clustering, coarsening, and structure learning. Based on comprehensive experiments, GraphFLEx outperforms multiple representative baselines on real and synthetic datasets, while achieving up to 3x speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### **Strengths**\n 1. This paper focus on an interesting but challenging topic, conduct graph structure learning on a large and expanding graph. Since the most existing GSL methods utilize the node pair similarity to update structure, their complexity is extremely high. So it is a challenging and important task in this research field.\n\n2. In this manuscript, the authors evaluate GraphFLEx on more than 10 graph datasets, acrossing different domains and sizes, which effectively prove the soundness of the proposed method.\n\n3. The experimental part is comprehensive, including multiple downstream tasks and settings.\n\n4. This manuscript provides sufficient theoretical analysis of the proposed method."}, "weaknesses": {"value": "### **Weaknesses**\n\n1. Although the authors compare their proposed method with multiple baselines, there is a lack of latest state-of-the-art GSL methods in the experimental part [1].\n\n2. This work only uses homophilic graph datasets in the experiments, while ignoring the heterophilic graph datasets.\n\n3. The related work of latest GSL works is not sufficient.\n\n[1] Li, Zhixun, et al. \"GSLB: the graph structure learning benchmark.\" Advances in Neural Information Processing Systems 36 (2023): 30306-30318."}, "questions": {"value": "### **Questions**\n\n1. Could the authors provide more comparison experiments with representative GSL baselines [1]?\n\n2. In this paper, the authors only focus on the effectiveness and efficiency of GraphFLEx, how about the robustness of the proposed method?\n\n[1] Li, Zhixun, et al. \"GSLB: the graph structure learning benchmark.\" Advances in Neural Information Processing Systems 36 (2023): 30306-30318."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R4Ny0g0hvY", "forum": "YwwGqR3BTa", "replyto": "YwwGqR3BTa", "signatures": ["ICLR.cc/2026/Conference/Submission13333/Reviewer_nmQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13333/Reviewer_nmQu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915904547, "cdate": 1761915904547, "tmdate": 1762923991181, "mdate": 1762923991181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a unified and scalable framework for graph structure learning (GSL) designed for large-scale and expanding graphs. It addresses the limitation of existing GSL methods which require full recomputation when new nodes arrive. It features modular integration of graph clustering, graph coarsening, and structure learning, to enable edge discovery within localized and structurally relevant subsets of nodes, instead of across the entire graph. The approach reduces the search space, computational cost, and memory overhead, allowing the framework to scale to graphs with millions of nodes. It supports 48 configurable pipelines by combining different clustering, coarsening, and learning models, making it adaptable to diverse settings and both missing-graph and partially observed graph scenarios. The authors provide theoretical guarantees on neighbourhood preservation, edge recovery, and computational complexity. Experiments are performed on 26 datasets which demonstrate that their approach significantly improves scalability, and can achieve near-linear time growth while maintaining or surpassing the structure quality of classic GSL methods (shown by improved node classification and link prediction performance)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* studies the important problem of how to efficiently learn and update graph structures in large-scale and dynamically expanding graphs.\n* authors provide theoretical guarantees on neighbourhood preservation and computational complexity.\n* the framework supports 48 possible configurations and accommodates diverse learning paradigms.\n* can scale to graphs with over 2.4M nodes while maintaining structure quality."}, "weaknesses": {"value": "* the design relies heavily on accurate initial clustering and coarsening, so subsequent edge recovery may propagate errors if these early stages mis-assign nodes or oversimplify structure.\n* evaluations focus on static baselines and node classification/link prediction tasks, but do not rigorously compare against dynamic or continual GNN baselines"}, "questions": {"value": "* given the framework begins with clustering on G₀, how sensitive is performance to the quality of the initial cluster assignments?\n* why were dynamic or continual GNN baselines not included, as they do address incremental graph updates even if they don't explicitly learn structure.\n* graph coarsening reduces computation but may lose fine-grained structural details - have you performed  analysis of coarsening-induced edge errors or noise propagation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tOiSgu5xX9", "forum": "YwwGqR3BTa", "replyto": "YwwGqR3BTa", "signatures": ["ICLR.cc/2026/Conference/Submission13333/Reviewer_3ACf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13333/Reviewer_3ACf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166965080, "cdate": 1762166965080, "tmdate": 1762923990828, "mdate": 1762923990828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}