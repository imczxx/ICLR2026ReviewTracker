{"id": "ex2CrZ6x1r", "number": 10196, "cdate": 1758163579941, "mdate": 1759897667712, "content": {"title": "AIPO : Adaptive Intent-driven Preference Optimization", "abstract": "    Human preferences are diverse and dynamic, shaped by regional, cultural, and social factors. Existing alignment methods like Direct Preference Optimization (DPO) and its variants often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts.\n    To address these limitations, we introduce Adaptive Intent-driven Preference Optimization (A-IPO). Specifically, A-IPO introduces an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function, encouraging stronger alignment between the preferred model's responses and the user's underlying intentions. We demonstrate, both theoretically and empirically, that incorporating an intention--response similarity term increases the preference margin (by a positive shift of $\\lambda\\,\\Delta\\mathrm{sim}$ in the log-odds), resulting in clearer separation between preferred and dispreferred responses compared to DPO. \n    For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess real-world and adversarial preference alignment. \n    Through explicit modeling of diverse user intents, A-IPO facilitates pluralistic preference optimization while simultaneously enhancing adversarial robustness in preference alignment. Comprehensive empirical evaluation demonstrates that A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics: up to +24.8 win-rate and +45.6 Response-Intention Consistency (RIC) on Real-pref; up to +38.6 Response Similarity (RS) and +52.2 Defense Success Rate (DSR) on Attack-pref; and up to +54.6 Intention Consistency Score (ICS) on GlobalOpinionQA-Ext.", "tldr": "", "keywords": ["Preference Optimization", "latent intent", "Intention alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56fa46ec8545b474831c34961f1c5c1704dd6d76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **A-IPO (Adaptive Intent-Driven Preference Optimization)**, an extension of DPO that conditions the reward on a latent intent variable $I$ inferred from the prompt. The method uses:  \n1. prompt decomposition via LLM calls,  \n2. retrieval from Wikipedia through Pinecone,  \n3. fact-checking via Anah-v2, and  \n4. an intent classifier $q_\\phi(I|x)$ guiding an augmented DPO loss with a similarity term $\\lambda\\mathrm{sim}(y,I)$.  \n\nTheoretical results (Lemma 5.2, Theorem 5.3) show that this intent-conditioned term increases preference margins and decreases pairwise NLL. Experiments on GPT-2 Large and Pythia-2.8B show consistent gains on author-curated datasets (REAL-PREF, ATTACK-PREF, GlobalOpinionQA-Ext).  \n\nWhile the direction is interesting, the pipeline adds substantial computational overhead (multiple LLM calls, retrieval/fact-checking, and an auxiliary classifier) without cost reporting. It also doesn’t compare to simpler inference-time intent handling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses the limitation of DPO’s single-preference assumption and aims for pluralistic alignment.  \n- Provides a clean modular pipeline (intent inference + DPO with a similarity term).  \n- Offers formal though limited proofs of margin and NLL improvement.  \n- Shows consistent metric gains (Table 1, 2) and reasonable ablations.  \n- Includes detailed appendices with templates and dataset stats."}, "weaknesses": {"value": "1. **Mathematical correctness and internal consistency**\n   - **Similarity term outside the logit.** In the BT/PL setup, probabilities should be $\\sigma(\\text{logit})$. Eqs. (38), (39) for example, add $\\lambda[\\mathrm{sim}(y_w,I)-\\mathrm{sim}(y_l,I)]$ **after** the sigmoid/logit step, which can push the expression outside $[0,1]$ and isn’t a valid probability transformation. In Appendix D and Lemma 5.2, the same term is treated as a **logit shift** (i.e., inside the log-odds). This mismatch makes the central training objective mathematically incoherent, and I am not sure if the probability model is valid.\n   - **Theorem 5.1 statement.** The LHS is written as $r(x,y)$ (no $I$) while the RHS depends on $I$; the proof immediately “fixes $(x,I)$,” suggesting the intended statement is $r(x,y,I)$ (or “for each fixed $I$”). This should be fixed to avoid confusion about what is being identified.\n   - **Notation drift.** $D_{KL}$ vs.\\ $KL$; $q_\\phi(I|x)$ in some places vs.$q_\\tau(I|x)$ elsewhere; $\\mathcal D$ called a “dataset” but used as a sampling distribution $x\\sim \\mathcal D$. These are small individually, but in aggregate they are a bad look for the paper.\n\n2. **Undefined core component drives the main effect**\n   - The paper never specifies how $\\mathrm{sim}(y,I)$ is computed: What text encoders are used for $y$ and for $I$? Is $I$ represented by a discrete embedding or a continuous vector? What is the **range/scale** of $\\mathrm{sim}$ (bounded in $[0,1]$ or unbounded cosine, etc.)? Is there normalization or a temperature? Do gradients back-prop through the response encoder? Because $\\lambda$ scales $\\mathrm{sim}$ directly, the **numerical range** of $\\mathrm{sim}$ critically determines the margin effect and stability. Without these details, the method is not reproducible and the ablations are hard to interpret.\n\n3. **Ambiguity about intent supervision**\n   - The main text defines an element-wise BCE over $K$ intentions (multi-label), but the experimental setup later describes a single-label softmax classifier for the intention module. These correspond to **different problem formulations** (multi-label vs single-label). The paper should commit to one, explain why, and reflect that choice consistently in the loss and implementation.\n\n4. **Dataset–objective coupling that favors the proposed method**\n   - The REAL-PREF curation template instructs the generator to (i) craft a prompt that **explicitly embeds an intention**, (ii) produce an “accept” response that **honors** that intention, and (iii) a “reject” response that **contradicts** it. ATTACK-PREF similarly constructs synthetic corruptions. Then A-IPO optimizes a loss that **directly rewards intent–response similarity**. This structural alignment between how negatives are generated and how the model is trained very likely advantages A-IPO over baselines that do not explicitly encode an intent similarity term. This doesn’t invalidate the gains, but limits what we can conclude about generalization to less templated, naturally occurring preferences.\n\n5. **Baseline tuning and scope**\n   - All baselines reportedly share the same hyperparameters. Prior work shows DPO/GDPO often require **method-specific** $\\beta$ and optimizer schedules; a one-size setting can understate their performance. In addition, experiments are limited to GPT-2 Large and Pythia-2.8B; showing results on at least one larger open model would improve external validity.\n\n6. **Compute and systems cost not quantified; missing inference-time alternative**\n   - The method adds multiple moving parts around DPO: several LLM calls for prompt decomposition and intent extraction, retrieval (Pinecone) and sentence-level fact-checking (Anah-v2), plus an intent classifier and the augmented training loop. Even if some steps are precomputed offline, this **increases engineering complexity and retraining cost** whenever domains shift. The paper acknowledges “computational overhead” but reports no wall-clock, GPU-hour, or memory numbers.\n   - For the cultural/knowledge intents considered, a strong **inference-time baseline** (detect intent at serve time, retrieve evidence, and re-rank or constrain decoding using a similarity score) could deliver similar gains **without retraining**. The absence of this comparison leaves the practical cost–benefit unanswered.\n\n8. **Clarity gaps that impede reproduction**\n   - The prior $p(I)$ is not specified (uniform? learned?), and there is no check for posterior collapse or sensitivity to the prior.\n   - There are minor grammar/formatting issues (e.g., subject–verb agreement; missing spaces around symbols), which cumulatively reduce polish and increase cognitive load for the reader."}, "questions": {"value": "Every item raised in the *Weaknesses* section can be viewed as a question for the authors.  \nI may well be mistaken on several of these points, and I would sincerely appreciate clarification or correction wherever appropriate.  \nIf the authors can address or resolve even part of these concerns—whether by showing that I misunderstood something or by providing additional detail—it would be very helpful.\n\nThat said, my top three concerns I’d especially like to hear more about are:\n\n1. **The probabilistic modeling issue:** whether the similarity term’s placement outside the logit in Eqs. (8)/(39)/(40) is indeed an error or whether there’s a principled justification that I missed.  \n2. **The dataset generation bias:** how the authors think about potential coupling between their curated templates and the method’s design, and whether they have evidence of generalization beyond that distribution.  \n3. **The scale limitation:** whether they have tried or plan to test A-IPO on larger models (at least 7B parameters) to verify scalability and the claimed improvements on pluralistic alignment.\n\nI offer these comments in a constructive spirit and look forward to the authors’ perspective on these points and on any others mentioned in the *Weaknesses* section."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "* Cultural/identity framing. The REAL‑PREF curation explicitly encodes cultural/religious intents (e.g., names, taboos) and generates “accept/reject” labels accordingly (Appendix I.5). Even with guardrails (“Do not assert religion based solely on name…”, I.4), risks of stereotyping or misattribution remain, especially if deployed widely. \n\n* LLM‑generated, LLM‑judged data. Reliance on GPT‑4 both to generate and judge may import opaque biases; the paper does not report annotator demographics/compensation or human validation rates and agreements. Clearer data statements and human‑in‑the‑loop evaluations are recommended."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xICJJQqqcs", "forum": "ex2CrZ6x1r", "replyto": "ex2CrZ6x1r", "signatures": ["ICLR.cc/2026/Conference/Submission10196/Reviewer_pf2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10196/Reviewer_pf2h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761156041525, "cdate": 1761156041525, "tmdate": 1762921561692, "mdate": 1762921561692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Adaptive Intent-driven Preference Optimization (A-IPO), a framework for aligning large language models that tackles   several shortcomings of Direct Preference Optimization (DPO). The authors contend   that DPO’s use of a single, averaged preference signal tends to amplify majority opinions, overlook subtle user intentions, and weaken the model’s resilience to adversarial inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s greatest strength lies in its clarity and focus. It pinpoints a specific, well-recognized problem—DPO’s “tyranny of the majority”—and presents a solution that is both intuitive and well-motivated. The introduction clearly articulate the motivation and high-level idea, setting an excellent standard for exposition.\n\nThe empirical analysis is thorough and convincing. The authors introduce targeted benchmarks—REAL-PREF and ATTACK-PREF—to properly evaluate the model’s claims. Results demonstrate clear improvements over strong baselines, while the ablation study highlights the necessity of each component within A-IPO."}, "weaknesses": {"value": "The framework’s effectiveness largely depends on the intention module, which is trained in a supervised fashion using a cross-entropy loss on ground-truth binary intent labels. This design implies that A-IPO requires access to a dataset annotated with high-quality, discrete intent labels. While the paper criticizes GDPO for its reliance on belief or group partitioning, it remains unclear how A-IPO’s dependence on explicit intent supervision is more scalable or less costly."}, "questions": {"value": "Maybe I have missed this, but how is the sim(y, I) function (Eq 7) implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "T9wi7qVVJq", "forum": "ex2CrZ6x1r", "replyto": "ex2CrZ6x1r", "signatures": ["ICLR.cc/2026/Conference/Submission10196/Reviewer_kpR4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10196/Reviewer_kpR4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782739108, "cdate": 1761782739108, "tmdate": 1762921561133, "mdate": 1762921561133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes A-IPO, a framework that incorporates user-specific latents in reward modeling via an intention module, and demonstrate both empirically and theoretically that this increases separation between preferred and dispreferred responses for diverse users. This work introduces two new datasets, REAL-PREF and ATTACK-PREF, and extends the existing GlobalOpinionQA dataset, to benchmark real-world and adversarial preference alignment. A-IPO shows strong empirical results on these datasets when compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is very well written and easy to follow, with clear motivations addressing an important and relevant problem, i.e. adapting to the latent preferences of diverse individuals (pluralism).\n* This work proposes a learned intention module which decomposes a prompt into sub-questions and uses RAG with an external data source (Wikipedia) for supporting information. This is an interesting and novel contribution, and the authors show its importance empirically in Tab 2.\n* This work contributes two new datasets,  REAL-PREF and ATTACK-PREF, which are designed to tackle important goals, i.e. culturally distinct preferences and adversarial robustness\n* The authors show promising empirical results with A-IPO when compared to popular baselines like DPO and GDPO on their newly proposed datasets"}, "weaknesses": {"value": "* I am primarily concerned with the scaling behavior of A-IPO beyond the 3B regime (Pythia, Tab 1). While running these experiments may be impractical, I would like at least a discussion as to why the authors believe that A-IPO will scale well to larger LLMs and MoE models for real-world use cases\n\nMinor Weaknesses:\n* The proposed intention module learning a user-specific latent from a prompt is quite similar to the PAL-A model [2], which learns a weighted mixture of prototypes over a popular of users and weights over these prototypes for individuals. A-IPOs intention module should be compared and contrasted with this methodology.\n* As A-IPO is focused on learning latents specific to individual users, I am curious how A-IPO holds up against pluralistic baselines like P-RLHF [1] and PAL [2] on the popular Reddit TL;DR Summary dataset containing heterogeneous preferences. Comparing to these baselines would greatly add to the quality of this work.\n\n[1] Li et al., \"Personalized language modeling from personalized human feedback\", *arXiv 2024*.\n\n[2] Chen et al., \"PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment\", *ICLR 2025*.\n\nI am leaning towards an 8 (accept), but I would like these weaknesses addressed (and the question below) in the revision before I can increase my score."}, "questions": {"value": "* What happens if user intention cannot be fact checked from Wikipedia and just something specific to that user? How does the intention module function in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "En49gFw56C", "forum": "ex2CrZ6x1r", "replyto": "ex2CrZ6x1r", "signatures": ["ICLR.cc/2026/Conference/Submission10196/Reviewer_VQt8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10196/Reviewer_VQt8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974327743, "cdate": 1761974327743, "tmdate": 1762921560530, "mdate": 1762921560530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive Intent-driven Preference Optimization (A-IPO) to better capture diverse human preferences. By modeling latent user intent and incorporating it into the reward function, A-IPO improves alignment between responses and underlying intentions. Evaluations on three benchmarks show substantial gains in preference accuracy, intention consistency, and adversarial robustness over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce A-IPO, a novel framework extending DPO by incorporating an explicit intention module that infers latent user intent from each prompt. This module guides preference optimization to better capture diverse and context-sensitive user preferences, showing significant improvement when in comparison with existing method.\n2. They also curate two new benchmark datasets: REAL-PREF and ATTACK-PREF, along with an extended version of GlobalOpinionQA-Ext, to evaluate cultural diversity and adversarial robustness in LLM preference alignment."}, "weaknesses": {"value": "1. The experimental section lacks essential details. Although the paper introduces its own benchmark datasets, it does not describe the data collection process or provide dataset analysis, which are crucial for understanding why A-IPO performs well on these benchmarks. Additionally, the proposed evaluation metrics are insufficiently explained, with little justification for their design choices.\n2. Section 5, Theoretical Analyses of A-IPO, lacks clear motivation before each derivation, making it difficult to follow the reasoning behind the theoretical verifications."}, "questions": {"value": "1. Can you move Figure 1 on the second page for better understanding of the introduction?\n2. Why do you create new benchmark REAL-PREF and ATTACK-PREF in your paper? What's the difference between the dataset you created with existing benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WrXQv2rxEL", "forum": "ex2CrZ6x1r", "replyto": "ex2CrZ6x1r", "signatures": ["ICLR.cc/2026/Conference/Submission10196/Reviewer_jFRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10196/Reviewer_jFRJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984456789, "cdate": 1761984456789, "tmdate": 1762921559857, "mdate": 1762921559857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}