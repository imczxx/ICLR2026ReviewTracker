{"id": "xDLE5n3x9Y", "number": 20660, "cdate": 1758308714201, "mdate": 1763661584281, "content": {"title": "Overparametrization bends the landscape: BBP transitions at initialization in simple Neural Networks", "abstract": "High-dimensional non-convex loss landscapes play a central role in the theory of Machine Learning. Gaining insight into how these landscapes interact with gradient-based optimization methods, even in relatively simple models, can shed light on this enigmatic feature of neural networks. In this work, we will focus on a prototypical simple learning problem, which generalizes the Phase Retrieval inference problem by allowing the exploration of overparametrized settings. Using techniques from field theory, we analyze the spectrum of the Hessian at initialization and identify a Baik–Ben Arous–Péché (BBP) transition in the amount of data that separates regimes where the initialization is informative or uninformative about a planted signal of a teacher-student setup. Crucially, we demonstrate how overparameterization can \"bend\" the loss landscape, shifting the transition point, even reaching the information-theoretic weak-recovery threshold in the large overparameterization limit, while also altering its qualitative nature.\n We distinguish between continuous and discontinuous BBP transitions and support our analytical predictions with simulations, examining how they compare to the finite-N behavior. In the case of discontinuous BBP transitions strong finite-N corrections allow the retrieval of information at a signal-to-noise ratio (SNR) smaller than the predicted BBP transition. In these cases we provide estimates for a new lower SNR threshold that marks the point at which initialization becomes entirely uninformative.", "tldr": "We quantitatively analyze how overparametrization reshapes the high-dimensional loss landscape of a teacher–student setup in random positions, showing it can anticipate and qualitatively alter transitions between successful and failed signal recovery", "keywords": ["Overparametrization", "Loss landscapes", "Signal recovery", "High-dimensional learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1f401f4d2890492a0c5557332368862606c122c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors consider a teacher student model for finite-width, one-hidden-layer neural networks with quadratic activations (generalizing phase retrieval). \nThey study the Hessian of the loss landscape of the student at initialization, and characterize when a certain overlap between teacher weights and leading hessian eigenvector transitions from zero to finite (either in a continuous or discontinuous way). A positive overlap means that the Hessian at initialization contains easily accessible information on the teacher weights.\nThey compute the sample critical threshold, i.e. the minimum amount of samples M over dimension N such that there is information about the teacher weights in the initialization Hessian, and study the phenomenology as a function of the overparametrisation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors quantify the effects of overparametrisation in a non-convex problem, in particular the effect at initialization. Given the context on spectral initialization and GD dynamics, I find it a nice way to discuss such effects.\n\nThe authors provide an application of the recently discussed discontinuous BBP transition, showing that such behavior is not a totally abstract curiosity."}, "weaknesses": {"value": "The mismatch between finite N simulations of alpha_BBP in the discontinuous case and the theory should be characterized more precisely, even though it is clear that finite size corrections will be hard to \"eliminate\". \n- Is there a different set of observables (other than that in Figure 3) that could be probed numerically to highlight the transition? Maybe with less important finite size effects? \n- How prohibitive would it be to access finite size corrections from the field theory formalism?"}, "questions": {"value": "It is not apparent to me why one would need p,p* = O(1). What would fail in the derivation for p=O(d), for e.g.?\n\nline 116: I would like to highlight also the following relevant paper https://arxiv.org/abs/2505.17958 where over-parametrized phase retrieval is considered also in an empirical minimization setting (finding the same weak recovery threshold as Maillard et al. 2024) in the complementary setting p^* = O(d), p=O(d) with p^*/d -> 0.\n\nline 143: it is not clear to me why one would normalize the loss by 1/labels. Given that the authors remark that this is an important element of the subsequent analysis, it would be nice to have more intuition here.\n\nline 157: is it \"when\" or \"where\"? If the first, when one would expect such Gaussianity to hold?\n\nline 215: it would be nice to have an explicit definition of m here.\n\nline 254-258: is there an intuition for why exponential decay at the bulk boundary induces a first order transition in the overlap? \n\nline 333 and 351: is it the largest eigenvalue instead of smallest?\n\nline 370 - 400: it is not clear to me how is alpha_0 computed: is it an analytical quantity? A numerical one? Also, it is not clear why it should be a lower bound to the finite-N transition. I suggest clarifying a bit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NvsnQkhwMG", "forum": "xDLE5n3x9Y", "replyto": "xDLE5n3x9Y", "signatures": ["ICLR.cc/2026/Conference/Submission20660/Reviewer_W5zG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20660/Reviewer_W5zG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749937084, "cdate": 1761749937084, "tmdate": 1762934048956, "mdate": 1762934048956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how overparameterization affects the loss landscape at initialization in gradient-based learning. In particular, it considers a teacher-student setup where both teacher and student are two-layer neural networks with quadratic activations, generalizing previous works on phase retrieval to a multi-index setting. By analyzing the spectrum of the loss Hessian, the authors identify the conditions under which its leading eigenvector becomes informative about the teacher signal, and show that overparameterization bends the landscape, shifting the associated transition toward smaller sample sizes (lower SNR). In the infinite-width limit, the transition reaches the information-theoretic weak-recovery threshold. Finally, the authors investigate finite-size effects and compare them with the high-dimensional predictions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is theoretically and numerically sound. It addresses the important question of how overparameterization affects the learning landscape, offering novel, quantitative results in the specific setting of quadratic two-layer networks. Its originality lies in extending previous analyses of phase retrieval to a more general framework, providing a detailed characterization of the BBP phenomenology and valuable insights on finite-size effects. Overall, the presentation is clear and connects the findings to known information-theoretic thresholds and spectral methods."}, "weaknesses": {"value": "1. One main weakness is the lack of a methodological overview in the main text. The technical analysis is confined to the appendix, leaving readers without intuition about the derivation. The paper could strongly benefit from a short but insightful methodological summary in the main section, possibly by shortening the (sometimes redundant) conclusion and/or using the additional page.\n\n2. Some relevant references on the multi-index setting are missing. For instance, the critical SNR $p_\\star/2$ has also been derived as the computationally optimal threshold in [1]; spectral method for multi-index models, included the quadratic network studied here, have been investigated and rigorously characterized in [2, 3].\n\n3. The choice of the loss function may appear somewhat *ad hoc* and problem specific. It can benefit from further motivation and discussion.\n\n[1] Troiani et al., \"Fundamental limits of weak learnability in high-dimensional multi-index models\"\n\n[2] Kovačević et al., \"Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery\"\n\n[3] Defilippis et al., \"Optimal Spectral Transitions in High-Dimensional Multi-Index Models\""}, "questions": {"value": "1. Could you offer some intuition on how the label noise might qualitately affect the BBP phenomenology observed in this work?  \n\n2. In the conclusion, you mention that understanding the interplay between the emergence of a signal in the Hessian and the behavior of gradient-descent dynamics is an open direction. Do you have any preliminary numerical evidence or intuition on whether the BBP transition identified here corresponds to the point where gradient descent (or flow) begins to correlate with the teacher signal?\nIn particular, do you expect a qualitative behavior similar to the loss of correlation with the informative eigenvector observed in [Bonnaire et al., 2024], or would the overparameterized setting change this picture? Even a qualitative comment on whether such a connection is expected or not would be very interesting, although understandably beyond the main scope of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T88aHOL6Gf", "forum": "xDLE5n3x9Y", "replyto": "xDLE5n3x9Y", "signatures": ["ICLR.cc/2026/Conference/Submission20660/Reviewer_x8Zk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20660/Reviewer_x8Zk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831605785, "cdate": 1761831605785, "tmdate": 1762934048473, "mdate": 1762934048473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how overparameterization changes the geometry of simple neural networks at initialization.\nThe authors look at a two-layer quadratic network in a teacher–student setup, analyze the spectrum of the Hessian at random initialization, and show that it undergoes a Baik–Ben Arous–Péché (BBP) transition as the sample size (or SNR) increases.\nThe main finding is that wider networks “bend” the loss landscape: the BBP threshold shifts to smaller SNRs, the transition can become discontinuous, and in the large-width limit the threshold actually reaches the information-theoretic weak-recovery limit.\nThe work ties together ideas from random matrix theory, spectral initialization, and overparameterized learning, with clear analytical results and convincing numerical checks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a strong theoretical contribution. The idea that overparameterization changes the nature of the BBP transition—and that in the large-width limit one reaches the optimal weak-recovery threshold—is both interesting and novel.It deepens our understanding of why wide models are easier to train, and it connects two previously separate lines of work: loss-landscape curvature and spectral initialization.\n\nOriginal and timely topic: the interplay between overparameterization and loss-landscape geometry.\nTechnically clean derivations, connecting to known spectral and phase-retrieval results.\nSolid numerical support and a nice discussion of finite-size effects.\nClear writing and good figures that make a rather technical story accessible.\nConceptually important: shows how widening a network effectively reshapes the curvature of the loss, anticipating information about the teacher signal even before training starts."}, "weaknesses": {"value": "The analysis is limited to quadratic activations, which makes it less clear how general the conclusions are.\n\nThe field-theory derivations could be compressed; parts of the appendix are a bit heavy \"physics-style\"\n\nIt would have been nice to see a direct quantitative comparison with actual spectral initialization methods to highlight practical implications."}, "questions": {"value": "Beyond quadratic activations:\nThe current analysis focuses on quadratic activations, which make the problem analytically tractable. It would be useful to discuss whether similar BBP behavior should appear for other smooth nonlinear activations (e.g., ReLU, erf). How much of the observed “bending of the landscape” is a consequence of the quadratic structure, and how much would persist in more realistic nonlinear settings where the Hessian couples to the input distribution?\n\nInterpretation of discontinuous BBP transitions:\nThe discontinuous BBP transitions are a striking result. Can the authors clarify their physical or algorithmic interpretation? Do they correspond to a first-order–like instability in the optimization landscape, or to a sharp onset of alignment during early training dynamics?\n\nConnection to optimization dynamics:\nSince the paper analyzes the Hessian at random initialization, one may wonder why this spectral transition should meaningfully predict the onset of learnability for gradient descent in practice, especially at finite width. Are the BBP signatures expected to survive after a few optimization steps, or are they quickly “washed out” as the model moves in parameter space?\n\nRobustness beyond Gaussian inputs:\nHow sensitive are the predicted thresholds to the Gaussian data assumption? Would structured or correlated inputs (e.g., non-isotropic covariance, nonzero mean) qualitatively alter the BBP critical point or the continuous/discontinuous nature of the transition?\n\nRelation to implicit regularization and flatness:\nCould the authors connect their findings to the broader literature on implicit regularization in overparameterized models—such as the bias of gradient flow toward flat minima? Does the observed “bending” of the Hessian spectrum have an analogue in the flatness or margin properties of trained solutions, or suggest a theoretical link between curvature at initialization and generalization in the final model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aNXEmIGPOG", "forum": "xDLE5n3x9Y", "replyto": "xDLE5n3x9Y", "signatures": ["ICLR.cc/2026/Conference/Submission20660/Reviewer_upYX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20660/Reviewer_upYX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993260343, "cdate": 1761993260343, "tmdate": 1762934047869, "mdate": 1762934047869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses a phase transitional behavior of the Hessian at initialization in the overparametrized setting. They discuss the setup where the teacher and student networks are two-layer neural networks with quadratic activation, and the width of the student is possibly larger than the width of the teacher. In classical BBP transition analysis, it is known that there is a threshold where the SNR is larger than this threshold the largest eigenvector aligns with the true signal. They analyze the threshold in the overparametrized student-teacher setup, and show that 1) transition happens either continuously or discontinuously 2) overparametrization decreases the threshold and makes training easier 3) as the width of the student -> infinity, the threshold becomes optimal. The claims are supported with experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. An interesting perspective on neural network training by discussing overparametrized phase retrieval - the theory involves machinery from quantum mechanics (of which I did not fully understand), which shows an interesting link between learning theory and physics.\n\n2. The related works are cited extensively and mentioned appropriately in relevant parts of the manuscript."}, "weaknesses": {"value": "1. Clarification in terminology is needed. \n - Why would this be a \"loss landscape\" result? Seems to me that the result is mostly on Hessian \"at initialization\" - which to me, it is not natural to understand the result as loss landscape result (of course, Hessians and loss landscape are related, but the training dynamics is not discussed).\n - What does \"bend the landscape\" mean? \n - What is the SNR that is repeated throughout the paper? I assume it would be alpha = M/N, I am wondering if the terminology SNR is used to express such quantity.\n - Using the term overparametrized could be a little bit misleading, because in general the term is used to state that the number of parameters >= number of data points. \n\n2. It is a little hard to understand the technical novelty of the paper. What is the technique that is needed to study the overparametrized setting, which is different from previous approaches? Are there any challenges? \n\n3. The experiments are good in the sense that the discoveries are verified, but the experiments are quite small-scale. $p, p^{*}$ are in the scale of 1,2,3,4. Larger experiments may be helpful. Also, I can see that the threshold becomes smaller when overparametrization occurs, but does that imply that it needs to better training? I don't see direct evidence of it. For instance, it would be helpful if there is an experiment where you apply gradient descent on the actual learning problem and show that overparametrization yields faster convergence/better generalization etc.\n\n4. It would be good if we could see justifications of certain theoretical problem settings. e.g. Why should the activation be quadratic? Why should we train with normalized quadratic loss function?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5nGQ5ZJ12", "forum": "xDLE5n3x9Y", "replyto": "xDLE5n3x9Y", "signatures": ["ICLR.cc/2026/Conference/Submission20660/Reviewer_Z5EM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20660/Reviewer_Z5EM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762737988184, "cdate": 1762737988184, "tmdate": 1762934047298, "mdate": 1762934047298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Reply"}, "comment": {"value": "First of all we deeply thank all the referees for the many insightful suggestions and stimulating questions. \nWe addressed the specific points in the individual answers and we devote this general reply to topics raised by more than one reviewer.\n\n## Quadratic Activations\n\nThe reason we focus on the quadratic activation function is that it can be connected to the Phase Retrieval problem, which is an interesting well studied learning task for which a huge wealth of results exists. In (Bonnaire et al., 2025), the authors observe a BBP-at-initialisation phenomenon precisely for this activation function. Furthermore, particular features of this problem make the study of the Hessian at initialisation particularly interesting.  For example, we expect this informative eigenvector to be particularly relevant for non-convex rough landscapes, where the optimization task with gradient descent is known to be hard. By choosing the quadratic activation we are ensuring that we are thus in such a relevant setting.\nOur derivation can be repeated for a generic non-linear activation function $\\sigma(x)$. The slight complication is that the matrix $\\boldsymbol{F}^\\mu = a\\,\\sigma'(\\boldsymbol{\\lambda})\\sigma'(\\boldsymbol{\\lambda})^T + b\\,\\text{Diag}\\big(\\sigma''(\\boldsymbol{\\lambda})\\big)$ is not rotationally invariant anymore, and does not allow explicit analytic expressions for its eigenvalues for $p$ larger than 3. Both problems can be overcome by writing the fixed point equation for the full matrix $\\boldsymbol{G}^b$, and evaluating averages such as $\\mathbb{E}\\boldsymbol{F}\\left(\\boldsymbol{I}_p-\\boldsymbol{G}^b \\boldsymbol{F}\\right)^{-1}$ using Monte Carlo methods.\nWe would like to stress that although the theoretical analysis is more complicated, the same theoretical picture is expected to hold for different nonlinearities. We have now added a section in the appendix where we show that a similar BBP-like phenomenon holds for Hessian defined with the sigmoid and tanh activation functions. In both cases an outlier that is correlated with the teacher emerges to the left of the spectrum of the Hessian at initialization, thus showing that this phenomenon is not restricted to the quadratic activation function. Preliminary numerical simulations seem also to show the advantage of overparametrization from this point of view. \n\n## Choice of Loss Function\n\nAll analytical computations performed in this work can be applied to any loss function that provides a finite left edge in the Hessian eigenspectrum. The specific loss function used here is a generalization of the classical mean squared error (MSE) which incorporates a denominator $(y + a)$. The need of the denominator is due to the unboundedness of $y$. Numerically, this term acts as a regularizer to prevent instabilities arising from rare events with very small or large teacher outputs $y$.   Analytically, its purpose is to condition the Hessian eigenspectrum, guaranteeing a finite left edge in the $N \\to \\infty$ limit. A bounded activation function should not require any additional regularizer term in the loss to grant the existence of a finite left edge in the Hessian spectrum.  \nConsequently, under these conditions, while the precise transition points may vary with the choice of loss function, the qualitative mechanisms and interpretations proposed in this work are not unique to this specific form and are expected to generalize to other regression losses.\nIf one instead chooses a combination of activation function and loss function that does not show a finite left edge, there is currently no known theory that can obtain these kinds of results for $N \\to \\infty$, even if numerical simulations suggest that the qualitative finite size behavior remains similar.\n\n## Connection to Dynamics\n\nWe thank the reviewers for encouraging us to explore the connection between static phase transitions and learning dynamics. In response, we have added a new Appendix F presenting preliminary results on how the BBP transition at initialization correlates with gradient descent dynamics.  Our simulations across different values of $p$, $N$, and $a$ (with $p^* = 1$) reveal that the BBP transition marks the onset of weak recovery in the dynamics, with overparameterization generally anticipating this transition. \nFrom the results of (Bonnaire et al, 2025) we expect gradient descent dynamics at finite $N$ to be controlled not only from the BBP transition in the Hessian at initialization, but also from the BBP transition of the Hessian at threshold states at higher SNR, which should remain the only discriminant in the large $N$ limit. \nThese preliminary results indeed show that the curves move to the right as $N$ increases, aligning with the intuition that as $N$ increases, the dynamical transition will shift toward a later BBP transition of threshold states.\nA complete characterization of this latter transition remains an important direction for future work."}}, "id": "Nd34vMFRgg", "forum": "xDLE5n3x9Y", "replyto": "xDLE5n3x9Y", "signatures": ["ICLR.cc/2026/Conference/Submission20660/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20660/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20660/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763660480471, "cdate": 1763660480471, "tmdate": 1763660480471, "mdate": 1763660480471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}