{"id": "6Pv1dvbliS", "number": 5453, "cdate": 1757911436409, "mdate": 1759897973925, "content": {"title": "Support Your Local LMs: Redistributing LM Traffic from Cloud to Edge with TrafficBench", "abstract": "The vast majority of large language model (LLM) queries today are processed by frontier models in centralized cloud infrastructure. However, recent advances have produced small language models (≤20B parameters) that match or exceed larger models on many tasks while offering superior energy and cost efficiency. To better understand what fraction of inference workloads can be shifted away from cloud to local compute, we present TrafficBench, a comprehensive benchmark for evaluating query routing between local and cloud-deployed LLMs. TrafficBench is comprised of 1M real-world queries derived from ChatGPT user conversations and naturalistic reasoning queries, with evaluations across 10 state-of-the-art (SOTA) models, 4 hardware accelerators, and 8 performance metrics. Using TrafficBench, we address three critical questions: (1) what fraction of current inference queries can be handled by small LMs on local accelerators, (2) how effectively can modern routing architectures identify these queries, and (3) what are the downstream efficiency implications of local routing? Our analysis reveals that 80.7% of TrafficBench queries can be successfully handled by small local models, with coverage varying by domain—exceeding 90% for creative tasks but dropping just below 68% for technical fields. We start by evaluating existing SOTA embedding- and decoder-based routing approaches, finding that they do not push the Pareto frontier beyond individual local models. To enable better routing, we introduce a novel binary variation of decoder-based routing that achieves superior performance (F1 = 0.851) when we have access to large training datasets (>100K); we also show that embedding models excel in data-constrained settings (<10K). When deployed over real-world traffic distributions, our decoder-based router reduces energy by 77.1%, compute by 67.1%, and cost by 60.2% versus cloud-only deployment, while maintaining comparable task accuracy. Our longitudinal analysis from 2023-2025 shows a 9.5× improvement in intelligence efficiency (accuracy per watt), with the fraction of locally-serviceable queries increasing from 23.2% to 80.7%, suggesting significant efficiency gains from better routing systems. We release TrafficBench along with a hardware-agnostic profiling harness for measuring model efficiency metrics (e.g., energy utilization), enabling reproducible benchmarking and supporting new research as models and accelerators emerge.", "tldr": "TrafficBench benchmark (1M queries) shows 80.7% of LLM workloads can run on small local models (<20B), with routers achieving 77.1% energy, 67.1% compute, and 60.2% cost savings vs cloud-only deployment.", "keywords": ["model routing", "local-cloud compute", "inference workloads", "inference-time compute"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c9114c7250ee3ab974754392eea2dabe823214f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TrafficBench, a comprehensive benchmark for evaluating query routing between local and cloud-deployed Large Language Models (LLMs). TrafficBench distinguishes itself by utilizing 1 million real-world queries from diverse domains (WildChat and NaturalReasoning), evaluating across 10 state-of-the-art models and 4 hardware accelerators, and reporting 8 key performance metrics beyond just accuracy, including latency, throughput, energy consumption, and cost. The study reveals that 80.7% of queries can be handled by lightweight local models. The authors propose a novel binary decoder-based routing approach that involves predicting whether each model can solve a given question and then selecting the smallest capable model. This is compared with existing embedding-based routing and multi-class decoder-based routing approaches. The evaluation demonstrates a routing performance of F1 = 0.851, which is 0.156 higher than the multi-class decoder. On a real-world query distribution, the proposed binary decoder routing approach shows a 77% reduction in energy, 67.1% in compute, and 60.2% in cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ This paper reveals an important insight that 80.7% of real-world queries can be handled by local models, supported by comprehensive benchmark results.\n+ The benchmark is comprehensive because it uses real user queries, includes local and cloud models, supports multi-model scenarios, and reports various efficiency metrics.\n+ The design of changing the multi-class classifier to a binary one is a simple but effective contribution."}, "weaknesses": {"value": "- The evaluation could be improved by including a breakdown study of the latency and energy cost of the binary decoder itself.\n- The fact that this method uses a Qwen3-8B backbone to predict whether a question should be routed to Qwen3-4B might not be economical for queries with medium-to-long prompts.\n- The figures and tables require improvement. For example, Figure 1 mentions three data sources, which is inconsistent with the paper stating that it uses two data sources. Table 2 (Left) contains no new information, as the text already conveys it. Figure 3 contains two sets of unconventionally placed legends.\n- This paper contains many individual experiments. If they can be organized more logically, the paper would be easier to follow."}, "questions": {"value": "1. OpenAI’s GPT-5 contains three models—an efficient model, a powerful reasoning model, and a real-time router. The router decides based on conversation type, complexity, tool needs, user’s request to “think harder,” and the detection of sensitive topics like signs of acute distress. How does the router in TrafficBench differ from GPT-5’s approach?\n2. The local models used in the evaluation mainly consist of the Qwen 3 family and an additional GPT-OSS-20b. However, according to LMArena, the Gemma 3 (and 3n) family is also a strong competitor in this size range. Despite its difference in chain-of-thought capabilities, it would be beneficial to also include Gemma 3 in the evaluation.\n3. Again, using data from LMArena, GPT-4o-mini is less capable than most Qwen 3 models. How can the choice of GPT-4o-mini as the judge to verify accuracy be justified?\n\nA minor issue: Line 1048 leaks LaTeX source code into the PDF."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ahC2gohxA", "forum": "6Pv1dvbliS", "replyto": "6Pv1dvbliS", "signatures": ["ICLR.cc/2026/Conference/Submission5453/Reviewer_uH3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5453/Reviewer_uH3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962133994, "cdate": 1761962133994, "tmdate": 1762918070898, "mdate": 1762918070898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRAFFICBENCH, a large-scale benchmark (1M real-world LLM queries) designed to evaluate query routing between local (≤20B) and cloud (≥100B) language models. It covers 10 models, 4 hardware accelerators, and 8 efficiency metrics, aiming to quantify how much inference traffic can be handled by local LMs. The authors also propose a binary decoder-based routing method that outperforms prior embedding-based and multi-class decoder routers, achieving an F1 of 0.851 with large datasets. Empirical results suggest that 80.7% of queries can be locally handled, reducing energy (−77.1%), compute (−67.1%), and cost (−60.2%), with negligible accuracy loss. TRAFFICBENCH and a profiling harness are released to support reproducible efficiency benchmarking."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel and important problem:\nThis paper addresses an emerging and highly relevant question — how to achieve efficient local–cloud collaboration as on-device LMs become increasingly capable. The introduction articulates three insightful research questions within this context and provides thoughtful, evidence-based answers. The problem setting is timely and impactful for both research and deployment communities.\n\nFirst comprehensive local–cloud routing benchmark:\nThe authors introduce TRAFFICBENCH, the first benchmark that simultaneously offers real-world LLM traffic, hardware-level energy measurements, multi-model evaluation, and a reproducible profiling harness. Covering 10 models, 4 accelerators, and 8 efficiency metrics, TRAFFICBENCH provides a strong foundation for future work on distributed LLM inference.\n\nStrong binary decoder routing algorithm:\nThe proposed binary decoder routing method achieves state-of-the-art performance, significantly outperforming embedding-based and multi-class decoder routers (+0.13–0.17 F1). By decomposing routing into independent binary generation tasks, the approach improves scalability, robustness, and generalization across models and datasets.\n\nWell-structured efficiency evaluation framework:\nThe authors design a complete analytical framework to quantify the trade-offs between energy, compute, and cost. The results are compelling: 80.7% of queries can be served locally, reducing energy by 77.1%, compute by 67.1%, and cost by 60.2%, with minimal quality loss. This provides valuable insights into the feasibility and efficiency of distributed inference.\n\nExcellent clarity and readability:\nThe paper is clearly written and logically structured. The flow from problem motivation to methodology and results is easy to follow, figures are well-presented, and the experimental section is thorough. Overall, it is a well-crafted and accessible paper."}, "weaknesses": {"value": "Limited exploration of adaptive routing strategies:\nThe paper focuses only on embedding-based and decoder-based routers, but recent studies have explored reinforcement learning (RL), graph-based, and adaptive routing approaches for dynamic model selection. For example, PickLLM, GraphRouter, and RadialRouter introduce RL or structured policies that adapt routing decisions based on context or resource constraints. Discussing or comparing to such adaptive approaches would strengthen the paper’s positioning and highlight the boundaries of the proposed method.\n\nRestricted hardware coverage:\nThe evaluation mainly relies on high-end GPUs and Apple Silicon hardware, lacking measurements on mobile or low-power edge devices (e.g., Qualcomm Hexagon). Given the paper’s emphasis on offloading cloud traffic to local devices, this limitation weakens the argument for true “on-device feasibility.” Extending the evaluation to mobile hardware would make the results more general and practically relevant."}, "questions": {"value": "Analysis of routing failures:\nIt would be very helpful if the authors could analyze misrouted examples or common failure patterns. Some qualitative analysis could clarify the limitations of the current routing mechanism and inspire future improvements.\n\nDialog data setup:\nDoes TRAFFICBENCH primarily consist of single-turn queries, or does it also include multi-turn dialog data? If the latter, how is conversational context represented in the routing input? Clarifying this will help readers understand the generality of the benchmark.\n\nRouting in multi-turn dialog settings:\nIn multi-turn conversations, query difficulty and model suitability often depend on dialogue history (context length, user intent, prior model success). Static one-shot routing may be insufficient. Have the authors considered incorporating context-aware features or dynamic decision policies (e.g., RL-based adaptive routing) to address model switching and accumulated error in conversational scenarios? However, multi-shot routing has higher costs like kv cache recomputing or transferring. How to solve the problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07ciomTEkQ", "forum": "6Pv1dvbliS", "replyto": "6Pv1dvbliS", "signatures": ["ICLR.cc/2026/Conference/Submission5453/Reviewer_eq6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5453/Reviewer_eq6V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989016024, "cdate": 1761989016024, "tmdate": 1762918070311, "mdate": 1762918070311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRAFFICBENCH, a large-scale benchmark designed to evaluate how effectively inference queries to large language models (LLMs) can be routed between local and cloud deployments. The benchmark includes 1 million real-world queries, evaluated across 10 modern LLMs, 4 hardware accelerators, and 8 performance metrics.\nThe authors use TRAFFICBENCH to study:\n\t1.\tWhat proportion of LLM queries can be handled by smaller, locally deployed models;\n\t2.\tHow well query routing architectures can identify these cases;\n\t3.\tThe efficiency benefits (in energy, cost, and compute) of such routing.\n\nThe authors also propose a new binary decoder-based routing approach that achieves high accuracy with large training data, and they show that embedding-based routing works better when data is limited.\nDeploying their system on real traffic significantly reduces energy use, compute, and cost compared to cloud-only inference. \n\nHowever, the contribution of this work lies primarily on the engineering side rather than the research side. The proposed benchmark is largely derived from existing datasets (WILDCHAT and NATURALREASONING), and much of the effort appears focused on (1) cleaning and filtering queries, and (2) ensuring compatibility with modern hardware and models. The proposed binary classification router is not conceptually novel, and the paper lacks sufficient theoretical motivation, justification, or ablation studies to support this design choice.\n\nOverall, while the work demonstrates solid engineering and reproducibility efforts, it does not yet present enough methodological or theoretical innovation to be ready for publication as a research paper in its current form."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Strength:\n1. The writing is clear and easy to follow;\n2. The proposed benchmark is thoroughly constructed and well-designed;\n3. The proposed method is also very simple and easy to understand."}, "weaknesses": {"value": "Weaknesses:\n1. The contribution is limited. This work looks like a technical report instead of a research paper;\n2. The proposed benchmark mainly sources from existing works;\n3. The proposed method is not technically novel and lacks insights, motivations or ablation studies."}, "questions": {"value": "Questions are the following:\n\n1. In line 214-25, you propose to use LLM, GEMINI-2.5-pro specifically, as a judge for WILDCHAT. To me, it is not technically sound to use a single SOTA model. LLM nowadays still suffer from hallucination and instability. How can we ensure the accuracy and authenticity of the reference answers from a large model?\n2. In line 260-261, the notation M_routed is not defined;\n3. In line 375, when scaling to more routing targets, the performance drop. But in line 333, it is reported that increasing the local model options demonstrate substantial gains. It is a bit self-contradictory.\n4.  It is easy to convert a multi-class setting into binary-class settings, for example through one-vs-rest or one-vs-one. what motivates you to choose the proposed methodology?\n5. A sensitive analysis of the proposed method in relation to confidence threshold \\tau is missing;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fek9cdALXU", "forum": "6Pv1dvbliS", "replyto": "6Pv1dvbliS", "signatures": ["ICLR.cc/2026/Conference/Submission5453/Reviewer_t7Xt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5453/Reviewer_t7Xt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762703724632, "cdate": 1762703724632, "tmdate": 1762918069904, "mdate": 1762918069904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "## Overview\n\nThe paper addresses the growing efficiency gap in large language model (LLM) inference, where most queries are currently handled by centralized cloud-based models. Recent progress in **small language models (≤20B parameters)** has shown that they can match or even exceed the performance of larger models on many tasks while offering **better cost and energy efficiency**.  \n\nTo explore how much of today’s LLM workload could be shifted to local compute, the authors introduce **TRAFFICBENCH**, a large-scale benchmark designed to evaluate **query routing between local and cloud-deployed LLMs**.\n\n## Key Features of TRAFFICBENCH\n\n- **1 million real-world queries** derived from ChatGPT conversations and reasoning tasks.  \n- **10 state-of-the-art models**, **4 hardware accelerators**, and **8 performance metrics** evaluated.  \n- A focus on three central questions:\n  1. What fraction of inference queries can be handled by small local models?  \n  2. How effectively can routing architectures identify these queries?  \n  3. What are the efficiency gains from local routing?\n\n## Main Findings\n\n- **80.7%** of TRAFFICBENCH queries can be served by small local models.  \n  - Over **90% coverage** for creative tasks.  \n  - Around **68% coverage** for technical domains.  \n\n- Existing **embedding- and decoder-based routing** methods fail to extend the Pareto frontier beyond standalone local models.  \n\n- A proposed **binary variation of decoder-based routing** achieves **F1 = 0.851** when trained on large datasets (>100K samples).  \n  - Embedding-based models perform better under data-limited conditions (<10K samples).\n\n## Efficiency and Impact\n\nWhen applied to real-world traffic distributions, the decoder-based router achieves:\n- **77.1% reduction in energy use**  \n- **67.1% reduction in compute**  \n- **60.2% reduction in cost**  \nwhile maintaining **comparable task accuracy** to cloud-only deployment.\n\nLongitudinal analysis (2023–2025) shows:\n- **9.5× improvement** in *intelligence efficiency* (accuracy per watt).  \n- Growth in locally serviceable queries from **23.2% → 80.7%**.\n\n## Contributions\n\nTRAFFICBENCH provides:\n- A **reproducible benchmark** for evaluating routing systems.  \n- A **hardware-agnostic profiling harness** to measure energy and performance metrics.  \n- A foundation for future research on efficient LLM deployment as new models and accelerators emerge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is written clearly and the research questions are well motivated\n- The experiments, ablations and suite of models considered is thorough and exhaustive\n- The paper studies and presents very interesting aspects of traffic redistribution eg: number and quality of models used in the pool, the type of embedding encoder used, the type of queries processed and processing of out of distribution queries.\n- The observations in the paper would be of general interest to the ICLR community."}, "weaknesses": {"value": "- In general I think it would be interesting to diversify the pool of language models considered based on different architectural aspects such as type of attention used (GQA, MQA, MHA, MLA), attention-free models such as Mamba etc, to study how architectural diversity in the pool improves coverage. \n- I wouldn't consider H200 GPUs to be edge devices as these are not generally accessible especially in academic settings. It would be interesting to study and benchmark on AI accelerators such as NPUs [1]\n- Quantization: In general LLM deployment settings, models are quantized to 4 bit for example using quantization methods like Quarot [2]. I think it is extremely important to study traffic for quantized models as that is a more general and viable usecase. Do the authors quantize the model pool? If yes which quantization method do they use? Are smaller models also quantized?\n- Finetuning: In general most models do undergo some sort of parameter efficient finetuning before deployment. Are any of the base models in the pool finetuning for specific tasks?\n\n[1] Xu, D., Zhang, H., Yang, L., Liu, R., Huang, G., Xu, M. and Liu, X., 2025, March. Fast on-device LLM inference with npus. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1 (pp. 445-462).\n[2] Ashkboos, S., Mohtashami, A., Croci, M.L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T. and Hensman, J., 2024. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37, pp.100213-100240."}, "questions": {"value": "Check weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F6Y0KGCAiS", "forum": "6Pv1dvbliS", "replyto": "6Pv1dvbliS", "signatures": ["ICLR.cc/2026/Conference/Submission5453/Reviewer_9HEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5453/Reviewer_9HEc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762749610785, "cdate": 1762749610785, "tmdate": 1762918069630, "mdate": 1762918069630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}