{"id": "fBagP6w6yE", "number": 24632, "cdate": 1758358762167, "mdate": 1759896757712, "content": {"title": "From Natural Alignment to Conditional Controllability in Multimodal Dialogue", "abstract": "The recent advancement of Artificial Intelligence Generated Content (AIGC) has led to significant strides in modeling human interaction, particularly in the context of multi-modal dialogue. \nWhile current methods impressively generates realistic dialogue in speech and vision modalities, challenges remain in multi-modal conditional dialogue generation. \nThis paper focuses on the natural alignment between speech, vision, and text, aiming at expressive dialogue generation through multi-modal conditional control. \nSince existing datasets lack the richness and diversity in dialogue expressiveness, we introduce a novel multi-modal dialogue annotation pipeline to exploit meaningful dialogues from movies and TV series with fine-grained annotations across multi-modalities.\nThe resultant dataset, MM-Dia, provides over 360 hours and 54,700 dialogues, facilitating the Multimodal Dialogue Generation task through explicit control over style-controllable dialogue speech synthesis. \nWhile the proposed benchmark, MM-Dia-Bench, containing 309 dialogues that are highly expressive with visible dual/single speaker scenes, supporting the evaluation of implicit cross-modal control through downstream multi-modal dialogue generation tasks to assess the audio-visual style consistency across modalities. \nOur experiments demonstrate the effectiveness of our data in enhancing style controllability and reveal limitations in current frameworks' ability to replicate human interaction expressiveness, providing new insights and challenges for multi-modal conditional dialogue generation. Code, demo and data will be released at: https://mmdiaiclr26.github.io/mmdiaiclr26/.", "tldr": "We propose an expressive multimodal dialogue dataset with dialogue-level style annotations using an automated pipeline, then introduce explicit and implicit control in multimodal dialogue generation.", "keywords": ["Multimodal dialogue dataset", "Multimodal conditional dialogue generation", "Spoken dialogue generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51136b32092f788c9a3961eccbc51760a730c6b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses critical limitations in multimodal dialogue generation—specifically the overemphasis on content transmission over style controllability, scarcity of high-quality datasets, and lack of benchmarks for cross-modal consistency. It focuses on achieving expressive, controllable multimodal dialogue through natural alignment of speech, vision, and text, while constructing a large-scale dataset and systematic benchmarks to advance the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces MM-DIA, a dataset curated from 700+ hours of movies/TV series (200+ films, 9 shows) with 360.26 hours of dialogue, 54,700 clips, and 449,138 turns. It features fine-grained annotations across modalities. The paper’s flagship contribution—the MM-DIA dataset—is the first to center on \"multimodal dialogue expressiveness\". To evaluate implicit cross-modal style consistency (a long-overlooked gap), the paper builds MM-DIA-BENCH—a balanced benchmark of 309 dual-speaker dialogues (1.69 hours, 1,851 turns) with guaranteed speaker visibility. Experiments show MM-DIA significantly enhances style controllability."}, "weaknesses": {"value": "The paper claims MM-DIA and its findings support \"a wide range of applications in human–computer interaction, social computing, and film-making\" but exclusively uses cinematic data (movies/TV series) for dataset construction and experiments. This creates a critical gap: it is unclear if the proposed framework (annotations, tasks, model insights) generalizes to non-scripted, real-world multimodal dialogue—arguably the most impactful use case for HCI and social computing."}, "questions": {"value": "You claim MM-DIA supports \"broad applications in HCI and social computing\" but exclusively use cinematic data (movies/TV series) for training and testing. Given that movie dialogue is scripted and emotionally exaggerated (e.g., MM-DIA’s average emotion intensity score of 6.76/10 via Gemini; )—a stark contrast to casual real-world interactions—have you tested if models fine-tuned on MM-DIA (e.g., Higgs-Audio-V2-SFT) retain style controllability on real-world multimodal datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4YtVU2lGE", "forum": "fBagP6w6yE", "replyto": "fBagP6w6yE", "signatures": ["ICLR.cc/2026/Conference/Submission24632/Reviewer_8JZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24632/Reviewer_8JZi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821742070, "cdate": 1761821742070, "tmdate": 1762943140346, "mdate": 1762943140346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MM-DIA, a large-scale, richly annotated multimodal dialogue dataset from movies and TV series, and MM-DIA-BENCH, a benchmark for evaluating cross-modal conditional generation. Experiments show MM-DIA improves style-controllable dialogue generation"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This is the first dataset to focus on dialogue expressiveness across multiple modalities. The benchmark (MM-DIA-BENCH) fills a gap for evaluating cross-modal style consistency, which is underexplored in prior work.\n2. The paper provides a unified framework for MDG, with well-defined tasks and evaluation metrics. Experiments are thorough, with both objective and subjective metrics."}, "weaknesses": {"value": "1. The paper’s main contribution is dataset and benchmark creation; the modeling advances are limited to fine-tuning existing architectures and adapter modules for controllability. No novel end-to-end model for multimodal dialogue generation is proposed or evaluated.\n2. The paper is too dense and at times it is difficult to follow, especially in the technical details of the pipeline and annotation process.\n3. The dataset is sourced primarily from movies and TV series, which may limit the diversity and generalizability to real-world, spontaneous dialogues"}, "questions": {"value": "1. How do you envision MM-DIA supporting research on more spontaneous, real-world dialogues? \n\n2. How does your approach handle long-range dependencies, such as multi-turn conversations or scenes with complex speaker dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sLcv1wdbEz", "forum": "fBagP6w6yE", "replyto": "fBagP6w6yE", "signatures": ["ICLR.cc/2026/Conference/Submission24632/Reviewer_KcRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24632/Reviewer_KcRL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866502020, "cdate": 1761866502020, "tmdate": 1762943140124, "mdate": 1762943140124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations in scale, expressiveness, and benchmarking of existing datasets for multimodal dialogue generation by proposing a novel data curation and annotation pipeline, resulting in the large-scale and expressive multimodal dialogue dataset MM-DIA. The authors further introduce a unified framework for Multimodal Dialogue Generation (MDG) and define three representative downstream tasks, including style-controllable speech synthesis, vision-conditioned speech synthesis, and speech-driven video generation. Through systematic benchmarks and experiments, the paper demonstrates the effectiveness of the new dataset in enhancing dialogue style controllability and cross-modal consistency, while revealing the shortcomings of current methods in expressiveness and multimodal alignment. This work provides valuable resources and new challenges for future research in conditional multimodal dialogue generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The unified framework for multimodal dialogue generation proposed in this paper is highly practical and extensible, providing systematic task definitions and benchmarking foundations that will facilitate further advances in the field.\n2. The experimental section is comprehensive, covering various downstream tasks and systematically benchmarking the dataset and methods for style controllability and cross-modal consistency, with convincing results.\n3. The paper is well-structured and clearly articulated, progressing logically from problem motivation, dataset construction, method design, to experimental evaluation, making it easy for readers to follow and understand the research."}, "weaknesses": {"value": "1. Although the dataset is large and expressive, it is mainly sourced from movies and TV series, which may differ from real-life conversations and affect the generalizability of models to real-world scenarios.\n2. The evaluation methodology in the paper is insufficient for assessing the generalization ability of the trained models. In Section 5.1, the authors mention an out-of-domain dataset, but do not clearly specify whether it comes from different data sources or demonstrate its differences. The model’s performance in real-world scenarios, as well as the potential degradation of its original capabilities after SFT (fine-tuning), require further consideration and analysis.\n3. There are some typographical errors in the paper, such as the table on page 782 not being cited."}, "questions": {"value": "1. In the evaluation, the authors use Gemini as a judge. Has the accuracy of using large models as judges been tested, and has the model’s performance been compared with human evaluation?\n2. In this paper, the authors achieve \"From Natural Alignment to Conditional Controllability\" from a data perspective. From a methodological standpoint, do you think further improvements at the model level could help achieve this goal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "X1ZXJbhYHq", "forum": "fBagP6w6yE", "replyto": "fBagP6w6yE", "signatures": ["ICLR.cc/2026/Conference/Submission24632/Reviewer_jqoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24632/Reviewer_jqoy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981276860, "cdate": 1761981276860, "tmdate": 1762943139909, "mdate": 1762943139909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MM-DIA, the first large-scale and highly expressive multimodal dialogue dataset designed for Multimodal Dialogue Generation (MDG). In addition, the authors present MM-DIA-BENCH, a dual-speaker benchmark specifically developed for evaluating cross-modal conditional generation.\nExperiments show that training on MM-DIA significantly enhances controllable dialogue generation, while evaluations on MM-DIA-BENCH reveal notable limitations of current models in achieving consistent multimodal style alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is well-motivated, and the proposed dataset paves the way for future research on style controllability of multi-modal dialogue generation.\n\n* Strong experimental validation with ablations on both controllability and user satisfaction metrics.\n\n* This paper is clearly structured and easy to follow.\n\n* Although the dataset creation heavily relies on models, the authors try to demonstrate that the proposed pipeline achieves human-level quality in annotation consistency and reliability"}, "weaknesses": {"value": "* The data creation and evaluation partly rely on GPT-based scoring, which could cause an upper bound of future research."}, "questions": {"value": "Pls refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuR5T8AhwV", "forum": "fBagP6w6yE", "replyto": "fBagP6w6yE", "signatures": ["ICLR.cc/2026/Conference/Submission24632/Reviewer_Nvzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24632/Reviewer_Nvzm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001235438, "cdate": 1762001235438, "tmdate": 1762943139682, "mdate": 1762943139682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}