{"id": "odsl3gqVSX", "number": 12038, "cdate": 1758205336313, "mdate": 1759897537756, "content": {"title": "Towards Optimism-Pessimism Trade-off in Model-based Offline-to-Online Reinforcement Learning", "abstract": "Model-based offline-to-online reinforcement learning (RL) provides a sample-efficient framework by pre-training environment models and control policies using offline data, followed by fine-tuning through limited online interactions. However, the distribution shifts between offline and online stages often hinders fine-tuning performance. Existing methods approach this problem by adjusting the trade-off between optimism and pessimism using a single-objective formulation, which requires online evaluation across tasks. This results in an expensive bi-level optimization procedure. In this work, we identify this optimism-pessimism trade-off during offline training as a key challenge: optimistic policies tend to generalize better to novel online tasks by exploring out-of-distribution states and actions, while pessimistic policies remain constrained to the offline data distribution and perform better on tasks that are similar to the offline tasks. To address this challenge, we propose a bi-objective formulation that captures this trade-off and yields a pool of Pareto policies during offline training. These policies reflect varying levels of trade-offs, enabling flexible selection of policies for various online tasks. To produce these policies, we introduce Multiple-Objective Soft Actor-critIC (MOSAIC), which solves multiple bi-objective optimization problems guided by reference vectors and refines the Pareto policy pool through neighborhood search. After offline training, a contextual bandit algorithm hierarchically selects the most suitable policy for fine-tuning at each online interaction step. Empirically, our pipeline,**Hi**erarchical **P**areto **P**olicy **P**ool (**HiP3**), achieves state-of-the-art performance on offline-to-online RL benchmarks with diverse online tasks. Comprehensive ablation studies are conducted to further elucidate the mechanisms behind HiP3.", "tldr": "", "keywords": ["Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/120d060d4542687f92284e6544a214497b5b4584.pdf", "supplementary_material": "/attachment/4a1aaec717a8fbdf9ef3eca0af17936261904001.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces HiP3 (Hierarchical Pareto Policy Pool), a model-based offline-to-online reinforcement learning framework. It utilizes MOSAIC (Multiple-Objective Soft Actor-critIC) to generate a pool of Pareto policies that capture different optimism–pessimism trade-offs during offline training, and then adopts a contextual bandit algorithm to adaptively select the most suitable policy for online fine-tuning. Experimental results demonstrate that HiP3 achieves state-of-the-art performance on multiple D4RL benchmarks, particularly showing superior adaptability to novel online tasks. Empirical evaluations confirm its superior performance compared to prior state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.By formulating the optimism–pessimism trade-off as a bi-objective optimization problem, the authors provide empirical evidence supporting a balanced trade-off between exploration and conservatism, as illustrated in Figure 1, where different Pareto policies exhibit varying optimism–pessimism trade-offs and corresponding online adaptation behaviors.\n2. The HiP3 framework integrates MOSAIC with a contextual bandit (LinUCB) for adaptive online policy selection, enabling state-dependent switching between optimistic and pessimistic policies.\n3. Extensive experiments on D4RL show consistent state-of-the-art results, particularly in novel online tasks."}, "weaknesses": {"value": "1. The HiP3 pipeline introduces complexity in both design and computation due to the multi-stage process of generating and selecting from a diverse Pareto policy pool, which increases both algorithmic complexity and the difficulty of implementation.\n2. The hierarchical selection strategy appears effective empirically but is justified mainly through intuition and experiments."}, "questions": {"value": "1. How large is the gap between novel online tasks and the original tasks in terms of task distribution shift? How do the authors propose to measure the difference between the two tasks, and is the novel online task inherently more difficult or easier than the original tasks? Additionally, how does HiP3 maintain performance across such variations?\n2. How computationally expensive is maintaining and updating the Pareto policy pool, especially in high-dimensional continuous action spaces?\n3. What is the computational overhead (training time and memory) introduced by maintaining multiple Pareto policies compared to other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oV3h6m2xcf", "forum": "odsl3gqVSX", "replyto": "odsl3gqVSX", "signatures": ["ICLR.cc/2026/Conference/Submission12038/Reviewer_Wm1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12038/Reviewer_Wm1E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798381155, "cdate": 1761798381155, "tmdate": 1762923016570, "mdate": 1762923016570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Hierarchical Pareto Policy Pool (HiP3) method was proposed to alleviate the distribution shift in offline-to-online reinforcement learning based on models and to improve the fine-tuning performance. This method mainly consists of the following three modules:\n\n1. A bi-objective formulation was proposed to balance the optimistic (model-predicted rewards) and pessimistic (model uncertainty) two indicators.\n\n2. Multiple-Objective Soft Actor-critic (MOSAIC) extends Soft Actor-Critic (SAC), solves the problem of multi-objective optimization, and combines the neighborhood search method to discover the Pareto optimal strategy.\n3. Utilize a hierarchical reinforcement learning approach, and combine a contextual bandit algorithm (LinUCB) as an advanced strategy to reduce the number of interactions with the environment and more efficiently select the most suitable strategy for fine-tuning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe \"Hierarchical Pareto Strategy Pool\" (HiP3) method was proposed. It alleviated the distribution bias in the process of offline-to-online reinforcement learning based on models, improved the fine-tuning performance, and achieved efficiency and low consumption.\n\n2.\tThe theoretical explanations and formula derivations are very thorough.\n\n3.\tThe experimental verification is very thorough."}, "weaknesses": {"value": "1.\tDuring the process of generating the strategy pool in the offline stage, the initial and final values of the reference vector, 0.1 and 0.9, were directly set without any related analysis, and there is a possibility that the actual situation may deviate from this.\n\n2.\tThe values of the reference vectors in the paper are uniform. However, in reality, non-uniform distribution may occur, which could lead to inefficiency."}, "questions": {"value": "1.\tHave you considered using an LLM?\n\n2.\tCurrently, the experiments are conducted in a fully observable environment. Have we considered exploring the adaptability in more complex environments (such as unobservable environments, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Bt3mcvhvF", "forum": "odsl3gqVSX", "replyto": "odsl3gqVSX", "signatures": ["ICLR.cc/2026/Conference/Submission12038/Reviewer_sVaX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12038/Reviewer_sVaX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810691218, "cdate": 1761810691218, "tmdate": 1762923016132, "mdate": 1762923016132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors address the optimism-pessimism trade-off in model-based offline-to-online RL. They introduce an algorithm (MOSAIC / HiP3) that seeks to get the best of both worlds by learning a pool of policies with different levels of optimism/pessimism, and learning how to switch between them."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- MOSAIC doesn’t require solving prohibitively expensive bi-level optimization problems.\n- Authors provide theoretical convergence statement for MOSAIC.\n- Authors provide code to reproduce their results."}, "weaknesses": {"value": "The paper has two key weaknesses: the evaluation environments, baselines, and results are underwhelming; and the motivation is weak, given that the method introduces a lot of complexity that does not seem to yield very significant improvements.\n\n- Very limited evaluation domains: the evaluation suite consists only of 3 different environments (with 4 datasets each) and only of locomotion domains. It is unclear how MOSAIC would perform in other domains.\n- The baseline comparisons are insufficient. The only other model-based method included is MBPO, which is 6 years old. Why were other newer model-based RL methods not included, e.g. TD-MPC2 (ICLR 2024 spotlight)?\n- Results across the paper use 3 seeds and 10 evaluations each. This is not enough (e.g. in table 1 MBPO and HiP3 have overlapping confidence intervals for total mean).\n- The method is significantly more complex than MBPO, but doesn’t get statistically significantly better performance (Tab. 1).\n- The explanation in the second paragraph for the proof-of-concept experiment is very unclear and hard to follow. It is unexplained what “pool of Pareto policies” means. It is also unclear what optimism and pessimism mean here. When authors mention fine-tuning these policies, it is unclear what the fine-tuning method they used was. Why dies (a) use unnormalized returns and (b, c) use normalized score?"}, "questions": {"value": "- Figure 1: what is “negative uncertainty predicted by the Env model”?\n- How were the environments shown in Figure 3 selected? Can you add another plot with average performance over every environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UyMThXyKFk", "forum": "odsl3gqVSX", "replyto": "odsl3gqVSX", "signatures": ["ICLR.cc/2026/Conference/Submission12038/Reviewer_jJ8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12038/Reviewer_jJ8E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967325394, "cdate": 1761967325394, "tmdate": 1762923015647, "mdate": 1762923015647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}