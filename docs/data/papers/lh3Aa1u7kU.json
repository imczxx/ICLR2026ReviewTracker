{"id": "lh3Aa1u7kU", "number": 12303, "cdate": 1758206950681, "mdate": 1763566812417, "content": {"title": "Stacked from One: Multi-Scale Self-Injection for Context Window Extension", "abstract": "The limited context window of contemporary large language models (LLMs) hinders broader application. In this work, we present SharedLLM, a novel approach grounded in the design philosophy of multi-grained context compression and query-aware information retrieval. SharedLLM is composed of two short-context LLMs: a lower moel (compressor) and an upper model (decoder). The lower model compresses context information, while the upper model processes compressed, context information from the lower model and performs context-aware modeling. Information transfer between the compressor and decoder occurs only at the lowest layers to reduce redundant computation. Based on this architecture, we introduce a specialized tree-style data structure to efficiently encode, store and retrieve multi-grained contextual information from text chunks. This entire process, wherein the sender and receiver are derived from the same LLM layer, is referred to as self-injection. In our evaluation on long-context modeling and understanding tasks, SharedLLM achieves superior or comparable results to several strong baselines, striking an effective balance between efficiency and performance. Meanwhile, with the aforementioned design choices, SharedLLM can greatly reduce memory consumption, and demonstrates substantial speed-ups over other advanced baselines. The core code of our implementation along with training and evaluation is available in appendix and supplementary.", "tldr": "An efficient method to fast extend the context window length of small language models", "keywords": ["long-context modeling; continual pretraining; extrapolation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bae2ced944afce1bc4e7d4205f34d01b96d56e4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the limitation of finite context windows in Large Language Models (LLMs), this paper proposes the SHAREDLLM framework. Grounded in the concepts of multi-grained context compression and query-aware information acquisition, SHAREDLLM consists of two stacked models derived from the same base short-context LLM: a lower \"compressor\" and an upper \"decoder\". It enables \"self-injection\" through shared key-value (KV) states at the lower layers, thus avoiding redundant computations. A core innovation is the context tree—a binary tree structure that dynamically encodes long context into coarse-to-fine representations, expanding only task-relevant nodes. Trained on 8K-length sequences, the model can generalize to 128K+ tokens. It outperforms baselines such as CEPE and Activation Beacon on language modeling (perplexity) and long-context understanding tasks (LongBench/InfiniBench), with an inference speed 2× faster than streaming architectures and 3× faster than encoder-decoder architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dynamic expansion of the context tree adapts to task requirements, balancing information retention and efficiency, representing a relatively novel idea. It provides researchers with a lightweight alternative, lowering the threshold for long-context research; efficiency improvements expand the application of LLMs in long-text scenarios."}, "weaknesses": {"value": "The model is only tested on up to 128K tokens, yet it claims to \"generalize to arbitrary lengths\"—supplementary experiments on 256K tokens or theoretical analysis are needed to support this claim.Experiments only provide quantitative results without qualitative examples (e.g., cases where the model correctly identifies key information in passkey retrieval tasks), making it difficult to intuitively demonstrate advantages."}, "questions": {"value": "Can experiments on 256K tokens be supplemented? If not, theoretical justification for the \"arbitrary length generalization\" claim is required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2H1XNQQnIf", "forum": "lh3Aa1u7kU", "replyto": "lh3Aa1u7kU", "signatures": ["ICLR.cc/2026/Conference/Submission12303/Reviewer_CrBs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12303/Reviewer_CrBs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827660611, "cdate": 1761827660611, "tmdate": 1762923232264, "mdate": 1762923232264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-LLM architecture for long-context inference. One LLM compresses the context, and the other LLM operates based on compressed context, achieving 2x speedup without hurting accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A more end-to-end approach that also includes LLM training stage."}, "weaknesses": {"value": "* This is a crowded space, and needs to be more embedded into related work to justify the novelty of this work."}, "questions": {"value": "* The proposed method --- using LLM to compress context --- reminds me of MemGPT and generative agent paper, where they also uses the LLM to compress the context. How do you compare with these two approaches? I understand that you train the model end-to-end, but algorithm-wise is there any reason to believe your approach is better?\n* Given the accuracy number of your system is close to token dropping based approach, and SnapKV, though it is a strong baseline, is not state-of-the-art based on NVIDIA's KVPress measurement (https://huggingface.co/spaces/nvidia/kvpress-leaderboard), I would suspect other baselines may have higher accuracy than SnapKV and be better than your approach. Is there any reason to believe that is not the case?\n* Regarding training --- why SFT instead of RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NjMoYaVgvW", "forum": "lh3Aa1u7kU", "replyto": "lh3Aa1u7kU", "signatures": ["ICLR.cc/2026/Conference/Submission12303/Reviewer_xDgb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12303/Reviewer_xDgb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888355757, "cdate": 1761888355757, "tmdate": 1762923231968, "mdate": 1762923231968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SharedLLM, a novel two-stage LLM framework designed to extend the context window of short-context language models. Specifically, it consists of 1) a lower model (i.e., compressor) that segments long input sequences and compresses each into hierarchical “context trees” and 2) an upper model (i.e., decoder) that retrieves relevant information from them. The mechanism is called self-injection. Cross-attention from query to the shared KV between the same base model’s layers efficiently injects the compressed/selected input context information. SharedLLM demonstrates long-context generalization (up to 128K tokens trained only on 8K) capability and outperforms baselines such as CEPE and Activation Beacon. In addition, the paper reports accelerated inference and memory usage savings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of using a single model for two complementary purposes in long-context processing is novel. Leveraging the same base model to ensure compatibility is both intuitive and efficient.\n* The context tree construction, which identifies relevant segments within each chunk without requiring an additional similarity computation module, is original and practical.\n* The method avoids complex attention mechanisms, enabling the reuse of existing optimization techniques such as FlashAttention."}, "weaknesses": {"value": "* As shown in Figure 4, performance improvements with respect to tree height and compression ratio appear somewhat inconsistent, suggesting potential sensitivity to hyperparameters. Furthermore, although the method is generally robust within moderate ranges, its reliance on rule-based policy selection and heuristic coarse-to-fine downsampling may limit task generalization.\n* The lack of inter-chunk dependency modeling is understandable for parallelization and optimization efficiency, but it may introduce limitations in long context integration.\n* The Passkey task might be too well aligned with SharedLLM’s query-aware design, making it an easier benchmark for this method. Additional justification or analysis would strengthen the empirical evaluation."}, "questions": {"value": "* The proposed mechanism seems to be designed for the prefill phase; it may have a limited impact during decoding.\n* When positional indices are added in the cross-attention module, are they implemented as sinusoidal position embeddings directly added to the key–value states, rather than as RoPE-style rotations?\n* During fine-tuning, are only the cross-attention components trained while other parameters are frozen, or is the model fully fine-tuned?\n* (suggestion) The near-half-randomness in context tree construction is interesting, but its benefits are not entirely clear. Given that later tokens within a chunk often implicitly encode earlier information, maybe a deterministic segmentation strategy, such as splitting at points of large neighboring vector similarity differences, would be better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oWzUOyc3TQ", "forum": "lh3Aa1u7kU", "replyto": "lh3Aa1u7kU", "signatures": ["ICLR.cc/2026/Conference/Submission12303/Reviewer_4Mtb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12303/Reviewer_4Mtb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977589566, "cdate": 1761977589566, "tmdate": 1762923231687, "mdate": 1762923231687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThis work presents SHAREDLLM, a novel framework addressing the challenge of efficient long-context inference. Its key innovation is a two-stage process involving a lower model, which compresses context chunks via a Context Tree structure, and an upper model, which decodes  successive tokens from the compressed KV-cache. This architecture enables an extended context window while enhancing inference efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**\n\n（1）This study presents a novel method for addressing the challenges of long-context inference, with its architectural details thoroughly elaborated. \n\n（2）The efficacy of the proposed framework is demonstrated through experiments, which confirm its capability to extend the context window and improve inference efficiency. Furthermore, ablation studies provide evidence for the effectiveness of the introduced context information injection mechanism."}, "weaknesses": {"value": "**Weakness**\n\n(1) The research motivation is not sufficiently clear. The point raised in the introduction—that \"specialized attention patterns may cause incompatibility with high-performance attention implementations\"—does not adequately motivate the proposed method. For instance, prompt compression methods (e.g., ICAE and UniICL) can also improve long-context inference efficiency without requiring specialized attention mechanisms. However, the authors neither compare their method with these alternatives nor clarify the uniqueness of the problem their approach aims to solve.\n\n(2) The abstract requires smoother expression. For example, the definition of \"self-injection\" is repeated, resulting in redundant content. Additionally, the mention of \"sender\" and \"receiver\" concepts in the abstract may lead readers to believe they are important modules, yet these terms are only mentioned there, which causes confusion.\n\n(3) If no information has been overlooked, Tables 2 and 3 do not report the memory usage or inference latency of the baseline methods. Without aligning these factors, it is difficult to ensure a fair comparison between different methods."}, "questions": {"value": "**Suggestions**\n\n(1) Improve the expression in the Abstract and Introduction to enhance readability.\n\n(2) Include the memory usage or inference latency of all compared methods in Tables 2 and 3 to ensure a fair comparison.\n\n(3) If feasible, conduct experiments on larger models to validate the scalability of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tMy1Qnx4t0", "forum": "lh3Aa1u7kU", "replyto": "lh3Aa1u7kU", "signatures": ["ICLR.cc/2026/Conference/Submission12303/Reviewer_AWmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12303/Reviewer_AWmy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762279007637, "cdate": 1762279007637, "tmdate": 1762923231274, "mdate": 1762923231274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}