{"id": "gUw8n109ki", "number": 19986, "cdate": 1758301235729, "mdate": 1759897008023, "content": {"title": "Why does Weak-OOD Help? A Further Step Towards Understanding Jailbreaking VLMs", "abstract": "Large Vision-Language Models (VLMs) are susceptible to jailbreak attacks: re-\nsearchers have developed a variety of attack strategies that can successfully by-\npass the safety mechanisms of VLMs. Among these approaches, jailbreak meth-\nods based on the Out-of-Distribution (OOD) strategy have garnered widespread\nattention due to their simplicity and effectiveness. This paper further advances\nthe in-depth understanding of OOD-based VLM jailbreak methods. Experimen-\ntal results demonstrate that jailbreak samples generated via mild OOD strategies\nexhibit superior performance in circumventing the safety constraints of VLMs—a\nphenomenon we define as “weak-OOD”. To unravel the underlying causes of this\nphenomenon, this study takes SI-Attack, a typical OOD-based jailbreak method,\nas the research object. We attribute this phenomenon to a trade-off between two\ndominant factors: input intent perception and model refusal triggering. The incon-\nsistency in how these two factors respond to OOD manipulations gives rise to this\nphenomenon. Furthermore, we provide a theoretical argument for the inevitabil-\nity of such inconsistency from the perspective of discrepancies between model\npre-training and alignment processes. Building on the above insights, we draw\ninspiration from optical character recognition (OCR) capability enhancement—a\ncore task in the pre-training phase of mainstream VLMs. Leveraging this ca-\npability, we design a simple yet highly effective VLM jailbreak method, whose\nperformance outperforms that of SOTA baselines. Code is available at Github", "tldr": "VLMs are vulnerable to jailbreaks (OOD methods effective). This study finds \"weak-OOD\" boosts jailbreak performance (linked to input intent-model refusal trade-off from training-alignment gaps) and designs a new method outperforming baselines.", "keywords": ["Jailbreak Attack", "VLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0b2b2f7e10ad68dfa7dae04db749a083d55014f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates Out-of-Distribution (OOD)-based jailbreak attacks on large Vision-Language Models (VLMs), focusing on a phenomenon called weak-OOD. The authors reaffirm that mild OOD perturbations to images (e.g., shuffling or mixup) often improve jailbreak success rates, whereas stronger perturbations degrade attack performance. The paper attributes this weak-OOD effect to a trade-off between two internal mechanisms within safety-aligned VLMs: mild OOD perturbations maintain malicious intent perception (via staying close to the pre-training distribution) while weakening the refusal trigger (via naturally lying far from the narrow alignment training distribution). Building on this understanding, the paper introduces a new attack that exploits discrepancy of OCR capability induced from the mismatched generalization between pre-training and safety-alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper reaffirms the vulnerability of VLMs with regard to weak OOD harmful inputs and the mismatched generalization between pre-training and safety-alignment training.\n- The proposed method extends prior work (i.e., FigStep) which leverages the poor generalization of safety-alignment on OCR harmful inputs, showing great jailbreak effectiveness even with simple modifications (e.g., font size variation) from the baseline (FigStep).\n- The paper proposed a mechanistic quantitative framework to measure how OOD manipulations affect intent preservation and refusal triggering in VLMs.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The paper’s central hypothesis that the weak-OOD phenomenon arises from the asymmetry between pre-training and alignment is not conceptually new. Similar insights have already been discussed in the previous work (https://arxiv.org/abs/2307.02483), which tackles mismatched generalization that arises when inputs are out-of-distribution for a model’s safety training data but within the scope of its\nbroad pretraining corpus.\n- This paper largely reiterates previously reported empirical findings rather than uncovering a new behavioral pattern, offering limited novelty in its characterization of the phenomenon: the weak-OOD phenomenon itself — where mild OOD perturbations preserve intent perception while lowering refusal rates, but stronger OOD disrupts intent understanding — was already empirically observed in prior work, such as JOOD.\n- Moreover, while the paper stresses the importance of weak-OOD manipulation, JOCR provides no principled procedure for generating or identifying weak-OOD inputs, relying instead on marginal modifications (e.g., font and spacing variations) on the baseline (FigStep). Such ad-hoc tweaks still might not offer guarantee of intent preservation, possibly yielding strong-OOD inputs.\n- (Minor) Some typos: \"the its\" in L81, \"is more than the distance\" in L385"}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kBuVWTpvFG", "forum": "gUw8n109ki", "replyto": "gUw8n109ki", "signatures": ["ICLR.cc/2026/Conference/Submission19986/Reviewer_Nqfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19986/Reviewer_Nqfw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811308023, "cdate": 1761811308023, "tmdate": 1762932891805, "mdate": 1762932891805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why “weak” Out-of-Distribution (OOD) perturbations are surprisingly effective in jailbreaking Vision-Language Models. The authors design JOCR, a new OOD-based jailbreak method that leverages the model’s OCR capability from pre-training. JOCR achieves state-of-the-art jailbreak success on multiple benchmarks (e.g., HarmBench, RedTeam-2K)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a novel theoretical insight by identifying the pretrain–alignment inconsistency as the key reason why weak OOD perturbations can effectively bypass safety mechanisms.\n It demonstrates strong empirical rigor, with comprehensive ablations and benchmarks confirming that mild OOD manipulations outperform existing jailbreak methods.\nThe proposed JOCR attack introduces an innovative OCR-aware mechanism that not only validates the theory but also exposes practical vulnerabilities in real-world VLM safety alignment."}, "weaknesses": {"value": "1. While the paper attributes the improved jailbreak success under mild perturbations to a weak-OOD effect arising from pretrain–alignment mismatch, an alternative explanation could be that the model is simply non-robust to low-level perturbations.\nThe experiments demonstrate that moderate perturbations yield higher attack success rates, but it is unclear whether this behavior necessarily reflects a semantic distribution shift rather than standard sensitivity to visual noise or encoding artifacts.\n\n2. The proposed “weak-OOD” notion is very close to established ideas in robustness research, yet the paper does not clearly explain what is new beyond existing robustness frameworks. \n\n3. If i do some perturbation and trained for robustness improvement, will that solve the issue?\n\n4. It is not very clear of why this happens in practical. For evaluations, could you add some reasoning/interpretation so that it can understand more context?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bDmo4eVWd8", "forum": "gUw8n109ki", "replyto": "gUw8n109ki", "signatures": ["ICLR.cc/2026/Conference/Submission19986/Reviewer_tECe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19986/Reviewer_tECe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862408034, "cdate": 1761862408034, "tmdate": 1762932891219, "mdate": 1762932891219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why weak-OOD manipulations (mild distributional shifts in jailbreak inputs) can outperform stronger perturbations when attacking VLMs. The authors identify a trade-off between input-intent perception and model-refusal triggering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is easy to follow and read.\n2. The intuition of seeking weak-OOD in perturbation is straightforward."}, "weaknesses": {"value": "1. Layer heterogeneity is ignored in the perception measurement. Simply doing the aggregation by averaging the Layer-Wise Cosine Similarity and Layer-Wise Refusal Similarity seems a bit too brutal.\n2. Especially for the jailbreak part, the different settings in creating the perturbations are not provided with ablation studies (those from appx. tab. C). The impact of the settings on the degree of intent perception and on the refusal triggering is not quantified or further analyzed. I think this is an important part that is missing from the paper."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "meYstRCgTx", "forum": "gUw8n109ki", "replyto": "gUw8n109ki", "signatures": ["ICLR.cc/2026/Conference/Submission19986/Reviewer_jc87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19986/Reviewer_jc87"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877495859, "cdate": 1761877495859, "tmdate": 1762932890440, "mdate": 1762932890440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why mild out-of-distribution perturbations (“weak-OOD”) often lead to higher jailbreak success rates on vision-language models (VLMs) than either no OOD or strong OOD perturbations. The authors analyze this effect through experiments on representative OOD jailbreak methods and show that moderate shifts preserve malicious-intent perception while weakening refusal behavior. They argue this arises due to misaligned robustness between pre-training (which enables stable intent perception) and safety alignment (more brittle refusal triggers). They then introduce JOCR, a jailbreak method that perturbs embedded text in ways consistent with OCR features learned during pre-training. JOCR achieves higher attack success rates than prior methods across multiple VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on a meaningful question—why weak OOD helps jailbreak VLMs, rather than only proposing another attack. The authors perform ablations across multiple attack methods and target models, illustrating the weak-OOD pattern with concrete quantitative trends and internal activation analyses. The mechanistic view separating intent perception and refusal triggers is supported by empirical signals across layers, and the theory connects naturally to pre-training versus alignment data distribution. Based on this insight, the paper builds JOCR, which demonstrates notable improvements over strong baselines. The argument is cohesive from empirical observation to conceptual explanation to method design."}, "weaknesses": {"value": "Although the central idea is interesting, the theoretical component still feels heuristic. The latent-space interpretation depends on assumptions about token roles and feature locality that recent work debates; the evidence is more correlational than causal. \n\nSome experimental choices raise questions about generalization. For example, the heavy reliance on shuffle-based perturbations when defining OOD magnitude, and the use of GPT-4o as a judge in evaluation, which may introduce bias or shortcuts. \n\nThe JOCR method is effective, but largely builds on a known idea (text embedding via typography) plus stochastic perturbations, so the novelty lies mainly in conceptual framing rather than methodology. \n\nThe work does not deeply explore defenses, which weakens practical impact for safety research."}, "questions": {"value": "How stable is the weak-OOD effect across non-shuffle OOD directions, such as color jitter, geometric warps, or structured occlusion?\n\nDoes JOCR degrade substantially when OCR content is noisy or stylized beyond typical pre-training patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3kMM3zFBM4", "forum": "gUw8n109ki", "replyto": "gUw8n109ki", "signatures": ["ICLR.cc/2026/Conference/Submission19986/Reviewer_Zt8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19986/Reviewer_Zt8k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941590276, "cdate": 1761941590276, "tmdate": 1762932889510, "mdate": 1762932889510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}