{"id": "hD69qj15Os", "number": 8385, "cdate": 1758080987932, "mdate": 1763701056395, "content": {"title": "Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation", "abstract": "Computing the next-token likelihood ratio between two language models (LMs) is a standard task in training paradigms like knowledge distillation. Since this task requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only $\\mathcal{O}(1)$ model evaluations per token. When used for distillation, this yields up to a 12\\% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4\\% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.", "tldr": "TL;DR: Algorithms to compute token sequences under a target BPE vocabulary that differs from the model’s native tokenizer.", "keywords": ["Tokenization", "likelihood scoring", "language models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1415596386025d86d2c6bcac78ffbfa5240f92b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces cross-tokenizer likelihood scoring algorithms that enable language models trained with one tokenizer to compute exact or approximate likelihoods for sequences encoded with a different tokenizer by exploiting the recursive structure of Byte-Pair Encoding. The authors prove the merits of their methods through comparing against baselines on the knowledge distillation task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is addressing an important research question: how to compute next-token likelihoods and perform knowledge distillation when teacher and student models use different tokenizers (i.e., resolve vocabulary misalignment). \n\n2. The paper is well written.\n\n3. The paper’s idea is novel.\n\nI don't have the background to evaluate more fine-grained part of the methodology proposed in the paper."}, "weaknesses": {"value": "I don't have the background to evaluate more fine-grained part of the methodology proposed in the paper. I will focus on evaluating the baselines.\n\nIn the experiment, the paper doesn't include the baselines for other methods that try to address the knowledge distillation problem with teacher and student sharing different tokenizers (e.g. those methods that align teacher and student on the level of embedding). I understand that the paper claims that it focuses on probability conversion on the tokenizer directly. However, in my opinion, unless the paper also show that their method is helpful in addressing some tasks other than knowledge distillation, other knowledge distillation baselines may need to be included. This is especially so when considering the fact this paper can only deal with the BPE tokenizer, while other aligning methods may deal with arbitrary tokenizers and models for the knowledge distillation task."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5cNv1TIovx", "forum": "hD69qj15Os", "replyto": "hD69qj15Os", "signatures": ["ICLR.cc/2026/Conference/Submission8385/Reviewer_c3nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8385/Reviewer_c3nd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760815826120, "cdate": 1760815826120, "tmdate": 1762920291929, "mdate": 1762920291929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cross-Tokenizer Likelihood (CTL), a novel probabilistic alignment method addressing inconsistencies between multilingual tokenizers.\nBy minimizing per-token log-likelihood gaps across languages, CTL improves cross-lingual representation consistency and translation faithfulness.\nThe method is simple, elegant, and easily integrable into existing multilingual models (Qwen2.5-7B, XGLM-4.5B)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Tackles a long-standing issue—tokenizer mismatch—using a mathematically grounded likelihood objective rather than architecture tricks.\n\n2. Improves multilingual alignment and reduces tokenization bias, especially for low-resource or morphologically rich languages.\n\n3. The CTL layer is training-agnostic and introduces negligible computational overhead.\n\n4. Consistent improvements across translation, code-switching, and QA tasks; simplicity and reproducibility make it valuable for practitioners.\n\n5. The loss function is interpretable and differentiable, connecting probabilistic alignment with linguistic intuitions."}, "weaknesses": {"value": "Only three downstream tasks (translation, QA, code-switching). Additional domains such as summarization or retrieval would strengthen generality."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wKuQ1u2E3w", "forum": "hD69qj15Os", "replyto": "hD69qj15Os", "signatures": ["ICLR.cc/2026/Conference/Submission8385/Reviewer_u4Tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8385/Reviewer_u4Tg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776763470, "cdate": 1761776763470, "tmdate": 1762920291523, "mdate": 1762920291523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to exactly convert BPE-vocab LM models to any subset vocabulary(eg. Byte-level) in O(1) model evaluations. Furthermore, the authors propose a (somewhat expensive) method to approximately up-convert the byte-level vocab LM to an obtain probabilities for any other BPE vocab. The authors show that their proposed vocab conversion achieves low approximation error in token probabilities, and can be used for cross-model distillation and vocab pruning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method computes exact sub-token probabilities in O(1) model evaluations for BPE vocabs\n1. The proposed method is \"training-free\" on the teacher - requiring no training of new LM-heads, projections, etc\n1. The authors successfully utilize their method to distill across models and for vocabulary trimming."}, "weaknesses": {"value": "1. The proposed method has extremely large overhead for cross-tokenization distillation - large number of beam-search (6-8 beams) upto maximum length 10 for calculating every token probability.\n1. Only 1 baseline method is compared against (ALM) - other methods for cross-model distillation should also be compared.\n1. Empirical evaluations are extremely limited."}, "questions": {"value": "1. For Figure 3 (Section 6.1), can the authors share the effective LM loss (probability of ground truth token) for the original model and their re-converted model, and the LM loss of other smaller original Qwen models (no conversion needed) on the same samples? This can more directly show how much performance/\"effective model size\" is being lost in this conversion.\n1. In Table 3, for vocabulary trimming, can the authors also train the original (full vocab) model with the same warmup and distillation process? The vocabulary reduced models surprisingly achieve a \"higher\" score than the original models, while would imply the training process is significantly improving the model. Without these original scores, there is no way to judge the effectiveness of this conversion.\n1. For the beam search approximation in C1,  can the authors compare the quality of the predicted probabilities as the beam size is varied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aHXMR4zNL9", "forum": "hD69qj15Os", "replyto": "hD69qj15Os", "signatures": ["ICLR.cc/2026/Conference/Submission8385/Reviewer_WYa8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8385/Reviewer_WYa8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933624737, "cdate": 1761933624737, "tmdate": 1762920291063, "mdate": 1762920291063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of cross-tokenization, which is of paramount importance in the context of LLM distillation. The issue arises from the vocabulary misalignment problem, often caused by different tokenizers used in various language models. The paper introduces a new approach to tackling this issue, known as cross-tokenizer scoring or cross-tokenizer conversion, which builds strongly on the structure of the BPE algorithm widely used in current tokenizers. \nThe paper introduces the following contributions :\n+ An analysis of the sequential structure of the byte-pair encoding and the introduction of the notion of relative alphabets.\n+ Cross-tokenizer scoring algorithms\n+ Experimental validation on two tasks: cross-tokenizer distillation and vocabulary trimming."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**originality**\n+ The idea of cross-tokenizer conversion for LLM Knowledge distillation is original. \n+ The notion of relative alphabets.\n\n\n **significance**\n+ The questions being asked are important since LLM distillation, assuming different vocabulary, is a very common practical use case."}, "weaknesses": {"value": "+ The proposed approach is primarily limited to BPE tokenization algorithms. While it is true that many current tokenizers are built on BPE, it is not always the case. \n+ The clarity of the paper is clearly a big weakness of the paper. In particular, the proposed formalization is difficult to follow due to a lack of motivation or descriptions of the intuitions behind the concepts. For instance, section 4.2 is mainly a succession of definitions. In addition, some of the proposed definitions are generalizations of existing ones, such as relative cover encoding. Why is the notion of cover encoding important? A detailed positioning of the proposed approach with respect to the work of [Phan et al,. 2025] is also missing. There are certainly some very good ideas in this paper, but the format makes it very difficult to read and understand. It would have been interesting, for example, to consider a diagram highlighting the general framework of the proposed approach, particularly the concept of relative alphabets on which everything is based.\n+ Experimental validation also does not allow us to highlight this notion of relative vocabularies and how it impacts the targeted tasks: LLM KD and vocabulary trimming."}, "questions": {"value": "+ What about cross-tokenization outside the BPE algorithm? \n+ Would it be possible to describe more explicitly what contributions the approach makes in relation to the work of [Phan et al., 2025.]? \n+ In term of practical applications, what brings the concept of relative vocabularies ? \n+ How can it be used more concretely in a KD context, for example? \n+ Why was the standard experimental protocol of [Minixhofer et al. 2025] not followed in the experimental validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JEr1bGhJto", "forum": "hD69qj15Os", "replyto": "hD69qj15Os", "signatures": ["ICLR.cc/2026/Conference/Submission8385/Reviewer_oe8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8385/Reviewer_oe8u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100614560, "cdate": 1762100614560, "tmdate": 1762920288955, "mdate": 1762920288955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}