{"id": "pQQuC1nIQq", "number": 9555, "cdate": 1758127372278, "mdate": 1763664916996, "content": {"title": "Understanding and improving Shampoo and SOAP via Kullback-Leibler Minimization", "abstract": "Shampoo and its efficient, Adam-stabilized variant SOAP, employ structured second-moment estimation and have received growing attention for their effectiveness. \nIn practice, Shampoo requires step-size grafting with Adam to achieve competitive performance. SOAP mitigates this by applying Adam in Shampoo's eigenbasis and further reducing per-iteration runtime. \nHowever, reliance on Adam introduces additional memory overhead in both methods.\nPrior theoretical interpretations have primarily examined their estimation schemes using the Frobenius norm. Motivated by the natural correspondence between the second moment and a covariance matrix, we reinterpret the estimation procedures in Shampoo and SOAP as instances of covariance estimation through the lens of Kullback–Leibler (KL) divergence minimization. This perspective reveals a previously overlooked theoretical limitation and motivates principled improvements to their design.\n Building on the KL perspective, we propose practical estimation schemes---KL-Shampoo and KL-SOAP---that \n match or exceed the performance of Shampoo and SOAP for pre-training a range of neural network models while maintaining SOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to achieve superior performance, thereby avoiding the associated memory overhead. Surprisingly, KL-Shampoo consistently outperforms the other methods in our experiments.", "tldr": "A new Kullback–Leibler perspective to interpret and improve Shampoo and SOAP", "keywords": ["Shampoo", "SOAP", "covariance estimation", "Kullback–Leibler divergence", "Gaussian", "optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/458fb898ffd096557a50d74ec7d515c3300a9ab2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes KL-Shampoo and KL-SOAP, which modify the Shampoo and Soap algorithms to try to find the best approximation to the gradient second moment in the KL norm rather than the Frobenius norm. In practice this yields an update rule where current estimates of the second moment of the RHS covariance are used to update the LHS covariance and visa-versa. The paper also proposes to use QR updates, but combined with frequent estimates of eigenvalues via EMA. Empirical results show gains over the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a new perspective on pre-conditioning for LLM optimization that yields principled algorithms.\n2. The presentation of related work and how the KL methods fit in is very nice.\n3. The method seems to work better than baselines in the experiments. And hyper parameter tuning seems relatively fair across baselines and runtime/memory is considered."}, "weaknesses": {"value": "1. It’s a lot to cover, but the presentation of the algorithm itself could be improved. It is too nonlinear with options here and there. Some clear, linear algorithm boxes for each separate algo in the appendix could provide clarity. \n2. I understand there are computational constraints, but the experiments are small-scale, so it is not clear whether the gains with scale. Perhaps even more than scaling up model size (which can get expensive), I would be interested to see substantially longer runs at the same size. The current experiments are effectively *very* early in training (less than chinchilla and far below the >>100 token-to-param rations that are more standard now), so it is not clear whether the gains matter later on as well."}, "questions": {"value": "1. In figure 3, why do the delta definitions have two lines each?\n2. In figure 3 bottom right, where is the modification that defines “augmented”?\n3. Are the output layers and/or layernorms treated differently in the empirical implementation (as is often the case with similar methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OSJS5XTzm0", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_Mv1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_Mv1d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420332702, "cdate": 1761420332702, "tmdate": 1762921113881, "mdate": 1762921113881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors recast diagonal estimation as covariance estimation via KL minimization: they treat the true second moment $E[gg^\\top]$ as a Gaussian covariance and seek the vector $d$ that minimizes the KL divergence between $\\mathcal{N}(0, E[gg^\\top])$ and $\\mathcal{N}(0, Q,\\mathrm{Diag}(d),Q^\\top)$. They prove that the KL-optimal $d$ equals the expected squared gradient in the eigenbasis, which provides a clean justification for SOAP’s “RMSProp in an eigenbasis” update as the solution to a KL problem. From this view, the classic Kronecker estimators in Shampoo and SOAP are suboptimal for the KL objective, which motivates revised estimators. They propose KL-SOAP, which keeps SOAP’s basic pattern but replaces Shampoo’s factor estimation with KL-Shampoo’s estimation to obtain a better eigenbasis $Q$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a well motivated problem and offers a clear explanation. The literature typically justifies SOAP with Frobenius norm approximations [1]. The authors replace that with a covariance estimation view that minimizes KL divergence between zero mean Gaussians. Under this view, the usual Kronecker estimators in Shampoo and SOAP do not solve the right objective when both factors are learned jointly, which motivates a two sided estimator. This reveals a limitation in the standard Kronecker update and leads to principled fixes, presented as KL Shampoo and KL SOAP. The result is better performance at roughly SOAP level iteration cost.\n\n\n[1] Morwani, Depen, et al. \"A New Perspective on Shampoo's Preconditioner.\" arXiv preprint arXiv:2406.17748 (2024)."}, "weaknesses": {"value": "The paper argues that the Frobenius-norm view misses the right geometry and proposes a KL objective instead. Since methods like SOAP ultimately aim to approximate a preconditioner, it is natural to assess estimation quality by a distance such as the Frobenius norm, and it should not be surprising that different distance functions produce different optimal approximations. I appreciate the point that reconstruction error is not the final goal, yet the justification still feels incomplete. The paper does not fully persuade me that the KL objective, rather than Frobenius or another principled choice, is the uniquely appropriate metric for evaluating structured preconditioners in this setting.\n\n\n\nThe paper presents KL-SOAP as a principled improvement to SOAP, but the mechanical change is small. In practice the algorithmic delta is limited to swapping SOAP’s Shampoo-based eigenbasis for a KL-Shampoo eigenbasis and adding an EMA rule for factor eigenvalues under basis staleness. The step form, the augmented diagonal, and the memory profile remain essentially the same. The narrative should reflect that KL-SOAP is best understood as SOAP with a stronger basis construction and more careful eigenvalue tracking, not as a distinct optimizer.\n\nThe experiments never report approximation error under the proposed KL objective."}, "questions": {"value": "Please address my concerns above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RKCewdBGJ9", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_TCse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_TCse"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768734115, "cdate": 1761768734115, "tmdate": 1762921113353, "mdate": 1762921113353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a KL-divergence perspective of looking at the updates provided by Shampoo and SOAP. It provides a new update rule for the kronecker factored preconditioner based on the KL divergence perspective and provides empirical results for the same. Empirically, KL-Shampoo and KL-SOAP outperform Shampoo and SOAP on various language modeling benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper provides a novel perspective of viewing the Kronecker factorization updates based on KL-divergence.\n\n2. It provides a new update rule for the kronecker factor updates based on this perspective, and also provides a computationally efficient way for implementing it.\n\n3. The empirical results of the method are pretty strong, outperforming recent second order methods such as Shampoo and SOAP."}, "weaknesses": {"value": "1. The paper is missing comparisons to Muon, another popular second order optimizer proposed recently."}, "questions": {"value": "1. Can the authors add comparison to Muon as well - for both runtime and number of iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "akpFpy1r19", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_1qBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_1qBq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883858951, "cdate": 1761883858951, "tmdate": 1762921112989, "mdate": 1762921112989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reinterprets the preconditioner estimation in Shampoo and SOAP as a covariance estimation problem solved via Kullback-Leibler (KL) divergence minimization.  This perspective may reveal a previously overlooked limitation in the one-sided estimation approach of Shampoo. Building on this insight, the authors propose new estimation rules, leading to KL-Shampoo and KL-SOAP. Through  experiments on multiple language models, the authors demonstrate that KL-Shampoo and KL-SOAP outperform Shampoo and SOAP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The KL divergence view is a well-motivated approach. The connection to covariance estimation and the justification via proximal-gradient steps are interesting. \n\n- The proposed methods are not just theoretical constructs; they are practical algorithms that deliver comparable performance over Shampoo and SOAP.\n\n- The paper is generally well-written."}, "weaknesses": {"value": "- The authors only demonstrate that KL-Shampoo and KL-SOAP outperform Shampoo and SOAP. They do not compare with other state-of-the-art large scale algorithms.\n\n- The authors do not provide any theoretical convergence guarantees."}, "questions": {"value": "Page 3, Figure 1 Caption: \"pre-iteration runtime\",  \"per-iteration runtime\"?\n\n\nAppendix G: It might be helpful to briefly summarize the conclusions of Fig. 7 and Fig. 8 in the main text, as they provide strong support for the choice of KL divergence over VN divergence.\n\nCould the authors provide theoretical guarantees, showing the advantage of the proposed algorithms?\n\nCould the authors provide more numerical results to compare with other emerging large scale algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MOGnCYU3w1", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_ENoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_ENoC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926026734, "cdate": 1761926026734, "tmdate": 1762921112632, "mdate": 1762921112632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "> Comparison with other state-of-the-art large-scale algorithms (Reviewer ENoC)\n\n> Missing comparisons to Muon (Reviewer 1qBq)\n\n> The experiments are small-scale, so it is not clear whether the gains with scale. (Reviewer Mv1d)\n\nWe evaluate KL-Shampoo, SOAP, and Muon [1] on a new, larger, model (Llama3-450M) using 100 runs per method.  This allows us to reduce the computation cost while properly and fairly tuning Muon.  We can only evaluate Muon on this model (and not the other models already in the paper) during the rebuttal phase due to limited compute resources. This experiment (450M) is larger than the largest experiment (227M) currently in the submission.\n\nThe plots about this experiment can be found at Figure 11, Appendix H.\nTL;DR: KL-Shampoo is better than Muon [1] per-iteration, but not per-time (10% overhead when performing QR in FP32 every 10 iterations) for pre-training Llama3-450M.\n\nAlthough our results show worse performance vs wall-clock time against Muon, we note that it is possible to improve KL-Shampoo's runtime, for example by performing the QR decompositions less frequently and using mixed-precision QR. This is because the QR decomposition is the main computational bottleneck. \nWe do not tune the decomposition frequency to optimize KL-Shampoo's runtime.\nThere are additional algorithmic modifications and engineering efforts that can further improve KL-Shampoo's runtime. We leave this for future work.\n\nFinally, we would like to highlight that the focus of this paper is on understanding and improving Shampoo and SOAP—optimizers that naturally support tensor-valued weights. Thus, we did not initially include Muon, which is a specialized method for matrix-valued weights (i.e., 2D tensors), in the paper. Shampoo and SOAP are more general than Muon, and correspondingly so are KL-Shampoo and KL-SOAP.\n\nReference:\n\n[1] Muon with an adjusted step-size; Liu et al, \"Muon is scalable for LLM training\", arXiv preprint arXiv:2502.16982 (2025)."}}, "id": "Mi7SbP1R6O", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662544597, "cdate": 1763662544597, "tmdate": 1763674269219, "mdate": 1763674269219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "> Comparison with other state-of-the-art large-scale algorithms (Reviewer ENoC)\n\n> Missing comparisons to Muon (Reviewer 1qBq)\n\n> The experiments are small-scale, so it is not clear whether the gains with scale. (Reviewer Mv1d)\n\nWe evaluate KL-Shampoo, SOAP, and Muon [1] on a new, larger, model (Llama3-450M) using 100 runs per method.  This allows us to reduce the computation cost while properly and fairly tuning Muon.  We can only evaluate Muon on this model (and not the other models already in the paper) using 100 runs (rather than 150 runs) during the rebuttal phase due to limited compute resources. This experiment (450M) is larger than the largest experiment (227M) currently in the submission.\n\nThe plots about this experiment can be found at Figure 11, Appendix H.\nTL;DR: KL-Shampoo is better than Muon [1] per-iteration, but not per-time (10% overhead when performing QR in FP32 every 10 iterations) for pre-training Llama3-450M.\n\nAlthough our results show worse performance vs wall-clock time against Muon, we note that it is possible to improve KL-Shampoo's runtime, for example by performing the QR decompositions less frequently and using mixed-precision QR. This is because the QR decomposition is the main computational bottleneck. \nWe do not tune the decomposition frequency to optimize KL-Shampoo's runtime.\nThere are additional algorithmic modifications and engineering efforts that can further improve KL-Shampoo's runtime. We leave this for future work.\n\nFinally, we would like to highlight that the focus of this paper is on understanding and improving Shampoo and SOAP—optimizers that naturally support tensor-valued weights. Thus, we did not initially include Muon, which is a specialized method for matrix-valued weights (i.e., 2D tensors), in the paper. Shampoo and SOAP are more general than Muon, and correspondingly so are KL-Shampoo and KL-SOAP.\n\nReference:\n\n[1] Muon with an adjusted step-size; Liu et al, \"Muon is scalable for LLM training\", arXiv preprint arXiv:2502.16982 (2025)."}}, "id": "Mi7SbP1R6O", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662544597, "cdate": 1763662544597, "tmdate": 1763700673041, "mdate": 1763700673041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New changes in the latest version"}, "comment": {"value": "* Main text\n    * improved the presentation of the algorithm box in Figure 3  (Reviewer Mv1d)\n    * updated Figure 5 to put KL-Shampoo, VN-Shampoo (matrix version of Adafactor), and F-Shampoo (two-sided Shampoo based on the Frobenius norm) side by side and demonstrate the advantages of KL-Shampoo (Reviewers ENoC and TCse)\n    * included a short discussion in Section 6 on the advantages of KL-divergence over VN-divergence  (Reviewer ENoC)\n* The appendix\n    * added a new Section (Appendix G) and  additional experiments (Figure 9, Appendix G) about Shampoo based on the Frobenius norm (i.e., F-Shampoo) for completeness (Reviewer TCse)\n    * included an individual algorithm box for KL-Shampoo (Figure 13, Appendix H) in the appendix (Reviewer Mv1d)\n    * added a new experiment (Figure 11, Appendix H) on Llama3-450M to include Muon as a strong baseline (Reviewers ENoC, 1qBq, Mv1d)\n    * added a new experiment (Figure 12, Appendix H) on Llama-134M using 20B tokens to support the advantages of KL-Shampoo when using larger training steps/tokens (Reviewer Mv1d)\n    * fixed some typos in Appendix C"}}, "id": "jesgCdG2Js", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662631013, "cdate": 1763662631013, "tmdate": 1763671068558, "mdate": 1763671068558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New changes in the latest version"}, "comment": {"value": "* Main text\n    * improved the presentation of the algorithm box in Figure 3  (Reviewer Mv1d)\n    * updated Figure 5 to put KL-Shampoo, original Shampoo (without grafting), VN-Shampoo (matrix version of Adafactor), and F-Shampoo (two-sided Shampoo based on the Frobenius norm) side by side and demonstrate the advantages of KL-Shampoo (Reviewers ENoC and TCse)\n    * included a short discussion in Section 6 on the advantages of KL-divergence over VN-divergence  (Reviewer ENoC)\n* The appendix\n    * added a new Section (Appendix G) and  additional experiments (Figure 9, Appendix G) about Shampoo based on the Frobenius norm (i.e., F-Shampoo) for completeness (Reviewer TCse)\n    * included an individual algorithm box for KL-Shampoo (Figure 13, Appendix H) in the appendix (Reviewer Mv1d)\n    * added a new experiment (Figure 11, Appendix H) on Llama3-450M to include Muon as a strong baseline (Reviewers ENoC, 1qBq, Mv1d)\n    * added a new experiment (Figure 12, Appendix H) on Llama-134M using 20B tokens to support the advantages of KL-Shampoo when using larger training steps/tokens (Reviewer Mv1d)\n    * fixed some typos in Appendix C"}}, "id": "jesgCdG2Js", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662631013, "cdate": 1763662631013, "tmdate": 1763729516521, "mdate": 1763729516521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New changes in the latest version (November 21)"}, "comment": {"value": "* Main text\n    * improved the presentation of the algorithm box in Figure 3  (Reviewer Mv1d)\n    * updated Figure 5 to put KL-Shampoo, original Shampoo (without grafting), VN-Shampoo (matrix version of Adafactor), and F-Shampoo (two-sided Shampoo based on the Frobenius norm) side by side and demonstrate the advantages of KL-Shampoo (Reviewers ENoC and TCse)\n    * included a short discussion in Section 6 on the advantages of using the KL-divergence over the VN-divergence  (Reviewer ENoC)\n* The appendix\n    * added a new Section (Appendix G) and  additional experiments (Figure 9, Appendix G) about Shampoo based on the Frobenius norm (i.e., F-Shampoo) for completeness (Reviewer TCse)\n    * included an individual algorithm box for KL-Shampoo (Figure 13, Appendix H) in the appendix (Reviewer Mv1d)\n    * added a new experiment (Figure 11, Appendix H) on Llama3-450M to include Muon as a strong baseline (Reviewers ENoC, 1qBq, Mv1d)\n    * added a new experiment (Figure 12, Appendix H) on Llama-134M using 20B tokens to support the advantages of KL-Shampoo when using larger training steps/tokens (Reviewer Mv1d)\n    * fixed some typos in Appendix C"}}, "id": "jesgCdG2Js", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662631013, "cdate": 1763662631013, "tmdate": 1763767222391, "mdate": 1763767222391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}