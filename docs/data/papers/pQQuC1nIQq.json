{"id": "pQQuC1nIQq", "number": 9555, "cdate": 1758127372278, "mdate": 1759897712495, "content": {"title": "Understanding and improving Shampoo and SOAP via Kullback-Leibler Minimization", "abstract": "Shampoo and its efficient, Adam-stabilized variant SOAP, employ structured second-moment estimation and have received growing attention for their effectiveness. \nIn practice, Shampoo requires step-size grafting with Adam to achieve competitive performance. SOAP mitigates this by applying Adam in Shampoo's eigenbasis and further reducing per-iteration runtime. \nHowever, reliance on Adam introduces additional memory overhead in both methods.\nPrior theoretical interpretations have primarily examined their estimation schemes using the Frobenius norm. Motivated by the natural correspondence between the second moment and a covariance matrix, we reinterpret the estimation procedures in Shampoo and SOAP as instances of covariance estimation through the lens of Kullback–Leibler (KL) divergence minimization. This perspective reveals a previously overlooked theoretical limitation and motivates principled improvements to their design.\n Building on the KL perspective, we propose practical estimation schemes---KL-Shampoo and KL-SOAP---that \n match or exceed the performance of Shampoo and SOAP for pre-training a range of neural network models while maintaining SOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to achieve superior performance, thereby avoiding the associated memory overhead. Surprisingly, KL-Shampoo consistently outperforms the other methods in our experiments.", "tldr": "A new Kullback–Leibler perspective to interpret and improve Shampoo and SOAP", "keywords": ["Shampoo", "SOAP", "covariance estimation", "Kullback–Leibler divergence", "Gaussian", "optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79aa00fbf0784ab248de959aa546b89c1714ed35.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes KL-Shampoo and KL-SOAP, which modify the Shampoo and Soap algorithms to try to find the best approximation to the gradient second moment in the KL norm rather than the Frobenius norm. In practice this yields an update rule where current estimates of the second moment of the RHS covariance are used to update the LHS covariance and visa-versa. The paper also proposes to use QR updates, but combined with frequent estimates of eigenvalues via EMA. Empirical results show gains over the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a new perspective on pre-conditioning for LLM optimization that yields principled algorithms.\n2. The presentation of related work and how the KL methods fit in is very nice.\n3. The method seems to work better than baselines in the experiments. And hyper parameter tuning seems relatively fair across baselines and runtime/memory is considered."}, "weaknesses": {"value": "1. It’s a lot to cover, but the presentation of the algorithm itself could be improved. It is too nonlinear with options here and there. Some clear, linear algorithm boxes for each separate algo in the appendix could provide clarity. \n2. I understand there are computational constraints, but the experiments are small-scale, so it is not clear whether the gains with scale. Perhaps even more than scaling up model size (which can get expensive), I would be interested to see substantially longer runs at the same size. The current experiments are effectively *very* early in training (less than chinchilla and far below the >>100 token-to-param rations that are more standard now), so it is not clear whether the gains matter later on as well."}, "questions": {"value": "1. In figure 3, why do the delta definitions have two lines each?\n2. In figure 3 bottom right, where is the modification that defines “augmented”?\n3. Are the output layers and/or layernorms treated differently in the empirical implementation (as is often the case with similar methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OSJS5XTzm0", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_Mv1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_Mv1d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420332702, "cdate": 1761420332702, "tmdate": 1762921113881, "mdate": 1762921113881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors recast diagonal estimation as covariance estimation via KL minimization: they treat the true second moment $E[gg^\\top]$ as a Gaussian covariance and seek the vector $d$ that minimizes the KL divergence between $\\mathcal{N}(0, E[gg^\\top])$ and $\\mathcal{N}(0, Q,\\mathrm{Diag}(d),Q^\\top)$. They prove that the KL-optimal $d$ equals the expected squared gradient in the eigenbasis, which provides a clean justification for SOAP’s “RMSProp in an eigenbasis” update as the solution to a KL problem. From this view, the classic Kronecker estimators in Shampoo and SOAP are suboptimal for the KL objective, which motivates revised estimators. They propose KL-SOAP, which keeps SOAP’s basic pattern but replaces Shampoo’s factor estimation with KL-Shampoo’s estimation to obtain a better eigenbasis $Q$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a well motivated problem and offers a clear explanation. The literature typically justifies SOAP with Frobenius norm approximations [1]. The authors replace that with a covariance estimation view that minimizes KL divergence between zero mean Gaussians. Under this view, the usual Kronecker estimators in Shampoo and SOAP do not solve the right objective when both factors are learned jointly, which motivates a two sided estimator. This reveals a limitation in the standard Kronecker update and leads to principled fixes, presented as KL Shampoo and KL SOAP. The result is better performance at roughly SOAP level iteration cost.\n\n\n[1] Morwani, Depen, et al. \"A New Perspective on Shampoo's Preconditioner.\" arXiv preprint arXiv:2406.17748 (2024)."}, "weaknesses": {"value": "The paper argues that the Frobenius-norm view misses the right geometry and proposes a KL objective instead. Since methods like SOAP ultimately aim to approximate a preconditioner, it is natural to assess estimation quality by a distance such as the Frobenius norm, and it should not be surprising that different distance functions produce different optimal approximations. I appreciate the point that reconstruction error is not the final goal, yet the justification still feels incomplete. The paper does not fully persuade me that the KL objective, rather than Frobenius or another principled choice, is the uniquely appropriate metric for evaluating structured preconditioners in this setting.\n\n\n\nThe paper presents KL-SOAP as a principled improvement to SOAP, but the mechanical change is small. In practice the algorithmic delta is limited to swapping SOAP’s Shampoo-based eigenbasis for a KL-Shampoo eigenbasis and adding an EMA rule for factor eigenvalues under basis staleness. The step form, the augmented diagonal, and the memory profile remain essentially the same. The narrative should reflect that KL-SOAP is best understood as SOAP with a stronger basis construction and more careful eigenvalue tracking, not as a distinct optimizer.\n\nThe experiments never report approximation error under the proposed KL objective."}, "questions": {"value": "Please address my concerns above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RKCewdBGJ9", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_TCse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_TCse"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768734115, "cdate": 1761768734115, "tmdate": 1762921113353, "mdate": 1762921113353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a KL-divergence perspective of looking at the updates provided by Shampoo and SOAP. It provides a new update rule for the kronecker factored preconditioner based on the KL divergence perspective and provides empirical results for the same. Empirically, KL-Shampoo and KL-SOAP outperform Shampoo and SOAP on various language modeling benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper provides a novel perspective of viewing the Kronecker factorization updates based on KL-divergence.\n\n2. It provides a new update rule for the kronecker factor updates based on this perspective, and also provides a computationally efficient way for implementing it.\n\n3. The empirical results of the method are pretty strong, outperforming recent second order methods such as Shampoo and SOAP."}, "weaknesses": {"value": "1. The paper is missing comparisons to Muon, another popular second order optimizer proposed recently."}, "questions": {"value": "1. Can the authors add comparison to Muon as well - for both runtime and number of iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "akpFpy1r19", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_1qBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_1qBq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883858951, "cdate": 1761883858951, "tmdate": 1762921112989, "mdate": 1762921112989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reinterprets the preconditioner estimation in Shampoo and SOAP as a covariance estimation problem solved via Kullback-Leibler (KL) divergence minimization.  This perspective may reveal a previously overlooked limitation in the one-sided estimation approach of Shampoo. Building on this insight, the authors propose new estimation rules, leading to KL-Shampoo and KL-SOAP. Through  experiments on multiple language models, the authors demonstrate that KL-Shampoo and KL-SOAP outperform Shampoo and SOAP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The KL divergence view is a well-motivated approach. The connection to covariance estimation and the justification via proximal-gradient steps are interesting. \n\n- The proposed methods are not just theoretical constructs; they are practical algorithms that deliver comparable performance over Shampoo and SOAP.\n\n- The paper is generally well-written."}, "weaknesses": {"value": "- The authors only demonstrate that KL-Shampoo and KL-SOAP outperform Shampoo and SOAP. They do not compare with other state-of-the-art large scale algorithms.\n\n- The authors do not provide any theoretical convergence guarantees."}, "questions": {"value": "Page 3, Figure 1 Caption: \"pre-iteration runtime\",  \"per-iteration runtime\"?\n\n\nAppendix G: It might be helpful to briefly summarize the conclusions of Fig. 7 and Fig. 8 in the main text, as they provide strong support for the choice of KL divergence over VN divergence.\n\nCould the authors provide theoretical guarantees, showing the advantage of the proposed algorithms?\n\nCould the authors provide more numerical results to compare with other emerging large scale algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MOGnCYU3w1", "forum": "pQQuC1nIQq", "replyto": "pQQuC1nIQq", "signatures": ["ICLR.cc/2026/Conference/Submission9555/Reviewer_ENoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9555/Reviewer_ENoC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926026734, "cdate": 1761926026734, "tmdate": 1762921112632, "mdate": 1762921112632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}