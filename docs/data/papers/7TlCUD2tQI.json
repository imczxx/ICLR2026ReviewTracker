{"id": "7TlCUD2tQI", "number": 22110, "cdate": 1758326168533, "mdate": 1763730020432, "content": {"title": "Augmenting Industrial Maintenance with LLMs: A Benchmark, Analysis, and Generalization Study", "abstract": "Monitoring the life cycle of complex industrial systems often relies on expertly curated temporal conditions derived from sensor data, a process that requires significant time investment and deep domain expertise. We explore the potential of utilizing Large Language Models (LLMs) to generate context-aware and accurate recommendations for maintenance based on their ability to reason and generalize on temporal sensor conditions. To this end, we formulate a novel pipeline that systematically converts human-authored symbolic conditions into a multiple-choice question answer (MCQA) dataset. We apply our pipeline by creating DiagnosticIQ, a 6,000+ MCQA dataset covering 16 different types of physical assets that represent real-world maintenance use cases. We assess 15 state-of-the-art large language models (LLMs) with this dataset and create a leaderboard for the maintenance action recommendation task. Furthermore, we evaluate and demonstrate the practical utility of DiagnosticIQ in two key aspects. First, as a knowledge base to enhance maintenance action recommendations, and secondly, as a fine-tuning resource to fine-tune a specialized LLM that generalizes across previously unseen assets to facilitate the rule creation process.", "tldr": "Augmenting Industrial Maintenance with LLMs: A Benchmark, Analysis, and Generalization Study", "keywords": ["Benchmarking", "Representation Learning", "Large Language Models", "Industry 4.0"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38a6a4280d89f272f2d937815993e78b91786d56.pdf", "supplementary_material": "/attachment/6a539ec7eb57801f1b3c113581a5370f53f92628.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces DiagnosticIQ, a benchmark for evaluating whether LLMs can recommend maintenance actions from symbolic, time-persistent sensor conditions used in industrial monitoring. The dataset contains 6,690 MCQs drawn from 120 rules across about 16 asset types, with several variants: DiagnosticIQPro, DiagnosticIQPert, DiagnosticIQRationale, and DiagnosticIQVerbose. The authors evaluate 15 LLMs in a zero-shot setting and report a leaderboard. Macro accuracy is highest for Claude-3-7-Sonnet, and most models drop sharply on the Pro split with larger answer choices. They also present a small human study assessing model rationales, a cross-asset fine-tuning study with SFT and GRPO, and detailed analyses by asset and by question type."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper targets a real gap: connecting anomaly rules to actionable maintenance guidance at scale.\n- The rules-to-MCQ formulation is well motivated by real maintenance workflows.\n- The benchmark construction pipeline is transparent and reproducible: condition trees are converted to disjunctive normal form, and question types are systematically constructed.\n- It covers diverse evaluation axes, including robustness to prompt perturbations, per-asset performance, question-type differences, and cross-asset transfer with SFT and GRPO."}, "weaknesses": {"value": "- Many models on the leaderboard seem outdated and inconsistent. There are more recent closed-source models for Gemini and OpenAI's reasoning models than the ones listed on the benchmark. I suggest updating the leaderboard with more recent model versions. Also, Qwen2.5 is tested for zero-shot but Qwen3-8B is tested in the generalization study, which look like inconsistent choices of models.\n- The generalization section uses three 8B models and shows inconsistent gains for GRPO versus SFT across splits. If the authors plan to keep these results, at least some discussion on why this happens would help readers decide what to use in the future."}, "questions": {"value": "- It would be great if the authors could add the meaning of * in the caption of Table 1 so that readers do not need to look for its meaning.\n- The embedding model (all-mpnet-base-v2) is used for creating incorrect options. Is it plausible to use that embedding model? Why not use LLMs (which would have more industrial knowledge) for constructing the negative options?\n- The claim \"For many enterprise customers, smaller language models will be key, as they provide a practical way to embed domain-specific knowledge directly into the model\" seems partially correct as a poor model with smaller parameters would not be preferable to a large quantized model with better capability. Could you elaborate more on this? Similarly, transfer learning is indeed important, but I do not see much of an advantage in fine-tuning over using larger, general models without fine-tuning if anomaly detection is a very important use case for LLMs.\n- \"Transfer learning between different shows\" seems like a typo."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gBcXBZr5Pt", "forum": "7TlCUD2tQI", "replyto": "7TlCUD2tQI", "signatures": ["ICLR.cc/2026/Conference/Submission22110/Reviewer_Lrjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22110/Reviewer_Lrjy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931728059, "cdate": 1761931728059, "tmdate": 1762942068597, "mdate": 1762942068597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DiagnosticIQ, a benchmark for evaluating large language models in industrial maintenance. It converts symbolic diagnostic rules into multiple-choice QA tasks to test reasoning, cross-equipment transfer, and maintenance recommendation. The work highlights the gap between current LLM capabilities and real-world industrial reasoning needs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an emerging but underexplored area: benchmarking large language models for industrial maintenance tasks. This direction is highly relevant to practical applications in Industry 4.0 and intelligent manufacturing.\n\n- Writing is good and easy to follow and understand.\n\n- The authors propose a well-structured pipeline that converts symbolic diagnostic rules into multiple-choice QA tasks (MCQA). This symbolic-to-language transformation is technically neat and represents a creative way to evaluate LLMs on reasoning grounded in real industrial knowledge."}, "weaknesses": {"value": "- The manuscript does not cite recent related benchmarks such as CAMB (“A Comprehensive Industrial LLM Benchmark on Civil Aviation Maintenance”) and Wind‑Turbine Maintenance Logs Benchmark (“A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs”). A clearer comparison with these works, including differences in task types, domain scope, modality coverage, and benchmark construction process, is needed to better highlight the novelty of the current benchmark.\n\n- Although the authors aim for broad industrial coverage, the dataset is restricted to a limited set of device types. The transferability to other equipment categories or industrial domains is not demonstrated.\n\n- While the authors provide code and claim reproducibility, my attempt to run the provided implementation faced issues (e.g., missing configuration files, unclear dependencies). The benchmark currently lacks full reproducibility, which limits its value as a standardized community resource."}, "questions": {"value": "See in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0CRrnBm7FV", "forum": "7TlCUD2tQI", "replyto": "7TlCUD2tQI", "signatures": ["ICLR.cc/2026/Conference/Submission22110/Reviewer_GLHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22110/Reviewer_GLHJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961111387, "cdate": 1761961111387, "tmdate": 1762942068124, "mdate": 1762942068124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a deterministic rule-to-question pipeline that transforms expert-authored industrial maintenance rules into multiple-choice question–answer (MCQA) format.\nApplied to 120 real maintenance rules accumulated over seven years, the pipeline yields DiagnosticIQ, a benchmark of 6.7k MCQA instances (plus several variants) spanning 16 industrial asset types.\nFifteen state-of-the-art LLMs are evaluated in zero-shot mode, producing the first leaderboard for “maintenance-action recommendation.”"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Rules, conditions, and actions originate from seven years of subject-matter-expert (SME) curation, giving the dataset strong industrial validity and relevance. \n\n2. The rule-to-MCQA conversion algorithm (§3, Alg. 1) is clearly specified, with formal DNF conversion, rule rewriting (RRSim), and interpretable α/β parameters controlling diversification.\n\n3. Visuals and tables are well-organized: e.g., Table 1 highlights the steep accuracy drop from DiagnosticIQ to its harder +Pro variant, while Fig. 3 clarifies asset imbalance motivating macro-accuracy metrics."}, "weaknesses": {"value": "1. About 58 % of items concern air-handling units (AHUs) (Fig. 3), yet overall accuracy (Table 1) remains the primary metric. While macro-accuracy is reported, several analyses aggregate raw accuracy, potentially overstating performance on dominant asset classes.\n\n2. The macro-accuracy equation (p. 6) omits the denominator $|D_a|$ under the outer summation, causing a dimensional mismatch.\nIn §3.2.3, the claim that larger α/β “increase question count but reduce diversity” lacks quantitative backing.\n\n3. Several recent benchmarks with strong thematic overlap are omitted: MME-Industry (Yi et al., 2025) – cross-industry multimodal evaluation. PHM-Bench (Yang et al., 2025) – maintenance and health-management tasks.\n\n4. The rules originate from a commercial monitoring system, yet the paper omits discussion of confidentiality, potential misuse, or licensing constraints, which are critical for public release."}, "questions": {"value": "1. How were action labels deduplicated into the 193-item observation set? Was synonym merging performed manually or algorithmically?\n\n2. What steps are in place to ensure IP compliance and anonymization when releasing SME-derived rules?\n\n3. How do we verify that the data truly reflects realistic maintenance reasoning, rather than just faithfully encoding the rule templates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CRw4sDXKPm", "forum": "7TlCUD2tQI", "replyto": "7TlCUD2tQI", "signatures": ["ICLR.cc/2026/Conference/Submission22110/Reviewer_xq1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22110/Reviewer_xq1p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987024709, "cdate": 1761987024709, "tmdate": 1762942067432, "mdate": 1762942067432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiagnosticIQ, a large-scale benchmark and dataset for evaluating LLMs in industrial maintenance action recommendation. It proposes a rule-to-MCQA pipeline that systematically transforms symbolic, expert-authored maintenance rules into multiple-choice QA datasets, encompassing over 6,600 validated questions across 16 asset types. The authors benchmark 15 LLMs and analyze reasoning, generalization, and robustness, releasing variants such as DiagnosticIQPro (10-option), Pert (perturbed), Verbose (NL conditions), and Rationale (explanation-based). The work also includes fine-tuning (SFT/GRPO) and deployment experiments (MAReE engine), showing that LLMs can partially generalize and reason about sensor-based maintenance tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents the first standardized benchmark for LLMs in industrial maintenance—a domain rarely addressed in LLM evaluation. The deterministic symbolic-to-MCQA pipeline is well-motivated and rigorously described, ensuring reproducibility and logical consistency. The authors benchmark 15 state-of-the-art LLMs with clear comparisons on reasoning, generalization, and robustness, producing actionable insights (e.g., domain sensitivity across assets, compositional reasoning gap). The integration of the dataset into a real-world recommendation engine (MAReE) is commendable, bridging benchmark analysis with deployable use cases. Dataset variants are thoughtfully designed to probe distinct reasoning dimensions (formatting, rationale, perturbation), increasing diagnostic value beyond simple accuracy."}, "weaknesses": {"value": "Despite the industrial framing, the dataset is dominated by AHU-related rules (≈58%), with only 10+ asset types, limiting claims of cross-domain generalization. The analysis focuses mostly on macro accuracy, with little discussion on statistical significance or variance across model families and seeds. No comparison against non-LLM baselines (e.g., rule-based or symbolic expert systems) to contextualize the LLM performance gains. The symbolic-to-natural-language conversion step and question templates are discussed but not quantitatively ablated (e.g., contribution of DNF conversion vs. text formatting).  While informative, the leaderboard lacks qualitative error analysis or failure categorization, making it unclear why models fail (e.g., semantic confusion vs. numerical reasoning)."}, "questions": {"value": "How consistent is the rule-to-MCQA generation pipeline across asset types with fundamentally different sensor modalities? \n\nCould the authors provide examples of incorrect reasoning patterns observed in LLMs (e.g., conflating conditions vs. missing causal links)?\n\nHow was expert validation performed—was inter-annotator agreement measured among SMEs?\n\nFor the fine-tuning experiments, how is overlap between training and test rules prevented beyond asset-based stratification?\n\nDid the authors consider incorporating numerical reasoning evaluation (e.g., comparing thresholds or temporal trends) explicitly in the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5QXmkr2i5J", "forum": "7TlCUD2tQI", "replyto": "7TlCUD2tQI", "signatures": ["ICLR.cc/2026/Conference/Submission22110/Reviewer_XTn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22110/Reviewer_XTn7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067666136, "cdate": 1762067666136, "tmdate": 1762942067153, "mdate": 1762942067153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you notes to all Reviewers : Summary of Changes"}, "comment": {"value": "We sincerely thank all reviewers for their insightful, constructive, and thoughtful comments. We have carefully addressed every point raised and provided detailed, evidence-backed responses in the rebuttal. Based on the feedback, we made several substantial improvements to the paper and the supplementary materials:\n\n- **Expanded the leaderboard** with newly released models, covering both proprietary and open-source model families.\n- **Added an embedding-based baseline** to broaden the evaluation beyond generative models.\n- **Introduced two new diagnostic analyses** — *Set-Size Sensitivity* and *Rank Correlation Analysis* — to provide deeper insights into model behavior.\n- **Generated 400 additional MCQA samples** using publicly available open-source documents to validate dataset scalability and strengthen domain coverage.\n- **Reported cumulative agreement scores among annotators**, increasing transparency in the expert validation process.\n- **Expanded reproducibility and ethics statements**, clarifying dataset release practices, risk mitigation, and ethical considerations.\n- **Enhanced related work with a parallel work and standard**, following reviewer suggestions to contextualize our contribution more clearly within the emerging literature.\n- **Revised Executable code with readme** - to enable the reviewer to run the experiment using a HuggingFace-hosted model, such as Qwen3-8B\n\nImportantly, **all original claims of the paper remain fully intact**. Even with the newly added models and the inclusion of parallel related work, we did not find any need to revise or weaken our initial claims.\n\nWe greatly appreciate the reviewers' time and effort. We believe these revisions significantly improve the clarity, rigor, and overall contribution of our work."}}, "id": "tBFoUcpM5N", "forum": "7TlCUD2tQI", "replyto": "7TlCUD2tQI", "signatures": ["ICLR.cc/2026/Conference/Submission22110/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22110/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission22110/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724035811, "cdate": 1763724035811, "tmdate": 1763728301428, "mdate": 1763728301428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}