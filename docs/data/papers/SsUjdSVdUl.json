{"id": "SsUjdSVdUl", "number": 20154, "cdate": 1758303107537, "mdate": 1759896998624, "content": {"title": "Critique-RL: Training Critiquing Language Models Through Two-Stage RL for Improved Discrimination and Constructive Feedback", "abstract": "Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor’s outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a $9.02\\%$ gain on in-domain tasks and a $5.70\\%$ gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "tldr": "We propose Critique-RL, an online RL framework for developing critique models without stronger supervision, improving both the discriminability and helpfulness of critiques through a two-stage optimization strategy.", "keywords": ["large language model", "Critique models", "LLM reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/703499ade6875d4533a01effd8de1ba0491400d4.pdf", "supplementary_material": "/attachment/422658c8c53a1f873a440cc1db5fabcf6876881e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Critique-RL, a two-stage reinforcement learning (RL) framework for training critiquing language models. The critic  models that assess the correctness of an actor’s response and provide natural language feedback to guide refinement. The core motivation is to avoid reliance on stronger supervisors for critique annotation, which is costly and hard to scale. The key insight is that optimizing solely via indirect rewards, whether the actor’s refined output is correct, leads to poor discriminability, the critic’s ability to accurately judge whether an initial response is correct. To address this, Critique-RL introduces a two-stage training process. Experiments on mathematical reasoning benchmarksshow consistent improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem setup, motivation, and methodology are clearly articulated with intuitive figures (e.g., Figure 2, 3) and precise definitions of evaluation metrics\n2. Scalable oversight remains a critical bottleneck in LLM alignment and self-improvement. By explicitly disentangling and jointly optimizing two key critique capabilities, the work offers a practical and empirically validated pathway toward more reliable critique models.\n3. The identification of problem due to conflicting objectives under indirect rewards is compelling."}, "weaknesses": {"value": "1. The paper claims to avoid “stronger supervisors” for critique data, yet relies heavily on ground-truth answers (i.e., golden labels) to construct both direct (Stage I) and indirect (Stage II) reward signals. These labels are typically human-annotated or derived from curated datasets (e.g., MATH, GSM8K). Thus, the method still depends on human-provided supervision, albeit not in the form of natural language critiques. This undermines the central motivation that existing approaches “rely on stronger supervisors for annotating critique data”—since here, human-labeled answers serve as an equally strong (if not stronger) form of supervision.\n2. Lack of analysis of some highly related works in this paper [1], which also proposes to use the refinment as the additional feedback.\n\n> [1] Training Language Models to Critique With Multi-agent Feedback"}, "questions": {"value": "No question"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqfXckyIxe", "forum": "SsUjdSVdUl", "replyto": "SsUjdSVdUl", "signatures": ["ICLR.cc/2026/Conference/Submission20154/Reviewer_FEHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20154/Reviewer_FEHf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921081134, "cdate": 1761921081134, "tmdate": 1762933438490, "mdate": 1762933438490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of insufficient scalable supervision for LLMs in complex reasoning tasks. It proposes Critique-RL, a two-stage reinforcement learning framework without strong supervision. In the first stage, the critique model’s discriminative ability is optimized using direct rule-based rewards. In the second stage, the framework integrates indirect rewards from agent-corrected response accuracy with regularization to enhance the usefulness of feedback while preserving discriminative capacity. Critique-RL achieves  performance improvements over baselines on several mathematical reasoning and question-answering tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed two-stage RL method is effective to provide constructive critique feedback for better refinement and precise filter for effective time-time scaling.\n- The experiments contain several benchmarks across different tasks.\n- This paper is well-written and easy to follow."}, "weaknesses": {"value": "My main concern lies in the experimental design, as I am not fully convinced that the current experiments sufficiently demonstrate the proposed method’s advantage on complex reasoning tasks.\n\n- Since the authors explicitly state in the Abstract and Introduction that their method targets complex reasoning tasks, more challenging benchmarks such as AIME and GPQA should have been included in the evaluation.\n\n- Although the main text reports significant improvements on Qwen-3B and Qwen-7B, the appendix reveals that the performance gains on stronger models, such as DeepSeek-R1-Distill-Qwen-7B and Qwen2.5-72B-Instruct, are quite limited. These results should be reported and discussed in the main paper.\n\n- The proposed method requires access to ground-truth answers to compute rewards. Under this setting, it remains unclear what advantages it offers over directly training the model’s reasoning ability using RLVR methods (e.g., GRPO, DAPO). Additional experiments are needed to clarify this distinction."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3FwnnFcE3N", "forum": "SsUjdSVdUl", "replyto": "SsUjdSVdUl", "signatures": ["ICLR.cc/2026/Conference/Submission20154/Reviewer_ao7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20154/Reviewer_ao7e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960828479, "cdate": 1761960828479, "tmdate": 1762933382789, "mdate": 1762933382789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Critique-RL, a two-stage reinforcement learning (RL) approach to train critiquing language models without requiring stronger supervision. The authors first show that baseline RL methods, which use only indirect rewards from an actor's refinement, fail. This is because they improve the critic's helpfulness (constructive feedback) but not its discriminability (judging correctness), leading to poor performance. Critique-RL solves this by:\n- Stage I: Explicitly optimizing discriminability using direct, rule-based reward signals.\n- Stage II: Optimizing helpfulness using indirect rewards (actor refinement) while using regularization to maintain the discriminability from Stage I.\n\nThis two-stage strategy delivers performance improvements on both in-domain and out-of-domain tasks, e.g., +9.02% in-domain and +5.70% OOD for Qwen2.5-7B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's core originality is its clear diagnosis of a key failure mode in training critics: baseline RL methods create a conflict between \"discriminability\" (judging correctness) and \"helpfulness\" (providing feedback), optimizing the latter at the expense of the former.\n- The paper's quality is high, with a rigorous methodology. The training dynamics in Figure 3 clearly show the baseline's failure , while decisive ablation studies in Table 3 prove that both stages of Critique-RL and its specific regularization are essential for success.\n- The work significantly contributes to scalable oversight by providing an effective method to train critics without stronger supervisors. Its value is shown through broad applicability, including \"weak-to-strong\" generalization (a 7B critic improving a 72B actor) and effectiveness on OOD and open-ended tasks ."}, "weaknesses": {"value": "- The paper's primary motivation is to train critics \"without stronger supervision\"1. However, the entire method, especially the critical Stage I, is heavily reliant on an \"oracle reward function\" $r_{oracle}(x,y)$ to compute the direct discrimination reward $r_{dis}$. For the main experiments on math tasks, this oracle is a rule-based verifier that knows the correct answer. This oracle is a form of strong, external supervision.\n- The framework's success, particularly in Stage II, hinges on a critical assumption: the fixed actor model $\\pi_{\\theta}$ is already a good \"refiner\". The authors state the actor is pre-trained to be \"capable of... faithfully refining them according to critiques\". This assumes away a large part of the problem. The helpfulness reward $r_{refine}$ is a convolved signal of both the critique's quality and the actor's ability to understand it. If the actor is a poor refiner, $r_{refine}$ becomes a noisy or meaningless signal, and Stage II would fail to optimize for helpfulness.\n- The paper correctly highlights its inference-time compute-efficiency benefits (e.g., in Figure 1 and Figure 6). However, it completely omits the training-time cost of this complex, two-stage RL pipeline. Stage II, for example, requires at least three model forward passes per training sample (one for the critic $c=\\pi_{\\phi}(x,y)$, one for the actor's refinement $y^{\\prime}=\\pi_{\\theta}(x,y,c)$, and one for the oracle/RM $r_{refine}=r_{oracle}(x,y^{\\prime})$). This is significantly more expensive than the SFT or baseline RL methods it's compared against.\n- The paper's core insight is that final-outcome rewards are insufficient, and a more direct signal is needed. The proposed solution, $r_{dis}$, is a direct reward for judging the outcome of the original response. This overlooks a more direct comparison to Process-based Reward Models (PRMs), which provide supervision at each step of the reasoning. The qualitative examples (Figs. 8, 9) show the critic is evaluating step-by-step, but it is only trained on the final answer's correctness."}, "questions": {"value": "The paper is motivated as an approach for training critique models \"without stronger supervision\". However, the method's crucial first stage relies entirely on a direct reward $r_{dis}$ from an \"oracle reward function\" $r_{oracle}(x,y)$. This oracle, which knows the ground-truth correctness, seems to be a form of strong supervision.\n\nCould you please clarify this apparent contradiction?\n\n1. How do you formally define the \"stronger supervision\" (which you avoid) versus the \"oracle verifier\" (which you use)?\n2. The paper's contribution seems to be a novel way to distill the knowledge of a \"weak\" binary oracle (answer-checker) into a \"strong\" generative critic (feedback-generator). Would you agree with this framing?\n3. How does this framework scale to complex domains (e.g., creative writing, complex coding) where no such simple oracle or high-quality reward model exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGwYweGnGZ", "forum": "SsUjdSVdUl", "replyto": "SsUjdSVdUl", "signatures": ["ICLR.cc/2026/Conference/Submission20154/Reviewer_WXJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20154/Reviewer_WXJP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762566687512, "cdate": 1762566687512, "tmdate": 1762933342891, "mdate": 1762933342891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an online RL approach called Critique-RL for developing critiquing language models without stronger supervision. This approach contains a two-player paradigm, where the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. The authors devise a two-stage optimization strategy, where stage I reinforces the discriminability of the critic with direct rule-based reward signals and stage II introduces indirect rewards based on actor refinement to improve the critic’s helpfulness. Experimental results show the effectiveness of Critique-RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed two-stage RL method is sound and well-motivated, which deals with the core problem of critique generation.\n2. Extensive experiments show the effectiveness of the proposed method.\n3. This paper is overall well-written and easy to follow."}, "weaknesses": {"value": "1. The design of indirect rewards based on actor refinement is similar to [1], which is not discussed in the current paper. The authors should further clarify the difference between this work and [1] to highlight their novelty.\n\n2. The quality of generated critiques should be individually measured via automatic metrics or human evaluation.\n\n\n[1] Training Language Model to Critique for Better Refinement. ACL 2025 Findings."}, "questions": {"value": "I have included my questions in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7p8Pygk9a5", "forum": "SsUjdSVdUl", "replyto": "SsUjdSVdUl", "signatures": ["ICLR.cc/2026/Conference/Submission20154/Reviewer_4UWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20154/Reviewer_4UWL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762600386885, "cdate": 1762600386885, "tmdate": 1762933296170, "mdate": 1762933296170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}