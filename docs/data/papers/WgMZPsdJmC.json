{"id": "WgMZPsdJmC", "number": 16611, "cdate": 1758266723937, "mdate": 1759897229783, "content": {"title": "An Analysis of the Cauchy Method for Different Steplength Coefficient", "abstract": "In this work we take the parameter r (recipprocal of optimal steplenth) as analysis target and introduce steplength coefficient t for classical steepest descent method for convex quadratic optimization problems, and we found the different coefficients affect the state of the entire system convergence. As the value of t varies, the overall system, including the value of r, may converge towards a fixed value, oscillate between two regions, or display chaotic behavior. We also conducted a specific analysis in the two-dimensional case.", "tldr": "", "keywords": ["SD method", "steplenght", "r value"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efda9353a3a12bf073fc307e6bfca28a961912a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyzes step-size selection for convex quadratic optimization problems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The manuscript clearly does not meet the bar for publication at ICLR:\n- Significance to the machine learning community is unclear\n- The optimization of convex quadratics has been studied since more than 180 years. It is unclear what the research contributes to the state of the art.\n- The presentation does not meet the standards of a top-tier venue."}, "weaknesses": {"value": "See above."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzS4IuSELQ", "forum": "WgMZPsdJmC", "replyto": "WgMZPsdJmC", "signatures": ["ICLR.cc/2026/Conference/Submission16611/Reviewer_7Vez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16611/Reviewer_7Vez"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775904562, "cdate": 1761775904562, "tmdate": 1762926681718, "mdate": 1762926681718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an analysis of the gradient descent method solving quadratic function, especially in two-dimensional space. The writing and presentation are very poor, lacking necessary punctuation (such as commas and periods), which are fundamental to writing a research paper. Moreover, the contribution in 2-D is insufficient to warrant publication, especially considering that this is a flagship conference for AI."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "No strength."}, "weaknesses": {"value": "Very poor writing and presentation, with missing basic punctuation.\nMajor contribution limited to a trivial 2-D convex quadratic case."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "283dh7h62h", "forum": "WgMZPsdJmC", "replyto": "WgMZPsdJmC", "signatures": ["ICLR.cc/2026/Conference/Submission16611/Reviewer_2t5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16611/Reviewer_2t5Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794556765, "cdate": 1761794556765, "tmdate": 1762926680853, "mdate": 1762926680853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines optimization methods, focusing specifically on the convex quadratic optimization problem. The paper identifies a phenomenon related to the reciprocal of the optimal step length. A two-dimensional case is presented to illustrate this finding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper investigates how different coefficients affect the convergence of the entire system."}, "weaknesses": {"value": "The overall paper is not well written.\n\nAlthough this is a mathematical optimization paper, it should be better polished to improve readability. Symbols should be defined clearly, and more background information should be provided. Since this is an AI conference, it would also be beneficial to include experiments related to AI."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oLynLfWxyl", "forum": "WgMZPsdJmC", "replyto": "WgMZPsdJmC", "signatures": ["ICLR.cc/2026/Conference/Submission16611/Reviewer_pwf8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16611/Reviewer_pwf8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979359609, "cdate": 1761979359609, "tmdate": 1762926680419, "mdate": 1762926680419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to provide new results regarding steepest descent on quadratic problems. Most importantly, the authors propose an interesting approach of analyzing how the method evolves with respect to the stepsize, instead of the traditional analyses focusing on the iterates."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This work offers a clear alternative viewpoint by modeling the steepest descent method as a one-dimensional dynamical system in terms of the optimal step size. For the two-dimensional quadratic case, the authors provide a concise and explicit characterization of the corresponding dynamics, which is then extended to the general multidimensional setting. The paper also includes experimental results that qualitatively support the analytical findings."}, "weaknesses": {"value": "The scope of this work is extremely narrow. \nNot only does it focus on the steepest descent method applied to **quadratic** problems, but the theoretically sound results are limited to the two-dimensional case, with claims for higher-dimensional extensions lacking formal justification. \nConsequently, the connection to modern machine learning optimization problems is very weak, raising questions about whether this paper fits the scope of the intended venue.\nMoreover, while the paper does not propose a fundamentally new optimization method but rather offers an alternative perspective on an existing algorithm, its limited scope makes it difficult to argue that it provides substantial new insight.\n\nAlso, the overall presentation is not up to publication standards. The paper contains numerous grammatical and stylistic issues (most notably, uncapitalized sentence beginnings), inconsistent notation without proper explanation or derivation (for example, the definition of $r$ in Equation 9), incorrect cross-references (such as “minimizing Eq.(3)” around line 034), and the use of unjustified or imprecise terminology (for instance, describing the system as “chaotic”, which is not supported by a rigorous definition or analysis). These issues collectively make the paper difficult to follow and detract from its technical clarity."}, "questions": {"value": "1. What is the ultimate goal of this analysis? As early as 2015, Kalousek already showed that step sizes can even be randomly chosen, and more recent works, such as the Silver Stepsize Schedule by Altschuler and Parrilo (2025), demonstrate that occasionally using large step sizes can actually accelerate convergence. These results suggest that one cannot meaningfully characterize or predict the convergence of the iterates $x_k$, which are what ultimately matter, solely by analyzing the evolution of the step sizes. How does the proposed one-dimensional analysis in terms of  $r_k$ advance our understanding of the underlying optimization process beyond what is already known from these prior studies?\n\n2. Could the authors clarify whether they envision the proposed analysis as a tool for designing new step-size adaptation schemes, or purely as a qualitative study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XkVx0PSYNp", "forum": "WgMZPsdJmC", "replyto": "WgMZPsdJmC", "signatures": ["ICLR.cc/2026/Conference/Submission16611/Reviewer_V1cP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16611/Reviewer_V1cP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001323534, "cdate": 1762001323534, "tmdate": 1762926680001, "mdate": 1762926680001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}