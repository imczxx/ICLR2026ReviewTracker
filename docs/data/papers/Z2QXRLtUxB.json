{"id": "Z2QXRLtUxB", "number": 2811, "cdate": 1757257462671, "mdate": 1762941703527, "content": {"title": "CARE: Confidence-aware Ratio Estimation for Medical Biomarkers", "abstract": "Ratio-based biomarkers -- such as the proportion of necrotic tissue within a tumor -- are widely used in clinical practice to support diagnosis, prognosis, and treatment planning. These biomarkers are typically estimated from soft segmentation outputs by computing region-wise ratios. Despite the high-stakes nature of clinical decision making, existing methods provide only point estimates, offering no measure of uncertainty. In this work, we propose a unified confidence-aware framework for estimating ratio-based biomarkers. Our uncertainty analysis stems from two observations: i) the probability ratio estimator inherently admits a statistical confidence interval regarding local randomness (bias and variance), ii) the segmentation network is not perfectly calibrated. We conduct a systematic analysis of error propagation in the segmentation-to-biomarker pipeline and identify model miscalibration as the dominant source of uncertainty. We leverage tunable parameters to control the confidence level of the derived bounds, allowing adaptation towards clinical practice. Extensive experiments show that our method produces statistically sound confidence intervals, with tunable confidence levels, enabling more trustworthy application of predictive biomarkers in clinical workflows.", "tldr": "This paper provides adaptive and tunable confidence intervals for ratio-based biomarkers, based on pretrained segmentation network.", "keywords": ["medical segmentation", "uncertainty quantification", "calibration error"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/979c3553a078baea92c6c32c6157752adb6f85ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CARE (confidence-aware ratio estimation) a drop-in, model-agnostic method for uncertainty in the form of confidence intervals for segmentation biomarker ratios by decomposing into model calibration error and estimation error using Markov inequality. Experiments are conducted on several tumor segmentation datasets and the authors find that miscalibration error is dominant uncertainty source. They claim CARE is computationally efficient and that adaptive confidence intervals can provide better coverage than standard baselines such as conformal prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Proposes confidence estimation strategy for downstream ratio estimation task by decomposing uncertainty into two bounds: an estimation-based interval of $\\hat{r}$ using Markov to get $\\hat{r} \\pm \\frac{\\sqrt{\\hat{\\text{SE}}}}{\\sqrt{\\alpha}}$ and calibration-based interval using calibration dataset (similar to conformal prediction) on the numerator and denominator at a segmentation instance level. The final interval is a sum of these two components. \n* The proposed method has similar computational overhead to conformal prediction and does not require modification to model, only calibration data similar to conformal prediction.\n* The authors validate their method on multiple datasets (MSD-Task01, BraTS21, KiTS23) and across multiple SOTA models (nnUNet, nnFormer, UNETR++)"}, "weaknesses": {"value": "* The method is designed for ratios of two volume estimates $r = \\frac{\\text{Vol}(A)}{\\text{Vol}(B)}$ and the confidence bounds are specific to this ratio structure. This limits applicability to other biomarkers that are not ratios and makes the contribution narrower and of limited interest to broader ICLR community. \n* Fig. 7 shows that miscalibration interval (ECE)  accounts for most of the overall interval, implying that the estimation-based components is unnecessary. This suggest the contribution from the estimation-based bound may add little practical value and calls into question the entire two-part uncertainty decomposition. A simpler variant using only the calibration based interval should be compared to the full CARE method. \n* The uncertainty bounds may be overly conservative due to union bound and dependence between the estimation and miscalibration error. \n* The experiments are unconvincing as the baselines are either flawed or unfairly compared. The resampling baselines perform unusually low (~10% coverage in Table 1) suggesting unfair or incorrect implementation. Sampling individual pixels independently is inherently flawed as it violates spatial correlation in segmentation and should not be used for comparison. Similarly, the Bayesian methods also have poor performance perhaps due to inappropriate choice of priors. For conformal prediction, the paper demonstrates that miscalibration is dominant source of uncertainty on tumor segmentation tasks but conformal prediction is applied directly to miscalibrated model outputs, which is good practice. A stronger baseline would be to first apply post-hoc calibration e.g. temperature scaling and then apply conformal prediction. Additionally there is a lack of more modern and relevant uncertainty quantification baselines. This lack of comparison to more recent state-of-the-art techniques weakens the evaluation"}, "questions": {"value": "* Could you please provide an ablation study that evaluates the performance (coverage and adaptiveness) of a simpler method using only the calibration-based interval?\n* The paper identifies miscalibration as the key issue. Have you compared CARE against a stronger baseline of \"Post-hoc Calibration + Conformal Prediction\"? This seems like a more direct and fair comparison.\n* The performance of Subsampling and Bootstrap in Table 1 is extremely low, could you please clarify the implementation and explain the poor performance? What about methods that account for spatial dependence?\n* Could you discuss the relationship between the estimation error and the calibration error? Is it possible to derive a tighter, combined bound by analyzing their dependence?\n* Can you compare conformal prediction methods that account for pixel correlation such as https://proceedings.mlr.press/v162/angelopoulos22a/angelopoulos22a.pdf ? \n* Given the method's high specificity to ratio-based biomarkers, how do you see this framework being generalized to other common biomarker types, such as lesion counts or non-ratio volume measurements?\n* Why were more modern UQ baselines, which also handle calibration, not included in your evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6aE2b4TzXb", "forum": "Z2QXRLtUxB", "replyto": "Z2QXRLtUxB", "signatures": ["ICLR.cc/2026/Conference/Submission2811/Reviewer_JpZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2811/Reviewer_JpZS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761321750302, "cdate": 1761321750302, "tmdate": 1762916387701, "mdate": 1762916387701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mBNy2Kg4fm", "forum": "Z2QXRLtUxB", "replyto": "Z2QXRLtUxB", "signatures": ["ICLR.cc/2026/Conference/Submission2811/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2811/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762940957123, "cdate": 1762940957123, "tmdate": 1762940957123, "mdate": 1762940957123, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CARE (Confidence-Aware Ratio Estimation), a post-hoc statistical framework for generating confidence intervals for ratio-based biomarkers (e.g., necrosis-to-tumor ratio) derived from medical image segmentation outputs.\nCARE analytically combines an estimation-based uncertainty term with a calibration-based uncertainty term. The method is model-agnostic, lightweight, and applicable to any pretrained segmentation network. Experiments on multiple architectures (nnUNet, nnFormer, UNETR++) demonstrate that CARE produces more reliable and adaptive confidence intervals than common uncertainty estimation baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a clean and interpretable mathematical formulation of uncertainty estimation. The derivation of the ratio confidence interval from a Markov upper bound and a calibration correction term is logically coherent and easy to follow. This level of theoretical transparency is uncommon in medical imaging works.\n\nCARE’s linear-time computation, single-pass inference, and absence of sampling or retraining make it far more efficient than Bayesian or Bootstrap-based uncertainty approaches. Its plug-in nature makes it attractive for clinical integration where inference time is critical.\n\nCARE is model-agnostic and requires no architectural modification, making the approach readily reproducible and easily applicable to other domains where ratio metrics are clinically important."}, "weaknesses": {"value": "1. Despite the neat formulation, CARE essentially repackages existing statistical concepts—Markov inequality and calibration errors—without introducing a genuinely new computational mechanism. The innovation lies primarily in combining known pieces into a domain-specific framework. For a top-tier conference, the level of originality is modest.\n\n2. The method is primarily motivated by identifying “uncertain” cases near clinical decision thresholds. However, such borderline cases are typically subject to manual review by radiologists in real-world workflows, even without algorithmic prompting. Consequently, the paper does not clearly justify the added clinical value of CARE beyond formalizing a process that already occurs in practice.\n\n3. While the authors claim negligible computational cost, no empirical timing data or resource usage is reported. For clinical deployment, even small delays can matter, so quantitative runtime benchmarks would make the efficiency claim more convincing.\n\n4. Most qualitative examples focus on near-threshold samples. The study lacks detailed analysis of scenarios where the model is apparently confident but actually miscalibrated far from the threshold, which could be the most dangerous case in medical AI. Such examples would strengthen the argument that CARE improves safety beyond obvious cases.\n\n5. The results are scattered across multiple tables and figures, each addressing coverage, interval width, adaptiveness, and tunability separately. A single summary table (or a “coverage–width efficiency” plot) consolidating the key quantitative results for CARE and all baselines would greatly enhance readability and allow a more holistic assessment of CARE’s overall advantage."}, "questions": {"value": "1. Given that borderline cases near clinical thresholds are already manually reviewed by radiologists, could the authors elaborate on the concrete added value of CARE in practical workflows? For instance, does CARE offer benefits in non-borderline or high-confidence yet miscalibrated scenarios?\n\n2. Can the authors provide quantitative runtime benchmarks comparing CARE with Bootstrap, Conformal methods and other methods to substantiate the claim of computational overhead?\n\n3. Have the authors examined cases where the segmentation model is confidently wrong (far from the threshold but miscalibrated)? Such analysis could demonstrate whether CARE enhances safety in situations beyond obvious uncertainty.\n\n4. Would the authors consider adding a consolidated summary that jointly compares coverage accuracy, interval width, adaptiveness, and computational cost across all methods? This would help readers better assess CARE’s overall advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UdwyuL0aPW", "forum": "Z2QXRLtUxB", "replyto": "Z2QXRLtUxB", "signatures": ["ICLR.cc/2026/Conference/Submission2811/Reviewer_BB7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2811/Reviewer_BB7t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792289668, "cdate": 1761792289668, "tmdate": 1762916386412, "mdate": 1762916386412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission studies ratio estimation in medical segmentation problems, a common task in clinical applications of machine learning methods. The submission proposes to use both notions of bias and miscalibration in the base predictor to construct uncertainty intervals that provide marginal coverage of the ground-truth ratio. The decomposition of uncertainty into bias and miscalibration would allow practitioners to form a better picture of the source of uncertainty. Experiments confirm the validity of the proposed procedure, and expand on properties of efficiency and adaptiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Uncertainty estimation of ratio estimation from machine learning models is an important problem.\n- The idea to use both bias and miscalibration is interesting.\n- Experiments are extensive."}, "weaknesses": {"value": "- Presentation is sometimes hand-wavy.\n- Certain experimental claims could be motivated and presented more clearly.\n\nI have a few clarifying questions and I am looking forward to discussing with the authors!"}, "questions": {"value": "**Confusion about notation**\n\nI am not sure I follow notation conventions throughout the paper. On line 179, $\\\\{z_i\\\\}_{i = 1}^n$ are defined as \"per-pixel inputs\". This notation is somewhat unusual. It would be more clear to define inputs are high-dimensional vectors, i.e. $z \\in [0,1]^n$? Could the authors expand on this choice?\n\nModels are interchangeably defined as acting on individual pixels, i.e. $z_i$, or $z$. Given my prior confusion about notation, I am not sure I understand what this means. Defining the segmentation model to act on individual pixels is counterintuitive, as the segmentation depends on the entire image, not individual pixels only. Could the authors clarify this?\n\nSimilarly, I am confused about the notion of Volume Bias. As it is written, I do not follow:\n\n- What distribution is the expectation taken over? The joint distribution of images and ground-truth segmentation masks, or individual pixels with their respective label? If it is the latter, wouldn't that kill spatial information across pixels?\n\n- Why is this a notion of volume bias, rather than pixel bias? Shouldn't the volume bias take into consideration the entire image?\n\n**Definition of target fraction**\n\nIn Prop. 3.6, Line 239 defines $r = \\mathbb{E}[y]/\\mathbb{E}[x]$. Modulo my previous confusions about notation, could the authors expand on this choice rather than $r = \\mathbb{E}[y/x]$? \n\nIn particular, the latter seems more natural, i.e. the expected ratio over the joint distribution of images and their respective segmentations, rather than the ratio of the expected  volumes---which does not take into consideration the dependency between $y$ and $x$. This seems like an important choice that should be motivated.\n\n**Questions about Prop 3.6 and 3.8**\n\n- Equations (5) and (8) seem to provide coverage guarantees of different quantities. Could the authors clarify whether the propositions consider the same quantities, and, if so, unify notation to avoid confusion?\n\n- In their current formulation, the statements do not seem to depend on the size of the calibration set. I assume the authors mean to take the calibrated empirical quantiles of the respective quantities in each proposition? See my minor comment below about quantiles vs calibrated empirical quantiles.\n\n- Prop. 3.6 is in terms of $\\beta_{r,\\alpha}$, which needs to be estimated, and consistency is guaranteed only asymptotically. This seems to break the finite-sample validity of the statement? Could the authors clarify this point, and the role of Prop. 3.7 when implementing the procedure on finite data? As an aside, would something weaker than consistency, e.g. unbiasedness, suffice to prove the statement?\n\n**Adaptiveness**\n\nThe argument provided in Sec. 4.2 is not totally convincing. Usually, in CP literature, we argue for shorter, more efficient intervals. This is because larger intervals trivially provide coverage. Here, an argument is made in favor of larger intervals because they are adaptive to the hardness of the problem. Adaptiveness is certainly an important property, but vis-a-vis marginal guarantees, larger interval lengths do not imply adaptiveness.\n\nFigure 5 is more convincing. However, the comparison with standard split CP may be considered unfair, since it is well known that marginal coverage does not imply conditional coverage. To make this argument more solid, I would suggest comparing with existing group-conditional calibration methods that are designed to achieve adaptive behavior (e.g., [1]). Finally, are stratifications in Fig. 5 computed with ground-truth or prediction labels?\n\n**Experiments**\n\nUsing $C = 0.68$ in the experiments seems very specific. Could the authors motivate this choice?\n\nThe fact that standard CP does not achieve coverage in Fig. 6 is concerning. Could the authors clarify how this figure is generated? Baseline CP should achieve coverage by construction.\n\n**Prior works**\n\nThere exist some prior work that studies metric guided uncertainty quantification in medical imaging problems that might be worth citing [2].\n\nFurthermore, it might be good to include references to existing works on learning to reject / defer, who share a similar motivation as the introductory example of an automatic flag when the uncertainty interval includes a clinically-relevant threshold []. \n\n**References**\n\n[1] Ding et al. \"Class-Conditional Conformal Prediction with Many Classes\", 2023.\n\n[2] Cheung et al. \"Metric-Guided Conformal Bounds for Probabilistic Image Reconstruction\", 2024.\n\n[3] Cortes et al. \"Learning with Rejection\", 2017.\n\n---\n\n**Minor comments**\n\n- Throughout the manuscript, empirical quantiles are mentioned simply as \"quantiles\" (for example, Prop. 3.2). I assume the authors mean the adjusted calibrated quantile that takes into consideration the finite number of samples available? Could the authors make this precise in the manuscript?\n- Line 281: \"which is analogous to multiple testing\" might be unclear to a first-time reader not familiar with hypothesis testing, could the authors expand on this sentence?\n- Computational complexity: without a formal statement of what the algorithm is, it is not possible to verify the claim that it runs in linear time (in the number of images? or in the number of inputs?)\n- Lines 308 - 309: typo in validation vs calibration dataset?\n- Lines 336 - 337: what are numerous forward passes for Bayesian methods? why are they intractable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0M9DMf4lh", "forum": "Z2QXRLtUxB", "replyto": "Z2QXRLtUxB", "signatures": ["ICLR.cc/2026/Conference/Submission2811/Reviewer_a8jx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2811/Reviewer_a8jx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835212257, "cdate": 1761835212257, "tmdate": 1762916386173, "mdate": 1762916386173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of producing uncertainty-aware confidence intervals for ratio-based biomarkers frequently used in medical image segmentation tasks. The authors propose a framework aimed at improving the reliability of these ratio estimates, particularly in clinical contexts where uncertainty quantification is critical."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The focus on uncertainty-aware predictions is highly relevant for healthcare applications, where reliability and transparency are essential for real-world deployment.\n- The experimental evaluation is well designed, encompassing two tumor segmentation tasks and comparing performance across two widely used architectures, nnUNet and nnFormer."}, "weaknesses": {"value": "- The motivation for restricting the framework to ratio-based predictions is insufficiently justified. It remains unclear why the proposed approach could not be extended to other types of continuous or derived metrics.\n- The uncertainty quantification baselines are somewhat limited, focusing only on sampling-based methods, bootstrapping, and conformal prediction. The paper would be strengthened by comparisons with more advanced or recent UQ approaches, such as Bayesian deep learning or ensemble-based methods."}, "questions": {"value": "- Could the authors present the implementation details of the proposed method in algorithmic form to improve reproducibility and clarity?\n- What are the theoretical or methodological reasons that make the proposed approach specific to ratio estimators? Please elaborate on the assumptions or constraints that prevent its generalization to other predictive quantities.\n- Can you run experiments for other types of ratios and models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JNTy4dZzot", "forum": "Z2QXRLtUxB", "replyto": "Z2QXRLtUxB", "signatures": ["ICLR.cc/2026/Conference/Submission2811/Reviewer_owTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2811/Reviewer_owTF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762798503230, "cdate": 1762798503230, "tmdate": 1762916385999, "mdate": 1762916385999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}