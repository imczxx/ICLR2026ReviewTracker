{"id": "5gFKVyohGd", "number": 977, "cdate": 1756826041834, "mdate": 1759898233039, "content": {"title": "Fast Data Mixture Optimization via Gradient Descent", "abstract": "While large and diverse datasets have driven recent advances in large models, identifying the optimal data mixture for pre-training and post-training remains a significant open problem. We address this challenge with FastMix, a novel framework that automates data mixture discovery while training only a single proxy model. Instead of relying on predefined heuristics or resource-intensive simulations, FastMix jointly optimizes mixture coefficients and model parameters, substantially improving efficiency and scalability over prior approaches. At the core of FastMix is a reformulation of mixture selection as a bilevel optimization problem.\nUnder this reformulation, we show that optimizing mixture ratios is mathematically equivalent to assigning per-source loss weights under uniform source sampling. This embeds the mixture coefficients directly into the differentiable iterative optimization objective, enabling efficient, gradient-based optimization of both mixture and model. To solve the optimization problem, FastMix implements an approximate iterative optimization procedure, alternating between (i) updating model parameters on data sampled according to current mixture ratios (inner loop) and (ii) updating mixture ratios based on validation feedback (outer loop). Across pre- and post-training, FastMix outperforms baselines while drastically reducing search cost: in pre-training, it attains an average score of 48.2 with 1.3 GPU-hours ($\\times 550$ vs. RegMix; $\\times 55$ vs. CLIMB), and in post-training (SFT) it leads with 65.4 with a $+5.5$ gain over the next best, completing search in 2.2 GPU-hours compared to the 115 GPU-hours required by CLIMB/RegMix.", "tldr": "", "keywords": ["Large models", "Data-centric AI", "AutoML"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0249fc063ce4d732c8faa8550d56d91567e58826.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper “Fast Data Mixture Optimization via Gradient Descent (FastMix)” proposes a scalable, differentiable framework for optimizing data mixtures for large-scale model training. The authors address how to efficiently determine the optimal proportions of multiple data sources to maximize performance on a target validation objective, both for pre-training and post-training (SFT) stages.\n\nFastMix reformulates data mixture optimization as a differentiable bilevel optimization problem. Instead of non-differentiable data sampling (drawing examples according to mixture ratios), the authors show that mixture selection can be equivalently represented as per-source loss weighting under uniform sampling. This reparameterization embeds mixture coefficients directly into the training loss, making them continuous and differentiable parameters.\n\nThis leads to significant performance gains over the baselines, while comparatively the time taken is less due to the use of a single proxy model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "FastMix presents an interesting advance in data mixture optimization by reframing the problem as a differentiable bilevel optimization, enabling end-to-end gradient-based updates of both model and mixture parameters within a single proxy run. This eliminates the need for computationally intensive multi-proxy searches used in prior works such as RegMix and CLIMB, while delivering superior accuracy and generalization across both pre-training and post-training stages. Its simplicity, efficiency, and demonstrated scalability (spanning 1B to 7B models) make it a practical and elegant solution.\n\nThe method, while seemingly simple, is a novel contribution to the data mixture problem. While this kind of continuous and differentiable transformation is seen in other works, it has not been attempted in mixture optimization for data mixtures before. The bilevel optimization reformulation and its implementation with a single proxy model is also novel. This kind of technique is clearly inspired by advances in hyperparameter optimization. Noteworthy is the direct application to large scale models, opening avenues for e.g. in LLM pre-training. Finally, while other papers usually limit themselves to evaluation on one of pre-training or post-training, this paper does both.\n\nTo summarize,\n1. The authors demonstrate significant performance gains and time savings: up to 550× faster mixture search compared to RegMix and 55× faster than CLIMB, and gains in  pre-training (48.2 avg score) and post-training (65.4 avg score), with top ranks on multiple benchmarks.\n2. The practicality and simplicity of their method, barring the fact that it is not a dynamic method, is a plus point.\n3. The use of a single proxy model as against hundreds is appreciable and makes training with this method more accessible."}, "weaknesses": {"value": "FastMix represents a conceptually clean and computationally efficient approach to data mixture optimization. However, its limitations arise from the simplifying assumptions underlying its differentiable reparameterization.\n\nFor one, the method assumes smooth, stationary relationships between mixture weights and validation loss. This assumption can break down in real, non-linear training regimes. Further,  the paper’s experimental evaluation, though broad, is limited in terms of cross-scale validation, dynamic adaptation, and proxy–target transferability. The use of a static mixture throughout training restricts its ability to handle evolving data distributions (the authors mention this). Additionally, the efficiency comparisons rely on downscaled baselines, and the generality of results across diverse domains (e.g., multimodal or highly imbalanced datasets) remains untested. Overall, the work is methodologically elegant but still somewhat idealized in scope and validation. Further experimentation is necessary.\n\n1. The authors explicitly note that FastMix optimizes a fixed mixture that remains constant throughout training. The mixture distribution may change over time depending on the scenario. For example, it is possible in non-stationary training regimes that early exposure to simpler data is more beneficial and with time, fine grained data becomes more important.\n\n2. One major issue is that the method assumes the validation loss is a smooth, differentiable function of the mixture weights. This means that it is expected that small changes in \\alpha lead to predictable changes in validation performance. In realistic settings, the loss formulation may exhibit non-smooth regions (although this depends on the parameterization of the network). FastMix’s gradient updates may lead to suboptimal mixtures in this case.\n\n3. Yet another issue is that their method leaves open a proxy-target gap: the experiments rely on 1B param models to optimize mixtures for 7B param target models. There is an implicit scale transfer assumption, that the mixtures should be optimal across sizes. While the paper exhibits strong results, it does not quantify the correlation between proxy and target performance. It is then unclear how well the proxy captures data utility for much larger models.\n\n4. While a relatively minor issue, in the post-training experiments, both RegiMix and CLIMB are limited to 64 proxy models. While this is necessary for ease of computation, the baselines’ potential is undervalued. The efficiency gains of FastMix remain valid but somewhat exaggerated in absolute terms.\n\n5. FastMix optimizes a single scalar validation objective, in which case there is a single downstream goal. It lacks demonstration on multi-domain training and instruction tuning, where mixtures may become biased towards domains that correlate strongly with validation data. The evaluation needs to be more robust and demonstrate realistic tradeoffs in such use cases.\n\n6. All experiments are seemingly conducted on text benchmarks. Experiments on multi-modal mixtures (for e.g. text and image) or even long-tail domains are missing. Thus the claim of scalability remains largely within text.\n\n7. The issue of requiring surrogate validation losses in the case of non-smoothness has been handled cursorily. The authors mostly experiment on the cross-entropy validation loss, and there ought to be a longer discussion about other metrics (e.g. BLEU, Exact Match, etc)."}, "questions": {"value": "1. The proposed method assumes that validation loss varies smoothly with mixture weights. How robust is FastMix when this assumption fails, e.g., for highly compositional or discontinuous domains? You could try evaluating on benchmarks such as GSM8K [see Zhang et al, 2024 for an analysis], MATH or HumanEval [see Bradbury et al, 2024 for an analysis] (even one would do for the purpose of the rebuttal).\n\n2. How well do mixture weights learned on smaller proxies transfer to larger models? Ther should be an ablation using multiple proxy sizes (e.g., 0.5B, 1B, 3B).\n\n3. A simple extension towards dynamic mixture optimization could be the following: every K steps, compute $\\alpha_{t+1, \\text{EMA}}$ as a moving average of the current $\\alpha_t$ and the newly computed $\\alpha_{t+1}$, and then resume training the model with the new $\\alpha_{t+1, \\text{EMA}}$. The authors could experiment with this on the Pile itself (Books, Arxiv, Wikipedia etc) which has heterogenous domains where early training may benefit from broad text but later training may become more code/math/etc focused.\n\n4. Can the method generalize to other modalities or multi-domain mixtures? For example, you could try applying FastMix to the MUGEN dataset (which has three modalities, which you can count as data mixture sources) and then optimize mixture weights for downstream video prediction or other multimodal generation task. Another possible experiment is using image-text pretraining corpora such as CC3M, CC12M, LAION-400, and subsequently, FastMix could discover mixture ratios for downstream tasks on validation benchmarks, instead of how the mixtures may have to be manually tuned for CLIP [Gadre et al, 2024][Nguyen et al, 2023].\n\n5. I noticed the mixture visualization in Appendix A and was wondering if it is fair to compare FastMix to RegMix and CLIMB if they don’t incorporate similar regularization to prevent mixture collapse. Can you update your analysis with RegMix and CLIMB variants that prevent this collapse, or can you explain why it is not possible to incorporate such regularization in their frameworks?\n\n6. The approach relies on differentiable validation losses wrt mixture weights. How would it adapt to discrete or non-differentiable metrics such as BLEU, EM, or ROUGE?\n\n7. How sensitive is FASTMIX to the choice of regularization hyperparameters (\\alpha, \\beta) in promoting sparse or diverse mixtures? There should be a regularization ablation showing mixture entropy vs. downstream accuracy, and this would help clarify whether performance depends on sparse or more diffuse mixtures.\n\n8. The method optimizes mixtures based on a single validation dataset. How sensitive are the learned mixtures to the choice of validation task? Does the mixture learned on a particular validation task/dataset generalize to other validation tasks/datasets? Roughly speaking, is there task transfer robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3VTXAG5v55", "forum": "5gFKVyohGd", "replyto": "5gFKVyohGd", "signatures": ["ICLR.cc/2026/Conference/Submission977/Reviewer_Kg4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission977/Reviewer_Kg4B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946083810, "cdate": 1761946083810, "tmdate": 1762915651697, "mdate": 1762915651697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FASTMIX, a gradient-based framework for optimizing data mixture coefficients in language model training. The key idea is to treat mixture optimization as a differentiable problem by viewing mixture coefficients as loss weights. This enables end-to-end joint optimization of mixture and model parameters with a smaller proxy model. The optimization alternates between inner-loop model updates with outer-loop mixture updates based on validation gradients, using entropy and training-loss for regularization. \n\nIn both pre-training and post-training settings, the authors show FASTMIX outperforms prior mixture-optimization baselines (e.g., CLIMB, RegMix) while being 50–550× more compute-efficient in finding optimal mixture compared to some baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper shows strong results in downstream performance in both pretraining and post-training settings.\n- The derivation of Eq. 6 is novel and the intuition behind it is clearly presented and insightful.\n- The ablation studies on key hyperparameters are informative"}, "weaknesses": {"value": "- The ADO method by Jiang et al. [1] is, in my opinion, particularly relevant to this paper. But the authors have not compared against ADO in the results. It is relevant for two reasons: (1) it is, to my knowledge, the best data mixture optimization method that does not incur time cost in searching for data mixture and (2) ADO shares the same intuition that one should upsample data mixture that results in the greatest decrease in validation loss. \n- Similarly, the paper misses a few other relevant literature [2, 3, 4]. \n- The paper does not report error bars (in Tables 1 and 2). It would be helpful to include multiple runs to verify that the method reliably outperforms other approaches.\n\n[1] Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws\n\n[2] Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance\n\n[3] Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework\n\n[4] ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization"}, "questions": {"value": "- Could the authors clarify what exactly constitutes the “search target” throughout the paper? Does this refer to Eq. 7 as a whole, or only the validation-loss component? Additionally, in practice, was the gradient in Eq. 6 computed using the full expression in Eq. 7, or only its first term?\n- For Fig. 2c, what aspects of the model or training are randomized in the “random initialization” runs?\n- How were the initial mixture weights selected?\n- Does the mixture optimization converge to a stable distribution after a certain number of iterations? How sensitive is the process to different initial mixtures? A plot illustrating mixture trajectories over training would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6nzTxnhTGr", "forum": "5gFKVyohGd", "replyto": "5gFKVyohGd", "signatures": ["ICLR.cc/2026/Conference/Submission977/Reviewer_PnEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission977/Reviewer_PnEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972961599, "cdate": 1761972961599, "tmdate": 1762915651567, "mdate": 1762915651567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of data mixture optimization for LLM pre-training and SFT. Existing strong methods such as RegMix and CLIMB find good mixtures but do so by training many proxy models, which makes them expensive and slow to iterate on.  The paper proposes FASTMIX, which makes mixture search differentiable by (i) reformulating mixture selection as a weighted bilevel optimization problem and (ii) showing that sampling from a mixture is equivalent in expectation to doing uniform source sampling but applying per-source loss weights. This reparameterization allows the authors to update both model weights and mixture weights in the same training loop with a lightweight hyper-gradient.\n\nThe paper claims:\n\t1.\tOn pre-training over Pile-style subsets, FASTMIX achieves the best average score (48.2) at ~1.3 GPU-hours of search, which the authors say is 55× faster than CLIMB and 550× faster than RegMix while still slightly better in accuracy. \n\t2.\tOn post-training/SFT (math-anchored, tested on coding & STEM QA), FASTMIX reaches 65.4, about +5.5 over the next best method, in 2.2 GPU-hours. \n\nSo the core contribution is a practical, differentiable, single-proxy recipe for data-mixture search that aims to match proxy-based methods in quality but at 1–2 GPU-hours instead of 100+."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The key equivalence of “sampling by α” ≡ “uniform source sampling + per-source loss weights α” is written down cleanly and plugged straight into a bilevel objective. This makes the method implementable in an existing LLM training codebase with only per-batch source tags and a vector of α.\n2. Compute saving is compelling. The claim “1.3 GPU-hours vs 55–550× more for baselines” is helpful, because current RegMix-style methods really do train large banks of proxies. The authors also highlight that they train only one proxy model."}, "weaknesses": {"value": "No uncertainty / significance reporting. Gains like “+0.7 to +1.0” in pre-training are plausible but small. Without multiple seeds or CIs, especially on the final model, it’s hard to tell whether the improvement is robust or just a friendly seed. This matters because RegMix/CLIMB papers do report variability."}, "questions": {"value": "Can the method enforce per-source lower/upper bounds (e.g. “at least 10% safety-filtered”, “at most 5% web-crawl”)? If yes, how does that change the hyper-gradient update?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YnVkWU36Hl", "forum": "5gFKVyohGd", "replyto": "5gFKVyohGd", "signatures": ["ICLR.cc/2026/Conference/Submission977/Reviewer_Tw57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission977/Reviewer_Tw57"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056486536, "cdate": 1762056486536, "tmdate": 1762915651424, "mdate": 1762915651424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}