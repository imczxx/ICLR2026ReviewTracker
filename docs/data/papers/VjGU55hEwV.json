{"id": "VjGU55hEwV", "number": 25346, "cdate": 1758367040920, "mdate": 1759896723936, "content": {"title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models", "abstract": "Nowadays, Large Lange Models (LLMs) are able to propose rules in natural language, overcoming constrains of a predefined predicate space inherent in traditional rule learning. However, existing methods using LLMs often overlook the combination effects of rules, and the potential of coupling LLMs with probabilistic rule learning to ensure robust inference is not fully explored. \nTo address this gap, we introduce **RLIE**, a unified framework that integrates LLMs with probabilistic modeling to learn a set of probabilistic rules. \nThe RLIE framework comprises four stages: (1) **R**ule generation, where a LLM proposes and filters candidate rules; (2) **L**ogistic regression, which learns the probabilistic weights of the rules for global selection and calibration; (3) **I**terative refinement, which continuously optimizes the rule set based on prediction errors; and (4) **E**valuation, which compares the performance of the weighted rule set as a direct classifier against various methods of injecting the rules into an LLM. \nGenerated rules are the evaluated with different inference strategies on multiple real-world datasets. While applying rules directly with corresponding weights brings us superior performance, prompting LLMs with rules, weights and classification results from the logistic model will surprising degrade the performance.\nThis result aligns with the observation that LLMs excel at semantic generation and interpretation but are less reliable at fine-grained, controlled probabilistic integration.\nOur work investigates the potentials and limitations of using LLMs for inductive reasoning tasks, proposing a unified framework which integrates LLMs with classic probabilistic rule combination methods, paving the way for more reliable neuro-symbolic reasoning systems.", "tldr": "", "keywords": ["Rule Learning", "Neuro-Symbolic", "LLM"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d5bc1ea7d11b77ca666b7f36d65c53cfbae6733.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RLIE, a framework that combines LLMs with probabilistic rule learning to generate and refine rules for better decision-making. The RLIE process involves four stages: generating candidate rules using an LLM, applying logistic regression to learn the weights of these rules, refining the rule set iteratively based on prediction errors, and evaluating the model by comparing its performance to other methods of rule integration. The study shows that applying weighted rules directly results in superior performance, but injecting rules into an LLM using prompt-based methods can lead to degraded performance. This suggests that while LLMs are strong in semantic generation, they struggle with fine-tuned, controlled probabilistic integration. The main contribution of the paper is the development of a unified framework that combines LLMs with traditional probabilistic rule combination techniques, advancing the field of neuro-symbolic reasoning systems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and methodologically sound. It provides a clear and detailed explanation of the RLIE framework, including the rule generation, logistic regression, iterative refinement, and evaluation stages. \n2. The work has significant potential for advancing the field of neuro-symbolic AI. By successfully combining LLMs with probabilistic rule learning, it addresses an important challenge in AI: integrating the flexibility of generative models with the precision of rule-based reasoning."}, "weaknesses": {"value": "1. The work lacks experimentation on more models to ensure that its effectiveness is broad and not limited to specific ones.\n2. There are significant formatting issues with Tables 1 and 2, as they exceed the page width.\n3. The introduction to the task in the work is not clear enough. More concrete examples should be introduced to describe the entire process, in order to improve the readability of the paper.\n4. The method is not broadly effective across all tasks, and its average performance improvement over previous work is limited."}, "questions": {"value": "1. Can you reproduce some comparative experiments on weaker open-source models (such as Qwen3) and stronger closed-source models (such as GPT-5) to demonstrate the generalizability of the method?\n2. Why does the method perform significantly worse than the IO Refinement method on the Dreaddit and LLM Detect datasets?\n3. The generalizability of the conclusions found in Section 5.2 is questionable. Is the performance drop caused by the introduction of LLM due to the selection of relatively weaker models? Please add experiments on more models to further support your findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "js9bpQInyH", "forum": "VjGU55hEwV", "replyto": "VjGU55hEwV", "signatures": ["ICLR.cc/2026/Conference/Submission25346/Reviewer_Xf2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25346/Reviewer_Xf2f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725480131, "cdate": 1761725480131, "tmdate": 1762943408918, "mdate": 1762943408918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a large language model (LLM)-based rule learning framework, RLIE. The framework leverages LLMs to generate natural language rules, which are assigned probabilistic weights via a logistic regression model. In the iterative optimization process, difficult samples from task predictions are fed back to the LLM along with the current rule set to generate new rules. The authors evaluate the method on six text classification datasets under different reasoning strategies. The results demonstrate that directly using the rules and their learned weights for prediction achieves better performance.\n\nAlthough the idea of using logistic regression to learn rule weights proposed in this paper is reasonable, the method still has several important flaws in its experimental design that need to be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well-structured and readable.\n2.\tThe methodology on using logistic regression to learn rule weights is sound and shows a certain degree of novelty, compared with the traditional top-K methods."}, "weaknesses": {"value": "1.\tInsufficient baselines for experimental comparisons.   In the experiments, the authors use approximately 400 labeled samples but do not compare with the methods such as few-shot in-context learning (ICL) or fine-tuning neural networks, as discussed in the studies like: “What Makes Good In-Context Examples for GPT 3?” and “In-Context Learning Learns Label Relationships but Is Not Conventional Learning.”\nMoreover, the authors do not assess how the proposed method scales with varying model capacities. It only uses a single backbone model (GPT 4o mini). It is suggested to compare with the models of different sizes or different architectures. \n2.\tLimited experimental tasks. The experiments are confined to binary text classification task, which are relatively simple. For example, in Table 1, the single-rule baseline (IO Refinement) achieves the best performance on two datasets, suggesting that the reasoning complexity is limited. Additionally, the test sets are small (about 300 samples), which further restricts the generalizability of the method.\n3.\tInsufficient analysis of method effectiveness. The paper does not evaluate the quality of the generated rules, such as how the diversity of rules influences the effectiveness of method, what is the impact of the key hyperparameters. Although the coverage threshold γ is fixed at 0.2, it is still required to conduct experiments to examine its influence.\n4.\tLack of clarity in methodological details. It is unclear whether the optimization of rule weights adequately covers all rules. During incremental rule generation, the process for assigning weights to new rules is not explained. Furthermore, in scenarios with frequent rule updates, it remains uncertain whether these weights receive sufficient training."}, "questions": {"value": "1.\tThe single-rule baseline (IO Refinement) outperforms the proposed method RLIE. Does this imply that combining multiple rules might be unnecessary in such cases? How do the authors explain the inferior performance of RLIE compared to a simpler approach on these specific tasks?\n2.\tIt is unclear whether each rule’s weight is sufficiently trained during the iterative optimization process. While stopping criteria for the iterations are provided, there is a lack of statistical analysis or visualizations illustrating the frequency of weight updates or the convergence behavior of individual rules.\n\n\nFinally, it is recommended that the authors conduct further analysis of the generated rules and reasoning outcomes, such as examining the distribution of error types in incorrectly reasoned samples and evaluating whether the new rules have effectively corrected these errors. This is particularly important given that the rules are expressed in natural language, where interpretability is a crucial factor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fpixyzAyeh", "forum": "VjGU55hEwV", "replyto": "VjGU55hEwV", "signatures": ["ICLR.cc/2026/Conference/Submission25346/Reviewer_nMcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25346/Reviewer_nMcw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790355019, "cdate": 1761790355019, "tmdate": 1762943408741, "mdate": 1762943408741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RLIE, a framework designed to integrate Large Language Models (LLMs) with probabilistic rule learning. The primary contribution is a four-stage process that aims to overcome the limitations of traditional rule learning by leveraging the natural language capabilities of LLMs. The stages are: (1) Rule generation, where an LLM proposes and filters candidate rules in natural language; (2) Logistic regression, which learns probabilistic weights for the rules to enable global selection and calibration; (3) Iterative refinement, where the rule set is continuously optimized based on prediction errors; and (4) Evaluation, which assesses the performance of the learned rule set. The goal is to create a more robust neuro-symbolic reasoning system by combining the generative power of LLMs with classical probabilistic methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important and challenging problem: integrating the semantic capabilities of LLMs with more structured, probabilistic reasoning frameworks.\n- The proposed iterative refinement loop, where the LLM is prompted to revise rules based on model errors, is an interesting idea for automated feature engineering.\n- The work explores different ways of combining learned rules with LLMs for inference, leading to an interesting (though negative) result about the difficulty of fine-grained probabilistic control in LLMs."}, "weaknesses": {"value": "- The central concept of a \"rule\" is ill-defined and misleading. What the paper calls \"rules in natural language\" are effectively just natural language prompts or questions posed to an LLM to generate ternary features (+1, 0, -1). These \"rules\" lack the formal structure, interpretability, and compositionality of rules in traditional symbolic systems.\n- The experimental comparison is flawed. The paper compares the performance of a trained logistic regression model against a prompted LLM that is given the rules and weights. This is an unfair comparison, as one is a trained system while the other is not. The potential of the LLM-based classifier has not been fully explored.\n- The empirical evaluation is weak and lacks rigor. The experiments are conducted using only a single, small model (\"gpt-4o-mini\"). The paper's conclusions about LLM capabilities are therefore based on very limited evidence and may not generalize to other, more capable models.\n- The system demonstrates no compositionality between rules, which is a key feature of traditional rule-based systems. The \"rules\" are treated as independent features for a linear model."}, "questions": {"value": "1. The paper's claims revolve around \"rules,\" but the learned artifacts appear to be non-compositional natural language prompts for feature extraction. Can you justify the use of the term \"rule\" and explain how these differ from simple learned features, given their lack of formal structure or compositionality?\n2. The paper's primary conclusion relies on an experimental setup that compares a trained model (Logistic Regression) with an untrained one (a zero-shot LLM), using only a single, non-frontier model. How can the general claims about LLM limitations be supported by this specific and seemingly flawed comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X3vC68fEYy", "forum": "VjGU55hEwV", "replyto": "VjGU55hEwV", "signatures": ["ICLR.cc/2026/Conference/Submission25346/Reviewer_95e3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25346/Reviewer_95e3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895604655, "cdate": 1761895604655, "tmdate": 1762943408485, "mdate": 1762943408485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RLIE (Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation), a neuro-symbolic framework that combines large language models with classical probabilistic modeling for interpretable rule learning. The proposed pipeline consists of four stages: rule generation via LLMs, weight estimation using logistic regression, iterative refinement on hard examples, and evaluation under four inference strategies (E1–E4). Experiments on six binary classification tasks demonstrate that the linear-only logistic model (E1) achieves the best performance, suggesting that LLMs are effective in generating rule candidates but less reliable at probabilistic integration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of combining LLM-based semantic rule generation with a probabilistic model for global reasoning is conceptually appealing."}, "weaknesses": {"value": "1. Although the paper presents a well-organized framework that integrates LLM-based rule generation with probabilistic weighting via logistic regression and iterative refinement, the overall idea is conceptually incremental. The notion of combining LLM-generated symbolic rules with classical probabilistic or statistical models has already appeared in several recent neuro-symbolic or rule-learning works.\n\n2. The experiments rely solely on GPT-4o-mini with a near-deterministic decoding setting. It remains unclear whether the observed results, especially the relative advantages of the linear combiner over LLM-augmented inference, would hold for other LLMs.\n\n3. The paper contains several noticeable formatting problems that affect readability. For example, Tables 1 and 2 exceed the page margins, and some layout elements are misaligned. In addition, there are minor typographical errors—most notably, “Lange Models” in the abstract should be “Language Models.” Careful proofreading and layout adjustments are recommended before publication.\n\n4. No error analysis is provided to explain where and why the proposed method succeeds or fails. Moreover, although the authors claim that the learned rules are interpretable, there are no visualizations or case studies demonstrating the semantic plausibility or human-understandability of these rules. Adding such analyses would significantly enhance the paper’s insightfulness and credibility."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n5J6n79Ebr", "forum": "VjGU55hEwV", "replyto": "VjGU55hEwV", "signatures": ["ICLR.cc/2026/Conference/Submission25346/Reviewer_gbA1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25346/Reviewer_gbA1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996785305, "cdate": 1761996785305, "tmdate": 1762943408203, "mdate": 1762943408203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}