{"id": "YM6KIpl6aR", "number": 18198, "cdate": 1758285069242, "mdate": 1759897120271, "content": {"title": "Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction", "abstract": "Transductive conformal prediction addresses the simultaneous prediction for multiple data points. Given a desired confidence level, the objective is to construct a prediction set that includes the true outcomes with the prescribed confidence. We demonstrate a fundamental trade-off between confidence and efficiency in transductive methods, where efficiency is measured by the size of the prediction sets. Specifically, we derive a strict finite-sample bound showing that any non-trivial confidence level leads to exponential growth in prediction set size for data with inherent uncertainty. The exponent scales linearly with the number of samples and is proportional to the conditional entropy of the data. Additionally, the bound includes a second-order term, dispersion, defined as the variance of the log conditional probability distribution. We show that this bound is achievable in an idealized setting. Finally, we examine a special case of transductive prediction where all test data points share the same label. We show that this scenario reduces to the hypothesis testing problem with empirically observed statistics and provide an asymptotically optimal confidence predictor, along with an analysis of the error exponent.", "tldr": "Fundamental bounds on the trade-off between the efficiency of transductive confidence predictors and the confidence levels", "keywords": ["Conformal prediction", "fundamental limits of efficiency", "information theory", "finite block length analysis", "transductive prediction"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b00eb71b806258c1cc0dd5344bd3e8f095cdd11b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission is essentially split into two separate parts: first, it provides bounds on the set size for \"transductive\" CP by assuming a known data distribution; and then it provides a set-based extension of a known result by Gutman on a special case of the problem.\n\nThe first part builds on standard information-theoretic tools, while the second leverage standard results on binary hypothesis testing with training data.\n\nExperimental results are provided for the first part only."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The two parts of the submission, while limited on their own, offer a useful reference for researchers interested in \"transductive\" CP (defined here as the problem of producing a set prediction for a batch of test samples). \n\nThe paper is clearly and formally written, and useful pointers are provided to the literature."}, "weaknesses": {"value": "The analysis in Section 3 essentially assumes knowledge of the data distribution. While it is true that this assumption yields upper  bounds on the true performance of transductive CP, it is also the case that the assumption appears to completely hide the role of the calibration data size m. \n\nFurthermore, the results of the analysis appear to be rather expected and limited in scope. The typical set of a sequence of i.i.d. variables grows exponentially with the entropy, and so must also any prediction set with non-vanishing coverage. The result in Theorem 3.4 is also a refinement of the same idea. \n\nThe setting studied in Section 4 is a direct extension of Gutman's work on  binary hypothesis testing with training data. In fact, most of the section is devoted to reviewing existing results, and the new contribution follows directly by reframing the problem as one of set prediction. \n\nNo experimental results are provided for the setting studied in Section 4."}, "questions": {"value": "1) How can the analysis in Section 3 be extended to provide insights on the role of the size of the calibration dataset?\n\n2) Can Section 4 be rewritten to address directly the case with any number of hypotheses?\n\n3) If so, can Theorem 5 be simplified to provide clearer insights into the average prediction set size?\n\n4) How do the results in Section 4 connect to the theorem in Section 3?\n\n5) Can experimental results be provided to relate the material in Section 3 and Section 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9vi2QWMD3L", "forum": "YM6KIpl6aR", "replyto": "YM6KIpl6aR", "signatures": ["ICLR.cc/2026/Conference/Submission18198/Reviewer_psdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18198/Reviewer_psdB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760520720858, "cdate": 1760520720858, "tmdate": 1762927945051, "mdate": 1762927945051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In standard setup for conformal prediction we guarantee the coverage per each test point. This paper addresses the problem for joint prediction sets where the guarantee is to be derived for joint coverage — all labels should be in all prediction sets within a batch to be count as a coverage. The authors pose two questions, one on the information theoretic bound over the trade off between the set size and the joint coverage, and the other is how can er find an optimal way to leverage the entire labeled set (including the training set) for conformal prediction without sacrificing the coverage guarantee (conventionally, this guarantee breaks if we introduce training points to the process).\n\nThe authors provide lower bound on this setup showing that either the set size increases drastically or the guarantee vanishes to zero by increasing the number of points over which the decision is jointly made. \n\nFurthermore they discuss a specific case where the joint prediction is over a single label -- the case where a single datapoint is examined several times for robustness. \n\n*Disclaimer.* I should note that due to the representation issues I mentioned in the weaknesses, I could not fully follow the paper."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The targetted problem is very interesting (however in the naming it intersects with full conformal prediction). Being able to provide joint guarantee better than bounds from the Bonferroni correction is applicable. \n2. The theory of the paper is concretely written, and (to the best of my understanding during reading the paper) all theorems are proved solidly.\n3. The connection of their work to robustness (while it should not be mistaken with the adversarial and probabilistic robustness) is very interesting."}, "weaknesses": {"value": "**Introduction is not easy to read.** Surely the introduction is written with a solid mathematical notation and there is no issues with that. But it is at least not friendly to non-expert reader. I would suggest elaborating more to the comparison of inductive and transductive conformal prediction for example and offer a one sentence brief explanation by what you mean when introducing a new concept. One really helpful way to improve the readability is to directly say that “by transductive conformal inference on a batch of datapoints we are interested on he probability that all labels are within all prediction sets”. I know that formally it can be inferred from the text, but explicitly saying that in the introduction increases the reading speed considerably.\n\nThe authors introduce two interesting questions in Section 2, while I can not see a footprint of those questions (specifically the second one) in the introduction.\n\nEven the setup with all equal labels is not presented as it is in the introduction. At least it could be better if the authors mentioned the robust prediction application when introducing the setup for the first time.\n\n**General readability.** The paper (due to the subject) is already not easy to read, and the authors sometimes introduce a new notation without fixing it. For instance $P_e$ in line 48. The authors also do not provide any intuitive message from the theorems they proof (for example in theorem 3.1 I can not parse what the theorem is trying to say about the joint probabilities). \n\n**Limited Experimental Setup.** The paper only introduces numerical results on MNIST dataset. I do not count this as a negative point in my score as the paper is a theory paper. The question remains that how their results could be written in terms of a lower bound for any dataset. If the authors provide a clear algorithmic approach to derive the bounds over any dataset, then I would increase my score."}, "questions": {"value": "1. In line 54 is the term $P(Y^{m+n}_{m+1}m | X^{m+n}_{m+1}m)$ equal to the product of all conditional probabilities from m+1 to m+n? If so, can you elaborate why it not a function of the number of elements? From reading your proof I assume this is because the value alpha already encodes the number of the elements but I am not sure why.\n2. Is the entropy mentioned in Theorem 3.2, written in terms of the true confidence? Is there any bounds on the accuracy of the model?\n3. What is delta in theorem 3.4? Also is there any intuitive understanding about the other variables sigma and rho? What are they encoding?\n4. How your results can be expressed in terms of a guaranteed upper bound on the joint guarantee for any dataset? Is this bound affected by the number of classes or the quality of the model? Or is there any need to exactly specify the ground truth p(y|x) to derive these bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "acC2Ywq76A", "forum": "YM6KIpl6aR", "replyto": "YM6KIpl6aR", "signatures": ["ICLR.cc/2026/Conference/Submission18198/Reviewer_MyY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18198/Reviewer_MyY1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820598080, "cdate": 1761820598080, "tmdate": 1762927944151, "mdate": 1762927944151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a theoretical paper that characterizes the trade off between coverage and set size of transductive conformal prediction in the classification setting. In transductive conformal prediction, the goal is to construct prediction sets for $n$ test points such that all test labels fall within the joint prediction set with probability at least $1-\\alpha$. The paper proves both asymptotic limits and non-asymptotic bounds for the “efficiency rate”, a normalized measure of the size of the transductive prediction set."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes some interesting connections between conformal prediction and information theory and applies some interesting tools."}, "weaknesses": {"value": "As someone who does not have a strong background in information theory or familiarity with Gutman’s test, I found it hard to follow section 4."}, "questions": {"value": "1. Theorem 3.1: Can you add the interpretation of this result in words? This applies to all theorem statements. \n2. Do you view this work as more than simply “theory for the sake of theory”? Can this theory eventually lead to work that will inform practice? \n3. On line 53, you write “When all test points share the same label, a scenario relevant to safety-critical applications…” — what is an example? \n\nTypos/stylistic comments:\n* There are multiple places where \\citep is used where \\citet should be used instead\n* I would mention somewhere that what you call “confidence” is commonly called “coverage” in the conformal prediction literature\n* Line 53: “same label” -> “same unknown label” \n* Line 143: Capitalize “in”\n* Line 174-175: asymptotically is used twice in this sentence\n* Line 175: “In case” -> “In the case” \n* Line 245: remove “setup”\n* Line 249-250: I would replace the “=“ with “:=“  \n* Line 281: “prediction sets a single” -> “prediction sets with a single”\n* Line 283: math mode error\n* Theorem 4.4: “M class”? Should this be “M-class setting”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VfZ4kgWLWs", "forum": "YM6KIpl6aR", "replyto": "YM6KIpl6aR", "signatures": ["ICLR.cc/2026/Conference/Submission18198/Reviewer_FpE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18198/Reviewer_FpE3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957099941, "cdate": 1761957099941, "tmdate": 1762927943567, "mdate": 1762927943567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper asks how small a joint prediction set can be when doing transductive conformal prediction at a given confidence. It proves lower bounds on expected set size. The main asymptotic message is that to maintain any non-trivial confidence, the expected joint set must grow roughly like exp of n times conditional entropy. A non-asymptotic refinement adds a second order “dispersion” term that depends on the variance of log probabilities. The authors also give an achievability result in an oracle setting where P of Y given X is known, and they analyze a special case where all test points share one true label by connecting to classical hypothesis testing with the generalized Jensen–Shannon divergence. They include a small MNIST toy study to illustrate trends."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Puts a clean information-theoretic lens on transductive conformal prediction and tries to sharpen earlier entropy-based limits with a finite sample expansion. The reduction to hypothesis testing in the same-label case is neatly explained and hooks into known optimal tests. \n\nThe statements are precise, the asymptotic and finite-blocklength styles are consistent, and the proofs trace to standard tools like information density bounds, Berry–Esseen, and method of types. The paper is careful about what is guaranteed and where the constants come from. \n\nKey quantities like efficiency rate, dispersion, and the role of conditional entropy are defined clearly. The contrast between joint confidence and per-point Bonferroni is made explicit and illustrated. The same-label section is self-contained and readable. \n\nJoint guarantees are relevant in batch certification, safety screening, and ranking. Having even pessimistic lower bounds helps practitioners understand why transductive sets often balloon as n grows. The work could become a common reference when teams debate whether transductive guarantees are worth the price in set size."}, "weaknesses": {"value": "The headline asymptotic bound reaffirms the conditional entropy barrier already highlighted in recent work on conformal efficiency. The finite sample refinement with a dispersion term is welcome, but the experiments show a persistent gap that closes slowly, which makes it hard to see the practical sharpening. \n\nThe achievability result assumes oracle access to P of Y given X. That turns the task into thresholding products of true class probabilities and inevitably matches the converse to first and second order. This is informative theoretically but not actionable. The paper stops short of proposing any implementable transductive procedure that approaches the bound when P of Y given X is learned with error.\n\n\nEfficiency is measured only by expected set size. In transductive practice, teams often optimize other surrogates like false coverage proportion, cost-weighted set size, or rank-based utility. The bounds are said to “extend” to other notions, but those are deferred. Without at least one nontrivial worked out alternative, the generality claim feels thin. \n\nThe MNIST label noise toy is not enough. It uses simple noise models where H of Y given X is tractable, then shows that Bonferroni blows up. That result is unsurprising. There is no stress test on real transductive use cases such as ranking or batch classification with covariate shift, no study of how well one can estimate H and dispersion from data, and no attempt to check tightness against a strong transductive algorithm rather than a Bonferroni baseline. \n\nDispersion is defined as the standard deviation of log P of Y given X. This could be quite useful as a diagnostic, yet the paper does not show how to estimate it reliably from finite data or how it correlates with observed set growth. The reader is left without a recipe to turn the bound into an engineering rule of thumb."}, "questions": {"value": "Can you give a theorem or a corollary that shows your finite sample lower bound strictly dominates prior entropy-only bounds over a clear domain of alpha and n, with explicit constants? A small synthetic where you can compute both exactly would make the gain concrete. \n\nSuppose P of Y given X is approximated by a calibrated classifier with a known risk or Bregman divergence to the truth. Can you translate that misspecification into a slack in the achievability side, even if loose? A bound of the form “gap grows as epsilon to some power” would be valuable. \n\nPick one alternative efficiency metric, for example expected rank of the true label inside the joint set or a budgeted FCP, and carry your full derivation through to a nontrivial corollary. This would support the claim that the framework covers broader measures. \n\nCan you sketch an extension of the finite sample bound to continuous X with mild regularity on scores, perhaps using empirical process tools instead of types? Even a simplified theorem for plug-in density ratios would widen the impact. \n\nProvide a practical estimator and a confidence band for H of Y given X and dispersion from held-out data, with a study of bias and variance. Then compare the predicted lower bound against observed growth on at least one real task. This would turn the theory into a planning tool. \n\nCompare your bound with the joint set sizes produced by a modern transductive method that is more nuanced than Bonferroni, across a range of n and calibration sizes. Highlight the regimes where the bound is close to achievable and where there is a big gap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e4wy91nNnu", "forum": "YM6KIpl6aR", "replyto": "YM6KIpl6aR", "signatures": ["ICLR.cc/2026/Conference/Submission18198/Reviewer_QYpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18198/Reviewer_QYpU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026078737, "cdate": 1762026078737, "tmdate": 1762927943031, "mdate": 1762927943031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}