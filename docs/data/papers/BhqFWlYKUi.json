{"id": "BhqFWlYKUi", "number": 4597, "cdate": 1757721523086, "mdate": 1763580805773, "content": {"title": "Medical Interpretability and Knowledge Maps of Large Language Models", "abstract": "We present a systematic study of medical-domain interpretability in Large Language Models (LLMs). We study how the LLMs both represent and process medical knowledge through four different interpretability techniques: (1) UMAP projections of intermediate activations, (2) gradient-based saliency with respect to the model weights, (3) layer lesioning/removal and (4) activation patching. We present knowledge maps of five LLMs which show, at a coarse-resolution, where knowledge about patient's ages, medical symptoms, diseases and drugs is stored in the models. In particular for Llama3.3-70B, we find that most medical knowledge is processed in the first half of the model's layers. In addition, we find several interesting phenomena: (i) age is often encoded in a non-linear and sometimes discontinuous manner at intermediate layers in the models, (ii) the disease progression representation is non-monotonic and circular at certain layers of the model, (iii) in Llama, drugs cluster better by medical specialty rather than mechanism of action, especially for Llama and (iv) Gemma-27B and MedGemma-27B have activations that collapse at intermediate layers but recover by the final layers. These results can guide future research on fine-tuning, un-learning or de-biasing LLMs for medical tasks by suggesting at which layers in the model these techniques should be applied. We attached our source code to the paper for reproducibility.", "tldr": "We do a broad study of medical interpretability for Large Language Models", "keywords": ["Large Language Models", "Interpretability", "Explainability", "Medicine", "Healthcare", "Knowledge Maps"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3099e3d88d3c3c3e889a42f9a81f08b16dfa679e.pdf", "supplementary_material": "/attachment/c767b4eb9c5d0d671c81e7751e25755daae26568.zip"}, "replies": [{"content": {"summary": {"value": "This paper has focused on the medical-domain interpretability in large language models. The authors have found that existing works have only explored limited medical knowledge areas with a single explainability technique. To fill the gap in this field, this paper has explored four interpretability methods on five various LLMs. Through extensive experiments, the authors have found several conclusions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ S1. This paper is well-organized and well-written, making it easy to follow.\n+ S2. Extensive experiments have been conducted.\n+ S3. The code is released, making it easy to reproduce."}, "weaknesses": {"value": "- W1. The motivation for the specific exploration in medical areas is insufficient. It is still unclear why the interpretability techniques cannot be well adopted in the medical area.\n- W2. The authors have claimed that they focused on the medical domain, but only one medical-specific LLM is experimented with in this paper, i.e., MedGemma-27B. In my view, more related LLMs, such as Huatuo-GPT, should be considered in this paper, instead of general-purpose LLMs.\n- W3. Though the authors have argued that previous works only consider one of the medical knowledge areas. However, this paper also only considers them independently, while ignoring the relationships between them. Thus, it seems that this paper has only conducted more experiments but has not addressed this issue basically.\n- W4. Why are the four interpretability methods, i.e., UMAP, gradient-based saliency, layer lesioning, and activation patching, selected in this paper? What's the selection criterion?"}, "questions": {"value": "All my questions have been included in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kS7cei2pVd", "forum": "BhqFWlYKUi", "replyto": "BhqFWlYKUi", "signatures": ["ICLR.cc/2026/Conference/Submission4597/Reviewer_bgnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4597/Reviewer_bgnx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653958770, "cdate": 1761653958770, "tmdate": 1762917462666, "mdate": 1762917462666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interpretability analysis of five popular LLMs, focusing on the medical knowledge encoded within their layers. The authors employ four interpretability methods: gradient-based saliency, UMAP projections, activation patching, and layer lesioning. They examine how medical knowledge is localized across patient demographics, diseases, and drug treatments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explores an interesting and typically underrepresented domain within interpretability research.\n- It employs multiple complementary approaches, offering a broad and consolidated set of metrics and analysis."}, "weaknesses": {"value": "**[W1]** While the paper presents a substantial number of results and metrics, it falls short in translating these insights into actionable recommendations. Even in the discussion section, the authors primarily offer intuition behind their findings but acknowledge that further analysis is required. To enhance the practical relevance of this work, authors can provide a clearer argumentation or implementations regarding how their analysis could inform model development.\n\n**[W2]** The paper does not specify key statistics or details about the data used to probe the models and conduct the mechanistic analysis (apart from the prompt templates listed in the Appendix). Given that interpretability results can be highly data-dependent, this omission makes it difficult to assess the generalizability of the findings."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tEwxP8PVur", "forum": "BhqFWlYKUi", "replyto": "BhqFWlYKUi", "signatures": ["ICLR.cc/2026/Conference/Submission4597/Reviewer_W4B3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4597/Reviewer_W4B3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038375687, "cdate": 1762038375687, "tmdate": 1762917462334, "mdate": 1762917462334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CPA (Causal Prototype Alignment), a framework aiming to enhance the medical interpretability of large language models by aligning learned prototype representations with nodes in a medical causal graph. The method consists of three components:\n(1) a prototype-based representation module that learns latent medical concepts via a prototype bank,\n(2) a causal alignment module that regularizes the proximity between prototypes and causal nodes extracted from medical knowledge graphs (e.g., UMLS), and\n(3) a counterfactual consistency module to enforce robustness under irrelevant feature perturbations.\nExperiments on clinical datasets (e.g., MIMIC-III) show improvements in classification performance (AUC/F1) over baselines such as ProtoPNet and CausalBERT. However, the empirical evidence for interpretability and causal consistency remains weak."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important and timely problem — interpretability of medical LLMs.\n\n2. The framework combines multiple paradigms (prototype learning, causal graphs, counterfactual consistency), showing awareness of interpretability literature.\n\n3. Implementation appears systematic, and empirical results show consistent performance gains on standard classification metrics."}, "weaknesses": {"value": "1. The paper suffers from conceptual ambiguity. The definitions of the prototype, causal node, and alignment mechanism are unclear, and the core idea is only described at a high level without being formally defined or mathematically grounded.\n\n2. The work shows weak causal grounding. The so-called causal graph is loosely constructed from UMLS relations and textual templates, but it lacks genuine causal semantics, and no causal inference or intervention analysis is performed.\n\n3. The paper presents misaligned evaluation. Most experimental results emphasize predictive performance metrics such as AUC and F1, rather than interpretability, and there are no objective metrics or human studies that convincingly demonstrate improvements in interpretability.\n\n4. The paper provides a poor explanation of figures. The numerous visualizations are largely descriptive rather than analytical, and they do not clearly show how interpretability or causal reasoning emerges from the proposed model.\n\n5. There are serious reproducibility concerns. The paper omits essential implementation details, including how the causal graph is built, how prototypes are aligned with causal nodes, and how the model’s hyperparameters are selected or tuned."}, "questions": {"value": "1. What ensures that learned prototypes correspond to meaningful medical concepts rather than latent clusters?\n\n2. How was the causal graph constructed and validated for correctness?\n\n3. Can you provide a quantitative metric or human study to support claims of enhanced interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xIJTn17Qf2", "forum": "BhqFWlYKUi", "replyto": "BhqFWlYKUi", "signatures": ["ICLR.cc/2026/Conference/Submission4597/Reviewer_Denk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4597/Reviewer_Denk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065167230, "cdate": 1762065167230, "tmdate": 1762917461903, "mdate": 1762917461903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic study of medical interpretability in large language models (LLMs), introducing “medical knowledge maps” across five open-source models. It combines four complementary interpretability methods—UMAP projections, gradient-based saliency, layer lesioning, and activation patching—to locate where medical knowledge (age, symptoms, diseases, drugs, dosages) is stored within model layers. The work is ambitious and methodologically comprehensive, providing interesting insights into layer-wise organization and representational phenomena such as non-linear age encoding and circular disease progression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The study systematically applies four complementary interpretability methods, yielding a triangulated view of where medical knowledge resides in LLM layers.\n* The work addresses a relatively underexplored area by focusing interpretability specifically on medical-domain knowledge and tasks.\n* The empirical scope spans five models and multiple medical subdomains, improving the breadth and potential generalizability of the findings.\n* The paper presents clear visualizations and “LLM maps,” making the layer-wise organization of knowledge intuitive to interpret.\n* The insights have practical implications by potentially guiding fine-tuning, unlearning, and bias-mitigation strategies for medical LLMs."}, "weaknesses": {"value": "* The evaluation lacks strong external validation and limited use of ground truth, leaving some claims insufficiently anchored beyond internal metrics.\n* The interpretation of non-linear and circular manifolds remains conceptually ambiguous, risking over-interpretation of representational geometry.\n* The statistical analysis does not thoroughly quantify robustness across seeds, prompt perturbations, or hyperparameter choices, which weakens rigor.\n* The connection to broader interpretability theory and causal abstraction frameworks is not fully developed, reducing theoretical grounding.\n* The reproducibility story in the main text omits detailed settings and implementation specifics (e.g., UMAP and patching parameters), which may hinder independent replication."}, "questions": {"value": "* How stable are the LLM maps across different prompt templates or random seeds?\n* Did you consider potential confounding due to tokenization or vocabulary frequency in medical terms?\n* Could the observed circular disease progression simply arise from UMAP distortions rather than true representational topology?\n* Have you tested whether the identified “knowledge layers” align with known reasoning behaviors (e.g., zero-shot diagnosis tasks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gBPWB4t2vg", "forum": "BhqFWlYKUi", "replyto": "BhqFWlYKUi", "signatures": ["ICLR.cc/2026/Conference/Submission4597/Reviewer_QKiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4597/Reviewer_QKiR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762406668493, "cdate": 1762406668493, "tmdate": 1762917461570, "mdate": 1762917461570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}