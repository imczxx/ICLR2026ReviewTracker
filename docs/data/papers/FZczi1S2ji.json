{"id": "FZczi1S2ji", "number": 21345, "cdate": 1758316484902, "mdate": 1759896927254, "content": {"title": "Learnable Wavelet-Enhanced Bidirectional Autoencoders: A Unified Framework for Multi-Resolution Speech Enhancement", "abstract": "Effective representation, reconstruction, and denoising of speech signals are critical challenges in speech signal processing, where signals often exhibit complex, multi-resolution structures. Traditional autoencoders address these tasks using separate networks for encoding and decoding, which increases memory usage and computational overhead. This paper introduces the Learnable Wavelet-Enhanced Bidirectional Autoencoder (WEBA), a framework tailored for efficient signal reconstruction and denoising. WEBA employs a bidirectional architecture, reusing the same network for both encoding and decoding, significantly reducing resource requirements. The framework incorporates adaptive wavelet-based representations through Learnable Fast Discrete Wavelet Transforms, ensuring multi-resolution analysis suited to complex signal structures. Additionally, it leverages Conjugate Quadrature Filters for orthogonal signal decomposition, a Learnable Asymmetric Hard Thresholding function for noise suppression, and a Sparcity-Enforcing Loss Function. By unifying these components into an end-to-end trainable framework, WEBA demonstrates superior performance in signal reconstruction and denoising, surpassing state-of-the-art methods on Valentini’s VoiceBank-DEMAND dataset and CHiME-4 dataset in five key metrics while enhancing computational efficiency. The source code of this paper is shared in this anonymous repository: https://anonymous.4open.science/r/LWE-BAE-A-Unified-Framework-for-Multi-Resolution-Signal-Processing-0DE4.", "tldr": "WEBA: a bidirectional autoencoder with learnable wavelet transforms, adaptive thresholding, and sparsity-aware loss, that delivers SOTA speech enhancement on VoiceBank-DEMAND and CHiME-4 while using roughly half the parameters of competing models.", "keywords": ["speech enhancement", "learnable wavelet transform", "bidirectional autoencoder", "multi-resolution analysis", "deep learning for audio", "conjugate quadrature filters"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/869b0246c9b45b064b49d954d43e7844f8fca147.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Summary: This paper proposes a wavelet-enhanced speech enhancement framework based on bidirectional autoencoders. Core contributions are summarized at three-fold. First, a light-weight learnable fast discrete wavelet transform is proposed to empower the convolution layer with better capability in signal decomposition and reconstruction. Second, a learnable asymmetric hard threshold function is proposed, with better denoising nature and interpretability. Third, a sparsity-enforcing loss is proposed to balance time-domain fidelity and wavelet-domain sparsity. Experiments on VBD and Chime4 datasets validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Strengthens:\n1. I am impressed by the performance as well as its lightweight/elegant design of the method. Despite wavelet design is not novel in speech processing field, it is still rather challenging to keep ``performance-lightweight-elegant’’ within one work. Despite limited datasets and scenarios are considered in this paper, extensive analysis is provided in Appendix, which further validates the effectiveness and potential.\n2. I am glad that the authors provide detailed derivations w.r.t gradient boundary of the LAHT, which supports for the activation effectiveness from a theoretical perspective.\n3. Despite pretrained on a scenario-limited denoising dataset, promising robustness is validated through the evaluations on the mismatched dereverberation task and cross-corpus Chime4. Besides, detailed statistical analysis w.r.t ablation studies are also conducted, rendering the results more convincing."}, "weaknesses": {"value": "Weakness:\n1. As I have mentioned, limited scenario is considered. I think two additional experiments should be considered: Larger denoising benchmark and universal speech enhancement task. For the former, DNS-Challenge [1] is a suitable choice since it has larger speech and noise datasets and can better reflect the performance ranks of various models. For the latter, I recommend using the generation script by URGENT-Challenge [2] for comparisons under different acoustic degradations.\n2. Despite the authors claim that the proposed method enjoys notably lower parameters and computational complexity, no quantitative result is provided.\n3. Despite the performance superiority, no qualitative comparison (i.e., spectral visualizations by different methods) is given.\n\nRefs: \n\n[1] Reddy, C.K., Dubey, H., Gopal, V., Cutler, R., Braun, S., Gamper, H., Aichner, R. and Srinivasan, S., 2021, June. ICASSP 2021 deep noise suppression challenge. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6623-6627). IEEE.\n\n[2] Zhang, W., Scheibler, R., Saijo, K., Cornell, S., Li, C., Ni, Z., Kumar, A., Pirklbauer, J., Sach, M., Watanabe, S. and Fingscheidt, T., 2024. Urgent challenge: Universality, robustness, and generalizability for speech enhancement. arXiv preprint arXiv:2406.04660."}, "questions": {"value": "Questions:\n1. The authors mention the superiority of bidirectional autoencoder in parameter reduction. However, they actually share a similar computational complexity. I think the ablation comparison w.r.t the use of bidirectional AE should also be conducted.\n2. In Page. 6, what is the meaning of Card(.) operation?\n3. No spectral visualization is given, and I suggest that the authors can provide the intermediate result visualizations by low-pass and high-pass branches for better demonstrations.\n4. In Sec. 4.5, for ablation study w.r.t LAHT layer, do the authors directly remove LAHT layer or replace it with other activation functions (e.g. GELU) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IrmRgKoSY2", "forum": "FZczi1S2ji", "replyto": "FZczi1S2ji", "signatures": ["ICLR.cc/2026/Conference/Submission21345/Reviewer_2hkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21345/Reviewer_2hkX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896113083, "cdate": 1761896113083, "tmdate": 1762941710792, "mdate": 1762941710792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WEBA a denoising autoencoder with shared encoder and decoder weights for speech enhancement based on a learnable wavelet multi resolution transform, a learnable thresholding activation and a loss that encourage sparsity of the learned filters. \nThe paper is very interesting and the performance seems strong."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approach is very interesting as it is grounded in classical DSP theory (CQF, perfect reconstruction). \nAdvantages are interpretability and a built in inductive bias that can help in synthesis and reconstruction\nThere are several prior works that incorporate wavelets into DNNs but I still find this contribution novel in its entirety. \n\nThe empirical results are strong on VoiceBank-DEMAND and significance testing is also conducted in the Appendix. \n\nThe weight sharing of the encoder and decoder can be helpful in reducing memory footprint of the model however further optimization might be needed for CPU (0.7 real time). The authors already outline possible mitigations for this. \n\n\"Zero-shot CHiME-4\" cross-datasets generalization experiments show promising numbers. Generalization is a huge problem for speech enhancement and separation and I am grateful that the Authors add this experiment. \n\nThe paper is quite comprehensive in the analysis of the method: sensitivity analysis, robustness to pink and reverberation. While lengthy I enjoyed this additional analysis. \n\nCode will be released."}, "weaknesses": {"value": "There are however some critical things that in my opinion are missing in the paper. \n\n1. Minor: What will be the performance of the same approach but with non weight shared encoder and decoder ? \n\n\n2.  More critical: I think that the metrics are too much signal-focused. Perceptual metrics such as UTMOS should be also reported together with these. Or alternatively listening tests should be performed. A good mix of metrics is usually signal-based (e.g. COVL, CBAK, CSIG, SISDR/SDR etc), perceptual+signal based (PESQ, STOI), perceptual model based (UTMOS, NISQA, DNSMOS etc) and maybe downstream performance (e.g. WER or phoneme error rate). \nI think at least UTMOS should be added. \n\n3. Another pitfall is that I do not understand why in Table 3 there is no comparison with some of the non diffusion models trained on VoiceBank-DEMAND e.g.  Mamba-SEUNet, MP-SENet, SEMamba, CMGAN: https://github.com/ruizhecao96/CMGAN or other popular models like USES, TF-GridNet which are non diffusion and the code is open source. I can understand that for some of those in Table 1 the code is not open source but at least a subset with open source code can be found and can be added. \nI feel that right now these results appear cherry picked and maybe the proposed method did generalize worse than other competing methods from Table I. It is not true that diffusion based methods generalize better than standard discriminative/predictive ones, they just sometimes present higher UTMOS but lower PESQ and signal based metrics due to hallucinations. \n\nThe paper is strong overall but these pitfalls unfortunately need to be addressed in order to be published in my opinion."}, "questions": {"value": "Questions are in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3eQDWMCXnw", "forum": "FZczi1S2ji", "replyto": "FZczi1S2ji", "signatures": ["ICLR.cc/2026/Conference/Submission21345/Reviewer_BdtF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21345/Reviewer_BdtF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933774728, "cdate": 1761933774728, "tmdate": 1762941710504, "mdate": 1762941710504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the speech enhancement problem. The authors propose a speech-enhancement mehtod that incorporates a bidirectional autoencoder reusing the same weights for encoding/decoding to cut parameters, with a learnable fast discrete wavelet transforms (LFDWT) for perfect reconstruction and multi-resolution analysis. In the proposed method, denoising is performed via a learnable asymmetric hard-thresholding (LAHT) nonlinearity in the wavelet domain, while a sparsity-enforcing loss (SELF) is used to balance time-domain fidelity and wavelet-domain sparsity through a scheduled weighting. Provided experiments on VoiceBank-DEMAND  show sota results across the metrics of COVL/CSIG/CBAK, as well as STOI/PESQ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper integrates a learnable wavelet transform (LFDWT) with conjugate quadrature filter (CQF) constraints to ensure perfect reconstruction. This provides a principled way to embed wavele into end-to-end speech enhancement networks. The proposed learnable asymmetric hard thresholding (LAHT) allows the network to adaptively suppress noise in the wavelet domain. In combination with a sparsity-enforcing loss (SELF), it promotes compressibility and improves denoising efficiency. Moreover, using compact bidirectional autoencoder that shares weights between encoder and decode reduces the number of parameters and memory usage. The model achieves promising results on VoiceBank-DEMAND."}, "weaknesses": {"value": "The core ideas of learnable wavelets, threshold-based shrinkage, sparsity regularization, tied-weight autoencoders have all been explored in prior literature. The novelty lies mainly in engineering integration rather than in proposing a fundamentally new method. \n\nIt would be better to include subjective evaluation scores such as MOS and DNSMOS, and downstream ASR WER results to demonstrate the practical usefulness of the proposed method.\n\nAlthough the bidirectional architecture is claimed to be efficient, the authors do not provide inference speed, latency, memory usage, or computational cost analysis."}, "questions": {"value": "See the above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jaov42TjfR", "forum": "FZczi1S2ji", "replyto": "FZczi1S2ji", "signatures": ["ICLR.cc/2026/Conference/Submission21345/Reviewer_Sy1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21345/Reviewer_Sy1B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003031412, "cdate": 1762003031412, "tmdate": 1762941710090, "mdate": 1762941710090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WEBA (Wavelet-Enhanced Bidirectional Autoencoder), a framework for multi-resolution speech enhancement that integrates learnable fast discrete wavelet transforms (LFDWT), conjugate-quadrature filters (CQF), a learnable asymmetric hard-thresholding (LAHT) activation, and a sparsity-enforcing loss (SELF) into a bidirectional autoencoder that shares weights between encoding and decoding.\nThe authors claim that this architecture enables efficient, interpretable denoising with reduced parameters and superior performance on standard speech-enhancement benchmarks (VoiceBank-DEMAND, CHiME-4).\nEmpirically, WEBA outperforms recent deep baselines such as Mamba-SEUNet and CMGAN by modest but consistent margins across PESQ, STOI, and MOS-derived metrics. The paper also provides extensive appendices with mathematical justification, ablation, and significance testing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows a high level of technical rigor and completeness. Its empirical evaluation is careful and well structured, with ablation studies, statistical tests, and cross-corpus experiments that demonstrate consistent improvements across multiple objective metrics. The methodology reflects solid engineering practice: datasets, training procedures, and hyperparameters are clearly described, supporting reproducibility. A notable strength is the integration of classical DSP intuition with deep learning frameworks, particularly the use of multiresolution analysis, sparsity priors, and bidirectional weight sharing, which reflects an effort to bridge interpretability and efficiency. The model’s compactness and reduced parameter count compared to transformer or diffusion architectures make it appealing for resource-constrained applications. Overall, the authors provide good theoretical motivation and empirical validation."}, "weaknesses": {"value": "A core modeling assumption of WEBA is that there exists a learnable, approximately orthogonal transform domain in which speech is sparse and noise is distributed as small, incoherent coefficients that can be removed through thresholding. While this assumption mirrors classical wavelet denoising theory, it oversimplifies the real complexity of speech–noise interactions. In practice, speech and non-stationary noise often share overlapping spectral and temporal characteristics, making any fixed transform-based separation inherently limited. The learned filterbank therefore performs a task-specific statistical mapping rather than discovering a genuine orthogonal subspace where noise is suppressed, meaning that the supposed separation dynamics are largely absent from the actual architecture. More broadly, while the paper is technically solid and empirically well executed, its conceptual novelty is limited. Once the wavelet filters are made learnable, the architecture effectively reduces to a multiscale convolutional autoencoder with tied weights and structured regularization, making the “wavelet” framing largely rhetorical. The claimed orthogonality and perfect reconstruction properties of the learnable filters are only approximated through parameter tying and normalization, rather than being strictly enforced via paraunitary or lifting-based formulations. This weakens the theoretical contribution and blurs the distinction between WEBA and existing CNN-based denoising architectures. The presentation further suffers from verbosity and excessive theoretical exposition; lengthy sections on wavelet theory, uncertainty principles, and energy conservation distract from the core model and its empirical validation. Moreover, the paper refers to COVL, CSIG, and CBAK as “MOS” scores, but these are objective MOS predictors rather than true human-rated Mean Opinion Scores, which may mislead readers regarding the subjective perceptual quality evaluation. Although the model itself is lightweight and computationally efficient, the framing around it overstates its conceptual innovation, and the empirical improvements, while consistent, remain modest relative to contemporary approaches."}, "questions": {"value": "Once the filters are trained, are they still interpretable as valid wavelets (e.g., with measurable vanishing moments)?\nWould adding an explicit paraunitary or lifting-scheme constraint change training stability or quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SYakKPE3tK", "forum": "FZczi1S2ji", "replyto": "FZczi1S2ji", "signatures": ["ICLR.cc/2026/Conference/Submission21345/Reviewer_HHg3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21345/Reviewer_HHg3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053606571, "cdate": 1762053606571, "tmdate": 1762941709753, "mdate": 1762941709753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}