{"id": "FkeURAdA0h", "number": 19125, "cdate": 1758293739611, "mdate": 1759897058219, "content": {"title": "Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning", "abstract": "While goal-conditioned behavior cloning (GCBC) methods can perform well on in-distribution training tasks, they do not necessarily generalize zero-shot to tasks that require conditioning on novel state-goal pairs, i.e. combinatorial generalization. In part, this limitation can be attributed to a lack of temporal consistency in the state representation learned by BC; if temporally correlated states are properly encoded to similar latent representations, then the out-of-distribution gap for novel state-goal pairs would be reduced. We formalize this notion by demonstrating how encouraging long-range temporal consistency via successor representations (SR) can facilitate\ngeneralization. We then propose a simple yet effective representation learning objective, $\\text{BYOL-}\\gamma$ for GCBC, which theoretically approximates the successor representation in the finite MDP case through self-predictive representations, and achieves competitive empirical performance across a suite of challenging tasks requiring combinatorial generalization.", "tldr": "", "keywords": ["Sequential Decision Making", "Combinatorial Generalization", "Representation Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eda5cecf57a473654c94f003e5fcace63f997591.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed BYOL-$\\gamma$ for learning representations in goal-conditioned reinforcement learning (GCRL) problems. The core idea of BYOL-$\\gamma$ is simple: instead of learning representations by aligning one-step latent transitions as in prior RL methods utilizing BYOL, BYOL-$\\gamma$ learns representations by aligning multi-step latent transitions following the successor measures. Empirically, BYOL-$\\gamma$ achieves competitive empirical performance across a suite of challenging tasks from the OGBench benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Learning representations for control is an important topic in RL. The paper motivates learning temporal representation for the GCBC problem by using the successor measure. Instead of bootstrapping the representations of the next state as in BYOL, BYOL-$\\gamma$ proposes to bootstrap the representations of a future state sampled from the successor measure, which encodes temporal information in the dataset. The learned representations are then used to drive the learning of the goal-conditioned policy via behavioral cloning.\n\n- The author conducted experiments on the challenging OGBench benchmarks, showing that BYOL-$\\gamma$ achieves competitive empirical performance, compared to prior representation learning methods for GCBC and prior goal-conditioned reinforcement learning methods in the offline setting. Additional experiments ablate the key components of BYOL-$\\gamma$ and demonstrate that BYOL-$\\gamma$ can generalize over longer horizons than prior methods."}, "weaknesses": {"value": "-  The paper reviewed different representation learning objectives for RL in Sec. 3.1 and reviewed the generalization gap from prior work in Sec. 3.2 and the beginning of Sec. 4 (line 223 - line 249). However, Sec. 4.1 proposed the BYOL-$\\gamma$ objective directly without explaining its connection with the preliminaries. It is difficult to understand the roles of Sec. 3.1, Sec. 3.2, and the beginning of Sec. 4 in the paper.\n\n- One of the motivations of developing BYOL-$\\gamma$ is to learn representations that enable combinatorial generalization in GCRL. The objective function in Eq. 8, the following Eq. 9 and Theorem 4.1 do not provide enough explanations for why the objective function can enable combinatorial generalization. \n\n- Sec 4.2 introduces another auxiliary loss function called TD-SR for learning representations. The relationships between TD-SR and BYOL-$\\gamma$ are not clear from the current draft, making it unclear whether the paper proposed BYOL-$\\gamma$ or TD-SR as its main representation learning objective.\n\n- Results in Table 3 seem to suggest some components of BYOL-$\\gamma$ has minor effects.\n\nOverall, the motivations, the algorithms, and the theory are somewhat inconsistent."}, "questions": {"value": "- In Sec. 3.1, what are the connections between CL and TD-SR to BYOL-$\\gamma$? Does the connection between CL and TD-SR, line 174: \"an n-step version of TD-SR is related to CL\", help us to understand BYOL-$\\gamma$?\n\n- Sec. 3.2 and the beginning of Sec. 4 reviewed prior definition of generalization gap in GCBC. What are the connections between these definitions and the BYOL-$\\gamma$ objective in Sec. 4.1?\n\n- The motivation of developing the objective in Eq. 8 is to enable combinatorial generalization. But why does this objective enable combinatorial generalization? What is the policy $\\pi$ in this equation? Is it the behavioral policy or the learned policy?\n\n- Eq. 10 includes a bidirectional prediction term. Any explanation for the reason for including this bidirectional prediction term? Results in Table 3 seems to suggest the effect of this bidirectional prediction term is minor.\n\n- Sec 4.2 introduces another auxiliary loss function called TD-SR. What’s the relationship between TD-SR and BYOL-$\\gamma$? Does the paper propose TD-SR or BYOL-$\\gamma$ as the main representation learning objective?\n\n- The second and third columns in Table 1 are confusing. From line 205, it looks like $\\tilde{M}^{\\beta}(s, s_{+})$ and $\\sum_j p(\\beta_j \\mid s) \\tilde{M}^{\\beta_j}(s, s_{+})$ are equivalent to each other. What’s the difference between these two columns in the context of different methods?\n\n- Line 335: More loss terms with the same batch of dataset typically results in higher learning efficiency. Is this sentence saying that BYOL-$\\gamma$ is less efficient than CL and TD-SR?\n\n- Sec 5.1 visualizes the representation learned by different methods. From Fig 2, it looks like the cosine similarity of BYOL-$\\gamma$ and TD-SR are more concentrated than BYOL and TRA. How can we relate these representation heatmaps to the reachability of different states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lUlvy6geMc", "forum": "FkeURAdA0h", "replyto": "FkeURAdA0h", "signatures": ["ICLR.cc/2026/Conference/Submission19125/Reviewer_c6Ng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19125/Reviewer_c6Ng"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545319743, "cdate": 1761545319743, "tmdate": 1762931146149, "mdate": 1762931146149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the question of representation learning to improve combinatorial generalization in behavior cloning: what learning objective best recovers the latent dynamics of the environment? They focus on the self-predictive Bring Your Own Latent framework, introducing a modified objective BYOL-$\\gamma$ that displays better ability to stitch together long trajectories, enabling better generalization in longer-horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of the paper is generally good (easy to follow, good motivation of the problem, good coverage of related work)\n    - In particular, the paragraph that discusses successor representations at the beginning of section 3 was very clear\n2. The introduction to the experiments section (section 5 on page 7) is very nice and thorough\n3. The various kinds of experimental results reported (eg, Figures 2 and 3) are interesting\n    - The decision to \"bold values within 95% of [the best] value in the same row\" (table 2) is a great idea, very helpful for quickly parsing/comparing methods within each row\n    - The latent state representation visualizations are very interesting\n4. The inclusion of a limitations section at the end of the paper is appreciated\n5. The BYOL-$\\gamma$ method is motivated both intuitively and theoretically\n    - Because of this, as well as because the paper seeks to study the broader topic of representation-learning objectives, I would say that the simplicity of the method (i.e., the similarity to the prior BYOL objective, eq. 3) is not really a weakness."}, "weaknesses": {"value": "1. The theoretical assumption of symmetric transition dynamics does not seem realistic.\n2. The qualitative results of the representation visualizations aren't convincing: the representation similarities for BYOL-$\\gamma$ do not look clearly better than, e.g., BYOL and TRA.\n    - In mazes, presumably (based on the concepts in this paper) the embedding similarity should decrease roughly monotonically with distance from the goal; thus, along with the provided visuals in Figures 2 and 6, it could be helpful to provide a one-dimensional scatterplot of \"similarity (e.g., dot product) vs. true shortest-path distance from goal\" for each sampled point in the space; this could facilitate easier (and less subjective) comparison of the methods.\n3. The results in Figure 5 (Appendix F) seem to show that BYOL-$\\gamma$ is not really better than the other methods (i.e., the plot for Figure 3 in the main text seems cherry-picked).\n4. It could be useful to study some more interesting environments of different types, since all the experiments are just done in various mazes."}, "questions": {"value": "1. The topic I am most unclear on is that equations (7) and (8) seem that they would be prone to collapse. How is this avoided? Is it related to the use of `stop gradient`? (the below points should be interpreted as both parts of this question and notes/suggestions)\n    - The invariance $\\phi(s_f) \\approx \\phi(s_w)$ seems that, without any \"counter-balancing\" from a term that differentiates states, it would coerce $\\phi(\\cdot)$ to encode all states similarly. How is this avoided?\n    - The theoretical results in Appendix D mention stability, but it could be helpful to also discuss this in the main text.\n2. This is not a question, but I wanted to note that the referenced paper (Richens et al., 2025) reminded me of the paper \"Resolving Causal Confusion in Reinforcement Learning via Robust Exploration\" (Lyle et al., 2021) that similarly discusses the importance of learning about the environment in order to generalize. This is not to say that it should be cited here; I just thought it might be interesting to the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ws6v13Zvv0", "forum": "FkeURAdA0h", "replyto": "FkeURAdA0h", "signatures": ["ICLR.cc/2026/Conference/Submission19125/Reviewer_JKTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19125/Reviewer_JKTQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776272067, "cdate": 1761776272067, "tmdate": 1762931145373, "mdate": 1762931145373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on Goal-Conditioned Behavior Cloning and its inability to “stitch” or as they refer, perform combinatorial generalization. They attribute this inability to the lack of temporal consistency in state representations. The authors look into dynamics aware state representations and connections to successor measures to instill representations that are close for the states visited by can be sampled by successor measure of the policy collecting the data. In other words, the representations of all the states collected by a policy should be close. To ensure scalability, the authors look into BYOL style contrastive methods rather than TD based methods to learn representations that can predict “future” state representations from the policy. The authors further create a few variants of their method using a bidirectional loss. The policy is trained using BC with this BYOL-$\\gamma$ being an auxiliary loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The authors formalize the combinatorial generalization gap or the stitching error using a mixture policy. This formulation is interesting and can be used to study several algorithms and future works. \n\n(2) The authors expand on self-predictive representations from single step or n-step to a general geometric distribution over long horizons. The authors also link this representation to successor measures and show how these representations can be used to estimate successor measures (and features)."}, "weaknesses": {"value": "(1) Empirically there is a minor improvement over TD-SR. In visual setting, the performance is similar to GCBC. First of all, how is GCBC stitching? Wasn’t the argument that GCBC cant stitch?\n\n(2) To goal to enforce stitching in GCBC was to come up with an offline RL method that can scale (as the authors claim the TD methods dont). But in visual environments, their method does not scale well too (similar performance to GCBC)\n\n(3) $\\psi$ is a network and $\\psi^\\pi$ is successor features which is confusing, a different symbol should be used.\n\n(4) While the motivation for adding stitching ability to GCBC makes some sense, the motivation for the representation learning objective is not strong. The representation sure can be used to estimate successor measures but does that make them better for BC or does that allow stitching? Are these representations helping with stitching in a theoretical way or its an empirical hypothesis/observation?\n\n(5) Does adding a bidirectional prediction affect the theoretical properties? As the loss changed so the solution would change too."}, "questions": {"value": "(1) The authors say (on page 5, line 238-243) that they want an invariance $\\phi(s_f) \\approx \\phi(s_w)$. This can be fine for the BC policy that was conditioned for $\\phi(s_f)$ but would feel is going towards $\\phi(s_w)$. But what about the BC policy from $\\phi(s_w)$ to $\\phi(s_f)$? Wont it assume that it's already at the goal?\n\n(2) How is a collapse avoided? It looks like all states of a trajectory would be mapped close to each other so for the same example, $\\phi(s_0) \\approx \\phi(s_w)$ as they belong to some policy and $\\phi(s_f) \\approx \\phi(s_w)$ which would also imply, $\\phi(s_f) \\approx \\phi(s_0)$. How is a collapse prevented?\n\n(3) Suppose there are four trajectories, $s_0$ to $s_w$, $s_w$ to $s_f$, $s_0$ to $s_y$ and $s_y$ to $s_f$. The trajectory $s_0 - s_y - s_f$ is optimal. Would this method be able to find out the optimal as there is no notion of reward/cost/value?\n\n(4) Are the results statistically significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "atnMSAL0YP", "forum": "FkeURAdA0h", "replyto": "FkeURAdA0h", "signatures": ["ICLR.cc/2026/Conference/Submission19125/Reviewer_o9Vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19125/Reviewer_o9Vq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984081376, "cdate": 1761984081376, "tmdate": 1762931144944, "mdate": 1762931144944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the failure of Goal-Conditioned Behavioral Cloning (GCBC) on tasks requiring combinatorial generalization (\"stitching\"). The authors propose BYOL-γ, a self-predictive auxiliary loss. The core idea is that this loss encourages the learning of representations that approximate the Successor Representation (SR) without the instability of TD-learning or the pessimism of contrastive learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- simple, intuitive design.\n- well-suited for offline setting (as shown in TD-SR < BYOL-γ).\n- good empirical performance."}, "weaknesses": {"value": "- The authors explicitly benchmark only against non-hierarchical methods. However, the current state-of-the-art on the OGBench 'stitch' tasks (e.g., HIQL) is hierarchical. By omitting this comparison, the paper fails to demonstrate how its performance stacks up against the actual SOTA, making its practical significance unclear. It also remains unclear whether BYOL-γ's benefits could be combined with hierarchical methods.\n\n\n- The empirical improvements on visual-based tasks (visual-antmaze, visual-scene-play) are marginal at best (Table 2). In some cases (e.g., visual-antmaze-large), the proposed BYOL-γ (26.0) performs worse than the standard GCBC baseline (29.2), questioning the method's applicability and benefits for high-dimensional visual inputs."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9QtVJHa2ac", "forum": "FkeURAdA0h", "replyto": "FkeURAdA0h", "signatures": ["ICLR.cc/2026/Conference/Submission19125/Reviewer_6mmR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19125/Reviewer_6mmR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137526275, "cdate": 1762137526275, "tmdate": 1762931144505, "mdate": 1762931144505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}